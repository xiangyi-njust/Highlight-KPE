"Title","Year","Source title","DOI","Link","Abstract","Author Keywords"
"PAGroup: Privacy-aware grouping framework for high-performance federated learning","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146431077&doi=10.1016%2fj.jpdc.2022.12.011&partnerID=40&md5=d729e6cbda135a68e8722ef1494302d6","Federated Learning is designed for multiple mobile devices to collaboratively train an artificial intelligence model while preserving data privacy. Instead of collecting the raw training data from mobile devices to the cloud, Federated Learning coordinates a group of devices to train a shared model in a distributed manner with the training data located on the devices. However, unbalanced data distribution and heterogeneous hardware configurations across different devices badly hurts the performance of collaborative model and severely impacts the overall training progress. Thus, a framework that can well balance the model accuracy and the training progress is urgently required. In this paper, we propose PAGroup, a privacy-aware grouping framework for high-performance Federated Learning. PAGroup intelligently divides the participating clients into different groups through carefully analyzing the privacy requirement of the training data and the solid social relationship of the participating clients. After that, PAGroup conducts data shaping and capability-ware training in order to improve the model performance while accelerating the overall training process. We evaluate the performance of PAGroup with both simulation and hardware testbed. The evaluation results show that PAGroup improves model accuracy up to 21%. Meanwhile, it decreases 81% communication overhead, 29% computation cost and 84% wall-clock time at best comparing with the baselines. © 2023 Elsevier Inc.","Client grouping; Federated learning; Social relationship"
"A parallel algorithm for approximating the silhouette using a ball tree","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142807629&doi=10.1016%2fj.jpdc.2022.11.001&partnerID=40&md5=c4bd0f490f6fe16d94e40a15bf58af05","Clustering is widely used in many scientific fields. The contribution of enumerating the value of the silhouette is twofold: firstly it can help choosing a suitable cluster count and secondly it can be used to evaluate the quality of a clustering. Enumerating the silhouette exactly is an extremely time-consuming task, especially in big data applications; it is therefore common to approximate its value. This article presents an efficient shared-memory parallel algorithm for approximating the silhouette, which uses a ball tree. The process of initialising the ball tree and enumerating the silhouette are fully parallelised using the OpenMP API. The results of our experiments show that the proposed parallel algorithm substantially increases the speed of computing the silhouette whilst retaining necessary precision for real-world applications. © 2022 Elsevier Inc.","Ball tree; Data mining; High-performance computing; k-means clustering; Silhouette"
"TurBO: A cost-efficient configuration-based auto-tuning approach for cluster-based big data frameworks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150353543&doi=10.1016%2fj.jpdc.2023.03.002&partnerID=40&md5=e2985af21f5d0b21ccad543848ed6916","Big data processing frameworks such as Spark usually provide a large number of performance-related configuration parameters, how to auto-tune these parameters for a better performance has been a hot issue in academia as well as industry for years. Through delicately tradeoff between exploration and exploitation, Bayesian Optimization (BO) is currently the most appealing algorithm to achieve configuration auto-tuning. However, considering the tuning cost constraint in practice, there are three critical limitations preventing conventional BO-based approaches from being directly applied into auto-tuning cluster-based big data frameworks. In this paper, we propose a cost-efficient configuration auto-tuning approach named TurBO for big data frameworks based on two enhancements of vanilla BO:1) To reduce the essential iteration times, TurBO integrates a well-designed adaptive pseudo point mechanism with BO; 2) To avoid the time-consuming practical evaluation of sub-optimal configurations as possible, TurBO leverages the proposed CASampling method to intelligently tackle with these sub-optimal configurations based on ensemble learning with historical tuning experiences. To evaluate the performance of TurBO, we conducted a series of experiments on a local Spark cluster with 9 different HiBench benchmark applications. Overall, compared with 3 representative BO-based baseline approaches OpenTuner, Bliss and ResTune, TurBO is able to speedup the tuning procedures respectively by 2.24×, 2.29× and 1.97× on average. Besides, TurBO can always achieve a positive cumulative performance gain under the simulated dynamic workload scenario, which means TurBO is indeed appropriate for workload changes of big data applications. © 2023 Elsevier Inc.","Bayesian optimization; Big data framework; Configuration parameter; Pseudo point; Tuning cost"
"A multi-objective cloud energy optimizer algorithm for federated environments","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145646090&doi=10.1016%2fj.jpdc.2022.12.007&partnerID=40&md5=f7e78e2792c8b1ff9527c4b890561b3e","Energy is a considerable portion of the cost of a cloud data center. In addition to energy, carbon emission tax imposes another cost on data centers, which have different policies in different cities around the world. Most of the cloud data centers are geographically distributed, which can have a huge advantage in reducing such costs. In addition, in recent years, cloud federation, in which multiple cloud providers share their IT infrastructures voluntarily, could play a crucial role in minimizing the energy consumption of cloud data centers. On the other hand, improper VM placement results frequent VM migration, constant turning off/on physical machines, service quality degradation and energy consumption increase. To tackle these weaknesses, we propose a multi-objective algorithm, Called OUR-ACS, to minimize data centers' energy consumption, carbon emission, and the total expenses (energy cost + carbon tax) in a federated environment considering both initial VM placement and VM consolidation approaches. We evaluate the efficiency of our proposed algorithm by using the ClousSim Plus simulator toolkit using real-world datasets. Compared to the competing algorithms, our simulation results indicate that our proposed algorithm could reduce the total energy consumption, carbon emission, and the total costs of multiple cloud data centers by an average of 37.6%, 41%, and 25%, respectively, in a federated environment. © 2022 Elsevier Inc.","Ant colony system (ACS); Cloud computing; Cloud federation; Energy consumption; Virtual machine placement (VMP)"
"Energy allocation and task scheduling in edge devices based on forecast solar energy with meteorological information","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151301636&doi=10.1016%2fj.jpdc.2023.03.005&partnerID=40&md5=8899654eb6572d9ab647bdcaece9fe68","Offloading tasks from edge devices to the cloud is an important method to enhance the performance of the edge device. With the help of EH (Energy Harvesting) technology, the edge device can use the collected green energy to support its operations. Most offloading scheduling methods use as much green energy as the edge device collected. Unlike prior research, we consider the long-term benefits of energy. In this paper, we forecast the solar energy supply with meteorological methods which are based on the weather forecast data. Then, we use quadratic programming to allocate energy based on the forecast energy to maximize energy efficiency. Finally, we use NSGA (Non-dominated Sorting Genetic Algorithms) to offload tasks in the edge device. Simulations show that our proposed method not only minimizes the execution time and the energy consumption of clouds, but also enhances the QoE of users. © 2023 Elsevier Inc.","Edge device; NGSA; Offloading task; Weather forecast"
"On the impact of mode transition on phased transactional memory performance","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145558234&doi=10.1016%2fj.jpdc.2022.11.009&partnerID=40&md5=da387efdbfdf55edeca4588e68e96879","Several transactional memory implementations that employ state-of-the-art software and hardware techniques to deliver performance have been investigated in the last decade. Phased-based Transactional Memory (PhTM) systems run transactions in phases, such that all transactions in a phase execute in the same (hardware/software) mode. In PhTM, a runtime monitors the execution and decides when to change all transactions to another execution mode. Identifying the right moment to perform a mode transition is a central problem to achieve performance in PhTM systems. This article analyzes PhTM and provides a characterization of mode transitions and their impact on performance. We consider three PhTM implementations: (i) PhTM*, the first phased-based TM designed; (ii) Commit Throughput Measurement (CTM), a general-purpose runtime; and (iii) GoTM, a Graph-oriented runtime. We conduct a performance analysis to identify the drawbacks and benefits of each PhTM implementation with respect to their associated parameters. Results with speedups of up to 10× over the sequential baseline for CTM show that this mechanism generally shows better performance for a diverse set of applications. © 2022 Elsevier Inc.","Hardware transactional memory; Software transactional memory"
"Improving concurrency and memory usage in distributed operating systems for lightweight manycores via cooperative time-sharing lightweight tasks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144020121&doi=10.1016%2fj.jpdc.2022.12.006&partnerID=40&md5=e140e293732cbb5523df54be1d8d5b1d","Lightweight manycore processors arise to reconcile performance, energy efficiency, and scalability requirements on a single chip. Operating Systems (OSes) for these processors feature a distributed design, where isolated OS instances cooperate to mitigate programmability and portability issues coming from their architectural intricacies. Currently, OS services often resort to traditional execution flow abstractions (processes or threads) to implement small, periodic, or asynchronous functionalities. Although these abstractions considerably simplify the system design, they have a non-negotiable impact on the limited on-chip memories. Due to the memory restrictions, we argue that OS-level abstractions can be reshaped to reduce the OS memory footprint without introducing considerable overhead. In this context, we propose a complementary OS-level execution engine that supports cooperative time-sharing lightweight tasks that share a unique execution stack and features task synchronization via control flow and dependency graphs. This solution is orthogonal to the underlying execution support and provides numerous OS-level execution flows with reduced memory consumption. We implemented our engine in a distributed OS and executed experiments on a lightweight manycore. Our results show that it has the following advantages when compared to the classical thread abstraction: (i) it provides 63.2× more execution flows per MB of memory; (ii) it features less overhead to manage execution flows and system calls; (iii) it improves core utilization; and (iv) it exhibits competitive results on real-world applications. © 2022 Elsevier Inc.","Asymmetric microkernel; Distributed operating systems; Lightweight manycores; Memory constraints"
"SECDA-TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143630317&doi=10.1016%2fj.jpdc.2022.11.005&partnerID=40&md5=a035f68ce5976eead29ebba802ce5cf2","In this paper we propose SECDA-TFLite, a new open source toolkit for developing DNN hardware accelerators integrated within the TFLite framework. The toolkit leverages the principles of SECDA, a hardware/software co-design methodology, to reduce the design time of optimized DNN inference accelerators on edge devices with FPGAs. With SECDA-TFLite, we reduce the initial setup costs associated with integrating a new accelerator design within a target DNN framework, allowing developers to focus on the design. SECDA-TFLite also includes modules for cost-effective SystemC simulation, profiling, and AXI-based data communication. As a case study, we use SECDA-TFLite to develop and evaluate three accelerator designs across seven common CNN models and two BERT-based models against an ARM A9 CPU-only baseline, achieving an average performance speedup across models of up to 3.4× for the CNN models and of up to 2.5× for the BERT-based models. Our code is available at https://github.com/gicLAB/SECDA-TFLite. © 2022 The Authors","Design methodology; DNN accelerator design; Hardware-software co-design; HLS; Simulation; SystemC"
"Distributed approximate minimal Steiner trees with millions of seed vertices on billion-edge graphs","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164241756&doi=10.1016%2fj.jpdc.2023.104717&partnerID=40&md5=104c690d618a64c05f45a73b484fa25f","In this paper, we present a parallel 2-approximation Steiner minimal tree algorithm and its MPI-based distributed implementation. In place of expensive distance computations between all pairs of seed vertices, the solution we employ exploits a cheaper Voronoi cell computation. Our design leverages asynchronous processing and message prioritization to accelerate convergence of distance computations, and harnesses vertex and edge centric processing to offer fast time-to-solution. We demonstrate scalability and performance using real-world graphs with up to 128 billion edges and 512 compute nodes, and show the ability to find Steiner trees with up to one million seed vertices. Using 12 data instances, we present comparison with the state-of-the-art exact solver, SCIP-Jack, and two sequential 2-approximate algorithms. We empirically show that, on average, the total distance of the Steiner tree identified by our solution is 1.1290 times greater than the Steiner minimal tree – well within the theoretical approximation bound of 2. © 2023 Elsevier Inc.","Asynchronous processing; Distributed computing; Graph algorithm; hpc; Steiner tree"
"Effective switchless inter-FPGA memory networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160320978&doi=10.1016%2fj.jpdc.2023.05.002&partnerID=40&md5=4d445ad7946b2055416b16848a888bfd","FPGA network bandwidth increases to 400 Gbps or higher by high-density optical integration, e.g., Co-Packaged Optics (CPO) and onboard Si-photonics transceivers. This study presents its lightweight switchless network architecture by exploiting an indirect path, consisting of up to k one-hop paths for a diameter-k network topology. It integrates interconnection network and memory channels into a lightweight memory-to-memory inter-FPGA network. It has a uni-directional network topology for connecting the largest number of FPGAs. A newly introduced indirect flow control enables lossless uni-directional networks. We present multi-path point-to-point communications, multi-port message-combine collective communications, and efficient WDM (Wavelength Division Multiplexing) cabling method for the uni-directional networks. We implement and evaluate the uni-directional switchless network on an FPGA cluster consisting of six custom Stratix10 MX2100 FPGA cards connected with a 16-wavelength Arrayed Waveguide Grating (AWG) router. Simulation results show that the multi-port message-combine collective communication achieves 7× faster than conventional communication with 272 FPGAs. Effective WDM cabling using 16 wavelengths decreases the number of cables by 88% compared with parallel optical cabling for 272 FPGAs. © 2023 Elsevier Inc.","Collective communication; Field-programmable gate array (FPGA); Interconnection network; Uni-directional network topology; Wavelength division multiplexing (WDM)"
"ParaLiNGAM: Parallel causal structure learning for linear non-Gaussian acyclic models","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150370050&doi=10.1016%2fj.jpdc.2023.01.007&partnerID=40&md5=48b848bfcc4cf08dbbbadc72c61e9b9f","One of the key objectives in many fields in machine learning is to discover causal relationships among a set of variables from observational data. In linear non-Gaussian acyclic models (LiNGAM), it can be shown that the true underlying causal structure can be identified uniquely from merely observational data. The DirectLiNGAM algorithm is a well-known solution to learn the true causal structure in a high dimensional setting. DirectLiNGAM algorithm executes in a sequence of iterations and it performs a set of comparisons between pairs of variables in each iteration. Unfortunately, the runtime of this algorithm grows significantly as the number of variables increases. In this paper, we propose a parallel algorithm, called ParaLiNGAM, to learn casual structures based on DirectLiNGAM algorithm. We propose a threshold mechanism that can reduce the number of comparisons remarkably compared with the sequential solution. Moreover, in order to further reduce runtime, we employ a messaging mechanism between workers. We also present an implementation of ParaLiNGAM on GPU, considering hardware constraints. Experimental results on synthetic and real data show that our proposed solution outperforms DirectLiNGAM by a factor up to 4788X, and by a median of 2344X. © 2023 Elsevier Inc.","Causal discovery; DirectLiNGAM algorithm; GPU acceleration; Machine learning; Parallel processing"
"PHY: A performance-driven hybrid communication compression method for distributed training","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104719","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161701418&doi=10.1016%2fj.jpdc.2023.104719&partnerID=40&md5=6842a0b153811dbb69d48122099a9653","Distributed training is needed to shorten the training time of deep neural networks. However, the communication overhead often hurts performance efficiency, especially in a distributed computing environment with limited network bandwidth. Hence, gradient compression techniques have been proposed to reduce communication time. But, compression also has the risk of causing lower model accuracy and longer training time due to compression loss and compression time. As a result, compression may not consistently achieve desired results, and there are limited discussions on when and which compression should be used. To address this problem, we propose a performance-driven hybrid compression solution. We make three main contributions. (1) We describe a hybrid compression strategy that chooses the compression method for individual model gradients. (2) We build an offline performance estimator and an online loss monitor to ensure the compression decision can minimize training time without sacrificing mode accuracy. (3) Our implementation can be imported to existing deep learning frameworks and applicable to a wide range of compression methods. Up to 3.6x training performance speedup was observed compared to other state-of-the-art methods. © 2023 Elsevier Inc.","Data compression; Deep learning; Distributed computing; Optimization; Performance"
"Soundness-preserving composition of synchronously and asynchronously interacting workflow net components","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159451477&doi=10.1016%2fj.jpdc.2023.04.005&partnerID=40&md5=5424fb6fc76bd44ccdf7dff1b385e3df","In this paper, we propose a compositional approach to constructing correct formal models of information systems from correct models of interacting components. Component behavior is represented using workflow nets — a class of Petri nets. Interactions among components are encoded in an additional interface net. The proposed approach is used to model and compose synchronously and asynchronously interacting workflow nets. Using Petri net morphisms and their properties, we prove that the composition of interacting workflow nets preserves the correctness of components and of an interface. © 2023 Elsevier Inc.","Composition; Interaction; Morphisms; Soundness; Workflow nets"
"Energy-aware mapping and scheduling strategies for real-time workflows under reliability constraints","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148540411&doi=10.1016%2fj.jpdc.2023.02.004&partnerID=40&md5=a2156ed8b78f173ef3348788bdcb3210","This paper focuses on energy minimization for the mapping and scheduling of real-time workflows under reliability constraints. Workflow instances are input periodically to the system. Each instance is composed of several tasks and must complete execution before the arrival of the next instance, and with a prescribed reliability threshold. While the shape of the dependence graph is identical for each instance, task execution times are stochastic and vary from one instance to the next. The reliability threshold is met by executing several replicas for each task. The target platform consists of identical processors equipped with Dynamic Voltage and Frequency Scaling (DVFS) capabilities. A different frequency can be assigned to each task replica to save energy, but it may have negative effect on the deadline and reliability target. This difficult tri-criteria mapping and scheduling problem (energy, deadline, reliability) has been studied only recently for workflows with arbitrary dependence constraints. We investigate new mapping and scheduling strategies based upon layers in the task graph. These strategies better balance replicas across processors, thereby decreasing the time overlap between different replicas of a given task, and saving energy. We compare these strategies with two state-of-the-art approaches and a reference baseline on a variety of benchmark workflows. Our best heuristics achieve an average energy gain of 60% over the competitors and of 82% over the baseline. © 2023 Elsevier Inc.","Energy-aware scheduling; Makespan; Real-time workflows; Reliability; Tri-criteria optimization"
"Mixed precision support in HPC applications: What about reliability?","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166320047&doi=10.1016%2fj.jpdc.2023.104746&partnerID=40&md5=e0978849baf39b7cc57c7c49f8b42405","In their quest for exascale and beyond, High-Performance Computing (HPC) systems continue becoming ever larger and more complex. Application developers, on the other hand, leverage novel methods to improve the efficiency of their own codes: a recent trend is the use of floating-point mixed precision, or the careful interlocking of single- and double-precision arithmetic, as a tool to improve performance as well as reduce network and memory boundedness. However, while it is known that modern HPC systems suffer hardware faults at daily rates, the impact of reduced precision on application reliability is yet to be explored. In this work we aim to fill this gap: first, we propose a qualitative survey to identify the branches of HPC where mixed precision is most popular. Second, we show the results of instruction-level fault injection experiments on a variety of representative HPC workloads, comparing vulnerability to Silent Data Errors (SDEs) under different numerical configurations. Our experiments indicate that use of single and mixed precision leads to comparatively more frequent and more severe SDEs, with concerning implications regarding their use on extreme-scale, fault-prone HPC platforms. © 2023 Elsevier Inc.","Fault injection; Floating-point unit; Hardware reliability; High-performance computing; Mixed precision"
"Reputation based novel trust management framework with enhanced availability for cloud","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152146198&doi=10.1016%2fj.jpdc.2023.03.010&partnerID=40&md5=0c6f1cd5744da53928a15d7060c332ce","To select a cloud service provider (CSP) out of immense number of relevant CSPs available, cloud customer (CC) trusts on the reviews provided by the CCs who have used the services provided by CSPs. However, relying on these feedbacks may be hazardous if the review(s) have been deliberately posted with wicked motive. To mitigate the impact of malicious feedbacks, a framework, namely highly available credible cloud trust management framework (HA2CTMF) has been proposed with the intent of revamping the reputation and availability parameters of QoS by evaluating the trust of cloud service providers (CSPs) based on the credibility of feedbacks furnished by CC. HA2CTMF detects collusion and Sybil attacks on the labeled CloudArmor dataset and increases availability of the trust service. Collusion attack has been detected by comparing review count; service count; & mean of reviews with their respective threshold values. Further, Sybil attack has been uncovered with the help of these aspects: deviation from mean of reviews; pattern detection; & comparison of polarity value from both lower & upper threshold values. Additionally, enhanced availability has been ensured by working on disaster management and diminishing repercussions of single point of failure. The results have been accessed on the following parameters: precision; recall; f1-score & availability percentage on the dataset comprising 8774 data points. This framework has exhibited precision of 85.69%, recall of 89.45%, f1-score of 87.53% and availability of 96.71% compared to CloudArmor framework values on the same parameters as: 44.47%; 62.21%; 51.85% & 70.43% respectively. As per the results attained, we conclude that proposed HA2CTMF has demonstrated better outcome. © 2023 Elsevier Inc.","Cloud customer reviews; Fake feedback detection; QoS performance; Reputation management; Trust management model"
"k-CSqu: Ensuring connected k-coverage using cusp squares of square tessellation","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104749","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168805286&doi=10.1016%2fj.jpdc.2023.104749&partnerID=40&md5=a3fcaae1e61a11693ac70dbd46f2a1fa","In planar wireless sensor networks (PWSNs), the most essential functionalities of the sensor nodes are both sensing and communication, which are evaluated using two fundamental concepts, namely coverage and connectivity, respectively. However, coverage alone is not sufficient for the correct operation of PWSNs. Additionally, it is important that network connectivity be ensured, where all the sensors are connected to one another, so that every pair of sensors can communicate with each other. To account for both coverage and connectivity, this paper aims at solving the problem of connected k-coverage in PWSNs, where every point in a field of interest is covered by at least k sensors (k>1) at the same time, while all the sensors are mutually connected directly or indirectly. In order to solve this problem, we initially tessellate the entire field into adjacent and non-intersecting square tiles. Then, we construct a cusp-square inside each square tile of the tessellation for sensor placement. Based on this cusp-squared square tile, we compute the minimum planar sensor density for k-coverage in PWSNs. Also, we establish a relationship between the sensing and communication radii of the sensors to guarantee network connectivity in PWSNs. Finally, we validate our theoretical analysis using simulation results. © 2023 Elsevier Inc.","Connected k-coverage; Planar wireless sensor networks; Sensor density; Square tessellation"
"A low-power WNoC transceiver with a novel energy consumption management scheme for dependable IoT systems","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141546035&doi=10.1016%2fj.jpdc.2022.10.010&partnerID=40&md5=d72e190efc09e8a0aef6ca886b34c328","Wireless network-on-chip architectures (WNoCs), by combining wired and wireless modules and links, provide fast and efficient communication infrastructures for complex on-chip systems such as various IoT and intelligent systems. The most challenging step of designing high-performance and energy-efficient WNoCs is the structure of transceivers. In fact, transceivers and wired routers are the most power-hungry elements in WNoCs; therefore, any cut-down of power consumption in transceivers and routers significantly reduce the power consumption of WNoCs. In this paper, a low-power transceiver for WNoC is designed and then, a high-performance and configurable energy consumption management scheme for the transceiver is proposed. In the designed transceiver, various strategies are employed at both circuit-level and also at architectural-level for reducing static and dynamic power consumption. The proposed energy management scheme controls the activity and tunes the voltage level of the power supply of each module, in a distributed and dynamic manner. The evaluation results demonstrate that a WNoC utilizing the proposed scheme provides lower energy consumption, less latency, and higher reliability and higher network throughput compared to both of relevant single-chip and multi-chip WNoC architectures, at the cost of negligible impact on silicon area. © 2022 Elsevier Inc.","Energy management; Power-gating; Transceiver; Wireless network-on-chip"
"Efficient deterministic MapReduce algorithms for parallelizable problems","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149833790&doi=10.1016%2fj.jpdc.2023.02.010&partnerID=40&md5=dd696dab672cad3ce731711452556ce2","The MapReduce framework has firmly established itself as one of the most widely used parallel computing platforms for processing big data on tera- and peta-byte scale. Approaching it from a theoretical standpoint has proved to be notoriously difficult, however. In continuation of Goodrich et al.'s early efforts, explicitly espousing the goal of putting the MapReduce framework on footing equal to that of long-established models such as the PRAM, we investigate the obvious complexity question of how the computational power of MapReduce algorithms compares to that of combinational Boolean circuits commonly used for parallel computations. Relying on the standard MapReduce model introduced by Karloff et al. a decade ago, we develop an intricate simulation technique to show that any problem in NC (i.e., a problem solved by a logspace-uniform family of Boolean circuits of polynomial size and a depth polylogarithmic in the input size) can be solved by a MapReduce computation in O(T(n)/log⁡n) rounds, where n is the input size and T(n) is the depth of the witnessing circuit family. Thus, we are able to closely relate the standard, uniform NC hierarchy modeling parallel computations to the deterministic MapReduce hierarchy DMRC by proving that NCi+1⊆DMRCi for all i∈N. Besides the theoretical significance, this result has important applied aspects as well. In particular, we show for all problems in NC1—many practically relevant ones, such as integer multiplication and division, the parity function, and recognizing balanced strings of parentheses being among these—how to solve them in a constant number of deterministic MapReduce rounds. © 2023 The Author(s)","Circuit complexity; Complexity theory; MapReduce; Nick's class; Parallel algorithms"
"Affinity-aware resource provisioning for long-running applications in shared clusters","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150055386&doi=10.1016%2fj.jpdc.2023.02.011&partnerID=40&md5=b0780e449db68cf156e826acb0e93789","Resource provisioning plays a pivotal role in determining the right amount of infrastructure resource to run applications and reduce the monetary cost. A significant portion of production clusters is now dedicated to long-running applications (LRAs), which are typically in the form of microservices and executed in the order of hours or even months. It is therefore practically important to plan ahead the placement of LRAs in a shared cluster for the minimized number of compute nodes required by them. Existing works on LRA scheduling are often application-agnostic, without particularly addressing the constraining requirements imposed by LRAs, such as co-location affinity constraints and time-varying resource requirements. In this paper, we present an affinity-aware resource provisioning approach for deploying large-scale LRAs in a shared cluster subject to multiple constraints, with the objective of minimizing the number of compute nodes in use. We investigate a broad range of solution algorithms which fall into three main categories: Application-Centric, Node-Centric, and Multi-Node approaches, and tune them for typical large-scale real-world scenarios. Experimental studies driven by the Alibaba Tianchi dataset show that our algorithms can achieve competitive scheduling effectiveness and running time, as compared with the heuristics used by the latest work including Medea and LRASched. Best results are obtained by the Application-Centric algorithms, if the algorithm's running time is of primary concern, and by Multi-Node algorithms, if the solution quality is of primary concern. © 2023 The Author(s)","Long-running applications; Resource scheduling; Vector bin packing"
"Tile low-rank approximations of non-Gaussian space and space-time Tukey g-and-h random field likelihoods and predictions on large-scale systems","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104715","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160736553&doi=10.1016%2fj.jpdc.2023.104715&partnerID=40&md5=56d91319513786f4f11ec111d6df96de","Large-scale statistical modeling has become necessary with the vast flood of geospace data coming from various sources. In space statistics, the Maximum Likelihood Estimation (MLE) is widely considered for modeling geospace data by estimating a set of statistical parameters related to a predefined covariance function. This covariance function describes the correlation between a set of geospace locations where the main goal is to model given data samples and impute missing data. Climate/weather modeling is a prevalent application for the MLE operation where data interpolation and forecasting are highly required. In the literature, the Gaussian random field is often used to describe geospace data as one of the most popular models for MLE. However, real-life datasets are often skewed and/or have extreme values, and non-Gaussian random field models are more appropriate for capturing such features. In this work, we provide an exact and approximate parallel implementation of the well-known Tukey g-and-h (TGH) non-Gaussian random field in the context of climate/weather applications. The proposed implementation alleviates the computation complexity of the log-likelihood function, which requires O(n2) storage and O(n3) operations, where N is the number of geospace locations, M is the number of time slots, and n=N×M. Based on tile low-rank (TLR) approximations, our implementation of the TGH model can tackle large-scale problems. Furthermore, we rely on task-based programming models and dynamic runtime systems to provide fast execution for the MLE operation in space and space-time cases. We assess the performance and accuracy of the proposed implementations using synthetic space and space-time datasets up to 800K. We also consider a 12-month precipitation dataset in Germany to demonstrate the advantage of using non-Gaussian over Gaussian random field models. We evaluate the prediction accuracy of the TGH model on the precipitation dataset using the Probability Integral Transformation (PIT) tool showing that the TGH model outperforms the Gaussian modeling in the real dataset. Moreover, our performance assessment indicates that TLR computations allow solving larger matrix sizes while preserving the required accuracy for prediction. The TLR-based approximation shows a speedup up to 7.29X and 2.96X over the exact solution. © 2023 Elsevier Inc.","Climate/weather data; Large-scale geostatistical modeling; Likelihood approximation; Non-Gaussian random field; Tile Low-Rank (TLR) approximation"
"Novel schemes for embedding Hamiltonian paths and cycles in balanced hypercubes with exponential faulty edges","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151493300&doi=10.1016%2fj.jpdc.2023.03.007&partnerID=40&md5=20c10dd86a8d1e2cb1aa31940d4123af","The balanced hypercube BHn plays an essential role in large-scale parallel and distributed computing systems. With the increasing probability of edge faults in large-scale networks and the widespread applications of Hamiltonian paths and cycles, it is especially essential to study the fault tolerance of networks in the presence of Hamiltonian paths and cycles. However, existing researches on edge faults ignore that it is almost impossible for all faulty edges to be concentrated in a certain dimension. Thus, the fault tolerance performance of interconnection networks is severely underestimated. This paper focuses on three measures, t-partition-edge fault-tolerant Hamiltonian, t-partition-edge fault-tolerant Hamiltonian laceable, and t-partition-edge fault-tolerant strongly Hamiltonian laceable, and utilizes these measures to explore the existence of Hamiltonian paths and cycles in balanced hypercubes with exponentially faulty edges. We show that the BHn is 2n−1-partition-edge fault-tolerant Hamiltonian laceable, 2n−1-partition-edge fault-tolerant Hamiltonian, and (2n−1−1)-partition-edge fault-tolerant strongly Hamiltonian laceable for n≥2. Comparison results show the partitioned fault model can provide the exponential fault tolerance as the value of the dimension n grows. © 2023 Elsevier Inc.","Balanced hypercubes; Exponential faults; Fault tolerance; Hamiltonian laceable; Interconnection networks"
"Exploring edge TPU for network intrusion detection in IoT","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159775754&doi=10.1016%2fj.jpdc.2023.05.001&partnerID=40&md5=55b30250514ca2d4be8580b575c2fab0","This paper explores Google's Edge TPU for implementing a practical network intrusion detection system (NIDS) at the edge of IoT, based on a deep learning approach. While a significant number of related works explore machine learning-based NIDS for the IoT edge, they generally lack considering the issue of the required computational and energy resources. The focus of this paper is the exploration of deep learning-based NIDS at the edge of IoT, and in particular, the computational and energy efficiency. In particular, the paper studies Google's Edge TPU as a hardware platform and considers the following three key metrics: computation (inference) time, energy efficiency and traffic classification performance. Various scaled model sizes of two major deep neural network architectures are used to investigate these three metrics. The performance of the Edge TPU-based implementation is compared with that of an energy-efficient embedded CPU (quad-core ARM Cortex-A53). © 2023 Elsevier Inc.","Deep learning; Edge machine learning; Google edge TPU; IoT security; Network intrusion detection systems"
"EdgeDecAp: An auction-based decentralized algorithm for optimizing application placement in edge computing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146426843&doi=10.1016%2fj.jpdc.2023.01.002&partnerID=40&md5=6ecca392481df5b56aab9643344a2e00","In edge computing, application components can be placed over a range of computational devices from cloud data centers to nodes at the network edge. Application placement can have significant impact on important metrics like latency and resource utilization. Thus, application placement is an important optimization problem. In edge computing, the characteristics of both the infrastructure and the application may change over time, which may require the dynamic re-optimization of the application placement. Most algorithms suggested so far for the dynamic re-optimization of edge application placement are centralized, i.e., they rely on one entity collecting information from the whole infrastructure and making decisions centrally. However, centralized approaches suffer from limited scalability and are vulnerable to failures. In this paper, we present a decentralized approach for the dynamic re-optimization of edge application placement. We adopt an algorithm of Malek et al. for distributed systems and modify it to make it applicable to edge computing. In this approach, each node makes decisions autonomously, using auctions for coordination. Our empirical results demonstrate that the proposed algorithm is very effective in optimizing edge application placement. In an edge system with 637 edge nodes and 563 end devices, our algorithm achieves 54% higher reduction of application latency than a previous decentralized algorithm. © 2023 The Author(s)","Application placement; Auction; Decentralized algorithm; Edge computing; Fog computing"
"A parallel algorithm for constructing multiple independent spanning trees in bubble-sort networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104731","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164267112&doi=10.1016%2fj.jpdc.2023.104731&partnerID=40&md5=f0c08bc768be3f7432b7e060528b18cd","The use of multiple independent spanning trees (ISTs) for data broadcasting in networks provides a number of advantages, including the increase of fault-tolerance and secure message distribution. Thus, the designs of multiple ISTs on several classes of networks have been widely investigated. Kao et al. (2019) [18] proposed an algorithm to construct independent spanning trees in bubble-sort networks. The algorithm is executed in a recursive function and thus is hard to parallelize. In this paper, we focus on the problem of constructing ISTs in bubble-sort networks Bn and present a non-recursive algorithm. Our approach can be fully parallelized, i.e., every vertex can determine its parent in each spanning tree in constant time. This solves the open problem from the paper by Kao et al. Furthermore, we show that the total time complexity O(n⋅n!) of our algorithm is asymptotically optimal, where n is the dimension of Bn and n! is the number of vertices of the network. © 2023 Elsevier Inc.","Bubble-sort networks; Independent spanning trees; Interconnection networks"
"Policy enforcement in traditional non-SDN networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150346438&doi=10.1016%2fj.jpdc.2023.02.005&partnerID=40&md5=9adea3df3090eb0a9558145996e8395c","Middleboxes are widely used in modern networks for a variety of network functions in cybersecurity, performance enhancement, and monitoring. Middlebox policy enforcement is however complex and tedious with unreliable manual re-configuration of legacy routers. The existing solution on automated policy enforcement relies on software-defined networking and does not apply to the traditional non-SDN networks, which remain popular today in enterprise deployment and core networks. This paper proposes a new architecture based entirely on software-defined middleboxes (instead of using software-defined switches in the prior art) to enable dependable and automated policy enforcement in non-SDN networks whose routers forward packets based on traditional routing protocols that are not policy-sensitive. We present a hot-potato enforcement strategy, which is then enhanced with two optimizations for load-balanced policy enforcement among software-defined middleboxes. Next, we propose two additional optimizations that minimize total traffic and aggregate end-to-end delays subject to link capacity constraints. Further enhancements are made to relieve middlebox processing overhead, avoid packet fragmentation due to policy enforcement, recover from failures, and mitigate delay for time-sensitive applications. We evaluate the proposed architecture on a real-life campus network topology and two simulated topologies to demonstrate the superior performance of our load-balanced enforcement strategies. © 2023 Elsevier Inc.","Computer network; Middleboxes; Network optimization; Network security; Policy enforcement"
"Optimizing performance and energy across problem sizes through a search space exploration and machine learning","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104720","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163492415&doi=10.1016%2fj.jpdc.2023.104720&partnerID=40&md5=62d6007da47e4bb3883e960a7cdcc43d","HPC systems expose configuration options to assist optimization. Configurations such as parallelism, thread and data mapping, or prefetching have been explored but with a limited optimization objective (e.g., performance) and a fixed problem size. Unfortunately, efficient strategies in one scenario may poorly generalize when applied in new contexts. We investigate the impact of configuration options and different problem sizes over performance and energy. Well-adapted NUMA-related options and cache-prefetchers provide significantly more gains for energy (5.9×) than performance (1.85×). Moreover, reusing optimization strategies from performance to energy only provides 40% of the gains found when natively optimizing for energy, while transferring strategies across problem sizes limits to 70% of the original gains. We fill this gap with ML: simple decision trees predict the best configuration for a target size using only information collected on another size. Our models achieve 88% of the native gains when cross-predicting across performance and energy, and 85% across problem sizes. © 2023 Elsevier Inc.","Energy; Machine learning; NUMA; Page and thread mapping; Prefetch"
"Atomic Appends in Asynchronous Byzantine Distributed Ledgers","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168315389&doi=10.1016%2fj.jpdc.2023.104748&partnerID=40&md5=d0b744767f13b70ef7ef9f480bb9a015","A Distributed Ledger Object (DLO) is a concurrent object that maintains a totally ordered sequence of records. In this work we formalize a linearizable Byzantine-tolerant Distributed Ledger Object (BDLO), which is a linearizable DLO where clients and servers processes may deviate arbitrarily from their intended behavior (i.e. they may be Byzantine). The proposed formal definition is accompanied by algorithms that implement BDLOs on top of an underlying Byzantine Atomic Broadcast service. Then we develop a suite of algorithms, based on the previous BDLO implementations, that solve the Atomic Appends problem in the presence of asynchrony, Byzantine clients and Byzantine servers. This problem occurs when clients have a composite record (set of basic records) to append to different BDLOs, in such a way that either each basic record is appended to its BDLO (and this must occur in good circumstances), or no basic record is appended. Distributed algorithms are presented, which solve the Atomic Appends problem when the clients (involved in the Atomic Appends) and the servers (which maintain the BDLOs) may be Byzantine. Finally we provide proof of concept implementations and an experimental evaluation of the presented algorithms. © 2023 Elsevier Inc.","Asynchrony; Atomic Appends; Blockchain; Byzantine Fault Tolerance; Distributed Ledger Object"
"Extreme flow decomposition for multi-source multicast with intra-session network coding","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146705542&doi=10.1016%2fj.jpdc.2023.01.003&partnerID=40&md5=0510327991385cd9a9317233ac69a9dd","Network coding (NC), when combined with multipath routing, enables a linear programming (LP) formulation for a multi-source multicast with intra-session network coding (MISNC) problem. However, it is still hard to solve using conventional methods due to the enormous scale of variables or constraints. In this paper, we try to solve this problem in terms of throughput maximization from an algorithmic perspective. We propose a novel formulation based on the extreme flow decomposition technique, which facilitates the design and analysis of approximation and online algorithms. For the offline scenario, we develop a fully polynomial time approximation scheme (FPTAS) which can find a (1+ω)-approximation solution for any specified ω>0. For the online scenario, we develop an online primal-dual algorithm which proves O(1)-competitive and violates link capacities by a factor of O(log⁡m), where m is the link number. The proposed algorithms share an elegant primal-dual form and thereby have inherent advantages of simplicity, efficiency, and scalability. To better understand the proposed approach, we devise delicate numerical examples on an extended butterfly network to validate the effects of algorithmic parameters and make an interesting comparison between the offline and online cases. We also perform large-scale simulations on real networks to validate the effectiveness of the proposed FPTAS and online algorithm. © 2023 Elsevier Inc.","Approximation algorithm; Extreme flow; Multicast; Network coding; Online algorithm"
"Supporting efficient overlapping of host-device operations for heterogeneous programming with CtrlEvents","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159185299&doi=10.1016%2fj.jpdc.2023.04.009&partnerID=40&md5=19e44aaf57bbb8f8e26edecf8a2a12f5","Heterogeneous systems with several kinds of devices, such as multi-core CPUs, GPUs, FPGAs, among others, are now commonplace. Exploiting all these devices with device-oriented programming models, such as CUDA or OpenCL, requires expertise and knowledge about the underlying hardware to tailor the application to each specific device, thus degrading performance portability. Higher-level proposals simplify the programming of these devices, but their current implementations do not have an efficient support to solve problems that include frequent bursts of computation and communication, or input/output operations. In this work we present CtrlEvents, a new heterogeneous runtime solution which automatically overlaps computation and communication whenever possible, simplifying and improving the efficiency of data-dependency analysis and the coordination of both device computations and host tasks that include generic I/O operations. Our solution outperforms other state-of-the-art implementations for most situations, presenting a good balance between portability, programmability and efficiency. © 2023 The Author(s)","Asynchronous operations; Events; GPUs; Heterogeneous programming; Parallel programming"
"Intelligent energy-efficient scheduling with ant colony techniques for heterogeneous edge computing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145350479&doi=10.1016%2fj.jpdc.2022.10.003&partnerID=40&md5=e834dec61835495685d2bfc238222baa","Energy efficiency is a significant issue in heterogeneous edge computing systems for a large number of latency-sensitive applications. This article presents an efficient technique to minimize energy overhead of time-constrained applications modeled by DAGs in heterogeneous edge computing. The technique is divided into three stages. First, we design a new method to compute task priority and propose the ant-colony based energy-aware scheduling algorithm to get a preliminary scheduling result. Second, taking the slack time between tasks and their deadlines into consideration, we propose the downward proportionally reclaiming slack algorithm to further cut down energy overhead by the DVFS technique. Third, taking the slack time between tasks into consideration, we propose the upward and downward proportionally reclaiming slack algorithm to cut down energy overhead by the DVFS technique again. Simulated results indicate that the presented technique is highly efficient in reducing energy overhead compared with state-of-the-art techniques using benchmarks of distinct characteristics. © 2022","Ant-colony; Deadline; DVFS; Energy-efficient; Heterogeneous edge computing"
"Parallel computing in finance for estimating risk-neutral densities through option prices","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142901989&doi=10.1016%2fj.jpdc.2022.11.010&partnerID=40&md5=3d3372c37b9a0f4ca8707953526e4baa","Option pricing is one of the most active Financial Economics research fields. Black-Scholes-Merton option pricing theory states that risk-neutral density is lognormal. However, markets' pieces of evidence do not support that assumption. More realistic assumptions impose substantial computational burdens to calculate option pricing functions. Risk-neutral density is a pivotal element to price derivative assets, which can be estimated through nonparametric kernel methods. A significant computational challenge exists for determining optimal kernel bandwidths, addressed in this study through a parallel computing algorithm performed using Graphical Processing Units. The paper proposes a tailor-made Cross-Validation criterion function used to define optimal bandwidths. The selection of optimal bandwidths is crucial for nonparametric estimation and is also the most computationally intensive. We tested the developed algorithms through two data sets related to intraday data for VIX and S&P500 indexes. © 2022 The Author(s)","Finance; GPU computing; Nonparametric methods; Options pricing; Parallel computing"
"Scheduling coflows for minimizing the total weighted completion time in heterogeneous parallel networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799888&doi=10.1016%2fj.jpdc.2023.104752&partnerID=40&md5=16e03a129c9d214dca369af9d763ec6e","Coflow is a network abstraction used to represent communication patterns in data centers. The coflow scheduling problem encountered in large data centers is a challenging NP-hard problem. Many previous studies on coflow scheduling mainly focus on the single-core model. However, with the growth of data centers, this single-core model is no longer sufficient. This paper addresses the coflow scheduling problem within heterogeneous parallel networks, which feature an architecture consisting of multiple network cores running in parallel. In this paper, two polynomial-time approximation algorithms are developed for the flow-level scheduling problem and the coflow-level scheduling problem in heterogeneous parallel networks, respectively. For the flow-level scheduling problem, the proposed algorithm achieves an approximation ratio of O(log⁡m/log⁡log⁡m) when all coflows are released at arbitrary times, where m represents the number of network cores. On the other hand, in the coflow-level scheduling problem, the proposed algorithm achieves an approximation ratio of O(m(log⁡m/log⁡log⁡m)2) when all coflows are released at arbitrary times. Moreover, we propose a heuristic algorithm for the flow-level scheduling problem. Simulation results using synthetic traffic traces validate the performance of our algorithms and show improvements over the previous algorithm. © 2023 The Author(s)","Approximation algorithms; Coflow; Datacenter network; Heterogeneous parallel network; Scheduling algorithms"
"Improving the performance of classical linear algebra iterative methods via hybrid parallelism","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159765728&doi=10.1016%2fj.jpdc.2023.04.012&partnerID=40&md5=bc9bd98aa399d11e9b766036a6268557","We propose fork-join and task-based hybrid implementations of four classical linear algebra iterative methods (Jacobi, Gauss–Seidel, conjugate gradient and biconjugate gradient stabilized) on CPUs as well as variations of them. This class of algorithms, that are ubiquitous in computational frameworks, are duly documented and the corresponding source code is made publicly available for reproducibility. Both weak and strong scalability benchmarks are conducted to statistically analyse their relative efficiencies. The weak scalability results assert the superiority of a task-based hybrid parallelisation over MPI-only and fork-join hybrid implementations. Indeed, the task-based model is able to achieve speedups of up to 25% larger than its MPI-only counterpart depending on the numerical method and the computational resources used. For strong scalability scenarios, hybrid methods based on tasks remain more efficient with moderate computational resources where data locality does not play an important role. Fork-join hybridisation often yields mixed results and hence does not seem to bring a competitive advantage over a much simpler MPI approach. © 2023 Elsevier Inc.","Distributed-memory; Hybrid parallelism; Linear algebra; MPI; Shared-memory"
"Programming parallel dense matrix factorizations and inversion for new-generation NUMA architectures","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147326107&doi=10.1016%2fj.jpdc.2023.01.004&partnerID=40&md5=a297d890a46f3e1e21af5d0c2b03e620","We propose a methodology to address the programmability issues derived from the emergence of new-generation shared-memory NUMA architectures. For this purpose, we employ dense matrix factorizations and matrix inversion (DMFI) as a use case, and we target two modern architectures (AMD Rome and Huawei Kunpeng 920) that exhibit configurable NUMA topologies. Our methodology pursues performance portability across different NUMA configurations by proposing multi-domain implementations for DMFI plus a hybrid task- and loop-level parallelization that configures multi-threaded executions to fix core-to-data binding, exploiting locality at the expense of minor code modifications. In addition, we introduce a generalization of the multi-domain implementations for DMFI that offers support for virtually any NUMA topology in present and future architectures. Our experimentation on the two target architectures for three representative dense linear algebra operations validates the proposal, reveals insights on the necessity of adapting both the codes and their execution to improve data access locality, and reports performance across architectures and inter- and intra-socket NUMA configurations competitive with state-of-the-art message-passing implementations, maintaining the ease of development usually associated with shared-memory programming. © 2023 The Author(s)","Chiplets; Dense linear algebra; NUMA architectures; Portability; Shared memory programming"
"HyperTP: A unified approach for live hypervisor replacement in datacenters","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163796636&doi=10.1016%2fj.jpdc.2023.104733&partnerID=40&md5=edd2bffa904803f41b582eaaedecd671","Maintenance of virtualized datacenters is often needed for the purposes of introducing new features, fixing bugs or mitigating security problems. However, current maintenance methods are either highly disruptive to the operation of VMs, utilize large amounts of computing resources or require high development efforts. We build HyperTP, a generic framework which combines in a unified way two approaches: in-place server micro-reboot-based hypervisor transplant (noted InPlaceTP) and live VM migration-based hypervisor transplant (noted MigrationTP). HyperTP hinges on a VM state hierarchy for organizing different types of hypervisor memory states in terms of their relation to VM execution, and an Unified Intermediate State Representation that abstracts VM-relevant memory states between multiple different hypervisors. We describe our implementations of both approaches, including technical details of our UISR design and the transplant process. Our evaluation results show that HyperTP delivers satisfactory performance: (1) MigrationTP changes a VM's underlying hypervisor while taking the same time and impacting virtual machines (VMs) with the same performance degradation as normal live migration; and (2) InPlaceTP imposes minimal VM downtime, even under increasing number of VMs and memory sizes. Finally, we discuss how the combination of InPlaceTP and MigrationTP can be used to address the challenges of upgrading a hypervisor cluster, and to mitigate known unpatched hypervisor vulnerabilities. © 2023 Elsevier Inc.","Hypervisor compatibility; Hypervisor transplant; Live patching; Live update; Virtualization"
"Uncovering I/O demands on HPC platforms: Peeking under the hood of Santos Dumont","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168099311&doi=10.1016%2fj.jpdc.2023.104744&partnerID=40&md5=95467e7cccaec292a541b483a5a5e3d4","High-Performance Computing (HPC) platforms are required to solve the most diverse large-scale scientific problems in various research areas, such as biology, chemistry, physics, and health sciences. Researchers use a multitude of scientific softwares, which have different requirements. These include input and output operations, which directly impact performance due to the existing difference in processing and data access speeds. Thus, supercomputers must efficiently handle mixed workload when storing data from the applications. Understanding the set of applications and their performance running in a supercomputer is paramount to understanding the storage system's usage, pinpointing possible bottlenecks, and guiding optimization techniques. This research proposes a methodology and visualization tool to evaluate a supercomputer's data storage infrastructure's performance, taking into account the diverse workload and demands of the system over a long period of operation. As a study case, we focus on the Santos Dumont supercomputer, identifying inefficient usage, problematic performance factors, and providing guidelines on how to tackle those issues. © 2023 The Author(s)","High-performance storage; I/O characterization; Lustre; Metadata; Parallel File System"
"Tensor slicing and optimization for multicore NPUs","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146693719&doi=10.1016%2fj.jpdc.2022.12.008&partnerID=40&md5=e7417f89969dc51abe0d44019ca4dbf1","Although code generation for Convolution Neural Network (CNN) models has been extensively studied, performing efficient data slicing and parallelization for highly-constrained Multicore Neural Processor Units (NPUs) is still a challenging problem. Given the size of convolutions' input/output tensors and the small footprint of NPU on-chip memories, minimizing memory transactions while maximizing parallelism and MAC utilization are central to any effective solution. This paper proposes a TensorFlow XLA/LLVM compiler optimization pass for Multicore NPUs, called Tensor Slicing Optimization (TSO), which: (a) maximizes convolution parallelism and memory usage across NPU cores; and (b) reduces data transfers between host and NPU on-chip memories by using DRAM memory burst time estimates to guide tensor slicing. To evaluate the proposed approach, a set of experiments was performed using the NeuroMorphic Processor (NMP), a multicore NPU containing 32 RISC-V cores extended with novel CNN instructions. Experimental results show that TSO is capable of identifying the best tensor slicing that minimizes execution time for a set of CNN models. Speed-ups of up to 21.7% result when comparing the TSO burst-based technique to a no-burst data slicing approach. To validate the generality of the TSO approach, the algorithm was also ported to the Glow Machine Learning framework. The performance of the models were measured on both Glow and TensorFlow XLA/LLVM compilers, revealing similar results. © 2022 Elsevier Inc.","Burst-based cost model; Convolutional neural network; Mapping strategies; NPU"
"A task processing efficiency improvement scheme based on Cloud-Edge architecture in computationally intensive scenarios","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166929056&doi=10.1016%2fj.jpdc.2023.104742&partnerID=40&md5=871b6a20db528d781eac76873dec87a9","As Cloud-Edge architecture combines low latency and high-performance computing, it has a wider range of application scenarios. Considering that, how to reasonably allocate tasks to the edge cloud or the center cloud and how to select appropriate virtual machines (VMs) for tasks are two key aspects affecting the efficiency of Cloud-Edge architecture. For the former, the ARDT algorithm is proposed, which dynamically adjusts the optimal threshold by predicting the number of real-time end devices. For the latter, the DBMR algorithm is designed based on the Max-Fit algorithm, in which a structure of the blocking table is put forward to reduce the time complexity of the original algorithm from O(n2) to O(n(n−m)). Experimental results show that the ARDT-DBMR algorithm optimizes the average service time and task failure rate by 30%-40% compared with the baseline algorithms and other advanced algorithms and significantly ameliorates the problem of abnormal fluctuations in VM utilization in computationally intensive scenarios. © 2023 Elsevier Inc.","Cloud-Edge architecture; Dynamic threshold; Task assignment; VM selection"
"Accelerating matrix-centric graph processing on GPUs through bit-level optimizations","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150017448&doi=10.1016%2fj.jpdc.2023.02.013&partnerID=40&md5=f56c5aaa064a100cf0202f94c5808c6c","Even though it is well known that binary values are common in graph applications (e.g., adjacency matrix), how to leverage the phenomenon for efficiency has not yet been adequately explored. This paper presents a systematic study on how to unlock the potential of the bit-level optimizations of graph computations that involve binary values. It proposes a two-level representation named Bit-Block Compressed Sparse Row (B2SR) and presents a series of optimizations to the graph operations on B2SR by the intrinsics of modern GPUs. It additionally introduces Deep Reinforcement Learning (DRL) as an efficient way to best configure the bit-level optimizations on the fly. The DQN-based adaptive tile size selector with dedicated model training can reach 68% prediction accuracy. Evaluations on the NVIDIA Pascal and Volta GPUs show that the optimizations bring up to 40[Formula presented] and 6555[Formula presented] for essential GraphBLAS kernels SpMV and SpGEMM, respectively, accelerating GraphBLAS-based BFS by up to 433[Formula presented], SSSP, PR, and CC 35[Formula presented], and TC 52[Formula presented]. © 2023 Elsevier Inc.","Bit manipulation; Deep reinforcement learning; GPU; GraphBLAS; Sparse matrix"
"JMDC: A joint model and data compression system for deep neural networks collaborative computing in edge-cloud networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142469930&doi=10.1016%2fj.jpdc.2022.11.008&partnerID=40&md5=20d2a14db396a653683f3faf5ecc6227","Deep Neural Networks (DNNs) have shown exceptional promise in providing Artificial Intelligence (AI) to many computer vision applications. Nevertheless, complex models, intensive computations, and resource constraints hinder the use of DNNs in edge computing scenarios. Existing studies have already focused on edge-cloud collaborative inference, where a DNN model is decoupled at an intermediate layer, and the two parts are sequentially executed at the edge device and the cloud server, respectively. In this work, we examine the status quo approaches of DNN execution, and find that it still takes a lot of time on edge device computation and edge-cloud data transmission. Using this insight, we propose a new edge-cloud DNN collaborative computing framework, JMDC, based on Joint Model and Data Compression. In JMDC, we adopt the attention mechanism to select important model channels for efficient inference computation, and important regions of the intermediate output for transferring to the cloud. We further use the quantization technique to reduce actual bits needed to be transferred. Depending on the specific application requirements on latency or accuracy, JMDC can adaptively determine the optimal partition point and compression strategy under different resource conditions. By extensive experiments based on the Raspberry Pi 4B device and the CIFAR10 dataset, we demonstrate the effectiveness of JMDC in enabling on-demand low-latency DNN inference, and its superiority over other baseline schemes. © 2022 Elsevier Inc.","Collaborative inference; Compression approaches; Deep neural network; Edge intelligence"
"GraphCS: Graph-based client selection for heterogeneity in federated learning","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150897020&doi=10.1016%2fj.jpdc.2023.03.003&partnerID=40&md5=1ea849eea3edd4b8b21a0fae26a87b56","Federated Learning coordinates many mobile devices to train an artificial intelligence model while preserving data privacy collaboratively. Mobile devices are usually equipped with totally different hardware configurations, leading to various training capabilities. At the same time, the distribution of the local training data is highly heterogeneous across different clients. Randomly selecting the clients to participate in the training process results in poor model performance and low system efficiency. In this paper, we propose GraphCS, a graph-based client selection framework for heterogeneity in Federated Learning. GraphCS first measures the distribution coupling across the clients via the model gradients. After that, it divides the clients into different groups according to the diversity of the local datasets. At the same time, it well estimates the runtime training capability of each client by jointly considering the hardware configuration and resource contention caused by the concurrently running apps. With the distribution coupling information and runtime training capability, GraphCS selects the best clients in order to well balance the model accuracy and overall training progress. We evaluate the performance of GraphCS with mobile devices with different hardware configurations on various datasets. The experiment results show that our approach improves model accuracy up to 45.69%. Meanwhile, it reduces communication and computation overhead 87.35% and 89.48% at best, respectively. Furthermore, GraphCS accelerates the overall training process up to 35×. © 2023 Elsevier Inc.","Client selection; Federated learning; Heterogeneity"
"On distributed data aggregation and the precision of approximate histograms","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163724277&doi=10.1016%2fj.jpdc.2023.104722&partnerID=40&md5=4d50dca22b937bd4e2180e908f31f2ef","We consider selected aspects of distributed data aggregation in wireless sensor networks (WSNs), including random sampling, averaging, and the construction of histograms. We propose a novel algorithm for distributed random sampling based on the extrema propagation technique. We also consider the problem of construction of approximate histograms in WSNs and estimation of the average of sensory data and bin counts using these histograms. We present the results of theoretical analysis of the precision of estimators calculated from approximate histograms and propose a modification of the original method which allows for construction of approximate equi-depth histograms based on a random sample from data. The theoretical and experimental results show that the estimates calculated from approximate equi-depth histograms are more precise than those obtained using equi-width histograms. We also present an idea of an algorithm for a distributed variant of the COUNT DISTINCT problem, which uses the extrema propagation technique. © 2023 Elsevier Inc.","Approximate histogram; Distributed data aggregation; Extrema propagation; Random sampling; Wireless ad-hoc networks"
"Towards elastic in situ analysis for high-performance computing simulations","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150427055&doi=10.1016%2fj.jpdc.2023.02.014&partnerID=40&md5=5cf525949c4272baf08fa05beab9e44d","In situ analysis and visualization have grown increasingly popular for enabling direct access to data from high-performance computing (HPC) simulations. As a simulation progresses and interesting physical phenomena emerge, however, the data produced may become increasingly complex, and users may need to dynamically change the type and scale of in situ analysis tasks being carried out and consequently adapt the amount of resources allocated to such tasks. To date, none of the production in situ analysis frameworks offer such an elasticity feature, and for good reason: the assumption that the number of processes could vary during run time would force developers to rethink software and algorithms at every level of the in situ analysis stack. In this paper we present Colza, a data staging service with elastic in situ visualization capabilities. We demonstrate the use of Colza with the Deep Water Impact and the AMR-Wind simulations, coupling them with the ParaView Catalyst and Ascent in situ libraries, and show that Colza enables dynamic rescaling of these widely-used frameworks with no interruption to the simulation or staging service. We highlight the challenges of enabling such elasticity, which requires overcoming these frameworks' reliance on MPI, using distinct engineering approaches, namely dependency injection and dependency overload. To the best of our knowledge, this work is the first to enable elastic in situ visualization capabilities for HPC applications on top of existing production analysis tools. © 2023 Elsevier Inc.","Colza; Elastic; In situ analysis; In situ visualization; Mochi"
"Leaderless consensus","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149827189&doi=10.1016%2fj.jpdc.2023.01.009&partnerID=40&md5=36e0ff4fe96cfa493572a8302617a4a2","Classic synchronous consensus algorithms are leaderless in that processes exchange proposals, choose the maximum value and decide when they see the same choice across a couple of rounds. Indulgent consensus algorithms are typically leader-based. Although they tolerate unexpected delays and find practical applications in blockchains running over an open network like the Internet, their performance is highly dependent on the activity of a single participant. This paper asks whether, under eventual synchrony, it is possible to deterministically solve consensus without a leader. The fact that the weakest failure detector to solve consensus is one that also eventually elects a leader seems to indicate that the answer to the question is negative. We prove in this paper that the answer is actually positive. We first give a precise definition of the very notion of a leaderless algorithm. Then we present three indulgent leaderless consensus algorithms, each we believe interesting in its own right: (i) for shared memory, (ii) for message passing with omission failures and (iii) for message passing with Byzantine failures. Finally, we implement a Byzantine fault tolerant (BFT) state machine replication (SMR), that is leaderless. Our empirical results demonstrate that it is faster and more robust than HotStuff, the recent BFT SMR algorithm used in the Facebook Libra blockchain when deployed in a wide area network. © 2023 Elsevier Inc.","Byzantine; Fast-path; Leaderless termination; Synchronizer; Synchronous-k"
"High performance HITA based Binary Edward Curve Crypto processor for FPGA platforms","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152448630&doi=10.1016%2fj.jpdc.2023.03.008&partnerID=40&md5=604d03ccf9b33ea555a0c607bfa60166","In an embedded and resource-constrained environment, Elliptic Curve Cryptography (ECC) has been noted as an efficient and suitable methodology for achieving information security via public-key cryptography. However, the drawback of ECC is its lack of unifiedness in point operation that makes it prone to side-channel attack. Also, ECC does not satisfy the completeness property due to which the addition formula is not defined for all the pairs of input points. Edward curve, with its unified addition law and completeness property, proved to be the answer to aforementioned flaws. High throughput while maintaining low resource is a key issue for elliptic curve cryptography (ECC) hardware implementations in many applications. This paper presents the implementation of a Binary Edward curve Crypto processor over GF(2233) for FPGA platforms. The architecture is modified to perform scalar multiplication in a parallel manner using two hybrid Karatsuba field multipliers. Field inversion being one of the most tedious operations while reconversion, is also performed in a parallel manner using an efficient Hex Itoh-Tsujii inversion algorithm. The hardware resources are shared for performing point operations and inversion. Exploiting parallelism in point and inversion operations has resulted in reduction of the clock cycles consumed and the resultant architecture is more efficient in terms of throughput over area. The design takes 0.038 ms on Xilinx Virtex-4 and 0.031 ms on Virtex-7 FPGA platforms to perform a 233-bit point multiplication operation. It takes 73.57%, 13.71%, 14.76% and 48.76% more efficient than existing scalar multiplication with BEC. This proposed scalable, side-channel attack resilient design outperforms the existing techniques with respect to throughput over area. © 2023 Elsevier Inc.","Edward curve cryptoprocessor; Elliptic Curve Cryptography (ECC); Field Programmable Gate Array (FPGA); Finite field"
"VM performance-aware virtual machine migration method based on ant colony optimization in cloud environment","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149893012&doi=10.1016%2fj.jpdc.2023.02.003&partnerID=40&md5=8fdd12e8d942e5fcfda2d7dc9f14a485","Many virtual machine (VM) allocation methods have been proposed to reduce the number of physical machines (PMs), improve resource utilization for cloud service providers. If VMs are migrated on the same PM, then there will be substantial resource competition among these VMs, which results in VM performance reduction. Many VM migration (VMM) methods neglect VM performance reduction. Although some performance-aware VM allocation methods were proposed, the performance optimization objective of these methods mainly aimed at guaranteeing service level agreement or reducing VM downtime during migration, and they did not scientifically analyze how the VM performance degrades. Therefore, how to minimize the VM performance reduction for users when migrating VMs remains a major challenge. This paper proposes a VM Performance-Aware VMM method (PAVMM) for both users and cloud service providers. To maximize VM performance for users, it utilizes the VM performance model, which was built in our previous works, to predict the VM performance after migrating VMs. It then establishes an optimization objective of maximizing VM performance for users. Meanwhile, minimizing the number of active PMs and the total migration cost are regarded as additional optimization objectives for cloud service providers. Therefore, we formulate VMM as a multi-objective optimization problem, which tries to maximize VM performance for users and minimize the number of active PMs and the total migration cost for cloud service providers simultaneously. Then an ant colony optimization (ACO)-based algorithm is proposed to solve the NP-hard VMM problem. Lastly, the experiments are conducted to evaluate PAVMM, and the results verify its efficiency. © 2023 Elsevier Inc.","Ant colony optimization; Cloud; Virtual machine migration; VM performance model"
"Why blockchain needs graph: A survey on studies, scenarios, and solutions","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163773765&doi=10.1016%2fj.jpdc.2023.104730&partnerID=40&md5=d85f97463557ad18a1dc46acd31d5c5a","The popularity of blockchain platforms and their applications in industry and academia keeps rising. The multifarious requirements stimulate another technique, graph data and algorithms, to join the blockchains; thus, studies, scenarios, and solutions about graph-related blockchains have emerged. This paper aims to see whether the state-of-the-art studies satisfy the applications through a comprehensive survey on graph-related blockchains. To answer why a blockchain needs graphs in general, we analyze literature about blockchain and graph, as well as use cases on the application-oriented and graph-related scenarios collected from practical blockchain projects. The paper summarizes three graph-related blockchain studies: graph algorithms for blockchains, graph data in blockchains, and graph applications on blockchains. Based on these summarization, it figures out the gaps between the studies and the applications, that is, few of studies natively integrate graph computing into a blockchain. Here, the “graph integration” means processing on-chain graph data, which contains blockchain information, in a real-time, distributed, and consensual manner. We propose the prospect of the Graph-integrated Blockchain Platform (GiBP for short), and explain why a GiBP is inevitable, the challenges of a GiBP, and the GiBPs' main functions and features people expect for future research. © 2023 Elsevier Inc.","Blockchain; Blockchain applications; Blockchain platform; Graph algorithm; Graph computing"
"Reducing energy consumption using heterogeneous voltage frequency scaling of data-parallel applications for multicore systems","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147255526&doi=10.1016%2fj.jpdc.2023.01.005&partnerID=40&md5=0f6174fe57fe7806b4a3015787d7c7b5","This paper investigates the exploitation of heterogeneous DVFS (dynamic voltage frequency scaling) control for improving the energy efficiency of data-parallel applications on ccNUMA shared-memory systems. We propose to adjust the clock frequency individually for the appropriately selected groups of cores, taking into account the diversified costs of parallel computation. This paper aims to evaluate the proposed approach using two different data-parallel applications: solving the 3D diffusion problem, and MPDATA fluid dynamics application. As a result, we observe the energy-savings gains of up to 20 percentage points over the traditional homogeneous frequency scaling approach on the server with two 18-core Intel Xeon Gold 6240. Additionally, we confirm the effectiveness of our strategy using two 64-core AMD EPYC 7773X. This paper also introduces two pruning algorithms that help select the optimal heterogeneous DVFS setups taking into account the energy or performance profile of studied applications. Finally, the cost and efficiency of developed algorithms are verified and compared experimentally against the brute-force search. © 2023 Elsevier Inc.","ccNUMA; Data-parallel applications; Energy saving; Heterogeneous voltage frequency scaling; Multicore"
"Exploring job running path to predict runtime on multiple production supercomputers","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147194837&doi=10.1016%2fj.jpdc.2023.01.001&partnerID=40&md5=5dbc046379456fe29b00ebca3512893b","There are massive jobs submitted in the supercomputer, and the job management system is typically deployed to schedule these jobs and allocate compute resources. FCFS (First Come First Serve) is a popular scheduling policy and the job's priority is determined based on the arrival time. However, under the FCFS strategy, if existing idle resources cannot meet the requirement of the head job in the waiting queue, they cannot be allocated to other jobs, which suffers from resource waste. To optimize the resource utilization, the backfilling method is proposed, which allocates the reserved idle compute nodes to a small-size and short-running non-head job, on the premise of not delaying the original head job. Obtaining the job's runtime in advance is necessary for backfilling and the traditional method relies on the user's estimation. Unfortunately, the estimated runtime provided by users is generally overestimated. Many studies extract features from historical job logs and adopt machine learning to predict the runtime. However, traditional features are insufficient to characterize the job. In this paper, we collect job logs from two supercomputers and present a novel runtime prediction framework called PREP. PREP explores the job's running path as a new feature, which implies plentiful information about the job's properties, such as the user, the project, the scale of data sets, and the parameters used. As there is a strong correlation between the job's runtime and its running path, we group jobs with similar paths into a cluster and train a runtime prediction model for each cluster respectively. Extensive evaluations demonstrate that introducing the new feature can achieve higher prediction accuracy (88.5% and 82.3% in two production supercomputers respectively), and our framework has a more desirable prediction performance than other popular strategies like Last-2 and IRPA. In addition, the predicted runtime is inserted into the real job trace of a slurm simulator to verify the advantages of PREP. © 2023 Elsevier Inc.","Backfilling; Machine learning; Path; Runtime prediction"
"List and shelf schedules for independent parallel tasks to minimize the energy consumption with discrete or continuous speeds","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146056315&doi=10.1016%2fj.jpdc.2022.12.003&partnerID=40&md5=2f54332d670ffe22818087307d0c9bee","Scheduling independent tasks on a parallel platform is a widely-studied problem, in particular when the goal is to minimize the total execution time, or makespan (P||Cmax problem in Graham's notations). Also, many applications do not consist of sequential tasks, but rather parallel tasks, either rigid, with a fixed degree of parallelism, or moldable, with a variable degree of parallelism (i.e., for which we can decide at the execution on how many processors they are executed). Furthermore, since the energy consumption of data centers is a growing concern, both from an environmental and economical point of view, minimizing the energy consumption of a schedule is a main challenge to be addressed. One can then decide, for each task, on how many processors it is executed, and at which speed the processors are operated, with the goal to minimize the total energy consumption. We further focus on co-schedules, where tasks are partitioned into shelves, and we prove that the problem of minimizing the energy consumption remains NP-complete when static energy is consumed during the whole duration of the application. We are however able to provide an optimal algorithm for the schedule within one shelf, i.e., for a set of tasks that start at the same time. Several approximation results are derived, both with discrete and continuous speed models, and extensive simulations are performed to show the performance of the proposed algorithms. © 2022","Co-schedule; DVFS; Energy minimization; Parallel tasks; Scheduling"
"Speculative inter-thread store-to-load forwarding in SMT architectures","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142738495&doi=10.1016%2fj.jpdc.2022.11.007&partnerID=40&md5=7e5b023d49594b158c87b980355f9e23","Applications running on out-of-order cores have benefited for decades of store-to-load forwarding which accelerates communication of store values to loads of the same thread. Despite threads running on a simultaneous multithreading (SMT) core could also access the load queues (LQ) and store queues (SQ) / store buffers (SB) of other threads to allow inter-thread store-to-load forwarding, we have skipped exploiting it because if we allow communication of different SMT threads via their LQs and SQs/SBs, write atomicity may be violated with respect to the outside world beyond the acceptable model of read-own-write-early multiple-copy atomicity (rMCA). In our prior work, we leveraged this idea to propose inter-thread store-to-load forwarding (ITSLF). ITLSF accelerates synchronization and communication of threads running in a simultaneous multi-threading processor by allowing stores in the store-queue of a thread to forward data to loads of another thread running in the same core without violating rMCA. In this work, we extend the original ITSLF mechanism to allow inter-thread forwarding from speculative stores (Spec-ITSLF). Spec-ITSLF allows forwarding store values to other threads earlier, which further accelerates synchronization. Spec-ITSLF outperforms a baseline SMT core by 15%, which is 2% better on average (and up to 5% for the TATP workload) than the original ITSLF mechanism. More importantly, Spec-ITSLF is on par with the original ITSLF mechanism regarding storage overhead but does not need to keep track of the speculative state of stores, which was an important source of overhead and complexity in the original mechanism. © 2022 The Author(s)","Memory consistency; Multiple-copy atomicity; Simultaneous multithreading; Store-to-load forwarding"
"A secure and efficient three-factor authentication protocol for IoT environments","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160349706&doi=10.1016%2fj.jpdc.2023.104714&partnerID=40&md5=41bc56dd79681737060e76c0adb7dfd8","The Internet of Things (IoT) is an information carrier based on the Internet and traditional telecommunications network, which enables all ordinary physical objects that can be independently addressed to form an interconnected network. User authentication protocol is an essential technology for security and privacy in the IoT environment. This paper analyzes the security of Mirsaraei et al.'s three-factor authentication scheme for IoT environments (Mirsaraei et al., 2022 [31]), and finds that the scheme cannot provide users with untraceability, perfect forward secrecy or the resistance of key compromise impersonation attack. The article improves Mirsaraei et al.'s scheme and proposes a three-factor authentication protocol with perfect forward secrecy using elliptic curve cryptosystem, which retains the general process of Mirsaraei et al.'s scheme. The formal security analysis of the proposed protocol is carried out by ROR (Real-or-Random) model, and the formal security verification of the proposed protocol is implemented by Proverif tool. The cryptoanalysis results demonstrate that the proposed protocol makes up for the shortcomings of Mirsaraei et al.'s scheme in security and can resist more malicious attacks as opposed to recent schemes. Moreover, the performance analysis using MIRACL (Multiprecision Integer and Rational Arithmetic C/C++ Library) shows that, the proposed protocol has great advantages over analogical three-factor authentication schemes in terms of computational overhead and communication overhead. © 2023 Elsevier Inc.","Elliptic curve cryptosystem; Internet of Things; Performance; Security; Three-factor authentication"
"A parallel computing architecture based on cellular automata for hydraulic analysis of water distribution networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152595864&doi=10.1016%2fj.jpdc.2023.03.009&partnerID=40&md5=b23a2801a55561ab082adb3dfc4b9efe","Water distribution networks (WDNs) are one of the largest infrastructures in society. Various methods for formulation and hydraulic analysis of water distribution networks, including numerical and non-numerical methods, have been previously proposed. Due to the complexity, the nonlinearity of the hydraulic equations of water distribution networks, and the need for multiple executions and uncertainties in parameters, solving the hydraulic model of water distribution networks has high time complexity. In this paper, a parallel computational architecture based on the concept of cellular automata is proposed to accelerate the numerical solution of the steady-state water distribution network model. Taylor series is proposed to solve hydraulic equations. The presented architecture was implemented as a parallel hardware platform on a field-programmable gate array. The performance of the proposed method was compared with EPANET software for networks with different complexities and topologies. The results show that the proposed parallel algorithm can accelerate the hydraulic analysis of regular water distribution networks up to 700 times and 250 times for small and large networks, respectively. © 2023 Elsevier Inc.","Cellular automata; Field programmable gate array; Steady-state hydraulic analysis; Taylor series; Water distribution network"
"Multi-agent DRL for joint completion delay and energy consumption with queuing theory in MEC-based IIoT","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149278228&doi=10.1016%2fj.jpdc.2023.02.008&partnerID=40&md5=399d5258dc012fe5a25d132362de7448","In the Industrial Internet of Things (IIoT), there exist numerous sensor devices with weak computing power and small energy storage. To meet the real-time and big data computing requirements of industrial production, EIIoT (Edge computing-based IIoT) that combines mobile edge computing with IIoT has emerged. It is necessary to offload computing tasks to nearby edge servers for data storage and processing in EIIoT, thus inevitably causing the edge servers to overload. To this end, we propose a jointly constrained optimization model of delay and energy consumption based on queuing theory; this model can effectively solve the task offloading problem in EIIoT. Subsequently, to satisfy the unique offloading requirements of EIIoT, we improve the MAPPO (multi agent proximal policy optimization) algorithm structure to form a lightweight optimal task offloading algorithm called Multi-Agent Deep Reinforcement Learning based on Queuing theory (MAQDRL), which is more suitable for EIIoT. In the algorithm, we systematically integrate queuing theory and use Multi-Agent Deep Reinforcement Learning (MADRL) to obtain the optimal offloading strategy in dynamic and random multiuser offloading environments. We also improve the structure of neural networks of MADRL by analyzing the structural characteristics of the input data. As a result, the algorithm that we proposed exhibits good convergence and exceptional performance in terms of the task arrival rate, bandwidth, energy consumption, latency and other indicators. The simulation results indicate that compared with other classical algorithms, MAQDRL is effective for solving the EIIoT offloading problem. © 2023 Elsevier Inc.","Industrial Internet of things; Mobile edge computing; Multi-agent deep reinforcement learning; Queuing theory; Task offloading"
"Performance analysis of DPDK-based applications through tracing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141918110&doi=10.1016%2fj.jpdc.2022.10.012&partnerID=40&md5=e06048930656a03e8871dbcdff65dbd7","The Data Plane Development Kit (DPDK) is an efficient framework developed based on the kernel bypass approach to accelerate packet processing in userspace. This framework includes many libraries and Poll Mode Drivers (PMDs) that are optimized for maximum network performance. Several studies have demonstrated that porting existing data plane applications to DPDK can considerably increase their performance. Unfortunately, kernel bypassing makes traditional network monitoring and debugging tools obsolete. Therefore, considering the complexity of networking software, detecting unexpected behaviors, and diagnosing its root causes became a tedious task. The literature reports a few tools intended for debugging DPDK application bugs, but they are mostly ineffective against performance bugs. In this paper, we propose a tracing-based performance analysis framework that is dedicated to DPDK applications. The framework provides many analyses enabling the practitioner to gain insights into the application internals and diagnose performance bugs. We use an efficient approach to collect tracing data from DPDK libraries and drivers, with a very low overhead (<0.2%). Our framework correlates the data collected to build a data model that illustrates the states of the monitored application' components. We leverage this data model to calculate adapted performance metrics and display them in time-synchronized graphical views. The numerous use cases that we present demonstrate the ability of our tool to identify and diagnose efficiently various performance bugs. © 2022 Elsevier Inc.","DPDK; Performance analysis; Root cause analysis; Tracing; Troubleshooting"
"Anthropomorphic diagnosis of runtime hidden behaviors in OpenMP multi-threaded applications","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150018202&doi=10.1016%2fj.jpdc.2023.02.012&partnerID=40&md5=b2a65379af81b5922fd7ba008fa24248","Extreme-scale computing involves hundreds of millions of threads with multi-level parallelism running on large-scale hierarchical and heterogeneous hardware. Some OpenMP multi-threaded applications increasingly suffer from runtime hidden behaviors owning to shared resource contention as well as software- and hardware-related problems. Such hidden behaviors can result in failure and inefficiencies and are among the main challenges in system resiliency. To minimize the impact of hidden behaviors, one must quickly and accurately detect and diagnose the hidden behaviors that cause the failures. However, it is difficult to identify hidden behaviors in the dynamic and noisy data collected by OpenMP multi-threaded monitoring infrastructures. This paper presents an anthropomorphic diagnosis framework for hidden behaviors of OpenMP multi-threaded applications. In the framework, we first design injected heartbeat functions for OpenMP multi-threaded applications. Then, we leverage the heartbeat sequences to extract features of hidden behaviors. Finally, we develop a feature learning-based algorithm using heartbeat analysis, namely HSA, to diagnose hidden behaviors. To evaluate our framework, the NAS Parallel NPB benchmark, EPCC OpenMP micro-benchmark suite, and Jacobi benchmark are used to test the performance of our proposed framework. The experimental results demonstrate that our framework successfully identifies 90.3% of the injected hidden behaviors of OpenMP multi-threaded applications while acquiring low overhead. © 2023 Elsevier Inc.","Heartbeat; Hidden behaviors; High performance computing; Machine learning; OpenMP"
"IAP-SpTV: An input-aware adaptive pipeline SpTV via GCN on CPU-GPU","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104741","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165956323&doi=10.1016%2fj.jpdc.2023.104741&partnerID=40&md5=891c72b683e3798de651f29864e72eac","Sparse tensor-times-vector (SpTV) is the core computation of tensor decomposition. Optimizing the computational performance of SpTV on CPU-GPU becomes a challenge due to the complexity of the non-zero element sparse distribution of the tensor. To solve this problem, we propose IAP-SpTV, an input-aware adaptive pipeline SpTV via Graph Convolutional Network (GCN) on CPU-GPU. We first design the hybrid tensor format (HTF) and explore the challenges of the HTF-based Pipeline SpTV algorithm. Second, we construct Slice-GCN to overcome the challenge of selecting a suitable format for each slice of HTF. Third, we construct an IAP-SpTV performance model for pipelining to achieve the maximum overlap between transfer and computation time during pipelining. Finally, we conduct experiments on two CPU-GPU platforms of different architectures to verify the correctness, effectiveness, and portability of IAP-SpTV. Overall, IAP-SpTV provides a significant performance improvement of about 24.85% to 58.42% compared to the state-of-the-art method. © 2023 Elsevier Inc.","Format selection; GCN; Hybrid format; Pipeline model; SpTV"
"Multi-directional Sobel operator kernel on GPUs","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151477380&doi=10.1016%2fj.jpdc.2023.03.004&partnerID=40&md5=31eb359cefe8d1ae15c976f371a360f7","Sobel is one of the most popular edge detection operators used in image processing. To date, most users utilize the two-directional 3×3 Sobel operator as detectors because of its low computational cost and reasonable performance. Simultaneously, many studies have been conducted on using large multi-directional Sobel operators to satisfy their needs considering the high stability, but at an expense of speed. This paper proposes a fast graphics processing unit (GPU) kernel for the four-directional 5×5 Sobel operator. To improve kernel performance, we implement the kernel based on warp-level primitives, which can significantly reduce the number of memory accesses. In addition, we introduce the prefetching mechanism and operator transformation into the kernel to significantly reduce the computational complexity and data transmission latency. Compared with the OpenCV-GPU library, our kernel shows high performances of 6.7x speedup on a Jetson AGX Xavier GPU and 13x on a GTX 1650Ti GPU. © 2023 Elsevier Inc.","Acceleration algorithm; Graphics processor; Multi-directional; Sobel operator"
"A distributed message passing algorithm for computing perfect demand matching","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159212579&doi=10.1016%2fj.jpdc.2023.04.007&partnerID=40&md5=580eed61080d2b6dc62a4fe228a5b36c","In this paper, we consider the perfect demand matching problem (PDM) which combines aspects of the knapsack problem along with the b-matching problem. It is a generalization of the maximum weight matching problem which has been fundamental in the development of theory of computer science and operations research. This problem is NP-hard and there exists a constant ϵ>0 such that the problem admits no 1+ϵ-approximation algorithm, unless P=NP. Here, we investigate the performance of a distributed message passing algorithm called Max-sum belief propagation for computing the problem of finding the optimal perfect demand matching. As the main result, we demonstrate the rigorous theoretical analysis of the Max-sum BP algorithm for PDM, and establish that within pseudo-polynomial-time, our algorithm could converge to the optimal solution of PDM, provided that the optimal solution of its LP relaxation is unique and integral. Different from the techniques used in previous literature, our analysis is based on primal-dual complementary slackness conditions, and thus the number of iterations of the algorithm is independent of the structure of the given graph. Moreover, to the best of our knowledge, this is one of a very few instances where BP algorithm is proved correct for NP-hard problems. © 2023 Elsevier Inc.","Belief propagation; Distributed algorithm; Linear programming dual; Message passing; Perfect demand matching"
"A massively parallel implicit 3D unstructured grid solver for computing turbulent flows on latest distributed memory computational architectures","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168749717&doi=10.1016%2fj.jpdc.2023.104750&partnerID=40&md5=9a4e0baba8a00f89118395e0d4923d12","An implicit unstructured grid density-based solver–PRAVAHA based on a parallel variant of the lower upper symmetric Gauss-Seidel (LUSGS) method is developed to compute large-scale engineering problems. A four-layered parallel algorithm is designed to efficiently compute three-dimensional turbulent flows on massively parallel modern multiple instruction multiple data-stream (MIMD) computational hardware. This data parallel approach achieves multiple layers of parallelism including continuity of flow solution, transfer of solution gradients, and calculation of drag/lift/solution residuals, right up to the innermost implicit LUSGS solver sub-routine, which is relatively less explored in the literature. Domain decomposition is performed using the METIS software based on multi-level graph partitioning algorithms. Non-blocking message-passing interface library functions are used to manage inter-processor communication through explicit message passing, efficiently. Super-linear scalability of the parallel solver is established on the current state-of-the-art supercomputing facility, the 838 teraflops PARAM seva on up to 6144 cores. Linear or even super-linear speedup on problems of significant size is observed even on ad-hoc parallel computing platforms like workstations and multi-node clusters, for turbulent flow simulations. © 2023 Elsevier Inc.","Implicit LUSGS; Message passing; Parallel code; Scalability; Unstructured grid"
"PTTS: Power-aware tensor cores using two-sided sparsity","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142513519&doi=10.1016%2fj.jpdc.2022.11.004&partnerID=40&md5=397632ce8c8f3901f9261e40d73e7154","Deep Neural networks (DNNs) have become the compelling solution for a broad range of applications such as automatic translation, advertisement recommendation, and speech recognition. Matrix multiplication is the fundamental operation used in various classes of DNNs. Introduction of tensor cores (TCs) in NVIDIA GPGPUs particularly aims at acceleration of neural networks. A TC is a specialized unit dedicated to compute matrix-multiply-and-accumulate (MMA) operations. State-of-the-art DNNs are known to be power-hungry and compute-intensive due to increasing depth of networks, i.e. multiple layers with massive number of neurons. This causes excessive energy in TCs. Sparse neural networks have emerged as an effective solution to address massive amount of computations in DNNs. While sparsity is widely used for acceleration of DNNs, only a handful of studies focused on energy aspect of sparse DNNs and energy-efficient architectures for TCs are scarce. We exploit sparsity in DNNs and propose power gating multipliers with sparse activations or weights. We show that sparsity occurs in TCs for short intervals and conventional power gating techniques cannot fully exploit these idle intervals mainly due to overhead of power gating. To mitigate the overhead of power gating, we propose power-aware tensor cores using two-sided sparsity (PTTS) which monitors inputs of multipliers and turns them off only if inputs remain sparse for long intervals. We also propose an architectural technique that shuffles multiplications to pack idle intervals and increase opportunities for power gating. We introduce a low-cost and implementation-efficient sparse input operand interconnect to change order of effectual multiplications. Our proposed techniques combined are able to save energy by 68% in Tensor Cores with negligible impact on performance while maintaining accuracy. © 2022 Elsevier Inc.","Accelerator architecture; Deep neural networks; Energy; Tensor core"
"Enabling zero knowledge proof by accelerating zk-SNARK kernels on GPU","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142177836&doi=10.1016%2fj.jpdc.2022.10.009&partnerID=40&md5=2df0b9a1ff02da4f9dd86d66917c6f34","As a recent cryptography protocol, Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARK) allows one party to prove that it possesses certain information without revealing information to these untrusted proof provers. This mechanism has the ability to provide the function for constructing and verifying information integrity without privacy leaking. However, the computation kernels of zk-SNARK consume too much computing power and produce a significant performance bottleneck with the growing data volume and security requirement. In this paper, we take advantage of Graphic Processing Unit (GPU) to enhance zk-SNARK efficiency by accelerating the most time-consuming computation kernels: modular multiplication and Number-Theoretic Transform (NTT)/Inverse Number-Theoretic Transform (INTT) in Elliptic Curve Cryptography (ECC) pairing with two major improvements: (1) Adopting interval limbs multiply-add quaternary operation to directly accelerate ECC pairing by making full advantage of information entropy within the limited hardware bit width; (2) Data layout and shuffle methods in GPU global memory and shared memory for data space consistency maintenance accelerating NTT/INTT which indirectly works on ECC pairing. To the best of our knowledge, our work would be the first exploration to accelerate these improvements on GPU. The measured results show that our methods are able to accelerate modular multiplication and NTT/INNT by 1.22× and 4.67× times respectively compared with the previous GPU implementation. With these accelerated kernels, we are able to achieve 3.14× speedup for Groth16, which is the most efficient zk-SNARK implementation working on BLS12-381 ECC field. With the bottleneck tackled, our work will expand the deployment scenarios of zk-SNARK in Zero Knowledge Proof (ZKP). © 2022 Elsevier Inc.","GPU acceleration; Montgomery multiplication; NTT/INTT; zk-SNARK; ZKP"
"Fault-tolerant unicast using conditional local safe model in the data center network BCube","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104732","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164224377&doi=10.1016%2fj.jpdc.2023.104732&partnerID=40&md5=921f6500f9d0ea4182bac9255d73c76e","As an essential infrastructure to support cloud computing services, data center networks (DCNs) are primarily used for transmitting and storing data. BCube is a large-scale DCN architecture that can effectively handle the massive data generated by network end devices due to the explosive growth of the Internet. However, as server failures in DCNs become more frequent, ensuring reliable data communication in BCube is critical. In this paper, we first establish a conditional local safe model in BCube, which divides the fault-free nodes in sub-BCube by adding some restrictions, and effectively avoids communication obstacles that may arise from faulty nodes. Then, based on the proposed model, a fault-tolerant unicast path algorithm is designed, which can construct a reliable data transmission path in the local safe sub-BCube of BCube and can tolerate more faulty nodes than existing algorithms. Finally, we conduct simulation experiments to verify that our algorithm can construct the shortest path with a high probability and achieve almost complete success in data transmission when the number of faulty nodes does not exceed half of the total nodes in BCube. © 2023 Elsevier Inc.","Algorithm; BCube; Conditional local safe information; Data center network; Fault-tolerant unicast"
"RASM: Resource-Aware Service Migration in Edge Computing based on Deep Reinforcement Learning","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172175432&doi=10.1016%2fj.jpdc.2023.104745&partnerID=40&md5=846cc0300cebbda0e584f97eea18ca7f","Multi-access Edge Computing (MEC) paradigm allows devices to offload their intensive service tasks that require high Quality of Experience (QoE). Devices mobility forces services to migrate between MECs to maintain QoE in terms of delay. The decision on when to migrate a service requires a cost and QoE tradeoff, and destination MEC selection needs to be done upon latency and resource availability constraints to minimize migrations. To this end, we propose a novel Resource-Aware Service Migration (RASM) mechanism using Deep Q-Network (DQN) to make migration decisions by achieving tradeoff between the QoE in terms of delay and migration cost. Moreover, DQN learns the best policy for maximizing QoE by selecting the migration destination based on the MECs proximity to the device and estimated resource availability at the servers using queuing model. Results show faster convergence to optimal policy, reduced average end-to-end service delay by 27%, and smaller service rejection rate by 24% comparing to the state-of-the-art. © 2023 Elsevier Inc.","Deep Q-Network (DQN); Deep Reinforcement Learning (DRL); Multi-access Edge computing; Resource management; Service migration"
"Accelerating distributed machine learning with model compression and graph partition","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159478591&doi=10.1016%2fj.jpdc.2023.04.006&partnerID=40&md5=9eabe304e6db2aaed1fa94ff1b6af9fb","The rapid growth of data and parameter sizes of machine learning models makes it necessary to improve the efficiency of distributed training. It is observed that the communication cost usually is the bottleneck of distributed training systems. In this paper, we focus on the parameter server framework which is a widely deployed distributed learning framework. The frequent parameter pull, push, and synchronization among multiple machines leads to a huge communication volume. We aim to reduce the communication cost for the parameter server framework. Compressing the training model and optimizing the data and parameter allocation are two existing approaches to reducing communication costs. We jointly consider these two approaches and propose to optimize the data and parameter allocation after compression. Different from previous allocation schemes, the data sparsity property may no longer hold after compression. It brings additional opportunities and challenges for the allocation problem. We also consider the allocation problem for both linear and deep neural network (DNN) models. Fixed and dynamic partition algorithms are proposed accordingly. Experiments on real-world datasets show that our joint compression and partition scheme can efficiently reduce communication overhead for linear and DNN models. © 2023","Data sparsity; Distributed machine learning; Graph partition; Parameter server framework"
"Anomaly-based intrusion detection system in the Internet of Things using a convolutional neural network and multi-objective enhanced Capuchin Search Algorithm","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146417419&doi=10.1016%2fj.jpdc.2022.12.009&partnerID=40&md5=2a4ebcefc10b08c277843b11bb4630cb","Nowadays, the growth and pervasiveness of Internet of Things (IoT) devices have led to increased attacks by hackers and attackers. On the other hand, using IoT infrastructure in various fields has increased the number of node security breaches, attacks, and anomalies. Therefore, detecting anomalies in IoT devices is vital to reduce attacks and strengthen security. Over the past few years, various research has been conducted in anomaly-based intrusion detection using machine learning and deep learning methods. The biggest challenge in machine learning methods is the inability to extract new features. To do this, researchers use deep learning methods to extract new features that lead to increased accuracy in intrusion detection. There are important unsolved challenges in research, including determining important features in detecting malicious attacks, extracting features from raw network traffic data using deep networks, and insufficient accuracy in detecting attacks against IoT devices. Convolutional neural networks are considered a powerful and reliable method in this field due to the ability to automatically extract features from data and perform faster calculations. This study has designed and implemented the IoT features extraction convolutional neural network called IoTFECNN with hybrid layers for better anomaly detection in the IoT. Moreover, a binary multi-objective enhanced Capuchin Search Algorithm (CSA) called BMECapSA is developed for efficient feature selection. The combination of the IoTFECNN and BMECapSA methods has led to the introduction of a new hybrid method called CNN-BMECapSA-RF. Finally, the proposed method is implemented and tested on two data sets, NSL-KDD and TON-IoT. The results of various experiments exhibit that the proposed method has better results regarding classification criteria compared to existing deep learning and machine learning-based anomaly detection systems. The proposed method has reached 99.99% and 99.85% accuracy by identifying 27% and 44% of the effective features on the TON-IoT and NSL-KDD datasets, respectively. © 2023 Elsevier Inc.","Convolutional neural network; Internet of Things; Intrusion detection system; Multi-objective CSA"
"As easy as ABC: Optimal (A)ccountable (B)yzantine (C)onsensus is easy!","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167569981&doi=10.1016%2fj.jpdc.2023.104743&partnerID=40&md5=c54e12820edc9894d4de7d4ea1d3997b","In a non-synchronous system with n processes, no t0-resilient (deterministic or probabilistic) Byzantine consensus protocol can prevent a disagreement among correct processes if the number of faulty processes is ≥n−2t0. Therefore, the community defined the accountable Byzantine consensus problem: the problem of (i) solving Byzantine consensus whenever possible (e.g., when the number of faulty processes does not exceed t0), and (ii) allowing correct processes to obtain proofs of culpability of n−2t0 faulty processes whenever a disagreement occurs. This paper presents ABC, a simple yet efficient transformation of any non-synchronous t0-resilient (deterministic or probabilistic) Byzantine consensus protocol into its accountable counterpart. In the common case (up to t0 faults), ABC introduces an additive overhead of two communication rounds and O(n2) exchanged bits. Whenever they disagree, correct processes detect culprits by exchanging O(n3) messages, which we prove optimal. Lastly, ABC is not limited to Byzantine consensus: ABC provides accountability for other essential distributed problems (e.g., reliable and consistent broadcast). © 2023 The Author(s)","Accountability; Byzantine fault tolerance; Distributed consensus; Fault detection"
"PRI: PCH-based privacy-preserving with reusability and interoperability for enhancing blockchain scalability","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104721","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161679219&doi=10.1016%2fj.jpdc.2023.104721&partnerID=40&md5=29d715439cf9ca475e5e2d4e7847b712","Blockchain systems, one of the most popular distributed systems, are well-applied in various scenarios, e.g., logistics and finance. However, traditional blockchain systems suffer from scalability issues. To tackle this issue, Payment Channel Hubs (PCHs) are proposed. Recent efforts, such as A2L (SP'21) and Teechain (SOSP'19), enhance the privacy, reusability, and interoperability properties of PCHs. Nevertheless, these solutions have intrinsic limitations: they rely on trusted hardware or suffer from the deposit lock-in problem. Furthermore, the functionalities of some of these solutions are restricted to fixed-amount payments and do not support multi-party participation. These aforementioned problems limit their capabilities to alleviate blockchain scalability issues. In this paper, we propose PRI, a novel PCH solution that simultaneously guarantees transaction Privacy (i.e., relationship unlinkability and value confidentiality), deposit Reusability, and blockchain Interoperability, which can mitigate the aforementioned problems. PRI is constructed by several new building blocks, including (1) an atomic deposit protocol that enforces user and hub to deposit equivalent assets in a shared address for building a fair payment channel; (2) a privacy-preserving deposit certification scheme that leverages the Pointcheval and Sanders signature and non-interactive zero-knowledge proof to resolve the deposit lock-in issue in maintaining payment channels; (3) a range proof which ensures the legality and confidentiality of transaction values. We conduct extensive experimental evaluations of PRI, demonstrating that it improves the state-of-the-art approaches in terms of performance. © 2023 Elsevier Inc.","Blockchain; Interoperability; Reusability; Scalability"
"Energy-aware fully-adaptive resource provisioning in collaborative CPU-FPGA cloud environments","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149186652&doi=10.1016%2fj.jpdc.2023.02.009&partnerID=40&md5=2afd3694980a3fc75d0179163fab05f4","Cloud warehouses have been exploiting multi-tenancy in CPU-FPGA collaborative environments, so clients can share the same infrastructure, achieving scalability and maximizing resource utilization. Therefore, the distribution of tasks across CPU and FPGA must be well-balanced so performance and energy are optimized in a highly variant workload scenario. In this paper, we take a step further and, in contrast to existing approaches, exploit DVFS (Dynamic Voltage and Frequency Scaling) on the CPU, together with an intelligent CPU-FPGA resource provisioning mechanism, to further improve energy. For that, we propose EASER, an end user-transparent framework that employs multiple strategies and dynamically selects the most appropriate one to optimize resource provisioning and DVFS according to the warehouse needs, workload properties, and target architecture. Our synergistic DVFS optimization brings up to 22% additional energy gains over our dynamic provisioning alone. Compared to fixed single strategies with DVFS, EASER brings, on average, 71% of energy gains. © 2023 Elsevier Inc.","Adaptive provisioning; CPU-FPGA; DVFS; Energy-aware; Multi-tenancy"
"Parallel global edge switching for the uniform sampling of simple graphs with prescribed degrees","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146099780&doi=10.1016%2fj.jpdc.2022.12.010&partnerID=40&md5=d3139ce4f787544f254945cbdfa89869","The uniform sampling of simple graphs matching a prescribed degree sequence is an important tool in network science, e.g. to construct graph generators or null-models. Here, the Edge Switching Markov Chain (ES-MC) is a common choice. Given an arbitrary simple graph with the required degree sequence, ES-MC carries out a large number of small changes, called edge switches, to eventually obtain a uniform sample. In practice, reasonably short runs efficiently yield approximate uniform samples. In this work, we study the problem of executing edge switches in parallel. We discuss parallelizations of ES-MC, but find that this approach suffers from complex dependencies between edge switches. For this reason, we propose the Global Edge Switching Markov Chain (G-ES-MC), an ES-MC variant with simpler dependencies. We show that G-ES-MC converges to the uniform distribution and design shared-memory parallel algorithms for ES-MC and G-ES-MC. In an empirical evaluation, we provide evidence that G-ES-MC requires not more switches than ES-MC (and often fewer), and demonstrate the efficiency and scalability of our parallel G-ES-MC implementation. © 2022 Elsevier Inc.","Degree Sequence; Edge Switch; Markov Chain; Random Graph; Uniform Sampling"
"On the correctness of highly available systems in the presence of failures","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161634309&doi=10.1016%2fj.jpdc.2023.04.008&partnerID=40&md5=accdbcb3ff986d101354ae09cb9107fb","In this paper we formally study the guarantees provided by highly available, eventually consistent replicated systems in an environment in which machine failures and network splits are possible. Our analysis accounts for possible replica recovery after a crash, and clients that are (1) stateless or stateful, (2) sticky (always connect to a concrete set of replicas) or mobile, and (3) which can timeout before receiving a response to the sent request. We show why the approaches to prove protocol correctness prevalent in the literature, which do not take into account replica or network crashes, may lead to incorrect conclusions regarding the guarantees offered by the protocol. We adapt the existing formal correctness criteria, such as basic eventual consistency, to the considered environment by defining the family of failure-aware consistency guarantees. We formally identify a set of undesired phenomena (in particular phantom operations) observed by the clients, which, as we prove, are unavoidable in highly available systems in which unrecoverable replica crashes are possible. We also introduce context preservation, a new client-side requirement for eventually consistent systems that expose concurrency to the client, i.e., allow clients to use, e.g., multi-value registers or observed-remove sets. Context preservation is incomparable with classic session guarantees. © 2023 Elsevier Inc.","CAP; Eventual consistency; Fault-tolerance"
"Mutual visibility by fat robots with slim omnidirectional camera","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160847896&doi=10.1016%2fj.jpdc.2023.104716&partnerID=40&md5=c7c1b45efcbb8fbcd45716a550644ecb","In the existing literature of the MUTUAL VISIBILITY problem for autonomous robot swarms, the adopted visibility models have some idealistic assumptions that are not consistent with practical sensing device implementations. This paper investigates the problem in the more realistic visibility model called opaque fat robots with slim omnidirectional camera. The robots are modeled as unit disks, each having an omnidirectional camera represented as a disk of smaller size. The region obstructed by a single robot in this model is a truncated infinite cone. This makes the visibility model significantly challenging compared to the previous ones. We assume that the robots have compasses that allow agreement in the direction and orientation of both axes of their local coordinate systems. The robots are equipped with visible lights which serve as a medium of communication and also as a form of memory. We present a distributed algorithm for the MUTUAL VISIBILITY problem which is provably correct in the semi-synchronous setting. Our algorithm also provides a solution for LEADER ELECTION which we use as a subroutine in our main algorithm. Although LEADER ELECTION is trivial with two axis agreement in the full visibility model, it is challenging in our case and is of independent interest. © 2023 Elsevier Inc.","Mutual visibility; Opaque fat robots; Robot swarm; Robots with lights; Slim omnidirectional camera"
"A federated collaborative recommendation model for privacy-preserving distributed recommender applications based on microservice framework","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144315051&doi=10.1016%2fj.jpdc.2022.12.002&partnerID=40&md5=6711c64573bcf4d38ef959e02c6dc207","The wide adoption of IoT technologies has accelerated the accumulation of big data. Recommender systems (RS) is one of the most effective methods to extract user interested items from the huge volume of big data. However, implementing a recommender system over the distributed error-prone IoT devices faces two challenges. On the one hand, the distributed IoT devices may randomly fail to deliver its local data due to hardware malfunction, which may cause unavailability of the recommender system. Moreover, collecting the raw data from the distributed IoT devices may cause data privacy leakage issue, since the privacy data of user-item interaction records may be abused by vicious parties. In view of these challenges, we propose a federated collaborative recommendation model based on microservice framework in this paper to implement privacy preserving distributed recommendation applications. Firstly, we utilize the federated learning framework to train the collaborative recommendation model, where the raw data on each distributed device is kept locally and only item related model parameters are exposed to train the federated recommendation model. Moreover, we adopt the microservice framework to encapsulate different functions of the federated recommender model. Each distributed device can participate in the federated training process via service registration and service discovery function of the microservice framework. Furthermore, we enhance the typical Neural Collaborative Filtering model with the proposed FedNeuMF model by fusing auxiliary user profiles and item attributes to improve the recommendation accuracy. Finally, we conduct a set of experiments on three real-world datasets to check the performance of our proposal. © 2022 Elsevier Inc.","Distributed recommender system; Federated collaborative recommendation model; Microservice framework; Privacy preservation"
"RATS: A regulatory anonymous transaction system based on blockchain","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169600705&doi=10.1016%2fj.jpdc.2023.104751&partnerID=40&md5=1629d78a37eab93de8cd5714034b5d37","With the rapid development of digital currency such as Bitcoin, the digital currency transaction system with blockchain as the key underlying technology is booming, but the traditional transaction system has the risk of revealing sensitive information such as transaction content and identity. In order to enhance the security, many transactions system with privacy preserving have been proposed, such as Zerocoin, Monero, etc. As there are a decentralized credit entity and strong anonymity in these transaction systems, the authorities can effectively audit and control the participants and transactions in the digital currency system, thus making digital currency a tool for illegal transactions. Recognizing the importance of privacy preserving and regulatory, in this paper, we propose a new regulatory anonymous transaction system based on blockchain—RATS, which can not only protect the privacy of transactions on the blockchain, but also regulate illegal transactions. Firstly, we introduce regulators into the system, and at the same time, we propose a regulatory mechanism that the regulator is allowed to trace the identity of the user without affecting the transaction operation of the system when suspicious transactions are found. In particularly, during the normal transaction, we protect the privacy of users' transaction contents and addresses without affecting the anonymity of the original system. We formalize the system model and security model of RATS. Moreover, the security of the proposed scheme is strictly proved and analyzed. © 2023 Elsevier Inc.","Anonymous; Blockchain; Digital currency system; Privacy; Regulatory"
"Fault tolerance analysis for hamming graphs with large-scale faulty links based on k-component edge-connectivity","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142743974&doi=10.1016%2fj.jpdc.2022.11.011&partnerID=40&md5=7e8d6159e2cd797ea102d247bdc81d53","The L-ary n-dimensional hamming graph KLn is one of the most attractive interconnection networks for parallel processing and computing systems. Analysis of the link fault tolerance of topology structure can provide the theoretical basis for the design and optimization of the interconnection networks. The k-component edge-connectivity cλk(G) of an interconnection network G, is the minimum number of set of faulty links, such that these malfunctions disconnect the network G with at least k connected subnetworks. It gives a more precise quantitative analysis of indicators of the robustness of a parallel distributed system in the event of faulty links. Let [Formula presented] if L is even, and [Formula presented] if L is odd. In this paper, we prove that the (k+1)-component edge-connectivity of L-ary n-dimensional hamming graphs is [Formula presented] for k≤Lt, n≥7, where exk(KLn) represents the maximum degree sum of the subgraph induced by all k processors of KLn. As the exact values of (k+1)-component edge-connectivity of KLn oscillate greatly, an efficient O(log⁡N) algorithm is designed to determine the exact values of cλk+1(KLn) for each k≤Lt and N=Ln. Our result improves those of Xu et al. [33] and Liu et al. [27]. © 2022 Elsevier Inc.","Algorithm; Hamming graph; Interconnection networks; k-component edge-connectivity; Link fault tolerance"
"RD-FCA: A resilient distributed framework for formal concept analysis","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159159760&doi=10.1016%2fj.jpdc.2023.04.011&partnerID=40&md5=d5ef7a3a6741f91e35a175650b113ea9","Formal Concept Analysis (FCA) is a data analysis technique with applications in data mining, artificial intelligence, software engineering, etc. The algorithms for FCA are computationally expensive, and their recursion tree is highly irregular and dynamic in nature. Several distributed FCA algorithms have been proposed to exploit parallelism within and across machines. However, none of the distributed approaches are able to recover from failures in the system. We propose RD-FCA, the first resilient distributed framework for FCA that uses novel load-balancing strategy, handles fail-stop failures and provides at-least-once semantics for concept discovery. Our asynchronous snapshot mechanism with incremental updates reduces the snapshot overhead and minimizes recalculation of concepts during recovery. RD-FCA also supports dynamic addition of workers to accelerate performance. Compared to MapReduce based approaches, RD-FCA performs an order of magnitude faster. We show through extensive evaluation that RD-FCA recovers efficiently from single, multiple, independent and cascading failures. © 2023 Elsevier Inc.","Distributed; Formal concept analysis; Resilient; Scalable"
"An efficient GPU-based method to compute high-order Zernike moments","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163202782&doi=10.1016%2fj.jpdc.2023.104729&partnerID=40&md5=fade07e5745433dffd1dc32d1db1ac93","The utilization of Zernike moments has been extensive in various fields, including image processing and pattern recognition, owing to their desirable characteristics. However, the application of Zernike moments is hindered by two significant obstacles: computational efficiency and accuracy. These issues become particularly noticeable when computing high-order moments. This study presents a novel GPU-based method for efficiently computing high-order Zernike moments by leveraging the computational power of the Single Instruction Multiple Data (SIMD) architecture. The experimental results demonstrate that the proposed method can compute Zernike moments up to order 500 within 0.5 seconds for an image of size 512×512. To achieve greater accuracy in Zernike moments computation, a k×k sub-region scheme was incorporated into our approach. The results show that the PSNR value of the Lena image reconstructed from 500-order Zernike moments computed using the 9×9 scheme can reach 39.20 dB. © 2023 Elsevier Inc.","Fast computation; GPU; Image reconstruction; Zernike moments"
"An arbitrable outsourcing data audit scheme supporting credit reward and punishment and multi-user sharing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153281756&doi=10.1016%2fj.jpdc.2023.04.001&partnerID=40&md5=5957175cfa83aeb949bf32b0ee72a152","In the era of the Internet of Things, cloud storage allows users to outsource their data without retaining a local copy. However, a serious problem caused by this pattern is that the data owner loses control of the data. In this case, to ensure outsourced storage data integrity, researchers proposed Provable Data Possession. However, most systems cannot deal with disagreements between entities throughout the process. Due to this, we propose an auditing system for outsourced data. By using undeniable blockchain information, it solves disputes between entities and avoids accounting delays by third-party auditors. A credit chain based on the public blockchain is designed. A novel dynamic credit calculation algorithm is adopted in the credit chain to realize an effective reward and punishment mechanism. Security proofs are provided in detail for the proposed scheme. Evaluation experiments show that compared with existing similar schemes, the scheme achieves arbitrable audit and has a lower computational cost. © 2023 Elsevier Inc.","Arbitrable audit; Cloud storage; Credit reward and punishment; Data integrity; Provable Data Possession"
"Analytical performance estimation during code generation on modern GPUs","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143722122&doi=10.1016%2fj.jpdc.2022.11.003&partnerID=40&md5=ddf955fc1aadbea8208829e399646979","Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. We propose an alternative to time-intensive autotuning, scenario-specific performance models, or black-box machine learning to select the best-performing configuration. This paper identifies the relevant performance-defining mechanisms for memory-intensive GPU applications through a performance model coupled with an analytic hardware metric estimator. This enables a quick exploration of large configuration spaces to identify highly efficient code candidates with high accuracy. We examine the changes of the A100 GPU architecture compared to the predecessor V100 and address the challenges of how to model the data transfer volumes through the new memory hierarchy. We show how our method can be coupled to the “pystencils” stencil code generator, which is used to generate kernels for a range-four 3D-25pt stencil and a complex two-phase fluid solver based on the Lattice Boltzmann Method. For both, it delivers a ranking that can be used to select the best-performing candidate. The method is not limited to stencil kernels but can be integrated into any code generator that can generate the required address expressions. © 2022 Elsevier Inc.","Analytical performance modeling; GPU; GPU performance model; Layer condition; Stencil codes"
"PARMA-CC: A family of parallel multiphase approximate cluster combining algorithms","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150234285&doi=10.1016%2fj.jpdc.2023.02.001&partnerID=40&md5=e3a30213cc0acf86e6bbbca09a78a4f2","Clustering is a common task in data analysis applications. Despite the extensive literature, the continuously increasing volumes of data produced by sensors (e.g., rates of several MB/s by 3D scanners such as LIDAR sensors), and the time-sensitivity of the applications leveraging the clustering outcomes (e.g., detecting critical situations such as detecting boundary crossing from a robot arm that could injure human beings) demand for efficient data clustering algorithms that can effectively utilize the increasing computational capacities of modern hardware. To that end, we leverage approximation and parallelization, where the former is to scale down the amount of data, and the latter is to scale up the computation. Regarding parallelization, we explore a design space for synchronization and workload distribution among the threads. As we study different parts of the design space, we propose representative Parallel Multiphase Approximate Cluster Combining, abbreviated as PARMA-CC, algorithms. We show that PARMA-CC algorithms yield equivalent clustering outcomes despite their different approaches. Furthermore, we show that certain PARMA-CC algorithms can achieve higher efficiency with respect to certain properties of the data to be clustered. Generally speaking, in PARMA-CC algorithms, parallel threads compute summaries associated with clusters of data (sub)sets. As the threads concurrently combine the summaries, they construct a comprehensive summary of the sets of clusters. By approximating a cluster with its respective geometrical summaries, PARMA-CC algorithms scale well with increasing data volumes, and, by computing and efficiently combining the summaries in parallel, they enable latency improvements. PARMA-CC algorithms utilize special data structures that enable parallelism through in-place data processing. As we show in our analysis and evaluation, PARMA-CC algorithms can complement and outperform well-established methods, with significantly better timeliness especially when utilizing multiple threads, while still providing highly accurate results in a variety of data sets, even with skewed data distributions, which cause the traditional approaches to exhibit their worst-case behaviour. © 2023 The Author(s)","Approximation; Data structures; Parallel clustering; Synchronization"
"HOTD: A holistic cross-layer time-delay attack detection framework for unmanned aerial vehicle networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150471164&doi=10.1016%2fj.jpdc.2023.03.001&partnerID=40&md5=fac52781d39ac9a57e29c9f88918e416","Recently, unmanned aerial vehicle (UAV) networks have been widely used in military and civilian scenarios; however, they suffer various attacks. Time-delay attacks maliciously delay the transmission of packets without tampering with the contents or significantly affecting the transmission pattern, making detection difficult. In this paper, a holistic cross-layer time-delay attack detection framework (HOTD) is proposed for UAV networks. A holistic selection of the delay-related features available at all layers is performed, before adopting supervised learning to build a consistency model between these features and the corresponding forwarding delay to calculate the degree of consistency of each node. Finally, the clustering method is used to distinguish malicious from benign nodes according to their degree of consistency. Experimental results show that the performance of HOTD is superior to that of state-of-the-art detection methods, and it achieves a detection accuracy higher than 85% with less than 2.5% additional overhead. © 2023 Elsevier Inc.","Clustering; Cross-layer; Supervised learning; Time-delay attack; Unmanned aerial vehicle networks"
"Interactive anomaly-based DDoS attack detection method in cloud computing environments using a third party auditor","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153241690&doi=10.1016%2fj.jpdc.2023.04.003&partnerID=40&md5=d315fe1c43cad311f0f65fe2b24e8f0e","Cloud computing environments are indispensable components of the majority of information technology organizations and users' lives. Despite multiple benefits of cloud computing environments, cloud users (CUs) as well as cloud service providers (CSPs) may experience unpleasant conditions by detrimental results of distributed denial of service (DDoS) attacks such as unavailability of cloud services or lengthy response times of the services. In this paper, we provide a threshold anomaly-based DDoS attack detection method to protect cloud environments against DDoS attack. Our proposed method is introduced to reduce DDoS attack consequences in CSPs. Our suggested method includes three newly defined components: 1. A third party auditor (TPA) which acquires direct interaction with each datacenter of the CSP, 2. A zone delimiter (ZD) which encapsulates the sensitive internal specifications of a CSP from TPA, and 3. A protocol which is defined to coordinate TPA, ZD, and CSPs for DDoS attack detection via TPA. We analyze our proposed method by determining and conducting a simulation strategy for an intrusion detection system in CSPs. Results illustrate that interactive communication between TPA and datacenters of CSPs improves the user experience of CUs in the time of DDoS attacks by reducing excessive attack filtering stages. Moreover, by using an intrusion detection system (IDS), we investigate efficiency of the proposed method to recover CSPs from DDoS attacks. We further indicate the efficiency of our proposed method by providing accuracy and qualitative comparisons with other existing methods. © 2023 Elsevier Inc.","Cloud security; DDoS attack; Interactive method; Third party auditor; Threshold-anomaly detection"
"Deep learning based data prefetching in CPU-GPU unified virtual memory","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144449438&doi=10.1016%2fj.jpdc.2022.12.004&partnerID=40&md5=52d4630dee9bd2745c1ae2f0805e8ded","Unified Virtual Memory (UVM) relieves the developers from the onus of maintaining complex data structures and explicit data migration by enabling on-demand data movement between CPU memory and GPU memory. However, on-demand paging soon becomes a performance bottleneck of UVM due to the high latency caused by page table walks and data migration over interconnect. Prefetching is considered a promising solution to this problem given its ability to leverage the locality of program memory access patterns. However, existing locality-based prefetching schemes can not handle all the situations. An ideal prefetcher should not only look at narrow regions of the requested address space but also capture global context to deliver a good prediction of the memory access pattern. This paper proposes a novel framework for page prefetching for UVM through deep learning. We first show that a powerful Transformer learning model can provide high accuracy for UVM page prefetching. We then perform analysis to interpret this Transformer model and derive several insights that allow us to design a simpler model to match the unconstrained model's accuracy with orders of magnitude lower cost. We use a pattern-based method to make the UVM page preditor general to different GPU workloads. We evaluate this framework on a set of 11 memory-intensive benchmarks from popular benchmark suites. Our solution outperforms the state-of-the-art (SOTA) UVM framework, improving the performance by 10.89%, improving the device memory page hit rate by 16.98% (89.02% vs. 76.10% for prior art), and reducing the CPU-GPU interconnect traffic by 11.05%. According to our proposed unified metric, which combines the accuracy, coverage, and page hit rate, our solution is approaching the ideal prefetching scheme more than the SOTA design (0.90 vs. 0.85, with the perfect prefetcher of 1.0). © 2022 Elsevier Inc.","Data prefetching; Deep learning; Graphics processing unit; Transformer; Unified virtual memory"
"Information-theoretic and algorithmic aspects of parallel and distributed reconstruction from pooled data","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104718","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161005833&doi=10.1016%2fj.jpdc.2023.104718&partnerID=40&md5=e1c33fe640d4468e652873be13a195a8","In the pooled data problem the goal is to efficiently reconstruct a binary signal from additive measurements. Given a signal σ∈{0,1}n, we can query multiple entries at once and get the total number of non-zero entries in the query as a result. We assume that queries are time-consuming and therefore focus on the setting where all queries are executed in parallel. First, we propose and analyze a simple and efficient greedy reconstruction algorithm. Secondly, we derive a sharp information-theoretic threshold for the minimum number of queries required to reconstruct σ with high probability. Finally, we consider two noise models: In the noisy channel model, the result for each entry of the signal flips with a certain probability. In the noisy query model, each query result is subject to random Gaussian noise. We pin down the range of error probabilities and distributions for which our algorithm reconstructs the exact initial states with high probability. Our theoretical findings are complemented by simulations where we compare our simple algorithm with approximate message passing (AMP) that is conjectured to be optimal in a number of related problems. © 2023 Elsevier Inc.","Noisy pooled data; Non-adaptive algorithm; Pooled data; Quantitative group testing"
"Performance modeling on DaVinci AI core","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149058504&doi=10.1016%2fj.jpdc.2023.01.008&partnerID=40&md5=ddbf7694de7c76d66a1d6721fa3a7d8c","The extensive use of Deep Neural Networks (DNNs) encourages people to design domain-specific hardware called Artificial Intelligence (AI) processors. The novel hardware makes optimizations challenging without a proper performance model that reveals working details and performance implications. This paper presents a performance model, Verrocchio, for Huawei DaVinci AI Core, which predicts the execution time of real-world DaVinci kernels. We propose specially-crafted micro-benchmarks to identify contention source, runtime behaviors, and bandwidth sharing, which significantly determine performance. Since DaVinci Core adopts a binary semaphore mechanism for synchronization, Verrocchio views each instruction as a discrete event and manages its execution time based on the programming logic. For evaluation, Verrocchio achieves average error rates of 2.62% and 2.30% in sample kernels for single-core and double-core execution. We demonstrate an optimizing process of matrix multiplications with Verrocchio, achieving speedups of 1.70× for operators and 1.53× for applications and error rates of 5.06% and 5.25%. © 2023 Elsevier Inc.","AI processors; Benchmarking; Performance modeling"
"Optimal placement of applications in the fog environment: A systematic literature review","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144437290&doi=10.1016%2fj.jpdc.2022.12.001&partnerID=40&md5=bebf011aec75506a7c5ea3b39eb042a6","The fog-computing paradigm complements cloud computing to support the deployment and execution of latency-sensitive applications at the network edge by offering enhanced computational power. Optimal placement of such applications over a fog network comprising geographically distributed, heterogeneous, and resource-constrained fog nodes is a core challenge in fog-computing paradigm research. This study systematically reviews existing research on optimal fog application placement over the cloud-to-thing continuum. Surveyed articles are analyzed in four aspects: i) layers of the cloud-to-thing continuum considered for placing an application; ii) application characteristics that are considered in making placement decisions; iii) application placement mechanism; iv) tools and technology for placing an application. This review also categorizes the research problems associated with fog application placement. Finally, based on this review, we suggest directions for future adaptive fog-application placement research. © 2022 Elsevier Inc.","Application placement; Fog computing; Resource management; Service placement"
"A parallel branch-and-bound algorithm with history-based domination and its application to the sequential ordering problem","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141769749&doi=10.1016%2fj.jpdc.2022.10.007&partnerID=40&md5=c334c7fd373bfbd31df72439bf57073b","In this paper, we describe the first parallel Branch-and-Bound (B&B) algorithm with a history-based domination technique. Although history-based domination substantially speeds up a B&B search, it makes parallelization much more challenging. Our algorithm is the first parallel exact algorithm for the Sequential Ordering Problem using a pure B&B approach. To effectively explore the solution space, we have developed three novel parallelization techniques: thread restart, parallel history domination, and history-table memory management. The proposed algorithm was experimentally evaluated using the SOPLIB and TSPLIB benchmarks on multi-core processors. Using 32 threads with a time limit of one hour, the algorithm gives geometric-mean speedups of 72x and 20x on the medium-difficulty SOPLIB and TSPLIB instances, respectively. On the hard instances, it solves 12 instances that the sequential algorithm does not solve, with geometric-mean speedups of 16x on SOPLIB and 32x on TSPLIB. Super-linear speedups up to 366x are seen on 16 instances. © 2022 Elsevier Inc.","Combinatorial optimization; History domination; NP-complete problems; Parallel branch-and-bound; Sequential ordering problem"
"Analysis of the analytical performance models for GPUs and extracting the underlying Pipeline model","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142194968&doi=10.1016%2fj.jpdc.2022.11.002&partnerID=40&md5=5bf7225360b521139e64807d6f94a0cc","This work presents an in-depth study of the analytical models for the performance estimation of GPUs. We show that the models' analytical equations can be derived from a pipeline analogy that models each GPU subsystem as an abstract pipeline. We call this the Pipeline model. All the equations are reformulated based on generic pipeline characteristics, namely throughput and latency. Our analysis shows equivalences between models and reveals substantial problems with some of the equations. Rather than relying on equations, the Pipeline model is then used to simulate the behavior of kernel executions based on the same hardware parameters as the analytical models. The simplicity of the model and relying on simulation mean that this approach needs less assumptions, is more comprehensive and is more flexible. More performance aspects can be taken into consideration. The different models are compared and evaluated empirically with 14 kernels of the Rodinia benchmark suite with varying occupancy. The Pipeline model gives an average MAPE of 24, while the average MAPE values of the other models lie between 27 and 136. © 2022 Elsevier Inc.","Analytical performance models; GPU architecture; GPU computing"
"Integrating batched sparse iterative solvers for the collision operator in fusion plasma simulations on GPUs","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152598942&doi=10.1016%2fj.jpdc.2023.03.012&partnerID=40&md5=433483fe3f76247e2f2206af0ad914c7","Batched linear solvers, which solve many small related but independent problems, are increasingly important for highly parallel processors such as graphics processing units (GPUs). GPUs need a substantial amount of work to keep them operating efficiently and it is not an option to solve smaller problems one-by-one. Because of the small size of each problem, the task of implementing a parallel partitioning scheme and mapping the problem to hardware is not trivial. In recent history, significant attention has been given to batched dense linear algebra. However, there is also an interest in utilizing sparse iterative solvers in a batched form. An example use case is found in a gyrokinetic Particle-In-Cell (PIC) code used for modeling magnetically confined fusion plasma devices. The collision operator has been identified as a bottleneck, and a proxy app has been created for facilitating optimizations and porting to GPUs. The current collision kernel linear solver does not run on the GPU—a major bottleneck. As these matrices are sparse and well-conditioned, batched iterative sparse solvers are an attractive option. A batched sparse iterative solver capability has recently been developed in the GINKGO library. In this paper, we describe how GINKGO's batched solver technology can integrate into the XGC collision kernel and accelerate the simulation process. Comparisons for the solve times on NVIDIA V100 and A100 GPUs and AMD MI100 GPUs with one dual-socket Intel Xeon Skylake CPU node with 40 cores are presented for matrices from the collision kernel of XGC. Further, the speedups observed for the overall collision kernel are presented in comparison to different modern CPUs on multiple supercomputer systems. The results suggest that GINKGO's batched sparse iterative solvers are well suited for efficient utilization of the GPU for this problem, and the performance portability of GINKGO in conjunction with Kokkos (used within XGC as the heterogeneous programming model) allows seamless execution on exascale-oriented heterogeneous architectures. © 2023","Batched solvers; GPU; Performance portability; Plasma simulation; Sparse linear systems"
"Distributed non-negative RESCAL with automatic model selection for exascale data","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160545323&doi=10.1016%2fj.jpdc.2023.04.010&partnerID=40&md5=19ebcfac68c391f4c0c32f0e6af79ab2","With the boom in the development of computer hardware and software, social media, IoT platforms, and communications, there has been exponential growth in the volume of data produced worldwide. Among these data, relational datasets are growing in popularity as they provide unique insights regarding the evolution of communities and their interactions. Relational datasets are naturally non-negative, sparse, and extra-large. Relational data usually contain triples (subject, relation, object) and are represented as graphs/multigraphs, called knowledge graphs, which need to be embedded into a low-dimensional dense vector space. Among various embedding models, RESCAL allows the learning of relational data to extract the posterior distributions over the latent variables and to make predictions of missing relations. However, RESCAL is computationally demanding and requires a fast and distributed implementation to analyze extra-large real-world datasets. Here we introduce a distributed non-negative RESCAL algorithm for heterogeneous CPU/GPU architectures with automatic selection of the number of latent communities (model selection), called pyDRESCALk. We demonstrate the correctness of pyDRESCALk with real-world and large synthetic tensors and the efficacy showing near-linear scaling that concurs with the theoretical complexities. Finally, pyDRESCALk determines the number of latent communities in an 11-terabyte dense and 9-exabyte sparse synthetic tensor. © 2023","High performance computing; Knowledge graphs; Latent communities; Non-negative RESCAL; Relational data"
"A real-time and ACO-based offloading algorithm in edge computing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159168647&doi=10.1016%2fj.jpdc.2023.04.004&partnerID=40&md5=f77af4610ed450b60b71d1e2c410b111","With the increasingly widespread use of networks and end devices, more and more data and computations must be processed. With processing constrained by the limited resources of the end device, edge computing plays an important role. Edge computing offloads computation to surrounding edge nodes with corresponding computing capabilities so that the end device can get a response within a reasonable latency to meet the user's needs. Since these edge nodes are composed of multiple heterogeneous computing units, any system's task-offloading strategy must necessarily affect the system's load balance and execution time. This study proposes a real-time, two-stage ant colony algorithm (RTACO) with the following goals: 1) the algorithm requires low latency; 2) the algorithm minimizes the makespan of all tasks; 3) the algorithm optimizes the system load and reduces the burden of the task-offloading algorithm, thereby providing a stable and high-performance edge computing system. Experiments show that RTACO requires low execution time, and can still effectively achieve good results even when the system has limited resources. © 2023 Elsevier Inc.","Edge computing; Task offloading"
"LOCATOR: Low-power ORB accelerator for autonomous cars","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144416540&doi=10.1016%2fj.jpdc.2022.12.005&partnerID=40&md5=2e8c1072ceb9161fc2d0481160f14d9c","Simultaneous Localization And Mapping (SLAM) is crucial for autonomous navigation. ORB-SLAM is a state-of-the-art Visual SLAM system based on cameras used for self-driving cars. In this paper, we propose a high-performance, energy-efficient, and functionally accurate hardware accelerator for ORB-SLAM, focusing on its most time-consuming stage: Oriented FAST and Rotated BRIEF (ORB) feature extraction. The Rotated BRIEF (rBRIEF) descriptor generation is the main bottleneck in ORB computation, as it exhibits highly irregular access patterns to local on-chip memories causing a high-performance penalty due to bank conflicts. We introduce a technique to find an optimal static pattern to perform parallel accesses to banks based on a genetic algorithm. Furthermore, we propose the combination of an rBRIEF pixel duplication cache, selective ports replication, and pipelining to reduce latency without compromising cost. The accelerator achieves a reduction in energy consumption of 14597× and 9609×, with respect to high-end CPU and GPU platforms, respectively. © 2022 The Authors","Hardware accelerator; ORB; ORB-SLAM"
"CSMV: A highly scalable multi-versioned software transactional memory for GPUs","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159128146&doi=10.1016%2fj.jpdc.2023.04.002&partnerID=40&md5=4fda23483ac3f91b35528404947eb2cc","This paper introduces CSMV (Client Server Multiversioned), a multi-versioned Software TM (STM) for GPUs that adopts an innovative client-server design. By decoupling the execution of transactions from their commit process, CSMV provides two main benefits: (i) it enables the use of fast on chip memory to access the global metadata used to synchronize transaction (ii) it allows for implementing highly efficient collaborative commit procedures, tailored to take full advantage of the architectural characteristics of GPUs. Via an extensive experimental study, we show that CSMV achieves up to 3 orders of magnitude speed-ups with respect to state of the art STMs for GPUs and that it can accelerate by up to 20× irregular applications running on state of the art STMs for CPUs. © 2023 Elsevier Inc.","Concurrent programming; GPU; Multi-version concurrency control; Synchronization; Transactional memory"
"Fargraph+: Excavating the parallelism of graph processing workload on RDMA-based far memory system","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150897992&doi=10.1016%2fj.jpdc.2023.02.015&partnerID=40&md5=eb353f3288cb3292f31a9542dbb08a39","Disaggregated architecture brings new opportunities to memory-consuming applications like graph processing. It allows one to outspread memory access pressure from local to far memory, providing an attractive alternative to disk-based processing. Although existing works on general-purpose far memory platforms show great potentials for application expansion, it is unclear how graph processing applications could benefit from disaggregated architecture, and how different optimization methods influence the overall performance. In this paper, we take the first step to analyze the impact of graph processing workload on disaggregated architecture by extending the GridGraph framework on top of the RDMA-based far memory system. We propose Fargraph+, a system with parallel graph data offloading and far memory coordination strategy for enhancing efficiency of graph processing workload on RDMA-based far memory architecture. Specifically, Fargraph+ reduces the overall data movement through a well-crafted, graph-aware data segment offloading mechanism. In addition, we use optimal data segment splitting and asynchronous data buffering to achieve graph iteration-friendly far memory access. We further configure efficient parallelism-oriented control to accelerate performance of multi-threading processing on graph iterations while improving memory efficiency of far memory access by utilizing RDMA queue features. We show that Fargraph+ achieves near-oracle performance for typical in-local-memory graph processing systems. Fargraph+ shows up to 11.2× speedup compared to Fastswap, the state-of-the-art, general-purpose far memory platform. © 2023 Elsevier Inc.","Far memory; Graph processing; RDMA"
"A parallel deep learning-based code clone detection model","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.104747","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169562667&doi=10.1016%2fj.jpdc.2023.104747&partnerID=40&md5=4c04ade09ad4864e020dcea97071d298","Code clone detection is a crucial task in software development and maintenance. While deep learning-based methods have been proposed to tackle this problem, most of them neglect the time and memory consumption issues which can be significant when working with limited computational resources. Given the inability of recurrent neural networks to train in a parallel manner, this paper presents a parallel code clone detection model based on temporal convolutional networks. The proposed method splits the corresponding abstract syntax tree into a set of code statement sequences, utilizes a temporal convolutional neural network to generate representations containing complexity features found in the source code, and finally measures the distance between these representations. The proposed method is evaluated on a real-world dataset for code clone detection, and the experimental results demonstrate that it performs comparably to state-of-the-art methods while requiring significantly less time and memory costs. © 2023 Elsevier Inc.","Abstract syntax tree; Code clone detection; Code representation; Temporal convolutional network"
"A two-phase heuristic algorithm for power-aware offline scheduling in IaaS clouds","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151677563&doi=10.1016%2fj.jpdc.2023.03.006&partnerID=40&md5=a4e056ba609df931b678ab5993a30bd7","The paper aims at mitigating hot-spots during Offline Scheduling in IaaS (Infrastructure-as-a-Service) cloud systems. Unlike previous studies, the research focuses on identifying and resolving hot-spots not at servers, but at server racks. A two-phase algorithm for performing power-aware offline scheduling is proposed. The first phase aims at identifying and mitigating hot-spots at racks, while the second phase performs VM consolidation, i.e. minimization of the number of occupied servers while maintaining a feasible VM mapping and low migration costs. The proposed algorithm takes into account the dynamic nature of VM's resource consumption: it does not only resolve detected hot-spots, but also tries to avoid hot-spots in a reasonable future time period. The algorithm was tested with the data from a real IaaS cloud with different sets of algorithm's parameters. Experimental evaluation showed that the statistical estimates of the future VM's resource consumption provide the most reliable mapping, which is a result of minimization of the number of new hot-spot occurrences. © 2023 Elsevier Inc.","Cloud power constraints; IaaS; Power-aware offline scheduling"
"Research on resource allocation technology in highly trusted environment of edge computing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152128555&doi=10.1016%2fj.jpdc.2023.03.011&partnerID=40&md5=e547b3645db7b04830665c15e1c87e42","Edge computing can use many edge devices to provide users with real-time computing and storage functions. With the development of the Internet of Things (IOT), edge computing is becoming more and more prevalent currently. However, the consequent challenges in search efficiency, reliability requirements and resource allocation appear followed. Therefore, this article focuses on resource allocation and security performance issues. A lightweight trust evaluation mechanism was constructed and time-varying trust coefficients were introduced as incentives to address the problem of distrust between user terminals and edge server entities in multi-cell and multi-user scenarios. This enables the user terminal to immediately distinguish malicious servers. Considering the limited and dynamic changes of computing resources, the problem of complete migration of multi-user tasks was transformed into an issue of computing resource distribution to reduce the total system energy consumption. As a Markov game model, a system was developed to address the problems of centralized single-agent algorithms, including the explosion of action space and difficulty in convergence with increasing the number of users. Besides, a resource allocation algorithm was proposed based on a trust model and multi-agents that follows a centralized training and distributed implementation architecture. The simulated consequences indicated that the proposed algorithm resists malicious attacks, and can quickly make reasonable task migration decisions based on different system states, thereby efficiently decreasing the consumption of the total system energy, and providing a better user experience. © 2023 Elsevier Inc.","Edge computing; Resource allocation; Task offloading; Trust model"
"Locally solvable tasks and the limitations of valency arguments","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2023.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148690614&doi=10.1016%2fj.jpdc.2023.02.002&partnerID=40&md5=5f8724b11f6a34df940866c6ad2c8520","An elegant strategy for proving impossibility results in distributed computing was introduced in the celebrated FLP consensus impossibility proof. This strategy is local in nature as at each stage, one configuration of a hypothetical protocol for consensus is considered, together with future decisions of possible extensions. This proof strategy has been used in numerous situations related to consensus, leading one to wonder why it has not been used in impossibility results of two other well-known tasks: set agreement and renaming. This paper provides an explanation of why impossibility proofs of these tasks have been of a global nature. It shows that a protocol can always solve such tasks locally, in the following sense. Given a configuration and all its future decisions, if a single successor configuration is selected, then the protocol can reveal all decisions in this branch of executions, satisfying the task specification. This result is shown for both set agreement and renaming, providing evidence that there are no local impossibility proofs for these tasks. © 2023 Elsevier Inc.","Impossibility proofs; Renaming; Set agreement; Wait-freedom; Weak symmetry breaking"
"A general approach for supporting nonblocking data structures on distributed-memory systems","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142372637&doi=10.1016%2fj.jpdc.2022.11.006&partnerID=40&md5=934c7bc359beff05875e75a5913029d9","Nonblocking data structures are an essential part of many parallel applications in that they can help to improve fault tolerance and performance. Although there are scores of nonblocking data structures, such as stacks, queues, double-ended queues (deques), lists, widely used in practice, most of them are designed to be used on shared-memory machines only, and cannot be used in a distributed-memory setting. Several recent studies focus on the development of novel tailor-made nonblocking distributed data structures and omit the potential for adapting a great wealth of existing nonblocking shared-memory ones for the distributed-memory case. Hence, we propose a general approach for bridging the gap between most existing nonblocking data structures and distributed-memory machines in this work. Several challenges, such as safe memory reclamation and solving the ABA problem, must be overcome. To address these issues, we present a global memory management scheme. The scheme takes advantage of hazard pointers which are widely used to tackle the problems in shared-memory environments. To demonstrate our general approach, we take stacks as a typical example of nonblocking data structures. This work also provides a survey of well-known nonblocking stack algorithms along with our analysis and evaluation in distributed-memory environments. Moreover, this paper depicts how to improve performance of a stack algorithm by making use of node locality. Besides, a cost model based on worst cases is devised to help gain a better understanding into experimental results of nonblocking distributed data structures, along with our analysis of the influence of two popular lock-free programming patterns on performance. © 2022 Elsevier Inc.","ABA problem; Distributed-memory systems; Global memory reclamation; Nonblocking data structures; Nonblocking stacks"
"Time-varying dual accelerated gradient ascent: A fast network optimization algorithm","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128232173&doi=10.1016%2fj.jpdc.2022.03.014&partnerID=40&md5=3d99d498af667b795d301ee807e1c7a6","We propose a time-varying dual accelerated gradient method for minimizing the average of n strongly convex and smooth functions over a time-varying network with n nodes. We prove that the time-varying dual accelerated gradient ascent method converges at an R-linear rate with the time to reach an ϵ-neighborhood of the solution being of O([Formula presented]ln⁡[Formula presented]), where c is a constant depending on the graph and objective function parameters and M is a constant depending on the initial values. We test the proposed method on two classes of problems: L2-regularized least squares and logistic classification problems. For each class, we generate 1000 problems and use the Dolan-Moré performance profiles to compare our obtained results with the ones obtained by several state-of-the-art algorithms to illustrate the efficiency of our method. © 2022 Elsevier Inc.","Distributed convex optimization; Time-varying network optimization"
"Online computation offloading with double reinforcement learning algorithm in mobile edge computing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138162817&doi=10.1016%2fj.jpdc.2022.09.006&partnerID=40&md5=07dddb4b0a50e211f964dd07b3cebd23","Smart mobile devices have recently emerged as a promising computing platform for computation tasks. However, the task performance is restricted by the computing power and battery capacity of mobile devices. Mobile edge computing, an extension of cloud computing, solves this problem well by providing computational support to mobile devices. In this paper, we discuss a mobile edge computing system with a server and multiple mobile devices that need to perform computation tasks with priorities. The limited resources of the mobile edge computing server and mobile device make it challenging to develop an offloading strategy to minimize both delay and energy consumption in the long term. To this end, an online algorithm is proposed, namely, the double reinforcement learning computation offloading (DRLCO) algorithm, which jointly decides the offloading decision, the CPU frequency, and transmit power for computation offloading. Concretely, we first formulate the power scheduling problem for mobile users to minimize energy consumption. Inspired by reinforcement learning, we solve the problem by presenting a power scheduling algorithm based on the deep deterministic policy gradient (DDPG). Then, we model the task offloading problem to minimize the delay of tasks and propose a double Deep Q-networks (DQN) based algorithm. In the decision-making process, we fully consider the influence of task queue information, channel state information, and task information. Moreover, we propose an adaptive prioritized experience replay algorithm to improve the model training efficiency. We conduct extensive simulations to verify the effectiveness of the scheme, and the simulation results show that compared with the conventional schemes, our method reduces the delay by 48% and the energy consumption by 53%. © 2022 Elsevier Inc.","Computation offloading; Deep deterministic policy gradient; Double Deep Q-Networks; Mobile edge computing; Power control"
"Subversion analyses of hierarchical networks based on (edge) neighbor connectivity","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138478066&doi=10.1016%2fj.jpdc.2022.09.010&partnerID=40&md5=5785786cd667a6040acb409a090e121e","The notion of neighbor connectivity was derived from the assessment of subversion in spy networks caused by the underground resistance movement. For a network G, the neighbor connectivity κNB(G) (resp. edge neighbor connectivity λNB(G)) is defined as the least number of vertices (resp. edges) such that if we remove the closed neighborhoods of them, the network will become disconnected, empty, or complete (resp. trivial). The two connectivities can also provide more accurate measures regarding the reliability and fault-tolerance of networks. Star graphs Sn and hypercubes Qn are the two most famous structures in the family of Cayley graphs. They are widely studied in the research of developing multiprocessor systems. In this paper, we investigate the neighbor connectivity and edge neighbor connectivity of two kinds of hierarchical networks, called hierarchical star network HSn and complete cubic network CCn, which take Sn and Qn as building blocks, respectively. Specifically, we obtain the following results: κNB(HSn)=n−1 and λNB(HSn)=n for n≥3, and κNB(CCn)=⌈[Formula presented]⌉+1 and λNB(CCn)=n+1 for n≥2. In addition, to further determine the distribution of the number of subverted vertices and their occurrence status, we also carry out experiments to simulate the subversion at the vertices on these networks. © 2022 Elsevier Inc.","(Edge) neighbor connectivity; Complete cubic networks; Graph subversion; Hierarchical cubic networks; Hierarchical star networks"
"A novel blockchain-based and proxy-oriented public audit scheme for low performance terminal devices","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133463749&doi=10.1016%2fj.jpdc.2022.06.002&partnerID=40&md5=9b302e4965d30afacea35a6e64e3746f","With the rapid development of cloud computing technology, more and more individuals/organizations are inclined to store their data in cloud server (CS). Cloud storage audit schemes can help data owner (DO) to confirm the integrity of their data, but the existing schemes still have some limitations. On one hand, the existing schemes assume that DO's terminal devices are computationally powerful enough to handle various operations in a timely manner. While in practice, the terminal devices probably are mobile phones, tablets and other devices with low computing power. On the other hand, the existing schemes rely on a fully trusted third-party auditor (TPA), but it is not in line with real application scenarios. Therefore, in this paper, we proposed a novel blockchain-based and proxy-oriented public audit (BBPO-PA) scheme for low performance terminal devices. Firstly, we introduced a trusted proxy authorized by DO, which can process and upload DO's encrypted files. Secondly, we applied blockchain in our scheme and utilized smart contracts instead of untrusted TPA to improve the reliability and stability of audit results. Thirdly, we took advantage of an index table to ensure that our scheme can support dynamic data operation. Finally, security analysis revealed that our scheme is provably secure in random oracle model. Meanwhile, performance analysis demonstrated that our scheme is efficient and especially suitable for low performance terminal devices. © 2022 Elsevier Inc.","Blockchain; Low performance terminal device; Proxy; Public audit scheme; Smart contract"
"Simulation-based optimization and sensibility analysis of MPI applications: Variability matters","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129325332&doi=10.1016%2fj.jpdc.2022.04.002&partnerID=40&md5=b1f26f4f209159ee0fd39455dfe399f5","Finely tuning MPI applications and understanding the influence of key parameters (number of processes, granularity, collective operation algorithms, virtual topology, and process placement) is critical to obtain good performance on supercomputers. With the high consumption of running applications at scale, doing so solely to optimize their performance is particularly costly. Having inexpensive but faithful predictions of expected performance could be a great help for researchers and system administrators. The methodology we propose decouples the complexity of the platform, which is captured through statistical models of the performance of its main components (MPI communications, BLAS operations), from the complexity of adaptive applications by emulating the application and skipping regular non-MPI parts of the code. We demonstrate the capability of our method with High-Performance Linpack (HPL), the benchmark used to rank supercomputers in the TOP500, which requires careful tuning. We briefly present (1) how the open-source version of HPL can be slightly modified to allow a fast emulation on a single commodity server at the scale of a supercomputer. Then we present (2) an extensive (in)validation study that compares simulation with real experiments and demonstrates our ability to predict the performance of HPL within a few percent consistently. This study allows us to identify the main modeling pitfalls (e.g., spatial and temporal node variability or network heterogeneity and irregular behavior) that need to be considered. Last, we show (3) how our “surrogate” allows studying several subtle HPL parameter optimization problems while accounting for uncertainty on the platform. © 2022 Elsevier Inc.","HPL; Sensibility analysis; SimGrid; Simulation; Validation"
"A novel flow-vector generation approach for malicious traffic detection","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133369172&doi=10.1016%2fj.jpdc.2022.06.004&partnerID=40&md5=0f61a42548dd81e1e73fb7461ee75801","Malicious traffic detection is one of the most important parts of cyber security. The approaches of using the flow as the detection object are recognized as effective. Benefiting from the development of deep learning techniques, raw traffic can be directly used as a feature to detect malicious traffic. Most existing work usually converts raw traffic into images or long sequences to express a flow and then uses deep learning technology to extract features and classify them, but the generated features contain much redundant or even useless information, especially for encrypted traffic. The packet header field contains most of the packet characteristics except the payload content, and it is also an important element of the flow. In this paper, we only use the fields of the packet header in the raw traffic to construct the characteristic representation of the traffic and propose a novel flow-vector generation approach for malicious traffic detection. The preprocessed header fields are embedded as field vectors, and then a two-layer attention network is used to progressively generate the packet vectors and the flow vector containing context information. The flow vector is regarded as the abstraction of the raw traffic and is used to classify. The experiment results illustrate that the accuracy rate can reach up to 99.48% in the binary classification task and the average of AUC-ROC can reach 0.9988 in the multi-classification task. © 2022 Elsevier Inc.","Attention mechanism; Deep learning; Embedding; Malicious traffic"
"Understanding the impact on convolutional neural networks with different model scales in AIoT domain","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136007462&doi=10.1016%2fj.jpdc.2022.07.011&partnerID=40&md5=173b631c82688a0f919af63015f86a70","In recent years many amazing deep learning models have been developed, but in the process of practical applications, people often find that these deep learning models have high requirements for hardware storage space and computing power. In Artificial Intelligent of Things (AIoT) scenario, the computing power of the edge or terminal side are relatively limited, therefore, most conventional deep learning models are difficult to be deployed into AIoT devices. It is significant to explore the different performance under different scales of deep learning models. In this paper, we mainly propose a method to analyze the impact of deep learning models with various sizes through various experiments. We employ slimmable network as a Neural Archtecture Search (NAS) tool to realize various model size freely, and evaluate them on the indicators of flops, robustness and accuracy. The experimental results show the variation of flops, robustness and accuracy with the various model sizes, which help understand the impact on performance of deep learning models with different scales in AIoT systems. © 2022 Elsevier Inc.","Adversarial examples; Adversarial training; AIoT; Keyword robust deep learning"
"Process mapping on any topology with TOPOMATCH","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138444322&doi=10.1016%2fj.jpdc.2022.08.002&partnerID=40&md5=c57d9bf7e95b10648bd6e7c4ed6b6e3c","Process mapping (or process placement) is a useful algorithmic technique to optimize the way applications are launched and executed onto a parallel machine. By taking into account the topology of the machine and the affinity between the processes, process mapping helps reducing the communication time of the whole parallel application. Here, we present TOPOMATCH, a generic and versatile library and algorithm to address the process placement problem. We describe its features and characteristics, and we report different use-cases that benefit from this tool. We also study the impact of different factors: sparsity of the input affinity matrix, trade-off between the speed and the quality of the mapping procedure as well as the impact of the uncertainty (noise) onto the input. © 2022 Elsevier Inc.","Adaptative algorithm; Noise analysis; Process mapping"
"Optimal load balancing and assessment of existing load balancing criteria","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134821304&doi=10.1016%2fj.jpdc.2022.07.002&partnerID=40&md5=e44c79552e43932f18a33e3213a84af1","Parallel iterative applications often suffer from load imbalance, one of the most critical performance degradation factors. Hence, load balancing techniques are used to distribute the workload evenly to maximize performance. A key challenge is to know when to use load balancing techniques. In general, this is done through load balancing criteria, which trigger load balancing based on runtime application data and/or user-defined information. In the first part of this paper, we introduce a novel, automatic load balancing criterion derived from a simple mathematical model. In the second part, we propose a branch-and-bound algorithm to find the load balancing iterations that lead to the optimal application performance. This algorithm finds the optimal load balancing scenario in polynomial time while, to the best of our knowledge, it has never been addressed in less than an exponential time. Finally, we compare the performance of the scenarios produced by state-of-the-art load balancing criteria relative to the optimal load balancing scenario in synthetic benchmarks and parallel N-body simulations. In the synthetic benchmarks, we observe that the proposed criterion outperforms the other automatic criteria. In the numerical experiments, we show that our new criterion is, on average, 4.9% faster than state-of-the-art load balancing criteria and can outperform them by up to 17.6%. Moreover, we see in the numerical study that the state-of-the-art automatic criteria are at worst 26.43% slower than the optimum and at best 10% slower. © 2022 The Author(s)","Dynamic load balancing; High performance computing; Load balancing criteria; Parallel computing; Performance optimization"
"Evaluation of Intel's DPC++ Compatibility Tool in heterogeneous computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127799762&doi=10.1016%2fj.jpdc.2022.03.017&partnerID=40&md5=99343a404ebc63f9cdc588377f40aab6","The Intel DPC++ Compatibility Tool is a component of the Intel oneAPI Base Toolkit. This tool automatically transforms CUDA code into Data Parallel C++ (DPC++), thus assisting in the migration process. DPC++ is an implementation of the programming standard for heterogeneous computing known as SYCL, which unifies the development of parallel applications on CPUs, GPUs or even FPGAs. This paper analyzes the DPC++ Compatibility Tool by considering the manual intervention required and the problems encountered while migrating the Rodinia benchmarks. For this suite, this tool achieves an impressive rate of almost 87% for code successfully migrated. Moreover, a comparative study of the performance obtained by the migrated code was carried out, showing a moderate overhead in most of the migrated examples. Finally, a performance comparison on different devices was also performed. © 2022 The Author(s)","CUDA; DPC++; Intel DPCT; oneAPI; Rodinia"
"An energy and carbon-aware algorithm for renewable energy usage maximization in distributed cloud data centers","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128693998&doi=10.1016%2fj.jpdc.2022.04.001&partnerID=40&md5=47f7ef45897fb471d59c7ee67c9f827e","The vigorous development and the increasing popularity of cloud computing highlight the necessity of reducing data center energy consumption and the environmental impact of carbon dioxide emissions. For geographically distributed data centers, cloud servers are connected to the conventional power grid and in addition they are supported by an attached renewable energy source. Since the carbon footprint rate of energy consumption has dynamic differences in space, reducing energy consumption does not mean decrease carbon emission, which indicates that energy consumption and carbon footprint need to be synergistically optimized. In this paper, an energy and carbon-aware algorithm for virtual machine placement is proposed. The goal is to obtain a virtual machine allocation scheme that aims to achieve the trade-off between energy consumption and carbon emissions by improving renewable energy utilization. The experimental results show that the proposed approach is more energy-efficient and greener, which can also maximize the renewable energy utilization with 73.11% while ensuring the SLA violation with 0.2% in comparison to the baseline algorithms. © 2022 The Author(s)","Carbon emission; Energy consumption; Renewable energy; Virtual machine placement"
"A job scheduling algorithm based on parallel workload prediction on computational grid","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138796124&doi=10.1016%2fj.jpdc.2022.09.007&partnerID=40&md5=ddc9d419432a12947dd604aa481b6216","Generally, the computational grid consists of a large number of computing nodes, some of them are idle due to the uneven geographical distribution of computing requirements. This may cause workload unbalancing problems, which affect the performance of large-scale computational grids. In order to balance the computing requirements and computing nodes, we propose a job scheduling algorithm based on the workload prediction of computing nodes. We first analyze the causes of workload imbalance and the feasibility of reallocating computing resources. Secondly, we design an application and workload-aware scheduling algorithm (AWAS) by combining the previously designed workload prediction model. To reduce the complexity of the AWAS algorithm, we propose a parallel job scheduling method based on computing node workload prediction. The experiments show that the AWAS algorithm can balance the workload among different computing nodes on the real-world dataset. In addition, we propose the parallelism of workload prediction model from the perspective of internal structure and data set to make AWAS apply to more computing nodes of the large-scale computing grids. Experimental results show that the combination of the two can achieve satisfactory acceleration efficiency. © 2022 Elsevier Inc.","Computational grid; Job scheduling; Neural network; Parallel; Workload balancing"
"Dynamic resource provisioning for service-based cloud applications: A Bayesian learning approach","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132741265&doi=10.1016%2fj.jpdc.2022.06.001&partnerID=40&md5=d82df4b3e40afcc07d7d810c3200baed","Deciding the correct extent of resources needed to run the various cloud services is always a challenge. Often in such dynamic environments, there is a tremendous need for accurate predictions and timely decision making methodologies to estimate the future demands within a minimal cost. This brings in a need to elucidate the research divergence for optimal dynamic resource provisioning that predicts the future enumerated resources on the support of application's type. This paper proposes a framework to provision the resources in an optimal way, by combining the concepts of autonomic computing, linear regression and Bayesian learning. The use of Bayesian learning to the proposed model helps in a proactive decision making process and provide a solid theoretical framework to estimate the future predictions using the prior information available. The autonomic resource provisioning framework proposed here is developed using CloudSim toolkit inspired by a cloud layer model. The efficacy of the proposed technique is evaluated using real world workload traces from google followed by the traces from Clarknet. The model is evaluated for various parameters namely – response time, SLA violations, virtual machine usage hours and cost. It is found that the proposed model lowers the overall cost by 31% with the increase in the usage of resources by 12% when compared with the other existing approaches. © 2022 Elsevier Inc.","Autonomic computing; Bayesian learning; Resource allocation; Resource provisioning; Virtual machines"
"An edge intelligence empowered flooding process prediction using Internet of things in smart city","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127367401&doi=10.1016%2fj.jpdc.2022.03.010&partnerID=40&md5=9367616349bbc7f586351a1b081d175a","Floods result in substantial damage throughout the world every year. Accurate predictions of floods can significantly alleviate casualties and property losses. However, due to the complexity of hydrology process especially in a city with complicated pipe network, the accuracy of traditional flood forecasting models suffer from the performance degradation with the increasing of required prediction period. In the work, based on the collected historical data of Xixian City, Henan Province, China, using the Internet of Things system (IoT) in 2011-2018, a Bidirectional Gated Recurrent Unit (BiGRU) multi-step flood prediction model with attention mechanism is proposed. In our model, the attention mechanism is used to automatically adjust the matching degree between the input features and output. Besides, we use a bidirectional GRU model, which can process the input sequence from two directions of time series (chronologically and antichronologically), then merge their representations together. Compared with the prediction model using Long Short Term Memory (LSTM), our method can generate better prediction result, as can be seen from the arrival time error and peak error of floods during multi-step predictions. © 2022 Elsevier Inc.","Attention mechanism; Flood forecasting; GRU neural network; Internet of things; Smart city"
"gVMP: A multi-objective joint VM and vGPU placement heuristic for API remoting-based GPU virtualization and disaggregation in cloud data centers","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141437823&doi=10.1016%2fj.jpdc.2022.10.008&partnerID=40&md5=50a9832fd50c951b474b4e33bf8ce7ec","The diverse needs of customers drive cloud providers to incorporate more GPU-enabled services. It is known that users barely utilize GPUs. Hence, GPU virtualization techniques are employed to enable GPU sharing and increase resource utilization. Among GPU virtualization techniques, API remoting leverages disaggregation for GPU provisioning, allowing GPUs to be accessed remotely by non-resident VMs. It enables a VM to access any available GPU in the data center which, in turn, provides more flexibility to reach a better placement. It also enables users to choose VM and vGPU instances separately based on computational needs as GPU-enabled VM instances with fixed configurations may not proportionally utilize allocated resources. However, an inefficient VM or vGPU placement may result in poor performance and increase in energy consumption and rejection ratio. There is also a network communication overhead for remotely allocated vGPUs. The main challenge is how to place VMs and vGPUs such that rejection ratio is decreased and the negative performance impacts caused by GPU sharing and remote access are kept at a minimum. In this work, we formally define the joint problem of VM and vGPU placement based on API remoting as a multi-objective ILP model and introduce gVMP as a heuristic to solve it. The proposed method is compared against state-of-the-art heuristics and commercial solutions that are based on API remoting and GRID technologies for the placement of GPU-enabled VMs. We study the effectiveness of the proposed gVMP algorithm for low and high arrival rates in slow and fast networks. Our results show that under different scenarios and in comparison to state-of-the-art policies, our heuristic improves average request duration, efficiency, and SLA by up to 40%, 25% and 29%, respectively. © 2022 Elsevier Inc.","API remoting; Cloud computing; GPU disaggregation; Multi-objective optimization; VM placement"
"Challenging the security of “A PUF-based hardware mutual authentication protocol”","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134815265&doi=10.1016%2fj.jpdc.2022.06.018&partnerID=40&md5=4b1beea714d348b858ed58fd42af443e","Recently, using Physical Unclonable Functions (PUF) to design lightweight authentication protocols for constrained environments such as the Internet of Things (IoT) has received much attention. In this direction, Barbareschi et al. recently proposed PHEMAP in Journal of Parallel and Distributed Computing, a PUF based mutual authentication protocol. Also, they extended it to the later designed Salted PHEMAP, for low-cost cloud-edge (CE) IoT devices. This paper presents the first third-party security analysis of PHEMAP and Salted PHEMAP to the best of our knowledge. Despite the designer's claim, we show that these protocols are vulnerable to impersonation, de-synchronization, and traceability attacks. The success probability of the proposed attacks is ‘1’, while the complexity is negligible. In addition, we introduce two enhanced lightweight authentication protocols based on PUF chains (called PBAP and Salted PBAP), using the same design principles as PHEMAP and Salted PHEMAP. With the performance evaluation and the security analysis, it is justified that the two proposed schemes are practically well suited for use in resource-constrained IoT environments. © 2022 Elsevier Inc.","Authentication; IoT; PHEMAP; PUF; Security analysis"
"ESDU: An elastic stripe-based delta update method for erasure-coded cross-data center storage systems","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130554086&doi=10.1016%2fj.jpdc.2022.05.003&partnerID=40&md5=1c529bbfb4b42066f0ae0c03b48533ea","The erasure-coded cross-data center storage system can achieve high disaster tolerance and low redundancy. But as it has large cross-data center update traffic, its data update time is long. In erasure-coded storage systems, each data object is sequentially divided into several stripes, and each stripe consists of several data packets. When erasure-coded stripes do not undergo insertion or deletion, existing work can effectively reduce cross-data center update traffic by performing delta updates—delta update methods can update the old stripe without transferring matched packets that are new and old stripes' duplicate packets with the same offset-within-stripe. However, because existing delta update methods' stripe size is fixed, when a stripe undergoes insertion or deletion, its subsequent stripes' duplicate packets' offset-within-stripe will change. In this scenery, the matched packet number is small, resulting in large cross-data center update traffic. This paper proposes an elastic stripe-based delta update method for erasure-coded cross-data center storage systems (ESDU). Under insertion or deletion, ESDS tries to avoid duplicate packets' offset-within-stripe changing (i.e., maximizing matched packets) by adjusting the stripes' size flexibly according to the duplicate packet locating result. So, it can reduce cross-data center traffic. Moreover, ESDU can optimize stripes' update topology based on the location information of storage nodes to reduce cross-data center update traffic further. In addition, we implement an erasure-coded cross-data center storage system adopting ESDU, called ECESD. Experiments with the workloads derived from EduCoder's real-world trace show that compared with the existing erasure-coded cross-data center storage system adopting the fixed stripe-based delta update method, ECESD reduces average update time by 89.6%. Moreover, compared with a replication-based storage system with a delta update method (HadoopRsync), ECESD achieves an 8.3% shorter update time and much smaller redundancy. © 2022 Elsevier Inc.","Cross-data center storage; Delta update method; Disaster tolerance; Distributed storage system; Erasure code"
"An intelligence energy consumption model based on BP neural network in mobile edge computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130531143&doi=10.1016%2fj.jpdc.2022.05.005&partnerID=40&md5=63320321805954e00f4218b120bec9a9","Establishing an accurate edge server power model is helpful for resource providers to predict and optimize power consumption within edge data centers. Considering the fact that the accuracy of the previous energy consumption model is easily affected by the workload types, this paper develops an edge server power model based on BP (back propagation) neural network and feature selection, which is denoted by DSBF. For different task types, DSBF leverages “principal component analysis (PCA)” to analyze the contribution of each energy consumption parameter and selects “representative parameter”, and then builds a power model based on BP neural network. In contrast to other power models, DSBF can effectively handle the variable workload. To measure the effectiveness of the DSBF model, a series of experiments were conducted. The results suggest that compared with other energy consumption models, DSBF can better adapt to the changing workload and has advantages in predicting the accuracy of the energy consumption model. © 2022 Elsevier Inc.","BP neural network; Edge server; Energy consumption model; Power modeling; Principal component analysis (PCA)"
"Energy-latency tradeoffs for edge caching and dynamic service migration based on DQN in mobile edge computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129296782&doi=10.1016%2fj.jpdc.2022.03.001&partnerID=40&md5=77170cdae63c5525ae24f7d751a2eaca","Mobile edge computing sinks computing and storage capabilities to the edge of the network to provide reliable and low-latency services. However, the mobility of users and the limited coverage of edge servers can cause service interruptions and reduce service quality. A cooperative edge caching strategy based on energy-latency balance is proposed to solve high power consumption and latency caused by processing computationally intensive applications. In the cache selection phase, the request prediction method based on a deep neural network improves the cache hit rate. In the cache placement stage, the objective function is established by comprehensively considering power consumption and latency, and We use the branch-and-bound algorithm to get the optimal value. We propose an improved service migration method to solve the problem of service interruption caused by user movement. The service migration problem is modeled using a Markov decision process (MDP). The optimization goal is to reduce service latency and improve user experience under the premise of specified cost and computing resources. Finally, the optimal solution of the model is solved by the deep Q-Network (DQN) algorithm. Experiments show that our edge caching algorithm has lower latency and energy consumption than other algorithms in the same conditions. The service migration algorithm proposed in this paper is superior to different service migration algorithms in migration cost and success rate. © 2022 Elsevier Inc.","Deep Q-Network; Dynamic service migration; Edge caching; Energy-latency tradeoffs"
"Targeting a light-weight and multi-channel approach for distributed stream processing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129759423&doi=10.1016%2fj.jpdc.2022.04.022&partnerID=40&md5=9ae24264e60769c649a1f99bad3d43ab","Processing high-throughput data-streams has become a major challenge in areas such as real-time event monitoring, complex dataflow processing, and big data analytics. While there has been tremendous progress in distributed stream processing systems in the past few years, the high-throughput and low-latency (a.k.a. high sustainable-throughput) requirement of modern applications is pushing the limits of traditional data processing infrastructures. This paper introduces a new distributed stream processing engine (DSPE), called Asynchronous Iterative Routing (or simply “AIR”), which implements a light-weight, dynamic sharding protocol. AIR expedites direct and asynchronous communication among all the worker nodes via a channel-like communication protocol on top of the Message Passing Interface (MPI), thereby completely avoiding the need for a dedicated driver node. The system adopts a new progress-tracking protocol, called hew-meld, which has been experimentally observed to show a low processing latency on our asynchronous master-less architecture when compared to the conventional low-watermark technique. The current version of AIR is also equipped with two fault tolerance and recovery strategies namely checkpointing & rollback and replication. With its unique design, AIR scales out particularly well to multi-core HPC architectures; specifically, we deployed it on clusters with up to 16 nodes and 448 cores (thus reaching a peak of 435.3 million events and 55.14 GB of data processed per second), which we found to significantly outperform existing DSPEs. © 2022 Elsevier Inc.","Asynchronous iterative routing; Dataflow programming; Distributed stream processing systems; Stateful windowed operators"
"Secured distributed algorithms without hardness assumptions","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139347139&doi=10.1016%2fj.jpdc.2022.09.012&partnerID=40&md5=4df52f5c9d798b71c34a6c46239cec2f","We study algorithms in the LOCAL model that produce secured output. Specifically, each vertex computes its part in the output, the entire output is correct, but each vertex cannot discover the output of other vertices, with a certain probability. As the extensive research in the distributed algorithms field yielded efficient decentralized algorithms, the discussion about the security of distributed algorithms was somewhat neglected. Nevertheless, many protocols and algorithms were devised in the research area of secure multi-party computation problem. However, the focus in those protocols was to work for every function f at the expense of increasing the round complexity, or the necessity of several computational assumptions. We present a novel approach, which identifies and develops those algorithms that are inherently secure (which means they do not require any further constructions). This approach yields efficient secure algorithms for various labeling and decomposition problems without requiring any hardness assumption, but only a private randomness generator in each vertex. © 2022 Elsevier Inc.","Distributed algorithms; Generic algorithms; Graph coloring; Multi-party computation; Privacy preserving"
"Distributed three-way formal concept analysis for large formal contexts","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139592929&doi=10.1016%2fj.jpdc.2022.09.011&partnerID=40&md5=b3583101de241e57eabe759d022a65a8","Three-way concept analysis (3WCA) is a framework based on Formal concept analysis and three-way decisions is used in the field of knowledge discovery to solve uncertainties in many domains like machine learning, data mining and software engineering. The 3WCA requires both the formal context and its complement context for generating concepts and constructing the concept lattice. In three-way concept analysis, data are analyzed using two types of concept lattices constructed using classical formal concept analysis (FCA) algorithms: object-induced (OE) concept lattices and attribute-induced (AE) concept lattices. The existing formal concept analysis algorithms focus on the sequential generation of OE and AE concepts rather than finding them in parallel and cannot process large datasets efficiently. The main contribution of this paper is that we propose a novel parallel algorithm for concept generation and construction of the three way concept lattice for knowledge discovery and representation in large datasets. Aiming to construct an efficient algorithm for 3WCA, this paper primarily discusses the existing algorithms for concept generation. Further we develop an efficient algorithm for OE and AE concept generation and lattice construction. Extensive experiments are conducted on various datasets to evaluate the efficiency of the proposed algorithm. Both the experimental and statistical results demonstrate the efficacy of the algorithm on larger datasets. Also the proposed algorithm can significantly decrease the time required for OE/AE concept generation and lattice construction compared to the existing classical FCA algorithms. © 2022 Elsevier Inc.","3WCA; Apache Spark; Formal concept analysis; RDD's; Three-way concept lattice"
"Using skip graphs for increased NUMA locality","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129695993&doi=10.1016%2fj.jpdc.2022.04.006&partnerID=40&md5=8b8940f2425cb80747d8f3663390db1b","We present a NUMA-aware concurrent data structure design based on a data-partitioned, concurrent skip graph indexed by thread-local sequential maps. Our design brings significant quantitative and qualitative improvements on NUMA locality, as well as reduced contention for synchronized memory accesses. Maps show up to 6x higher compare-and-swap (CAS) locality, up to a 68.6% reduction on the number of remote CAS operations, and an increase from 88.3% to 99% on the CAS success rate compared to a control implementation. Remote memory accesses are not only reduced in number, but the larger the NUMA distance between threads, the larger the reduction is. Relaxed priority queues implemented using our technique show similar scalability improvements, with provable reduction in contention and decrease in relaxation in one of our proposed implementations. © 2022 Elsevier Inc.","Concurrent data structures; Locality; NUMA; Skip graphs"
"An SMDP-based approach to thermal-aware task scheduling in NoC-based MPSoC platforms","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127482655&doi=10.1016%2fj.jpdc.2022.03.016&partnerID=40&md5=be3e6c94abf94f26aadace4f923b64b5","In this paper, we consider the operation of a thermal-aware task scheduler, dispatching tasks from an arrival queue as well as setting the voltage and frequency of the processing cores to optimize the mean temperature margin of the entire chip (i.e., cores as well as the NoC routers). We model the decision process of the task scheduler as a semi-Markov decision problem (SMDP) to account for the most common uncertainties prevalent in MPSoC systems (including: the stochastic nature of the workload inter-arrival times, time-varying workload characteristics, the uncertain chip thermal profile as well as random inter-task communications). SMDP is among the fairly general variants of continuous-time optimization frameworks from the stochastic control theory and is a much more efficient choice compared to discrete-time formalisms (which would lead to an increase in task waiting times and degraded system performance). To solve the formulated SMDP, we propose two reinforcement learning (RL) algorithms that are capable of computing the optimal task assignment policy without requiring the statistical knowledge of the stochastic dynamics underlying the system states. The proposed algorithms also rely on function approximation techniques to handle the infinite length of the task queue as well as the continuous nature of temperature readings. Compared to previous work, the simulations demonstrate nearly 6 Kelvin reduction in system average peak temperature and 66 milliseconds decrease in mean task service time. © 2022 Elsevier Inc.","Multiprocessor system on chip (MPSoC); Online task assignment; Reinforcement learning (RL); Semi-Markov decision problem (SMDP); Thermal management"
"Distributed-memory tensor completion for generalized loss functions in python using new sparse tensor kernels","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135109176&doi=10.1016%2fj.jpdc.2022.07.005&partnerID=40&md5=16fa8e3d6142b7ce86b46d2792d4c97c","Tensor computations are increasingly prevalent numerical techniques in data science, but pose unique challenges for high-performance implementation. We provide novel algorithms and systems infrastructure which enable efficient parallel implementation of algorithms for tensor completion with generalized loss functions. Specifically, we consider alternating minimization, coordinate minimization, and a quasi-Newton (generalized Gauss-Newton) method. By extending the Cyclops library, we implement all of these methods in high-level Python syntax. To make possible tensor completion for very sparse tensors, we introduce new multi-tensor primitives, for which we provide specialized parallel implementations. We compare these routines to pairwise contraction of sparse tensors by reduction to hypersparse matrix formats, and find that the multi-tensor routines are more efficient in theoretical cost and execution time in experiments. We provide microbenchmarking results on the Stampede2 supercomputer to demonstrate the efficiency of the new primitives and Cyclops functionality. We then study the performance of the tensor completion methods for a synthetic tensor with 10 billion nonzeros and the Netflix dataset, considering both least squares and Poisson loss functions. © 2022 Elsevier Inc.","CP decomposition; Cyclops tensor framework; Tensor completion"
"Ad hoc systems management and specification with distributed Petri nets","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134800518&doi=10.1016%2fj.jpdc.2022.06.015&partnerID=40&md5=893d2d45d23d0920ea15e53289c53149","Managing mobile ad hoc systems is a difficult task due to the high volatility of the systems' topology. Ad hoc systems are commonly defined by means of their constituent entities and the relationships between such entities, however, a formal specification and run-time execution model is missing. The benefit of a formal specification is that it can enable reasoning about local and global system properties, for example, determining whether the system can reach a given state. We propose a Petri net-based specification and execution model to manage ad hoc distributed systems. Our model enables spontaneous communication between previously unknown system components. The model is locally equivalent to standard Petri nets, and hence could be used for the verification of properties for system snapshots static with respect to connections and disconnection, in which it is possible to analyze liveness, reachability, or conflicts. We validate the usability of our distributed ad hoc Petri net model by modeling distributable systems as described by existing distributed Petri nets approaches. Additionally, we demonstrate the applicability and usability of the proposed model in distributed ad hoc networks by implementing the communication behavior of two prototypical ad hoc network applications, disaster and crisis management, and VANETs, successfully validating the appropriate behavior of the system in each case. © 2022 Elsevier Inc.","Ad hoc networks; Distributed systems; Petri nets"
"FPGA fabric conscious architecture design and automation of speed-area efficient Margolus neighborhood based cellular automata with variegated scan path insertion","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129740100&doi=10.1016%2fj.jpdc.2022.04.020&partnerID=40&md5=ea8812c6103a8f857d71306905ba8d05","Optimized Field Programmable Gate Array (FPGA) implementation of Cellular Automata (CA) for high speed design requires knowledge of the platform specific logic cell architecture. In this paper, we have proposed architectures and design automation of a particular class of CA, essentially a Finite State Machine (FSM), which obey rules governed by principles of Margolus neighborhood. Under this proposition, the inputs to the next state function of the FSM for every CA cell alternates between two sets of data in every successive clock cycle. Careful choice of logic elements and their compact placement was ensured for speed-area efficient implementation. Variants of scan insertion were carried out for fault localization by properly utilizing the logic cells realizing the original Margolus CA, so that area-delay overhead is minimized. We outperform behavioral or register transfer level (RTL) based descriptions for CA implementations, expressed through conventional higher levels of abstraction, with respect to delay and occupancy count of logic slices. © 2022 Elsevier Inc.","Cellular automata; Field programmable gate array; Margolus neighborhood; Primitive instantiation; Scan insertion"
"An intelligent hybrid method: Multi-objective optimization for MEC-enabled devices of IoE","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138221573&doi=10.1016%2fj.jpdc.2022.09.008&partnerID=40&md5=1ad3f29543cb9d91c8d1bbceaf8561d1","Emerging Internet-of-Everything (IoE) services require connected devices to respond instantly and operate for long durations. As smart mobile devices (SMDs) are often powered by batteries of limited capacity, offloading some computational tasks to nearby edge servers is a promising solution to reduce the latency and energy consumption of SMDs in operation. However, a challenge of computation offloading in the IoE system is the large number of SMDs that need to be handled. To address this problem, in this paper, we propose an intelligent two-stage computation offloading scheme with multiple optimization objectives. In the first stage, we categorize the computation tasks into classes (e.g., computation-intensive, data-intensive) and make early offloading decisions on some classes of tasks with offloading preferences. In the second stage, we make offloading decisions on the remaining tasks by solving a multi-objective optimization problem using the powerful Non-dominated Sorting Genetic Algorithm (NSGA-II). This two-stage design can help reduce the size of task instances in the second optimization stage, thus accelerating the convergence of the NSGA-II algorithm. The extensive simulation results show that, compared to the existing NSGA-II algorithm-based optimization methods, the proposed offloading scheme improves the performance by 10% in terms of latency and energy consumption. © 2022 Elsevier Inc.","Computation offloading; Edge computing; Latency and energy consumption; Multi-objective optimization"
"A secure three-factor authentication scheme for IoT environments","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133437515&doi=10.1016%2fj.jpdc.2022.06.011&partnerID=40&md5=73f4529ac93d34e622fe2592e6215c6f","Today, many users' extensive use of the Internet of Things (IoT) has made authentication an inevitable issue in the IoT. The currently existing authentication methods are subjected to many challenges by various factors such as the limited resources, the lack of authorization, and the need for a light-weighted authentication process. Therefore, it is essential to provide a security framework and protect the users' privacy at the lowest cost. This paper proposes a three-factor-based authentication scheme, called defense-in-depth, for the IoT environments on the blockchain platform. The proposed protocol applies mutual authentication with user authorization using smart card registration on a private blockchain without the need for a trustable server. The use of Elliptic-Curve Cryptography (ECC) and the analysis of the security of the proposed protocol using AVISPA tool, formal/informal security analysis altogether indicate that the proposed scheme is more secure and efficient in terms of computational and communications costs. © 2022 Elsevier Inc.","Authentication; Blockchain; Elliptic-Curve Cryptography(ECC); Internet of Things; Security"
"ALBERT: An automatic learning based execution and resource management system for optimizing Hadoop workload in clouds","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132352653&doi=10.1016%2fj.jpdc.2022.05.013&partnerID=40&md5=63be4133d66a0c95fd05ac54c8a3212d","Hadoop is a popular computing framework designed to deliver timely and cost-effective data processing on a large cluster of commodity machines. It relieves the burden of the programmers dealing with distributed programming, and an ecosystem of Big Data solutions has developed around it. However, Hadoop's job execution time can greatly depend on its runtime configurations and resource selections. Given the more than 100 job configuration settings provided by Hadoop, and diverse resource instance options in a cloud or virtualized computing environment, running Hadoop jobs still requires a substantial amount of expertise and experience. To address this challenge, we apply a deep neural network to predict Hadoop's job time based on historical execution data, and propose optimization methods to reduce job execution time and cost. The results show that our prediction method achieves almost 90% time prediction accuracy and clearly outperforms three other state-of-the-art regression-based prediction methods. Based on the time prediction, our proposed configuration search method and job scheduling algorithm successfully shorten the execution time of a single Hadoop job by more than a factor of 2 and reduce the time of processing a batch of Hadoop jobs by 40%∼65%. © 2022 Elsevier Inc.","Data analytic; Deep learning; Job scheduling; Optimization; Time prediction"
"Asynchronous simulated annealing on the placement problem: A beneficial race condition","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134820514&doi=10.1016%2fj.jpdc.2022.07.001&partnerID=40&md5=52c78d90984ccd3209c04661eee9f2d2","Race conditions, which occur when compute workers do not synchronise correctly, are considered undesirable in parallel computing, as they introduce often-unintended stochastic behaviour. This study presents an asynchronous parallel algorithm with a race condition, and demonstrates that it reaches a superior solution faster than the equivalent synchronous algorithm without the race condition. Specifically, a parallel simulated annealing algorithm that solves a graph mapping problem (placement) is used to explore this. This paper illustrates how problem size and degree of parallelism affects both the collision rate caused by the race condition, and convergence time. The asynchronous approach reaches a superior solution in half the time of the equivalent synchronous approach. The solver presented here can be applied to application deployment in distributed systems, and the concept can be applied to problems solvable by global optimisation methods, where fitness errors can be tolerated in exchange for faster execution. © 2022 The Author(s)","High performance computing; Optimization; Parallel computing; Place and route; Simulated annealing"
"A stochastic conditional gradient algorithm for decentralized online convex optimization","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135710434&doi=10.1016%2fj.jpdc.2022.07.010&partnerID=40&md5=e39a06395162ae2e0581dcb690d48740","We study in this paper several problems at the intersection of decentralized optimization and online learning. Decentralized optimization plays a vital role in machine learning and has recently garnered much attention due to its inherent advantage in handling edge computations. Many decentralized optimization algorithms, both projection and projection-free algorithms with theoretical guarantees, have been proposed in the literature, focusing mainly on offline settings. However, for most real-world machine learning problems, the data is often revealed online, for example, in the case of recommender systems, image/video processing, and stock portfolio management. Therefore, in this work, we study decentralized optimization within the framework of online settings with constraints imposed on the optimization solutions (e.g., sparsity or low rank of matrices). More specifically, we consider the problem of optimizing an aggregate of convex loss functions that arrive over time such that their components are distributed over a connected network. We present a consensus-based online decentralized Frank-Wolfe algorithm that uses stochastic gradient estimates, which achieves an asymptotically tight regret guarantee of O(T) where T is a given time horizon. Furthermore, we demonstrate the performance of this algorithm for optimizing the online multiclass logistic regression model on real-world standard image datasets (MNIST, CIFAR10) by comparing with centralized online algorithms. We achieve better regret bounds than the previously best-known decentralized constrained online algorithms. © 2022 Elsevier Inc.","Distributed learning; Edge computing; Machine learning; Online learning"
"GPU-accelerated scalable solver with bit permutated cyclic-min algorithm for quadratic unconstrained binary optimization","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130091293&doi=10.1016%2fj.jpdc.2022.04.016&partnerID=40&md5=69c90938db1c84884406a847c032d781","A wide range of combinatorial optimization problems can be reduced to the Ising model, and equivalently the quadratic unconstrained binary optimization (QUBO) problem. Thus, in recent years, researchers have proposed to solve QUBO on FPGAs, GPUs, and special-purpose processors. The adaptive bulk search (ABS) is a previously-proposed framework for solving QUBO in parallel on multiple GPUs. In the ABS, a CPU host performs a GA-based global search while GPUs asynchronously perform many local searches in parallel. An original ABS adopts a simple local search algorithm called a cyclic-min algorithm, which does not use pseudo random numbers. However, the lack of randomness may cause a potential drawback of restricted bit-flipping operations in a local search. To avoid this drawback, this paper proposes a cyclic-min algorithm with randomly-generated multiple bit permutations, which enables a more effective local search with random number generation in CPUs (not in GPUs). Furthermore, this paper introduces a scalable implementation of the ABS with MPI and OpenMP. Our experimental results on TSUBAME3.0 show that the solution quality improves and the throughput linearly increases as the number of GPUs increases; with 256 GPUs, it evaluates 20.1×1012 solutions per second. © 2022 Elsevier Inc.","Combinatorial optimization; GPGPU; Ising model; Parallel computing; Quantum annealing"
"Decoupling GPGPU voltage-frequency scaling for deep-learning applications","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127318798&doi=10.1016%2fj.jpdc.2022.03.004&partnerID=40&md5=6663d82585c82db477ab4caa122b0399","The use of GPUs to accelerate DNN training and inference is already widely adopted, allowing for a significant performance increase. However, this performance is usually obtained at the cost of a consequent increase in energy consumption. While several solutions have been proposed to perform voltage-frequency scaling on GPUs, these are still one-dimensional, by simply adjusting the frequency while relying on default voltage settings. To overcome this limitation, this paper introduces a new methodology to fully characterize the impact of non-conventional DVFS on GPUs. The proposed approach was evaluated on two devices, an AMD Vega 10 Frontier Edition and an AMD Radeon 5700XT. When applying this non-conventional DVFS scheme to DNN training, the obtained results show that it is possible to safely decrease the GPU voltage, allowing for a significant reduction of the energy consumption (up to 38%) and of the EDP (up to 41%) on the training procedure of CNN models, with no degradation of the networks accuracy. © 2022 Elsevier Inc.","CNN; DVFS; Energy efficiency; GPU; Undervoltage"
"Forseti: Dynamic chunk-level reshaping for data processing on heterogeneous clusters","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138196783&doi=10.1016%2fj.jpdc.2022.09.003&partnerID=40&md5=cd6509937b9b392ffdc3dfe3d55953bf","Data-intensive computing frameworks typically split job workload into fixed-size chunks, allowing them to be processed as parallel tasks on distributed machines. Ideally, when the machines are homogeneous and have identical speed, chunks of equal size would finish processing at the same time. However, such determinism in processing time cannot be guaranteed in practice. Diverging processing times can result from various sources such as system dynamics, machine heterogeneity, and variable network conditions. Such variation, together with dynamics and uncertainty during task processing, can lead to significant performance degradation at job level, due to long tails in job completion time resulted from residual chunk workload and stragglers. In this paper, we propose Forseti, a novel processing scheme that is able to reshape data chunk size on the fly with respect to heterogeneous machines and a dynamic execution environment. Forseti mitigates residual workload and stragglers to achieve significant improvement in performance. We note that Forseti is a fully online scheme and does not require any a priori knowledge of the machine configuration nor job statistics. Instead, it infers such information and adjusts data chunk sizes at runtime, making the solution robust even in environments with high volatility. In its implementation, Forseti also exploits a virtual machine reuse feature to avoid task start-up and initialization cost associated with launching new tasks. We prototype Forseti on a real-world cluster and evaluate its performance using several realistic benchmarks. The results show that Forseti outperforms a number of baselines, including default Hadoop by up to 68% and SkewTune by up to 50% in terms of average job completion time. © 2022 Elsevier Inc.","Distributed computing; Scheduling"
"Abaci-finder: Linux kernel crash classification through stack trace similarity learning","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132796139&doi=10.1016%2fj.jpdc.2022.06.003&partnerID=40&md5=8c2ca39ca5f894d7be8a76b8cb1042ab","Developers often classify crashes by stack traces to analyze, locate and fix kernel bugs. Existing stack-trace-based crash classification approaches rely on string matching and statistical features, which ignore crash semantic contexts and cannot explore high-order correlations. Deep-learning-based approaches use crash embeddings and output end-to-end features for classification. However, they ignore kernel-specific information, which limits classification performance. Regarding these issues, we propose abaci-finder, a deep-learning-based classification framework specific to Linux kernel crashes. We first model the kernel stack trace as a stack frames sequence and then perform stack trace preprocessing. Then, we propose a vectorization method specific to kernel stack traces, called kstack2vec, to extract features with consideration for function semantics and kernel-specific offsets information. Finally, we exploit an attention-based BiLSTM neural network for classification, with consideration for both frame context and key frames in traces. The experiments on the real Linux kernel crash dataset indicate that abaci-finder outperforms existing methods of crash classification. © 2022 Elsevier Inc.","Crash classification; Linux kernel; Neural network; Stack trace"
"ZeroCross: A sidechain-based privacy-preserving Cross-chain solution for Monero","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135862061&doi=10.1016%2fj.jpdc.2022.07.008&partnerID=40&md5=c979a7555ea53887ca0217c82bcf40bf","Sidechain-based Cross-chain exchange protocols enable payers to exchange cryptocurrencies among different blockchains via a sidechain. Many efforts, such as P2DEX (ACNS' 21), have been proposed to enhance cross-chain exchange privacy protection. However, existing sidechain-based cross-chain solutions for Monero on privacy concerns have limitations: requiring multiple pairs of parties paying simultaneously or fixed transaction amounts. This paper proposes ZeroCross, a novel privacy-preserving sidechain-based scheme that guarantees transaction unlinkability, exchanging fairness, and value confidentiality. ZeroCross designs: (i) a key exchange mechanism that guarantees exchanging fairness and (ii) a verification mechanism that utilizes CP-SNARK to ensure the transaction is confirmed without revealing the details of transactions. In addition, we discuss the influence of the remote side-channel attack in cross-chain exchange and the defence strategy. Finally, we prove the privacy and security of ZeroCross under the Universal Composability (UC) framework and evaluate the practical performance on computation and communication costs. © 2022 Elsevier Inc.","Blockchain; Cryptocurrency exchange; Monero; Zero-knowledge"
"Towards a component-based acceleration of convolutional neural networks on FPGAs","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130093750&doi=10.1016%2fj.jpdc.2022.04.025&partnerID=40&md5=17b8fcff869894b176f196ea328034c6","In recent years, Convolution Neural Networks (CNN) have been extensively adopted in broad Artificial Intelligence (AI) applications and have demonstrated ability and effectiveness in solving learning problems. However, developing high-performance hardware accelerators on Field Programmable Gate Array (FPGA) for CNNs often demands skills in hardware design and verification, accurate distribution localization, and long development cycles. Besides, the depth of CNN architectures increases by reusing and replicating several layers. In this work, we take advantage of the replication of CNN layers to achieve improvement in design performance and productivity. We propose a programming flow for CNNs on FPGA to generate high-performance accelerators by assembling CNN pre-implemented components as a puzzle based on the graph topology. Using pre-implemented components allows us to use minimum of resources, predict the performance, and gain in productivity since there is no need to synthesize any Hardware Description Language (HDL) source code. Furthermore, the pre-implemented components are reused for different range of applications, reducing the engineering time. Through prototyping, we demonstrate the viability and relevance of our approach. Experiments show a productivity improvement of up to 69% compared to a traditional FPGA implementation while achieving over 1.75× higher Fmax with lower resources and higher energy efficiency. © 2022 Elsevier Inc.","CNN inference; Data flow graph; FPGA; Pre-implemented flow"
"Hybrid multi-grid parallelisation of WAVEWATCH III model on spherical multiple-cell grids","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130575454&doi=10.1016%2fj.jpdc.2022.05.002&partnerID=40&md5=f4ac6c81a46e7573e0c4c8f21ed6566e","Spherical Multiple-Cell (SMC) grid is an unstructured grid, supporting flexible domain shapes and multi-resolutions. It retains the quadrilateral cells as in the latitude-longitude (lat-lon) grid so that simple finite-difference schemes can be used. Sub-timesteps are applied on refined cells and grid cells are merged at high latitudes to relax the CFL restriction. A fixed reference direction is used in polar regions to solve vector polar problems. The SMC grid was implemented in the WAVEWATCH III (WW3) wave model in 2012 as an alternative for the lat-lon grid and updated in the latest WW3 V6.07. The WW3 model is parallelised by wave spectral component decomposition (CD) in MPI mode, which has a limit on number of MPI ranks. Hybrid or combined MPI-OpenMP parallelisation may extend the node usage but the OpenMP scalability flattens out beyond a few threads. Another parallelisation method that combines CD with domain decomposition (DD) is enabled in WW3 model by a multi-grid framework for further extension of node usage. Those regular lat-lon grid parallelisation options are gradually added to the SMC grid and this article reports the recent extension of the SMC grid into the multi-grid framework with hybrid parallelisation. The flexible domain shape of the SMC grid allows optimised domain splitting and minimised boundary exchanges. The combined CD-DD method is tested on SMC sub-grids with various hybrid node-thread combinations. Results indicate that switching from MPI to hybrid MPI-OpenMP mode can halve the global model elapsed time and using hybrid CD-DD on 3 SMC sub-grids may reduce it further by 30%. Elapsed time for one model day run is reduced from about 3 min on 12 nodes to less than 1 min on 90 or 180 nodes. Besides, the hybrid multi-grid method reduces memory demand on one computing node and allows future model updates for higher resolutions. © 2022","Domain decomposition; Large-scale computing; MPI-OpenMP; Ocean surface waves; SMC grid"
"Goal-driven scheduling model in edge computing for smart city applications","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129931846&doi=10.1016%2fj.jpdc.2022.04.024&partnerID=40&md5=455864996aaa2ab999bc9f1180753b83","A formidable challenge in scheduling user applications lies in collecting and representing the user's goals and requirements. We introduce a “science goal” as a mechanism for users to define scientific objectives and conditions of interest. To provide an abstraction to run applications on an ensemble of edge computing nodes, we implement a two-layered scheduler—cloud and edge scheduler. In this scheduling model, the users submit their goals to the cloud scheduler. These goals are conveyed to the appropriate nodes based on a variety of constraints including geographical area, resource availability, node capabilities, and applicability. The edge scheduler, with complete understanding of the current conditions, assumes the responsibility for executing the applications on the nodes so that the users' science goals are met. This paper provides a framework for the two-layered scheduling model for goal-driven edge computing and motivates and informs its architecture through a case study. © 2022 The Author(s)","Context-aware scheduling; Edge computing; Goal-driven scheduling; Logical reasoning"
"A method for reducing cloud service request peaks based on game theory","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127745711&doi=10.1016%2fj.jpdc.2022.03.002&partnerID=40&md5=0461fec28665116facad4d066ce71d62","The rapid development of Internet of Things (IoT) technology and the popularity of Artificial Intelligence (Al) technology research have brought new opportunities for the development of cloud computing (CC). With the increasing number of mobile Internet access devices and IoT access devices, the number of task requests from CC customers for AI services in the network has also experienced an explosive growth. In this paper, the focus is on the possible overload of cloud providers during the peak period of cloud service requests. Thetime attributes of cloud task execution are classified to avoid overloading the cloud provider as much as possible. In a distributed cloud environment, it is necessary to consider the time flexible attributes of cloud tasks to reasonably compete for cloud resources. In this work, game theory (GT) is introduced to formulate a cloud service scheduling game, in which participants are cloud customers who participate in the purchase of cloud services. The players' strategies are the time flexibility of each cloud task. The problem is formulated as minimizing the cost of scheduling cloud services and a noncooperative game among the customers (as players) is presented. Then the existence of the Nash equilibrium (NE) solution of the game has been proved and a new algorithm has been proposed in this paper to compute it. In addition, the analysis process of the convergence of the proposed PCA algorithm and the proof of its convergence to NE are also included in this paper. At the end of the paper, simulations were performed to verify the theoretical analysis presented. The experimental results show that the proposed PCA algorithm can converge to the Nash equilibrium very quickly, effectively reducing the peak value and increasing profit. © 2022 Elsevier Inc.","Cloud computing; Game theory (GT); Nash equilibrium; Peak value"
"Distributed intelligence on the Edge-to-Cloud Continuum: A systematic literature review","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134542998&doi=10.1016%2fj.jpdc.2022.04.004&partnerID=40&md5=12851031ebbd6cd12568d4e7f1173ad4","The explosion of data volumes generated by an increasing number of applications is strongly impacting the evolution of distributed digital infrastructures for data analytics and machine learning (ML). While data analytics used to be mainly performed on cloud infrastructures, the rapid development of IoT infrastructures and the requirements for low-latency, secure processing has motivated the development of edge analytics. Today, to balance various trade-offs, ML-based analytics tends to increasingly leverage an interconnected ecosystem that allows complex applications to be executed on hybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC systems in what is called the Computing Continuum, the Digital Continuum, or the Transcontinuum. Enabling learning-based analytics on such complex infrastructures is challenging. The large scale and optimized deployment of learning-based workflows across the Edge-to-Cloud Continuum requires extensive and reproducible experimental analysis of the application execution on representative testbeds. This is necessary to help understand the performance trade-offs that result from combining a variety of learning paradigms and supportive frameworks. A thorough experimental analysis requires the assessment of the impact of multiple factors, such as: model accuracy, training time, network overhead, energy consumption, processing latency, among others. This review aims at providing a comprehensive vision of the main state-of-the-art libraries and frameworks for machine learning and data analytics available today. It describes the main learning paradigms enabling learning-based analytics on the Edge-to-Cloud Continuum. The main simulation, emulation, deployment systems, and testbeds for experimental research on the Edge-to-Cloud Continuum available today are also surveyed. Furthermore, we analyze how the selected systems provide support for experiment reproducibility. We conclude our review with a detailed discussion of relevant open research challenges and of future directions in this domain such as: holistic understanding of performance; performance optimization of applications; efficient deployment of Artificial Intelligence (AI) workflows on highly heterogeneous infrastructures; and reproducible analysis of experiments on the Computing Continuum. © 2022 Elsevier Inc.","Big Data Analytics; Computing Continuum; Distributed intelligence; Edge computing; Reproducibility"
"On-demand inference acceleration for directed acyclic graph neural networks over edge-cloud collaboration","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139072133&doi=10.1016%2fj.jpdc.2022.09.005&partnerID=40&md5=86fec552fac8c0ed22ba85d566603d89","As the development of deep neural network (DNN) tasks that feature a directed acyclic graph (DAG) surges in modern industries, simultaneously meeting the latency and accuracy requirements of these key DNN tasks has remained elusive. The general feasible solution is to find an optimal DNN partition that combines cloud with edge computing to implement edge–cloud collaboration for DNN inference. Nevertheless, the dynamicity of network conditions and the uncertain availability of cloud computing resources pose formidable obstacles to this collaborative inference. In this study, we formulate the optimization of DNN partition as a minimum cut problem in DAG derived from DNN and then propose an early-exit DAG-DNN inference (EDDI) framework that supports synergistically on-demand inference acceleration for DAG DNN. This framework introduces two novel components: (1) Evaluator that derives an approximately optimal solution for constructing DNN into appropriate DAG, assisting in solving the optimization problem; and (2) Optimizer that enables collaborative optimization of the early exit and DNN partition strategy at run time to improve performance while meeting user-defined latency requirements. Quantitative evaluations show that EDDI outperforms state-of-the-art schemes by 10.6%, 3.3%, and 3%, on average, in terms of model accuracy, inference latency, and throughput under diverse latency constraints, respectively. Meanwhile, latency speedup ratio increases by an average of 8% and 4% under varying network conditions and cloud server capacities, respectively. © 2022 Elsevier Inc.","Cloud computing; Computation offloading; Deep neural networks; Edge computing; Edge intelligence"
"A distributed-memory MPI parallelization scheme for multi-domain incompressible SPH","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137167501&doi=10.1016%2fj.jpdc.2022.08.004&partnerID=40&md5=f351616484477cfd3482f1437e82ba94","A parallel scheme for a multi-domain truly incompressible smoothed particle hydrodynamics (SPH) approach is presented. The proposed method is developed for distributed-memory architectures through the Message Passing Interface (MPI) paradigm as communication between partitions. The proposal aims to overcome one of the main drawbacks of the SPH method, which is the high computational cost with respect to mesh-based methods, by coupling a multi-resolution approach with parallel computing techniques. The multi-domain approach aims to employ different resolutions by subdividing the computational domain into non-overlapping blocks separated by block interfaces. The particles belonging to different blocks are efficiently distributed among processors ensuring well balanced loads. The parallelization procedure handles particle exchanges both throughout the blocks and the competence domains of the processors. The matching of the velocity values between neighbouring blocks is obtained solving a system of interpolation equations at each block interfaces through a parallelized BiCGSTAB algorithm. Otherwise, a whole pseudo-pressure system is solved in parallel considering the Pressure Poisson equations of the fluid particles of all the blocks and the interpolation equations of all the block interfaces. The employed test cases show the strong reduction of the computational efforts of the SPH method thanks to the interaction of the employed multi-resolution approach and the proposed parallel algorithms. © 2022 Elsevier Inc.","Load balancing; MPI; Multi-domain approach; Parallel distributed-memory computation; Smoothed particle hydrodynamics (SPH)"
"Understanding node connection modes in Multi-Rail Fat-tree","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130525963&doi=10.1016%2fj.jpdc.2022.04.019&partnerID=40&md5=5f73d90728232dc95c4e00c1d1cb801e","Using Multi-Rail networks has become a popular choice for many leading HPC systems to overcome bandwidth limitations. Little is known, theoretically or practically, how the connection modes between multi-port nodes and switching network influence the performance of system. This work provides a detailed analysis of different node connection modes in Multi-Rail Fat-tree, which involves Single-Plane Multi-Rail Fat-tree and Multi-Plane Multi-Rail Fat-tree. We evaluate different node connection modes by theoretical analysis and flit level simulation. We show that there are great differences across various node connection modes. Among the key differences are the following: cost, average shortest path length, fault-tolerance, path diversity, latency and throughput. In addition, we propose MR-tree and MP-tree respectively to gain a deep understanding of relevant issues and improve the network performance. Our findings leave open the possibility that optimization of node connection mode can yield better results for Multi-Rail Fat-tree. © 2022 Elsevier Inc.","Connection mode; Fat-tree; HPC; Multi-Rail; Topology"
"Reducing response latency of composite functions-as-a-service through scheduling","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129285960&doi=10.1016%2fj.jpdc.2022.04.011&partnerID=40&md5=d7b242cee0bd60d89c419d01a62c2cc4","In Function-as-a-Service (FaaS) clouds, customers deploy to cloud individual functions, in contrast to complete virtual machines (IaaS) or Linux containers (PaaS). FaaS offerings are available in the largest public clouds (Amazon Lambda, Google Cloud Functions, Azure Serverless); there are also popular open-source implementations (Apache OpenWhisk) with commercial offerings (Adobe I/O Runtime, IBM Cloud Functions). A recent addition to FaaS is the ability to compose functions: a function may call another functions, which, in turn, may call yet another function — forming a directed acyclic graph (DAG) of invocations. From the perspective of the infrastructure, a composed function is less opaque than a virtual machine or a container. We show that this additional information about the internal structure of the function enables the infrastructure provider to reduce the response latency. In particular, knowing the successors of a function in a DAG, the infrastructure can schedule these future invocations along with necessary preparation of environments. We model resource management in FaaS as a scheduling problem combining (1) sequencing of invocations; (2) deploying execution environments on machines; and (3) allocating invocations to deployed environments. For each aspect, we propose heuristics that employ FaaS-specific features. We explore their performance by simulation on a range of synthetic workloads and on workloads inspired by trace from existing system. Our results show that if the setup times are long compared to invocation times, algorithms that use information about the composition of functions consistently outperform greedy, myopic algorithms, leading to significant decrease in response latency. © 2022 Elsevier Inc.","DAG; Scheduling; Serverless; Setup times; Workflow"
"A deep learning-based edge caching optimization method for cost-driven planning process over IIoT","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132515584&doi=10.1016%2fj.jpdc.2022.06.007&partnerID=40&md5=91a7e17e344aaa31a6b58cb05eac3440","Edge computing has been considered as a leading paradigm to satisfy the low latency demand for some computation-intensive or data-intensive applications, especially for IIoT applications such as automatic line scheduling of the Internet of Vehicles, time-sensitive supply-chain supervision, and smart control of complex industrial processes. In the edge computing environment, app vendors prefer to cache their app data on edge servers to ensure low latency service. However, it is frequently a challenge in practice, because cache spaces on edge servers are limited and expensive. In view of this challenge, a deep learning-based edge caching optimization method, named DLECO, is proposed to reduce the cost during the cache planning process. In this paper, the edge app data caching problem is formulated as a constrained optimization problem. Then, the specific design of DLECO with a deep learning model is shown, which aims to minimize the overall system cost with lower service latency. The performance of DLECO is analyzed theoretically and experimentally with a collection of data from the real world. The experimental results show its superior performance through comparison with three representative methods. © 2022","App data caching; Constraint optimization; Cost-driven; Deep learning; Edge computing"
"Reliability of arrangement networks in terms of the h-restricted edge connectivity","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137271809&doi=10.1016%2fj.jpdc.2022.08.003&partnerID=40&md5=af9fc60c08c6f2359d6118ceeb5c8ef3","The h-restricted edge connectivity λh is an important parameter to measure the reliability of massive parallel computing system. The λh of G is the minimum cardinality of a set of edges in G, if any, whose deletion disconnects G, and every remaining component has minimum degree at least h. The arrangement graph is a generalization of the star graph, but its order is more flexible than that of the star graph. This paper investigates the h-restricted edge connectivity of the (n,k)-arrangement graph An,k, and determines that λ1(An,k)=2k(n−k)−2, λ2(An,k)=3k(n−k)−6 with k≤n−2. © 2022 Elsevier Inc.","Arrangement graph; Fault tolerance; h-restricted edge connectivity; Reliability"
"A forward and backward private oblivious RAM for storage outsourcing on edge-cloud computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129022920&doi=10.1016%2fj.jpdc.2022.04.008&partnerID=40&md5=a02f4cf303e7646ddea4520b0f451294","In recent years, edge-cloud computing is regarded as a promising solution to meet the requirements of mobile computing and Internet-of-Things (IoT). However, due to the limited storage resources of edge equipment, there is a security threat when users outsource their sensitive data to the cloud computing center. The users usually adopt a data-encryption approach, Oblivious RAM (ORAM), which enables a user to read/write her outsourced private data without access pattern leakage. Not all users like the fully functional ORAM all the time since the ORAM protocol is usually highly interactive or occupies large edge storage space. We show that forward-private/backward-private (FP/BP) ORAMs are good alternatives for secure storage outsourcing. We introduce the FP/BP-ORAM definitions and present LL-ORAM, the first FP/BP-ORAM that achieves near-zero edge storage, single-round-trip read/write, and worst-case sublinear access time. For any outsourced record, LL-ORAM provides both an oblivious-access interface and a nonoblivious-access interface. FP-ORAM concerns more data-write privacy than data-read privacy. BP-ORAM concerns more data-read privacy. The constructions involve a tree data structure named LL-tree, which supports fast computation in the cloud with an access-pattern-reduced leakage profile. The security analysis shows that LL-ORAM meets the proposed forward and backward security model. The experimental results demonstrate that LL-ORAM is round-efficient and can be deployed on edge-cloud computing systems. © 2022 Elsevier Inc.","Dynamic searchable encryption; Mobile edge computing; Oblivious RAM"
"A GAN-based method for time-dependent cloud workload generation","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132397746&doi=10.1016%2fj.jpdc.2022.05.007&partnerID=40&md5=f1d3e6f684292633531835c9124504de","To design repeatable and comparable resource management policies for data centers, researchers mainly conduct experiments in the simulation environment, which requires large-scale workload traces to simulate real scenes. However, issues related to data collection, security and privacy hinder the public availability of cloud workload datasets. Though workload generation is a promising solution, due to the unpredictable time dependency, cloud workloads are difficult to model. In light of this, we propose a novel end-to-end model for time-dependent cloud workload generation using Generative Adversarial Networks, which adopts improved Temporal Convolution Networks and Spectral Normalization to capture the time dependency and stabilize the adversarial training. Experimental results on real cloud datasets demonstrate that our model can efficiently generate realistic workloads that fulfill the diversity, fidelity and usefulness. Further, we also propose a conditional GAN which is trained with labeled data and can generate specific kind of workloads according to the input. © 2022 Elsevier Inc.","Cloud computing; Deep learning; Generative adversarial networks; Time-dependent workload generation"
"Evaluating execution time predictions on GPU kernels using an analytical model and machine learning techniques","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138477175&doi=10.1016%2fj.jpdc.2022.09.002&partnerID=40&md5=0f6b0ee252f880e0566fd76015012386","Predicting the performance of applications executed on GPUs is a great challenge and is essential for efficient job schedulers. There are different approaches to do this, namely analytical modeling and machine learning (ML) techniques. Machine learning requires large training sets and reliable features, nevertheless it can capture the interactions between architecture and software without manual intervention. In this paper, we compared a BSP-based analytical model to predict the time of execution of kernels executed over GPUs. The comparison was made using three different ML techniques. The analytical model is based on the number of computations and memory accesses of the GPU, with additional information on cache usage obtained from profiling. The ML techniques Linear Regression, Support Vector Machine, and Random Forest were evaluated over two scenarios: first, data input or features for ML techniques were the same as the analytical model and, second, using a process of feature extraction, which used correlation analysis and hierarchical clustering. Our experiments were conducted with 20 CUDA kernels, 11 of which belonged to 6 real-world applications of the Rodinia benchmark suite, and the other were classical matrix-vector applications commonly used for benchmarking. We collected data over 9 NVIDIA GPUs in different machines. We show that the analytical model performs better at predicting when applications scale regularly. For the analytical model a single parameter λ is capable of adjusting the predictions, minimizing the complex analysis in the applications. We show also that ML techniques obtained high accuracy when a process of feature extraction is implemented. Sets of 5 and 10 features were tested in two different ways, for unknown GPUs and for unknown Kernels. For ML experiments with a process of feature extractions, we got errors around 1.54% and 2.71%, for unknown GPUs and for unknown Kernels, respectively. © 2022 Elsevier Inc.","Bulk synchronous parallel model; GPU applications; Machine learning; Performance prediction"
"A cluster-based routing method with authentication capability in Vehicular Ad hoc Networks (VANETs)","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133413177&doi=10.1016%2fj.jpdc.2022.06.009&partnerID=40&md5=57ddfd4888dfc2b52ce53e2ebb2f63af","Routing is challenging in vehicular ad hoc networks due to their features, such as high mobility of nodes and unstable wireless communication links. Therefore, it is an interesting issue for researchers. In addition, it is very important to design an authentication mechanism between the source node and the destination node because these networks are exposed to many attacks due to their features mentioned above. In this paper, we present a fuzzy logic-based routing method with authentication capability in vehicular ad hoc networks. The proposed routing method has three phases: clustering phase, routing phase between cluster head nodes, and authentication phase. In the first phase, vehicles are clustered using an efficient scheme. In the proposed method, we define two types of data packets: immediate and ordinary. Various data packets have different route discovery processes that are described in Phase 2. Note that any data packet type is divided into two groups: simple and secure. Simple data packets have no authentication mechanism. On the other hand, secure data packets use an authentication mechanism based on message authentication code (MAC) and symmetric key cryptography. We simulate the proposed method using NS2. Simulation results are compared with three routing protocols including, AODV, R2SCDT, and 3VSR. Experiments show that the proposed method outperforms others in terms of end-to-end delay, packet collision, packet delivery rate (PDR), packet loss rate (PLR), and throughput. However, it increases the routing overhead slightly. © 2022 Elsevier Inc.","Authentication; Cryptography; Routing; Security; Vehicular Ad hoc Networks (VANETs)"
"Zoro: A robotic middleware combining high performance and high reliability","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129514499&doi=10.1016%2fj.jpdc.2022.04.010&partnerID=40&md5=dc08f64325991cb0991e862f139b5181","With the significant advances of AI technology, robotic systems have achieved remarkable development and profound effects. The robotic middleware plays a critical role in robotic systems to provide message definition, data transmission, and service discovery functions. As the modern robotic systems are usually operated in safety critical environments, such as various autonomous driving scenarios, it requires the design of robotic middleware to combine both high performance and high reliability in order to achieve system reliability and product safety. However, conventional robotic middleware used in the majority of robotic systems is based on an inefficient socket communication mechanism and relies on a hasty service discovery design, which leads to system instability and high resource usage. In this work, we propose a sophisticated robotic middleware, Zoro, to fulfill both high performance and high reliability. For communication, we employ shared memory for performance improvement and propose a socket-based communication control algorithm to improve reliability during data transmission. Also, a hierarchical memory protection mechanism is proposed to address safety problems caused by shared memory. Furthermore, we design a light-weight service discovery, to achieve high performance and a weak centralized mechanism for high reliability. Experiments show the communication latency of Zoro significantly outperforms state-of-the-art robotic middleware such as ROS2 and CyberRT by up to 41%. In terms of service discovery, Zoro reduces CPU usage by up to 44% compared to ROS. Zoro achieves reliability with respect to communication and service discovery. © 2022 Elsevier Inc.","High performance; High reliability; Inter-process communication; Robotic systems; Service discovery"
"HY-DBSCAN: A hybrid parallel DBSCAN clustering algorithm scalable on distributed-memory computers","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132764177&doi=10.1016%2fj.jpdc.2022.06.005&partnerID=40&md5=7da4c4dbc55ae7051747f3bee06e4a08","DBSCAN is a density-based clustering algorithm which is well known for its ability to discover clusters of arbitrary shape as well as to distinguish noise. As it is computationally expensive for large datasets, research studies on the parallelization of DBSCAN have been received a considerable amount of attention. In this paper we present an exact, efficient and scalable parallel DBSCAN algorithm which we call HY-DBSCAN. It employs three major techniques to enable scalable data clustering on distributed-memory computers i) a modified kd-tree for domain decomposition, ii) a spatial indexing approach based on grid and inference, and iii) a cluster merging scheme based on distributed Rem's UNION-FIND algorithm. Moreover, HY-DBSCAN exploits process level and thread level parallelization. In experiments, we have demonstrated performance and scalability using two scientific datasets on up to 2048 cores of a distributed-memory computer. Through extensive evaluation, we show that HY-DBSCAN significantly outperforms previous state-of-the-art DBSCAN implementations. © 2022 Elsevier Inc.","Dbscan; Density-based clustering; Distributed-memory; Parallel algorithm"
"The g-extra connectivity of folded crossed cubes","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129506918&doi=10.1016%2fj.jpdc.2022.04.014&partnerID=40&md5=906fd9d9586bb294cbf0304152c93388","There are various ways to measure the reliability and fault tolerance of diverse networks. The g-extra connectivity κg(G) of a connected graph G is the minimal cardinality of vertex set F, if any, whose deletion disconnects G and every remaining component of G−F has at least g+1 vertices. Folded crossed cubes FCQn, a kind of interconnection network, has more reliable properties. In this paper, we explore the g-extra connectivity of n-dimensional folded crossed cubes. It is shown that when [Formula presented], κg(FCQn)=(g+1)n−g−(g2)+1 for n≥8. As a byproduct, we get g-extra conditional fault-diagnosability tgp˜(FCQn)=(g+1)n−(g2)+1 of FCQn under PMC model. © 2022 Elsevier Inc.","Diagnosability; Extra-connectivity; Fault tolerance; Folded crossed cubes; PMC model"
"ULSED: An ultra-lightweight SED model for IoT devices","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129932549&doi=10.1016%2fj.jpdc.2022.04.007&partnerID=40&md5=9f791fc47135460c308147b56a111862","Sound event detection (SED) technology has been widely used in applications such as audio surveillance systems and smart home. Compared to the traditional machine learning methods, the neural networks (NN) based methods have been proposed in recent years to significantly improve the detection accuracy. However, a major issue of the NN-based SED models is that they often involve a large number of parameters and floating point operations (FLOPs), resulting in significant processing time, power consumption and memory storage. This poses a challenge to SED on IoT devices with constrained computational resources and power budget. To address this issue, in this work, an ultra-lightweight SED model (ULSED) with a selective separable convolution scheme and a coordinate attention scheme is proposed to significantly reduce the computational complexity while achieving high detection accuracy. The proposed ULSED model is evaluated on the ESC-10, ESC-50 and UrbanSound8K(US8K) datasets. Compared with several state-of-the-art models, the number of parameters and the number of FLOPs is significantly reduced by up to 388 times and 1140 times while achieving high detection accuracy of 97.0%, 88.3% and 83.5% on the ESC-10, ESC-50 and US8K respectively. The proposed ULSED model is suitable for power- and hardware-constrained IoT devices. © 2022 Elsevier Inc.","IoT; Sound event detection; Ultra-lightweight CNN model"
"Coordinate-based efficient indexing mechanism for intelligent IoT systems in heterogeneous edge computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129334813&doi=10.1016%2fj.jpdc.2022.04.012&partnerID=40&md5=fc1845e55776ec893136fe3d0d4dcbbd","Powered by edge servers (also called as edge nodes) which are close to the data source, distributed edge AI processes the huge amounts of data generated by Internet of Things (IoT) devices, extracting value for users. In edge computing, massive data are stored in several distributed edge nodes with heterogeneous capabilities. Intelligent applications running on one edge node may need data from other edge nodes. An efficient data indexing mechanism can rapidly locate the edge node where the data is kept, supporting latency-sensitive intelligent applications. The existing indexing methods in edge computing assume that all edge nodes are the same in capability and the number of edge nodes is constant. This paper proposes CREIM, a coordinate-based efficient indexing mechanism for intelligent IoT systems in heterogeneous edge computing. CREIM achieves fair load balancing on edge nodes with heterogeneous capabilities. The indexing mechanism deals well with the horizontal scaling of edge nodes. Besides, CREIM addresses a fast lookup with one overlay hop, providing low latency data retrieval for edge intelligent applications. In the experiments, CREIM is applied in a realistic network simulated by the mininet and the routing forwarding is supported by the P4 switch. The experiments are constructed by combining real location datasets of Shanghai Telecoms base stations with the real-collected requests of end-devices. The experimental results demonstrate that CREIM achieves a near-optimal latency of index-lookup, adapts the heterogeneous capabilities among edge nodes and reduces the cost of increasing/decreasing edge nodes by 56.36% compared with the state-of-the-art method. © 2022 Elsevier Inc.","Distributed storage; Heterogeneous edge computing; Indexing mechanism; Internet of Things system"
"Workflow simulation and multi-threading aware task scheduling for heterogeneous computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131463400&doi=10.1016%2fj.jpdc.2022.05.011&partnerID=40&md5=17c38a84da63a5d743f0046252b76bc5","Efficient application scheduling is critical for achieving high performance in heterogeneous computing systems. This problem has proved to be NP-complete even for the homogeneous case, heading research efforts in obtaining low complexity heuristics that produce good quality schedules. Such an example is HEFT, one of the most efficient list scheduling heuristics in terms of makespan and robustness. In this paper, we propose two task scheduling methods for heterogeneous computing systems that can be integrated to several task scheduling algorithms. First, a method that improves the scheduling time (the time for obtaining the output schedule) of a family of task scheduling algorithms is delivered without sacrificing the schedule length, when the computation costs of the application tasks are unknown. Second, a method that improves the scheduling length (makespan) of several task scheduling algorithms is proposed, by identifying which tasks are going to be executed as single-threaded and which as multi-threaded implementations, as well as the number of the threads used. We showcase both methods by using HEFT popular algorithm, but they can be integrated to other algorithms too, such as HCPT, HPS, PETS and CPOP. The experimental results, which consider 14580 random synthetic graphs and five real world applications, show that by enhancing HEFT algorithm with the two proposed methods, significant makespan gains and high scheduling time gains, are achieved. © 2022 The Author(s)","HEFT; Heterogeneity; Makespan; Scheduling time; Task scheduling"
"CP-SGD: Distributed stochastic gradient descent with compression and periodic compensation","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133268023&doi=10.1016%2fj.jpdc.2022.05.014&partnerID=40&md5=209aa2f834740609ec0270e0cbeec8b4","Communication overhead is the key challenge for distributed training. Gradient compression is a widely used approach to reduce communication traffic. When combined with a parallel communication mechanism method like pipeline, gradient compression technique can greatly alleviate the impact of communication overhead. However, there exist two problems of gradient compression technique to be solved. Firstly, gradient compression brings in extra computation cost, which will delay the next training iteration. Secondly, gradient compression usually leads to a decrease in convergence accuracy. In this paper, we combine parallel mechanism with gradient quantization and periodic full-gradient compensation, and propose a new distributed optimization method named CP-SGD, which can hide the overhead of gradient compression, overlap part of the communication and obtain high convergence accuracy. The local update operation in CP-SGD allows the next iteration to be launched quickly without waiting for the completion of gradient compression and the current communication process. Besides, the accuracy loss caused by gradient compression is solved by k-step correction method introduced in CP-SGD, which provides a gradient correction every k iterations. We prove that CP-SGD has a convergence guarantee and it achieves at least [Formula presented] convergence rate, where K is the number of iterations. We conduct extensive experiments on MXNet to verify the convergence properties and scaling performance of CP-SGD. Experimental results on a 32-GPU cluster show that convergence accuracy of CP-SGD is close to or even slightly better than that of S-SGD, and its end-to-end time is 30% less than 2-bit gradient compression under a 56Gbps bandwidth environment. In addition, we analyze the performance of CP-SGD when training on 8, 16 and 32 GPUs. It is found that CP-SGD is suitable for most compression-supported update algorithms, and its scalability is approximately linear. © 2022 Elsevier Inc.","Communication mechanism optimization; Distributed communication optimization; Gradient compression"
"MB-MaaS: Mobile Blockchain-based Mining-as-a-Service for IIoT environments","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132214562&doi=10.1016%2fj.jpdc.2022.05.008&partnerID=40&md5=64d75c54d70b73c6256a3f8f0eb9487f","In this paper, we propose a mobile blockchain (MB) based mining-as-a-service (MaaS) scheme, MB-MaaS for resource-constrained industrial internet-of-things (IIoT) environments. The scheme addresses the research gaps of fixed static allocation for miners to perform computationally intensive mining tasks through a multi-hop computational offloading (CO) scheme and addresses an auction mechanism for a fair bidding process among the miner nodes. The scheme operates in three phases. In the first phase, a multi-hop CO scheme with a fair incentive policy is formulated for miners. The CO schemes offer guaranteed offloading services to mobile devices from far-edge systems through a chain of neighbor nodes. Then, in the second phase, MaaS is proposed to leverage expensive mining tasks through 5G-enabled pico/femtocells. Integration of 5G allows massive end-to-end device and service connectivity. As IIoT ecosystems have limited memory and compute requirements, MaaS assures that the proposed consensus has a responsive validation and mining time. To make the data exchange in the consensus process lightweight, and allow a large number of sensors to share the data in a lightweight manner, an effective consensus mechanism Lightweight Proof-of-Proximity (LPoP), is proposed that forms group validations instead of single block validation. The data is exchanged through javascript object notation (JSON) format, maintaining a steady transaction rate. MB-MaaS is compared against the existing scheme for parameters bid thresholds and request servicing times, and mining and consensus formation. For example, the request serving time at 12 requests is improved by 56.78%, and a significant improvement of 26.47% is observed for processed blocks; parsing time on average is improved by 7.89%. The comparative analysis suggests that the scheme is more efficient than other competing approaches. © 2022 Elsevier Inc.","5G; Data offloading; Mining-as-a-Service; Mobile blockchain; Mobile edge computing"
"A blockchain-orchestrated deep learning approach for secure data transmission in IoT-enabled healthcare system","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140305039&doi=10.1016%2fj.jpdc.2022.10.002&partnerID=40&md5=0e83b55f2229c8f64bc0faf150fe6b6e","The integration of the Internet of Things (IoT) with traditional healthcare systems has improved quality of healthcare services. However, the wearable devices and sensors used in Healthcare System (HS) continuously monitor and transmit data to the nearby devices or servers using an unsecured open channel. This connectivity between IoT devices and servers improves operational efficiency, but it also gives a lot of room for attackers to launch various cyber-attacks that can put patients under critical surveillance in jeopardy. In this article, a Blockchain-orchestrated Deep learning approach for Secure Data Transmission in IoT-enabled healthcare system hereafter referred to as “BDSDT” is designed. Specifically, first a novel scalable blockchain architecture is proposed to ensure data integrity and secure data transmission by leveraging Zero Knowledge Proof (ZKP) mechanism. Then, BDSDT integrates with the off-chain storage InterPlanetary File System (IPFS) to address difficulties with data storage costs and with an Ethereum smart contract to address data security issues. The authenticated data is further used to design a deep learning architecture to detect intrusion in HS network. The latter combines Deep Sparse AutoEncoder (DSAE) with Bidirectional Long Short-Term Memory (BiLSTM) to design an effective intrusion detection system. Experiments on two public data sources (CICIDS-2017 and ToN-IoT) reveal that the proposed BDSDT outperformed state-of-the-arts in both non-blockchain and blockchain settings and have obtained accuracy close to 99% using both datasets. © 2022 The Author(s)","Blockchain; Deep learning; Healthcare systems; Internet of Things; Zero Knowledge Proof"
"Simulating structural plasticity of the brain more scalable than expected","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138446022&doi=10.1016%2fj.jpdc.2022.09.001&partnerID=40&md5=b661fe8e309ecca9d77f6b71f96d734c","Structural plasticity of the brain describes the creation of new and the deletion of old synapses over time. Rinke et al. (JPDC 2018) introduced a scalable algorithm that simulates structural plasticity for up to one billion neurons on current hardware using a variant of the Barnes–Hut algorithm. They demonstrate good scalability and prove a runtime complexity of O(nlog2⁡n). In this comment paper, we show that with careful consideration of the algorithm and a rigorous proof, the theoretical runtime can even be classified as O(nlog⁡n). © 2022 Elsevier Inc.","Barnes–Hut; Brain; Connectome; Large-scale; Simulation"
"TERMS: Task management policies to achieve high performance for mixed workloads using surplus resources","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138789691&doi=10.1016%2fj.jpdc.2022.08.005&partnerID=40&md5=70e5aab1fb9327fa56cde03029716a8a","Resource contentions and performance interferences can lead to workload performance degradation in mixed-workload deployment clusters. Previous work guarantees the resource requirements of latency-sensitive tasks and reduces performance losses to batch jobs by reclaiming surplus resources from over-provisioned tasks. While the fragmentation of resources leads to a mismatch between provisioned resources and task requirements, resulting in high operation overheads and losses of task fairness. This paper proposes TERMS, the task management policies based on task relevance, resource distribution, and task fairness to achieve efficient and low-cost task management. TERMS mainly includes three types of management policies. The task scheduling policy can schedule new tasks according to task relevance. Task selection strategies select tasks for resource provisioning and task resumption based on resource requirements and task fairness. If necessary, the node selection strategy can be used to choose befitting target nodes based on task relevance and node resource information for task migration when eliminating straggler tasks. Evaluation results show that TERMS can further improve the performance of latency-sensitive services and batch jobs, reduce management overheads, and avoid operation failures. © 2022 Elsevier Inc.","Resource management; Resource reclamation; Surplus resource; Task management policy; Task selection"
"Fault-tolerability of the hypercube and variants with faulty subcubes","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130407965&doi=10.1016%2fj.jpdc.2022.05.001&partnerID=40&md5=191f5a081a8bf3b0acf8a3bb31043dc5","A faulty vertex may probably affect its neighbors and further causes them being faulty, which makes a subnetwork (structure) fail. Therefore, looking into the effect caused by some structures becoming faulty is meaningful. The connectivity and diagnosability are two important parameters to evaluate the fault-tolerability of networks. The connectivity of a network is the minimum number of vertices whose removal will disconnect the network or trivial. We call the network to be t-diagnosable if the number of faulty vertices does not exceed t and all faulty vertices can be identified without a replacement. Let Qn be the n-dimensional hypercube and EH(s,t) be the exchanged hypercube, which is the variant of hypercube. In this paper, we study connectivity, diagnosability, and 1-good-neighbor conditional diagnosability based on structure faults, respectively. Specifically, we first determine the connectivity (1≤k≤n−1 and n≥3) and diagnosability (1≤k≤n−1 and n≥4) of Qn−Qk under the PMC model. Then, we determine the connectivity (2≤k≤min⁡{s,t}) and diagnosability (2≤k≤min⁡{s,t} and min⁡{s,t}≥3) of EH(s,t)−Qk under the PMC model. Finally, we show that the 1-good-neighbor conditional diagnosability of Qn−Qk is 2n−3 for n≥5 and 1≤k≤n−1 under the PMC model, which is almost twice as the traditional diagnosability for a large n. © 2022 Elsevier Inc.","Connectivity; Diagnosability; Exchanged hypercube; Hypercube; PMC model"
"Cohort-based federated learning services for industrial collaboration on the edge","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129742911&doi=10.1016%2fj.jpdc.2022.04.021&partnerID=40&md5=3b64365ab9911513320b2439274ac196","Machine Learning (ML) is increasingly applied in industrial manufacturing, but often performance is limited due to insufficient training data. While ML models can benefit from collaboration, due to privacy concerns, individual manufacturers often cannot share data directly. Federated Learning (FL) enables collaborative training of ML models without revealing raw data. However, current FL approaches fail to take the characteristics and requirements of industrial clients into account. In this work, we propose an FL system consisting of a process description and a software architecture to provide FL as a Service (FLaaS) to industrial clients deployed to edge devices. Our approach deals with skewed data by organizing clients into cohorts with similar data distributions. We evaluated the system on two industrial datasets. We show how the FLaaS approach provides FL to client processes by considering their requests submitted to the Industrial Federated Learning (IFL) Services API. Experiments on both industrial datasets and different FL algorithms show that the proposed cohort building can increase the ML model performance notably. © 2022 Elsevier Inc.","Collaborative AI; Edge computing; Federated learning; Industrial collaboration; Service-based architecture"
"An optimized protocol for cost effective communication in a multi-agent environment","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133289355&doi=10.1016%2fj.jpdc.2022.06.010&partnerID=40&md5=8246cb851f2e7e6a673f4b10b1fc6612","Mobile agents can migrate and communicate through interaction messages in a multi-agent environment. A multi-agent environment should provide a mechanism where mobile agents can be tracked easily and communication among them can take place while consuming the least amount of resources. Among all the proposed schemes the home based and forward pointing based scheme seems to be the best in achieving the desired objectives. The analysis shows that besides offering some advantages the aforementioned approach suffers from using high amounts of memory, high transmission cost, and the single point of failure problems. In this research we propose managing forwarding pointers intelligently by removing them from hosts when they are not required, preventing unnecessary update messages when the agent's mobility goes high, calculating the shortest path instantly after the agent's migration, and acknowledging only high priority interaction messages. We have validated our proposed approach with the help of experiments on a simulation tool OCEMAgents. The results show that the proposed approach has achieved all the objectives that are claimed and it has reduced memory and network overhead by minimizing the usage of forwarding pointers and by reducing the transmission cost. Besides that the proposed approach can effectively handle the single point of failure problem. Based on the results, we can safely declare that the proposed approach can be cost-effective if it is implemented in a real multi-agent system. © 2022 Elsevier Inc.","Forwarding pointers; Message priority; Mobile agents; Shortest path; Transmission cost"
"Non-cooperative game algorithms for computation offloading in mobile edge computing environments","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140204203&doi=10.1016%2fj.jpdc.2022.10.004&partnerID=40&md5=043cb79670120ac273f7accddab13110","Mobile Edge Computing (MEC) has become a promising technology for 5G networks. Computation offloading is an essential issue of MEC, which enables mobile User Equipment (UE) to enjoy rich wireless resources and huge computing power anywhere. This paper considers the Quality-of-Experience (QoE) of UEs in 5G MEC systems and presents a dynamic non-cooperative game (QCOG-DG) algorithm and a static non-cooperative game (QCOG-SG) algorithm for computation offloading of MEC applications. We establish an MEC computation offloading model by considering the QoE requirements of UEs, and discuss the communication overheads, computation cost, and energy consumption models to minimize the energy consumption and time delay of each UE. Considering that there are multiple UEs who want to offload their computation tasks to a resource-constrained MEC server, and each UE is selfish and competitive, we formulate the problem of computation offloading decision as a non-cooperative game model. We prove the existence of a Nash Equilibrium (NE) solution for the proposed game model. In addition, we propose an algorithm that jointly optimizes energy consumption and time delay under QoE preferences to achieve optimal offloading benefits for each UE. Moreover, we respectively propose a dynamic non-cooperative game (QCOG-DG) algorithm and a static non-cooperative game (QCOG-SG) algorithm to efficiently find the NE solution. Extensive simulation experiments are conducted to verify the effectiveness of the proposed MEC computation offloading model and the QCOG-DG and QCOG-SG algorithms. Simulation results show that the proposed QCOG-DG algorithm can efficiently find the NE solutions in the MEC scenarios with UEs of different sizes. © 2022 Elsevier Inc.","5G networks; Computation offloading; Dynamic game; Mobile edge computing; Non-cooperative game"
"Reliability of the weight vector generation method of the multi-objective evolutionary algorithm and application","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134208851&doi=10.1016%2fj.jpdc.2022.06.016&partnerID=40&md5=1d25f0082a604085fdee7b8ad70f75fe","The decomposition-based multi-objective evolutionary algorithm first generates a set of weight vectors in advance, and it is very important to select a set of appropriate weight vectors for the decomposition-based algorithm. A variety of weight vector generation methods have been proposed in the existing algorithms, but in most algorithms, a pre-defined weight vector generation method is still used, the pre-defined weight vector is too specialized for the simplex-like front surface, which results in poor performance on the front surface with irregularities. At the same time, most of the existing algorithms have proposed many new adaptive strategies for weight vectors, but if you generate a set of more suitable weight vectors at the beginning, and then use the update strategy, it can make the algorithm achieve a better balance between diversity and convergence. In order to select a suitable weight vector, this paper proposes a multi-stage MOEA to select a suitable weight vector. The algorithm is divided into multiple stages according to the evolution process, first of all, in the early stage of evolution, the reliability of multiple weight vector generation methods was evaluated according to the spearman correlation coefficient in statistics, choose the most suitable weight generation method; Secondly, this method can be applied to the search for high-quality solutions in the middle of evolution; Finally, a weight vector adaptive strategy is adopted in the overall evolution process. In the experiment, the proposed algorithm was analyzed in the benchmark test problem, mechanical bearing and light aircraft gear reducer. The experimental results show the effectiveness of the proposed algorithm. © 2022 Elsevier Inc.","Adaptive weight vector adjustment; Decomposition; Evolutionary algorithm; Mechanical optimization; Multi-objective optimization"
"Agent coalitions for load balancing in cloud data centers","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140137245&doi=10.1016%2fj.jpdc.2022.10.006&partnerID=40&md5=e39c147139c9cce0981f4848d168dc00","The workload of Cloud data centers is constantly fluctuating causing imbalances across physical hosts that may lead to violations of service-level agreements. To mitigate workload imbalances, this work proposes a concurrent agent-based problem-solving technique supported by cooperative game theory capable of balancing workloads by means of live migration of virtual machines (VMs). Nearby agents managing physical hosts are partitioned into coalitions in which agents play coalitional games to progressively balance separate sections of a data center while considering the coalition's benefit of migrating a VM as well as its associated network overhead. Simulation results show that, in general, the proposed coalition-based load balancing mechanism outperformed a load balancing mechanism based on a hill-climbing algorithm used by a top data center vendor when considering altogether (i) the standard deviation of resource usage, (ii) the number of migrations, and (iii) the number of switch hops per migration. © 2022 Elsevier Inc.","Cloud computing; Cloud data centers; Cooperative game theory; Intelligent agents; Load balancing"
"Data stream clustering for low-cost machines","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129958801&doi=10.1016%2fj.jpdc.2022.04.009&partnerID=40&md5=1ae38754ea856957f197c5ccd75ef3c6","Nowadays, the operations performed by the Internet of Things (IoT) systems are no more trivial since they rely on more sophisticated devices than in the past. The IoT system is physically composed of connected computing, digital, mechanical devices such as sensors or actuators. Most of the time, each of them incorporates a logical arithmetic unit that can pre-compute or compute on the device. To extract value from the data produced at the edge, processing power offered by cloud computing is still utilized. However, streaming data to the cloud exposes some limitations related to the increased communication and data transfer, which introduces delays and consumes network bandwidth. Clustering data is one example of a treatment that can be executed in the cloud. In this paper, we propose a methodology for solving the data stream clustering problem at the edge. Data Stream clustering is defined as the clustering of data that arrive continuously, such as telephone records, multimedia data, sensors data, financial transactions, etc. Since we use low-cost and low-capacity devices, the objective is, given a sequence of points, to construct a good clustering of the stream using a small amount of memory and time. We propose a ‘windowing’ scheme, coupled with a sampling scheme to respect the objective. Under the experimental conditions, experiments show that the clustering solutions can be controlled, with difficulties for time-stamped data but not for random data or data with well-delimited clusters. The main advantage of our schema is that we are clustering data “on the fly” with no knowledge or assumption regarding the available data. We do not assume that all the data are known before a treatment batch by batch. Our schema also has the potential to be adapted to other classes of machine learning algorithms. © 2022 Elsevier Inc.","Edge AI; Experiments on heterogeneous and low cost hardware; Machine-learning algorithms; Online data stream clustering"
"Implementation of bio-inspired hybrid algorithm with mutation operator for robotic path planning","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134263175&doi=10.1016%2fj.jpdc.2022.06.014&partnerID=40&md5=97db0fff8e1349634bea657b7600399d","Path planning is an NP-hard problem that is aimed to satisfy multi-constraint optimization requirements. In autonomous robotics applications, path planning together with collision avoidance presents a challenging task. It necessitates the generation of possible search directions from a designated point to a fixed varying destination location satisfying spatial constraints. This paper presents a framework for the design of an intelligent multi-objective robotic path planning algorithm. The algorithm relies on the generation of way-points by hybridizing two meta-heuristics techniques, namely Grey Wolf Algorithm (GWO) and Particle Swarm Optimization (PSO). A frequency-based modification in GWO search operators is introduced to fasten the search process. An improvised search strategy is employed for collision detection and avoidance, which converts non-desired points into the desired point. Sensors are deployed around the robot vicinity for search optimization. Mutation operators are then introduced to improve path length by smoothing out the trajectory. The proposed algorithm's effectivity is then validated through extensive simulations, in which different condition environments are simulated. To validate the effectiveness of the proposed methodology, the results are compared with contemporary algorithms namely Minimum Angle Artificial Bee Colony (MAABC) algorithms, Hybrid Cuckoo Search-Bat Algorithm (BA-CSA), Bacterial Bolony (BC) and Genetic Algorithm (GA) algorithms. The results conclusively demonstrated that the proposed algorithm ensures effective performance in path smoothness and safety under a wide range of conditions. © 2022 Elsevier Inc.","Bio-inspire computing; Collisions detection; Heuristic; Particle swarm optimization; Path planning"
"Reinforcement learning for cost-effective IoT service caching at the edge","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132773066&doi=10.1016%2fj.jpdc.2022.06.008&partnerID=40&md5=4d9556536e5b2f8c4cbd0f24723683a1","In the edge computing environment, Internet of Things (IoT) application service providers can rent resources from edge servers to cache their service items such as datasets and code libraries, and thus significantly reducing the service request latency and the core network traffic. Since IoT service providers need to pay for the rented edge computing resources, it is essential to find a dynamical service caching strategy to minimize the service cost while optimizing the performance objective such as service latency reduction. However, most of the existing studies either overlooked the problem of collaborative service caching or failed to consider the system's long-term service cost and latency. In this paper, to address such a problem, we coordinate multiple edge servers to cache service items and formulate the collaborative service caching problem using a multi-agent multi-armed bandit model. Furthermore, we propose a utility-aware collaborative service caching (UACSC) scheme based on a multi-agent reinforcement learning. The UACSC scheme can coordinate multiple edge servers to make a dynamic joint caching decision, aiming at maximizing the system's long-term utility. To evaluate the performance of our proposed scheme, we implement four representative baseline algorithms and compare them with six different performance metrics. In addition, a real-world case study is also presented to demonstrate the effectiveness of the UACSC scheme. Comprehensive experimental results show that the UACSC scheme can effectively coordinate multiple edge servers to cache service items, and achieve higher service latency reduction and lower service cost compared with other baseline algorithms. © 2022 Elsevier Inc.","Collaborative service caching; Edge computing; Multi-agent multi-armed bandit model; Multi-agent reinforcement learning"
"Optimization of elliptic curve scalar multiplication using constraint based scheduling","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131429354&doi=10.1016%2fj.jpdc.2022.05.006&partnerID=40&md5=d56523d6eecbd054e95f8c9ffadfd5d9","Elliptic Curve Cryptography is public key cryptography that features smaller keys, ciphertexts, and signatures and is faster than RSA at the same security level. Scalar multiplication is the main and the most compute-intensive operation in the generation of keys. Point Addition, Doubling and Inversion are the basic operations for scalar multiplication. Inversion is a very expensive operation as compared to multiplication, addition and squaring in the finite fields with an affine coordinate system. López-Dahab coordinates are the best alternative to reduce the inversion overhead in scalar computation. Area, Delay and Power trade-offs are the main constraints in hardware implementations of scalar multiplication. In this paper, optimization of elliptic curve scalar multiplication using constraint-based scheduling for the López-Dahab coordinate system is proposed. Data dependency graphs of point addition and doubling are modified for optimization of area and delay. The proposed architecture is implemented on Altera Stratix-II FPGA. The constraint is applied on the field multiplication operation and the considerable area is reduced. The proposed architecture computes scalar multiplication in 11.43 μs and takes 9856 ALMs. The performance comparison with state of the art shows that area is reduced by 41.21%, delay is reduced by 2.4% and Area-Delay-Product is improved. © 2022 Elsevier Inc.","Constraint based scheduling; Elliptic curve cryptography; López-Dahab coordinates; Public key cryptography; Scalar multiplication"
"A two-level network-on-chip architecture with multicast support","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141539404&doi=10.1016%2fj.jpdc.2022.10.011&partnerID=40&md5=b49e22ce6ba3608f42fabac12ce94173","It is essential for implementing processing systems of edge computing, internet of things (IoT) and wireless multimedia sensor networks (WMSN) to use low-power parallel and distributed architectures with high speed and low power consumption. Most of the artificial intelligence and machine learning applications need to be executed on efficient distributed architectures with multicast support. In this paper, TLAM, a high-performance and low-cost NoC architecture is proposed. TLAM stands for Two-Level network-on-chip Architecture with Multicast support and includes a hybrid path/tree based multicast routing algorithm and a specific two-level mesh topology. The routing algorithm of TLAM is basically working according to the Hamiltonian paths. In the proposed architecture, the topology is partitioned and the two-level links provide an efficient infrastructure for low-cost and low-latency transmission of multicast and broadcast messages between partitions. In TLAM, in addition to multicasting routing algorithm, hardware components for handling multicasting packets are designed in order to achieve performance improvements. The goal is to improve the efficiency and performance, especially latency, while handling both unicast and multicast packets. Experimental evaluations including network-level and router-level analysis with different configurations under various traffic patterns were performed. The evaluations in terms of latency, throughput, area and power consumption, indicate that TLAM provides higher performance, especially for dense multicasting and broadcasting, in comparison with the existing architectures. © 2022 Elsevier Inc.","Hamiltonian principle; Multicast routing algorithm; Network-on-chip; Two-level mesh"
"Error-sensitive proof-labeling schemes","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129651907&doi=10.1016%2fj.jpdc.2022.04.015&partnerID=40&md5=9c7d7ca8c5f4e9c9c1915f133d3daf93","Proof-labeling schemes are known mechanisms providing nodes of networks with certificates that can be verified locally by distributed algorithms. Given a boolean predicate on network states, such schemes enable to check whether the predicate is satisfied by the actual state of the network, by having nodes interacting with their neighbors only. Proof-labeling schemes are typically designed for enforcing fault-tolerance, by making sure that if the current state of the network is illegal with respect to some given predicate, then at least one node will detect it. Such a node can raise an alarm, or launch a recovery procedure enabling the system to return to a legal state. In this paper, we introduce error-sensitive proof-labeling schemes. These are proof-labeling schemes which guarantee that the number of nodes detecting illegal states is linearly proportional to the Hamming distance between the current state and the set of legal states. By using error-sensitive proof-labeling schemes, states which are far from satisfying the predicate will be detected by many nodes. We provide a structural characterization of the set of boolean predicates on network states for which there exist error-sensitive proof-labeling schemes. This characterization allows us to show that classical predicates such as, e.g., cycle-freeness, and leader admit error-sensitive proof-labeling schemes, while others like regular subgraphs do not. We also focus on compact error-sensitive proof-labeling schemes. In particular, we show that the known proof-labeling schemes for spanning tree and minimum spanning tree, using certificates on O(log⁡n) bits, and on O(log2⁡n) bits, respectively, are error-sensitive, as long as the trees are locally represented by adjacency lists, and not just by parent pointers. © 2022 Elsevier Inc.","Distributed decision; Distributed property testing; Fault-tolerance"
"Malware detection using image representation of malware data and transfer learning","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140319497&doi=10.1016%2fj.jpdc.2022.10.001&partnerID=40&md5=0dba4c8f2643bb0d735ced5c4cdbfc2a","With the increased proliferation of internet-enabled mobile devices and large internet use, cybercrime incidents have grown exponentially, often leading to huge financial losses. Most cybercrimes are launched through malware attacks, phishing attacks, denial/distributed denial of attacks, looting people's money, stealing credential information for unauthorized use, personal identity thefts, etc. Timely detection of malware can avoid such damage. However, it requires an efficient and effective approach to detecting such attacks. This study attempts to devise a malware detection approach using transfer learning and machine learning algorithms. A hybrid approach has been adopted where pre-trained models VVG-16 and ResNet-50 extract hybrid feature sets from the data to be used with the machine learning algorithms. In doing so, this study contrives the Bi-model architecture where the same models are combined sequentially in the stacked form to obtain higher performance as the output of the first model is used to train the second model. With the Bi-model structure, 100% accuracy is obtained for a 25 classes problem. Performance comparison with state-of-the-art models and T-test proves the superior performance of the proposed approach. © 2022 Elsevier Inc.","Hybrid features; Malware detection; Stacked model; Transfer learning"
"Fine-grain data classification to filter token coherence traffic","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138449447&doi=10.1016%2fj.jpdc.2022.09.004&partnerID=40&md5=714dd048f851e55b8db1fc89abc2446f","Snoop-based cache coherence protocols perform well in small-scale systems by enabling low latency cache-to-cache data transfers in just two-hop coherence transactions. However, they are not a scalable alternative as they require frequent broadcast of coherence requests. Token coherence protocols were proposed to improve the scalability of snoop-based protocols by removing a large amount of traffic due to broadcast responses. Still, broadcasting coherence requests on every cache miss represents a scalability issue for medium and large-scale systems. In this paper, we propose to reduce the number of broadcast operations in Token coherence protocols by performing an efficient fine-grain private-shared data classification and disabling broadcasts for misses to data classified as private. Our fine-grain classification is orchestrated and stored by the Translation Look-aside Buffers (TLBs), where entries are kept for a longer time than in local caches. We explore different classification granularity accounting for different storage overheads and their impact on filtering coherence traffic. We evaluate our proposals on a set of parallel benchmarks through full-system cycle-accurate simulation and show that a subpage-grain classification offers the best trade-off when accounting for storage, traffic, and performance. When running a 16-core configuration, our subpage-grain classification eliminates 40.1% of broadcast operations compared to not performing any classification and 13.7% of broadcast operations more than a page-grain data classification. This reduction translates into less network traffic (16.0%), and finally, performance improvements of 12.0% compared to not having a classification mechanism. © 2022 Elsevier Inc.","Cache coherence; Multicore; Private-shared data classification; Token protocol"
"How to achieve adaptive security for asynchronous BFT?","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134874223&doi=10.1016%2fj.jpdc.2022.07.004&partnerID=40&md5=77160118996961f70484d2bf171e3052","We consider how to build practical asynchronous BFT protocols with adaptive security. In particular, we build two protocols in both the computational setting (where the adversary is limited to polynomial-time) and the stronger information-theoretic model (where the adversary is unbounded). In the computational model, we provide EPIC using adaptively secure key generation and common coin protocols. In the information-theoretical model, we provide HALE leveraging the classic local coin protocol of Bracha. HALE is more robust than EPIC and does not need distributed key generation. Via a five-continent deployment on Amazon EC2, we show EPIC is slightly slower for small and medium-sized networks than the most efficient asynchronous BFT protocols with static security. As the number of replicas is smaller than 46, EPIC's throughput is stable, achieving peak throughput of 8,000–12,500 tx/sec with a transaction size of 250 bytes. When the network size grows larger, EPIC is not as efficient as asynchronous BFT protocols with static security, with throughput of 4,000–6,300 tx/sec. We also show while HALE is in general less efficient than EPIC, HALE is reasonably fast, achieving 42,000 tx/sec and 3,400 tx/sec for the 4-server setting in the LAN/WAN environments, respectively. Remarkably, HALE outperforms EPIC in LANs when the number of replicas is smaller than 16. © 2022 Elsevier Inc.","Adaptively secure BFT; Asynchronous Byzantine fault tolerance; Common coin; Local coin; Threshold cryptography"
"Path planning mechanism for mobile anchor-assisted localization in wireless sensor networks","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127349463&doi=10.1016%2fj.jpdc.2022.03.015&partnerID=40&md5=7a99fcd08896ad1a58c0c354324db2d2","The detection of a deployed sensor node location plays a vital role in all domains of Wireless Sensor Networks since its location provides the source of the information. Many approaches have been proposed in recent years. One of these approaches uses a single location-aware node, known as the Anchor or Beacon, that exchanges information with other nodes to help in their position estimation. Although this approach provides good accuracy, it creates new challenges in the picture. The key issue is to design an optimal trajectory for the anchor node in order to achieve maximum localization accuracy in the shortest amount of time. In this paper, we propose a static trajectory for a mobile beacon node for localization in WSNs. This trajectory ensures that the localization success ratio can be improved for lower communication range of an anchor node with higher localization precision and minimum localization time as compared to other static models. The proposed trajectory also overcomes the collinearity problem and ensures that all the unknown sensor nodes receive good quality beacon positions for position estimation. © 2022 Elsevier Inc.","Mobile anchor node; Static trajectory; Wireless sensor networks"
"Application-aware adaptive parameter control for LoRaWAN","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129689088&doi=10.1016%2fj.jpdc.2022.04.023&partnerID=40&md5=9f060701af0d119e13d1d90ed2fe2ba7","Advanced wireless communication technologies are leading towards large-scale, geographically distributed systems, which consist of thousands of co-existing devices with potentially conflicting application requirements, such as high data delivery ratio and low power consumption. At the same time, devices are challenged by varying environmental conditions, which can also lead to degradation in network performance. To ensure the overall network performance and service stability offered by individual devices, the devices must co-exist and adapt to changes in their surrounding environment. In this paper, an application-aware adaptive method is proposed to allow individual devices to dynamically and automatically decide and configure its communication parameters according to its application requirements and environmental conditions. The algorithm aims to enhance and maintain the network performance over time, while allowing every device to satisfy its application requirements. The algorithm is realised on the modern LoRaWAN network protocol. Simulated experiments confirm the proposed algorithm's ability to adapt to changing application requirements at runtime while maintaining network performance. They demonstrate an improvement in packet delivery ratio, energy consumption, and the novel ability to respond to an application's individual performance requirements. © 2022 Elsevier Inc.","Large-scale networks; LPWAN; Network dynamics; Run-time adaptive"
"A stochastic mobility model for traffic forecasting in urban environments","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128186008&doi=10.1016%2fj.jpdc.2022.03.005&partnerID=40&md5=6b2e4095c7d12cb43066bc3edcc0f65a","With the steadily growing traffic demand, urban traffic congestion is becoming a critical issue threatening several factors, including public safety, emissions of greenhouse gas, and transport inefficiencies. Thus, intelligent transport systems (ITS) have emerged as a promising solution to easing the burden of congestion. ITS rely on different technologies such as VANET (Vehicular Adhoc Networks) which provide the transportation system with ubiquitous connectivity allowing the exchange of traffic information between vehicles and roadside terminals. This can support numerous smart mobility applications such as traffic signal control and real-time traffic management. Hence, mobility models were developed to emulate and forecast the distribution of traffic which will be helpful to the design and management of traffic control strategies. In this context, this study specifically concentrates on developing a mobility model that reflects vehicular activities in urban environments based on vehicular information collected using vehicular communications. The behavior of vehicles along multi-lane roads and intersections is modeled as a stochastic process using queuing theory. Particularly, the queue system is analyzed as a continuous-time Markov chain (CTMC) and by calculating the steady-state probabilities, different performance measures are derived and analyzed under various scenarios. To validate the model, the obtained forecasts are compared with a queue model and realistic traces. The results show that the model is capable of reproducing the realistic behavior of traffic in urban roads without incurring heavy costs and time-consuming computing. The obtained estimates were then used to design an actuated traffic light and a vehicle speed adaptor. From the simulation results, it is clear that using the proposed traffic forecasting model helps reduce vehicles idling and travel times. © 2022 Elsevier Inc.","ITS; Markov chain; Mobility model; Queuing theory; VANET"
"Architecture slack exploitation for phase classification and performance estimation in server-class processors","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134571894&doi=10.1016%2fj.jpdc.2022.06.017&partnerID=40&md5=4215b58f1c005819275edc860a94ca3a","In this paper, we present a highly accurate performance estimation methodology that accounts for architecture slack in workloads. Our work leverages the advanced instrumentation available in POWER8 processor that monitors core pipeline activity in relation to off-core memory accesses to build metrics for architecture slack characterization for workloads. Using these metrics, we construct a workload classifier that classifies workloads as core-bound and memory-bound and propose a performance prediction model for change in processor frequency for each class of workload – cPerf and mPerf, respectively. We evaluated these models with SPECCPU and PARSEC benchmark suites on a POWER8 based OpenPOWER system. We observed that the predicted performance with our models has high accuracy (97%) for both CPU and memory intensive benchmarks. We validated that the classifier is suitable to accurately classify phase of workloads during execution intervals. We developed an algorithm that uses classifier for phase classification and prediction models for performance estimation at runtime. We leveraged this algorithm and evaluated the execution time impacts of CPU and memory classified benchmarks. © 2022 Elsevier Inc.","Performance prediction; Phase classification; Workload classification"
"A secure data transmission scheme based on multi-protection routing in datacenter networks","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131442098&doi=10.1016%2fj.jpdc.2022.05.010&partnerID=40&md5=e35c994bfb60bc04a1ce893f21e7e9f3","The adoption of protection routing guarantees the existence of a loop-free alternate path for packet forwarding when a single link or node failure occurs. By Tapolcai's method, the presence of two completely independent spanning trees (CISTs for short) suffices to configure a protection routing. This article extends the idea of protection routing to involve more CISTs and attach a secure mechanism in the configuration, which we call the secure multi-protection routing scheme (SMPR-scheme). Then, we use the SMPR-scheme to deal with privacy-preserving data transmissions, such as downloading personal medical records, tax bills, or other private information. To evaluate the effectiveness of the SMPR-scheme, we develop a probabilistic model that allows some malicious nodes to collect information illegally through neighboring access. We experimented with SMPR-scheme on the BCube (i.e., the generalized hypercube) datacenter network. Simulation results show that data transmission using SMPR-scheme ensures confidentiality (i.e., no node other than the recipient can receive the complete message) and effectively resists privacy collection even under malicious infringement. © 2022 Elsevier Inc.","BCube; Completely independent spanning trees (CISTs); Confidentiality; Datacenter networks (DCNs); Secure multi-protection routing"
"Strategic investments in distributed computing: A stochastic game perspective","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135706784&doi=10.1016%2fj.jpdc.2022.07.012&partnerID=40&md5=44bccf3490663139c10172263e2e55a7","We study a stochastic game with a dynamic set of players, for modeling and analyzing their computational investment strategies in distributed computing. Players obtain a certain reward for solving a problem, while incurring a certain cost based on the invested time and computational power. We present our framework while considering a contemporary application of blockchain mining, and show that the framework is applicable to certain other distributed computing settings as well. For an in-depth analysis, we consider a particular yet natural scenario where the rate of solving the problem is proportional to the total computational power invested by the players. We show that, in Markov perfect equilibrium, players with cost parameters exceeding a certain threshold, do not invest; while those with cost parameters less than this threshold, invest maximal power. We arrive at an interesting conclusion that the players need not have information about the system state as well as each others' parameters, namely, cost parameters and arrival/departure rates. With extensive simulations and insights through mean field approximation, we study the effects of players' arrival/departure rates and the system parameters on the players' utilities. © 2022 Elsevier Inc.","Blockchain mining; Distributed computing; Markov perfect equilibrium; Stochastic games"
"Utilization prediction-based VM consolidation approach","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136665959&doi=10.1016%2fj.jpdc.2022.08.001&partnerID=40&md5=121878c787d494607cee5888096be3a5","Reducing energy consumption and optimizing resource usage in large cloud data centers is still an essential target for the current researchers and cloud providers. The state-of-the-art highlights the effectiveness of VM consolidation and live migrations in achieving reasonable solutions. However, most proposals consider only the real-time workload variations to decide whether a host is overloaded or underloaded, or to trigger migration actions. Such approaches may apply frequent and needless VM migrations leading to energy waste, performance degradation, and service-level agreement (SLA) violations. In this paper, we propose a consolidation approach based on the resource utilization prediction to determine the overloaded and underloaded hosts. The prediction method combines a Kalman filter and support vector regression (SVR) to forecast the host's future CPU utilization. Simulations are conducted on Cloudsim using real PlanetLab workloads to verify the performance of our proposal against existing benchmark algorithms. Experimental results demonstrate that our consolidation technique significantly reduces the SLA violation rate, number of VM migrations, and energy consumed in the datacenter. © 2022 Elsevier Inc.","Cloud computing; Kalman filter; Support vector regression; Utilization prediction; VM consolidation"
"POCache: Toward robust and configurable straggler tolerance with parity-only caching","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130410691&doi=10.1016%2fj.jpdc.2022.05.004&partnerID=40&md5=79619686952eab40052d696413d5ea79","Stragglers (i.e., nodes with slow performance) are prevalent and incur performance instability in large-scale storage systems, yet it is challenging to detect stragglers in practice. We make a case by showing how erasure-coded caching provides robust straggler tolerance without relying on timely and accurate straggler detection, while incurring limited redundancy overhead in caching. We first analytically motivate that caching only parity blocks can achieve effective straggler tolerance. To this end, we present POCache, a parity-only caching design that provides robust straggler tolerance. To limit the erasure coding overhead, POCache slices blocks into smaller subblocks and parallelizes the coding operations at the subblock level. It further adopts a configurable straggler-aware cache algorithm (CSAC) that takes into account both file access popularity and straggler estimation to decide which parity blocks should be cached. CSAC enables POCache to configure various cache admission and eviction algorithms with straggler awareness and supports cache prefetching. We implement a POCache prototype atop Hadoop 3.1 HDFS, while preserving the performance and functionalities of normal HDFS operations. Extensive experiments on both local and Amazon EC2 clusters show that in the presence of stragglers, POCache can reduce the read latency by up to 87.9% compared to vanilla HDFS. © 2022 Elsevier Inc.","Caching; Erasure coding; Stragglers"
"Efficient hierarchical hash tree for OpenFlow packet classification with fast updates on GPUs","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130159006&doi=10.1016%2fj.jpdc.2022.04.018&partnerID=40&md5=9e197f0120095986151e21fd9f48fa55","Packet classification is an important functionality of modern routers/switches, needed in packet forwarding, Quality of Service (QoS), firewall etc. In order to better utilize routers on the Internet, Software Defined Network (SDN) decouples control plane from data plane to fulfill centralized management. Based on OpenFlow standards, packet classification in SDN is designed for multi-field rules which are more complex than traditional 5-tuple rules. In the paper, we propose a novel packet classification algorithm, called hierarchical hash tree (H-HashTree), based on the two IP address fields and the 7 exact-match fields to partition rules into groups. An extended Bloom filter is also proposed to accelerate search process by skipping groups in the hash tree. To further improve the performance, H-HashTree is implemented on GPU. We tested on 100K rules including synthesized rules containing characteristics of ACL, FW, and IPC with different wildcard ratios in exact-match fields, and real OpenFlow rules from Open vSwitch. Compared with the existing state-of-the-art algorithms, CutTSS and TabTree [19][18], H-HashTree achieves the best performance on both search and update speeds. H-HashTree achieves 1.17-13.9 and 2.48-12.7 times faster in search speed and 2.03-6.0 and 1.87-4.53 times faster in rule updates from synthesized rulesets than CutTSS and TabTree, respectively. On the GPU platform, H-HashTree can achieve up to 114 MPPS in search speed and less than 0.04 usec/rule in rule updates. © 2022 Elsevier Inc.","GPU; Hash table; OpenFlow; Packet classification"
"Energy-saving optimization of application server clusters based on mixed integer linear programming","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139328547&doi=10.1016%2fj.jpdc.2022.09.009&partnerID=40&md5=3e842e8d6950770b06a1c1176f0c8c7a","The issue of how to dynamically optimize the deployment of an application server cluster according to the changing load to reduce energy consumption is an important problem that must be urgently solved. In this paper, we propose an energy-saving optimization strategy for application server clusters, whose optimization content includes the on/off state, CPU frequency, and load size of each server. Compared with existing research, our strategy is not only more accurate in power and load models but also considers the switching cost of servers to avoid server switching jitter. The strategy includes two schemes, which both formulate the cluster energy-saving optimization as a mixed integer linear programming (MILP) problem and then adopt a toolkit to solve the problem. One scheme defines variables for each server, and the resulting programming problem is called the MILP4PH problem. The other scheme defines variables for each server type, resulting in a programming problem called the MILP4GH problem. The experimental results reveal that for clusters with poor homogeneity, the MILP4PH problem has fewer variables and can be solved in real time, while for clusters with good homogeneity, the MILP4GH problem has fewer variables and can be solved in real time. © 2022 Elsevier Inc.","Application server cluster; Energy-saving optimization; Homogeneity; Mixed integer linear programming; Switching jitter"
"Intelligent user-collaborative edge device APC-based MEC 5G IoT for computational offloading and resource allocation","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135539140&doi=10.1016%2fj.jpdc.2022.07.007&partnerID=40&md5=d20325a1d132b29d945be0782e0bdc60","Mobile edge computing (MEC) supports delay-sensitive and excellent processing capacity services on the 5G Internet of Things (IoT) network. This research proposes an intelligent resource allocation policy to minimize average service latency and average energy consumption for an IoT system while maximizing the available processing capacity (APC). We express the APC as a function of communication and computation resources at the user and task edge devices. We demonstrated that reducing execution delay and energy usage could improve overall system service performance. We evaluate the savings in average latency and average energy consumption when we reduce execution delay by allocating resources to maximize APC. The 5G IoT network uses natural actor-critic deep reinforcement learning to tackle complicated resource allocation decisions, and the simulation shows that reducing execution time improves overall system performance. Our results improved execution time and energy usage compared to random search, greedy search, and deep Q-learning. In addition, our single centralized agent DRL outperforms Multi-agent DRL for the number of rewards and completed task achievable under different episodes. © 2022 Elsevier Inc.","Available processing capacity (APC); Internet of Things (IoT); Mobile edge computing (MEC); Natural actor-critic deep reinforcement learning; Resource allocation"
"MIDP: An MDP-based intelligent big data processing scheme for vehicular edge computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129362471&doi=10.1016%2fj.jpdc.2022.04.013&partnerID=40&md5=a99d3cf3e42c2a9caf43344ed522a058","The number of Vehicle Equipment (VE) connected to the Internet is increasing, and these VEs generate tasks that contain large amounts of data. Processing these tasks requires a lot of computing resources. Therefore, it is a promising issue that offloading compute-intensive tasks from resource-limited vehicles to Vehicular Edge Computing (VEC) servers, which involves big data transmission, processing and computation. In a network, multiple providers provide VEC servers. When a vehicle generates a task, our goal is to make an intelligent decision on whether and when to offload this task to VEC servers to minimize the task completion time and total big data processing time. When each vehicle passes VEC servers, the vehicle can decide to offload its task to the VEC server in the current communication range, or continue to drive until it reaches the next server's communication range. This issue can be considered as an asset selling problem. It is a challenging issue to make a smart decision for the vehicle with a location view because the vehicle is not sure when the next VEC server will be available and how much about the available computing capacity of the next VEC server. Firstly, this paper formulates the problem as a Markov Decision Process (MDP), defines and analyzes the state set, action set, reward model, and state transition probability distribution. Then it uses Asynchronous Advantage Actor-Critic (A3C) algorithm to solve this MDP problem, builds the various elements of the A3C algorithm, uses Actor (the strategy function) to generate two actions of the vehicle: offloading and moving without offloading. Thirdly, it uses Critic (the value function) to evaluate Actor's behavior, and guide Actor's actions in subsequent stages. The Actor starts from the initial state in the state space until it enters the termination state, forming a complete decision-making process. It minimizes the completion time of task offloading through learning thereby reducing the delay of big data processing. Compared to the Immediately Offload (IO) scheme and Expect Offload (EO) scheme, the MIDP scheme proposed in this paper reduces the average task offloading delay to 29.93% and 29.99%, close to the EO scheme in terms of task completion rate and up to 66.6% improvement compared to the IO scheme. © 2022 Elsevier Inc.","Big data; Delay-aware; Energy efficient; Task offload; Unmanned aerial vehicles"
"A fast parallel max-flow algorithm","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134848143&doi=10.1016%2fj.jpdc.2022.07.003&partnerID=40&md5=7108161c85b9e2ee9b087acc2c66f38a","A new parallel algorithm for the max-flow problem on directed networks with single-source and single-sink is proposed. The algorithm is based on tree sub-networks and on efficient parallel algorithm to compute max-flows on the tree sub-networks. The latter algorithm is proved to be work-optimal and time-optimal. The parallel implementation of the complete algorithm is more efficient than the best known parallel algorithm for the max-flow problem in terms of time-complexity and the sequential implementation of the algorithm achieves the best known sequential time-complexity, without using any complex data-structures or complex manipulations on the network. © 2022 Elsevier Inc.","Combinatorial optimization; Complexity theory; Discrete optimization; Network flow problems; Parallel algorithms"
"Towards scalable and efficient Deep-RL in edge computing: A game-based partition approach","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132723351&doi=10.1016%2fj.jpdc.2022.06.006&partnerID=40&md5=2bc6049acf885356b01caaa234642fab","Currently, most edge-based Deep Reinforcement Learning (Deep-RL) applications have been deployed in the edge network, however, their mainstream studies are still short of adequate considerations on its limited compute and bandwidth resources. In this paper, we investigate the near on-policy of actions taking in distributed Deep-RL architecture, and propose a “hybrid near on-policy” Deep-RL framework, called Coknight, by leveraging a game-theory based DNN partition approach. We first formulate the partition problem into a variant of knapsack problem in device-edge setting, and then transform it into a potential game with a formal proof. Finally, we show the problem is NP-complete whereby an efficient distributed algorithm based on the potential game theory is developed from device perspective to achieve fast and dynamic partitioning. Coknight not only significantly improves the resource efficiency of the Deep-RL but also allows the inference to enforce the scalability of the actor policy. We prototype the framework with extensive experiments to validate our findings. The experimental results show that with the premise of a rapid convergence guarantee, Coknight, compared with Seed-RL, can reduce GPU utilization by 30% while providing large-scale scalability. © 2022 Elsevier Inc.","Deep reinforcement learning; DNN partition; Game theory; Mobile edge computing"
"Efficient and portable GEMM-based convolution operators for deep neural network training on multicore processors","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131762801&doi=10.1016%2fj.jpdc.2022.05.009&partnerID=40&md5=62bdb5b406f6ecd2fcf04ead2d510bef","Convolutional Neural Networks (CNNs) play a crucial role in many image recognition and classification tasks, recommender systems, brain-computer interfaces, etc. As a consequence, there is a notable interest in developing high performance realizations of the convolution operators, which concentrate a significant portion of the computational cost of this type of neural networks. In a previous work, we introduced a portable, high performance convolution algorithm, based on the BLIS realization of matrix multiplication, which eliminates most of the runtime and memory overheads that impair the performance of the convolution operators appearing in the forward training pass, when performed via explicit IM2COL transform. In this paper, we extend our ideas to the full training process of CNNs on multicore processors, proposing new high performance strategies to tackle the convolution operators that are present in the more complex backward pass of the training process, while maintaining the portability of the realizations. In addition, we conduct a full integration of these algorithms into a framework for distributed training of CNNs on clusters of computers, providing a complete experimental evaluation of the actual benefits in terms of both performance and memory consumption. Compared with baseline implementation, the use of the new convolution operators using pre-allocated memory can accelerate the training by a factor of about 6%–25%, provided there is sufficient memory available. In comparison, the operator variants that do not rely on persistent memory can save up to 70% of memory. © 2022 The Author(s)","Clusters of multicore processors; Convolutional neural networks; Distributed training; High performance; Python"
"A novel task offloading algorithm based on an integrated trust mechanism in mobile edge computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134571245&doi=10.1016%2fj.jpdc.2022.07.006&partnerID=40&md5=59e9c919e95ac38e6bd4d7989c97526d","As a new computing model, mobile edge computing (MEC) is designed to better deal with various forms of service requests, such as computing intensity and delay sensitivity, in the era of big data and the Internet of Things (IoT). However, the development of MEC is still in its infancy, and many issues need to be further investigated. One of the key issues that needs to be addressed in MEC is rational task offloading. Due to the dynamic, real time and complex nature of the MEC environment, the security and reliability of edge data are becoming increasingly important. Based on the above problems, we construct a task offloading integrated trust evaluation mechanism and, combined with the double deep Q-network (DDQN) algorithm in deep reinforcement learning (DRL), propose a novel task offloading algorithm, named DDTMOA. Simulation results show that the DDTMOA algorithm can effectively reduce the average task response time and total system energy consumption while ensuring task offloading performance compared to other classical algorithms. © 2022 Elsevier Inc.","Deep reinforcement learning; Integrated trust mechanism; Mobile edge computing; Task offloading"
"Local node feature modeling for edge computing based on network embedding in dynamic networks","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139051133&doi=10.1016%2fj.jpdc.2022.09.013&partnerID=40&md5=c23e91adf832fc8efb4eba69351a27cf","In a dynamic network, the characteristics of local nodes include first and higher-order proximity among the nodes as well as different attributes attached to each node. This complexity impose significant challenge for dynamic network modeling. As a result, few dynamic network studies have considered high-order proximity among local nodes. In this paper, we adopt the network embedding method to map high-order proximity of local nodes into low-dimensional, dense and real-valued vectors. Morevoer, we incorporate it into a model-based evolutionary clustering method through regularity conditions. Such a unified framework can increase the effectiveness and robustness of dynamic community detection while pertaining a good explanatory and visualization ability. Experiments based on synthetic and real world data sets show that our model can produce better community detection results than other popular models such as DECS and Genlouvain in dense networks. This result is consistent with the advantage of network embedding method in dense networks. © 2022 Elsevier Inc.","Dynamic network; Local node characteristics; Network embedding; Non-negative matrix factorization"
"APT: The master-copy-free training method for quantised neural network on edge devices","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128997291&doi=10.1016%2fj.jpdc.2022.04.005&partnerID=40&md5=eb8281223dad1098a432fb836d405c16","Quantisation is a commonly applied technique to improve the efficiency of a neural network on edge devices. Many applications also require that a machine learning model extend its learning cycle into the field. Training a quantised neural network on edge devices is a non-trivial task since the resource available on the edge is limited. Most of the quantisation-aware training methods maintain a quantised model, as well as an extra full-precision model, which is used to prevent large accuracy losses. The keep of the full precision model is based on the assumption that there are sufficient memory and energy supply to the machine for training. The assumption is contradictory to the situation of edge AI. In this paper, we propose the Adaptive Precision Training method (APT), which only keeps a quantised model. The challenge is that a quantised model has difficulty in learning, due to the quantisation underflow issue. APT employs a metric called Gavg to quantify the learning ability of each layer and dynamically adjusts per-layer bitwidth to ensure the model can learn effectively. Experiments on image classification and text classification tasks suggest that APT trains quantised models effectively with limited accuracy loss. Compared with the 8-bit traditional QAT method, APT saves 60-72.5% memory space for model parameters. We investigate the bitwidth necessary for effective training and gain preliminary insights into the relationship between architecture and its learning ability. © 2022 Elsevier Inc.","Adaptive precision; Edge AI; Efficient training; Neural network; Parameter update"
"AMBLE: Adjusting mini-batch and local epoch for federated learning with heterogeneous devices","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136535208&doi=10.1016%2fj.jpdc.2022.07.009&partnerID=40&md5=bc352d5ad78a60d034d9056e1bbddc5a","As data privacy becomes increasingly important, federated learning applied to the training of deep learning models while ensuring the data privacy of devices is entering the spotlight. Federated learning makes it possible to process all data at once while processing data independently from various devices without collecting distributed local data in a central server. However, there are still challenges to overcome for the system of devices in federated learning such as communication overheads and the heterogeneity of the system. In this paper, we propose the Adjusting Mini-Batch and Local Epoch (AMBLE) approach, which adaptively adjusts the local mini-batch and local epoch size for heterogeneous devices in federated learning and updates the parameters synchronously. With AMBLE, we enhance the computational efficiency by removing stragglers and scaling the local learning rate to improve the model convergence rate and accuracy. We verify that federated learning with AMBLE is a stably trained model with a faster convergence speed and higher accuracy than FedAvg and adaptive batch size scheme for both identically and independently distributed (IID) and non-IID cases. © 2022 Elsevier Inc.","Federated averaging; Federated learning; Local mini-batch SGD; System heterogeneity"
"ProvNet: Networked bi-directional blockchain for data sharing with verifiable provenance","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129538064&doi=10.1016%2fj.jpdc.2022.04.003&partnerID=40&md5=7f499f751014cc81a39f70cac06304e3","Data sharing is increasingly popular especially for scientific research and business fields where large volume of datasets need to be used, but it involves data security and privacy concerns. This paper mitigates such concerns by tracking and logging the history of shared data (i.e., provenance records) while preserving data privacy. This is a challenging problem in the data sharing scenario in this paper because the environment is decentralized and internal logs are not accessible publicly due to privacy concerns. We present ProvNet, a decentralized data sharing platform that can detect malicious users and provide secure provenance records using the newly proposed networked blockchain without disclosing raw data contents. Valid sharing records are collected and stored in the blocknet and misbehavior is detected with the stored provenance records according to our accountable protocols. We give a proof-of-concept implementation, and evaluation results show that the overhead is acceptable. © 2022 Elsevier Inc.","Blockchain; Data provenance; Secure data sharing"
"Parallel-FST: A feature selection library for multicore clusters","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133651241&doi=10.1016%2fj.jpdc.2022.06.012&partnerID=40&md5=c3feb1a1a33f839b16fbffbb7809855b","Feature selection is a subfield of machine learning focused on reducing the dimensionality of datasets by performing a computationally intensive process. This work presents Parallel-FST, a publicly available parallel library for feature selection that includes seven methods which follow a hybrid MPI/multithreaded approach to reduce their runtime when executed on high performance computing systems. Performance tests were carried out on a 256-core cluster, where Parallel-FST obtained speedups of up to 229x for representative datasets and it was able to analyze a 512 GB dataset, which was not previously possible with a sequential counterpart library due to memory constraints. © 2022 The Author(s)","Feature selection; High performance computing; HyperThreading; MPI; Mutual information"
"LOSC: A locality-optimized subgraph construction scheme for out-of-core graph processing","2023","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140326727&doi=10.1016%2fj.jpdc.2022.10.005&partnerID=40&md5=cccaedb86974b9098aca7cfb49ddd355","Big data applications increasingly rely on the analysis of large graphs. In recent years, a number of out-of-core graph processing systems have been proposed to process graphs with billions of edges on just one commodity computer, by efficiently using the secondary storage (e.g., hard disk, SSD). Unfortunately, these graph processing systems continue to suffer from poor performance, despite of many solutions proposed to address the disk I/O bottleneck problem, a commonly recognized root cause. However, our experimental results show that another root cause of the poor performance is the subgraph construction phase of graph processing, which induces a large number of random memory accesses that substantially weaken cache access locality and thus greatly degrade the performance. In this paper, we propose an efficient out-of-core graph processing system, LOSC, to substantially reduce the overheads of subgraph construction. LOSC proposes a locality-optimized subgraph construction scheme that significantly improves the in-memory data access locality of the subgraph construction phase. Furthermore, LOSC adopts a compact edge storage format and a lightweight replication of vertices to reduce I/O traffic and improve computation efficiency. Extensive evaluation results show that LOSC is respectively 9.4× and 5.1× faster than GraphChi and GridGraph, two representative out-of-core systems. In addition, LOSC outperforms other state-of-art out-of-core graph processing systems including FlashGraph, GraphZ, G-Store and NXGraph. For example, LOSC can be up to 6.9× faster than FlashGraph. © 2022 Elsevier Inc.","Graph processing; Out-of-core; Subgraph construction"
"Towards soft real-time fault diagnosis for edge devices in industrial IoT using deep domain adaptation training strategy","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119064364&doi=10.1016%2fj.jpdc.2021.10.005&partnerID=40&md5=2a5fb371c47cb7db0c752b550c442d9e","Artificial intelligence and industrial internet of things (IIoT) have been rejuvenating the fault diagnosis systems in Industry 4.0 for avoiding major financial losses caused by faults in rotating machines. Meanwhile, the diagnostic systems are provided with a number of sensory inputs that introduce variations in input space which causes difficulty for the algorithms in edge devices. This issue is generally dealt with bi-view cross-domain learning approach. We propose a soft real-time fault diagnosis system for edge devices using domain adaptation training strategy. The investigation is carried out using deep learning models that can learn representations irrespective of input dimensions. A comparative analysis is performed on a publicly available dataset to evaluate the efficacy of the proposed approach which achieved accuracy of 88.08%. The experimental results show that our method using long short-term memory network achieves the best results for the bearing fault detection in an IIoT environmental setting. © 2021 Elsevier Inc.","Condition monitoring; Deep learning; Domain adaptation; Fault detection; Induction motor"
"Algorithms for addressing line-of-sight issues in mmWave WiFi networks using access point mobility","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118863834&doi=10.1016%2fj.jpdc.2021.10.008&partnerID=40&md5=982231b925a32d63781585ad529ebdc2","Line-of-sight (LOS) is a critical requirement for mmWave wireless communications. In this work, we explore the use of access point (AP) infrastructure mobility to optimize indoor mmWave WiFi network performance based on the discovery of LOS connectivity to stations (STAs). We consider a ceiling-mounted mobile (CMM) AP as the infrastructure mobility framework. Within this framework, we propose two heuristic algorithms (basic and weighted) derived from Hamming distance computation and a machine learning (ML) solution fully exploiting available network state information to address the LOS discovery problem. Based on the ML solution, we then propose a systematic solution WiMove, which can decide if and where the AP should move to for optimizing network performance. Using both ns-3 based simulation and experimental prototype implementation, we show that the throughput and fairness performance of WiMove is up to 119% and 15% better compared with single static AP and brute force search. © 2021 Elsevier Inc.","Heuristic algorithms; Infrastructure mobility; Machine learning (ML); mmWave WiFi"
"Distributed collaboration and anti-interference optimization in edge computing for IoT","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124467007&doi=10.1016%2fj.jpdc.2022.01.028&partnerID=40&md5=d0acbbb28890a24bf506a0d48affed02","The edge computing (EC) systems for Internet of Things (IoT) can bring out low latency, high reliability, distributed intelligence, and network bandwidth savings to industrial real-time applications. However, limited computing and processing capabilities of edge devices remains to be difficult to meet complex data processing and artificial intelligence (AI) analysis requirements for diverse services. Besides, the scarcity of wireless spectrum resources in harsh industrial environment makes the interference between devices more serious. To address these challenges, this paper proposes an adaptive distributed collaborative anti-interference optimization scheme for IoT-edge system. Firstly, the EC system model is established, and the model of link failure probability is derived theoretically. Then, an Occupy-Interference Mitigation (O-IM) algorithm based on full-frequency multiplexing is proposed. The algorithm combines adaptive full-frequency multiplexing and interference mitigation to reduce the dependence of the reliable collaboration on bandwidth and signal-to-noise ratio (SNR). In addition, an anti-interference algorithm based on Interference Mitigation and inter-cellfrequency multiplexing Interference Avoidance (IM-IA) is proposed to balance bandwidth and SNR. This algorithm adopts interference cancellation scheme in the broadcasting phase, and adopts orthogonal frequency division in the collaboration phase. Extensive simulation results on Mininet platform verify that the proposals can obtain lower failure probability, and are less than traditional solutions in transmitting power, bandwidth, and SNR requirements. Moreover, the O-IM is suitable for low power scenarios, while the IM-IA is more suitable for high power case. © 2022 Elsevier Inc.","Anti-interference; Collaborative communication; Distributed collaboration; Edge computing network; Spatial diversity"
"General-purpose GPU hashing data structures and their application in accelerated genomics","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124880521&doi=10.1016%2fj.jpdc.2022.01.006&partnerID=40&md5=d1ba7822b1cd40ec2c3f9e23cc83f5de","A broad variety of applications relies on associative data structures that exclusively support insert, retrieve, and delete operations. Hash maps represent such a class of effective dictionary implementations. Properties such as amortized constant time complexity for these table operations as well as a compact memory layout make them versatile data structures with manifold applications in data analytics and artificial intelligence. The rapidly growing amount of data emerging in many scientific fields can often only be tackled with modern massively parallel accelerators such as GPUs. Numerous GPU hash table implementations have been proposed over the recent years. However, most of these implementations lack flexibility in order to be used in existing analytics pipelines or suffer from significant performance degradation for certain application scenarios. As a more recent approach, the WarpCore framework aims to alleviate these aforementioned restrictions by placing a focus on both versatility and performance. In this work we reflect the key concepts of the WarpCore library and provide an extensive performance evaluation against the state-of-the-art. We further explore how WarpCore can be used for accelerating two bioinformatics applications (metagenomic classification and k-mer counting) with significant speedups. © 2022 Elsevier Inc.","Bioinformatics; GPUs; Hash tables"
"Parallel implementation of cellular automata model of electron-hole transport in a semiconductor","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114664782&doi=10.1016%2fj.jpdc.2021.08.006&partnerID=40&md5=7e4c4e740d8c7edfeae839db4b2c730f","A parallel implementation of a three-dimensional cellular automaton (CA) model of electron — hole transport in a semiconductor is presented. Carriers transport is described by a nonlinear system of drift-diffusion-Poisson equations. This system includes the drift-diffusion equations in divergence form for electrons and holes and the Poisson equation for the potential, the gradient of which enters the drift-diffusion equations as the drift velocity. We solve the drift-diffusion-Poisson system for the three-dimensional case using the CA approach. A regular mesh is introduced in the three-dimensional domain, and the solution is calculated in all lattice cells. The drift-diffusion-Poisson system is solved by an iterative algorithm consisting of two alternating steps. In the first step, the electron and hole concentrations are calculated. In the second step, the drift velocity is calculated as the gradient of the solution to the Poisson equation with the right-hand side depending on the electron and hole concentrations. The correctness of both CA models is tested against the exact solutions of the drift-diffusion and Poisson equations for some special cases. A parallel implementation of the iterative CA algorithm using the domain decomposition method is presented. The efficiency of the parallel code is analyzed. The simulation results are obtained for the model parameters specific to GaN semiconductors. © 2021 Elsevier Inc.","Carrier transport; Drift-diffusion-Poisson equations; Multi-particle cellular automaton; Parallel computing"
"RF-RISA: A novel flexible random forest accelerator based on FPGA","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111226766&doi=10.1016%2fj.jpdc.2021.07.001&partnerID=40&md5=9768749ca651fd84d4b9602167b7755d","Recently, FPGA has been utilized to accelerate the Random Forest prediction process to meet the speed requirements of real-time tasks. However, the existing accelerators impose restrictions on the parameters of the accelerated model. The accelerators have to be reconfigured to adapt to a model whose parameters exceed the predefined restrictions. When these accelerators are applied in the scenarios where the model updates or switches frequently, non-trivial time overhead and maintenance costs may be introduced. To solve the above problem, a flexible accelerator RF-RISA, Random Forest Reduced Instruction Set Accelerator, is presented in this paper. Compared with the existing accelerators, RF-RISA eliminates all the restrictions by decoupling the model parameters from its hardware implementation. Specifically, RF-RISA encodes the information of the model into a group of instructions, then the instructions are stored in the memory rather than are hardcoded in the hardware. Meanwhile, a mapping scheme is proposed to map the instructions into the memory dynamically. Finally, a new hardware architecture is designed to support the pipelined computing. The theoretical analysis and experimental results show that the proposed RF-RISA can accelerate a wide range of RF models without reconfiguration. At the same time, it can achieve the same throughput as the state-of-the-art. © 2021 Elsevier Inc.","FPGA; Hardware accelerator; Prediction acceleration; Random forest"
"Energy aware fuzzy approach for placement and consolidation in cloud data centers","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122196564&doi=10.1016%2fj.jpdc.2021.12.001&partnerID=40&md5=374daa7cb3c072b0af42cd1ecadd593b","Virtual Network Function (VNF) is one of the pillars of a Cloud network that separates network functions and their dedicated hardware devices, such as routers, firewalls, and load balancers, to host their services on virtual machines. The VNF is responsible for network services that run on virtual machines and can connect each of them alone or organize themselves into a single enclosure to use all the resources available in that enclosure. This flexibility allows physical and virtual resources to be used in a way that ensures control over power consumption, balance in resource use, and minimizing costs and latency. In order to consolidate VNF groups into a minimum number of Virtual Machine (VM) with estimation of the association relation to a measure of confidence under the context of possibility theory, we propose a new Fuzzy-FCA approach for VNF placement based on Formal Concept Analysis (FCA) and fuzzy logic in mixed environment based on cloud data centers and Multiple access Edge Computing (MEC) architecture. Thus, the inclusion of this architecture in the cloud environment ensures the distribution of compute resources to the end user in order to reduce end-to-end latency. To confirm the effectiveness of our solution, we compared it to one of the best algorithms studied in the literature, namely the MultiSwarm algorithm. The results of the series of experiments carried out show the feasibility and efficiency of our algorithm. Indeed, the harvested results confirm the capability of maximizing and balancing the use of resources, of minimizing the latency and the cost of energy consumption. The performance of our solution in terms of average latency represents 16%, a slight increase compared to MultiSwarm, and an average gain, in runtime, of 49%, compared to the same algorithm. © 2021 Elsevier Inc.","Cloud data centers; Consolidation; Multiple access edge computing; Network function virtualization; Placement"
"Dispersion of mobile robots using global communication","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120945965&doi=10.1016%2fj.jpdc.2021.11.007&partnerID=40&md5=a1742d80939d80b70cfaa743f4e2ce81","The dispersion problem on graphs asks k≤n robots placed initially arbitrarily on the nodes of an n-node anonymous graph to reposition autonomously to reach a configuration in which each robot is on a distinct node of the graph. This problem is of significant interest due to its relationship to other fundamental robot coordination problems, such as exploration, scattering, load balancing, and relocation of self-driven electric cars (robots) to recharge stations (nodes). In this paper, we consider dispersion using the global communication model where a robot can communicate with any other robot in the graph (but the graph is unknown to robots). We provide two novel deterministic algorithms for arbitrary graphs in a synchronous setting where all robots perform their actions in every time step. Our first algorithm is based on a DFS traversal and guarantees (i) O(kΔ) steps runtime using O(log⁡(k+Δ))) bits at each robot and (ii) O(min⁡(m,kΔ)) steps runtime using O(Δ+log⁡k) bits at each robot, where m is the number of edges and Δ is the maximum degree of the graph. The second algorithm is based on a BFS traversal and guarantees O((D+k)Δ(D+Δ)) steps runtime using O(log⁡D+Δlog⁡k)) bits at each robot, where D is the diameter of the graph. Our results complement the existing results established using the local communication model where a robot can communication only with other robots present at the same node. © 2021 Elsevier Inc.","Autonomous mobile robots; Dispersion; Distributed algorithms; Multi-agent systems; Time and memory complexity"
"Techniques and tools for visually introducing freshmen to object-based thread abstractions","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110551896&doi=10.1016%2fj.jpdc.2021.05.013&partnerID=40&md5=9c6a94a6d7f1ae901ed663db32b6caca","An approach to adding concurrency to an existing undergraduate course should integrate driving problems used in analogies, worked examples, and assignments; amplify the subject/topics of the existing course; and ensure that the addition does not have a negative impact on (a) conceptual focus, and (b) student learning, struggle, and engagement. When concurrency is added to a course covering object-based programming, five related design principles for meeting these, sometimes conflicting, requirements are: (a) analogies, worked examples, and assignments are all implementations of simulations of moving physical objects, (b) the user-interface components of the simulations are created automatically or implemented manually using the MVC design pattern, (c) assignment implementations are layered to follow the logical dependence among concepts, (d) the concurrency aspects of the functional components of the simulations are implemented using reusable loop and design patterns, and (e) students can experiment with concurrency extensions to implementations of worked examples and assignments. We followed these principles in multiple course offerings that added concurrency to two different courses on object-based programming. Our data-based evaluation of these offerings, using new inferencing algorithms, analyzed the number of posts and contributions to class discussion forums; the number of entries in class participation diaries; the number of times an automated test is run before it passes; and the percentage of attempts to implement different aspects of concurrency that yield no success. The results show that adding thread execution and creation, synchronization, and coordination has little or no significant effect on measures of engagement, learning, and struggle. © 2021 Elsevier Inc.","Concurrency; Education; Java; Object-based programming"
"A taxi dispatch system based on prediction of demand and destination","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111323949&doi=10.1016%2fj.jpdc.2021.07.002&partnerID=40&md5=715088106e2656dccd52c86256cff420","In this paper we describe an intelligent taxi dispatch system that has the goal of reducing the waiting time of the passengers and the idle driving distance of the taxis. The system relies on two separate models that predict the probability distributions of the taxi demand and destinations respectively. The models are learned from historical data and use a combination of long short term memory cells and mixture density networks. Using these predictors, taxi dispatch is formulated as a mixed integer programming problem. We validate the performance of the predictors and the overall system on a real world dataset of taxi trips in New York City. © 2021 Elsevier Inc.","Demand prediction; Destination prediction; Distribution learning; Mixture density network; Taxi dispatch"
"A module-based introduction to heterogeneous computing in core courses","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112436437&doi=10.1016%2fj.jpdc.2021.07.011&partnerID=40&md5=e6b9bc65b6d3586bcef6070913459316","Heterogeneous architectures have emerged as a dominant platform, not only in high-performance computing but also in mobile processing, cloud computing, and the Internet of Things (IoTs). Because the undergraduate computer science curriculum includes so many topics, adding a new course as a required part of the curriculum without increasing the number of hours to graduation is difficult. Integration of heterogeneous computing requires a module-driven approach in which coverage of the topics is broken down into smaller units and dispersed throughout the curriculum. The module-driven approach has been successfully implemented in introducing parallel and distributed computing concepts. In this paper, we present a set of four teaching modules that introduce fundamental concepts in heterogeneous computing in lower-division computer science courses. The goal of these modules is not to teach students how to program heterogeneous systems but rather to expose them to this emerging trend and prepare them for material in future classes. Although concepts are covered at a high level, the modules emphasize active learning and include lab assignments that provide students with hands-on experience. We also present initial evaluation results for two of these modules based on their use in undergraduate courses at Texas State University. The results are quite encouraging both in terms of learning outcomes and student engagement and interest. © 2021 Elsevier Inc.","Heterogeneous computing; Module-based instruction; Pedagogy"
"Evolving PDC curriculum and tools: A study in responding to technological change","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111017238&doi=10.1016%2fj.jpdc.2021.07.003&partnerID=40&md5=6ac3f2837283a5689ff74053b62c0852","Much has changed about parallel and distributed computing (PDC) since the author began teaching the topic in the late 1990s. This paper reviews some of the key changes to the field and describes their impacts on his work as a PDC educator. Such changes include: the availability of free implementations of the message passing interface (MPI) for distributed-memory multiprocessors; the development of the Beowulf cluster; the advent of multicore architectures; the development of free multithreading languages and libraries such as OpenMP; the availability of (relatively) inexpensive manycore accelerator devices (e.g., GPUs); the availability of free software platforms like CUDA, OpenACC, OpenCL, and OpenMP for using accelerators; the development of inexpensive single board computers (SBCs) like the Raspberry Pi, and other changes. The paper details the evolution of PDC education at the author's institution in response to these changes, including curriculum changes, seven different Beowulf cluster designs, and the development of pedagogical tools and techniques specifically for PDC education. The paper also surveys many of the hardware and software infrastructure options available to PDC educators, provides a strategy for choosing among them, and provides practical advice for PDC pedagogy. Through these discussions, the reader may see how much PDC education has changed over the past two decades, identify some areas of PDC that have remained stable during this same time period, and so gain new insight into how to efficiently invest one's time as a PDC educator. © 2021 Elsevier Inc.","Beowulf cluster; Education; HPC; PDC; Supercomputing"
"Online delay-guaranteed workload scheduling to minimize power cost in cloud data centers using renewable energy","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117197679&doi=10.1016%2fj.jpdc.2021.09.002&partnerID=40&md5=0011cf1d693dc1297123e3c3c0655593","More and more cloud data centers are turning to leverage on-site renewable energy to reduce power cost for sustainable development. But how to effectively coordinate the intermittent renewable energy with workload remains to be a great challenge. This paper investigates the problem of workload scheduling for power cost minimization under the constraints of different Service Level Agreements (SLAs) of delay tolerant workload and delay sensitive workload for green data centers in a smart grid. Different from the existing studies, we take into consideration of the impact of zero price in the smart grid and the cost of on-site renewable energy. To handle the randomness of workload, electricity price and renewable energy availability, we first formulate the problem as a constrained stochastic problem. Then we propose an efficient online control algorithm named ODGWS (Online Delay-Guaranteed Workload Scheduling) which makes online scheduling decisions achieve a bounded guarantee from the worst scheduling delay for delay tolerant workload. Compared with the existing solutions, our ODGWS decomposes the problem into that of solving a simple optimization problem within each time slot in O(1) time without needing any future information. The rigorous theoretical analysis demonstrates that our algorithm achieves a [O([Formula presented]),O(V)] cost-delay tradeoff, where V is a balance parameter between the cost optimality and service quality. Extensive simulations based on real-world traces are done to evaluate the performance of our algorithm. The results show that ODGWS saves about 5% average power cost compared with the baseline algorithms. © 2021 Elsevier Inc.","Cloud data center; Delay tolerant scheduling; Lyapunov optimization; Renewable energy; Smart grid"
"Machine learning for optimal selection of sparse triangular system solvers on GPUs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111845250&doi=10.1016%2fj.jpdc.2021.07.013&partnerID=40&md5=6cfb4f73684cb9bad9ed2eb3dac5f3a2","Many numerical algorithms for science and engineering applications require the solution of sparse triangular linear systems (SPTRSV) as their most costly stage. For this reason, considerable research has been dedicated to produce efficient implementations for almost all high performance computing platforms. In the case of graphics processing units (GPUs), there are several strategies to perform this operation, which translate into a handful of different routines. In general, it is difficult to establish a priori which is the best routine for a given problem, and thus, an automatic procedure able to select the best solver for each matrix can entail large performance benefits. This work extends a previous effort, in which we relied on machine learning techniques to predict the best SPTRSV routine for each matrix, by improving both the accuracy and the speed of the selection procedure. Specifically, we focus on the most efficient machine learning techniques regarding the speed of their training and prediction stages; evaluate the artificial generation of sparse matrices to expand our dataset; and propose heuristics to compute approximations of some expensive features. The experimental results show that we can strongly improve the runtime of our procedure without compromising the quality results. © 2021 Elsevier Inc.","Graphics processors; High performance; Machine learning; Sparse triangular linear systems"
"A mobility-based deployment strategy for edge data centers","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126987189&doi=10.1016%2fj.jpdc.2022.03.007&partnerID=40&md5=4a55cd51d998ddb3cbc80bacb98d5c5b","The main objective of Multi-access Edge Computing (MEC) is to bring computational capabilities at the edge of the network to better support low-latency applications. Such capabilities are typically offered by Edge Data Centers (EDC). The MEC paradigm is not tied to a single radio technology, rather it embraces both cellular and other radio access technologies such as WiFi. Distributed intelligence at the edge for AI purposes requires careful spatial planning of computing and storage resources. The problem of EDC deployment in urban environments is challenging and, to the best of our knowledge, it has been explored only for cellular connectivity so far. In this paper, we study the possibility of deploying EDC without analyzing the expected data traffic load of the cellular network, a kind of information rarely shared by network operators. To this purpose, we propose in this work CLUB, CLUstering-Based strategy tailored on the analysis of urban mobility. We analyze two experimental mobility data sets, and we analyze some mobility features in order to characterize their properties. Finally, we compare the performance of CLUB against state-of-the-art techniques in terms of the outage probability, namely the probability an EDC is not able to serve a request. Our results show that the CLUB strategy is always comparable with respect to our benchmarks, but without using any information related to network traffic. © 2022 Elsevier Inc.","Edge data center; Mobile CrowdSensing; Mobility; Multi-access edge computing"
"Modelling and analysing IoT systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111178358&doi=10.1016%2fj.jpdc.2021.07.004&partnerID=40&md5=8e709b5ed0f36487a99d42e9e51533d5","The Internet of Things is deeply shaping our society and our lives. Smart devices automatically collect, aggregate and exchange data on our behalf and free us from the drudgery of doing it. These data are often crucial because critical decisions, such as controlling cyber-physical systems, are made depending on them or because they feed learning algorithms. Safety and security issues related to devices and to data can have a major impact on smart systems and can have serious consequences if they oversee essential services, such as delivering power, water, transport, and so on. For this reason, it is crucial to identify the most critical components in a network of devices and to evaluate how they are vulnerable to accidental or to intentional failures. We propose to use the process calculus IOT-LYSA to model systems and to apply a Control Flow Analysis to statically predict the manipulation of data, as well as on which data the critical decisions depend, in particular those affecting actuations. By exploiting suitable metrics, we can use the results of the analysis so as to provide system administrators with estimates of the safety and security of their systems. © 2021 Elsevier Inc.","Actuation; Cost evaluation; Internet of Things; Safety; Security"
"Dynamic load balancing with over decomposition in plasma plume simulations","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124393365&doi=10.1016%2fj.jpdc.2022.01.023&partnerID=40&md5=a0146d8d0bbbefd19ca3c53679a8c3e5","An electric propulsion plasma plume simulation employs the Particle-in-Cell (PIC) model for heavy species (i.e. ions and neutral atoms) and yields a highly non-uniform particle distribution at steady state. For parallel simulations, partitioning the computational domain uniformly and evenly assigning the subdomains to MPI processes would never lead to properly balanced loads across MPI processes. For this reason, a patch-based dynamic load balancing method with over-decomposition and Hilbert space-filling curve has been implemented into the Thermophysics Universal Research Framework (TURF). The data transfer is accomplished by utilizing TURF's serialization and deserialization feature, in which the tree hierarchical object structure is copied into a contiguous memory block of data, and the tree hierarchy is reconstructed after the data transfer. This generalized dynamic load balancing approach allows for the same routine to be used for any models besides the PIC model in TURF (e.g. fluid, continuum kinetic, hybrid models, etc). This paper first introduces individual pieces of TURF that enable dynamic load balancing when they are combined. Then, the dynamic load balancing method is demonstrated in three different particle simulations, showing performance gains when the number of subdomains is larger than that of MPI processes. The benefit is even more notable in simulations with highly non-uniform particle distributions at steady state, commonly seen in plasma plume simulations. © 2022 Elsevier Inc.","Dynamic load balancing; Hilbert space-filling curve; Over decomposition; Particle simulation"
"Dynamic fault tolerant scheduling with response time minimization for multiple failures in cloud","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113225697&doi=10.1016%2fj.jpdc.2021.07.019&partnerID=40&md5=e77851e64fa385a3f6a399abd27524c5","With the increasing demand for large amount of computing resources, the cloud is widely used for executing large number of independent tasks. In order to successfully execute more tasks and maximize the revenues, the cloud service providers (CSPs) should provide reliable services, while maximizing the resource utilization. Providing better Quality of Service (QoS), while maximizing the resource utilization in the event of failures is a critical research issue which needs to be addressed. In this paper, an Elastic pull-based Dynamic Fault Tolerant (E-DFT) scheduling mechanism is designed for minimizing the response time while executing the backups during multiple failures of independent tasks. A basic core primary backup model is also used and integrated with the backup tasks overlapping (BTO) and backup tasks fusion (BTF) techniques to tolerate multiple simultaneous failures. Simulation results show that the proposed E-DFT scheduling can achieve better performance in terms of guarantee ratio and resource utilization over other existing scheduling algorithms. © 2021 Elsevier Inc.","Cloud computing; Fault tolerant; Resource utilization; Scheduling"
"Communication lower-bounds for distributed-memory computations for mass spectrometry based omics data","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119588879&doi=10.1016%2fj.jpdc.2021.11.001&partnerID=40&md5=214f91af0023b56b28d57c99a1ab609d","Mass spectrometry (MS) based omics data analysis require significant time and resources. To date, few parallel algorithms have been proposed for deducing peptides from mass spectrometry-based data. However, these parallel algorithms were designed, and developed when the amount of data that needed to be processed was smaller in scale. In this paper, we prove that the communication bound that is reached by the existing parallel algorithms is [Formula presented], where m and n are the dimensions of the theoretical database matrix, q and r are dimensions of spectra, and p is the number of processors. We further prove that communication-optimal strategy with fast-memory [Formula presented] can achieve [Formula presented] but is not achieved by any existing parallel proteomics algorithms till date. To validate our claim, we performed a meta-analysis of published parallel algorithms, and their performance results. We show that sub-optimal speedups with increasing number of processors is a direct consequence of not achieving the communication lower-bounds. We further validate our claim by performing experiments which demonstrate the communication bounds that are proved in this paper. Consequently, we assert that next-generation of provable, and demonstrated superior parallel algorithms are urgently needed for MS based large systems-biology studies especially for meta-proteomics, proteogenomic, microbiome, and proteomics for non-model organisms. Our hope is that this paper will excite the parallel computing community to further investigate parallel algorithms for highly influential MS based omics problems. © 2021 Elsevier Inc.","Communication-avoiding; Mass spectrometry; Parallel algorithms; Proteomics"
"A novel neural network approach for airfoil mesh quality evaluation","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127004840&doi=10.1016%2fj.jpdc.2022.03.006&partnerID=40&md5=8fa41c2357fffa0d170bb60d0d796f28","Evaluating mesh quality before solving is crucially important for error control in the numerical simulation of airfoils. Traditional mesh quality metrics are used to identify distorted mesh elements by analyzing their geometric shape information like angles and edges. However, these metrics fail to recognize numerical errors stemming from quality attributes such as improper mesh density or distribution. This deficiency has imposed a burden on intensive manual re-evaluation, which heavily increases the meshing cost. In this paper, we introduce deep neural networks to airfoil mesh quality evaluation to improve its automation and efficiency. Specifically, we propose a neural network-based evaluation model, MQNet, accompanied by the first large-scale mesh benchmark dataset, AirfoilSet. MQNet is trained on the dataset to learn the quality-related attributes including mesh orthogonality, smoothness, and density. Thereafter, the trained network can be used as a black box to automatically evaluate and output the overall quality of the airfoil mesh. To demonstrate the effectiveness and generalization behaviors of the proposed network, we establish an MQNet-based mesh generation workflow. Experimental results show that MQNet is small and accurate. It outperforms widely-used neural networks and can be a useful tool to guide the high-quality mesh generation process with minimal user intervention. © 2022 Elsevier Inc.","Benchmark dataset; Deep neural network; Mesh quality; Numerical simulation"
"Mutation and dynamic objective-based farmland fertility algorithm for workflow scheduling in the cloud","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125652300&doi=10.1016%2fj.jpdc.2022.02.005&partnerID=40&md5=1e2d21f9bb138629c620db32b7fe0c92","Nowadays, many scientific applications are deployed in the cloud to execute at a lower cost. However, the growing scale of workflows makes scheduling problems challenging. To minimize the workflow execution cost under deadline constraints, this article proposes a Mutation and Dynamic Objective-based Farmland Fertility (MDO-FF) algorithm for obtaining a near-optimal solution within a relatively shorter time. A Dynamic Objective Strategy (DOS) is introduced to accelerate the convergence speed, while a multi-swarm evolutionary approach and mutation strategies are incorporated to enhance the search diversity and help to escape from local optima. By seeking new potential solutions and searching in its corresponding neighborhoods, our proposed MDO-FF can make a good trade-off between exploration and exploitation. Extensive experiments are conducted on well-known scientific workflows with different types and sizes. The experimental results demonstrate that in most cases, our MDO-FF outperforms the existing algorithms in terms of constraint satisfiability and solution quality. © 2022 Elsevier Inc.","Cloud computing; Deadline constraints; Meta-heuristics; Workflow scheduling"
"Comparing the performance of general matrix multiplication routine on heterogeneous computing systems","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118477820&doi=10.1016%2fj.jpdc.2021.10.002&partnerID=40&md5=f75ac31df408601b02f33cb66797c706","This paper contains the results of research on the general matrix multiplication routine performance on modern heterogeneous computing systems. In addition to the single-threaded and multi-threaded performance of the routine for matrices of double-precision real and complex numbers on the IBM POWER and Intel Xeon CPUs, the possibility of automatic offload calculation to NVIDIA GPUs, which is supported by certain BLAS library implementations, was studied. Special attention was paid to the impact on the performance of the bandwidth of the interconnects, which ensure CPU-to-GPU interaction. The obtained results show that IBM computing systems with a high-speed NVLink interconnect demonstrate the best performance doing matrix multiplication on GPUs. Accordingly, these systems can be used to accelerate the solution of tasks that utilize this routines without the need to significantly alter the existing software. It should be noted that CPUs of Intel computing system and the Intel MKL library show the best efficiency performing operations with small matrices. Research results can be used to develop approaches to improving the performance of software, which utilize the general matrix multiplication routine. © 2021 Elsevier Inc.","Automatic computation offloading; Graphics processing unit; Heterogeneous computing; High performance computing; Matrix multiplication"
"Improved distributed approximation for Steiner tree in the CONGEST model","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114715256&doi=10.1016%2fj.jpdc.2021.08.004&partnerID=40&md5=d627eefdca3c5768f50fb0a1efb86c73","In this paper we present two deterministic distributed algorithms for the Steiner tree (ST) problem in the CONGEST model. The first algorithm computes a 2(1−1/ℓ)-approximate ST using O(S+nlog⁎⁡n) rounds and O(mS+n3/2) messages for a graph of n nodes and m edges, where S is the shortest path diameter of the graph and ℓ is the number of leaf nodes in the optimal ST. It improves the round complexity of the best distributed ST algorithm known so far, which is O˜(S+min{St,n}) [34], where t is the number of terminal nodes. The second algorithm improves the message complexity of the first one by dropping the additive term of O(n3/2) at the expense of a logarithmic multiplicative factor in the round complexity. We also show that for graphs with S=O(log⁡n), a 2(1−1/ℓ)-approximate ST can be deterministically computed using O˜(n) rounds and O˜(m) messages and these complexities almost coincide with the results of some of the singularly-optimal minimum spanning tree (MST) algorithms proposed in [15,22,37]. © 2021 Elsevier Inc.","Distributed approximation algorithm; Singularly-optimal; Steiner tree"
"Improving scalability of parallel CNN training by adaptively adjusting parameter update frequency","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116059861&doi=10.1016%2fj.jpdc.2021.09.005&partnerID=40&md5=ac265f7dece4d5482b8e0955ccf106e5","Synchronous SGD with data parallelism, the most popular parallelization strategy for CNN training, suffers from the expensive communication cost of averaging gradients among all workers. The iterative parameter updates of SGD cause frequent communications and it becomes the performance bottleneck. In this paper, we propose a lazy parameter update algorithm that adaptively adjusts the parameter update frequency to address the expensive communication cost issue. Our algorithm accumulates the gradients if the difference of the accumulated gradients and the latest gradients is sufficiently small. The less frequent parameter updates reduce the per-iteration communication cost while maintaining the model accuracy. Our experimental results demonstrate that the lazy update method remarkably improves the scalability while maintaining the model accuracy. For ResNet50 training on ImageNet, the proposed algorithm achieves a significantly higher speedup (739.6 on 2048 Cori KNL nodes) as compared to the vanilla synchronous SGD (276.6) while the model accuracy is almost not affected (<0.2% difference). © 2021 Elsevier Inc.","Communication cost; Data parallelism; Deep learning; Parameter update frequency"
"Sboing4Real: A real-time crowdsensing-based traffic management system","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123697094&doi=10.1016%2fj.jpdc.2022.01.017&partnerID=40&md5=ad952e68a5a25f15040469b69574295e","This work describes the architecture of the back-end engine of a real-time traffic data processing and satellite navigation system. The role of the engine is to process real-time feedback, such as speed and travel time, provided by in-vehicle devices and derive real-time reports and traffic predictions through leveraging historical data as well. We present the main building blocks and the versatile set of data sources and processing platforms that need to be combined together to form a fully-functional and scalable solution. We also present performance results focusing on meeting system requirements while keeping the need for computing resources low. The lessons and results presented are of value to additional real-time applications that rely on both recent and historical data. Finally, we discuss the application of the aforementioned solution to a successful pilot study, where the full system was deployed and processed data from 800 taxis for a period of 3 months. © 2022 Elsevier Inc.","IoT; Massive parallelism; OLAP; Stream processing; Vehicle traffic monitoring"
"QoS provision for vehicle big data by parallel transmission based on heterogeneous network characteristics prediction","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124297577&doi=10.1016%2fj.jpdc.2022.01.018&partnerID=40&md5=ac5c10f489a6c41888385f020aca186f","Multipath parallel transmission has become an important research direction to improve big data transmission efficiency of connected vehicles. However, due to the heterogeneity and time-varying characteristics of parallel transmission paths, packets transmitted in parallel are usually out-of-order delivered to the destination, which greatly limits the throughput. To Lift the restriction of out-of-order delivery on the efficiency of big data transmission, this paper proposes a packet-granular real-time shortest delay scheduling scheme for multipath transmission based on path characteristics prediction. The scheme first clusters and models the heterogeneous network, which greatly reduces the complexity of the network. Subsequently, a prediction algorithm that can quickly converge to real-time delay is proposed. Then the details of the scheduling scheme are introduced by modules, and the bandwidth aggregation efficiency close to the theoretical upper limit is proved through simulation. Finally, we summarize the applicable scenarios and future work of the scheme. © 2022 Elsevier Inc.","Big data; Multipath parallel transmission; Network bottleneck prediction; Quality of service; Vehicular networks"
"Mapping series-parallel streaming applications on hierarchical platforms with reliability and energy constraints","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124296006&doi=10.1016%2fj.jpdc.2022.01.016&partnerID=40&md5=7f336fdcd234f618b98d039efcb998ea","Streaming applications come from various application fields such as physics, where data is continuously generated and must be processed on the fly. Typical streaming applications have a series-parallel dependence graph, and they are processed on a hierarchical failure-prone platform, as for instance in miniaturized satellites. The goal is to minimize the energy consumed when processing each data set, while ensuring real-time constraints in terms of processing time. Dynamic voltage and frequency scaling (DVFS) is used to reduce the energy consumption, and we ensure a reliable execution by either executing a task at maximum speed, or by triplicating it, so that the time to execute a data set without failure is bounded. We propose a structure rule to partition the series-parallel applications and map the application onto the platform, and we prove that the optimization problem is NP-complete. We design a dynamic-programming algorithm for the special case of linear chains, which is optimal for a special class of schedules. Furthermore, this algorithm provides an interesting heuristic and a building block for designing heuristics for the general case. The heuristics are compared to a baseline solution, where each task is executed at maximum speed. Simulations on realistic settings demonstrate the good performance of the proposed heuristics; in particular, significant energy savings can be obtained. © 2022 Elsevier Inc.","Energy; Hierarchical platforms; Reliability; Series-parallel streaming applications; Task mapping"
"A high-performance VLSI array reconfiguration scheme based on network flow under row and column rerouting","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114386287&doi=10.1016%2fj.jpdc.2021.08.005&partnerID=40&md5=baa9872cc66907add36b15661abe8c11","The reconfiguration algorithms have been extensively investigated to ensure the reliability and stability for the processor arrays with faults. It is important to reduce the power consumption, capacitance and communication costs in the processors by reducing the interconnection length of the VLSI array. This paper discusses the reconfiguration problem of the high-performance VLSI processor array under the row and column rerouting constraints. A novel method, making use the idea of network flow, is proposed in this paper. Firstly, a network flow model of the VLSI processor array is constructed, such that the high-performance VLSI target array can be obtained by utilizing the minimal cost flow algorithm. Secondly, we propose a new strategy for bottleneck row selection in the logical array using the minimum cut technique, which can find a more suitable bottleneck row. Finally, we conducted reliable experiments to clearly reveal the efficiency of the new rerouting scheme and algorithm in reducing the number of long interconnects. The experimental results show that, for a host array with size of 256×256, the number of long interconnects in the subarray can be reduced by up to 79.22% and 55.88% without performance penalty for random faults with density of 1% and 25% respectively, when compared with state-of-the-art. In addition, the proposed scheme improves existing algorithm in terms of subarray size. On a 256×256 host array with 25% faulty density, the average improvement in subarray size is up to 3.77% compared with state-of-the-art. © 2021 Elsevier Inc.","Algorithm; Degradable VLSI array; Fault tolerance; Network flow; Reconfiguration"
"Blockchain-based automated and robust cyber security management","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124186957&doi=10.1016%2fj.jpdc.2022.01.002&partnerID=40&md5=6b4957d08c1c20c7b26d3a20da468cdf","We initiate the study on the problem of automated and robust Cyber Security Management (CSM). We exemplify the problem by investigating how CSM should respond to the discovery of cyber intelligence that identifies new attackers, victims, or defense capabilities. Given the complexity of CSM, we divide it into three classes, referred to as Network-centric (N-CSM), Tools-centric (T-CSM) and Application-centric (A-CSM). These lead to a range of functions for examining whether, and to what extent, a network has been compromised. Moreover, we propose to incorporate blockchain (via Hyperledger Fabric) to build a decentralized CSM system, dubbed B2CSM, that ensures the retrieval of valid invocation results for CSM purposes. We also integrate B2CSM with a decentralized storage network (DSN), instantiated by InterPlanetary File System (IPFS), to reduce on-chain storage costs without hindering its robustness. We present the design and implementation of the prototype B2CSM system. Experiments with real-world datasets show that the CSM solutions and system are effective and efficient. © 2022 Elsevier Inc.","Blockchain; Cyber security management; Hyperledger fabric; IPFS"
"Secure data outsourcing in presence of the inference problem: A graph-based approach","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117572283&doi=10.1016%2fj.jpdc.2021.09.006&partnerID=40&md5=ae7e6b62f02d975c7af69f45ee514abf","In light of the emergence of Database-as-a-Service paradigm, secure data outsourcing has become one of the crucial challenges which strongly imposes itself. In such a scenario, access control is considered as a major challenge. In fact, access control policies of the data owner must be preserved when data is moved to the cloud. Here, we address this problem by considering inference leakage that could be produced by exploiting functional dependencies. The proposed approach is based on vertical partitioning to produce a set of secure sub-schemas stored in separated partitions in the distributed system. Then, we extend this approach by presenting a secure query processing model to preserve access control policies when querying data from distributed partitions. The effectiveness of our algorithms is confirmed through observations from a variety of conducted experiments. © 2021 Elsevier Inc.","Access control; Data dependencies; Distributed databases; Inference control; Security and privacy"
"Extending an asynchronous runtime system for high throughput applications: A case study","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124701552&doi=10.1016%2fj.jpdc.2022.01.027&partnerID=40&md5=70adc3eab453d28ba2bd50d8e843624a","Current supercomputers are mostly composed of vast numbers of nodes enhanced with accelerators (usually in the form of GPUs). However, having these heterogeneous designs in the forefront have exposed the software toolchains and application designers to the underlying complexities of an ever evolving hardware substrate. The need for a more dynamic view from the system software (i.e., compilers and runtimes) has become more apparent in these environments. Due to this, adaptive, fine grain runtime systems have seen a rise in popularity in the past decades. With low overhead and small tasks, these runtimes help to hide long latency operations by exploiting the massive concurrency presented in different application workflows. Such features allow the reduction of idle time (a result from ever deeper and complex memory hierarchies and memory types) with the execution of unrelated work across the machine. Of these runtimes, the Asynchronous Many Task (AMT) Runtimes are excellent exemplars as they can efficiently map onto hardware substrates and exhibit a high degree of latency hiding. Because of their latency tolerant characteristics, applications such as Graph Analytics and Big Data applications (which are latency sensitive) can use these runtimes very efficiently. Thanks to these characteristics, we present how a careful design can help to exploit the properties of an AMT when running high latency applications such as the ones encountered in the Big Data domain. In addition, when combined with introspection / adaptive capabilities, the runtime can further exploit optimization opportunities during its execution based on the ever changing state of the underlying hardware substrate. As a vehicle for this exploration, we use the Performance Open Community Runtime (P-OCR) to test all these concepts with Big Data workloads. © 2022","Asynchronous runtime systems; Big Data; Performance analysis"
"Improving the performance of batch schedulers using online job runtime classification","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125645265&doi=10.1016%2fj.jpdc.2022.01.003&partnerID=40&md5=f9f7a5292273ca120863f6bd30519aa5","Job scheduling in high-performance computing platforms is a hard problem that involves uncertainties on both the job arrival process and their execution times. Users typically provide only loose upper bounds for job execution times, which are not so useful for scheduling heuristics based on processing times. Previous studies focused on applying regression techniques to obtain better execution time estimates, which worked reasonably well and improved scheduling metrics. However, these approaches require a long period of training data. In this work, we propose a simpler approach by classifying jobs as small or large and prioritizing the execution of small jobs over large ones. Indeed, small jobs are the most impacted by queuing delays, but they typically represent a light load and incur a small burden on the other jobs. The classifier operates online and learns by using data collected over the previous weeks, facilitating its deployment and enabling a fast adaptation to changes in the workload characteristics. We evaluate our approach using four scheduling policies on seven HPC platform workload traces. We show that: first, incorporating such classification reduces the average bounded slowdown of jobs in all scenarios, second, in most considered scenarios, the improvements are comparable to the ideal hypothetical situation where the scheduler would know in advance the exact running time of jobs. © 2022 Elsevier Inc.","Classification; HPC; Machine learning; Online scheduling"
"A distributed intrusion detection system to detect DDoS attacks in blockchain-enabled IoT network","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125476319&doi=10.1016%2fj.jpdc.2022.01.030&partnerID=40&md5=46bc3c950fb0d45ee6d1f2d34a90f5d0","The Internet of Things (IoT) is emerging as a new technology for the development of various critical applications. However, these applications are still working on centralized storage architecture and have various key challenges like privacy, security, and single point of failure. Recently, the blockchain technology has emerged as a backbone for the IoT-based application development. The blockchain can be leveraged to solve privacy, security, and single point of failure (third-part dependency) issues of IoT applications. The integration of blockchain with IoT can benefit both individual and society. However, 2017 Distributed Denial of Service (DDoS) attack on mining pool exposed the critical fault-lines among blockchain-enabled IoT network. Moreover, this application generates huge amount of data. Machine Learning (ML) gives complete autonomy in big data analysis, capabilities of decision making and therefore is used as an analytical tool. Thus, in order to address above challenges, this paper proposes a novel distributed Intrusion Detection System (IDS) using fog computing to detect DDoS attacks against mining pool in blockchain-enabled IoT Network. The performance is evaluated by training Random Forest (RF) and an optimized gradient tree boosting system (XGBoost) on distributed fog nodes. The proposed model effectiveness is assessed using an actual IoT-based dataset i.e., BoT-IoT, which includes most recent attacks found in blockchain-enabled IoT network. The results indicate, for binary attack-detection XGBoost outperforms whereas for multi-attack detection Random Forest outperforms. Overall on distributed fog nodes RF takes less time for training and testing compared to XGBoost. © 2022 Elsevier Inc.","Blockchain; DDoS attacks; Fog computing; Internet of things (IoT); Intrusion detection system; Mining pool"
"Fast quantum algorithm for protein structure prediction in hydrophobic-hydrophilic model","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127183130&doi=10.1016%2fj.jpdc.2022.03.011&partnerID=40&md5=a99e1c0f9dc599533a440d38710e6696","In its unfolded form, a protein is a linear sequence of amino acids. Protein structure prediction attempts to find the native conformation for a given protein, which has potential applications in drug and vaccine development. Classically, protein structure prediction is an NP-complete, unsolved computational problem. Quantum computing however promises to improve upon the performance of classical algorithms. Here we develop a quantum algorithm in hydrophobic-hydrophilic model on two-dimensional square lattice to solve the problem for any sequence of length N amino acids with a quadratic speedup over its classical counterpart. This speedup is achieved using Grover's quantum search algorithm. The algorithm can be used for amino acid sequences of arbitrary length. It consists of three stages: (1) preparation of a superposition state that encodes all possible 22(N−1) conformations, (2) calculation of coordinates and energy for each possible conformation in parallel, and (3) finding the conformation with the minimal energy. The asymptotic complexity with regard to space is O(N3), while the obtained speedup is quadratic compared to the classical counterpart. We have successfully simulated the algorithm on the IBM Quantum's qasm simulator using Qiskit SDK. Also, we have further confirmed the correctness of the results by calculating theoretical probability of finding the right conformation. © 2022 Elsevier Inc.","Molecular algorithms; NP-complete problems; Protein structure prediction; Quantum algorithms"
"Application of the Layered Algorithm in search of an airborne contaminant source","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115960904&doi=10.1016%2fj.jpdc.2021.09.001&partnerID=40&md5=ffd4e52814350d86669452cea2953889","The paper presents a new method of optimization by the Layered Algorithm (LA). The proposed algorithm reduces the initial area to the sub-area containing the optimum. The proposed technique is based on the classification of the optimized function values (data sampled by sensors). The classification method uses a two-dimensional three-state Cellular Automata (CA). The CA classifies all area points ascribed to the CA cells based on their values. Specification of the categorization layers to the data gives a possibility to identify the different levels areas. Consequently, after analysis, a sub-area containing the optimum can be designated. In this paper, the proposed algorithm is applied to find the location of the airborne contaminant source by analyzing the concentration of released substances reported by mobile sensors distributed over the domain of interest. The Gaussian dispersion model simulation of the contaminant dispersion in the urbanized area is applied to generate the data used to verify the efficiency of the proposed Layered Algorithm. The LA successfully estimates the sub-area of the considered domain where the contamination source is located, taking to account data from sensors solely. © 2021 The Authors","Area optimization; Cellular automata; Classification; Gaussian dispersion model; Sensors"
"Oppositional chaos game optimization based clustering with trust based data transmission protocol for intelligent IoT edge systems","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126907336&doi=10.1016%2fj.jpdc.2022.03.008&partnerID=40&md5=a893211b07e60fb32d6ca99f6ef2709e","In the past decade, the Internet of Things (IoT) becomes essential in consumer and industrial applications. The accessibility of high bandwidth Internet connection particularly with the arrival of robust 5G networks rises to innovative IoT solutions such as smart city, automobiles, industry 4.0, etc. IoT analytics represents edge computing as a term commonly employed for defining intelligent computational resources placed closer to the source of data generation. Despite the benefits of the IoT edge systems, security and energy efficiency remains major challenging issues. With this motivation, this paper presents an energy-efficient clustering based secure data transmission protocol (EEC-SDTP) for Intelligent IoT Edge systems. The goal of the EEC-SDTP technique is to select an appropriate set of cluster heads (CHs) and optimally secure routes for data transmission in the network. The proposed model involves oppositional chaos game optimization-based clustering (OCGOC) technique for proper CH selection and cluster construction. Besides, a trust-based model is designed to determine the trustworthiness of the node in the IoT edge systems. The proposed OCGOC technique derives a fitness function utilizing 3 input parameters like trust level, distance to neighbors, and energy. Finally, a trust-based secure routing protocol using the quantum sand piper optimization (SRP-QSPO) technique is employed to derive routes for secure data transmission. For examining the better efficiency of the proposed EEC-SDTP algorithm, an extensive group of experimentations were performed and the outcomes are investigated under several performance measures. The experimental outcomes highlighted the improved efficiency of the proposed method over the other related techniques. © 2022 Elsevier Inc.","Artificial intelligence; Edge computing; Internet of Things; Security; Trust sensing model"
"Hammer lightweight graph partitioner based on graph data volumes","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111924534&doi=10.1016%2fj.jpdc.2021.07.008&partnerID=40&md5=e6a8161605bf21b8050d0bf5fdc3788c","The graph partitioning challenge is well known and ongoing classical problem. Many heuristic methods tried to propose solutions focusing mainly on load processing and cost-efficiency. With the emergency of big data technology, the graph partitioning challenge became even more demanding, as an imminent need to handle big volume of data in real time. This reveals a new challenge as most of the existing studies does not consider the volume metric with their streaming graph algorithms causing imbalanced workloads and graph storage. With this article, we propose a specific lightweight algorithm which we called “Hammer” Algorithm. Our proposed Hammer algorithm is a streaming graph based on volume metric to ensure optimal load processing and communication cost efficiency. Our proof of concept was done on real world dataset and the Hammer algorithm showed considerable performance against some existing graph partitioning algorithms. © 2021 Elsevier Inc.","Balance size; Balance volume; Graph partitioning methods; Property graph"
"Multi-objective biogeography-based optimization and reinforcement learning hybridization for network-on chip reliability improvement","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119405914&doi=10.1016%2fj.jpdc.2021.11.005&partnerID=40&md5=7f4a778d58622412143bfd28d16f7688","Reliability is increasingly a major concern in network-on-a-chip (NoC) design, alongside increased performance demands from new applications and the need for continued miniaturization of silicon technology. In this article, we look at the task migration mechanism, used to recover from permanent processing element (PE) failures in NoCs, by remapping tasks performed on faulty cores to spare ones. An innovative reliability-aware task mapping technique is presented, based on a hybridization between Multi-Objective Optimization (MOO) and Reinforcement Learning (RL). It takes place in two steps. In the first, a set of optimal remapping solutions for different failure scenarios is generated at design-time, using a Biogeography-Based Multi-Objective Optimization algorithm, while considering communication energy and migration costs. In the second step, an artificial neural network agent is trained to select the best remapping solution, from those generated at design-time, to recover from execution failures at run-time. Experiments were carried out to evaluate our technique for different sizes of networks and on different benchmarks. The results obtained show that the technique based on the hybridization MOO_RL brings a great improvement in the reliability of the NoC and achieves a good compromise between reliability and performance. It also guarantees a reduction of the overhead caused by the storage space of the remapping solutions, compared to the existing solutions. © 2021 Elsevier Inc.","Multi-objective optimization; Network-on chip; Reinforcement learning; Reliability; Spare placement; Task migration"
"A hardware/software co-design methodology for in-memory processors","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119611015&doi=10.1016%2fj.jpdc.2021.10.009&partnerID=40&md5=96e41bd584b87ae7cd8decb4d3fef40b","The bottleneck between the processor and memory is the most significant barrier to the ongoing development of efficient processing systems. Therefore, a research effort begun to shift from processor-centric architectures to memory-centric architectures. Various in-memory processor architectures have been proposed to break this barrier to pave the way for ever-demanding memory-bound applications. Associative in-memory processing is a successful candidate for truly in-memory computing, in which processor and memory are combined in the same location to eliminate the expensive data access costs. The architecture exhibits an unmatched advantage for data-intensive applications due to its memory-centric design principles. On the other hand, this advantage can be revealed fully by an efficient design methodology. This study puts further progressive effort by proposing a hardware/software design methodology for associative in-memory processors. The methodology aims to decrease energy consumption and area requirement of the processor architecture specifically programmed to perform a given task. According to the evaluation of nine different benchmarks, such as fast Fourier transform and multiply-accumulate, the proposed design flow accomplishes an average ∼7% reduction in memory area and ∼18% savings in total energy consumption. © 2021 Elsevier Inc.","Associative processor; Content addressable memory; Hardware/Software co-design; In-memory computing; Single instruction multiple data (SIMD)"
"Using hardware performance counters to speed up autotuning convergence on GPUs","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117699383&doi=10.1016%2fj.jpdc.2021.10.003&partnerID=40&md5=4dc1284b7d06f8190e3cc7dbb4b45c44","Nowadays, GPU accelerators are commonly used to speed up general-purpose computing tasks on a variety of hardware. However, due to the diversity of GPU architectures and processed data, optimization of codes for a particular type of hardware and specific data characteristics can be extremely challenging. The autotuning of performance-relevant source-code parameters allows for automatic optimization of applications and keeps their performance portable. Although the autotuning process typically results in code speed-up, searching the tuning space can bring unacceptable overhead if (i) the tuning space is vast and full of poorly-performing implementations, or (ii) the autotuning process has to be repeated frequently because of changes in processed data or migration to different hardware. In this paper, we introduce a novel method for searching generic tuning spaces. The tuning spaces can contain tuning parameters changing any user-defined property of the source code. The method takes advantage of collecting hardware performance counters (also known as profiling counters) during empirical tuning. Those counters are used to navigate the searching process towards faster implementations. The method requires the tuning space to be sampled on any GPU. It builds a problem-specific model, which can be used during autotuning on various, even previously unseen inputs or GPUs. Using a set of five benchmarks, we experimentally demonstrate that our method can speed up autotuning when an application needs to be ported to different hardware or when it needs to process data with different characteristics. We also compared our method to state of the art and show that our method is superior in terms of the number of searching steps and typically outperforms other searches in terms of convergence time. © 2021 Elsevier Inc.","Auto-tuning; Cuda; Performance counters; Search method"
"Workload-aware storage policies for cloud object storage","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124701039&doi=10.1016%2fj.jpdc.2022.01.026&partnerID=40&md5=5f8f28fbf793ee4f3fe67dad44ae6ece","Different applications have different access characteristics and various performance requirements. Thus, the shared cloud object store entails providing tenant-specific policies. However, the limited configurability of existing storage policies makes it difficult to provide efficient and flexible policies to meet tenants' evolving needs. First, existing policies that only control request forwarding cannot provide sufficient optimizations for workload performance. Second, those policies lack the flexibility to adapt to the possible workload changes during runtime. In this paper, we propose Mass, a programmable framework to provide the enhanced storage policies for diverse workloads based on their access characteristics. We also design its enhancements, C-Mass, extending Mass's capabilities through container-based policy deployment to efficiently handle workload changes. Compared with existing storage policies, the latency and throughput of workloads under Mass are improved by up to 81.6% and 231.5%, respectively. Further, the workload performance under C-Mass is optimized by up to 40%. © 2022 Elsevier Inc.","Cloud object storage; Middleware; Multi-tenancy; Resource management; Software-defined storage"
"Multi-GPU systems and Unified Virtual Memory for scientific applications: The case of the NAS multi-zone parallel benchmarks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114751453&doi=10.1016%2fj.jpdc.2021.08.001&partnerID=40&md5=fd2231c77b3184719844dab2d7625b10","GPU-based computing systems have become a widely accepted solution for the high-performance-computing (HPC) domain. GPUs have shown highly competitive performance-per-watt ratios and can exploit an astonishing level of parallelism. However, exploiting the peak performance of such devices is a challenge, mainly due to the combination of two essential aspects of multi-GPU execution: memory allocation and work distribution. Memory allocation determines the data mapping to GPUs, and therefore conditions all work distribution schemes and communication phases in the application. Unified Virtual Memory simplifies the codification of memory allocations, but its effects on performance depend on how data is used by the devices and how the devices' driver is going to orchestrate the data transfers across the system. In this paper we present a multi-GPU and Unified Virtual Memory (UM) implementation of the NAS Multi-Zone Parallel Benchmarks which alternate communication and computation phases offering opportunities to overlap these phases. We analyse the programmability and performance effects of the introduction of the UM support. Our experience shows that the programming efforts for introducing UM are similar to those of having a memory allocation per GPU. On an evaluation environment composed of 2 x IBM Power9 8335-GTH and 4 x GPU NVIDIA V100 (Volta), our UM-based parallelization outperforms the manual memory allocation versions by 1.10x to 1.85x. However, these improvements are highly sensitive to the information forwarded to the devices' driver describing the most convenient location for specific memory regions. We analyse these improvements in terms of the relationship between the computational and communication phases of the applications. © 2021 The Author(s)","Multi-GPU; NAS parallel benchmarks; Single address space; Unified Virtual Memory"
"Analysis of the Leaky Integrate-and-Fire neuron model for GPU implementation","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123953683&doi=10.1016%2fj.jpdc.2022.01.021&partnerID=40&md5=ea1155152a62dc3a47a09190c28da578","Understanding how neurons perform, when they are organized in interacting networks, is a key to understanding how the brain performs complex functions. Different models that approximate the behavior of interconnected neurons have been proposed in the literature. Implementing these models to simulate neuron behavior at an appropriately detailed level to observe collective phenomena is computationally intensive. In this study we analyze the coupled Leaky Integrate-and-Fire model and report on the issues that affect performance when the model is implemented on a GPU. We conclude that the problem is heavily memory-bound. Advances in memory technology at the hardware level seem to be the deciding factor to achieve better performance on the GPU. Our results show that using an NVidia K40 GPU a modest 2x speedup can be achieved compared to a parallel implementation running on a modern multi-core CPU. However, a substantial speedup of 11.1x can be achieved using an NVidia V100 GPU, mainly due to the improvements in its memory subsystem. © 2022 Elsevier Inc.","Computational neuroscience; GPU processing; Leaky Integrate-and-Fire model; Neural models; Neural networks"
"MiCS-P:Parallel mutual-information computation of big categorical data on spark","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121379208&doi=10.1016%2fj.jpdc.2021.12.002&partnerID=40&md5=ee162d36919aa860b03d90348e3e985c","Mutual information can effectively measure the correlation between categorical attributes. However, it is found to be quite computationally intensive and time consuming process for enormous size and different distribution data sets. It involves steps for computation of marginal entropies, probability distribution and so on. Spark is a fast, general-purpose parallel framework designed specifically for large-scale data processing. Main motivation of this paper is to provide an intelligent method for parallel mutual information calculation based on Spark computing environment with maintaining the synchronization between different computing nodes. Proposed method named MiCS-P has been able to execute with different number of computing nodes, and gives significant speedup working with different dimensions and sizes of data sets. The MiCS-P algorithm adopts column-wise transformation scheme, which is conducive to the calculation of mutual information between a large number of feature pairs. And to alleviate imbalanced load causing long execution times, we implement a two-phase virtual partitioning scheme running on Spark. © 2021 Elsevier Inc.","Big categorical data; Data skewness; Feature grouping; Parallel mutual-information computation; Spark"
"Quorums over codes","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119511040&doi=10.1016%2fj.jpdc.2021.11.002&partnerID=40&md5=5d17773af602e147b4d87826c85ea93e","We consider the design and analysis of quorum systems over erasure coded warm data (with low frequency of writes and accesses in general) to guarantee sequential consistency under a fail-stop model while supporting atomic read-modify-write operations by multiple clients. We propose a definition of asymmetric quorum systems that suit the framework of coded data by explicitly exploiting the structural properties of code and instantiate it over distinct families of coding strategies: maximum distance separable (MDS) codes and codes with locality, and we indicate a mechanism for synchronizing stale nodes using differential updates, which again exploits the code structures. The proposed quorum system's behavior is analyzed theoretically, exploring several aspects: viability of quorums under node unavailability; contention of resources between read and write operations; and quorum load. We complement these theoretical exploration with simulation based experiments to quantify the behavior of the proposed mechanism. The overall study demonstrates the feasibility and practicality of quorums over codes under practicable assumptions for achieving a stringent form of consistency, specifically, sequential consistency, while the stored data is being mutated by potentially multiple processes that might read and then modify the existing data. We achieve this in-place, without having to resort to store multiple versions of the data. © 2021 Elsevier Inc.","Erasure codes; Quorums; Read-modify-write; Sequential consistency"
"Scalable blockchain model using off-chain IPFS storage for healthcare data security and privacy","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126923855&doi=10.1016%2fj.jpdc.2022.03.009&partnerID=40&md5=95c298aee0607c4977519e81f3707cdb","Traditional healthcare systems in the present scenario follow centralized client-server architecture to store and process patient-health related information. Data stored in each of the healthcare institution remain in silos which cannot be easily shared with other institutions due to technical and infrastructural constraints. Hospitals do not have an effective and secure data sharing mechanism leading to monetary and resource loss in the case of a person visiting different hospitals. Blockchain, a disruptive technology with secure and reliable decentralized framework, and can be used to circumvent problems in traditional healthcare architecture for secure storage, sharing and retrieval of Electronic Health Records (EHR). A blockchain-based framework integrated with InterPlanetary File System (IPFS) for EHR in healthcare management has been proposed in this paper. This proposed framework will enable healthcare institutions to maintain fail-safe and tamper-proof healthcare ledgers in a decentralized manner. Hospitals and doctors act as lightweight nodes, whereas patient nodes can be full or lightweight nodes. The model proposes two-factor authentication and multi-factor authentication for preventing fake node attacks. Patient-centric access model allows the patients to act as digital stewards for their health data, allowing access to doctors and hospitals on demand and revoking it after stipulated time. Symmetric key encryption (AES-128) is used for encrypting data before storing into IPFS. Asymmetric encryption (RSA-4096) is used for generating digital envelopes to pass on symmetric key to authorized entities. Digital signatures (RSA-1024) make sure that the transactions are valid and from authorized nodes. Hashing of the encrypted data is done using SHA-256 algorithm. Multiple layers of security implemented in this model makes sure that adversaries cannot obtain data stored in IPFS; even if they retrieve the data, it will not be meaningful since it is encrypted. The proposed framework for off-chain storage of health data using IPFS saves blockchain structure from scalability issues. Further the proposal for blockchain integration with IPFS helps preserve privacy in the healthcare system, making it highly secure, scalable and robust. © 2022 Elsevier Inc.","Blockchain data security; Decentralized healthcare data; Electronic health records; Healthcare data security"
"Preemptive scheduling on unrelated machines with fractional precedence constraints","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111704740&doi=10.1016%2fj.jpdc.2021.07.010&partnerID=40&md5=821a17202f211fff07fc9dee4c6e2f82","Many programming models, e.g., MapReduce, introduce precedence constraints between the jobs. This paper formalizes a notion of precedence constraints, called fractional precedence constraints, where the progress of follower jobs only has to lag behind (fractionally) their leads. For a general set of fractional precedence constraints between the jobs, this paper provides a new class of preemptive scheduling algorithms on unrelated machines that have arbitrary processing speeds. In particular, for a given makespan, we establish both sufficient and necessary conditions on the existence of a feasible job schedule, and then propose an efficient scheduling algorithm based on a novel matrix decomposition method, if the sufficient conditions are satisfied. The algorithm is shown to be a Polynomial-Time Approximation Scheme (PTAS), i.e., its solution is able to achieve any feasible makespan with an approximation bound of 1+ϵ, for an arbitrary ϵ>0. © 2021 Elsevier Inc.","Birkhoff-von Neumann decomposition; Fractional precedence constraints; Preemptive scheduling; Unrelated machines"
"SG-PBFT: A secure and highly efficient distributed blockchain PBFT consensus algorithm for intelligent Internet of vehicles","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125015735&doi=10.1016%2fj.jpdc.2022.01.029&partnerID=40&md5=e6c442061405536ee3b94cc9016142b4","As an application of Internet of Things (IoT) technology, the Internet of Vehicles (IoV) faces two main security issues: (1) the central server of the IoV may not be powerful enough to support the centralized authentication of the rapidly increasing connected vehicles, (2) the IoV itself may not be robust enough to single-node attacks. To address these issues, in this paper, we propose SG-PBFT (Score Grouping-PBFT), a secure and efficient distributed consensus algorithm for blockchain applications in the IoV. The distributed structure can reduce the pressure on the central server and decrease the risk of single-node attacks. The SG-PBFT consensus algorithm improves the traditional practical Byzantine fault tolerance (PBFT) consensus algorithm by optimizing the PBFT consensus process and using a score grouping mechanism to achieve a higher consensus efficiency. The experimental results show that the method can greatly improve the consistency efficiency and effectively prevent single-node attacks. Specifically, when the number of consensus nodes reaches 1000, the consensus time of our algorithm is only about 27% of what is required for the state-of-the-art PBFT consensus algorithm. Our proposed SG-PBFT algorithm is versatile and can be used in other application scenarios which require high consensus efficiency. © 2022 Elsevier Inc.","Blockchain; Consensus algorithm; Identity authentication; Internet of things; Internet of vehicles"
"Probabilistic and temporal failure detectors for solving distributed problems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111928289&doi=10.1016%2fj.jpdc.2021.07.017&partnerID=40&md5=8e0475ef60c975775f943bcd491fdd73","Failure detectors (FD)s are celebrated for their modularity in solving distributed problems. Algorithms are constructed using FD building blocks. Synchrony assumptions to implement FDs are studied separately and are typically expressed as eventual guarantees that need to hold, after some point in time, forever and deterministically. But in practice, they may hold only probabilistically and temporarily. This paper studies FDs in a realistic system N, where asynchrony is inflicted by probabilistic synchronous communication. We first address a problem with ⋄S, the weakest FD to solve consensus: an implementation of “consensus with probability 1” is possible in N without randomness in the algorithm, while an implementation of “⋄S with probability 1” is impossible in N. We introduce ⋄S⁎, a new FD with probabilistic and temporal accuracy. We prove that ⋄S⁎ (i) is implementable in N and (ii) can replace ⋄S, in several existing deterministic consensus algorithms that use ⋄S, to yield an algorithm that solves “consensus with probability 1”. We extend our results to other FD classes, e.g., ⋄P, and to a larger set of problems (beyond consensus), which we call decisive problems. © 2021 Elsevier Inc.","Consensus; Failure detectors; Message loss; Modular algorithms; Probabilistic links"
"CS-Materials: A system for classifying and analyzing pedagogical materials to improve adoption of parallel and distributed computing topics in early CS courses","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112323802&doi=10.1016%2fj.jpdc.2021.05.014&partnerID=40&md5=dcf1f7a33b0fdfb2e38f42d47566a7a8","The NSF/IEEE-TCPP Parallel and Distributed Computing curriculum guidelines released in 2012 (PDC12) represents an effort to bring more parallel computing concepts into early computer science courses. To date, it has been moderately successful, with the inclusion of some PDC topics in the ACM/IEEE Computer Science curriculum guidelines in 2013 (CS13) and mentions of PDC topics in the Computing Curricula 2020. Additionally, some universities in the U.S. and around the world have started to cover some of these topics in early CS courses. Lack of knowledge of or training in PDC topics among instructors, along with the need to align early CS course content with prescribed learning objectives in the curricula, are often cited as hurdles for adoption in early CS courses. There have been attempts at bringing PDC materials, such as textbook chapters, lecture slides, assignments, and demos to assist instructors of early CS classes. However, the effort required on the part of the instructor to figure out what is relevant to a particular class can be daunting. In this work, we contend that simultaneously classifying pedagogical materials against the CS13 and the PDC12 curriculum guidelines can address some of the challenges faced by instructors and can promote broader adoption of PDC materials in early CS courses. We present CS Materials, a system that can be used to categorize pedagogical materials according to well-known and established curricular guidelines. We show that CS Materials can be leveraged 1) by instructors of early CS courses to find materials that are similar to the one that they use but that also cover PDC topics, and 2) by instructors to check the coverage of topics (and gaps) in a course, and 3) by PDC experts to identify topics for which PDC instructional materials do not exist or are insufficient in order to inform development of additional PDC curricular materials. © 2021","Analyzing the state of PDC education; cs education; Educational material recommendation; Integrating PDC in Early CS education"
"A fast and concise parallel implementation of the 8x8 2D forward and inverse DCTs using halide","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124318736&doi=10.1016%2fj.jpdc.2022.01.014&partnerID=40&md5=6b6105ff9731cbf79afc4696b88d08ab","The Discrete Cosine Transform (DCT) is commonly used for image and video coding and very efficient implementations of the forward and inverse transforms are of great importance. The popular libjpeg-turbo library contains handwritten, highly-optimised assembly language DCT implementations utilizing SIMD instruction sets for a variety of architectures. We present an alternative approach, implementing the 8x8 IDCT and FDCT written in the functional image processing language Halide. We show how less than 200 lines of Halide can replace over 20,000 lines of code the libjpeg-turbo library to perform JPEG encoding and decoding. The Halide implementation is compared for ARMv8 NEON and x86-64 SIMD extensions and shows a 5–25 percent performance improvement over the SIMD code in libjpeg-turbo for decoding and a 10–40 percent improvement for encoding. The Halide code is significantly easier to maintain and port to new architectures than the existing code. © 2022 Elsevier Inc.","Discrete cosine transform; Halide; JPEG; Parallel programming; SIMD"
"Joint optimization of cache placement and request routing in unreliable networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110665442&doi=10.1016%2fj.jpdc.2021.06.006&partnerID=40&md5=5792bc9d3646c141712c0e8fea8c5440","Edge caching is a prevailing media delivery technology where data is hosted at the edge nodes with computing and storage capability in close proximity to users, in order to expand the backhaul network capacity and enhance users' quality of experience (QoE). The existing work in this area often neglects the fact that large-scale distributed cache networks are not particularly reliable and many edge nodes are prone to failure. In this paper we investigate and develop a novel, cooperative caching mechanism for content placement and request routing. We aim to minimize the content access delay and achieve the optimization in polynomial time, taking into account failures in an unreliable network environment with limited edge storage and bandwidth. We introduce two optimization algorithms: 1) a primal-dual algorithm that is based on the Lagrangian dual decomposition and subgradient method, and 2) a greedy-based approximation algorithm with a proven approximation ratio. Numerical results show that the proposed algorithms outperform other comparative approaches in synthetic and real network environments, and the approximation algorithm is particularly suitable for networking scenarios with sparse node connectivity and resources in short supply. © 2021 Elsevier Inc.","Content placement; Edge caching; Network unreliability; Optimization; Request scheduling"
"Optically connected memory for disaggregated data centers","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124986604&doi=10.1016%2fj.jpdc.2022.01.013&partnerID=40&md5=3c7684ec907b0de16a8ff44af9c90454","Recent advances in integrated photonics enable the implementation of reconfigurable, high-bandwidth, and low energy-per-bit interconnects in next-generation data centers. We propose and evaluate an Optically Connected Memory (OCM) architecture that disaggregates the main memory from the computation nodes in data centers. OCM is based on micro-ring resonators (MRRs), and it does not require any modification to the DRAM memory modules. We calculate energy consumption from real photonic devices and integrate them into a system simulator to evaluate performance. Our results show that (1) OCM is capable of interconnecting four DDR4 memory channels to a computing node using two fibers with 1.02 pJ energy-per-bit consumption and (2) OCM performs up to 5.5× faster than a disaggregated memory with 40G PCIe NIC connectors to computing nodes. © 2022 Elsevier Inc.","Data-centers; Disaggregated memory; DRAM; Memory systems; Photonics"
"Exponential type of many-to-many edge disjoint paths on ternary n-cubes","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113267284&doi=10.1016%2fj.jpdc.2021.07.020&partnerID=40&md5=f6d68d7a4dce038cee987790393f03eb","One of most central issues in various interconnection networks of parallel and distributed systems is to find edge-disjoint paths concerned with an information transmission through links. It is universally acknowledged that an interconnection network can be modeled as an undirected simple graph G=(V,E) with processors and physical links between the processors represented as vertices and edges of the G, respectively. The studies on the edge-disjoint paths are closely related to the edge-connectivity. By the well-known Menger theorem, the maximum number of edge disjoint paths connecting any two disjoint connected subgraphs with g vertices in G can also define by the minimum modified edge-cut, called the g-extra edge-connectivity of G λg(G). It is the cardinality of the minimum set of edges in G, if such a set exists, whose deletion disconnects G and leaves every remaining component with at least g vertices. The k-ary n-cube network is known as one of the most attractive interconnection networks for parallel and distributed systems. This article intends to show that for [Formula presented], the g-extra edge-connectivity of ternary n-cube (also called 3-ary n-cube) (n≥3) exists a concentration behavior, and prove that these corresponding g-extra edge-connectivities of ternary n-cubes are the constants [Formula presented], where [Formula presented], and e=1 if n is odd and e=0 if n is even. The above upper and lower bounds of g are sharp. As the integer g varies within the range between [Formula presented] and [Formula presented], a necessary and sufficient condition of λg-optimality is found. Our results improve the several previous results. © 2021 Elsevier Inc.","Edge disjoint paths; Edge fault tolerance; g-extra edge-connectivity; Reliability evaluation; Ternary n-cubes"
"Amnis: Optimized stream processing for edge computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118829548&doi=10.1016%2fj.jpdc.2021.10.001&partnerID=40&md5=1786794f9f74f63d5c7fc6ddef13e1ad","The proliferation of Internet-of-Things (IoT) devices is rapidly increasing the demands for efficient processing of low latency stream data generated close to the edge of the network. Edge computing-based stream processing techniques that carefully consider the heterogeneity of the computational and network resources available in the infrastructure provide significant benefits in optimizing the throughput and end-to-end latency of the data streams. In this paper, we propose a novel stream query processing framework called Amnis that optimizes the performance of the stream processing applications through a careful allocation of computational and network resources available at the edge. The Amnis approach differentiates itself through its consideration of data locality and resource constraints during physical plan generation and operator placement for the stream queries. Additionally, Amnis considers the coflow dependencies to optimize the network resource allocation through an application-level rate control mechanism. We implement a prototype of Amnis in Apache Storm. Our performance evaluation carried out in a real testbed shows that the proposed techniques achieve as much as 200X improvement on the end-to-end latency and 10X improvement on the overall throughput compared to the default resource aware scheduler in Storm. © 2021 Elsevier Inc.","Coflow; Data locality; Edge computing; Operator placement; Stream processing"
"Efficient classification of private memory blocks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111310752&doi=10.1016%2fj.jpdc.2021.07.005&partnerID=40&md5=bc8e375ab39c5ab77b88600a9c7508bc","Shared memory architectures are pervasive in the multicore technology era. Still, sequential and parallel applications use most of the data as private in a multicore system. Recent proposals using this observation and driven by a classification of private/shared memory data can reduce the coherence directory area or the memory access latency. The effectiveness of these proposals depends on the accuracy of the classification. The existing proposals perform the private/shared classification at page granularity, leading to a miss-classification and reducing the number of detected private memory blocks. We propose a mechanism able to accurately classify memory blocks using the existing translation lookaside buffers (TLB), which increases the effectiveness of proposals relying on a private/shared classification. Our experimental results show that the proposed scheme reduces L1 cache misses by 25% compared to a page-grain classification approach, which translates into an improvement in system performance by 8.0% with respect to a page-grain approach. © 2021 Elsevier Inc.","Cache coherence; Chip multiprocessor; Private-shared data classification"
"Customer-satisfaction-aware and deadline-constrained profit maximization problem in cloud computing","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124479731&doi=10.1016%2fj.jpdc.2022.02.003&partnerID=40&md5=fa8729acecf15fd2e7bc699ac70d94e0","As a new and modern service method that can meet the requirements of customers effectively, cloud computing is becoming more and more popular. For cloud service providers, they play an important role in constructing the cloud computing platform, which is convenient for customers to enjoy the services without paying attention to the execution process of the services. For cloud service providers and customers, they focus on profit and satisfaction respectively in service supply-demand relationship, and the pursuit of their objectives both have an influence on each other. On one hand, the configuration of the cloud computing platform is the main solution for cloud service providers to earn profit, which affects customer satisfaction based on specific constraints. On the other hand, customer satisfaction affects the arrival rate of service requests, which will affect the profit in turn. In this paper, we devote ourselves to analyzing a deadline constrained profit maximization problem in a cyclic cascade queuing system, in which the maximum tolerance of customers towards task waiting time is taken into consideration. On this basis, the definition of customer satisfaction is given. And then, a mathematical model is formulated in detail on how customer satisfaction affects the revenues of cloud service providers, and further affects their profits. However, the exact solutions of such model can hardly be calculated due to its complexity. Hence, we propose a heuristic algorithm to find the high-quality solution, then the optimal configuration of cloud computing platform can be obtained to achieve the maximization of the profit for cloud service providers on the basis of promoting customer satisfaction. At last, a series of numerical simulations are conducted to validate the performance of the proposed algorithm. The results show that the proposed algorithm can not only optimize profit and customer satisfaction simultaneously but also maintain them at a high level in the long run of the system. © 2022 Elsevier Inc.","Cloud computing; Cyclic cascade queuing system; Heterogeneous; Profit maximization; Satisfaction; Waiting time"
"Teaching and learning HPC through serious games","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113349330&doi=10.1016%2fj.jpdc.2021.07.014&partnerID=40&md5=01936947256d37b4e50777891d4598a4","Serious games provide pathways for learners to develop intuition about concepts that are new to them. Such games are especially valuable in an educational context because they engage students in active learning and provide a means for students to develop mental models from their experiences. Furthermore, for short term learning experiences, such as informal, ungraded workshops, they act as ice-breaker activities that help build a sense of community among disparate learners. Collaborative games also spawn peer-to-peer discussion. These student experiences generally lead to deeper questions and discussions that allow instructors to uncover more concepts and clarify misunderstandings, all of which reinforce learning. The authors have created and incorporated role playing games within informal High Performance Computing (HPC) courses to provide students with tangible hands-on experiences with HPC concepts. All of the games are designed so that learners work collaboratively and take on a role associated with an HPC system. Engaging in the role playing highlights the strengths, weaknesses, and challenges of HPC. In this article we describe the games, how they were integrated into HPC courses, both in-person and virtual, and the lessons learned by both students and instructors. Though played with professional and university learners, the games are easily generalized to other cohorts. © 2021 Elsevier Inc.","HPC education; HPC training; Serious games"
"Intermediate data placement and cache replacement strategy under Spark platform","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124199568&doi=10.1016%2fj.jpdc.2022.01.020&partnerID=40&md5=39b703642e7d694c77f6752f597d2208","Spark is widely used due to its high performance caching mechanism and high scalability, which still causes uneven workloads and produces useless intermediate caching results when faced with data-intensive applications. A data placement strategy based on an improved reservoir sampling algorithm is proposed to solve the problem of intermediate data tilt in the shuffle stage of Spark. Compared with the traditional sampling algorithm, the amount of intermediate data is accumulated while sampling. The data skew measurement model is used to classify data into skewed data, and non-skewed and coarse-grained, and fine-grained placement algorithms are designed. To further improve Spark's system memory utilization and cache hit rate, an adaptive cache replacement algorithm is proposed to maximize cache gain. We analyze the operational dependencies and propose a cache gain model. Compared with the traditional method, the two known and unknown job arrival rates are considered separately to obtain an online adaptive cache replacement strategy that maximizes cache gain. Experimental results show that our data placement strategy effectively reduces Spark applications' execution time and improves the load balance of reduce tasks. Meanwhile, the proposed adaptive cache replacement strategy effectively reduces Spark's average completion time and improves the memory utilization and cache hit rate. © 2022 Elsevier Inc.","Adaptive cache replacement; Cache gain; Data shuffling; Intermediate data placement"
"Pipelined Preconditioned Conjugate Gradient Methods for real and complex linear systems for distributed memory architectures","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124630331&doi=10.1016%2fj.jpdc.2022.01.008&partnerID=40&md5=0f7943ab3d1668bfe30867e70c3a001b","Preconditioned Conjugate Gradient (PCG) is a popular method for solving large and sparse linear systems of equations. The performance of PCG at scale is affected due to the costly global synchronization steps that arise in dot-products on distributed memory systems. Pipelined PCG (PIPECG) removes the costly global synchronization steps from PCG by only executing a single non-blocking allreduce per iteration and overlapping it with independent computations. In our previous work, we have developed a novel pipelined PCG algorithm called PIPECG-OATI (One Allreduce per Two Iterations) for real linear systems which executes a single non-blocking allreduce per two iterations and provides a large overlap of global communication with independent computations at higher number of cores. Our method achieves this overlap by using iteration combination and by introducing new recurrence and non-recurrence computations. We implement optimizations in the PIPECG-OATI method to use cache memory efficiently. In this work, we present PIPECG-OATI-c method for linear systems with complex Hermitian positive definite and complex symmetric matrices. We compare our method with various pipelined CG methods on a variety of problems and demonstrate that our method always gives the least run times. We performed experiments with our method using 20M and 30M unknowns on up to 16K cores and obtained up to 2.48X performance improvement over PCG and 2.14X performance improvement over PIPECG methods. We also experimented with up to 1-billion unknowns on 16K cores, the largest problem size explored for the CG problem, to our knowledge, and obtained about 25% improvement over PCG. © 2022 Elsevier Inc.","Complex Hermitian positive definite systems; Complex symmetric systems; Overlapping communication and computations; Pipelining; Preconditioned Conjugate Gradient"
"FGFL: A blockchain-based fair incentive governor for Federated Learning","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124975447&doi=10.1016%2fj.jpdc.2022.01.019&partnerID=40&md5=70967a1053dd7b7878bc44aa8d50c7a5","Federated Learning is a framework that coordinates a large amount of workers to train a shared model in a distributed manner, in which the training data are located on the workers' sides in order to preserve data privacy. There are two challenges in the crowdsourcing of FL, the workers who participant in training need to consume computing and communication resources, so that they are reluctant to participate in the training process if they can not get reasonable rewards. Moreover, there may be attackers who send arbitrary updates to get undeserving compensation or even destroy the model, thus, effective prevention of malicious workers is also critical. An incentive mechanism is urgently required in order to encourage high-quality workers to participate in FL and to punish the attackers. In this paper, we propose FGFL, a blockchain-based incentive governor for Federated Learning. In FGFL, we assess the participants with reputation and contribution indicators. Then the task publisher rewards workers fairly to attract efficient ones while the malicious ones are punished and eliminated. In addition, we propose a blockchain-based incentive management system to manage the incentive mechanism. We evaluate the effectiveness and fairness of FGFL through theoretical analysis and comprehensive experiments. The evaluation results show that FGFL fairly rewards workers according to their corresponding behavior and quality. FGFL increases the system revenue by 0.2% to 3.4% in reliable federations compared with baselines. And in the unreliable scenario where contains attackers, the system revenue of FGFL outperforms the baselines by more than 46.7%. © 2022 Elsevier Inc.","Attack detection; Federated Learning; Incentive mechanism"
"FMapper: Scalable read mapper based on succinct hash index on SunWay TaihuLight","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119916442&doi=10.1016%2fj.jpdc.2021.11.004&partnerID=40&md5=35f35af7b2f3a00f1d10fd8dfe3ef499","One of the most important application in bioinformatics is read mapping. With the rapidly increasing number of reads produced by next-generation sequencing (NGS) technology, there is a need for fast and efficient high-throughput read mappers. In this paper, we present FMapper – a highly scalable read mapper on the TaihuLight supercomputer optimized for its fourth-generation ShenWei many-core architecture (SW26010). In order to fully exploit the computational power of the SW26010, we employ dynamic scheduling of tasks, asynchronous I/O and data transfers and implement a vectorized version of the banded Myers algorithm tailored to the 256 bit vector registers of the SW26010. Our performance evaluation demonstrates that FMapper using all 4 compute groups of a single SW26010 processor outperforms S-Aligner on the same hardware as well as RazerS3, Hobbes3, Minimap2 and BWA running on a 4-core Xeon W-2123v3 CPU and achieves speedups of 4.7, 24.8, 2.4, 4.6 and 14.7 respectively. Using several optimizations, we achieve a speedup of 6 compared to the naïve implementation on one compute group of an SW26010 processor and a strong scaling efficiency of 65% on 512 compute groups. © 2021","Bioinformatics; Heterogeneous computing; Read mapping; SunWay TaihuLight"
"MDScale: Scalable multi-GPU bonded and short-range molecular dynamics","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111327466&doi=10.1016%2fj.jpdc.2021.07.006&partnerID=40&md5=e8e0010cb459e8dc829846e4a93d27f5","GPUs have enabled a drastic change to computing environments, making massively parallel computing possible. Molecular dynamics is a perfect candidate problem for massively parallel computing, but to date it has not taken full advantage of multi-GPU environments due to the difficulty of partitioning molecular dynamics problems and exchanging problem data among compute nodes. These difficulties restrict the use of GPUs to only some of the computations in a full molecular dynamics problem, and hence prevent scalability beyond just a few GPUs. This work presents a scalable parallelization solution for the bonded and short-range forces present in a molecular dynamics problem. Together with existing solutions for long-range forces, it enables highly scalable, parallel molecular dynamics on multi-GPU computing environments. Specifically, the proposed solution divides the molecular volume into independent parts assigned to different GPUs, but it maintains a global bond structure that is efficiently exchanged when atoms move across GPUs. We demonstrate close-to-linear speedup of the proposed solution, simulating the dynamics of gigamolecules with 1 billion atoms on a computing environment with 96 GPUs, and obtaining superior performance to the well known molecular dynamics simulator NAMD. © 2021 The Author(s)","Bonded forces; Large scale; Molecular dynamics; Multi-GPU; NAMD; Van der walls forces"
"Hybrid heuristic-based key generation protocol for intelligent privacy preservation in cloud sector","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124456759&doi=10.1016%2fj.jpdc.2022.01.005&partnerID=40&md5=072547861d2a78dedd3ef6ae072bc451","Cloud Computing is defined as a set of software and hardware that are used together for delivering different kinds of cloud services based on user's demand. Cloud computing secures a major role in the Information Technology (IT) industry for accessing the services at any place around the world. On the other hand, there are increasing vulnerabilities and threats in the cloud environment due to the rise in popularity and demands in cloud computing services. Data privacy and integrity are the major issues in cloud computing while storing data in various geographical locations. So, it is necessary to consider the data privacy and integrity factors in a cloud computing environment. It is difficult to construct a common platform to interact in the cloud environment. So, it is essential for implementing the security solutions, which need to provide confidentiality while exchanging the data. The main intention of this paper is to design and develop a novel artificial intelligence approach for handling the privacy preservation problem in the cloud sector. Through the process of data sanitization, the sensitive data is hidden, so that it cannot be accessed by unauthorized users. To perform the sanitization process, the heuristic-based key generations play a vital role, and here, it is solved by considering a multi-objective function with constraints like the “degree of modification, hiding ratio, and information preservation ratio”. This multi-objective problem is solved by the adoption of novel Probability Switch searched Butterfly-Moth Flame Optimization (PS-BMFO). Finally, the restoration is also performed by the same PS-BMFO-based key generation. The analysis results confirm that the suggested model preserves privacy and ensures the integrity of the user's data against unauthorized parties. From the experimental analysis, the proposed PS-BMFO gives 12.5%, 10%, 30.7%, and 12.5% enriched than GWO, JA, MFO, and BOA, respectively. Therefore, the statistical analysis shows that the developed data privacy preservation model with suggested PS-BMFO performs better than the other conventional algorithms. © 2022 Elsevier Inc.","Data restoration; Data sanitization; Intelligent privacy preservation in cloud; Key generation protocol; Probability switch searched butterfly-moth flame optimization"
"SimGQ+: Simultaneously evaluating iterative point-to-all and point-to-point graph queries","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125219289&doi=10.1016%2fj.jpdc.2022.01.007&partnerID=40&md5=c05852abf1c06cdd38e68d8f4128755e","Graph processing frameworks are typically designed to optimize the evaluation of a single graph query. However, in practice, we often need to respond to multiple graph queries, either from different users or from a single user performing a complex analytics task. Therefore in this paper we develop SimGQ+, a system that optimizes simultaneous evaluation of a group of vertex queries that originate at different source vertices (e.g., multiple shortest path queries originating at different source vertices) and delivers substantial speedups over a conventional framework that evaluates and responds to queries one by one. Our work considers both point-to-all and point-to-point queries. The performance benefits are achieved via batching and sharing. Batching fully utilizes system resources to evaluate a batch of queries and amortizes runtime overheads incurred due to fetching vertices and edge lists, synchronizing threads, and maintaining computation frontiers. Sharing dynamically identifies shared queries that substantially represent subcomputations in the evaluation of different queries in a batch, evaluates the shared queries, and then uses their results to accelerate the evaluation of all queries in the batch. With four input power-law graphs and four graph algorithms SimGQ+ achieves speedups of up to 45.67× with batch sizes of up to 512 queries over the baseline implementation that evaluates the queries one by one using the state of the art Ligra system. Moreover, both batching and sharing contribute substantially to the speedups. © 2022 The Authors","Amortizing overhead; Batch of queries; Graph analytics; Power-law graphs; Sharing computation"
"VSIM: Distributed local structural vertex similarity calculation on big graphs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111867914&doi=10.1016%2fj.jpdc.2021.07.009&partnerID=40&md5=1b3d870915cad4751624447dd476029e","In many graph analytical applications, the local structural vertex similarity calculation is an essential prerequisite for advanced graph mining. The similarity calculation finds out all the similar vertex pairs whose local structural similarity scores (like the number of common neighbors, and the Jaccard index of adjacency sets) are above a given threshold. The real-world applications use a wide range of similarity thresholds. However, the existing distributed methods for the problem only optimize for either high thresholds (> 0.7) or low thresholds (< 0.1). To overcome the drawback, we propose a new distributed vertex similarity calculation framework VSIM that is efficient under a broad range of thresholds. VSIM processes static undirected graphs with local structural similarity scores that measure the similarity between vertices based on the first-order topology information. VSIM generates a similarity calculation task for every vertex in the graph and conducts all the tasks in parallel on a distributed computing platform along with a distributed key-value store. Each task finds vertices similar to a given center vertex with two task execution modes. The two modes optimize for high and low thresholds, respectively. Each task picks the suitable mode adaptively according to cost estimation models. We also propose an efficient implementation for VSIM on Apache Spark, with three optimization techniques to reduce communication costs and balance workloads on power-law graphs. The experimental evaluation shows that VSIM outperforms the state-of-the-art distributed methods by up to 67x speedup. VSIM can achieve near-linear node scalability in low-threshold and small cache scenarios. © 2021 Elsevier Inc.","Distributed graph processing; Graph data engineering; Local structural similarity score; Task-parallel algorithm; Vertex similarity calculation"
"XML2HBase: Storing and querying large collections of XML documents using a NoSQL database system","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120451174&doi=10.1016%2fj.jpdc.2021.11.003&partnerID=40&md5=258e5288d88f19e8c7824647c76fa231","Many big data applications such as smart transportation, healthcare, and e-commerce need to store and query large collections of small XML documents, which has become a fundamental problem. However, existing solutions are inadequate to deliver satisfactory query performance in such circumstances. In this paper, we propose a framework named XML2HBase to address this problem using HBase, a widely deployed NoSQL database. Within this framework, we design a novel encoding scheme called Pathed-Dewey Order and a two-layer mapping method to store XML documents in HBase tables. XML queries, which are represented as XPath expressions, are evaluated through their translation into queries over HBase tables. Based on an in-depth analysis of the characteristics of the proposed approach, we design and integrate four optimization strategies to reduce storage space and query response time. Extensive experiments on two well-known XML benchmarks demonstrate the superior performance of XML2HBase over three state-of-the-art methods. © 2021 Elsevier Inc.","NoSQL database; XML data mapping; XML encoding scheme; XML query processing and optimization"
"Momentum-driven adaptive synchronization model for distributed DNN training on HPC clusters","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116898435&doi=10.1016%2fj.jpdc.2021.09.007&partnerID=40&md5=67d2cccf1ec6644d36d6653564b70f63","Building a distributed deep learning (DDL) system on HPC clusters that guarantees convergence speed and scalability for the training of DNNs is challenging. The HPC cluster, which consists of multiple high-density multi-GPU servers connected by the Infiniband network (HDGib), compresses the computing and communication time for distributed DNNs' training but brings new challenges. The convergence time is far from linear scalability (with respect to the number of workers) for parallel DNNs training. We thus analyze the optimization process and identify three key issues that cause scalability degradation. First, the high-frequency update for parameters due to the compression of the computing and communication times exacerbates the stale gradient problem, which slows down the convergence. Second, the previous methods used to constrain the gradient noise (stochastic error) of the SGD are outdated, as HDGib clusters can support more strict constraints due to the Infiniband network connections, which can further constrain the stochastic error. Third, the same learning rate for all workers is inefficient due to the different training stages of each worker. We thus propose a momentum-driven adaptive synchronization model that focuses on solving the above issues and accelerating the training procedure on HDGib clusters. Our adaptive k-synchronization algorithm uses the momentum term to absorb the stale gradients and adaptively bind the stochastic error to provide an approximate optimal descent direction for the distributed SGD. Our model also includes an individual dynamic learning rate search method for each worker to further improve training performance. Compared with previous linear and exponent decay methods, it can provide a more precise descent distance for distributed SGD based on different training stages. Extensive experimental results indicate that the proposed model effectively improves the training performance of CNNs, which retains high accuracy with a speed-up of up to 57.76% and 125.3% on the CPU-based and GPU-based clusters, respectively. © 2021 Elsevier Inc.","Adaptive; Distributed deep learning system; High-performance computing; Momentum-driven; Synchronization model"
"Reachability in parallel programs is polynomial in the number of threads","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123235862&doi=10.1016%2fj.jpdc.2021.11.008&partnerID=40&md5=7278bf3b0ef17d271d2b6846a64de494","Reachability in parallel finite-state programs equipped with interleaving semantics is an inherently difficult, important problem. Its complexity in the number of threads n, while keeping the thread-local–memory size and the shared-memory size bounded by constants, has been explored only poorly. We significantly narrow this gap by measuring: (i) the diameter, i.e., the longest finite distance realizable in the transition graph of the program, (ii) the local diameter, i.e., the maximum finite distance from any program state to any thread-local state, and (iii) the computational complexity of finding bugs. We prove that all these are majorized by polynomials in n and, in certain cases, by linear, logarithmic, or even constant functions in n; we make the bounds explicit whenever possible. Our results shed new light on the widely expressed claim that one of the major obstacles to analyzing parallel programs is the exponential state explosion in the number of threads. © 2021 Elsevier Inc.","Diameter; Interleaving; Multithreading; State-space explosion; Transition graph"
"Early scheduling on steroids: Boosting parallel state machine replication","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125476641&doi=10.1016%2fj.jpdc.2022.02.001&partnerID=40&md5=e5217ce25e3661dff2e18fbbc0c6fb2c","State machine replication (SMR) is a standard approach to fault tolerance in which replicas execute requests deterministically and often serially. For performance, some techniques allow concurrent execution of requests in SMR while keeping determinism. Such techniques exploit the fact that independent requests can execute concurrently. A promising category of early scheduling solutions trades scheduling freedom for simplicity, allowing to expedite decisions during scheduling. This paper generalizes early scheduling and proposes a general method to schedule requests to threads, restricting scheduling overhead. Moreover, it explores improvements to the original early scheduling mechanism, namely the use of busy-wait synchronization and work-stealing techniques. We integrate early scheduling and its proposed improvements to a popular SMR framework. Performance results of the basic mechanism and its improvements are presented and compared to more classic approaches, where it is shown that early scheduling with our proposed enhancements can outperform the original early scheduling and other systems by a large margin in many scenarios. © 2022 The Author(s)","Scheduling; State machine replication; Synchronization; Work-stealing"
"Core-aware combining: Accelerating critical section execution on heterogeneous multi-core systems via combining synchronization","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123703395&doi=10.1016%2fj.jpdc.2022.01.001&partnerID=40&md5=cbd593e2628ef222cf2e8611658419a4","In heterogeneous multi-core systems, performance differences of the cores can affect lock synchronization, where the high-performance cores have to wait for slower cores to complete critical section execution. To better utilize the high-performance cores, we can offload critical section execution to high-performance cores. Since combining synchronization has the potential to transfer critical section execution to the combiner, this paper presents a core-aware combining approach for heterogeneous multi-core processors to accelerate critical section execution. In combining synchronization, one competing thread will become the combiner to help complete pending requests. It typically provides better performance than conventional locks on multi-core systems. To enable transferring critical section executions to a more efficient core, we implement the ideas of core efficiency-based selective lock ownership transfer and the dynamic helping quota in four combining implementations. On an aarch64 heterogeneous machine and an x86 asymmetric machine, we ran several micro-benchmarks and workloads to evaluate the performance of our core-aware implementations. The results show that core-aware combining implementations accelerate critical section execution and achieve better throughput than the original combining implementations. © 2022 Elsevier Inc.","Blocking algorithms; Combining; Critical section; Heterogeneous multi-core system; Synchronization"
"Energy efficient spiking neural network processing using approximate arithmetic units and variable precision weights","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114130013&doi=10.1016%2fj.jpdc.2021.08.003&partnerID=40&md5=9bb731f51dc4a232b611dabeaca2e7b0","Spiking neural networks (SNNs) have been getting more research attention in recent years as the way they process information is suitable for building neuromorphic systems effectively. However, realizing SNNs on hardware is computationally expensive. To improve their efficiency for hardware implementation, a field-programmable gate array (FPGA) based SNN accelerator architecture is proposed and implemented using approximate arithmetic units. To identify the minimal required bit-width for approximate computation without any performance loss, a variable precision method is utilized to represent weights of the SNN. Unlike the conventional reduced precision method applied to all weights uniformly, the proposed variable precision method allows different bit-widths to represent weights and provide the feasibility of maximizing truncation effort for each weight. Four SNNs adopting different network configurations and training datasets are established to compare the performance of proposed accelerator architecture using the variable precision method with the proposed one using the conventional reduced precision method. Based on the experimental results, more than 40% of the weights require less bit-width when applying the variable precision method instead of the reduced precision method. With the variable precision method, the proposed architecture achieves 28% fewer ALUTs and 29% less power consumption than the proposed one using the reduced precision method. © 2021 Elsevier Inc.","Approximate computing; Field programmable gate array; Hardware accelerator; Spiking neural network"
"Integrating big data and cloud computing topics into the computing curricula: A modular approach","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111827341&doi=10.1016%2fj.jpdc.2021.07.012&partnerID=40&md5=31487bb46a6036b000a0e4eab15f2ee2","Big data and cloud computing collectively offer a paradigm shift in the way businesses are now acquiring, using, and managing information technology. This creates the need for every CS student to be equipped with foundational knowledge in this collective paradigm and possess some hands-on experience in deploying and managing big data applications in the cloud. This study argues that, for substantial coverage of big data and cloud computing concepts and skills, the relevant topics need to be integrated into multiple core courses across the CS curriculum rather than creating additional courses and performing a major overhaul of the curriculum. Our approach to including these topics is to develop autonomous competency-based learning modules for specific core courses in which their coverage might find an appropriate context. In this paper, four such modules are discussed, and our classroom experiences during these interventions are documented. Student performance data and survey results show reasonable success in attaining student learning outcomes, enhanced engagement, and interests. © 2021 The Author(s)","Bigdata; Cloud computing; Competency-based learning; CS curriculum; Module"
"Collaborative deep learning framework on IoT data with bidirectional NLSTM neural networks for energy consumption forecasting","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124987996&doi=10.1016%2fj.jpdc.2022.01.012&partnerID=40&md5=0ec59d6838265385d0846e96a72f968a","Energy consumption forecasting based on IoT data and deep learning algorithm inheriting distributed and collaborative learning is a widely studied topic both in engineering and computer science fields. For different households with drastically different energy consumption patterns, the traditional centralized machine learning (ML) and deep learning (DL) methods suffer problems including inaccuracy, inefficiency and laggings of the prediction performance. In this study, we propose a sophisticated multi-channel bidirectional nested LSTM framework (MC-BiNLSTM) combined with discrete stationary wavelet transform (SWT) for highly accurate and efficient energy consumption forecasting. The main contributions of this study include the decomposition using SWT for accuracy improvement and the collaborative BiNLSTM structure for efficiency improvement. A real-world IoT energy consumption dataset, named UK-DALE, is adopted for the comparative study. The experimental results showed the outperformance of the proposed method from various perspectives over the cutting-edge methods existed in the literature. © 2022 Elsevier Inc.","Energy consumption; LSTM; Stationary wavelet transform; Time series forecasting"
"Lifespan-based garbage collection to improve SSD's reliability and performance","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125463282&doi=10.1016%2fj.jpdc.2022.02.006&partnerID=40&md5=1bd03d2e8f393cad12d7846cc4fe3b36","Given the “out-of-place” nature of write mode, Solid State Drive (SSD) has to execute a garbage collection (GC) to reclaim the space of invalidated data and free flash blocks for new written data. However, the GC can incur the movements of valid data inside SSD, resulting in the problems such as write latency, write amplification, etc. The GC overheads of SSD depend not only on the current write pattern but also on how data have been already placed in SSD. If data can be classified by their lifetime when written to SSD, and the data with similar lifetime are written into the same flash block, we can select the block with the shortest expected lifespan as the victim block. Ideally, the victim block contains few valid data, implying there are few valid pages that need to be migrated during the GC process with greatly reduced overhead. In this paper, we propose and implement a GC method, called Lifespan-based GC, for SSDs based on the data lifetime, together with the I/O distribution and the dynamic flash memory allocation. The results show that for those applications with the data lifetime characteristics, the lifespan-based GC can effectively reduce the amount of valid pages migrated in GC process, thereby minimizing the write latency and write amplification caused by GC. © 2022 Elsevier Inc.","Data lifetime; Garbage collection; Solid state drive"
"Component diagnosability in terms of component connectivity of hypercube-based compound networks","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122525064&doi=10.1016%2fj.jpdc.2021.12.004&partnerID=40&md5=0c723022a0359d90105659c4e2c61175","Enhancing the invulnerability of multiprocessor systems against malicious attacks has been regarded as one of the important issues in network science and big data era. Thus, in order to firmly characterize the robustness of systems, several variants of classic connectivity have been proposed so far. The component connectivity is a significant metric in evaluating the robustness and fault tolerability of interconnection network. For an interconnection network G and a positive integer h, the (h+1)-component connectivity of G, denoted cκh+1(G), is the cardinality of a minimum vertex cut F such that G−F has at least h+1 connected components. Based on component connectivity, component diagnosability has been proposed to measure the self-diagnosis capability of multiprocessor systems. In this paper, we suggest some characterizations of the (h+1)-component connectivity of a class of regular networks under some restrictions. Furthermore, we establish the relationship between component connectivity and component diagnosability of one class of networks. As by-products, we present the (h+1)-component diagnosability of the state-of-the-art compound networks based on hypercube, such as bicube network, generalized exchanged hypercube, hierarchical hypercube, half-hypercube, and so on. © 2022 Elsevier Inc.","Component connectivity; Component diagnosability; Multiprocessor systems; Robustness"
"Resource allocation using Dynamic Pricing Auction Mechanism for supporting emergency demands in Cloud Computing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115351199&doi=10.1016%2fj.jpdc.2021.07.016&partnerID=40&md5=e1f2d2cbef3dbc9f59030ab82c56f8f6","Cloud resources provide various categories of (VM) virtual machine requests which is assigned with clients for an exact timespan. Currently, the method of VM scheduling within the Cloud environment is decided by a fixed-price scheduling algorithm, during which the user pays a fixed amount per unit time so as to obtain the resources. However, such a scheduling algorithm is not effective for Cloud even though the Cloud resources are dynamically allocated and released. To address this issue, the adaptive scheduling algorithm called as Dynamic pricing based Combinatorial Auction allocation mechanism is proposed. It will be used to increase the resource utilization as well as user satisfaction through dynamic pricing with the combinatorial auction. Our proposed market-based scheduling algorithm uses the principle of auction mechanism for the purpose of extends the satisfaction of Cloud suppliers and clients. This technique reconstructs the current preferences of resource allotment so as to allot resources in advance for emergent virtual machine demands. Then Collective-target augmentation numerical prototype is demonstrated, which forms the minimal execution equivalent range connecting physical machines with virtual machines, and the objective of resource allotment is to obtain minimal quantity of physical machines. The simulation experimental outcomes express that the proposed scheduling methodology and Collective-target augmentation numerical prototypes are capable to adequately increase the quality of service (QoS), improves profit of suppliers and resource utilization. © 2021 Elsevier Inc.","Auction-based scheduling; Cloud Computing; Combinatorial auction; Priority; Resource allocation"
"Dynamic asynchronous iterations","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127028312&doi=10.1016%2fj.jpdc.2022.03.013&partnerID=40&md5=df8b3b0e4f545b48ab44adae089dead8","Many problems can be solved by iteration by multiple participants (processors, servers, routers etc.). Previous mathematical models for such asynchronous iterations assume a single function being iterated by a fixed set of participants. We will call such iterations static since the system's configuration does not change. However in several real-world examples, such as inter-domain routing, both the function being iterated and the set of participants change frequently while the system continues to function. In this paper we extend Üresin and Dubois's work on static iterations to develop a model for this class of dynamic or always on asynchronous iterations. We explore what it means for such an iteration to be implemented correctly, and then prove two different conditions on the set of iterated functions that guarantee the full asynchronous iteration satisfies this new definition of correctness. These results have been formalised in Agda and the resulting library is publicly available. © 2022 Elsevier Inc.","Agda; Asynchronous computation; Fixed points; Formal verification; Iteration"
"Architecting a congestion pre-avoidance and load-balanced wireless network-on-chip","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122324812&doi=10.1016%2fj.jpdc.2021.12.003&partnerID=40&md5=20f5e3fd134b55676f74a7d9dd92b9a1","The communication performance over conventional long-distant routers cannot satisfy the requirements of future multi-core systems. Wireless Network-on-Chip (WiNoC) architecture with CMOS compatible transceivers is utilized to obtain significant improvement in on-chip data transfer for multi-core systems. The wireless routers (WRs) in WiNoC architecture provide wireless shortcuts to alleviate the latency of multi-hop communications. Despite the additional bandwidth of wireless shortcuts, the WRs are prone to congestion due to the limited number of wireless channels and the shared use of these channels under unbalanced load. On the other hand, network performance will be severely degraded when the presence of head-of-line (HOL) blocking. In this paper, we establish a congestion pre-avoidance and load-balanced wireless network-on-chip architecture. To implement such architecture in our network, firstly, we propose an dynamic XY-YX routing algorithm detour WR to pre-avoid the additional traffic flow of the wireless router; Secondly, the virtual output queue scheme is adopted to handle HOL blocking, and on this basis we design a wireless router micro-architecture; Finally, a threshold-based load-balanced mechanism is designed, which uses the number of buffered flit as a guideline to avoid unbalance load. Through system-level simulations, the results demonstrate that the proposed WiNoC architecture can mitigate negative effect of congestion in WR. And with appropriate hardware overhead, network transmission latency and network throughput are significantly improved. © 2021 Elsevier Inc.","Congestion pre-avoidance; Load-balanced; Network-on-chip; Threshold-based; Wireless network-on-chip"
"Adaptive diagonal sparse matrix-vector multiplication on GPU","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111660674&doi=10.1016%2fj.jpdc.2021.07.007&partnerID=40&md5=a25589ea3b46004779f99fd2c2fb2e3b","For diagonal sparse matrices that have many long zero sections or scatter points or diagonal deviations from the main diagonal, a great number of zeros need be filled to maintain the diagonal structure while using DIA to store them, which leads to the performance degradation of the existing DIA kernels because the padded zeros consume extra computation and memory resources. This motivates us to present an adaptive sparse matrix-vector multiplication (SpMV) for diagonal sparse matrices on the graphics processing unit (GPU), called DIA-Adaptive, to alleviate the drawback of DIA kernels for these cases. For DIA-Adaptive, there are the following characteristics: (1) two new sparse storage formats, BRCSD (Diagonal Compressed Storage based on Row-Blocks)-I and BRCSD-II, are proposed to adapt it to various types of diagonal sparse matrices besides adopting DIA, and SpMV kernels corresponding to these storage formats are presented; and (2) a search engine is designed to choose the most appropriate storage format from DIA, BRCSD-I, and BRCSD-II for any given diagonal sparse matrix; and (3) a code generator is presented to automatically generate SpMV kernels. Using DIA-Adaptive, the ideal storage format and kernel are automatically chosen for any given diagonal sparse matrix, and thus high performance is achieved. Experimental results show that our proposed DIA-Adaptive is effective, and has high performance and good parallelism, and outperforms the state-of-the-art SpMV algorithms for all test cases. © 2021 Elsevier Inc.","CUDA; Diagonal sparse matrices; GPU; Sparse matrix-vector multiplication; Sparse storage format"
"EASYPAP: A framework for learning parallel programming","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113353285&doi=10.1016%2fj.jpdc.2021.07.018&partnerID=40&md5=e77d192623a4b0259a87bb6fea1af582","This paper presents EASYPAP, an easy-to-use programming environment designed to help students to learn parallel programming. EASYPAP features a wide range of 2D computation kernels that the students are invited to parallelize using Pthreads, OpenMP, OpenCL or MPI. Execution of kernels can be interactively visualized, and powerful monitoring tools allow students to observe both the scheduling of computations and the assignment of 2D tiles to threads/processes. By focusing on algorithms and data distribution, students can experiment with diverse code variants and tune multiple parameters, resulting in richer problem exploration and faster progress towards efficient solutions. We present selected lab assignments which illustrate how EASYPAP improves the way students explore parallel programming. © 2021 Elsevier Inc.","Education; Monitoring; OpenMP; Parallel programming; Visualization"
"DriverRep: Driver identification through driving behavior embeddings","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123840622&doi=10.1016%2fj.jpdc.2022.01.010&partnerID=40&md5=bf6e733a70ca8f525648805aaedc05a0","Driver identification has emerged as an active field of study to further personalize the integrated advanced driver-assistance systems into intelligent vehicles, provide security and safety for ride-hailing services, and prevent auto theft. Several studies, within recent years, have investigated non-intrusive identification approaches, focusing on driving behavior analysis to characterize drivers' driving behavior, in which a considerable volume of labeled data is required. This paper develops a deep learning architecture, namely DriverRep, to extract the latent representations associated with each individual, called driver embeddings. These embeddings represent the unique driving characteristics of drivers. To this aim, we introduce a fully unsupervised triplet loss that selects triplet samples from data in an unsupervised manner and extracts the embeddings using our proposed stacked encoder architecture. Dilated causal convolutions are used to make residual blocks of the encoder. To perform the task of driver identification, we leverage the classification accuracy of SVM on top of the obtained driver embeddings. The evaluation results over two datasets, each of which contains ten drivers, reveal that the DriverRep can successfully capture the underlying features within the data and outperform benchmark driver identification schemes, obtaining an average accuracy of 94.7% and 96% for two-way and three-way identification, respectively. We also investigate the ability of the DriverRep in handling sparsely labeled data. The results represent substantial improvements in comparison with the supervised approaches when applied to highly sparsely labeled data. © 2022 Elsevier Inc.","Deep learning; Driver embeddings; Driver recognition; Driving behavior analysis"
"Machine learning and the Internet of Things security: Solutions and open challenges","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123788236&doi=10.1016%2fj.jpdc.2022.01.015&partnerID=40&md5=7f6a55e142a96cbd06d68016e208a653","Internet of Things (IoT) is a pervasively-used technology for the last few years. IoT technologies are also responsible for intensifying various everyday smart applications improving the standard of living. However, the inter-crossing of IoT systems and the multi-directional elements responsible for these systems' placement have raised new safety concerns. They generate and share a massive amount of sensitive data. Unfortunately, both the data and the devices are susceptible to many privacy and security challenges. Much research has been done to secure these infrastructures; however, Machine Learning (ML), among others, provides higher accuracy. This survey covers the major security issues and open challenges encountered by IoT infrastructures. It also encompasses an in-depth study and analysis of ML-based state-of-the-art solutions used in securing such domains. The security challenges and requirements in IoT-based systems have been highlighted, along with a discussion on how ML supports security measures in the said domain. Furthermore, the challenges associated with ML-based security solutions have been identified concerning IoT. An analysis of prevailing ML security techniques' constraints is also contemplated. © 2022 Elsevier Inc.","Internet of Things; Machine learning; Security"
"An efficient heuristic switch migration scheme for software-defined vehicular networks","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125598228&doi=10.1016%2fj.jpdc.2022.01.011&partnerID=40&md5=cac2cf719fec56bb1ce2f32be71bfc16","Software-Defined Vehicular Networks (SDVNs) have been a vital addition to the design of intelligent vehicular networks. SDVNs elevate the constraints of static hardware network devices by using programmable units, providing a global view of the network status, and standardizing the interface between different wireless access technologies. However, the static deployment and assignment of switches to control units do not consider vehicular network's rapid mobility changes and diverse densities. In this article, we propose a mobility-based heuristic switch migration scheme for software-defined vehicular networks. The proposed approach utilizes vehicles' mobility among switch-enabled roadside units to efficiently migrate selected switches between different controllers. The proposed scheme efficiently migrated switch-enabled roadside units between distributed control units while maintaining low vehicles' migration delay and cost. We have evaluated the proposed method under different environments and scenarios and reported its migration cost and delay performance. © 2022","Heuristic approach; Mobility estimation; SDVN; Switch-migration; Vehicular networks"
"Model-based selection of optimal MPI broadcast algorithms for multi-core clusters","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127105756&doi=10.1016%2fj.jpdc.2022.03.012&partnerID=40&md5=112c5fc7f51d3d66b209caa568b11bc1","The performance of collective communication operations determines the overall performance of MPI applications. Different algorithms have been developed and implemented for each MPI collective operation, but none proved superior in all situations. Therefore, MPI implementations have to solve the problem of selecting the optimal algorithm for the collective operation depending on the platform, the number of processes involved, the message size(s), etc. The current solution method is purely empirical. Recently, an alternative solution method using analytical performance models of collective algorithms has been proposed and proved both accurate and efficient for one-process-per-CPU configurations. The method derives the analytical performance models of algorithms from their code implementation rather than from high-level mathematical definitions, and estimates the parameters of the models separately for each algorithm. The method is network and topology oblivious and uses the Hockney model for point-to-point communications. In this paper, we extend that selection method to the case of clusters of multi-core processors, where each core of the platform runs a process of the MPI application. We present the proposed approach using Open MPI broadcast algorithms, and experimentally validate it on three different clusters of multi-core processors, Grisou, Gros and MareNostrum4. © 2022 The Author(s)","Collective communication algorithms; Communication performance modeling; Message passing; MPI; Multi-core clusters"
"FELIDS: Federated learning-based intrusion detection system for agricultural Internet of Things","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127127139&doi=10.1016%2fj.jpdc.2022.03.003&partnerID=40&md5=dcb26cbc0c4d0b28e8368858e9f3a0d6","In this paper, we propose a federated learning-based intrusion detection system, named FELIDS, for securing agricultural-IoT infrastructures. Specifically, the FELIDS system protects data privacy through local learning, where devices benefit from the knowledge of their peers by sharing only updates from their model with an aggregation server that produces an improved detection model. In order to prevent Agricultural IoTs attacks, the FELIDS system employs three deep learning classifiers, namely, deep neural networks, convolutional neural networks, and recurrent neural networks. We study the performance of the proposed IDS on three different sources, including, CSE-CIC-IDS2018, MQTTset, and InSDN. The results demonstrate that the FELIDS system outperforms the classic/centralized versions of machine learning (non-federated learning) in protecting the privacy of IoT devices data and achieves the highest accuracy in detecting attacks. © 2022 Elsevier Inc.","Deep learning; Federated learning; Internet of Things; Privacy; Security"
"Implementing three exchange read operations for distributed atomic storage","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124270427&doi=10.1016%2fj.jpdc.2022.01.024&partnerID=40&md5=7d3689c88cca49d9f30b2697b346a01d","Communication latency typically dominates the performance of message-passing systems, and consequently defines the efficiency of operations of algorithms implementing atomic read/write objects in asynchronous, crash-prone, message-passing systems. Here latency is measured in terms of the number of communication exchanges (or simply exchanges) involved in each operation. We present four algorithms, two for the single-writer/multiple-reader (SWMR) and two for the multi-writer/multiple-reader (MWMR) settings, that allow reads to take two or three exchanges, advancing the state-of-the-art in this area. Writes take the same number of exchanges as in prior works (i.e., two for SWMR and four for MWMR settings). In contrast with existing efficient implementations, ours come with no constraints on reader participation in both settings, and on the number of writers in the MWMR setting. Correctness of algorithms is rigorously argued. We conclude with an empirical study demonstrating the practicality of the algorithms, and identifying settings in which their read performance, is clearly superior compared to relevant algorithms. © 2022 Elsevier Inc.","Atomicity; Fault-tolerance; Read/write objects; Shared memory"
"Performance analysis and optimization for SpMV based on aligned storage formats on an ARM processor","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113312600&doi=10.1016%2fj.jpdc.2021.08.002&partnerID=40&md5=080a6de63539f327842d6144dfb138d8","Sparse matrix-vector multiplication (SpMV) has always been a hot topic of research for scientific computing and big data processing, but the sparsity and discontinuity of the nonzero elements in a sparse matrix lead to the memory bottleneck of SpMV. In this paper, we propose aligned CSR (ACSR) and aligned ELL (AELL) formats and a parallel SpMV algorithm to utilize NEON SIMD registers on ARM processors. We analyze the impact of SIMD instruction latency, cache access, and cache misses on SpMV with different formats. In the experiments, our SpMV algorithm based on ACSR achieves 1.18x and 1.56x speedup over SpMV based on CSR and SpMV in PETSc, respectively, and AELL achieves 1.21x speedup over ELL. The deviations between the theoretical results and experimental results in the instruction latency and cache access are 10.26% and 10.51% in ACSR and 5.68% and 2.91% in AELL, respectively. © 2021 Elsevier Inc.","ARM; NEON; SIMD; SpMV; Storage formats"
"Efficient microscopy image analysis on CPU-GPU systems with cost-aware irregular data partitioning","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125494747&doi=10.1016%2fj.jpdc.2022.02.004&partnerID=40&md5=303bcfdd459c72d0722a742cd1f814ee","The analysis of high resolution whole slide tissue images is a computationally expensive task, which adversely impacts effective use of pathology imaging data in research. We propose runtime solutions to enable efficient execution of pathology image analysis applications on modern distributed memory hybrid platforms equipped with both CPUs and GPUs. Hybrid systems offer significant computation capacity, but taking advantage of this computing power is complex. An application developer may have to implement multiple versions of data processing codes targeted for different computing devices. The developer also has to tackle the challenges of efficiently distributing computational load among the nodes of a distributed memory machine and among computing devices on a node. This is particularly difficult in analysis of high resolution images because of irregular computing costs of processing different image regions. In order to address these problems, we have leveraged a high-level image processing language (Halide) and integrated it into our runtime system called Region Templates (RT). The language simplifies the application development while generating code for multiple devices, such as CPU and GPU. The integration with RT allows for efficient multiple node hybrid execution. We also developed a novel cost-aware data partitioning (CADP) strategy that considers the workload irregularity to minimize load imbalance. Our experimental evaluation shows significant performance improvements on hybrid CPU-GPU machines, as compared with using a single processor (CPU or GPU), as well as on multi-GPU systems. CADP resulted in 1.7× better performance than other workload partitioning approaches (e.g., KD-Trees) on a hybrid machine and was up to 2.24× faster in multi-node settings. © 2022 Elsevier Inc.","Distributed computing; Halide; Hybrid computing; Microscopy imaging; Region templates framework"
"Adaptive scheduling of multiprogrammed dynamic-multithreading applications","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123794643&doi=10.1016%2fj.jpdc.2022.01.009&partnerID=40&md5=2942e0dbb50c9b6eba7c6f024e7c6b00","Modern parallel platforms, such as clouds or servers, are often shared among many different jobs. However, existing parallel programming runtime systems are designed and optimized for running a single parallel job, so it is generally hard to directly use them to schedule multiple parallel jobs without incurring high overhead and inefficiency. In this work, we develop AMCilk (Adaptive Multiprogrammed Cilk), a novel runtime system framework, designed to support multiprogrammed parallel workloads. AMCilk has client-server architecture where users can dynamically submit parallel jobs to the system. AMCilk has a single runtime system that runs these jobs while dynamically reallocating cores, last-level cache, and memory bandwidth among these jobs according to the scheduling policy. AMCilk exposes the interface to the system designer, which allows the designer to easily build different scheduling policies meeting the requirements of various application scenarios and performance metrics, while AMCilk transparently (to designers) enforces the scheduling policy. AMCilk also enables its use in cloud environment where other processes may be sharing the system with AMCilk. In this scenario, an external scheduler can change the resource availability for AMCilk and AMCilk seamlessly adapts to these changes. The primary feature of AMCilk is the low-overhead and responsive preemption mechanism that allows fast reallocation of cores between jobs. Our empirical evaluation indicates that AMCilk incurs small overheads and provides significant benefits on application-specific criteria for a set of 4 practical applications due to its fast and low-overhead core reallocation mechanism. © 2022 Elsevier Inc.","Cilk; Cloud; Multiprogrammed; Parallel computing"
"Identifying challenges and opportunities of in-memory computing on large HPC systems","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125870800&doi=10.1016%2fj.jpdc.2022.02.002&partnerID=40&md5=b0c697841b4c2c325bd6a2f753a19149","With the increasing fidelity and resolution enabled by high-performance computing systems, simulation-based scientific discovery is able to model and understand microscopic physical phenomena at a level that was not possible in the past. A grand challenge that the HPC community facing is how to maintain the large amounts of analysis data generated from simulations. In-memory computing, among others, is recognized to be a viable path forward and has experienced tremendous success in the past decade. Nevertheless, there has been a lack of a complete study and understanding of in-memory computing as a whole on HPC systems. Given the enlarging disparity between compute and HPC storage I/O, it is urgent for the HPC community to assess the state of in-memory computing and understand the challenges and opportunities. This paper presents a comprehensive study of in-memory computing with regard to its software evolution, performance, usability, robustness, and portability. In particular, we conduct an indepth analysis on the evolution of in-memory computing based upon more than 3,000 commits, and use realistic workflows for two scientific workloads, i.e., LAMMPS and Laplace to quantitatively assess state-of-the-art in-memory computing libraries, including DataSpaces, DIMES, Flexpath, Decaf and SENSEI on two leading supercomputers, Titan and Cori. Our studies not only illustrate the performance and scalability, but also reveal the key aspects that are of interest to library developers and users, including usability, robustness, portability, potential design defects, etc. © 2022 Elsevier Inc.","Data analytics; High-performance computing; In-memory computing; Workflow"
"Cooperative energy transactions in micro and utility grids integrating energy storage systems","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119599336&doi=10.1016%2fj.jpdc.2021.11.006&partnerID=40&md5=88c20724134de5f160cbf98adbcf9275","Over the past few years, Micro Grids (MGs) have gained much popularity due to two-way communication in the power network with special emphasis on Distributed Energy Resources (DERs), which comprise of both Renewable Energy Sources (RESs) and Nonrenewable Energy Sources (NRESs). Currently, the main focus of researchers is to deal with the intermittent nature of RESs, which lead to the fluctuations in power production and dispatch. In this paper, direct energy trading among MGs is considered as an assuring solution for improving the grid stability, reducing power line losses and minimizing energy trading cost. The focus of this work is on energy transactions amongst multiple MGs within the same geographic region. In the proposed method, coalitions among MGs are made on the basis of the distance between them for energy transaction. Furthermore, an Energy Transaction Algorithm (ETA) is proposed for energy trading among MGs in the coalitions. Energy Storage System (ESS) is also integrated, which stores energy when an MG has surplus energy and utilizes the stored energy if it becomes energy deficient. Simulations are performed and the results demonstrate that energy transactions among MGs in the proposed method minimize the power line losses and energy trading cost up to 34.6% and 14%, respectively as compared to the existing method. © 2021 Elsevier Inc.","Distributed energy management; Energy storage system; Energy trading; Microgrid; Power line losses"
"Developing parallel programming and soft skills: A project based learning approach","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.07.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113977782&doi=10.1016%2fj.jpdc.2021.07.015&partnerID=40&md5=564ad1cf61c7558845e3273e731e021d","Upon graduation, a computer science student should have a good understanding of the current technology and have the soft skills necessary to secure a position in industry. Considering that typical computers and even the common smartphone are multicore, students should be skilled in parallel programming. Integrating parallel programming and soft skills within courses can help educate students on these essential skills. Our goal is to explore the effectiveness of using Project Based Learning (PBL) to teach these skills when classes are at content capacity. We divide 247 students into 51 diverse groups and assigned five projects, each of two-week duration. We use pre- and post-surveys to measure growth and found that incorporating PBL has a significant effect on the students' parallel programming and soft skills. We show that through teamwork, students collaboratively learn and apply fundamental parallel programming and soft skills without direct guidance, thus demonstrating the effectiveness of PBL. The implementation was conducted in a course that does not traditionally teach parallel programming concepts, but with the use of a PBL approach, students were able to acquire this new knowledge. © 2021 Elsevier Inc.","CS education; Open MP; Parallel computing; Project based learning; Shared memory"
"In-depth FPGA accelerator performance evaluation with single node benchmarks from the HPC challenge benchmark suite for Intel and Xilinx FPGAs using OpenCL","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119270256&doi=10.1016%2fj.jpdc.2021.10.007&partnerID=40&md5=0a08705e5fa505139c8753419b544190","Emerging high-level tools lead to a reduced development time for applications on FPGA accelerators while still producing high-quality results. This is one reason for the increased adoption of FPGAs in data center applications which emphasizes the need for a benchmark suite to enable the comparison of FPGA architecture, programming tools, runtimes, and libraries. Because of the lack of such a benchmark suite, we have developed an OpenCL-based open-source implementation of the HPCC benchmark suite for Xilinx and Intel FPGAs. In an in-depth evaluation, we show that the benchmarks allow to quantify the impact of HBM2 memory in comparison to FPGAs with DDR and to analyze differences in the arithmetic units on current FPGA architectures. Power measurements indicate, that not all benchmark implementations can utilize the full potential of the FPGAs in terms of power efficiency. We are continuing to optimize and port the benchmark for new generations of FPGAs and design tools and we encourage active participation to create a valuable tool for the community. © 2021 Elsevier Inc.","FPGA; High level synthesis; HPC benchmarking; OpenCL"
"Adaptive parallel and distributed simulation of complex networks","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124326679&doi=10.1016%2fj.jpdc.2022.01.022&partnerID=40&md5=58404e6377ab60348be0b9c7f68dd66a","Complex networks are an important methodology to model several (if not all) aspects of the real world, in which multiple entities interact, in some way. While many aspects related to such interactions can be investigated by looking at the general mathematical metrics of the networks, an alternative approach lies in the simulation of some application protocol on top of (large scale) complex networks. In this paper, we present a study on this intricate problem. The complexity of the simulation is due to the need to model all the interactions among network nodes. We focus on discrete-event simulation, a simulation methodology that enables both sequential (i.e. monolithic) and Parallel And Distributed Simulation (i.e. PADS) approaches. We discuss the performance and scalability requirements that the simulator should have. We also introduce a case study based on the agent-based simulation of gossip dissemination on top of a complex network. To demonstrate the viability of this simulation technique, we focus on a tool we built to simulate complex networks. The tool exploits adaptive partitioning mechanisms, which are essential to reduce the communication overhead in the PADS. An experimental evaluation has been conducted using different network topologies and simulator setups. Results demonstrate the feasibility of the approach to simulate complex networks. © 2022 Elsevier Inc.","Complex networks; Parallel and distributed simulation; Simulation"
"Distributed and individualized computation offloading optimization in a fog computing environment","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116425112&doi=10.1016%2fj.jpdc.2021.09.003&partnerID=40&md5=c37469e3ed9d173569f95df4026debae","In a newly emerged fog computing environment, various user equipments (UE) enhance their computing power and extend their battery lifetime by computation offloading to mobile edge cloud (MEC) servers. Such an environment is distributed and competitive in nature. In this paper, we take a game theoretical approach to computation offloading optimization in a fog computing environment. Such an approach captures and characterizes the nature of a competitive environment. The main contributions of the paper can be summarized as follows. First, we formulate a non-cooperative game with both UEs and MECs as players. Each UE attempts to minimize the execution time of its tasks with an energy constraint. Each MEC attempts to minimize the product of its power consumption for computation and execution time for allocated tasks. Second, we develop a heuristic algorithm for a UE to determine its “heuristically” best response to the current situation, an algorithm for an MEC to determine its best response to the current situation, and an iterative algorithm to find the Nash equilibrium. Third, we prove that our iterative algorithm converges to a Nash equilibrium. We demonstrate numerical examples of our non-cooperative games with and without MECs' participation. We observe that our iterative algorithm always quickly converges to a Nash equilibrium. The uniqueness of our non-cooperative games is that the strategy set of a player can be discrete and the payoff function of a player can be obtained by a heuristic algorithm for combinatorial optimization. To the best of the author's knowledge, there has been no such investigation of non-cooperative games based on combinatorial optimization for computation offloading optimization in a fog computing environment. © 2021 Elsevier Inc.","Computation offloading; Fog computing; Heuristic algorithm; Mobile edge cloud server; Non-cooperative game"
"On the power of randomization in distributed algorithms in dynamic networks with adaptive adversaries","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117071623&doi=10.1016%2fj.jpdc.2021.09.004&partnerID=40&md5=fdc71300f0b1c450f1cda4cfc27d80e5","This paper investigates the power of randomization in general distributed algorithms in dynamic networks where the network's topology may evolve over time, as determined by some adaptive adversary. In such a context, randomization may help algorithms to better deal with i) “bad” inputs to the algorithm, and ii) evolving topologies generated by “bad” adaptive adversaries. We prove that randomness offers limited power to better deal with “bad” adaptive adversary. We define a simple notion of prophetic adversary for determining the evolving topologies. Such an adversary accurately predicts all randomness in the algorithm beforehand, and hence the randomness will be useless against “bad” prophetic adversaries. Given a randomized algorithm P whose time complexity satisfies some mild conditions, we prove that P can always be converted to a new algorithm Q with comparable time complexity, even when Q runs against prophetic adversaries. This implies that the benefit of P using randomness for dealing with the adaptive adversaries is limited. © 2021 Elsevier Inc.","Adversary; Derandomization; Distributed algorithms; Dynamic networks"
"Realizing dynamic resource orchestration on cloud systems in the cloud-to-edge continuum","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119208493&doi=10.1016%2fj.jpdc.2021.10.006&partnerID=40&md5=7c1d9763bba6979a1e678007bbddb0ba","Cloud computing has been widely utilized to handle the huge volume of data from many cutting-edge research areas such as Big Data and Internet of Things (IoT). The fast growing of edge devices makes it difficult for cloud systems to process all data and jobs originating from edge devices, which leads to the development of edge computing by completing jobs on edges instead of clouds. Unfortunately, edge devices generally possess only limited computing power. Therefore, jobs demanding heavy computation under strict time constraints could have more difficulties to successfully complete their work on edges than on clouds in the Cloud-to-Edge continuum. If cloud systems could dynamically orchestrate cloud resources to expedite the execution of those jobs, not only their timely execution could be assured, also the loading of edge devices could be reduced. The Apache Hadoop is considered one of the most popular cloud systems in industry and academia. However, it does not support dynamic resource allocation. Previously we proposed and implemented a new model which can dynamically adjust the computing resources assigned to given jobs in the Hadoop cloud system to speed up their execution. Like other computer software, cloud systems completely rely on their underlying operating systems to access hardware components such as CPUs and hard drives. In this paper, we report our efforts to improve our model to collaborate with the Linux operating system to accelerate the execution of jobs with high priority to a greater extent. Compared with what our original model achieved, experiments show that our ameliorated model could further quicken the execution of prioritized jobs in Hadoop by up to around 21%. As a result, jobs from edges that require substantial computing resources promptly could have better chances to get accomplished on cloud systems. © 2021 Elsevier Inc.","Cloud computing; Edge computing; Hadoop; HDFS"
"Separating lock-freedom from wait-freedom at every level of the consensus hierarchy","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124520483&doi=10.1016%2fj.jpdc.2022.01.025&partnerID=40&md5=701a2746eafec06830ab3979c73d6e94","A long-standing open question has been whether lock-freedom and wait-freedom are fundamentally different progress conditions, namely, can the former be provided in situations where the latter cannot? This paper answers the question in the affirmative, by proving that there are objects with lock-free implementations, but without wait-free implementations—using the same set of objects of any finite coordination power. We precisely define an object called n-process long-lived approximate agreement (n-LLAA), in which two sets of processes associated with two sides, 0 or 1, need to decide on a sequence of increasingly closer outputs. We prove that 2-LLAA has a lock-free linearizable implementation using read/write objects only, and that n-LLAA has a lock-free linearizable implementation using read/write objects and (n−1)-process consensus objects. In contrast, we prove that there is no wait-free linearizable implementation of n-LLAA using read/write objects and specific (n−1)-process consensus objects, called (n−1)-window registers. © 2022 Elsevier Inc.","Consensus hierarchy; Linearizability; Lock-freedom; Progress conditions; Wait-freedom"
"Systematic search space design for energy-efficient static scheduling of moldable tasks","2022","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2022.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123721010&doi=10.1016%2fj.jpdc.2022.01.004&partnerID=40&md5=9acc3caf16653b0fad27375ac23b164c","Static scheduling of independent, moldable tasks on parallel machines with frequency scaling comprises decisions on core allocation, assignment, frequency scaling and ordering, to meet a deadline and minimize energy consumption. Constraining some of these decisions reduces the solution space, i.e. may increase energy consumption, but may also open the path to new, near-optimal approaches. We investigate how constraints of different steps influence energy consumption, starting with an unrestricted scheduler for moldable tasks. The constraints are partly derived from existing schedulers, but also generalized in a systematic way. We present integer linear programs for all scheduling variants. We compare energy consumption of schedules for a benchmark suite of synthetic task sets of different sizes and for task sets derived from real applications. In addition, we check how close the results are to the optimum results when the ILP solver meets a timeout. Our results indicate that constraints on task execution order, which avoid explicit representation of task order in ILPs, are mostly responsible for near-optimal energy consumption among large task sets. Furthermore, we find that for all steps except allocation, non-optimal fast heuristics can be used without sacrificing too much energy for the resulting schedule. Finally, we can show that an ILP for a new scheduler, for which also a heuristic version exists, is comparable in quality to more complicated schedulers. © 2022","Energy efficiency; Moldable tasks; Static scheduling"
"Efficiency and scalability of multi-lane capsule networks (MLCN)","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105869662&doi=10.1016%2fj.jpdc.2021.04.010&partnerID=40&md5=4acc9a6db498e8aeb88fdb8780a4b3f7","Some Deep Neural Networks (DNN) have what we call lanes, or they can be reorganized as such. Lanes are paths in the network which are data-independent and typically learn different features or add resilience to the network. Given their data-independence, lanes are amenable for parallel processing. The Multi-lane CapsNet (MLCN) is a proposed reorganization of the Capsule Network which is shown to achieve better accuracy while bringing highly-parallel lanes. However, the efficiency and scalability of MLCN had not been systematically examined. In this work, we study the MLCN network with multiple GPUs finding that it is 2x more efficient than the original CapsNet when using model-parallelism. We introduce the load balancing problem of distributing heterogeneous lanes in homogeneous or heterogeneous accelerators and show that a simple greedy heuristic can be almost 50% faster than a naïve random approach. Further, we show that we can generate MLCN models with heterogeneous lanes with a balanced fit for a given set of devices. We describe a Neural Architectural Search generating MLCN models matching the device's memory that are load balanced. This search discovered models with 18.6% better accuracy for CIFAR-10. © 2021 Elsevier Inc.","Capsule network; Deep learning; Multi-lane"
"Improving multitask performance and energy consumption with partial-ISA multicores","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103642660&doi=10.1016%2fj.jpdc.2021.02.007&partnerID=40&md5=e8e7422c477076bb84957276bd343dfd","Modern GPPs implement specialized instructions in the form of ISA extensions aiming to increase the performance of emerging applications. These extensions impose a significant overhead in the area and power of the processor because of their specific datapaths (e.g. hardware for SIMD and FP instructions may represent more than half of the core area). Considering that some devices (e.g., edge computing), must be as energy- and area-efficient as possible, and the sporadic usage of specialized instructions in many applications, we propose PHISA multicores. PHISA is composed of heterogeneous cores of the same single base ISA, but asymmetric functionality: some of the cores do not fully implement the costly instruction extensions, making room for the designers to add more efficient cores. We show that PHISA increases performance in (32%) and reduces energy consumption in (82%) compared to full-ISA systems with the same power budget, in multi-workload environments. © 2021 Elsevier Inc.","Energy efficiency; Heterogeneity; Overlapping-ISA; Partial-ISA; Scheduling"
"Django: Bilateral coflow scheduling with predictive concurrent connections","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102040994&doi=10.1016%2fj.jpdc.2021.01.006&partnerID=40&md5=f094705b581d8b3f01f6c043e4e888b5","For data-parallel frameworks, their communication is highly structured. Coflow is a networking abstraction proposed for their all-or-nothing job-specific semantics. Minimizing coflow completion time (CCT) decreases the completion time of corresponding jobs. However, state-of-the-art coflow scheduling approaches suffer from several drawbacks. On the one hand, both sender-driven and receiver-driven scheduling approaches fail to achieve optimal especially when the bandwidth bottleneck exists. On the other hand, they fail to optimize the number of concurrent connections since the CCT can be prolonged due to too many or too few concurrent connections. In this paper, we propose Django, a bilateral coflow scheduling framework. We first use Support Vector Machine (SVM) as the machine learning model to automatically identify the optimal number of concurrent connections, i.e., the queue limitation in the switch. Based on the predicted results, we further develop a set of distributed coflow scheduling algorithms in a scalable manner. Testbed experiments and trace-driven simulations show that Django can estimate the number of concurrent connections with an accuracy of 98%, reduce the average CCT and 95th percentile CCT by 15% and 40%, respectively. © 2021 Elsevier Inc.","Coflow scheduling; Data center; Prediction"
"Explore unlabeled big data learning to online failure prediction in safety-aware cloud environment","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103782290&doi=10.1016%2fj.jpdc.2021.02.025&partnerID=40&md5=425853afbfea33445ee1f5cc254e3c96","Proactive fault management is an important problem in many areas of data management, including cloud computing, big data, vision, machine learning and especially for the cross-domain research of distributed computing and AI (Artificial Intelligence). Unfortunately, most real-world online failure prediction is facing the problem that the used data are difficult to label although the failure prediction should be a supervised learning problem. We observe that, in many cases, the large-scale unlabeled data can be classified through feature extraction and clustering for available prediction, and thus ideas from their combination can be brought to bear. Based on this, we have proposed an online failure prediction framework approach UDFP (Unlabeled Data based online Failure Prediction). It introduces the clustering analysis method based on the combination of the KNN (k-nearest neighbor) and the modularity idea to achieve prediction modeling. It is shown analytically that UDFP can mitigate a supervised learning problem for failure prediction in our situation to some extent. Experimental results demonstrate that UDFP, as a framework approach, has avoided the manual tagging workload and the huge difficulties, improved the predictive accuracy, and reduced cost of data management in safety-aware distributed cloud data centers while enhancing fault-tolerant capabilities and robustness. © 2021 Elsevier Inc.","Clustering analysis; k-nearest neighbor; Modularity; Online failure prediction; Unlabeled data"
"Study of interconnect errors, network congestion, and applications characteristics for throttle prediction on a large scale HPC system","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103622930&doi=10.1016%2fj.jpdc.2021.03.001&partnerID=40&md5=2fec51c728f947c9211a79f82756ecdd","Today's High Performance Computing (HPC) systems contain thousand of nodes which work together to provide performance in the order of petaflops. The performance of these systems depends on various components like processors, memory, and interconnect. Among all, interconnect plays a major role as it glues together all the hardware components in an HPC system. A slow interconnect can impact a scientific application running on multiple processes severely as they rely on fast network messages to communicate and synchronize frequently. Unfortunately, the HPC community lacks a study that explores different interconnect errors, congestion events and applications characteristics on a large-scale HPC system. In our previous work, we process and analyze interconnect data of the Titan supercomputer to develop a thorough understanding of interconnects faults, errors, and congestion events. In this work, we first show how congestion events can impact application performance. We then investigate application characteristics interaction with interconnect errors and network congestion to predict applications encountering congestion with more than 90% accuracy. © 2021 Elsevier Inc.","Cray; Errors; Gemini; Interconnect; Titan"
"RDIC: A blockchain-based remote data integrity checking scheme for IoT in 5G networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101805634&doi=10.1016%2fj.jpdc.2021.02.012&partnerID=40&md5=456ac1b7b0df80d69cbc1442d48321fc","Internet of things (IoT) is one of the main application scenarios of 5th generation mobile networks (5G). Along with the rapid development of 5G, IoT terminal devices will create big data. Generally, IoT terminal devices are lightweight user equipments, for example, wearable devices. In order to take use of these lightweight terminal devices, it is a feasible way to outsource these created big data to the public cloud. When these data are out of the client's control, it is of crucial importance to ensure the remote data integrity. To solve the weaknesses of the existing remote data integrity checking schemes, we propose the concept of blockchain-based remote data integrity checking (RDIC) scheme for big data. The new concept makes use of blockchain technique which greatly improves the efficiency and security of RDIC. First, the system model and security definition are given for the proposed RDIC scheme by using blockchain. Then, by using efficient modular arithmetic, RSA digital signature, blockchain, etc, we construct a lightweight blockchain-based RDIC scheme. Finally, we analyze its security and performance. The detailed analysis shows that our scheme is provably secure and lightweight. © 2021 Elsevier Inc.","5G; Blockchain; Data integrity checking; IoT; RSA"
"DDF Library: Enabling functional programming in a task-based model","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101854757&doi=10.1016%2fj.jpdc.2021.02.009&partnerID=40&md5=8d7669e267df11b34b15d60bc08c3fea","In recent years, the areas of High-Performance Computing (HPC) and massive data processing (also know as Big Data) have been in a convergence course, since they tend to be deployed on similar hardware. HPC systems have historically performed well in regular, matrix-based computations; on the other hand, Big Data problems have often excelled in fine-grained, data parallel workloads. While HPC programming is mostly task-based, like COMPSs, popular Big Data environments, like Spark, adopt the functional programming paradigm. A careful analysis shows that there are pros and cons to both approaches, and integrating them may yield interesting results. With that reasoning in mind, we have developed DDF, an API and library for COMPSs that allows developers to use Big Data techniques while using that HPC environment. DDF has a functional-based interface, similar to many Data Science tools, that allows us to use dynamic evaluation to adapt the task execution in run time. It brings some of the qualities of Big Data programming, making it easier for application domain experts to write Data Analysis jobs. In this article we discuss the API and evaluate the impact of the techniques used in its implementation that allow a more efficient COMPSs execution. In addition, we present a performance comparison with Spark in several application patterns. The results show that each technique significantly impacts the performance, allowing COMPSs to outperform Spark in many use cases. © 2021 Elsevier Inc.","Big Data; COMPSs; Data-flow programming; Performance evaluation"
"Predicting Networks-on-Chip traffic congestion with Spiking Neural Networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105692666&doi=10.1016%2fj.jpdc.2021.03.013&partnerID=40&md5=e523ad0227e597f8fd965c2e1ccf0736","Network congestion is one of the critical reasons for degradation of data throughput performance in Networks-on-Chip (NoCs), with delays caused by data-buffer queuing in routers. Local buffer or router congestion impacts on network performance as it gradually spreads to neighbouring routers and beyond. In this paper, we propose a novel approach to NoC traffic prediction using Spiking Neural Networks (SNNs) and focus on predicting local router congestion so as to minimize its impact on the overall NoCs throughput. The key novelty is utilizing SNNs to recognize temporal patterns from NoC router buffers and predicting traffic hotspots. We investigate two neural models, Leaky Integrate and Fire (LIF) and Spike Response Model (SRM) to check performance in terms of prediction coverage. Results on prediction accuracy and precision are reported using a synthetic and real-time multimedia applications with simulation results of the LIF based predictor providing an average accuracy of 88.28%–96.25% and precision of 82.09%–96.73% as compared to 85.25%–95.69% accuracy and 73% and 98.48% precision performance of SRM based model when looking at congestion formations 30 clock cycles in advance of the actual hotspot occurrence. © 2021 Elsevier Inc.","Congestion prediction; Network traffic; Networks-on-Chip; Spiking Neural Networks"
"Practical parallelization of scientific applications with OpenMP, OpenACC and MPI","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108415442&doi=10.1016%2fj.jpdc.2021.05.017&partnerID=40&md5=32eba5524c5d5ad98539be13c9a33f10","This work aims at distilling a systematic methodology to modernize existing sequential scientific codes with a little re-designing effort, turning an old codebase into modern code, i.e., parallel and robust code. We propose a semi-automatic methodology to parallelize scientific applications designed with a purely sequential programming mindset, possibly using global variables, aliasing, random number generators, and stateful functions. We demonstrate that the same methodology works for the parallelization in the shared memory model (via OpenMP), message passing model (via MPI), and General Purpose Computing on GPU model (via OpenACC). The method is demonstrated parallelizing four real-world sequential codes in the domain of physics and material science. The methodology itself has been distilled in collaboration with MSc students of the Parallel Computing course at the University of Torino, that applied it for the first time to the project works that they presented for the final exam of the course. Every year the course hosts some special lectures from industry representatives, who present how they use parallel computing and offer codes to be parallelized. © 2021 Elsevier Inc.","CUDA; Loop parallelism; MPI; OpenACC; OpenMP"
"Rival-Model Penalized Self-Organizing Map enforced DDoS attack prevention mechanism for software defined network-based cloud computing environment","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105300107&doi=10.1016%2fj.jpdc.2021.03.005&partnerID=40&md5=e5ec8c29e52861afe54eb398b9086bbe","Reliability, and safety are considered as the indispensable twins during the adoption of the cloud computing environment since their breaches lead to catastrophic issues in poor resource management and unreliable service quality. To be specific, Distributed Denial of Service (DDoS) attack is determined as the most vulnerable threat in the cloud space as it lowers the ability of the predominant resources’ for preventing their optimal utilization. The advent of Software-Defined Networking (SDN) is estimated to wide open the feasibility in preventing DDoS attacks in the cloud space. In this paper, a Rival-Model Penalized Self-Organizing Map (RMP-SOM) enforced DDoS Attack Prevention Mechanism is proposed for the remarkable prevention of DDoS attack by utilizing the potential characteristics of SDN that focuses on the possibility of facilitating network global perspective, effective investigation of network traffic, and an enhanced process of rule updating. This proposed RMPSOM-SDNDM scheme utilizes the benefits of the constant rate in order to ensure priority to the closest neuron and its neighborhood rather than its farthest rival neuron for facilitating better detection accuracy. The simulation results of the proposed RMPSOM approach confirmed a phenomenal sensitivity, specificity and accuracy rate during the process of detecting DDoS attacks in the cloud space on par with the baseline DDoS mitigation schemes considered from the literature. © 2021 Elsevier Inc.","Cloud computing; DDoS attacks; Learning rate; Rival-Model Penalized Self-Organizing Map (RMPSOM); Software Defined Networking"
"PackStealLB: A scalable distributed load balancer based on work stealing and workload discretization","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099005409&doi=10.1016%2fj.jpdc.2020.12.005&partnerID=40&md5=5a68891464100043f47578e9c0227040","The scalability of high-performance, parallel iterative applications is directly affected by how well they use the available computing resources. These applications are subject to load imbalance due to the nature and dynamics of their computations. It is common that high performance systems employ periodic load balancing to tackle this issue. Dynamic load balancing algorithms redistribute the application's workload using heuristics to circumvent the NP-hard complexity of the problem However, scheduling heuristics must be fast to avoid hindering application performance when distributing the workload on large and distributed environments. In this work, we present a technique for low overhead, high quality scheduling decisions for parallel iterative applications. The technique relies on combined application workload information paired with distributed scheduling algorithms. An initial distributed step among scheduling agents group application tasks in packs of similar load to minimize messages among them. This information is used by our scheduling algorithm, PackStealLB, for its distributed-memory work stealing heuristic. Experimental results showed that PackStealLB is able to improve the performance of a molecular dynamics benchmark by up to 41%, outperforming other scheduling algorithms in most scenarios over almost one thousand cores. © 2020 Elsevier Inc.","Distributed load balancing; Work stealing; Workload discretization"
"Energy efficient IoT-Fog based architectural paradigm for prevention of Dengue fever infection","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098878365&doi=10.1016%2fj.jpdc.2020.12.002&partnerID=40&md5=5969e67a6644532f478b2c6e15b5ffe3","Dengue is one of the most common and widespread infectious illnesses in humans transmitted by female Aedes albopictis. The prevalence of Dengue cases has increased substantially leading to human morbidity. Inadequate availability of healthcare professionals and inaccessibility to healthcare institutions have aggravated the problem. The traditional medical technologies are too antiquated to serve the purpose. The innovative latest technologies like Internet of Things (IoT), Cloud Computing, Fog Computing have made real-time and remote healthcare possible with huge success. In this paper, an IoT based Fog–Cloud enabled system for monitoring, assessment and control of Dengue Fever has been proposed. IoT sensors acquire data about a large spectrum of health as well as environmental factors that contribute to infection. The battery constrained sensors set their sampling rate according to the degree of cruciality that saves power to make battery long lasting. The Fog layer employs Support Vector Machine (SVM) for Dengue infection evaluation with least latency and sends alerts including precautionary measures to the users, hospital officials and government agencies. Moreover, the proposed system utilizes Temporal Network Analysis (TNA) and Google map service to categorize areas as infected, uninfected or risk prone. The experimental results are evaluated by a number of analytical parameters to investigate the effect of proposed system. SVM performs the best in terms of accuracy, recall, specificity, precision and f-measure with values 93%, 95%, 89%, 94% and 95% respectively. Furthermore, TNA based outbreak assessment gives valuable inputs for the government institutions to control the outbreak. © 2020 Elsevier Inc.","Cloud Computing; Dengue fever; Fog Computing; Internet of Things; Temporal Network Analysis"
"Adaptive resource planning for cloud-based services using machine learning","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102499687&doi=10.1016%2fj.jpdc.2021.02.018&partnerID=40&md5=d88402517b95b852fb12c11a4ad830fc","The problem of using cloud computing resources for services is related to planning the amount of resources needed and their subsequent reservation. This problem occurs both on the side of the customer who tries to minimize the cost of the service and on the side of the cloud provider who wants to make the best use of existing infrastructure without introducing any modifications. In our article, we want to show how the problem of overestimating the utilization of resources for services which use cloud computing can be handled. Solving this problem will allow significant savings to be made by both the customer and the cloud infrastructure provider. The system we have developed demonstrates the considerable utility of machine learning methods when planning cloud resource reservation for network services. The models proposed, which use a multilayer perceptron, have yielded good results for both short- and long-term reservations. © 2021 Elsevier Inc.","Adaptive planning; Cloud computing; Cloud-based service; Machine learning; Predicting resource consumption"
"SparkDQ: Efficient generic big data quality management on distributed data-parallel computation","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107980002&doi=10.1016%2fj.jpdc.2021.05.012&partnerID=40&md5=d06b165b2fd40176aa2ab0d3e3c1396a","In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability. © 2021 Elsevier Inc.","Big data; Data quality management system; Distributed system; Multi-tasks scheduling; Parallel data quality algorithms"
"Parallel solving of multiple information-coordinated global optimization problems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105277135&doi=10.1016%2fj.jpdc.2021.04.009&partnerID=40&md5=975adf267e340c5b5e4b7f5e7c48c4d7","This paper proposes an efficient approach for the parallel solution of computationally time consuming problems of multiple global optimization, in which minimized functions can be multiextremal and calculating function values may require huge amounts of computations. The proposed approach is based on the information-statistical theory of global optimization, within which a general computational scheme of global optimization methods is proposed. In the paper, this general scheme is expanded by the possible reuse of search information obtained in the process of computations when solving multiple global optimization problems. Within the framework of the proposed generalized scheme, parallel algorithms are proposed for computational systems with shared and distributed memory. Results of computational experiments demonstrated that the proposed approach can significantly reduce the computational complexity of solving multiple global optimization problems. © 2021 Elsevier Inc.","Computational complexity; Dimensionality reduction; Global optimization; Parallel methods of global search; Search information"
"Efficient traversal of decision tree ensembles with FPGAs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105559601&doi=10.1016%2fj.jpdc.2021.04.008&partnerID=40&md5=192ad20a85853308abc8b636a44c5cbd","System-on-Chip (SoC) based Field Programmable Gate Arrays (FPGAs) provide a hardware acceleration technology that can be rapidly deployed and tuned, thus providing a flexible solution adaptable to specific design requirements and to changing demands. In this paper, we present three SoC architecture designs for speeding-up inference tasks based on machine learned ensembles of decision trees. We focus on QUICKSCORER, the state-of-the-art algorithm for the efficient traversal of tree ensembles and present the issues and the advantages related to its deployment on two SoC devices with different capacities. The results of the experiments conducted using publicly available datasets show that the solution proposed is very efficient and scalable. More importantly, it provides almost constant inference times, independently of the number of trees in the model and the number of instances to score. This allows the SoC solution deployed to be fine tuned on the basis of the accuracy and latency constraints of the application scenario considered. © 2021 Elsevier Inc.","Decision trees; FPGA; Machine learning; System on Chip"
"Proactive auto-scaling for cloud environments using temporal convolutional neural networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105090032&doi=10.1016%2fj.jpdc.2021.04.006&partnerID=40&md5=442ff6c104365dfece22b41ede6512fc","Auto-scaling systems can dynamically scale the required resources for cloud-based services at runtime. This is an effective mechanism, enabling services to adapt to environmental changes. These systems establish the foundation for achieving elasticity in the modern cloud computing paradigm. Given the dynamic and uncertain nature of the shared cloud infrastructure, cloud auto-scaling systems are one of the most complex and sophisticated created artifacts, aiming to achieve self-aware, self-adaptive, and dependable runtime scaling. To find an effective solution to this problem, an accurate prediction of the required amount of workload as well as the system metrics for future time periods are needed. Various solutions have already been proposed to tackle this problem. Many solutions make use of machine learning, statistical, and ensemble methods. In this paper, we view the auto-scaling problem as a sequence model and apply the convolutional neural networks to predict the future workload of cloud services. Also, by using neural networks, we obtain a mapping between the predicted workload as well as the real-time and future amounts of the required resources. We have also proposed a decision-making mechanism that takes into account different and sometimes conflicting user criteria resulting in the best-compromised decision. To this aim, we have used TOPSIS as a multi-criteria decision-making method for the decision-making component. In the evaluation section, we have examined the amount of prediction error, the amount of service level agreement violations, as well as the amount of resources' under-utilization. Evaluations demonstrate that the proposed approach for predicting the workload shows a 4 percent improvement over the existing approaches. © 2021 Elsevier Inc.","Auto-scaling; Cloud computing; Dynamic scalability; Multi-criteria decision making; Resources provisioning"
"A robust approach for barrier-reinforcing in wireless sensor networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098713166&doi=10.1016%2fj.jpdc.2020.12.007&partnerID=40&md5=116a136042c89136ee921fea58b31877","Wireless sensor network barrier coverage plays an important role in intrusion detection. How to construct a robust barrier is a key research issue. For the initial deployment of barriers, with the depletion of node energy, some node dies prematurely, resulting in the existence of more weak points in the barrier. A method of using the re-deployment of mobile nodes to strengthen the barrier is proposed. This method adopts the set-based max-flow algorithm to calculate the number of weak points that can be strengthened and deploys and schedules movable nodes according to the weak situation to strengthen the barrier. The enhanced barrier has better performance and solves the problem of strengthening the weak points of the barrier. Simulation results show that the proposed algorithm can effectively strengthen the barrier and extend the survival time of the barrier, and the complexity of the algorithm is also relatively low. © 2020 Elsevier Inc.","Barrier reinforcement; Node re-deployment; Set-based max-flow algorithm; Wireless sensor network"
"A learning experience toward the understanding of abstraction-level interactions in parallel applications","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108060365&doi=10.1016%2fj.jpdc.2021.05.008&partnerID=40&md5=fa1ac88df55e1cb38d4cdb9d0afe4079","In the curriculum of a Computer Engineering program, concepts like parallelism, concurrency, consistency, or atomicity are usually addressed in separate courses due to their thoroughness and extension. Isolating such concepts in courses helps students not only to focus on specific aspects, but also to experience the reality of working with modern computer systems, where those concepts are often detached in different abstraction levels. However, due to such an isolation, it exists a risk of inducing to the students an absence of interactions between these concepts, and, by extension, between the different abstraction levels of a system. This paper proposes a learning experience showcasing the interactions between abstraction levels addressed in laboratory sessions of different courses. The driving example is a parallel ray tracer. In the different courses, students implement and assemble components of this application from the algorithmic level of the tracer to the assembly instructions required to guarantee atomicity. Each lab focuses on a single abstraction level, but shows students the interactions with the rest of the levels. Technical results and student learning outcomes through the analysis of surveys validate the proposed experience and confirm the students learning improvement with a more integrated view of the system. © 2021 The Author(s)","Assembly instructions; Futex; Ray tracing; Semaphore; Task queue"
"SCAB - IoTA: Secure communication and authentication for IoT applications using blockchain","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105262373&doi=10.1016%2fj.jpdc.2021.04.003&partnerID=40&md5=05a80f54aa90c92c89104be6cd9cf4d4","The Internet of Things (IoT) becomes a leading field of computer science, which simplifies the life of human beings. The IoT, devices can interconnect and communicate with each other, in a fully autonomous mode. This autonomy has made the IoT network vulnerable to various security threats. However, due to this autonomy, there is a strong need for reliable security and storage mechanism for authentication to exchange data between IoT devices. To solve this problem, we propose an efficient method known as SCAB-IoTA, which ensures identification and authentication of IoT devices and also provides secure communication in the open environment. Along with authentication and secure communication, the scheme also ensures data integrity. SCAB-IoTA used the blockchain and hybrid cryptosystem (the combination of Advanced Encryption Standard (AES) and Elliptic Curve Digital Signature Algorithm (ECDSA) cryptographic techniques) to enhance the security of IoT applications to prevent various attacks along with less computational, and storage overhead. Furthermore, we have a secure cluster of IoT devices based on angular distance (AD), so that devices can securely communicate without any interruption. A secure and robust trust mechanism has employed to build these clusters, where each IoT device has to authenticate itself before joining the cluster. The obtained results satisfy the IoT security requirements with 60% reduced computation, storage and communication cost compared with state-of-the-art methods. The security analysis illustrates that SCAB-IoTA withstands against various cyberattacks such as impersonation, message replay, man-in-the-middle, and botnet attacks. © 2021 Elsevier Inc.","Angular motion; Hybrid cryptosystem; Secure IoT cluster; Smart contract"
"Parallel lossless HSI compression based on RLS filter","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098974727&doi=10.1016%2fj.jpdc.2020.12.004&partnerID=40&md5=d3e825dcda4d00068f2164448bc3cbdc","The recent advancement in the field of electronics has led to development of sensors that capture the image of an area or object in spectral-domain along with spatial information. Due to continuity of spectral domain in hyperspectral images, it is difficult to store, process, analyze or transmit the critical information contained in it. Prediction based compression technique is used to reduce this size by a certain level. It predicts the value of a pixel with some error from previous pixels using a filter and finally, encode that error using variable length encoder. The execution time taken by this technique is very high which can be reduced by high performance computing. In this paper, we designed a mechanism to use high-performance computing techniques in the execution of prediction based image compression algorithms. The average execution time of the RLS-filter based compression algorithm is reduced significantly (by a factor of 29 using 2 nodes with 28 cores each, on PARAM SHIVAY supercomputer) with the proposed technique. © 2020 Elsevier Inc.","Hyperspectral image compression; Multiprocessing; Multithreading; Prediction based compression; RLS filter"
"A scalable blockchain based trust management in VANET routing protocol","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102896997&doi=10.1016%2fj.jpdc.2021.02.024&partnerID=40&md5=2cfc56474c92ca975c52f00f92e006e5","Critical event information dissemination has been proliferating on VANET allowing road safety via connected vehicular communications. Despite the prospect of promising applications in vehicular networks, it faces unresolved challenges that hold the capability to slow down network performance upon deployment, especially in terms of security. Particularly, insider attacks such as Blackhole attacks that are carried out against VANET systems can disrupt the networks’ average performance and prevent communication between vehicles entirely. Many state-of-the-art solutions have been proposed to detect and eliminate such nodes based on reputation systems and broadcast routing. However, if the network consists of multiple malicious nodes, the message dissemination could fail due to broadcast message tampering attack or packet dropping. In this study, we explore to answer the question of “can we improve the insider attacks mitigation in VANET by enhancing the trust in the network system so that the possibility of successful attacks can be reduced?”. To answer this question, in this paper, we present the blockchain-based decentralized trust score framework for the participating nodes to detect and blacklist insider attackers in VANET proactively. We propose a two-level detection system, in which at the first level, neighboring nodes calculate the trust individually. In the second level, a consortium blockchain-based system with authorized Road Side Units (RSUs) as validators, aggregate trust scores for vehicular nodes. Then, based on trust scores reported by the neighboring nodes, the blacklist node tables are dynamically modified. The experimental analysis shows that the proposed system is efficient and scalable in terms of the network's practical size. Finally, we also present evidence that the proposed system improves the VANET performance by mitigating and blacklisting insider attack launching nodes. © 2021 Elsevier Inc.","Blockchain; Distributed system; Routing protocol; VANET"
"PARIS: Predicting application resilience using machine learning","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102972064&doi=10.1016%2fj.jpdc.2021.02.015&partnerID=40&md5=51175b549ac774f27f0385bda1fe045b","The traditional method to study application resilience to errors in HPC applications uses fault injection (FI), a time-consuming approach. While analytical models have been built to overcome the inefficiencies of FI, they lack accuracy. In this paper, we present PARIS, a machine-learning method to predict application resilience that avoids the time-consuming process of random FI and provides higher prediction accuracy than analytical models. PARIS captures the implicit relationship between application characteristics and application resilience, which is difficult to capture using most analytical models. We overcome many technical challenges for feature construction, extraction, and selection to use machine learning in our prediction approach. Our evaluation on 16 HPC benchmarks shows that PARIS achieves high prediction accuracy. PARIS is up to 450x faster than random FI (49x on average). Compared to the state-of-the-art analytical model, PARIS is at least 63% better in terms of accuracy and has comparable execution time on average. © 2021","Application resilience prediction; Fault injection; HPC fault tolerance; Silent data corruption; Transient faults"
"Resisting newborn attacks via shared Proof-of-Space","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099471217&doi=10.1016%2fj.jpdc.2020.12.011&partnerID=40&md5=34080592b922f121df39d1d60b20e11b","In the cryptocurrency literature, Proof-of-Space has been a potential alternative for permissionless distributed consensus protocols not only due to its recyclable nature but also the potential to support multiple chains simultaneously. Namely, the same storage resource can be contributed to the consensus of more than one chain. However, a direct shared proof of the same storage brings about newborn attacks on new chain launching since holders of a substantial amount of resources can easily devastate a new chain with minor underlying storage at almost no cost, deviating from the decentralized principle of cryptocurrencies. To fix this gap, we propose an innovative framework of single-chain Proof-of-Space and further present a novel multi-chain scheme which resists newborn attacks effectively by elaborately combining shared proof and chain-specific proof of storage. Our framework covers both classical Nakamoto consensus (with one leader per round) and hybrid consensus (with multiple leaders per round). Specific protocols for both cases are presented. A committee-based consensus is leveraged to realize the multiple leader case. We show that both consensus schemes have realized our desired functionality without compromising consistency or liveness. © 2021 Elsevier Inc.","Blockchain; Consensus; Cryptocurrency; Mechanism design; Proof-of-Space"
"Security-aware task scheduling with deadline constraints on heterogeneous hybrid clouds","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103630600&doi=10.1016%2fj.jpdc.2021.03.003&partnerID=40&md5=e89ffc45bbca12cacc6fab7961c537d3","Hybrid cloud is a cost-effective way to address the problem of insufficient resources for satisfying its users’ requirements in a private cloud by elastically scaling up or down its service capability by combining the private cloud and public clouds. However, it is a challenge to schedule tasks on hybrid resources concerning their performance and security requirements. To address the challenge, this paper aims at improving the number of finished tasks with deadline and security requirements and the resource usage cost in heterogeneous hybrid clouds, based on data protection technologies providing various security levels with different overheads for data transfers and task executions in public clouds. We first formulate the problem as a bi-objective binary nonlinear programming (BOBNP) model which is a NP-hard problem. Then, to solve the problem in polynomial time, we propose a Task Scheduling method concerning Security (TSS). To improve the cost, TSS iteratively assigns the task requiring maximum cost of public resources to the local cluster, and rents the public resource with the best cost-performance ratio first for outsourced tasks. To complete as many tasks as possible, TSS assigns tasks cannot be finished by public clouds to the local cloud at first, and employs the idea of Least Slack Time First (LSTF) with Earliest Deadline First (EDF) in each computing node. Extensive experimental results show the superior performance of TSS in satisfying task requirements, and in resource efficiency when task deadlines are not too tight, compared with four hybrid cloud scheduling methods proposed recently. © 2021 Elsevier Inc.","Cloud computing; Hybrid cloud; Security; Task scheduling"
"Distributed programming of a hyperspectral image registration algorithm for heterogeneous GPU clusters","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101739979&doi=10.1016%2fj.jpdc.2021.02.014&partnerID=40&md5=dec6e441bbab6ff4a4a09b2e436d287e","Hyperspectral image registration is a relevant task for real-time applications such as environmental disaster management or search and rescue scenarios. The HYFMGPU algorithm was proposed as a single-GPU high-performance solution, but the need for a distributed version has arisen due to the continuous evolution of sensors that generate images with finer spatial and spectral resolutions. In a previous work, we simplified the programming of the multi-device parts of an initial MPI+CUDA multi-GPU implementation of HYFMGPU by means of Hitmap, a library to ease the programming of parallel applications based on distributed arrays. The performance of that Hitmap version was assessed in a homogeneous GPU cluster. In this paper, we extend this implementation by means of new functionalities added to the latest version of Hitmap in order to support arbitrary load distributions for multi-node heterogeneous GPU clusters. Three different load balancing layouts are tested, which prove that selecting a proper layout affects the performance of the code and how this performance is correlated with the use of the GPUs available in the cluster. © 2021 Elsevier Inc.","Distributed arrays; Heterogeneous computing; Hyperspectral imaging; Image registration; Load balancing"
"An adaptive synthesis to handle imbalanced big data with deep siamese network for electricity theft detection in smart grids","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103689411&doi=10.1016%2fj.jpdc.2021.03.002&partnerID=40&md5=be1b9024c1a2b3c0f7db65fd7a6438a3","The bi-directional flow of energy and information in the smart grid makes it possible to record and analyze the electricity consumption profiles of consumers. Because of the increasing rate of inflation over the past few years, people started looking for means to use electricity illegally, termed as electricity theft. Many data analytics techniques are proposed in the literature for electricity theft detection (ETD). These techniques help in the detection of suspected illegal consumers. However, the existing approaches have a low ETD rate either due to improper handling of the imbalanced class problem in a dataset or the selection of inappropriate classifier. In this paper, a robust big data analytics technique is proposed to resolve the aforementioned concerns. Firstly, adaptive synthesis (ADASYN) is applied to handle the imbalanced class problem of data. Secondly convolutional neural network (CNN) and long-short term memory (LSTM) integrated deep siamese network (DSN) are proposed to discriminate the features of both honest and fraudulent consumers. Specifically, the task of feature extraction from weekly energy consumption profiles is handed over to the CNN module while the LSTM module performs the sequence learning. Finally, the DSN contemplates on the shared features provided by the CNN-LSTM and applies final judgment. The data analytics is performed on different train–test ratios of the real-time smart meters’ data. The simulation results validate the proposed model's effectiveness in terms of high area under the curve, F1-Score, precision and recall. © 2021 Elsevier Inc.","Adaptive synthesis; Big data analytics; Deep learning; Deep Siamese network; Electricity theft detection"
"Image super-resolution via enhanced multi-scale residual network","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102076367&doi=10.1016%2fj.jpdc.2021.02.016&partnerID=40&md5=f78dd653a54e7e076f70a2ff669fe580","Recently, a very deep convolutional neural network (CNN) has achieved impressive results in image super-resolution (SR). In particular, residual learning techniques are widely used. However, the previously proposed residual block can only extract one single-level semantic feature maps of one single receptive field. Therefore, it is necessary to stack the residual blocks to extract higher-level semantic feature maps, which will significantly deepen the network. While a very deep network is hard to train and limits the representation for reconstructing the hierarchical information. Based on the residual block, we propose an enhanced multi-scale residual network (EMRN) to take advantage of hierarchical image features via dense connected enhanced multi-scale residual blocks (EMRBs). Specifically, the newly proposed residual block (EMRB) is capable of constructing multi-level semantic feature maps by a two-branch inception. The two-branch inception in our proposed EMRB consists of 2 convolutional layers and 4 convolutional layers in each branch respectively, therefore we have different ranges of receptive fields within one single EMRB. Meanwhile, the local feature fusion (LFF) is used in every EMRB to adaptively fuse the local feature maps extracted by the two-branch inception. Furthermore, global feature fusion (GFF) in EMRN is then used to obtain abundant useful features from previous EMRBs and subsequent ones in a holistic manner. Experiments on benchmark datasets suggest that our EMRN performs favorably over the state-of-the-art methods in reconstructing further superior super-resolution (SR) images. © 2021 Elsevier Inc.","A two-branch inception; Enhanced multi-scale residual block (EMRB); Enhanced multi-scale residual network (EMRN); Image super-resolution"
"An adaptive multi-agent system for task reallocation in a MapReduce job","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104122459&doi=10.1016%2fj.jpdc.2021.03.008&partnerID=40&md5=d5c6f1fa0c4d4c123a5e320ce9f910e2","We study the problem of task reallocation for load-balancing of MapReduce jobs in applications that process large datasets. In this context, we propose a novel strategy based on cooperative agents used to optimize the task scheduling in a single MapReduce job. The novelty of our strategy lies in the ability of agents to identify opportunities within a current unbalanced allocation, which in turn triggers concurrent and one-to-many negotiations amongst agents to locally reallocate some of the tasks within a job. Our contribution is that tasks are reallocated according to the proximity of the resources and they are performed in accordance to the capabilities of the nodes in which agents are situated. To evaluate the adaptivity and responsiveness of our approach, we implement a prototype test-bed and conduct a vast panel of experiments in a heterogeneous environment and by exploring varying hardware configurations. This extensive experimentation reveals that our strategy significantly improves the overall runtime over the classical Hadoop data processing. © 2021 Elsevier Inc.","Artificial Intelligence; BigData; MapReduce; Multi-agent systems; Negotiation"
"Speed-area optimized VLSI architecture of multi-bit cellular automaton cell based random number generator on FPGA with testable logic support","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100388105&doi=10.1016%2fj.jpdc.2021.01.005&partnerID=40&md5=07da1447ead3b6607780524a4fd6fb8a","In this paper, we have addressed a speed-area efficient VLSI implementation of a cellular automaton (CA) based random number generator (RNG) on Field Programmable Gate Arrays (FPGAs), in which each CA cell was proposed to be a multi-bit word in the original algorithm. This is in contrast to typical CA algorithms comprising one bit per CA cell. The original algorithm is shown favorable for FPGA implementations on adopting a fabric conscious approach involving instantiation of physical FPGA primitives. We have supplemented the original architecture with scan path and alternating logic to facilitate fault localization without area and delay overhead. The overheads have been carefully nullified by increasing the utilization ratio of the configured primitives, and exploiting the fast hardwired fabric of the FPGA. Generation of the hardware description of the RNG through Verilog has been automated. Our proposed designs outperform equivalent behavioral implementations expressed at higher levels of abstraction, both in speed and area. © 2021 Elsevier Inc.","Bit-sliced design; Cellular automata; Fault localization; Field Programmable Gate Array; Primitive instantiation"
"Hadoop Perfect File: A fast and memory-efficient metadata access archive file to face small files problem in HDFS","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108089031&doi=10.1016%2fj.jpdc.2021.05.011&partnerID=40&md5=4967235e74b7d9bec22c19858a9096b7","HDFS faces several issues when it comes to handling a large number of small files. These issues are well addressed by archive systems, which combine small files into larger ones. They use index files to hold relevant information for retrieving a small file content from the big archive file. However, existing archive-based solutions require significant overheads when retrieving a file content since additional processing and I/Os are needed to acquire the retrieval information before accessing the actual file content, therefore, deteriorating the access efficiency. This paper presents a new archive file named Hadoop Perfect File (HPF). HPF minimizes access overheads by directly accessing metadata from the part of the index file containing the information. It consequently reduces the additional processing and I/Os needed and improves the access efficiency from archive files. Our index system uses two hash functions. Metadata records are distributed across index files using a dynamic hash function. We further build an order-preserving perfect hash function that memorizes the position of a small file's metadata record within the index file. © 2021 Elsevier Inc.","Distributed file system; Fast access; HDFS; Massive small files"
"The power of agents in a dispersed system – The Shapley-Shubik power index","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109983323&doi=10.1016%2fj.jpdc.2021.06.010&partnerID=40&md5=5780518535276e074b0ef41afaf7bee1","In this paper, dispersed knowledge – accumulated in several decision tables is considered. Dispersion of knowledge is not a part of the work of the system. We assume that the knowledge is already in the dispersed form when it provides to the system. An advanced process of detecting the relations between the decision tables and constructing coalitions is used. The purpose of this paper is to use the measure to determine the strength of the coalition. With this method, a simple method of combining vectors of decisions generated based on local decision tables was applied. The purpose of using the Shapley-Shubik index was to reduce the computational complexity compared to the approach proposed in the earlier papers. In this paper, the results of experiments are presented, and the two approaches are compared. Based on these results, some conclusions have been drawn. © 2021 Elsevier Inc.","Dispersed knowledge; Global decision; Group decisions and negotiations; Knowledge-based systems; Shapley-Shubik power index"
"Short- and long-term cost and performance optimization for mobile user equipments","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098938085&doi=10.1016%2fj.jpdc.2020.12.006&partnerID=40&md5=19f1302621a82aca598288d604547316","Task offloading strategy optimization in mobile edge computing (MEC) has always been a hot issue. However, the mobility of a user equipment (UE) seriously affects the UE's cost and performance. This paper proposes three mobility types depending on whether the mobility characteristic of a UE is known, and formulates an energy minimization problem and a latency minimization problem to optimize the cost and performance, respectively. We first develop greedy strategy based task offloading algorithms for UEs according to their mobility characteristics. However, accurately obtaining the mobility characteristics of the UEs over a long time in practice is a huge challenge, especially in a highly random environment like the MEC. To address the issue, we use a Lyapunov optimization method to develop the algorithms that do not require any prior knowledge of the mobility characteristics to minimize the long-term energy and latency of UEs. Experimental results show that the greedy strategy based algorithms can optimize the cost and performance of UEs by using their mobility characteristics, and perform better than the Lyapunov optimization based algorithms in a short-term. However, the Lyapunov optimization based algorithms perform better than the greedy strategy based algorithms over a long-term. © 2020 Elsevier Inc.","Greedy strategy; Lyapunov optimization; Mobile edge computing; Mobility characteristic; Task offloading strategy"
"Intelligent colocation of HPC workloads","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101822749&doi=10.1016%2fj.jpdc.2021.02.010&partnerID=40&md5=87b51d17626acb776bb667fd145828c9","Many HPC applications suffer from a bottleneck in the shared caches, instruction execution units, I/O or memory bandwidth, even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single application, so an attractive technique for increasing HPC system utilization is to colocate multiple applications on the same server. When applications share critical resources, however, contention on shared resources may lead to reduced application performance. In this paper, we show that server efficiency can be improved by first modeling the expected performance degradation of colocated applications based on measured hardware performance counters, and then exploiting the model to determine an optimized mix of colocated applications. This paper presents a new intelligent resource manager and makes the following contributions: (1) a new machine learning model to predict the performance degradation of colocated applications based on hardware counters and (2) an intelligent scheduling scheme deployed on an existing resource manager to enable application co-scheduling with minimum performance degradation. Our results show that our approach achieves performance improvements of 7% (avg) and 12% (max) compared to the standard policy commonly used by existing job managers. © 2021 Elsevier Inc.","Colocation; HPC systems; Machine learning; Performance Characterization; Performance counters; Resource manager"
"Visual analogy videos for understanding fundamental parallel scheduling policies","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104141735&doi=10.1016%2fj.jpdc.2021.03.014&partnerID=40&md5=2bab8d73ffc2e1430a270d5859673ae0","Parallel and distributed computing (PDC) education is increasingly gaining greater recognition as a core topic in undergraduate computing degrees. While the application of PDC concepts to software development involves the use of highly-technical tools and libraries typically reserved for advanced courses, PDC educators are seeking pedagogical approaches that can be used to introduce PDC concepts in earlier, introductory courses. This study presents such an approach, and aims to introduce undergraduate students to fundamental PDC concepts without the expectation that they can apply those concepts. The proposed approach is inspired by the success seen in the wider computing education literature, where analogies and visualization have helped students understand other abstract computing topics. The proposed learning resources come in the form of a series of short videos, carefully aligned to a learning activity that guides towards achieving the intended learning outcomes. In addition to being a simple activity to complete with students, evaluations illustrate its value even with minimal guidance from the instructor. The proposed approach is studied as both a synchronous in-class activity guided by the instructor, as well as an asynchronous online self-directed activity. These two studies produced different outcomes with respect to student learning, revealing an important implication for designers of instructional material to consider. © 2021 Elsevier Inc.","Analogies; Online learning; Parallel computing; Videos; Visualization"
"Scalable flow probe architecture for 100 Gbps+ rates on commodity hardware: Design considerations and approach","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105881362&doi=10.1016%2fj.jpdc.2021.04.015&partnerID=40&md5=d930f4a7ebffaa219f6abef1b1917100","The unrelenting growth of data intensive applications has been raising the bar for performance of high speed networks. To cater for this growth, the core segments of networks today are based on 100 Gbps links. Analyzing the huge volume of traffic over these networks is a challenging yet essential task from the perspectives of network administration, network security and law enforcement. In spite of the availability of adequate network, compute, and memory resources, designing traffic analysis solutions performing at line rate for high speed traffic brings up several challenges. Traffic analysis solutions built on commodity compute platforms bring distinct advantages in terms of cost, adaptability, up-gradation and scalability. Keeping this in view, this paper explores the feasibility of designing high speed traffic analysis solutions that can handle 100s of Gbps on commodity compute platforms. It begins this process by analyzing the design issues in line rate handling of traffic on commodity servers with 100 GbE NIC and by bringing out an optimized and scalable packet processing pipeline. Leveraging this processing pipeline, NAPA-FP, a NUMA aware flow probe architecture, has been designed. With an implementation of this architecture on a commodity server, we show that line rate processing of Internet-like traffic from a 100 GbE interface can be achieved with a single NUMA node, by using a suitably configured packet processing path. We also show that its performance scales linearly to multiple 100 Gbps with additional NUMA nodes and NICs. In particular, the implementation on a 4-socket commodity server with 3×100 GbE NICs is able to process 3×100 Gbps Internet-like traffic at line rate using 3 NUMA nodes. The optimizations with respect to resource allocation, sharing and processing pipeline are reported with corroborating experimental results. © 2021 Elsevier Inc.","Beyond 100 Gbps; Commodity servers; Linear scaling; Traffic analysis"
"Packing internally disjoint Steiner trees to compute the κ3-connectivity in augmented cubes","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105694663&doi=10.1016%2fj.jpdc.2021.04.004&partnerID=40&md5=36dbc0a2b3c9b116299980c4f4a79cac","Given a connected graph G and S⊆V(G) with |S|≥2, a tree T in G is called an S-Steiner tree (or S-tree for short) if S⊆V(T). Two S-trees T1 and T2 are internally disjoint if E(T1)∩E(T2)=∅ and V(T1)∩V(T2)=S. The packing number of internally disjoint S-trees, denoted as κG(S), is the maximum size of a set of internally disjoint S-trees in G. For an integer k≥2, the generalized k-connectivity (abbr. κk-connectivity) of a graph G is defined as κk(G)=min{κG(S)|S⊆V(G) and |S|=k}. The n-dimensional augmented cube, denoted as AQn, is an important variant of the hypercube that possesses several desired topology properties such as diverse embedding schemes in applications of parallel computing. In this paper, we focus on the study of constructing internally disjoint S-trees with |S|=3 in AQn. As a result, we completely determine the κ3-connectivity of AQn as follows: κ3(AQ4)=5 and κ3(AQn)=2n−2 for n=3 or n≥5. © 2021 Elsevier Inc.","Augmented cubes; Connectivity; Generalized connectivity; Interconnection networks; Internally disjoint Steiner trees"
"An optimal cluster-based routing algorithm for lifetime maximization of Internet of Things","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107090132&doi=10.1016%2fj.jpdc.2021.05.005&partnerID=40&md5=056b2399cf0a3b6264c65496a0b9902e","Edge computing for Internet of Things (IoT) is a promising framework that can help small devices such as low-powered sensor nodes to accomplish complex computational tasks. The limited power supply of sensor nodes is one of their major limitations. A successful approach to improve the network lifetime and the overall scalability of the IoT supported wireless sensor networks (WSNs) is clustering. However, in a clustered IoT supported WSNs, some of the Cluster Heads (CHs) bear more traffic load than the others and therefore die sooner leading to decrease the network lifetime. To overcome this problem and maximize the network lifetime, the load of the CHs must be balanced. This research work suggests a new clustering method to balance the traffic load imposed on the cluster heads in IoT supported WSNs. The proposed clustering method uses a 1.2-approximation algorithm. In addition, we introduce an energy-aware routing algorithm for transmitting data packets from the CHs to their destination. The proposed routing algorithm distributes the communication load of the data packets among more nodes near the destination by a proper segmentation of the area. The simulation results show that the proposed clustering and routing algorithms in addition to being practical for large-scale IoT supported WSNs, cause the network to have a better performance compared to other similar algorithms. © 2021 Elsevier Inc.","Approximation algorithm; Energy balance; Internet of Things; Routing protocol"
"A Petri net extension for systems of concurrent communicating agents with durable actions","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105564647&doi=10.1016%2fj.jpdc.2021.04.011&partnerID=40&md5=5c5177a580d8458373ebc7d27b65c491","This paper provides a true-concurrency approach for the specification and verification of systems of concurrent communicating agents with durable actions. We present high-level Petri nets with durable actions (DaHL) to cope with various details in such complex systems. We define a DaHL module as an open variant of time-dependent colored Petri nets. A DaHL system is a fused set of modules for systems consisting of concurrent agents which can interact with each other. We also introduce hybrid-based reachability graph that covers the entire state space of DaHL systems with a true-concurrency semantics. We show that such reachability graph allows us to check important properties such as deadlock-freeness, liveness, home space, and reversibility, and also to predict timing properties prior to real implementation. A case study is used to model and analyze a simple scenario where autonomous vehicles are able to transport containers freely in an enterprise environment. © 2021 Elsevier Inc.","Petri nets; Reachability; Systems of concurrent communicating agents; True-concurrency semantics; Verification"
"Avoiding data loss and corruption for file transfers with Fast Integrity Verification","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102036781&doi=10.1016%2fj.jpdc.2021.02.002&partnerID=40&md5=eb4f55faf12010dce63b578776733eab","End-to-end integrity verification is used to avoid silent data corruption in file transfers by comparing the checksum of files at source and destination end points. However, it increases transfer times significantly as checksum computation requires reading files back from the storage and running compute-intensive hash computation. In this paper, we propose Fast Integrity VERification (FIVER) algorithm which alleviates the overhead of end-to-end integrity verification by overlapping checksum computation with transfer operation and enabling I/O sharing between the two. The results obtained from various network and dataset settings show that FIVER is able to bring down the cost of end-to-end integrity verification from up to 120% by the state-of-the-art solutions to below 15%. Moreover, existing implementations of end-to-end integrity verification are vulnerable to permanent data loss in the case of an unexpected power outage due to completing the integrity verification process while data is still on memory. FIVER addresses this issue by enforcing dirty data on memory to be flushed to disk before finishing integrity verification such that power loss during any phase of a transfer would cause integrity verification to fail and transfer application to retransfer lost data. © 2021 Elsevier Inc.","Data corruption; Data loss; File transfers; Integrity verification"
"MIRAGE: A consolidation aware migration avoidance genetic job scheduling algorithm for virtualized data centers","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104923283&doi=10.1016%2fj.jpdc.2021.03.004&partnerID=40&md5=d85be2fe6f1c084d8ea82c9099c8653e","Modern virtualized data centers often rely on virtual machine (VM) migrations to consolidate workload on a single machine for energy saving. But VM migrations have many drawbacks, including performance degradation, service disruption etc. Hence, many approaches have been proposed to minimize the overhead when migrations occur. In contrast, this work aims to proactively avoid migrations from happening in the first place. We have proposed a novel consolidation aware scheduling algorithm to minimize the number of migrations for batch processing systems by taking advantage of the prior knowledge of consolidation strategy and job information. We show the problem can be formulated as an integer linear programming (ILP) problem, and an effective heuristic solution can be found by a genetic algorithm. Both real and synthetic workload traces were used to evaluate our methods. Experimental results showed that, after comparing with two popular job scheduling algorithms, our approach has reduced the number of migrations by more than 25%. © 2021 Elsevier Inc.","Consolidation; Data center; Genetic algorithm; Migration; Virtual machine"
"Spartan: Sparse Robust Addressable Networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099611165&doi=10.1016%2fj.jpdc.2020.12.013&partnerID=40&md5=59b4e87958ac7b3d69a28ef802ba91e4","A Peer-to-Peer (P2P) network is a dynamic collection of nodes that connect with each other via virtual overlay links built upon an underlying network (usually, the Internet). P2P networks are highly dynamic and can experience very heavy churn, i.e., a large number of nodes join/leave the network continuously. Thus, building and maintaining a stable overlay network is an important problem that has been studied extensively for two decades. In this paper, we present our P2P overlay network called Sparse Robust Addressable Network (Spartan). Spartan can be quickly and efficiently built in a fully distributed fashion within O(logn) rounds. Furthermore, the Spartan overlay structure can be maintained, again, in a fully distributed manner despite adversarially controlled churn (i.e., nodes joining and leaving) and significant variation in the number of nodes. Moreover, new nodes can join a committee within O(1) rounds and leaving nodes can leave without any notice. The number of nodes in the network lies in [n,fn] for any fixed f≥1. Up to ϵn nodes (for some small but fixed ϵ>0) can be adversarially added/deleted within any period of P rounds for some P∈O(loglogn). Despite such uncertainty in the network, Spartan maintains Θ(n∕logn) committees that are stable and addressable collections of Θ(logn) nodes each for polynomial(n) rounds with high probability. Spartan's committees are also capable of performing sustained computation and passing messages between each other. Thus, any protocol designed for static networks can be simulated on Spartan with minimal overhead. This makes Spartan an ideal platform for developing applications. We experimentally show that Spartan will remain robust as long as each committee, on average, contains 24 nodes for networks of size up to 10240. © 2020 Elsevier Inc.","Churn; Dynamic networks; Overlay networks; Peer-to-Peer networks"
"Efficient GPU-parallelization of batch plants design using metaheuristics with parameter tuning","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105697652&doi=10.1016%2fj.jpdc.2021.03.012&partnerID=40&md5=f54828cd80f518b86de2d5d0d03c0046","We address a practice-relevant optimization problem: optimizing multi-product batch plants, with a real-world use case study – optimal design of chemical-engineering systems. Our contribution is a novel approach to parallelizing this optimization problem on GPU (Graphics Processing Units) by combining two metaheuristics – Simulated Annealing (SA) and Ant Colony Optimization (ACO). We improve the implementation performance by tuning particular parameters of the ACO metaheuristic. Our tuning approach improves on the previous methods in two respects: (1) we do not have to rely on additional mechanisms like fuzzy logic or algorithms for online tuning; and (2) we use the high computation performance of GPU to speedup the tuning process. By parallelizing the tuning process on modern GPUs, we allow the user to experiment with large volumes of input data and find the optimal values of the ACO parameters in feasible time. Our experiments on NVIDIA GPU show the efficiency of our approach to parameter tuning for the ACO metaheuristic. © 2021","Ant Colony Optimization tuning; Batch plant design; Combinatorial optimization; Metaheuristics; Metaheuristics parameter tuning"
"Analysis of Work Stealing with latency","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104320204&doi=10.1016%2fj.jpdc.2021.03.010&partnerID=40&md5=14bc7407213dcd1054026c8eea308a9f","We study the impact of communication latency on the classical Work Stealing load balancing algorithm. Our paper extends the reference model in which we introduce a latency parameter. By using a theoretical analysis and simulation, we study the overall impact of this latency on the Makespan (maximum completion time). We derive a new expression of the expected running time of a bag of independent tasks scheduled by Work Stealing. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors to use for a given work/platform combination. All our results are validated through simulation on a wide range of parameters. © 2021 Elsevier Inc.","Latency; Makespan analysis; Work Stealing"
"Behavior analysis and blockchain based trust management in VANETs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101604235&doi=10.1016%2fj.jpdc.2021.02.011&partnerID=40&md5=00af957b92cbba5fb028021632fba024","The development of vehicular ad hoc networks (VANETs) is facing great challenges. Due to the open environment in VANETs, the false information sent by malicious vehicles not only affects the fairness of information interaction but also seriously threatens the driving safety of normal vehicles. Therefore, the study of trust evaluation and management in VANETs has become hot topics in recent years. In this paper, we propose a trust management model of VANETs based on blockchain. In this model, a hidden markov model (HMM) based vehicle trust evaluation method that improve the accuracy on the detection of malicious behavior is proposed. Besides, a trust management method based on the alliance chain is designed, which greatly improves the efficiency of trust updating and querying on the premise of security. Simulation results show that the model is promising and feasible, effective in the aspects of trust evaluation and trust management. © 2021 Elsevier Inc.","Behavior analysis; Blockchain; Trust management; VANETs"
"PPMCK: Privacy-preserving multi-party computing for K-means clustering","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105698861&doi=10.1016%2fj.jpdc.2021.03.009&partnerID=40&md5=7d4fe5bd1584fd2ac6f77be7ba7c0037","The powerful resource advantage of the cloud provides a suitable computing environment for data processing. By transferring local computing to the cloud, the efficiency of data processing can be improved. However, the open cloud environment has defects in data privacy-preserving. In order to strengthen the protection of data privacy and ensure the security of multi-party interaction, we propose a privacy-preserving multi-party computing scheme for K-means clustering (PPMCK). PPMCK can preserve data privacy in the cloud and in the local side for each party from multi-party computing. In addition, PPMCK uses homomorphic encryption to protect data privacy. To support the division operation and ciphertext value size comparison with which homomorphic encryption cannot handle, the corresponding measurements are adopted, which make homomorphic encryption work smoothly. The experimental results demonstrate that PPMCK is effective in both data processing and privacy-preserving. © 2021 Elsevier Inc.","Homomorphic encryption; K-means clustering; Privacy preserving; Secure multiparty computing"
"The cluster coffer: Teaching HPC on the road","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105580692&doi=10.1016%2fj.jpdc.2021.04.013&partnerID=40&md5=93bdd7017b8148cc601b4ea2b8998f4c","Teaching parallel programming and HPC is a difficult task. There is a large number of sophisticated hardware and software components, each complex on their own and often showing non-intuitive interaction when used in combination. We consider education in HPC among the more difficult topics in computer science due to the fact that larger distributed memory systems are ubiquitous yet inaccessible and intangible to students. In this work, we present the Cluster Coffer, a miniature cluster computer based on 16 ARM compute boards that we believe is suitable for reducing the entry barrier to HPC in teaching and public outreach. We discuss our design goals for providing a portable, inexpensive system that is easy to maintain and repair. We outline the implementation path we took in terms of hardware and software, in order to provide others with the information required to reproduce and extend our work. Finally, we present two use cases for which the Cluster Coffer has been used multiple times, and will continue to be used in the upcoming years. © 2021 Elsevier Inc.","High performance computing; Parallel programming; Portability; Public outreach; Teaching"
"Fast GPU 3D diffeomorphic image registration","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099188850&doi=10.1016%2fj.jpdc.2020.11.006&partnerID=40&md5=74ab510b7349c48c2f141039b70ea317","3D image registration is one of the most fundamental and computationally expensive operations in medical image analysis. Here, we present a mixed-precision, Gauss–Newton–Krylov solver for diffeomorphic registration of two images. Our work extends the publicly available CLAIRE library to GPU architectures. Despite the importance of image registration, only a few implementations of large deformation diffeomorphic registration packages support GPUs. Our contributions are new algorithms to significantly reduce the run time of the two main computational kernels in CLAIRE: calculation of derivatives and scattered-data interpolation. We deploy (i) highly-optimized, mixed-precision GPU-kernels for the evaluation of scattered-data interpolation, (ii) replace Fast-Fourier-Transform (FFT)-based first-order derivatives with optimized 8th-order finite differences, and (iii) compare with state-of-the-art CPU and GPU implementations. As a highlight, we demonstrate that we can register 2563 clinical images in less than 6 s on a single NVIDIA Tesla V100. This amounts to over 20× speed-up over the current version of CLAIRE and over 30× speed-up over existing GPU implementations. © 2020 Elsevier Inc.","Diffeomorphic image registration; Gauss–Newton–Krylov method; GPU computing; Mixed-precision solver; Parallel optimization"
"On the weakest information on failures to solve mutual exclusion and consensus in asynchronous crash-prone read/write systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104474800&doi=10.1016%2fj.jpdc.2021.03.015&partnerID=40&md5=dba3aa60e614968d6ba631258f6cdad6","Mutual exclusion and consensus are among the most important coordination problems encountered in asynchronous concurrent systems, whether processes communicate using read/write registers or message passing. Unfortunately, neither can be solved in crash-prone systems, as soon as even a single process may crash. Hence, an important question: which is the weakest information on failures that must be given to the processes so that these problems can be solved whatever the number of crashes. This approach to circumvent impossibility results is known under the name failure detectors. Considering mutual exclusion and consensus in a crash-prone asynchronous system where the processes communicate through read/write registers, this article answers the previous question by presenting two failure detectors. The first, called Quasi-Perfect (QP) allows mutual exclusion to be solved in the presence of any number of process crashes. The second, called Ω⁎, allows consensus to be solved in the general model where not all but an a priori unknown subset of processes participates in consensus. In addition to algorithms solving each of the previous problems with the help of the associated failure detector, the article shows that QP and Ω⁎ provides the weakest information on failures needed to solve mutex exclusion and participant-restricted consensus respectively. © 2021 Elsevier Inc.","Asynchronous read/write system; Consensus; Failure detector; Mutual exclusion; Process crash"
"PBBFMM3D: A parallel black-box algorithm for kernel matrix-vector multiplication","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105692755&doi=10.1016%2fj.jpdc.2021.04.005&partnerID=40&md5=c39c43067547a06c081cac34b3bcbb4a","Kernel matrix-vector product is ubiquitous in many science and engineering applications. However, a naive method requires O(N2) operations, which becomes prohibitive for large-scale problems. To reduce the computation cost, we introduce a parallel method that provably requires O(N) operations and delivers an approximate result within a prescribed tolerance. The distinct feature of our method is that it requires only the ability to evaluate the kernel function, offering a black-box interface to users. Our parallel approach targets multi-core shared-memory machines and is implemented using OpenMP. Numerical results demonstrate up to 19× speedup on 32 cores. We also present a real-world application in geo-statistics, where our parallel method was used to deliver fast principle component analysis of covariance matrices. © 2021 Elsevier Inc.","Covariance matrix; Fast multipole method; Kernel method; Matrix-vector multiplication; Shared-memory parallelism"
"BSF: A parallel computation model for scalability estimation of iterative numerical algorithms on cluster computing systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098600580&doi=10.1016%2fj.jpdc.2020.12.009&partnerID=40&md5=0ec58b9dd8d5ca261fff645cda51202d","This paper examines a novel parallel computation model called bulk synchronous farm (BSF) that focuses on estimating the scalability of compute-intensive iterative algorithms aimed at cluster computing systems. The main advantage of the proposed model is that it allows to estimate the scalability of a parallel algorithm before its implementation. Another important feature of the BSF model is the representation of problem data in the form of lists that greatly simplifies the logic of building applications. In the BSF model, a computer is a set of processor nodes connected by a network and organized according to the master/slave paradigm. A cost metric of the BSF model is presented. This cost metric requires the algorithm to be represented in the form of operations on lists. This allows us to derive an equation that predicts the scalability boundary of a parallel program: the maximum number of processor nodes after which the speedup begins to decrease. The paper includes examples of applying the BSF model to designing and analyzing parallel numerical algorithms. The large-scale computational experiments conducted on a cluster computing system confirm the adequacy of the analytical estimations obtained using the BSF model. © 2020 Elsevier Inc.","BSF model; Bulk synchronous farm; Cluster computing systems; Iterative numerical algorithms; Parallel computation model"
"SMS Observer: A dynamic mechanism to analyze the behavior of SMS-based malware","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108095705&doi=10.1016%2fj.jpdc.2021.05.004&partnerID=40&md5=4cbdc91ea12b35a08bfff0fda8a6cdae","Nowadays smartphones become an indispensable tool in many people's everyday life that makes themselves attractive targets for attackers. Among various malware targeting at smartphones, SMS-based malware is one of the most notorious ones. Though a number of Android dynamic analysis frameworks have been proposed to analyze SMS-based malware, most of these frameworks or some Android tools, such as Google Android Emulator, do not support an app or malware to send SMS messages to a real smartphone; hence, security researchers cannot use them directly to analyze the behavior of SMS-based malware. In our previous work, SMS Helper, we designed an application layer tool to allow an app or malware in an Android emulator to send and receive SMS messages to or from a real smartphone. Based on SMS Helper, this paper proposes an Android dynamic analysis framework, called SMS Observer, to assist security researchers to analyze SMS-based malware. SMS Observer integrates SMS Helper into it as a client agent, meanwhile, and it maintains the integrity of system logs. This paper also figures out a way to detect whether an app is executed in an emulator and describes how to use SMS Observer to prevent such evasion. Experimental results using real-world malware samples show SMS Observer is much more effective in detecting SMS-related behavior of SMS-based malware than existing frameworks, such as Google Android Emulator, Andrubis, CopperDroid, and DroidBox. SMS Observer can analyze sophisticated SMS-based malware samples and provide a comprehensive view of malicious behavior. © 2021 Elsevier Inc.","Emulation; Information security; Network security; System analysis and design; Unified messaging"
"Bounding schemes for the parallel machine scheduling problem with DeJong's learning effect","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108085583&doi=10.1016%2fj.jpdc.2021.05.003&partnerID=40&md5=dd0ef934d1d9c3efabbee472e96d24f3","In this paper, the parallel machine scheduling problem with DeJong's learning effect, is addressed. The objective function to be minimized is the makespan. This problem is proofed to be NP-Hard in the strong sense. This is the challenging theoretical side of the studied problem. Furthermore, several real-life situations in manufacturing and computer science are modeled using the current problem. Several algorithms intended to solve the studied problem within a reasonable computing time are proposed in literature. Among these algorithms the exact methods, which failed to solve the studied problem to optimality even for small size instances. In this paper several new heuristics and meta-heuristics are proposed. These heuristics are classified into three types. The first type is based on Longest Processing Time (LPT) rule. The innovation is the modification of the LPT rule in a way to cope efficiently with the learning effect concept, by randomizing the selection of the next scheduled job. The second type of heuristics, is taking advantage of an exact Branch and Bound algorithm, developed originally for the classical parallel machine scheduling problem. The contribution for this kind of heuristics is lying in the modification of the processing time values, according to a prefixed selected functions. The third type of methods is based in an adaptation of the Genetic Algorithm to the learning effect concept. This adaptation consists in enlarging the area of selection of the parameters values. To assess the performance and the efficiency of the proposed heuristics, a newly developed lower bound is proposed. This lower bound is based on a relaxation of the studied problem, which allows to obtain a minimum cost flow problem. Finally, an extensive experimental study is conducted over a benchmark test problems. The obtained results provide strong evidence that the proposed procedures outperform the earlier existing ones. © 2021 Elsevier Inc.","DeJong's learning effect; Heuristic; Lower bound; Parallel machine"
"Smart contract service migration mechanism based on container in edge computing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103237911&doi=10.1016%2fj.jpdc.2021.02.023&partnerID=40&md5=3729e99e50e8918fd3ea582ac69cec68","In edge computing, smart contracts usually run in the form of containers on edge nodes. However, the container has process nesting and strong association with image files, which leads to insufficient real-time migration. This article studies the container-based service migration problem in edge computing. In order to reduce the service delay of nodes, we propose a service migration mechanism based on mobility awareness. The service migration mechanism based on mobility awareness triggers service migration according to the service density of the current node, and selects the most suitable service set for migration in the current node with the optimization goal of minimizing service delay, and finally the corresponding destination node is selected for migration according to the migration cost and the moving direction of the device to which the service belongs. Experimental results show that our proposed service migration mechanism based on container can effectively reduce the waiting delay and migration delay in the service process, and optimize the service quality of edge nodes. © 2021 Elsevier Inc.","Blockchain; Container; Docker; Edge computing; Smart contract"
"Discriminating flash crowds from DDoS attacks using efficient thresholding algorithm","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102372393&doi=10.1016%2fj.jpdc.2021.02.019&partnerID=40&md5=125baffbbe45535cdca43a57f66bd409","Distributed Denial-of-Service attacks have been a challenge to cyberspace, as the attackers send a large number of attack packets similar to the normal traffic, to throttle legitimate flows. These attacks intentionally disrupt the services offered by the systems resulting in heavy cost. A flash crowd or flash event is an unexpected surge in the number of visitors to a particular website resulting in a sudden increase in server load. Flash crowds, which are legitimate flows, are difficult to be discriminated from Distributed Denial-of-Service attacks that are illicit flows. Effective and accurate detection of Distributed Denial of Service attacks still remains a challenge due to the difficulty in its detection and the false alerts generated in the case of flash crowds. There is a trade off between detection rate and false positive rate. This work deals with an efficient and early detection of distributed denial of service attacks and discriminates flash crowd by considering two network traffic parameters such as packet size and destination IP address. Using these traffic features two attributes are computed and its generalized entropies are calculated. The threshold is computed using the mean value of network attributes to detect the attacks. Threshold updater can automatically adjust the threshold values according to the changes in the channel conditions. The data sets used to evaluate the performance of the proposed approach are the MIT Lincoln Laboratory DARPA data set and a data set generated in a University network. Experimental results show this research approach achieves higher detection rate and lower false positives in a much reduced processing time as compared to the existing methods. © 2021 Elsevier Inc.","DDoS attack; Network security; Tsallis entropy"
"Mitigating the processor aging through dynamic concurrency throttling","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108065878&doi=10.1016%2fj.jpdc.2021.05.006&partnerID=40&md5=5acb6e3b9ecfd3bfb708276cd3a4c453","The increase in the number of cores in a single chip brings better capabilities to exploit thread-level parallelism (TLP). However, since power dissipated per area rises at each new node generation, higher temperatures are achieved, speeding up the aging of hardware components, which may provoke undesired system behavior. Considering that many applications do not scale with the number of cores, their execution with the maximum TLP available will not only degrade performance, but also unnecessarily increase temperature, further accelerating aging. Given that, we propose Hebe, a dynamic concurrency throttling approach that learns at run-time the degree of TLP that reduces the aging for OpenMP applications. Hebe is totally transparent, needing no modifications in the original binary code. With a set of extensive experiments (fifteen benchmarks and four multicore platforms), we show that Hebe outperforms state-of-the-art approaches with very close results from the best possible solution given by an exhaustive search. © 2021 Elsevier Inc.","Aging; Optimization; Runtime system; Thread-level parallelism"
"Secure blockchain enabled Cyber–physical systems in healthcare using deep belief network with ResNet model","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104602921&doi=10.1016%2fj.jpdc.2021.03.011&partnerID=40&md5=0f74c9f049003d2fe29349ef24a2bba3","Cyber–physical system (CPS) is the incorporation of physical processes with processing and data transmission. Cybersecurity is a primary and challenging issue in healthcare due to the legal and ethical perspective of the patient's medical data. Therefore, the design of CPS model for healthcare applications requires special attention for ensuring data security. To resolve this issue, this paper proposes a secure intrusion, detection with blockchain based data transmission with classification model for CPS in healthcare sector. The presented model performs data acquisition process using sensor devices and intrusion detection takes place using deep belief network (DBN) model. In addition, the presented model uses a multiple share creation (MSC) model for the generation of multiple shares of the captured image, and thereby achieves privacy and security. Besides, the blockchain technology is applied for secure data transmission to the cloud server, which executes the residual network (ResNet) based classification model to identify the presence of the disease. The experimental validation of the presented model takes place using NSL-KDD 2015, CIDDS-001 and ISIC dataset. The simulation outcome pointed out the effective outcome of the presented model. © 2021 Elsevier Inc.","Blockchain; Cyber–physical system; Deep learning; Intrusion detection; Security"
"Efficient shuffle management for DAG computing frameworks based on the FRQ model","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098226471&doi=10.1016%2fj.jpdc.2020.11.008&partnerID=40&md5=9691f4e75f20d9f35d859bd53f8c18b8","In large-scale data-parallel analytics, shuffle, namely the cross-network read and the aggregation of partitioned data between tasks with data dependencies, usually bring in large overhead. To reduce shuffle overhead, we present SCache, an open-source plug-in system that particularly focuses on shuffle optimization. SCache adopts heuristic pre-scheduling combining with shuffle size prediction to pre-fetch shuffle data and balance load on each node. Meanwhile, SCache takes full advantage of the system memory to accelerate the shuffle process. We also propose a new performance model called Framework Resources Quantification (FRQ) model to analyze DAG frameworks and evaluate the SCache shuffle optimization. The FRQ model quantifies the utilization of resources and predicts the execution time of each phase of DAG jobs. We have implemented SCache on both Spark and Hadoop MapReduce. The performance of SCache has been evaluated with both simulations and testbed experiments on a 50-node Amazon EC2 cluster. Those evaluations have demonstrated that, by incorporating SCache, the shuffle overhead of Spark can be reduced by nearly 89%, and the overall completion time of TPC-DS queries improves 40% on average. On Apache Hadoop MapReduce, SCache optimizes end-to-end Terasort completion time by 15%. © 2020 Elsevier Inc.","Distributed DAG frameworks; Optimization; Performance model; Shuffle"
"A fine-grained anonymous handover authentication protocol based on consortium blockchain for wireless networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110007308&doi=10.1016%2fj.jpdc.2021.06.007&partnerID=40&md5=5caae035d141d94fef408078fe5b3ca0","Given the ubiquitous nature of wireless networks in today's society, protecting users' information and identity during handover authentication is paramount. Mobile network operators hope to implement a more flexible and fine-grained authentication to provide better and safer services. In this paper, we propose a new multi-attribute authority attribute-based signature (MA-ABS) scheme that has a constant-size signature. More specifically, we propose an anonymous handover authentication protocol that uses MA-ABS and consortium blockchain. This new protocol not only has fine-grained access control, but it protects the user's privacy and ensures an efficient and private handover. © 2021 Elsevier Inc.","Attribute-based signature; Consortium blockchain; Handover authentication"
"PUMIPic: A mesh-based approach to unstructured mesh Particle-In-Cell on GPUs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108453521&doi=10.1016%2fj.jpdc.2021.06.004&partnerID=40&md5=9fb7278f8d913e8cd48fda98ec27d933","Unstructured mesh particle-in-cell, PIC, simulations executing on the current and next generation of massively parallel systems require new methods for both the mesh and particles to achieve performance and scalability on GPUs. The traditional approach to implementing PIC simulations defines data structures and algorithms in terms of particles with a full copy of the unstructured mesh on every process. To effectively scale the unstructured mesh and particles, mesh-based PIC uses the unstructured mesh as the predominant data structure with the particles stored in terms of the mesh entities. This paper details the PUMIPic library, a framework for developing efficient and performance-portable mesh-based PIC simulations on GPU systems. A pseudo physics simulation based on a five-dimensional gyro-kinetic code for modeling plasma physics is used to examine the performance of PUMIPic. Scaling studies of the unstructured mesh partition and number of particles are performed up to 4096 nodes of the Summit system at Oak Ridge National Laboratory. The studies show that mesh-based PIC can utilize a partitioned mesh and maintain scaling up to system limitations. © 2021 Elsevier Inc.","GPU; Particle-in-cell; Unstructured mesh"
"An improved framework of GPU computing for CFD applications on structured grids using OpenACC","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108076612&doi=10.1016%2fj.jpdc.2021.05.010&partnerID=40&md5=81384d82b693f71892c46f9cb91a8e9f","This work is focused on improving multi-GPU performance of a research CFD code on structured grids. MPI and PGI 18.1 OpenACC directives are used to scale the code up to 16 NVIDIA GPUs. This work shows that using 16 P100 GPUs and 16 V100 GPUs can be up to 30× and 90× faster than 16 Xeon CPU E5-2680v4 cores for three different test cases, respectively. A series of performance issues related to the scaling for the multi-block CFD code are addressed by applying various optimizations. Performance optimizations such as the pack/unpack message method, removing temporary arrays as arguments to procedure calls, allocating global memory for limiters and connected boundary data, reordering non-blocking MPI I_send/I_recv and blocking Wait calls, reducing unnecessary implicit derived type member data movement between the host and the device and the use of GPUDirect can improve the compute utilization, memory throughput, and asynchronous progression in the multi-block CFD code using modern programming features. © 2021 Elsevier Inc.","CFD; MPI; OpenACC; Performance optimization; Structured grid"
"A secured distributed detection system based on IPFS and blockchain for industrial image and video data security","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102880024&doi=10.1016%2fj.jpdc.2021.02.022&partnerID=40&md5=08e97f3019fb47d5e198a54dc29f489c","Copyright infringement adversely affects the interest of copyright holders of images and videos which are uploaded to different websites and peer-to-peer image sharing systems. This paper addresses the problem of detecting copyright infringement so that copyright holders are given due credit for their work. There are several images and videos that are shared every day by millions of users with some amount of modification in images and videos originally uploaded by the copyright holders such as photographers, graphic designers, and video providers. Copyright violators, who are not the original creators of multimedia content modify them using image processing and frame modification techniques such as grayscale conversion, cropping, rotation, frame compression, and frame speed manipulation. Then, upload the tampered images and videos. To address this problem, we propose an IPFS-based (InterPlanetary File System-based) decentralized peer-to-peer image and video sharing platform built on blockchain technology. We use a perceptual hash (pHash) technique to detect copyright violations of multimedia. When multimedia is to be uploaded to the IPFS, the pHash of the same content is determined and checked against existing pHash values in the blockchain network. Similarity with existing pHash values would result in the multimedia being detected as tampered with. Blockchain technology offers the advantage of non-involvement of a third party and consequently the avoidance of a single point of failure. © 2021 Elsevier Inc.","Blockchain; Copyright; Distributed Images; IPFS; Perceptual hash (pHash); Security"
"DDMTS: A novel dynamic load balancing scheduling scheme under SLA constraints in cloud computing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098458243&doi=10.1016%2fj.jpdc.2020.11.007&partnerID=40&md5=4b569553dbd1d910ee95e020d051ef55","Cloud computing is a computing method based on the Internet designed to share resources through virtualization technology. For a large number of requests waiting to be processed, task scheduling is used to reasonably allocate computing resources to requests. With the rapid development of computer hardware and software, deep reinforcement learning (DRL) provides a new direction for better solving task scheduling problems. In this paper, we propose a novel DRL-based dynamic load balancing task scheduling algorithm under service-level agreement (SLA) constraints to reduce the load imbalance of virtual machines (VMs) and task rejection rate. First, we use the DRL method to select a suitable VM for the task and then determine whether to execute the task on the selected VM violates the SLA. If the SLA is violated, the task is refused and feedback a negative reward for DRL training; otherwise, the task is received and executed, and feedback a reward according to the balance of the VMs load after the task is executed. Compared with three other task scheduling algorithms applied to randomly generated benchmark and Google real user workload trace benchmark, the proposed algorithm exhibits the best performance in balancing VMs load and reducing the task rejection rate, improving the overall level of cloud computing services. © 2020 Elsevier Inc.","Cloud computing; Deep reinforcement learning; Load balancing; Service-level agreement; Task scheduling"
"Teaching parallel and distributed computing concepts in simulation with WRENCH","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108073626&doi=10.1016%2fj.jpdc.2021.05.009&partnerID=40&md5=f4254f5a2dbeedc30e8ddc03b1fd8b3e","Teaching parallel and distributed computing topics in a hands-on manner is challenging, especially at introductory, undergraduate levels. Participation challenges arise due to the need to provide students with an appropriate compute platform, which is not always possible. Even if a platform is provided to students, not all relevant learning objectives can be achieved via hands-on learning on a single platform. In particular, it is typically not feasible to provide students with platform configurations representative of emerging and future cyberinfrastructure scenarios (e.g., highly distributed, heterogeneous platforms with large numbers of high-end compute nodes). To address these challenges, we have developed a set of pedagogic modules that can be integrated piecemeal into university courses. These modules include simulation-driven activities for students to experience relevant application and platform scenarios hands-on. These activities are supported by simulators developed using the WRENCH simulation framework. After motivating and describing our approach, we present and analyze results obtained from evaluations performed in two consecutive offerings of an undergraduate university course. © 2021 Elsevier Inc.","Computer science education; Parallel and distributed computing education; Simulation"
"A new genetic-based approach for solving k-coverage problem in directional sensor networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104570861&doi=10.1016%2fj.jpdc.2021.03.006&partnerID=40&md5=7476b75ce4e1187956aeedaf9024553b","In recent years, the use of directional sensor networks (DSNs) has continued to rise increasingly, which is due to their extensive use in many situations. In such networks, the main problem is how to cover the targets distributed in a defined area and simultaneously prolong the network lifetime as much as possible. The reason of this problem is the limitation of both sensing angle and energy resource of sensors in such networks. This problem gets more complex in cases where targets need to be covered by more than one sensor direction (i.e., each target needs to be monitored for at least k times). This problem is generally known as target k-coverage problem which its NP-completeness has been already proved in the literature. The k-coverage problem can be considered in over-provisioned and under-provisioned environments. In both of these environments, especially in the latter, it is important to create a balanced coverage, as these environments do not have enough sensors to monitor all targets for k times. Thus, in this paper, we proposed a genetic-based algorithm to solve the problem in over-provisioned environments, then developed the proposed algorithm in another way to solve the problem in under-provisioned networks so that it uses the minimum number of sensors. many experiments were performed to test the efficiency of the proposed algorithm, and the obtained results showed the high capacity of the proposed algorithm in solving the k-coverage problem in both environments. © 2021 Elsevier Inc.","Directional sensor networks; Genetic algorithm; Scheduling algorithms"
"Understanding failures through the lifetime of a top-level supercomputer","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105698600&doi=10.1016%2fj.jpdc.2021.04.001&partnerID=40&md5=25118f903337a752b9e4d459a38f34c1","High performance computing systems are required to solve grand challenges in many scientific disciplines. These systems assemble many components to be powerful enough for solving extremely complex problems. An inherent consequence is the intricacy of the interaction of all those components, especially when failures come into the picture. It is crucial to develop an understanding of how these systems fail to design reliable supercomputing platforms in the future. This paper presents the results on studying multi-year failure and workload records of a powerful supercomputer that topped the world rankings. We provide a thorough analysis of the data and characterize the reliability of the system through several dimensions: failure classification, failure-rate modelling, and interplay between failures and workload. The results shed some light on the dynamics of top-level supercomputers and sensitive areas ripe for improvement. © 2021 Elsevier Inc.","Failure analysis; Fault tolerance; High performance computing; Resilience"
"Flexible scheme for reconfiguring 2D mesh-connected VLSI subarrays under row and column rerouting","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100266803&doi=10.1016%2fj.jpdc.2021.01.003&partnerID=40&md5=c700fb493bcc960efbe78e2d95fb50ab","In the mesh-connected processors, some processor elements (PEs) become ineffective due to high temperature, overload and other factors, which can affect the stability of the system. This paper deals with the problem of reconfiguring the largest possible subarray from the processor with faults under the row and column rerouting constraint. Firstly, a flexible routing scheme, based on dynamic programming, is proposed to construct the local optimal logical columns. Secondly, we discuss and revise the PEs that cannot be connected between every two logical columns under this scheme. Finally, an efficient algorithm is presented to construct the maximum subarray in polynomial time. The experimental results show that, both on the random and clustered fault scenarios, the proposed algorithm under flexible rerouting scheme is capable of constructing the larger scale logical arrays. On a 48 × 48 host array with 15% fault density, the improvement on the use of fault-free PEs is up to 6.22% for random faults. On a 256 × 256 host array, the improvement can be up to 85.60% for clustered faults. Moreover, the proposed algorithm runs faster than previous algorithms under different size arrays and fault densities, the average improvement in running time is up to 99% compared with state-of-the-art. © 2021 Elsevier Inc.","Algorithm; Degradable VLSI array; Fault tolerance; Reconfiguration; Rerouting scheme"
"Granite: A distributed engine for scalable path queries over temporal property graphs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101797551&doi=10.1016%2fj.jpdc.2021.02.004&partnerID=40&md5=b9d7d3509190554c3f63050e9d0ffb7e","Property graphs are a common form of linked data, with path queries used to traverse and explore them for enterprise transactions and mining. Temporal property graphs are a recent variant where time is a first-class entity to be queried over, and their properties and structure vary over time. These are seen in social, telecom, transit and epidemic networks. However, current graph databases and query engines have limited support for temporal relations among graph entities, no support for time-varying entities and/or do not scale on distributed resources. We address this gap by extending a linear path query model over property graphs to include intuitive temporal predicates and aggregation operators over temporal graphs. We design a distributed execution model for these temporal path queries using the interval-centric computing model, and develop a novel cost model to select an efficient execution plan from several. We perform detailed experiments of our Granite distributed query engine using both static and dynamic temporal property graphs as large as 52M vertices, 218M edges and 325M properties, and a 1600-query workload, derived from the LDBC benchmark. We frequently offer sub-second query latencies on a commodity cluster, which is 149×–1140× faster compared to industry-leading Neo4J shared-memory graph database and the JanusGraph/Spark distributed graph query engine. Granite also completes 100% of the queries for all graphs, compared to only 32–92% workload completion by the baseline systems. Further, our cost model selects a query plan that is within 10% of the optimal execution time in 90% of the cases. Despite the irregular nature of graph processing, we exhibit a weak-scaling efficiency of ≥60% on 8 nodes and ≥40% on 16 nodes, for most query workloads. © 2021 Elsevier Inc.","Big data platforms; Distributed scheduling; Graph processing; Query planning; Temporal graphs"
"Certificateless privacy preserving public auditing for dynamic shared data with group user revocation in cloud storage","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108356622&doi=10.1016%2fj.jpdc.2021.06.001&partnerID=40&md5=d361b3d5e4595da253320158caa030bc","With the increasing popularity of data sharing among users of a group in clouds, shared data auditing has become an important issue in the cloud auditing field. To address this issue, many shared data auditing schemes have been proposed in the literature based on either public key infrastructure (PKI) or identity-based cryptography (IBC). However, these schemes suffer from issues of complex certificate management or key escrow problem. To address these problems, recently, a certificateless shared auditing scheme was put forward. However, it cannot support data dynamics and does not protect data privacy against a verifier, i.e., the verifier can derive data content when verifying the data integrity, which affects the scheme's security. This paper proposes certificateless privacy preserving public auditing scheme for dynamic shared data with group user revocation in cloud storage (CLPPPA). CLPPPA protects the privacy of data from the verifier by leveraging a random masking technique. Further, CLPPPA also supports shared data dynamics and group user revocation. We formally prove the security of CLPPPA under computational Diffie-Hellman (CDH) and discrete logarithm (DL) assumptions in the Random Oracle Model (ROM). The performance of CLPPPA is evaluated by theoretical analysis, experimental results, and compared with the state-of-the-art ones. The results demonstrate that CLPPPA achieves desirable efficiency. © 2021 Elsevier Inc.","Certificateless signatures; Cloud storage; Data dynamics; Privacy preserving; Shared data"
"A cloud framework for problem-based learning on grid computing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105560581&doi=10.1016%2fj.jpdc.2021.04.012&partnerID=40&md5=37c89e15938696c02f17f1e0e3161c7e","Training on Grid technologies has traditionally used existing Grid infrastructures to implement the hands-on education activities. However, these infrastructures are insufficient to develop all training skills as they can only be employed for the development of Grid applications, and they are limited for learning the management and configuration of Grid resources. The paper presents a set of educational activities grouped on a Project Based Learning (PBL) framework for training on Grid technologies. A Cloud-based tool has been implemented to provide Grid infrastructures as a Service on the cloud, with enhanced scalability and administration capabilities. The PBL has achieved a high impact in the teaching-learning process, addressing the training in all the necessary skills and efficiently providing Grid infrastructure resources on public clouds at a moderate cost. Finally, we evaluated the students' opinion on the activities achieving a very satisfactory result and a reasonable balance on the complexity of the PBL stages. © 2021 Elsevier Inc.","Cloud computing; Grid computing; Project based learning"
"Identifying compromised hosts under APT using DNS request sequences","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102048049&doi=10.1016%2fj.jpdc.2021.02.017&partnerID=40&md5=3388e01f329a645cf2941c372e3695fe","Advanced persistent threats (APTs) have become a major cyber threat to large organizations. To steal confidential data from specific organizations, attackers adopt highly targeted intrusion schemes. Prior to stealing critical data, APT activities hide themselves in legitimate activities and consistently elevate their privileges, making them very difficult to detect. The detection of malicious domains during domain name service (DNS) analysis accounts for the majority of existing detection methods. However, a limited number of available samples and rapidly changing sets of malicious domain names reduce the efficacy of such approaches. By investigating numerous APT reports, we determined that the activities of DNS requests in APT attacks exhibit clear temporal patterns that are ignored by most existing schemes. Therefore, we can analyze the DNS sequences requested by each host and their time-related features to identify compromised hosts. This paper summarizes the patterns of host DNS requests and proposes several assumptions. We take advantage of machine learning to identify compromised hosts by quantifying these assumptions in the form of feature vectors. We deployed the proposed approach into large-scale network environments and experimental evaluations demonstrated that our method is able to detect hosts compromised by APTs efficiently with a precision of 97.3% and detection rate of 96.2%. © 2021 Elsevier Inc.","Advanced persistent threat (APT); Compromised hosts; Domain name service (DNS) request sequence; Temporal patterns; Unsupervised learning"
"Leveraging teaching on demand: Approaching HPC to undergrads","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107980217&doi=10.1016%2fj.jpdc.2021.05.015&partnerID=40&md5=e9833f1282187231183151cd1015aa98","High Performance Computing (HPC) is a highly demanded discipline in companies and institutions. However, as students and also afterwards as professors, we observed a lack of HPC related content in the engineering degrees at our university, including Computer Science. Thus, we designed and offered the engineering students a non-mandatory course entitled “Build your own cluster employing Raspberry Pi” to provide the students with HPC skills. With this course, we covered the basics of supercomputing (hardware, networking, software tools, performance evaluation, cluster management, etc.). This was possible thanks to leveraging the flexibility and versatility of Raspberry Pi devices, and the students' motivation that arose from the hands-on experience. Moreover, the course included a “Teaching on demand” component to let the attendees choose a field to explore, based on their own interests. In this paper, we offer all the details to let anyone fully reproduce the course. Besides, we analyze and evaluate the methodology that let us fulfill our objectives: increase the students' HPC skills and knowledge in such a way that they feel capable of utilizing it in their mid-term professional career. © 2021 Elsevier Inc.","Computational cluster; Parallel and distributed computing; Raspberry Pi; System administration; Undergrad teaching"
"A novel hybrid resampling algorithm for parallel/distributed particle filters","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101376385&doi=10.1016%2fj.jpdc.2021.02.005&partnerID=40&md5=76d6e5a79c59a7347b55434b6714228e","Parallel/Distributed particle filters have been widely used in the estimation of states of dynamic systems by using multiple processing units (PUs). In parallel/distributed particle filters, the centralized resampling needs a central unit (CU) to serve as a hub to execute the global resampling. The centralized scheme is the main obstacle for the improved performance due to its global nature. To reduce the communication cost, the decentralized resampling was proposed, which only conducted the resampling on each PU. Although the decentralized resampling can improve the performance, it suffers from the low accuracy due to the local nature. Therefore, we propose a novel hybrid resampling algorithm to dynamically adjust the intervals between the centralized resampling steps and the decentralized resampling steps based on the measured system convergence. We formulate the proposed algorithm and prove it to be uniformly convergent. Since the proposed algorithm is a generalization of various versions of the hybrid resampling, its proof provides the solid theoretical foundation for their wide adoptions in parallel/distributed particle filters. In the experiments, we evaluate and compare different resampling algorithms including the centralized resampling algorithm, the decentralized resampling algorithm, and different types of existing hybrid resampling algorithms to show the effectiveness and the improved performance of the proposed hybrid resampling algorithm. © 2021 Elsevier Inc.","Convergence; Parallel and distributed computing; Particle filters; Resampling; Sequential Monte Carlo methods"
"A novel MPI+MPI hybrid approach combining MPI-3 shared memory windows and C11/C++11 memory model","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110076698&doi=10.1016%2fj.jpdc.2021.06.008&partnerID=40&md5=219d5a3d089b024dbce1bd6bfbe966b5","The increase of the number of cores in processors used in modern cluster architectures advocates hybrid parallel programming, combining Message Passing Interface (MPI) for internode operations and a shared memory treatment of intranode operations. We propose an MPI+MPI hybrid approach to parallel programming in which shared memory operations are managed by the combination of MPI shared memory windows introduced with MPI-3, C11/C++11 atomic operations and the associated multi-thread memory model. We illustrate the method on fundamental parallel operations (barrier, reduction) and on the ghost update, which is prevalent in many parallel numerical methods. The performance tests on Reedbush-U and Oakbridge-CX systems show that using the C11/C++11 memory model to manage shared memory windows can achieve levels of performance comparable to state of the art MPI implementations, while reducing the variance of execution times as well as increasing the level of synchronization between processes, especially in multiple nodes environments. It also reduces significantly the execution time of ghost updates compared to flat MPI, and the synchronization of shared data with the C++11 memory model is observed to be more efficient than other synchronization methods based on RMA utilities. © 2021 Elsevier Inc.","C++11 memory model; Hybrid parallel programming; MPI shared memory model; MPI-3"
"Edge computing server placement with capacitated location allocation","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102887967&doi=10.1016%2fj.jpdc.2021.03.007&partnerID=40&md5=c01d29bd79a66db4b1677bf9921d7dc9","The deployment of edge computing infrastructure requires a careful placement of the edge servers, with an aim to improve application latencies and reduce data transfer load in opportunistic Internet of Things systems. In the edge server placement, it is important to consider computing capacity, available deployment budget, and hardware requirements for the edge servers and the underlying backbone network topology. In this paper, we thoroughly survey the existing literature in edge server placement, identify gaps and present an extensive set of parameters to be considered. We then develop a novel algorithm, called PACK, for server placement as a capacitated location–allocation problem. PACK minimizes the distances between servers and their associated access points, while taking into account capacity constraints for load balancing and enabling workload sharing between servers. Moreover, PACK considers practical issues such as prioritized locations and reliability. We evaluate the algorithm in two distinct scenarios: one with high capacity servers for edge computing in general, and one with low capacity servers for Fog computing. Evaluations are performed with a data set collected in a real-world network, consisting of both dense and sparse deployments of access points across a city area. The resulting algorithm and related tools are publicly available as open source software. © 2021 The Author(s)","Clustering; Facility location; k-medoid; Multi-access edge computing"
"When services computing meets blockchain: Challenges and opportunities","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098694241&doi=10.1016%2fj.jpdc.2020.12.003&partnerID=40&md5=716f85f4133508386835bad32cc17711","Services computing can offer a high-level abstraction to support diverse applications via encapsulating various computing infrastructures. Though services computing has greatly boosted the productivity of developers, it is faced with three main challenges: privacy and security risks, information silo, and pricing mechanisms and incentives. The recent advances of blockchain bring opportunities to address the challenges of services computing due to its build-in encryption as well as digital signature schemes, decentralization feature, and intrinsic incentive mechanisms. In this paper, we present a survey to investigate the integration of blockchain with services computing. The integration of blockchain with services computing mainly exhibits merits in two aspects: i) blockchain can potentially address key challenges of services computing and ii) services computing can also promote blockchain development. In particular, we categorize the current literature of services computing based on blockchain into five types: services creation, services discovery, services recommendation, services composition, and services arbitration. Moreover, we generalize Blockchain as a Service (BaaS) architecture and summarize the representative BaaS platforms. In addition, we also outline open issues of blockchain-based services computing and BaaS. © 2020 Elsevier Inc.","Blockchain; Blockchain-as-a-Service; Security; Services computing; Smart contract"
"MATAR: A performance portability and productivity implementation of data-oriented design with Kokkos","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.03.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110211941&doi=10.1016%2fj.jpdc.2021.03.016&partnerID=40&md5=73896885ebf145c7b34a6bf1e48f39e0","There is a need for simple, fast, and memory-efficient multidimensional data structures for dense and sparse storage that arise with numerical methods and in software applications. The data structures must perform equally well across multiple computer architectures, including CPUs and GPUs. For this purpose, we developed MATAR, a C++ software library that allows for simple creation and use of intricate data structures that is also portable across disparate architectures using Kokkos. The performance aspect is achieved by forcing contiguous memory layout (or as close to contiguous as possible) for multidimensional and multi-size dense or sparse MATrix and ARray (hence, MATAR) types. Our results show that MATAR has the capability to improve memory utilization, performance, and programmer productivity in scientific computing. This is achieved by fitting more work into the available memory, minimizing memory loads required, and by loading memory in the most efficient order. This document describes the purpose of the work, the implementation of each of the data types, and the resulting performance both in some simple baseline test cases and in an application code. © 2021 Elsevier Inc.","Dense and sparse storage; GPUs; Memory efficiency; Performance; Portability; Productivity"
"PAASH: A privacy-preserving authentication and fine-grained access control of outsourced data for secure smart health in smart cities","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106341693&doi=10.1016%2fj.jpdc.2021.05.001&partnerID=40&md5=1729bb6a82def4fb0865be05b70a9ff4","Very recently, a number of intelligent applications have evolved in smart cities such as smart grid, smart parking, smart waste management, smart manufacturing, smart home, and smart health, making our cities more smarter. In the context of smart health, smart devices are employed to collect and transmit biomedical information to a medical server, allowing medical practitioners to give better and improved healthcare services. However, considering the sensitive nature of the medical data, any illegal access may have devastating effects on the patient's health condition. A lightweight privacy-preserving authentication and fine-grained access control scheme for smart health known as PAASH is proposed. In PAASH, a new ultraefficient certificateless signature scheme with compact aggregation is developed to achieve source authentication and data integrity. To achieve efficient user authorization and data confidentiality, a privacy-preserving access control technique using the developed aggregate signature and ciphertext-policy attribute-based encryption (CP-ABE) is implemented. An analysis of PAASH reveals that it can provide desirable privacy and security measures in smart health. Moreover, the formal security demonstrated in the random oracle model (ROM) shows that PAASH is semantically secure under the intractability of the Discrete Logarithm Problem (DLP). The performance evaluation shows that PAASH is par excellence practically suitable for smart health. © 2021 Elsevier Inc.","Access control; Aggregation; Certificateless aggregate signature; Random oracle; Smart health"
"Deep learning inspired routing in ICN using Monte Carlo Tree Search algorithm","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100085265&doi=10.1016%2fj.jpdc.2020.12.014&partnerID=40&md5=c1fc1ea4436a29a8469430f11daa30ba","Information Centric Networking (ICN) provides caching strategies to improve network performance based on consumer demands from the intermediate routers. It reduces the load on content server, network traffic, and improves end-to-end delay. The content requesters use an Interest packet containing the name of data to express their needs. If such Interest packets are routed efficiently, the end to end delay and throughput of the network could be improved further. This paper describes an efficient method of forwarding Interest packets to retrieve the requested content at the earliest possible time. Here the data source is found and considered as a single player game with content requester as its start state and location of the desired content as final or goal state. The Monte Carlo Tree Search (MCTS) algorithm is used for constructing the path from content requester to concerned data source. For performance evaluation, the proposed scheme is integrated with Leave Copy Down (LCD) and Leave Copy Everywhere (LCE), Cache Less for More (CL4M), and Probability based caching (ProbCache) In ns-3 simulation environment (ndnSim), all these are evaluated in terms of content search latency, server hit ratio, network load, overhead and throughput. Simulation observation reveals that the integration of MCTS significantly improves performance in regard to experimental parameters. © 2020 Elsevier Inc.","Content centric network (CCN); Forwarding; Information Centric Network (ICN); MCTS; Routing"
"A note on labeling methods to schedule unit execution time tasks in the presence of delayed precedence constraints","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106551154&doi=10.1016%2fj.jpdc.2021.05.002&partnerID=40&md5=4074aca8f29f9dd950c5edcdafec52ba","There is some evidence that labeling schemes as employed for instance in the famous Coffman-Graham algorithm may provide superior worst-case approximation guarantees than purely path- or level-based list schedules in the presence of (delayed) precedence constraints. In 1989, Bernstein, Rodeh, and Gertner proved that this also holds true for their labeling scheme targeting unit execution time tasks on a single processor provided that all delays imposed by a single task on all of its successors are uniformly either zero or some fixed positive integer. They further conjectured that this superiority is preserved when allowing the delays imposed by a task to differ successor-wise. It is shown in this note however that their labeling scheme as well as more general ones may perform as bad as any list schedule in this case. Moreover, a new lower bound on the worst-case performance of labeling methods in the multiprocessor setting is derived. © 2021 Elsevier Inc.","Coffman-Graham algorithm; List scheduling; Pipeline scheduling; Worst-case approximation analysis"
"Generalizing the over operator for parallelization and order-independency","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101645830&doi=10.1016%2fj.jpdc.2021.02.001&partnerID=40&md5=d111ae41b12ef9147530c9dbe6b62a1b","The over operator is commonly used for α-blending in various visualization techniques. In the current form, it is a binary operator and must strictly follow a specific composition order of all participating operands, hence posing a significant performance limit. In this paper, we derive a set of generic formulas for the over operator that work with any number of operands and completely remove the restriction on the composition order. We prove the correctness of these formulas and provide a step-by-step illustration in a blending context. We implement both a sequential and a parallel version of the improved over operator and apply them to the image composition process where operands are received out of order with different arrival time intervals. The performance superiority of the improved over operator over the original one is established by rigorous theoretical analyses and further validated by extensive experimental results. © 2021 Elsevier Inc.","Image composition; Over operator; Parallel visualization"
"Energy saving strategy and Nash equilibrium of hybrid P2P networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109843820&doi=10.1016%2fj.jpdc.2021.06.009&partnerID=40&md5=403ff5f368cd13d7c3534b58cd9d98cc","This paper proposes a penalty strategy with differentiated service rate based on the free riding phenomenon in P2P networks, and establishes an M/M/c+d queueing model. Based on this model, a sleep/wakeup mechanism is introduced for the peers at the service end, and a single asynchronous vacation strategy is adopted to reduce the energy consumption of the system. In addition, the energy consumption of peers in each state is quantified, and the relationship between the energy consumption and parameters of the system is analyzed. In order to avoid excessive requests for unnecessary services from requesting nodes and increasing energy consumption of the system, this paper analyzes the Nash equilibrium between the arrival rate and the net profit of a single node, and then studies the optimization of social profit. The stationary distribution of queueing model is obtained by the method of matrix geometric solution, the performance indicators of the system are constructed, and the system performance is analyzed by numerical experiments. Experimental results show that the model developed in this paper has a significant penalty effect on free riding behavior, and that the single asynchronous vacation strategy not only saves more than 10% of the total energy consumption compared with the single synchronous vacation strategy, but also makes the hybrid P2P networks more flexible and efficient. © 2021 Elsevier Inc.","Energy saving strategy; Hybrid P2P network; Matrix geometric solution; Nash equilibrium"
"Performance Analysis and Optimization Opportunities for NVIDIA Automotive GPUs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102043480&doi=10.1016%2fj.jpdc.2021.02.008&partnerID=40&md5=b90db2589a22e63d4c2eecc83bc5307d","Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD) bring unprecedented performance requirements for automotive systems. Graphic Processing Unit (GPU) based platforms have been deployed with the aim of meeting these requirements, being NVIDIA Jetson TX2 and its high-performance successor, NVIDIA AGX Xavier, relevant representatives. However, to what extent high-performance GPU configurations are appropriate for ADAS and AD workloads remains as an open question. This paper analyzes this concern and provides valuable insights on this question by modeling two recent automotive NVIDIA GPU-based platforms, namely TX2 and AGX Xavier. In particular, our work assesses their microarchitectural parameters against relevant benchmarks, identifying GPU setups delivering increased performance within a similar cost envelope, or decreasing hardware costs while preserving original performance levels. Overall, our analysis identifies opportunities for the optimization of automotive GPUs to further increase system efficiency. © 2021 Elsevier Inc.","Automotive GPUs; Design space exploration; Optimization; Performance analysis"
"Improving the accuracy of energy predictive models for multicore CPUs by combining utilization and performance events model variables","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101308968&doi=10.1016%2fj.jpdc.2021.01.007&partnerID=40&md5=605bd2f302d3811e02e62a1eaa5b5611","Energy predictive modeling is the leading method for determining the energy consumption of an application. Performance monitoring counters (PMCs) and resource utilizations have been the principal source of model variables primarily due to their high positive correlation with energy consumption. Performance events, however, have come to dominate the landscape due to their better prediction accuracy compared to utilization variables. Recently, the theory of energy of computing has been proposed whose practical implications for constructing accurate and reliable linear energy predictive models are unified in a consistency test that includes a selection criterion of additivity for model variables. In this work, we analyze the prediction accuracy of models employing utilization variables only, PMCs only, and combination of both utilization variables and PMCs, through the lens of this theory for modern multicore CPU platforms. We discover that employing utilization variables only in linear energy predictive models does not capture all the energy-consuming activities during an application execution. However, combination of utilization variables with PMCs that are highly additive and highly correlated with energy consumption, gives the most accurate linear energy predictive model. Our experimental results show that application-specific and platform-level models using both utilization variables and PMCs exhibit up to 3.6× and 2.6× better average prediction accuracy respectively when compared with models employing utilization variables only and highly additive PMCs only. © 2021 The Author(s)","CPU utilization; Energy modeling; Energy predictive models; Multicore CPU; Performance monitoring counters"
"Cross-domain secure data sharing using blockchain for industrial IoT","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108708835&doi=10.1016%2fj.jpdc.2021.05.007&partnerID=40&md5=893651c95ab7bcd945b868a733e42295","The Industrial Internet of Things (IIoT) enhances smart manufacturing process that escalates productivity through revolutionary techniques. The manufacturing process is sophisticated and complex because of various IoT domains (e.g. industries). A final product is an outcome of the efforts of several departments from different industries. However, this raises the cross-domain communication's privacy and security issues. Cross-domain data sharing for product manufacturing is a challenging research direction. This paper proposes a centralized cloud-based cross-domain data sharing platform using multiple security gateways. The security gateways use the blockchain to store the information into the centralized cloud. Once the application reported a malicious activity, the centralized cloud verifies the concern from the blockchain. Further, an action is taken against the party that performs malicious activity in the security gateways. The algorithms are designed for authentication and transaction of data. The proposed framework is able the secure data movement among different domains globally. The experiment result demonstrates that the proposed security and privacy framework helps to maintain trust among the industries that collaborate on manufacturing across the domains. © 2021 Elsevier Inc.","Authentication; Blockchain; Cross-domain data sharing; Industrial Internet of Things (IIoT); Smart contract"
"TSM2X: High-performance tall-and-skinny matrix–matrix multiplication on GPUs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101600502&doi=10.1016%2fj.jpdc.2021.02.013&partnerID=40&md5=3cd14c29f5f14db828bfc927cd590085","Linear algebra operations have been widely used in big data analytics and scientific computations. Many works have been done on optimizing linear algebra operations on GPUs with regular-shaped input. However, few works focus on fully utilizing GPU resources when the input is not regular-shaped. Current optimizations do not consider fully utilizing the memory bandwidth and computing power; therefore, they can only achieve sub-optimal performance. In this paper, we propose two efficient algorithms – TSM2R and TSM2L – for two classes of tall-and-skinny matrix–matrix multiplications on GPUs. Both of them focus on optimizing linear algebra operation with at least one of the input matrices tall-and-skinny. Specifically, TSM2R is designed for a large regular-shaped matrix multiplying a tall-and-skinny matrix, while TSM2L is designed for a tall-and-skinny matrix multiplying a small regular-shaped matrix. We implement our proposed algorithms and test on several modern NVIDIA GPU micro-architectures. Experiments show that, compared to the current state-of-the-art works, (1) TSM2R speeds up the computation by 1.6x on average and improves the memory bandwidth utilization and computing power utilization by 18.1% and 20.5% on average, respectively, when the regular-shaped matrix size is relatively large or medium; and (2) TSM2L speeds up the computation by 1.9x on average and improves the memory bandwidth utilization by up to 9.3% on average when the regular-shaped matrix size is relatively small. © 2021 Elsevier Inc.","CUDA; GPU; Matrix–matrix multiplication; Performance optimization; Tall-and-skinny matrix"
"Connected k-coverage in two-dimensional wireless sensor networks using hexagonal slicing and area stretching","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104111112&doi=10.1016%2fj.jpdc.2020.12.008&partnerID=40&md5=6d1d9aa9141d35c29d7e56fa8e7d4ff5","The problem of coverage in two-dimensional (2D) wireless sensor networks is challenging and is still open. Precisely, determining the minimum sensor density (i.e, minimum number of sensors per unit area) that is required to cover a 2D field of interest (FoI), where every point in the field is covered by at least one sensor, is still under investigation. The problem of 2D k-coverage, which requires that every point in a 2D FoI be covered by at least k sensors, where k≥1, is more challenging. In this paper, we attempt to address the 2D connected k-coverage problem, where a 2D FoI is k-covered, while the underlying set of sensors k-covering the field forms a connected network. We propose to solve this problem using an approach based on slicing 2D FoI into convex regular hexagons. Our goal is to achieve k-coverage of a 2D FoI with a minimum number of sensors in order to maximize the network lifetime. First, we compute the minimum sensor density for 2D k-coverage using the regular convex hexagon, which is a 2D paver (i.e., covers a 2D field without gaps or overlaps). Indeed, we found that the regular convex hexagon best assimilates the sensors’ sensing disk with respect to our proposed metric, sensing range usage rate. Second, we derive the ratio of the communication range to the sensing range of the sensors to ensure connected k-coverage. Third, we propose an energy-efficient connected k-coverage protocol based on hexagonal slicing and area stretching. To this end, we formulate a multi-objective optimization problem, which computes an optimum solution to the 2D k-coverage problem that meets two requirements: Maximizing the size of the k-covered area, Ck, so as to minimize the sensor density to k-cover a 2D FoI (Requirement 1) and maximizing the area of the sensor locality, Lk, i.e., the region where at least k sensors are located to k-cover Ck, so as to minimize the interference between sensors (Requirement 2). Fourth, we show various simulation results to substantiate our theoretical analysis. We found a close-to-perfect match between our theoretical and simulation results. © 2020 Elsevier Inc.","Area stretching; Connected k-coverage; Hexagonal slicing; Optimization; Wireless sensor networks"
"Emulous mechanism based multi-objective moth–flame optimization algorithm","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098693596&doi=10.1016%2fj.jpdc.2020.12.010&partnerID=40&md5=b1dcd2ec3850b94aef0a75babe484d02","In recent years, there has been growing interest in using metaheuristic algorithms to solve various complex engineering optimization problems. Most of the real-world problems comprise of more than one objective. Due to the inherent difficulty of such problems and lack of proficiency, researchers in different domains often aggregate multiple objectives and use single-objective optimization algorithms to solve them. However, the aggregation-based methods fail to solve the multi-objective problems (MOPs) effectively. Several multi-objective evolutionary algorithms (MOEAs) have been proposed and are being used to solve such problems in the past few years. In this paper, we propose an Emulous Mechanism-based multi-objective Moth–Flame Optimization (EMMFO) algorithm, where the moth positions are updated based on the pairwise competitions between the moths in each generation. The proposed EMMFO is tested on a diverse set of multi-objective benchmark functions like ZDT, DTLZ, WFG, CEC09 special session test suites and four constrained engineering design problems. The results are compared with various state-of-the-art multi-objective algorithms like NSGAII, SPEA2, PESA2, MOEA/D, MOPSO, MOACO, NSMFO, IEMO, CLPSO-LS, MOEA/D-CRA, PAL-SAPSO, and MORBABC/D. Extensive experimental results demonstrate superior optimization performance of the proposed algorithm. © 2020 Elsevier Inc.","Constrained engineering design; Emulous learning; Moth–lame optimization; Multi-objective algorithm; Pareto-optimal solutions"
"Sigmoid: An auto-tuned load balancing algorithm for heterogeneous systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109366108&doi=10.1016%2fj.jpdc.2021.06.003&partnerID=40&md5=2d40a6be6552c75ec23f8bb59885525c","A challenge that heterogeneous system programmers face is leveraging the performance of all the devices that integrate the system. This paper presents Sigmoid, a new load balancing algorithm that efficiently co-executes a single OpenCL data-parallel kernel on all the devices of heterogeneous systems. Sigmoid splits the workload proportionally to the capabilities of the devices, drastically reducing response time and energy consumption. It is designed around several features; it is dynamic, adaptive, guided and effortless, as it does not require the user to give any parameter, adapting to the behaviour of each kernel at runtime. To evaluate Sigmoid's performance, it has been implemented in Maat, a system abstraction library. Experimental results with different kernel types show that Sigmoid exhibits excellent performance, reaching a utilization of 90%, together with energy savings up to 20%, always reducing programming effort compared to OpenCL, and facilitating the portability to other heterogeneous machines. © 2021 The Author(s)","Adaptability; Energy efficiency; Heterogeneous systems; Load balancing; OpenCL"
"Inter-kernel communication facility of a distributed operating system for NoC-based lightweight manycores","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104679898&doi=10.1016%2fj.jpdc.2021.04.002&partnerID=40&md5=97906d13a85c1185d5b0faaf820414ff","Lightweight manycore processors deliver high performance and scalability by bundling in a single chip hundreds of low-power cores, a distributed memory architecture and Networks-on-Chip (NoCs). Operating Systems (OSes) for these processors feature a distributed design, in which a communication layer enables kernels to exchange information and interoperate. Currently, this communication infrastructure is based on mailboxes, which enable fixed-size message exchanges with low latency. However, this solution is suboptimal because it can neither fully exploit the NoC nor efficiently handle the diversity of OS communication protocols. We propose an Inter-Kernel Communication (IKC) facility that exposes two kernel-level communication abstractions in addition to mailboxes: syncs, for enabling a process to signal and unlock another process remotely, and portals, for handling dense data transfers with high bandwidth. We implemented the proposed facility in Nanvix, the only open-source distributed OS that runs on a baremetal lightweight manycore, and we evaluated our solution on a 288-core processor (Kalray MPPA-256). Our results showed that our IKC facility achieves up to 16.87× and 1.68× better performance than a mailbox-only solution, in synchronization and dense data transfers, respectively. © 2021 Elsevier Inc.","Distributed operating system; Lightweight manycore processor; Message-passing communication; Network-on-chip"
"A review of edge computing: Features and resource virtualization","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099789900&doi=10.1016%2fj.jpdc.2020.12.015&partnerID=40&md5=adfdf71f665d7308187922dcad311752","With the advent of Internet of Things (IoT) connecting billions of mobile and stationary devices to serve real-time applications, cloud computing paradigms face some significant challenges such as high latency and jitter, non-supportive location-awareness and mobility, and non-adaptive communication types. To address these challenges, edge computing paradigms, namely Fog Computing (FC), Mobile Edge Computing (MEC) and Cloudlet, have emerged to shift the digital services from centralized cloud computing to computing at edges. In this article, we analyze cloud and edge computing paradigms from features and pillars perspectives to identify the key motivators of the transitions from one type of virtualized computing paradigm to another one. We then focus on computing and network virtualization techniques as the essence of all these paradigms, and delineate why virtualization features, resource richness and application requirements are the primary factors for the selection of virtualization types in IoT frameworks. Based on these features, we compare the state-of-the-art research studies in the IoT domain. We finally investigate the deployment of virtualized computing and networking resources from performance perspective in an edge-cloud environment, followed by mapping of the existing work to the provided taxonomy for this research domain. The lessons from the reviewed are that the selection of virtualization technique, placement and migration of virtualized resources rely on the requirements of IoT services (i.e., latency, scalability, mobility, multi-tenancy, privacy, and security). As a result, there is a need for prioritizing the requirements, integrating different virtualization techniques, and exploiting a hierarchical edge-cloud architecture. © 2020 Elsevier Inc.","Classical virtualization; Fog Computing; Mobile Edge Computing; Network function Virtualization (NFV)"
"A trustworthy industrial data management scheme based on redactable blockchain","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103236343&doi=10.1016%2fj.jpdc.2021.02.026&partnerID=40&md5=eecfbf14edcd7ff21dd31aece018fe89","Industrial data plays a key role in the industrial internet, and its secure collection problem has been highly valued by researchers. As Industrial Internet of Things (IIoT) devices are geographically dispersed and difficult to link, blockchain technology is usually introduced to solve the security management problem of industrial data. Unfortunately, the IIoT device is not stable, and it may leave incorrect messages in the blockchain, which will be permanently stored with potentially catastrophic consequences. As an effective solution, the redactable blockchain technology can allow people to modify the data on the blockchain. However, the existing redactable blockchain cannot guarantee industrial data security in the industrial internet due to requirements of trusted third parties, large overheads or lack of accountability mechanism. In this paper, we propose a trustworthy industrial data management scheme based on redactable blockchain in the industrial internet. To avoid additional burdens on industrial blockchain systems, a double-blockchain architecture is established to separate trapdoor management transactions. Distributed chameleon hash parameter generation and trapdoor recovery methods can avoid the security problems faced by the centralized organization. The fault-tolerant trapdoor recovery mechanism based on verifiable secret sharing technology as an alternative enhances the security of the system. The blockchain will record various information in the trapdoor management process and use it as evidence for accountability when disputes arise. The theoretical analysis and experiments show that the approach can effectively deal with malicious behaviors and has acceptable overhead. © 2021 Elsevier Inc.","Accountability; Industrial data management; Redactable blockchain; Security; Trapdoor management"
"Letting future programmers experience performance-related tasks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105709161&doi=10.1016%2fj.jpdc.2021.04.014&partnerID=40&md5=cd833357b1c5c126a0ec64eb6727b0ea","Programming courses usually focus on software-engineering problems like software decomposition and code maintenance. While computer-science lessons emphasize algorithm complexity, technological problems are usually neglected although they may significantly affect the performance in terms of wall time. As the technological problems are best explained by hands-on experience, we present a set of homework assignments focused on a range of technologies from instruction-level parallelism to GPU programming to cluster computing. These assignments are a product of a decade of development and testing on live subjects – the students of three performance-related software courses at the Faculty of Mathematics and Physics of the Charles University in Prague. © 2021 Elsevier Inc.","Assignments; Education; GPU; HPC; Parallel computing"
"Shortest-path routing for optimal all-to-all personalized-exchange embedding on hierarchical hypercube networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099698253&doi=10.1016%2fj.jpdc.2021.01.004&partnerID=40&md5=8f1bc1ebbd619afe04ddbcf53b230033","Hypercube (HC) networks (N=2n) provide efficient communication for parallel-and-distributed computing (PDC) but the HC-based multi-processor (MP) system is costly and not scalable, while hierarchical hypercube (HHC) networks (N=2n, n=2m+m) are less expensive and more scalable. However, the traditional HHC-routing easily conflicts, especially when executing multiple tasks (k > 2m nodes-per-task). In the past, the node-disjoint-path routing could be used to avoid the conflict but that reliable (S, D) routing limited S(source)=0. Later, the parallel N2N (node-to-node) disjoint-path was proposed for reliable routing but limited (m+1)/2 pairs-of-nodes. Therefore, this study proposes the generalized N2N shortest-path (SP) routing (with arbitrary S) and the parallel N2N SP-routing, based on our hypothesis “the shortest-path routing and the proper HHC-partitioning can avoid the HHC-conflict directly”. Next, our innovation and contribution are 1. the GCD (grouping of cross dual-cube) partitioning to solve the HHC-conflict for k≤2m+1 nodes-per-task, and 2. the SP-ATAPE (all-to-all personalized exchange) embedding on the HHC-MPs. The correctness of the SP-routing was proven and the ATAPE communication was experimented to validate our conflict solution. The ATAPE results confirmed that in any group the GCD mapping could make all tasks (k = 2m+1 nodes-per-task), synchronized with the same control, working without the conflict. © 2021 Elsevier Inc.","ATAPE (all-to-all personalized exchange) embedding; Gray-code mapping and forward/backward reordering; Grouping of cross dual-cube (GCD) partitioning; Hierarchical hypercube (HHC) networks; Parallel shortest-path routing"
"Parallel ensemble methods for causal direction inference","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099469632&doi=10.1016%2fj.jpdc.2020.12.012&partnerID=40&md5=16875a66be7f43adf1c2c9d5914df02a","Inferring the causal direction between two variables from their observation data is one of the most fundamental and challenging topics in data science. A causal direction inference algorithm maps the observation data into a binary value which represents either x causes y or y causes x. The nature of these algorithms makes the results unstable with the change of data points. Therefore the accuracy of the causal direction inference can be improved significantly by using parallel ensemble frameworks. In this paper, new causal direction inference algorithms based on several ways of parallel ensemble are proposed. Theoretical analyses on accuracy rates are given. Experiments are done on both of the artificial data sets and the real world data sets. The accuracy performances of the methods and their computational efficiencies in parallel computing environment are demonstrated. © 2020 Elsevier Inc.","Causal direction inference; Parallel ensemble; Unstable learner"
"Randomized renaming in shared memory systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099384761&doi=10.1016%2fj.jpdc.2021.01.002&partnerID=40&md5=5d065539e56fad9675e40828b1835646","Renaming is a task in distributed computing where n processes are assigned new names from a name space of size m. The problem is called tight if m=n, and loose if m>n. In recent years renaming came to the fore again and new algorithms were developed. For tight renaming in asynchronous shared memory systems, Alistarh et al. describe a construction based on the AKS network that assigns all names within O(logn) steps per process. They also show that, depending on the size of the name space, loose renaming can be done considerably faster. For m=(1+ϵ)⋅n and constant ϵ, they achieve a step complexity of O(loglogn). In this paper we consider tight as well as loose renaming and introduce randomized algorithms that achieve their tasks with high probability. The model assumed is the asynchronous shared-memory model against an adaptive adversary. Our algorithm for loose renaming maps n processes to a name space of size m=(1+2∕(logn)ℓ)⋅n=(1+o(1))⋅n performing O(ℓ⋅(loglogn)2) test-and-set operations. In the case of tight renaming, we present a protocol that assigns n processes to n names with step complexity O(logn), but without the overhead and impracticality of the AKS network. This algorithm utilizes modern hardware features in form of a counting device which is also described in the paper. This device may have the potential to speed up other distributed algorithms as well. © 2021 Elsevier Inc.","Distributed algorithm; Loose renaming; Randomized algorithm; Shared memory model; Tight renaming"
"Collaborative execution of fluid flow simulation using non-uniform decomposition on heterogeneous architectures","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102046247&doi=10.1016%2fj.jpdc.2021.02.006&partnerID=40&md5=0a0de403372648853e49e8825db45fed","The demand for computing power, along with the diversity of computational problems, culminated in a variety of heterogeneous architectures. Among them, hybrid architectures combine different specialized hardware into a single chip, comprising a System-on-Chip (SoC). Since these architectures usually have limited resources, efficiently splitting data and tasks between the different hardware is primal to improve performance. In this context, we explore the non-uniform decomposition of the data domain to improve fluid flow simulation performance on heterogeneous architectures. We evaluate two hybrid architectures: one comprised of a general-purpose x86 CPU and a graphics processing unit (GPU) integrated into a single chip (AMD Kaveri SoC), and another comprised by a general-purpose ARM CPU and a Field Programmable Gate Array (FPGA) integrated into the same chip (Intel Arria 10 SoC). We investigate the effects on performance and energy efficiency of data decomposition on each platform's devices on a collaborative execution. Our case study is the well-known Lattice Boltzmann Method (LBM), where we apply the technique and analyze the performance and energy efficiency of five kernels on both devices on each platform. Our experimental results show that non-uniform partitioning improves the performance of LBM kernels by up to 11.40% and 15.15% on AMD Kaveri and Intel Arria 10, respectively. While AMD's Kaveri platform's performance efficiency is of up to 10.809 MLUPS with an energy efficiency of 142.881 MLUPKJ, Intel's Arria 10 platform's is of up to 1.12 MLUPS and 82.272 MLUPKJ. © 2021 Elsevier Inc.","FPGA; GPU; Lattice Boltzmann Method; Non-uniform partitioning; System-on-Chip"
"A novel cooperative resource provisioning strategy for Multi-Cloud load balancing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102632141&doi=10.1016%2fj.jpdc.2021.02.003&partnerID=40&md5=87c56dcf275ac6ebf25339c240fd49cc","The paradigm of cloud computing has heralded a new avenue of computing, offering benefits of increased data accessibility with low cost. Continuous Writing Applications (CWA) (e.g., augmented online services for Health Care) have specific requirements on data storage, computation and bandwidth, thus are cost-sensitive with limited budgets and time. Herein, we propose an architecture of multi-cloud service provider (CSP) or “Multi-Cloud” to provide services to CWA, and design a novel resource scheduling algorithm to minimize the system cost. The system models of classic CWAs to tackle the resource requirements of users on MCP are exploited. The study can help to understand the characteristics of different resources and conclude Multi-Cloud being the most attractive to many CWA implementations. Interconnections of multiple CSPs and their load paths (i.e., data passing through possible interconnections) are introduced. We then formulate the problem and present optimal user scheduling based on Minimum First Derivative Length (MFDL) of system load paths. Theoretical analysis demonstrated that the solutions with minimized costs can be achieved by the proposed algorithm, termed “Optimal user Scheduling” for Multi-Cloud (OSMC). Through rigorous simulations regarding different influencing factors, the proposed strategy has proven to be scalable, flexible, and efficient in many practical scenarios. © 2021 Elsevier Inc.","Cloud computing; Continuous writing application; Cost model; Queueing theory; Resource provisioning"
"On the implementation of memory reclamation methods in a lock-free hash trie design","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105566632&doi=10.1016%2fj.jpdc.2021.04.007&partnerID=40&md5=478c7bf3609b39897c60f3912e0f68c3","Hash tries are a trie-based data structure with nearly ideal characteristics for the implementation of hash maps. Starting from a particular lock-free hash map data structure, named Lock-Free Hash Tries, we focus on solving the problem of memory reclamation without losing the lock-freedom property. To the best of our knowledge, outside garbage collected environments, there is no current implementation of hash maps that is able to reclaim memory in a lock-free manner. To achieve this goal, we propose an approach for memory reclamation specific to Lock-Free Hash Tries that explores the characteristics of its structure in order to achieve efficient memory reclamation with low and well-defined memory bounds. We present and discuss in detail the key algorithms required to easily reproduce our implementation by others. Experimental results show that our approach obtains better results when compared with other state-of-the-art memory reclamation methods and provides a competitive and scalable hash map implementation, if compared to lock-based implementations. © 2021 Elsevier Inc.","Hash maps; Hazard pointers; Lock-freedom; Memory reclamation"
"On the correctness and efficiency of a novel lock-free hash trie map design","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2021.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100043441&doi=10.1016%2fj.jpdc.2021.01.001&partnerID=40&md5=fe1a9193c34eda478b6bae820216f304","Hash tries are a trie-based data structure with nearly ideal characteristics for the implementation of hash maps. In this paper, we present a novel, simple and scalable hash trie map design that fully supports the concurrent search, insert and remove operations on hash maps. To the best of our knowledge, our proposal is the first that puts together the following characteristics: (i) be lock-free; (ii) use fixed size data structures; and (iii) maintain the access to all internal data structures as persistent memory references. Our design is modular enough to allow different types of configurations aimed for different performances in memory usage and execution time and can be easily implemented in any type of language, library or within other complex data structures. We discuss in detail the key algorithms required to easily reproduce our implementation by others and we present a proof of correctness showing that our proposal is linearizable and lock-free for the search, insert and remove operations. Experimental results show that our proposal is quite competitive when compared against other state-of-the-art proposals implemented in Java. © 2021 Elsevier Inc.","Concurrent data structures; Hash tries; Lock-freedom"
"A popularity-aware reconstruction technique in erasure-coded storage systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089849326&doi=10.1016%2fj.jpdc.2020.08.003&partnerID=40&md5=8c4ffcc9df4eb415b8965aeacbfc1637","In this study, we develop a novel data reconstruction technique for parallel storage systems housed in modern data centers. We advocate for erasure-coded data storage systems to archive warm data (a.k.a., unpopular data), which attract a limited number of accesses or updates. Different from hot or cold data, warm data have to be treated in a distinctive way to optimize system performance and storage-space utilization. We pay particular attention to efficient data reconstruction in which faulty data nodes are rebuilt while responding to I/O requests. To achieve this goal, we employ two machine-learning algorithms to offer online data reconstruction in erasure coded storage systems. Our data reconstruction technique is conducive to recovering faulty nodes while boosting read performance for requests accessing data residing on the faulty nodes. Our system is reliant on a clustering mechanism to group files into multiple clusters, in each of which files share similar features. Furthermore, we implement a prediction module where a list of future popular data is projected by keeping track of historical I/O accesses. This popular-data list, in turn, provides predictions on files that are likely to be accessed in the not-too-distant future. The prediction module is responsible for computing similarities among users, thereby setting up priority levels of data blocks to be reconstructed. We implement our data reconstruction scheme in an erasure-coded parallel storage system to recover files with a guidance from the popular-data list. Our experimental results confirm that our system speeds up the data recovery of parallel storage systems while maintaining a high data access performance for on-line users. © 2020 Elsevier Inc.","Clustering; Erasure-coded data storage; Online reconstruction; Popularity awareness; Warm data"
"Achieving efficient and Privacy-preserving energy trading based on blockchain and ABE in smart grid","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090584601&doi=10.1016%2fj.jpdc.2020.08.012&partnerID=40&md5=11a79ba28e81ee068ea2e2a667438830","With the advent of the Industry 4.0 era, the development of smart cities based on the Internet of Things (IoT) has reached a new level. As a key component of the Internet of Things (IoT), the security of wireless sensor networks (WSN) has received widespread attention. Among them, Energy Internet, as an important part to support the construction of smart cities, its security and reliability research is becoming more and more important. In the Energy Internet, distributed energy transaction model is a promising approach to replace the traditional centralized transaction model and has become the leading direction of development in energy trading. As the underlying support, blockchain technology is attracting more and more attention due to its advantages, i.e., integrity and non-repudiation. However, most blockchain-based trading models face the problem of privacy disclosure. In this paper, to solve this problem, Ciphertext-Policy Attribute-Based Encryption (CP-ABE) is introduced as the core algorithm to reconstruct the transaction model. Specifically, we build a general model for distributed transactions called PP-BCETS (Privacy-preserving Blockchain Energy Trading Scheme). It can achieve fine-grained access control through transaction arbitration in the ciphertext form. This design can maximize the protection of privacy information, and considerably improve the security and reliability of the transaction model. Additionally, a credibility-based equity proof consensus mechanism is proposed in PP-BCETS, which can greatly increase the operation efficiency. The security analysis and experimental evaluations are conducted to prove the validity and practicability of our proposed scheme. © 2020 Elsevier Inc.","Access control; Blockchain; CP-ABE; Energy trading; Privacy protection"
"Interlaced: Fully decentralized churn stabilization for Skip Graph-based DHTs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096702476&doi=10.1016%2fj.jpdc.2020.10.008&partnerID=40&md5=0b488c64f72c4a03972cb38b62ed4e2b","As a distributed hash table (DHT) routing overlay, Skip Graph is used in a variety of peer-to-peer (P2P) systems including cloud storage. The overlay connectivity of P2P systems is negatively affected by the arrivals and departures of nodes to and from the system that is known as churn. Preserving connectivity of the overlay network (i.e., the reachability of every pair of nodes) under churn without compromising the overlay latency is a performance challenge in every P2P system including the Skip Graph-based ones. The existing decentralized churn stabilization solutions that are applicable to Skip Graphs mainly optimize the connectivity of the system under churn and do not consider routing latency of overlay as an optimization goal. Additionally, those existing solutions change the message complexity of Skip Graphs, distort its topology, or apply constant message overhead to the system. In this paper, we propose Interlaced, a fully decentralized churn stabilization mechanism for Skip Graphs that provides drastically stronger overlay connectivity and faster search queries without changing the asymptotic complexity of the Skip Graph in terms of storage, computation, and communication. We also propose the Sliding Window De Bruijn Graph (SWDBG ) as a tool to predict the availability of nodes with high accuracy. Our simulation results show that in comparison to the best existing DHT-based solutions, Interlaced improves the overlay connectivity of the Skip Graph under churn with the gain of about 1.73 times. Likewise, compared to the existing availability prediction approaches for P2P systems, SWDBG is about 1.26 times more accurate. A Skip Graph that benefits from Interlaced and SWDBG is about 2.47 times faster on average in routing the queries under churn compared to the best existing solutions. We also present an adaptive extension of Interlaced to be applied to other DHTs, for example, Kademlia. © 2020 Elsevier Inc.","Availability prediction; Churn stabilization; DHT; Highly-connected; Low-latency; Skip Graph"
"Schedulability analysis of Time-Sensitive Networks with scheduled traffic and preemption support","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086393651&doi=10.1016%2fj.jpdc.2020.06.001&partnerID=40&md5=38d2a84a97aed24d7d897f425c37d35a","The Time-Sensitive Networking (TSN) set of standards introduces in IEEE 802.1 switches and end stations novel features to meet the requirements of a broad spectrum of applications that are characterized by time-sensitive and mission-critical traffic flows. In particular, the IEEE802.1Qbv-2015 amendment introduces enhancements that provide temporal isolation for scheduled traffic, i.e., a traffic class that requires transmission based on a known timescale, while the IEEE802.1Qbu-2016 introduces preemption as a mechanism to allow time-critical messages to interrupt ongoing non time-critical transmissions. Both amendments, that are now enrolled in the IEEE802.1Q-2018 standard, are very important for industrial networks, where scheduled traffic and low-latency real-time flows have to coexist, on the same network, with best-effort transmissions. In this context, this work presents a response time analysis of TSN networks that encompasses the enhancements for scheduled traffic and preemption, in various combinations. The paper presents the proposed analysis and a performance comparison between the response times calculated by the analysis and the response times obtained through OMNeT++ simulations in three different scenarios. © 2020 Elsevier Inc.","Response time analysis; Schedulability analysis; Scheduled traffic; Simulation; Time-sensitive networking"
"Time–energy trade-offs in processing divisible loads on heterogeneous hierarchical memory systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086361381&doi=10.1016%2fj.jpdc.2020.05.015&partnerID=40&md5=fd9520028d747be9ce725a14e5373679","We analyze time and energy performance of distributed computations in heterogeneous systems with hierarchical memory. Different levels of memory hierarchy have different time and energy efficiency. Core memory may be too small to hold whole load to be processed, while computations using external storage are expensive in time and energy. In order to avoid the costs of processing the load in the external memory, it is allowed that the load is distributed to the worker processors in multiple installments. A minimum energy solution is found by use of mixed integer linear programming under a limit on schedule length. Two types of fast heuristics with several variants are also examined. The trade-off between the criteria of processing time and energy is studied. Key features of optimum solutions are analyzed. It is shown that holding machines in a diverse set of energy modes and limited use of the out-of-core memory can be beneficial for the time and energy performance. The proposed scheduling algorithms are evaluated in the terms of solution quality and runtimes. © 2020 The Author(s)","Divisible load theory; Energy-aware systems; Performance modeling and prediction; Storage hierarchies; Time–energy trade-off"
"Self-stabilizing token distribution on trees with constant space","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090424493&doi=10.1016%2fj.jpdc.2020.07.007&partnerID=40&md5=270b53cdd53e88cb0b62a320ee63122f","Self-stabilizing and silent distributed algorithms for token distribution in rooted tree networks are given. Initially, each process of a graph holds at most ℓ tokens. Our goal is to distribute the tokens uniformly in the whole network so that every process holds exactly k tokens. In the initial configuration, the total number of tokens in the network may not be nk where n is the number of processes in the network. The root process is given the ability to create a new token or remove a token from the network. We aim to minimize the convergence time, the number of token moves, and the space complexity. First, a self-stabilizing token distribution algorithm that converges within O(nℓ) asynchronous rounds and needs Θ(nhϵ) redundant (or unnecessary) token moves is given, where ϵ=min(k,ℓ−k) and h is the height of the tree network. Next, two novel mechanisms to reduce the number of redundant token moves are presented. One reduces the number of redundant token moves to O(nh) without any additional costs while the other reduces the number of redundant token moves to O(n), but increases the convergence time to O(nhℓ). All given algorithms have constant memory at each process and each link register. © 2020 The Author(s)","Constant space algorithm; Self-stabilization; Token distribution"
"Trustzone-based secure lightweight wallet for hyperledger fabric","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097126615&doi=10.1016%2fj.jpdc.2020.11.001&partnerID=40&md5=c5b8a8a698f3abfff0296172acad7875","With the development of blockchain-based digital currencies, the security of digital wallets becomes more and more important. As far as we know, there is no safe lightweight wallet in hyperledger fabric. To solve the problem, we proposed a Trustzone-based Secure Lightweight Wallet for Hyperledger Fabric (hereafter referred to as TSLWHF). Firstly, we designed an Unspent Transaction Output (UTXO) set of transactions under blockchain and a signature verification mechanism for transactions, which made it possible to implement the lightweight wallet in hyperledger fabric. Then, we implemented a reliable protection mechanism for private keys and wallet's address, which solved the problem that users’ information might be stolen or replaced. Meanwhile, the transaction verification results are guaranteed not to be tampered by hackers through verifying transactions in Trusted Execution Environment (TEE) and encrypting local block headers. Finally, to demonstrate utility, we deployed the system in hyperledger fabric and trustzone. Experiments showed that the wallet reduces the size of locally stored data while protecting the security of user's assets. The time spent on TSLWHF to execute a transaction is 0.589s, which improves transaction's performance compared to Bitcoin wallet. © 2020 Elsevier Inc.","Hyperledger fabric; Lightweight wallet; Simple payment verification; Trustzone"
"GPU acceleration of ADMM for large-scale quadratic programming","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086074713&doi=10.1016%2fj.jpdc.2020.05.021&partnerID=40&md5=273fd732012e3d76c0be1843aa2fc330","The alternating direction method of multipliers (ADMM) is a powerful operator splitting technique for solving structured convex optimization problems. Due to its relatively low per-iteration computational cost and ability to exploit sparsity in the problem data, it is particularly suitable for large-scale optimization. However, the method may still take prohibitively long to compute solutions to very large problem instances. Although ADMM is known to be parallelizable, this feature is rarely exploited in real implementations. In this paper we exploit the parallel computing architecture of a graphics processing unit (GPU) to accelerate ADMM. We build our solver on top of OSQP, a state-of-the-art implementation of ADMM for quadratic programming. Our open-source CUDA C implementation has been tested on many large-scale problems and was shown to be up to two orders of magnitude faster than the CPU implementation. © 2020 Elsevier Inc.","Alternating direction method of multipliers; GPU computing; Graphics processing unit; Quadratic programming"
"An efficient parallel direction-based clustering algorithm","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086985555&doi=10.1016%2fj.jpdc.2020.06.002&partnerID=40&md5=b4060f1c5fc0ac0b94aeee23da4a65cc","Clustering, which explores the visualization and distribution of data, has recently been studied widely. Although the existing clustering algorithms can well detect arbitrary shape clusters, most of them face the limitation that they cluster points on the basis of two physical metrics, distance and density, but ignore the orientation relationship of data distribution. Beside, they have a difficulty of selecting suitable parameters, which are important inputs of the clustering algorithms. In this paper, we firstly introduce a new physical metric, namely direction. Then, based on this new metric, we propose an adaptive direction-based clustering algorithm, namely ADC, which can automatically calculate appropriate parameters. Finally, we develop a parallel ADC algorithm based on multi-processors to improve the performance of the ADC algorithm. Compared with other clustering algorithms, experimental results demonstrate that the proposed algorithms are more general and can get much better clustering results. In addition, the parallel ADC algorithm has the best scalability over large data sets. © 2020 Elsevier Inc.","Adaptive; Clustering; Direction-based; Parallel"
"Optimal node-disjoint paths in folded hypercubes","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090889598&doi=10.1016%2fj.jpdc.2020.09.005&partnerID=40&md5=b2bff552e3a79285dd0a1fded369dec5","The constructions of node-disjoint paths have been well applied to the study of connectivity, diameter, parallel routing, reliability, and fault tolerance of an interconnection network. In order to minimize the transmission cost and latency, the total length and maximal length of the node-disjoint paths should be minimized, respectively. The construction of node-disjoint paths with their maximal length minimized (in the worst case) has been studied previously in folded hypercubes. In this paper, we construct m node-disjoint paths from one source node to other m (not necessarily distinct) target nodes, respectively, in an n-dimensional folded hypercube so that both of their total length and maximal length (in the worst case) are minimized, where m≤n+1. In addition, each path is either shortest or nearly shortest. The construction of these node-disjoint paths can be efficiently carried out in O(mn1.5+m3n) and O(mn2+ n2logn+m3n) time, respectively, for odd and even n by taking advantage of two specific routing functions, which provide another strong evidence for the effective applications of routing functions in deriving node-disjoint paths, especially for the variants of hypercubes. © 2020 Elsevier Inc.","Folded hypercube; Hypercube; Matching; Node-disjoint paths; Optimization"
"Sequential and parallel algorithms for all-pair k-mismatch maximal common substrings","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086440175&doi=10.1016%2fj.jpdc.2020.05.018&partnerID=40&md5=c45678e3fb7d00c82e5340440f61d560","Identifying long pairwise maximal common substrings among a large set of sequences is a frequently used construct in computational biology, with applications in DNA sequence clustering and assembly. Due to errors made by sequencers, algorithms that can accommodate a small number of differences are of particular interest. Formally, let D be a collection of n sequences of total length N, ϕ be a length threshold, and k be a mismatch threshold. The goal is to identify and report all k-mismatch maximal common substrings of length at least ϕ over all pairs of strings in D. Heuristics based on seed-and-extend style filtering techniques are often employed in such applications. However, such methods cannot provide any provably efficient run time guarantees. To this end, we present a sequential algorithm with an expected run time of O(NlogkN+occ), where occ is the output size. We then present a distributed memory parallel algorithm with an expected run time of [Formula presented] using Ologk+1N expected rounds of global communications, under some realistic assumptions, where p is the number of processors. Finally, we demonstrate the performance and scalability of our algorithms using experiments on large high throughput sequencing data. © 2020 Elsevier Inc.","Approximate sequence matching; Hamming distance; Parallel algorithms; String algorithms; Suffix trees"
"Fast shared-memory streaming multilevel graph partitioning","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091639739&doi=10.1016%2fj.jpdc.2020.09.004&partnerID=40&md5=1520ae78b7c78d4de20316b71a442158","A fast parallel graph partitioner can benefit many applications by reducing data transfers. The online methods for partitioning graphs have to be fast and they often rely on simple one-pass streaming algorithms, while the offline methods for partitioning graphs contain more involved algorithms and the most successful methods in this category belong to the multilevel approaches. In this work, we assess the feasibility of using streaming graph partitioning algorithms within the multilevel framework. Our end goal is to come up with a fast parallel offline multilevel partitioner that can produce competitive cutsize quality. We rely on a simple but fast and flexible streaming algorithm throughout the entire multilevel framework. This streaming algorithm serves multiple purposes in the partitioning process: a clustering algorithm in the coarsening, an effective algorithm for the initial partitioning, and a fast refinement algorithm in the uncoarsening. Its simple nature also lends itself easily for parallelization. The experiments on various graphs show that our approach is on the average up to 5.1x faster than the multi-threaded MeTiS, which comes at the expense of only 2x worse cutsize. © 2020","Graph partitioning; Multilevel graph partitioning; Parallel graph partitioning; Streaming algorithms"
"Intelligently modeling, detecting, and scheduling elephant flows in software defined energy cloud: A survey","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089484705&doi=10.1016%2fj.jpdc.2020.07.008&partnerID=40&md5=a6b9b44bfddd97aac5396d847238e734","Elephant flows (elephants) refer to the sequences of packets that contribute only 10% of the total volume but consume over 90% of the network bandwidth. They often cause network congestion and should be efficiently managed. Present cloud data centers often involve host- and switch-based approaches to detect and schedule elephants, but suffer (1) each host and switch in the network needs to be customized, and (2) dynamic models and advanced policies are difficult to be applied. Software Defined Cloud (SDC) addresses these issues by enabling controller-based approaches. With the aid of Machine Learning (ML) technologies, SDC can achieve learning-based models, flexible deployment, and early detection and schedule of elephants for the optimization of network performance and energy usage in a dynamic and intelligent manner. On this purpose, this article emphases the significance of models describing elephants, surveys the mechanisms that may apply to model, detect, and schedule elephants for SDC to optimize the network performance and energy usage. To the best of our knowledge, this work is the first effort that reviews the techniques in all these related subtopics simultaneously in the context of energy cloud. © 2020 Elsevier Inc.","Detection; Elephant flows; Modeling; Scheduling; Software defined energy cloud"
"Accurate, efficient and scalable training of Graph Neural Networks","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091779365&doi=10.1016%2fj.jpdc.2020.08.011&partnerID=40&md5=3185fb4a5eb00611f0ceee81a6faac69","Graph Neural Networks (GNNs) are powerful deep learning models to generate node embeddings on graphs. When applying deep GNNs on large graphs, it is still challenging to perform training in an efficient and scalable way. We propose a novel parallel training framework. Through sampling small subgraphs as minibatches, we reduce training workload by orders of magnitude compared with state-of-the-art minibatch methods. We then parallelize the key computation steps on tightly-coupled shared memory systems. For graph sampling, we exploit parallelism within and across sampler instances, and propose an efficient data structure supporting concurrent accesses from samplers. The parallel sampler theoretically achieves near-linear speedup with respect to number of processing units. For feature propagation within subgraphs, we improve cache utilization and reduce DRAM traffic by data partitioning. Our partitioning is a 2-approximation strategy for minimizing the communication cost compared to the optimal. We further develop a runtime scheduler to reorder the training operations and adjust the minibatch subgraphs to improve parallel performance. Finally, we generalize the above parallelization strategies to support multiple types of GNN models and graph samplers. The proposed training outperforms the state-of-the-art in scalability, efficiency and accuracy simultaneously. On a 40-core Xeon platform, we achieve 60× speedup (with AVX) in the sampling step and 20× speedup in the feature propagation step, compared to the serial implementation. Our algorithm enables fast training of deeper GNNs, as demonstrated by orders of magnitude speedup compared to the Tensorflow implementation. We open-source our code at https://github.com/GraphSAINT/GraphSAINT © 2020 Elsevier Inc.","Graph Neural Networks; Graph partitioning; Graph representation learning; Graph sampling; Memory optimization"
"Rule-based knowledge discovery of satellite imagery using evolutionary classification tree","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091247613&doi=10.1016%2fj.jpdc.2020.09.003&partnerID=40&md5=2efa71219b3544fa5e59ccc830b72789","The classification tree (CT) may be used to establish explicit classification rules for Satellite Imagery (SI). However, the accuracy of explicit classification rules attained by this method is poor. Back-propagation networks (BPN) and the support vector machine (SVM) may both be used to establish highly accurate models for predicting the classification of SI. However, neither is able to generate explicit rules. This study proposes the evolutionary classification tree (ECT) as a novel mining rule method. Composed of the particle bee algorithm (PBA) and classification tree (CT), the ECT produces self-organized rules automatically to predict the classification of SI. In ECT, CT serves as the architecture to represent explicit rules and PBA acts as the optimization mechanism to optimize CT in order to fit the experimental data. A total of 600 experimental datasets were used to compare the accuracy and complexity of four model-building techniques: CT, BPN, SVM, and ECT. The results demonstrate the ability of ECT to produce rules that are more accurate than CT and SVM but less accurate than BPN. However, because BPN is black box model, the ability of ECT to generate explicit rules makes ECT the best model for users wanting to mine the explicit rules and knowledge in practical applications. © 2020","Back-propagation networks (BPN); Evolutionary classification tree (ECT); Particle bee algorithm (PBA); Satellite Imagery (SI); Support vector machine (SVM)"
"Optimal task scheduling benefits from a duplicate-free state-space","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089838951&doi=10.1016%2fj.jpdc.2020.07.005&partnerID=40&md5=7b5cff6064b18f1325e2283af1d0c0b2","The NP-hard problem of task scheduling with communication delays (P|prec,cij|Cmax) is often tackled using approximate methods, but guarantees on the quality of these heuristic solutions are hard to come by. Optimal schedules are therefore invaluable for properly evaluating these heuristics, as well as being very useful for applications in time critical systems. Optimal solving using branch-and-bound algorithms like A* has been shown to be promising in the past, with a state-space model we refer to as exhaustive list scheduling (ELS). The obvious weakness of this model is that it leads to the production of large numbers of duplicate states during a search, requiring special techniques to mitigate this which cost additional time and memory. In this paper we define a new state-space model (AO) in which we divide the problem into two distinct sub-problems: first we decide the allocations of all tasks to processors, and then we order the tasks on their allocated processors in order to produce a complete schedule. This two-phase state-space model offers no potential for the production of duplicates. We also describe how the pruning techniques and optimisations developed for the ELS model were adapted or made obsolete by the AO model. An experimental evaluation shows that the use of this new state-space model leads to a significant increase in the number of task graphs able to be scheduled within a feasible time-frame, particularly for task graphs with a high communication-to-computation ratio. Finally, some advanced lower bound heuristics are proposed for the AO model, and evaluation demonstrates that significant gains can be achieved from the consideration of necessary idle time. © 2020 Elsevier Inc.","Branch-and-bound; Discrete optimisation; Pruning; State-space search; Task scheduling"
"Distributed load balancing frequent colossal closed itemset mining algorithm for high dimensional dataset","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086381261&doi=10.1016%2fj.jpdc.2020.05.017&partnerID=40&md5=a53d58fba40613d97346d29fff573f37","The focus of extracting colossal closed itemsets from high dimensional biological datasets has been great in recent times. A massive set of short and average sized mined itemsets do not confine complete and valuable information for decision making. But, the traditional itemset mining algorithms expend a gigantic measure of time in mining a massive set of short and average sized itemsets. The greater interest of research in the field of bioinformatics and the abundant data across the variety of domains paved the way for the generation of the high dimensional dataset. These datasets are depicted by an extensive number of features and a smaller number of rows. Colossal closed itemsets are very significant for numerous applications including the field of bioinformatics and are influential during the decision making. Extracting a huge amount of information and knowledge from the high dimensional dataset is a nontrivial task. The existing colossal closed itemsets mining algorithms for the high dimensional dataset are sequential and computationally expensive. Distributed and parallel computing is a good strategy to overcome the inefficiency of the existing sequential algorithm. Balanced Distributed Parallel Frequent Colossal Closed Itemset Mining (BDPFCCIM) algorithm is designed for high dimensional datasets. An efficient closeness checking method to check the closeness of the rowset and an efficient pruning strategy to snip the row enumeration mining search space is enclosed with the proposed BDPFCCIM algorithm. The proposed BDPFCCIM algorithm is the first distributed load balancing algorithm to mine frequent colossal closed itemsets from high dimensional biological datasets. The experimental results demonstrate the efficient performance of the proposed BDPFCCIM algorithm in comparison with the state-of-the-art algorithms. © 2020 Elsevier Inc.","Bioinformatics; Closeness checking; Distributed and parallel computing; High dimensional datasets; Load balancing"
"Short and long term optimization for micro-object conveying with air-jet modular distributed system","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086427869&doi=10.1016%2fj.jpdc.2020.05.016&partnerID=40&md5=2878393a046a44cfb248f896e13e69af","Smart surface is a new conveying technology composed of a 2D planar surface presenting a matrix of distributed autonomous blocks. Every block contains a micro-electro-mechanical system (MEMS) actuator that controls the transfer of a possible object located above the block to the neighboring blocks, using air-jet forces. The spatial characteristics of the blocks impose some limits on the memory, energy and computation capabilities of the MEMS blocks. On the other hand, the system can reach several thousands of blocks making necessary to propose scalable algorithmic solutions. This paper studies different distributed algorithms to convey an object from an initial to a target position in the smart surface. The conveying policy emphasizes the long term use of the smart surface and the objects conveying efficiency measured by the time of the transfer. The problem stands as an original case of multi-objective Shortest Path problem (MOSP). Original because the quality of a given path is not evaluated by the sum of the weights of its segments, and because the segment weights change according to the used paths as provided by the algorithm itself. Therefore, the efficiency of a given algorithm is assessed on the basis of its performance during a long period of time. We describe here the best way to combine these two objectives and we propose a scalable incremental distributed protocol for objects conveying. The path optimality is adjusted according to the required calculation complexity. The performances of the different algorithmic and modeling variations are analyzed in terms of memory, time, computation and exchanged messages complexity. The obtained results prove the scalability of the algorithm, with linear computational, memory and convergence time complexity, and confirm the improvement of smart surface usage compared to a naive approach. The system lifespan increases of up to 130% on 40 × 40 smart surface, while the transfer cost (time and energy) is reduced. We show also that the computation time of the path with the incremental algorithm can be significantly reduced without significant degradation of the conveying system performance. For example, in a 40 × 40 smart surface, the number of messages is divided by 4 while the number of conveyed objects is only reduced by a ratio of 4%. © 2020 Elsevier Inc.","Distributed conveying system; Incremental computing; Multicriteria optimization; Shortest path problem"
"Towards cost-effective service migration in mobile edge: A Q-learning approach","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090113588&doi=10.1016%2fj.jpdc.2020.08.008&partnerID=40&md5=c792bdaa05c419c45a21c7239c897994","Service migration in mobile edge computing is a promising approach to improving the quality of service (QoS) for mobile users and reducing the network operational cost for service providers as well. However, these benefits are not free, coming at costs of bulk-data transfer, and likely service disruption, which could consequently increase the overall service costs. To gain the benefits of service migration while minimizing its cost across the edge nodes, in this paper, we leverage reinforcement learning (RL) method to design a cost-effective framework, called Mig-RL, for the service migration with a reduction of total service costs as a goal in a mobile edge environment. The Mig-RL leverages the infrastructure of edge network and deploys a migration agent through Q-learning to learn the optimal policy with respect to the service migration status. We distinguish the Mig-RL from other existing works in several major aspects. First, we fully exploit the nature of this problem in a modest migration space, which allows us to constrain the number of service replicas whereby a defined state–action space could be effectively handled, as opposed to those methods that need to always approximate a huge state–action space for policy optimality. Second, we advocate a migration policy-base as a cache to save the learning process by retrieving the most effective policy whenever a similar migration pattern is encountered as time goes on. Finally, by exploiting the idea of software defined network, we also investigate the efficient implementation of Mig-RL in mobile edge network. Experimental results based on some real and synthesized access sequences show that Mig-RL, compared with the selected existing algorithms, can substantially minimize the service costs, and in the meantime, efficiently improve the QoS by adapting to the changes of mobile access patterns. © 2020 Elsevier Inc.","Dynamic service migration; Mobile edge computing; Q-learning; Reinforcement learning; Software-defined networking"
"Securing transmissions by friendly jamming scheme in wireless networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086641820&doi=10.1016%2fj.jpdc.2020.04.013&partnerID=40&md5=692275f83f1de82aee881298c0ab8fd2","In this paper, we focus on the design of optimal relay and jammer selection strategy in relay-aided wireless networks. Different from previous works, assuming that the channel state information (CSI) of illegitimate nodes was available and only an eavesdropper existed, we first analyze disadvantages of joint relay and jammer selection (JRJS), average optimal relay selection (AORS), traditional maximum relay selection (TMRS) schemes. Then, we design an optimal relay and jammer selection strategy where the ratio of received SNRs at the destination generated by any two relays is maximized. By applying proposed strategy, computation complexity can be reduced. Moreover, we derive the lower and upper bounds of the secrecy outage probability based on the assumptions of existence of only illegitimate node and symmetric case for mathematical convenience. Finally, simulation shows that the proposed strategy operating with no CSI of illegitimate nodes can work efficiently compared with JRJS, TMRS and AORS strategies. © 2020 The Author(s)","Computation complexity; CSI of illegitimate nodes; Physical layer security; Secrecy outage probability"
"Towards blockchain-enabled single character frequency-based exclusive signature matching in IoT-assisted smart cities","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086734750&doi=10.1016%2fj.jpdc.2020.05.013&partnerID=40&md5=8ed7a5c9c312543b247a7ce71019e953","With the increasing viability of Internet of Things (IoT), more devices are expected to be connected in a smart city environment. It can provide many benefits for people's daily life, but is also susceptible to many security threats in practice. Intrusion detection systems (IDSs), especially signature-based IDSs, are one of the most commonly adopted security mechanisms to safeguard various network environments like IoT-assisted smart city against cyber attacks. The process of signature matching is a key limiting factor for a signature-based IDS, and the exclusive signature matching (ESM) was designed based on the observation that most network packets would not match any IDS signatures. However, exclusive signature matching like the single character frequency-based ESM may be vulnerable to some attacks in a hostile environment. To mitigate this issue, in this work, we propose a blockchain-enabled single character frequency-based ESM, which can build a verifiable database of malicious payloads via blockchains. In the evaluation, we investigate the performance of our approach under flooding and character padding attacks in both a simulated and a real IoT network environment. The results demonstrate the effectiveness of our approach in enhancing the robustness of single character frequency-based ESM against malicious traffic. © 2020 Elsevier Inc.","Blockchain; Exclusive signature matching; Internet of Things; Intrusion detection; Smart city"
"EdgeKV: Decentralized, scalable, and consistent storage for the edge","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085997438&doi=10.1016%2fj.jpdc.2020.05.009&partnerID=40&md5=55c869405a9f7cf0c11ff4b4c0b717d6","Edge computing moves the computation closer to the data and the data closer to the user to overcome the high latency communication of cloud computing. Storage at the edge allows data access with high speeds that enable latency-sensitive applications in areas such as autonomous driving and smart grid. However, several distributed services are typically designed for the cloud and building an efficient edge-enabled storage system is challenging because of the distributed and heterogeneous nature of the edge and its limited resources. In this paper, we propose EdgeKV, a decentralized storage system designed for the network edge. EdgeKV offers fast and reliable storage, utilizing data replication with strong consistency guarantees. With a location-transparent and interface-based design, EdgeKV can scale with a heterogeneous system of edge nodes. We implement a prototype of the EdgeKV modules in Golang and evaluate it in both the edge and cloud settings on the Grid’5000 testbed. We utilize the Yahoo! Cloud Serving Benchmark (YCSB) to analyze the system's performance under realistic workloads. Our evaluation results show that EdgeKV outperforms the cloud storage setting with both local and global data access with an average write response time and throughput improvements of 26% and 19% respectively under the same settings. Our evaluations also show that EdgeKV can scale with the number of clients, without sacrificing performance. Finally, we discuss the energy efficiency improvement when utilizing edge resources with EdgeKV instead of a centralized cloud. © 2020 Elsevier Inc.","Consistency; DHT; Distributed systems; Edge computing; Key–value store"
"Towards an efficient combination of adaptive routing and queuing schemes in Fat-Tree topologies","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090593828&doi=10.1016%2fj.jpdc.2020.07.009&partnerID=40&md5=b912f45fec3e05ba516f5138b29297b9","The interconnection network is a key element in High-Performance Computing (HPC) and Datacenter (DC) systems whose performance depends on several design parameters, such as the topology, the switch architecture, and the routing algorithm. Among the most common topologies in HPC systems, the Fat-Tree offers several shortest-path routes between any pair of end-nodes, which allows multi-path routing schemes to balance traffic flows among the available links, thus reducing congestion probability. However, traffic balance cannot solve by itself some congestion situations that may still degrade network performance. Another approach to reduce congestion is queue-based flow separation, but our previous work shows that multi-path routing may spread congested flows across several queues, thus being counterproductive. In this paper, we propose a set of restrictions to improve alternative routes selection for multi-path routing algorithms in Fat-Tree networks, so that they can be positively combined with queuing schemes. © 2020 Elsevier Inc.","Adaptive routing; Congestion management; Fat-Trees; High-performance interconnection networks; Queuing schemes"
"I/O characteristic discovery for storage system optimizations","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092747349&doi=10.1016%2fj.jpdc.2020.08.005&partnerID=40&md5=4396514b35366e470e4c4da07928531a","In this paper, we introduce a new I/O characteristic discovery methodology for performance optimizations on object-based storage systems. Different from traditional methods that select limited access attributes or heavily reply on domain knowledge about applications’ I/O behaviors, our method enables capturing data-access features as many as possible to eliminate human bias. It utilizes a machine-learning based strategy (principal component analysis, PCA) to derive the most important set of features automatically, and groups data objects with a clustering algorithm (DBSCAN) to reveal I/O characteristics discovered. We have evaluated the proposed I/O characteristic discovery solution based on Sheepdog storage system and further implemented a data prefetching mechanism as a sample use case of this approach. Evaluation results confirm that the proposed solution can successfully identify access patterns and achieve efficient data prefetching by improving the buffer cache hit ratio up to 48.24%. The overall performance was improved by up to 42%. © 2020 Elsevier Inc.","Access pattern analysis; I/O characteristic discovery; I/O optimization; Object-based storage; Parallel/distributed file systems"
"High performance GPU primitives for graph-tensor learning operations","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096655609&doi=10.1016%2fj.jpdc.2020.10.011&partnerID=40&md5=467e0e27bf88d13a0830ad8eaa448176","Graph-tensor learning operations extend tensor operations by taking the graph structure into account, which have been applied to diverse domains such as image processing and machine learning. However, the running time of graph-tensor operations increases rapidly with the number of nodes and the dimension of data on nodes, making them impractical for real-time applications. In this paper, we propose a GPU library called cuGraph-Tensor for high-performance graph-tensor learning operations, which consists of eight key operations: graph shift (g-shift), graph Fourier transform (g-FT), inverse graph Fourier transform (inverse g-FT), graph filter (g-filter), graph convolution (g-convolution), graph-tensor product (g-product), graph-tensor SVD (g-SVD) and graph-tensor QR (g-QR). cuGraph-Tensor supports scalar, vector, and matrix data processing on each graph node. We propose optimization techniques on computing, memory accesses, and CPU–GPU communications that significantly improve the performance of the graph-tensor learning operations. Using the optimized operations, cuGraph-Tensor builds a graph data completion application for fast and accurate reconstruction of incomplete graph data. In the experiments, the proposed graph learning operations achieve up to 142.12× speedups versus CPU-based GSPBOX and CPU MATLAB implementations running on two Xeon CPUs. The graph data completion application achieves up to 174.38× speedups over the CPU MATLAB implementation, and up to 3.82× speedups with better accuracy over the GPU-based tensor completion in the cuTensor-tubal library. © 2020 Elsevier Inc.","GPU; Graph data processing; Graph operations; Graph-tensor; Library"
"Empowering mobile crowdsourcing apps with user privacy control","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090196774&doi=10.1016%2fj.jpdc.2020.07.011&partnerID=40&md5=80dbbcb873c6218bfb8dc3fe881aa804","Mobile crowdsourcing is being increasingly used by industrial and research communities to build realistic datasets. By leveraging the capabilities of mobile devices, mobile crowdsourcing apps can be used to track participants’ activity and to collect insightful reports from the environment (e.g., air quality, network quality). However, most of existing crowdsourced datasets systematically tag data samples with metadata (e.g., time and location stamps), which may inevitably lead to user privacy leaks by discarding sensitive information in the wild. This article addresses this critical limitation of the state of the art by proposing a software library that empowers legacy mobile crowdsourcing apps to increase user privacy without compromising the overall quality of the crowdsourced datasets. We propose a decentralized approach, named FOUGERE, to convey data samples from user devices to third-party servers. By introducing an a priori data anonymization process, we show that FOUGERE defeats state-of-the-art location-based privacy attacks with little impact on the quality of crowdsourced datasets. © 2020 Elsevier Inc.","Distributed applications; Location privacy; Location privacy protection mechanism; Mobile crowdsourcing"
"Reliability analysis of the augmented cubes in terms of the extra edge-connectivity and the component edge-connectivity","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091019934&doi=10.1016%2fj.jpdc.2020.08.009&partnerID=40&md5=19aba922ad28b13801af5fa6ed17da7d","Reliability evaluation of interconnection networks is of significant importance to the design and maintenance of interconnection networks. The extra edge-connectivity and component edge-connectivity are two important parameters for the reliability evaluation of interconnection networks. In this paper, we determine the h-extra edge-connectivity of an n-dimensional augmented cube for [Formula presented], n≥2 and [Formula presented] where n≥4, f=0 when n is even, and f=1 when n is odd. Moreover, we also determine the r-component edge-connectivity of an n-dimensional augmented cube for [Formula presented], n≥7. © 2020 Elsevier Inc.","Augmented cubes; Component edge-connectivity; Extra edge-connectivity; Reliability"
"Blockchain-assisted access for federated Smart Grid domains: Coupling and features","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086454904&doi=10.1016%2fj.jpdc.2020.05.012&partnerID=40&md5=b95c33e66e00ae9e1658b767ac997272","Industry 4.0 technological expansion and the multiple accesses to the diverse Smart Grid domains (power networks, control systems, market, customer premises) entail the need to provide efficient interconnection mechanisms with connection from anywhere, at any time and in anyhow. However, this type of requirement should not only consist in imposing interoperability solutions between entities and domains, but also in searching the way to justify and trace connections (how, when, where, who) for future governance or auditing actions. This paper, therefore, presents a three layer-based interconnection architecture and several interconnection strategies, all of them adapting the traditional policy decision and enforcement approaches together with the blockchain technology to manage reliable and secure connections among entities, processes and critical resources. With this architecture in mind, the paper also analyzes the coupling level of the blockchain technology, and explores which interconnection strategy is more suitable for Smart Grid domains and their control systems. © 2020 Elsevier Inc.","Access control; Blockchain; Industrial Internet of Things; Smart Grid; Technological coupling"
"Joint coflow routing and scheduling in leaf-spine data centers","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095966714&doi=10.1016%2fj.jpdc.2020.09.007&partnerID=40&md5=7630cfa34317ec3d6ce7e9e29814a240","Communication in data centers often involves many parallel flows that all share the same performance goal (e.g. to minimize the average completion time). A useful abstraction, coflow, is proposed to express the communication requirements of prevalent data parallel paradigms such as MapReduce and Spark. The multiple coflow routing and scheduling problem makes it challenging to derive a good theoretical performance ratio, as coexisting coflows may compete for the same network resources such as link bandwidths. In this paper, we focus on the coflow problem in one popular data center infrastructure: the Leaf-Spine topology. We first formulate the problem and study the path selection issue on this two-tier structure. In order to minimize the average coflow completion time (CCT), we propose the Multi-hop Coflow Routing and Scheduling strategy (MCRS) and prove that our method has a reasonably good competitive ratio. Extensive experiments and large-scale simulations show that MCRS outperforms the state-of-the-art heuristic schemes under the Leaf-Spine topology. © 2020 Elsevier Inc.","Coflow; Data centers; Leaf–spine; Routing; Scheduling"
"Online multimedia retrieval on CPU–GPU platforms with adaptive work partition","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093657597&doi=10.1016%2fj.jpdc.2020.10.001&partnerID=40&md5=2cf6d19452dccc6d1a5f35dfc714c4d2","Nearest neighbors search is a core operation found in several online multimedia services. These services have to handle very large databases, while, at the same time, they must minimize the query response times observed by users. This is specially complex because those services deal with fluctuating query workloads (rates). Consequently, they must adapt at run-time to minimize the response times as the load varies. In this paper, we address the aforementioned challenges with a distributed memory parallelization of the product quantization nearest neighbor search, also known as IVFADC, for hybrid CPU–GPU machines. Our parallel IVFADC implements an out-of-GPU memory execution scheme to use the GPU for databases in which the index does not fit in its memory, which is crucial for searching in very large databases. The careful use of CPU and GPU with work stealing led to an average response time reduction of 2.4× as compared to using the GPU only. Also, our approach to adapt the system to fluctuating loads, called Dynamic Query Processing Policy (DQPP), attained a response time reduction of up to 5× vs. the best static (BS) policy for moderate loads. The system has attained high query processing rates and near-linear scalability in all experiments. We have evaluated our system on a machine with up to 256 NVIDIA V100 GPUs processing a database of 256 billion SIFT features vectors. © 2020 Elsevier Inc.","Approximate nearest neighbor search; Hybrid systems; Online multimedia retrieval"
"Transparent speculation in geo-replicated transactional data stores","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085276034&doi=10.1016%2fj.jpdc.2020.04.014&partnerID=40&md5=18e92849e27edb09b96bcfe4d015cbe2","This work presents Speculative Transaction Replication (STR), a protocol that exploits transparent speculation techniques to enhance performance of geo-distributed, partially replicated transactional data stores. In addition, we define a new consistency model, Speculative Snapshot Isolation (SPSI), a variant of Snapshot Isolation (SI), which shelters applications from the subtle anomalies that arise when using speculative transaction processing techniques. STR provides a form of speculation that is fully transparent for programmers (it does not expose the effects of misspeculations to clients). Since the speculation techniques employed by STR satisfy SPSI, they can be leveraged by application programs in a transparent way, without requiring any source-code modification to applications designed to operate using SI. STR combines two key techniques: speculative reads, which allow transactions to observe pre-committed versions, which can reduce the ‘effective duration’ of pre-commit locks and enhance throughput; Precise Clocks, a novel timestamping mechanism that uses per-item timestamps with physical clocks, which together greatly enhance the probability of successful speculation. We assess STR's performance on up to nine geo-distributed Amazon EC2 data centers, using both synthetic benchmarks as well as realistic benchmarks (TPC-C and RUBiS). Our evaluation shows that STR achieves throughput gains up to 11× and latency reduction up to 10× (with respect to non-speculative systems that ensure SI) in workloads characterized by low inter-data center contention. Furthermore, thanks to a self-tuning mechanism that dynamically and transparently enables and disables speculation, STR offers robust performance even when faced with unfavorable workloads that suffer from high misspeculation rates. © 2020 Elsevier Inc.","Data store; Distributed transaction; Geo-replication; Speculation; Strong consistency"
"Efficient Performance Prediction for Apache Spark","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097135730&doi=10.1016%2fj.jpdc.2020.10.010&partnerID=40&md5=adbc4bede70128cfc7c0c2431ce5fbfa","Spark is a more efficient distributed big data processing framework following Hadoop. It provides users with more than 180 adjustable configuration parameters, and how to choose the optimal configuration automatically to make the Spark application run effectively is challenging. The key to address the above challenge is having the ability to predict the performance of Spark applications in different configurations. This paper proposes a new approach based on Adaboost, which can efficiently and accurately predict the performance of a given application with a given Spark configuration. In our approach, Adaboost is used to build a set of performance models at the stage-level for Spark. To minimize the overhead of the modeling, we use the classic projective sampling, a data mining technique that allows us to collect as few training samples as possible while meeting the accuracy requirements. We evaluate the proposed approach on six typical Spark benchmarks with five input datasets. The experimental results show that our approach is less than the previously proposed approach in prediction error and cost. © 2020 Elsevier Inc.","Adaboost; Performance prediction; Projective sampling; Spark; System configuration"
"Blockchain-based access control management for Decentralized Online Social Networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086078252&doi=10.1016%2fj.jpdc.2020.05.011&partnerID=40&md5=803d85e3c3f28011ff3cee48fe54802a","Online Social Networks (OSNs) represent today a big communication channel where users spend a lot of time to share personal data. Unfortunately, the big popularity of OSNs can be compared with their big privacy issues. Indeed, several recent scandals have demonstrated their vulnerability. Decentralized Online Social Networks (DOSNs) have been proposed as an alternative solution to the current centralized OSNs. DOSNs do not have a service provider that acts as central authority and users have more control over their information. Several DOSNs have been proposed during the last years. However, the decentralization of the social services requires efficient distributed solutions for protecting the privacy of users. During the last years the blockchain technology has been applied to Social Networks in order to overcome the privacy issues and to offer a real solution to the privacy issues in a decentralized system. However, in these platforms the blockchain is usually used as a storage, and content is public. In this paper, we propose a manageable and auditable access control framework for DOSNs using blockchain technology for the definition of privacy policies. The resource owner uses the public key of the subject to define auditable access control policies using Access Control List (ACL), while the private key associated with the subject's Ethereum account is used to decrypt the private data once access permission is validated on the blockchain. We provide an evaluation of our approach by exploiting the Rinkeby Ethereum testnet to deploy the smart contracts. Experimental results clearly show that our proposed ACL-based access control outperforms the Attribute-based access control (ABAC) in terms of gas cost. Indeed, a simple ABAC evaluation function requires 280,000 gas, instead our scheme requires 61,648 gas to evaluate ACL rules. © 2020 Elsevier Inc.","Access control list; Blockchain; Decentralized Online Social Networks; Privacy issue"
"A high efficient multi-robot simultaneous localization and mapping system using partial computing offloading assisted cloud point registration strategy","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097741015&doi=10.1016%2fj.jpdc.2020.10.012&partnerID=40&md5=53f8bdb5bdb3d302e742cafd89df6e79","The robots using visual simultaneous localization and mapping (SLAM) system are generally experiencing excessive power consumption and suffer from depletion of battery energy during the course of working. The intensive computation necessary to complete complicated tasks is overwhelming for inexpensive mobile robots with limited on-board resources. To address this problem, a novel task offloading strategy combined with a new dense point cloud map construction method is proposed in this paper, which is firstly used for the improvement of the system especially in indoor scenes. First, we develop a novel strategy to remotely offload computation-intensive tasks to cloud center so that the tasks that could not originally be achieved locally on the resource-limited robot systems become possible. Second, a modified iterative closest point algorithm (ICP), named fitness score hierarchical ICP algorithm (FS-HICP), is developed to accelerate point cloud registration. The correctness, efficiency, and scalability of the proposed strategy are evaluated with both theoretical analysis and experimental simulations. The results show that the proposed method can effectively reduce the energy consumption while increase the computation capability and speed of the multi-robot visual SLAM system, especially in indoor environment. © 2020 Elsevier Inc.","Computing offloading; Energy consumption; ICP algorithm; Multi-robot system; SLAM system"
"Estimating record linkage costs in distributed environments","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084958964&doi=10.1016%2fj.jpdc.2020.05.003&partnerID=40&md5=19c523f790dd68d896d22f854a0d7c5d","Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as TBF, for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario. © 2020 Elsevier Inc.","Cloud computing; Data quality; Record linkage; Theoretical model"
"SocialBlock: An architecture for decentralized user-centric data management applications for communications in smart cities","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086823570&doi=10.1016%2fj.jpdc.2020.06.004&partnerID=40&md5=82d61db8250883cdaebafba4992f88b8","Smart city projects stand out for including a wide array of communication systems and heterogeneous technologies operated by multiple entities that collect data from citizens in very different ways. Basically, these projects involve three main actors that may have conflicting interests regarding the ownership and exploitation of the generated data: public administrations, service providers and citizens. Nevertheless, this is not a new problem. Actually, the most popular communication systems on the Internet, such as social networks, often have issues of this kind that can become highly visible in the press, especially when service providers, who commonly store data generated by their users via centralized models, have a security issue leading to a massive data leak, or when providers take advantage of their dominant position to exploit data improperly or sell it to third parties. This paper proposes an architecture to create user-centric data management applications for communications in smart cities where data is stored and managed in a decentralized way to reduce dependency on service providers and return control of the data to the different actors involved in the communications. In this paper, a proof of concept of the proposed architecture is implemented, using Ethereum and the InterPlanetary File System (IPFS) as the main tools, to demonstrate its feasibility, show concrete protocols, and discuss the proposal on an empirical basis. © 2020 Elsevier Inc.","Blockchain; Ethereum; IPFS; Smart cities; Social networks"
"MAD-C: Multi-stage Approximate Distributed Cluster-combining for obstacle detection and localization","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092518260&doi=10.1016%2fj.jpdc.2020.08.013&partnerID=40&md5=d2f55b5b21059882a58a263a5dd4598b","The upcoming digitalization in the context of Cyber–physical Systems (CPS), enabled through Internet-of-Things (IoT) infrastructures, require efficient methods for distributed processing of the data, that is generated by multiple sources. We address the problem of obstacle detection and localization through data clustering, which is a common component for data processing in the fusion of multiple point clouds, each obtained by a LIDAR sensor. Such sensors generate data at high rates and can rapidly exhaust traditional methods that centrally gather and process the global data. To that end, we propose MAD-C, an approximate method for distributed data summarization through clustering, that can orthogonally build on known methods for fine-grained point-cloud clustering, and synthesize a decentralized approach, which exploits the distributed processing capacity efficiently and prevents saturation of the communication network. In MAD-C, corresponding to the point-cloud gathered by each LIDAR sensor, local clusters are first identified, each corresponding to an object in the sensed environment from the perspective of the respective sensor. Afterwards, the information about each locally detected object is transformed into a data-summary, computable in a continuous manner, with constant overhead in time and space. The summaries are then combined, in an order-insensitive, concurrent fashion, to produce approximate volumetric representations of the objects in the fused data. We show that the combined summaries, in addition to localizing objects and approximating their volumetric representations, can be used to answer relevant queries regarding the relative position of the objects in environment and a geofence. We evaluate the performance of MAD-C extensively, both analytically and empirically. The empirical evaluation is performed on an IoT test-bed as well as in simulation. Our results show that MAD-C leads to (i) communication savings proportional to the number of points, (ii) multiplicative decrease in the dominating component of the processing complexity and, at the same time, (iii) high accuracy (with RandIndex >0.95), in comparison to its baseline counterpart for obstacle detection and localization, as well as (iv) linear computational complexity in terms of the number of objects, for the geofence related queries. © 2020 Elsevier Inc.","Approximation; Clustering; Distributed processing; LIDAR; Point cloud"
"An energy efficient service composition mechanism using a hybrid meta-heuristic algorithm in a mobile cloud environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085161857&doi=10.1016%2fj.jpdc.2020.05.002&partnerID=40&md5=2b5147810e4a62b2732e431352b1d05c","By increasing mobile devices in technology and human life, using a runtime and mobile services has gotten more complex along with the composition of a large number of atomic services. Different services are provided by mobile cloud components to represent the non-functional properties as Quality of Service (QoS), which is applied by a set of standards. On the other hand, the growth of the energy-source heterogeneity in mobile clouds is an emerging challenge according to the energy saving problem in mobile nodes. In order to mobile cloud service composition as an NP-Hard problem, an efficient selection method should be taken by problem using optimal energy-aware methods that can extend the deployment and interoperability of mobile cloud components. Also, an energy-aware service composition mechanism is required to preserve high energy saving scenarios for mobile cloud components. In this paper, an energy-aware mechanism is applied to optimize mobile cloud service composition using a hybrid Shuffled Frog Leaping Algorithm and Genetic Algorithm (SFGA). Experimental results capture that the proposed mechanism improves the feasibility of the service composition with minimum energy consumption, response time, and cost for mobile cloud components against some current algorithms. © 2020 Elsevier Inc.","Energy consumption; Meta-heuristic algorithm; Mobile cloud computing; Service composition"
"Parallel algorithms for finding connected components using linear algebra","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085681019&doi=10.1016%2fj.jpdc.2020.04.009&partnerID=40&md5=c5a3ffe0346bf697b50c51116dbd03b8","Finding connected components is one of the most widely used operations on a graph. Optimal serial algorithms for the problem have been known for half a century, and many competing parallel algorithms have been proposed over the last several decades under various different models of parallel computation. This paper presents a class of parallel connected-component algorithms designed using linear-algebraic primitives. These algorithms are based on a PRAM algorithm by Shiloach and Vishkin and can be designed using standard GraphBLAS operations. We demonstrate two algorithms of this class, one named LACC for Linear Algebraic Connected Components, and the other named FastSV which can be regarded as LACC's simplification. With the support of the highly-scalable Combinatorial BLAS library, LACC and FastSV outperform the previous state-of-the-art algorithm by a factor of up to 12x for small to medium scale graphs. For large graphs with more than 50B edges, LACC and FastSV scale to 4K nodes (262K cores) of a Cray XC40 supercomputer and outperform previous algorithms by a significant margin. This remarkable performance is accomplished by (1) exploiting sparsity that was not present in the original PRAM algorithm formulation, (2) using high-performance primitives of Combinatorial BLAS, and (3) identifying hot spots and optimizing them away by exploiting algorithmic insights. © 2020 Elsevier Inc.","Connected Component; Distributed memory; GraphBLAS"
"Thermal-aware detour routing in 3D NoCs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086447742&doi=10.1016%2fj.jpdc.2020.04.010&partnerID=40&md5=e02142c3dff57f0e8244c57b0978bb4f","Three-dimensional Network-on-Chips (3D NoCs) is a popular design choice due to its low packet latency, low network power consumption and high packing density. However, 3D NoCs suffer from high temperature issues. The 3D stacking of Si-layers elongates heat transfer path from different Si-layers to the heat sink resulting in increase in peak temperature of the chip. Since routers of NoCs have high power densities, a higher router activity may result in signification increase in temperature of the chip. Therefore, a judicious selection of the routing path is necessary to reduce the chip temperature. As the routers placed at the lower Si-layers have higher thermal conductance to the heat sink, a routing path consisting of more number of routers in the lower Si-layers may improve the temperature profile of the chip. In this paper, we have proposed two different thermal-aware routing approaches, which use downward detoured routing for some optimally selected communication paths to reduce the chip temperature. The first technique is a thermal-aware application-specific Mixed Integer Linear Programming based method (named TMD), while the second one is an application-agnostic heuristic approach (named TSD). To predict the effect of detour decisions on the temperature profile of the chip, TMD technique has applied two different thermal models with a constraint on the average packet delay (APD) of the network. Experimental results show that a significant temperature reduction (up to 22∘) can be achieved within minimal performance loss (10% increase in APD) using either of the proposed techniques, compared to the minimal path routing algorithms – XYZ, ZXY and EDGE and the greedy detour approaches – All Detour and Random Detour. © 2020 Elsevier Inc.","3D network-on-chip; Average packet delay; Mixed Integer Linear Programming; Thermal-aware routing"
"Toward security as a service: A trusted cloud service architecture with policy customization","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097582364&doi=10.1016%2fj.jpdc.2020.11.002&partnerID=40&md5=51252487459cc596c845319b62cc554b","With the rise of concerns over security and privacy in the cloud, the “security-on-demand” service mode dynamically provides cloud customers with trusted computing environments according to their specific security needs. Major challenges, however, remain to achieve this goal: (1) integrating an auditable, tamper-resistant trust-management mechanism into the cloud infrastructure and (2) building a protocol to guarantee the consistency of customers’ policies during virtual machine (VM) migrations. This study develops a new security-on-demand framework called a “policy-customized trusted cloud service” (PC-TCS) architecture that comprises two core components: an attribute-based signature (ABS)-based remote-attestation scheme to achieve trusted remote attestation with customized security policies and an ABS- and blockchain-based VM-migration protocol to support policy-customized trusted migration. To prove the availability of this architecture, we implemented a PC-TCS prototype based on Xen Hypervisor, the results of which indicate that (1) PC-TCS can be integrated into cloud infrastructure as part of a trusted computing base; (2) cloud users can customize the security policies of computing environments and validate their enforcement throughout the service life-cycle with the support of PC-TCS; and (3) PC-TCS can support policy-customized remote attestation and policy-customized migration with a minimal impact on performance. © 2020 Elsevier Inc.","Policy-customized; Remote Attestation; Security on demand; Trusted cloud service"
"SEAPP: A secure application management framework based on REST API access control in SDN-enabled cloud environment","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090925911&doi=10.1016%2fj.jpdc.2020.09.006&partnerID=40&md5=9bab749f74d9f779684b4f20c962e5d4","Cloud computing provides scalable network services and makes network management more flexible by combining Software-Defined Networking (SDN). Through the northbound interface (e.g., REST API) offered by the SDN controller, users can easily deploy diversified applications to access the network resources. However, exploiting the openness of the northbound interface, malicious applications abuse APIs to launch hostile attacks, which poses serious threats to the network. In this paper, we propose SEAPP, a secure application management framework based on REST API access control. Our main idea is to granularly manage application permissions and encrypt REST API calls to defend against malicious attacks. SEAPP includes two components: 1) permissions detection engine identifies the facticity of application permissions by analyzing permission manifests and byte codes and further identifies the legality of permissions with constructed sensitive API list; 2) registration authorization engine executes encrypted registration between applications and controller by virtue of NTRU algorithm and authorizes applications to call the requested REST APIs based on their risk levels after securely authenticating them. Besides, SEAPP is a lightweight logic architecture between application plane and control plane and supports quick deployment and reconfiguration in runtime. Both theoretical analysis and evaluation results show the security and effectiveness of SEAPP. Besides, SEAPP introduces negligible CPU and memory overheads. © 2020 Elsevier Inc.","Application; Cloud; Network security; REST API; Software-Defined Networking"
"Towards cost-efficient resource provisioning with multiple mobile users in fog computing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089733152&doi=10.1016%2fj.jpdc.2020.08.002&partnerID=40&md5=1e1076c6939d5c4328708909f035fefa","Fog computing is an emerging paradigm that brings computing capabilities closer to distributed IoT devices, which provides networking services between end devices and traditional cloud data centers. One important mission is to further reduce the monetary cost of fog resources while meeting the ever-growing demands of multiple users. In this paper, we focus on minimizing the total cost for multiple mobile users to provide an efficient resource provisioning scheme in fog computing. The total cost includes two aspects: the replication cost and the transmission cost. We consider three cases for the resource provision problem by focusing on different cost models. First, one simple case where users can only upload one replication is discussed, and an optimal solution is proposed by converting the original problem into a bipartite graph matching. Then we consider a more complicated case in which each user can upload multiple replications on fog nodes in the resource provisioning. Specifically, two models are discussed: the 0–1 transmission cost model and the different transmission cost model. For the 0–1 transmission cost model, each user can upload multiple replications with a constant transmission cost, and one optimal greedy solution is proposed. For the different transmission cost model, the transmission cost is related to the distance of each pair of fog nodes. This problem is proven to be NP-hard. We first propose a non-adaptive algorithm which is proved to be bounded by [Formula presented]. Another 3+ϵ-approximation algorithm is proposed based on local search, which has better performance with higher complexity. Extensive simulations also prove the efficiency of our schemes. © 2020 Elsevier Inc.","Cost-efficiency; Fog computing; Mobility; Multiple users; Resource provision"
"Election in unidirectional rings with homonyms","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089577998&doi=10.1016%2fj.jpdc.2020.08.004&partnerID=40&md5=7dee1008d1c0b7b6cbf1067af3c70e57","We study leader election in unidirectional rings of homonyms that have no a priori knowledge of the number of processes. In this context, we show that there exists no algorithm that solves the process-terminating leader election problem for the class of asymmetrically labeled unidirectional rings. More precisely, we prove that there is no process-terminating leader election algorithm even for the subclass of unidirectional rings where at least one label is unique. Message-terminating leader election is also impossible for the class of unidirectional rings where only a bound on multiplicity is known. However, we show that the process-terminating leader election is possible for two particular subclasses of asymmetrically labeled unidirectional rings where the multiplicity is bounded. We propose three efficient algorithms and analyze their complexities. We also give some non-trivial lower bounds. © 2020 Elsevier Inc.","Homonyms; Leader election; Multiplicity; Unidirectional rings"
"Decentralized learning works: An empirical comparison of gossip learning and federated learning","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096190467&doi=10.1016%2fj.jpdc.2020.10.006&partnerID=40&md5=3d95be45d1eb21a4acadad8329f39b6e","Machine learning over distributed data stored by many clients has important applications in use cases where data privacy is a key concern or central data storage is not an option. Recently, federated learning was proposed to solve this problem. The assumption is that the data itself is not collected centrally. In a master–worker architecture, the workers perform machine learning over their own data and the master merely aggregates the resulting models without seeing any raw data, not unlike the parameter server approach. Gossip learning is a decentralized alternative to federated learning that does not require an aggregation server or indeed any central component. The natural hypothesis is that gossip learning is strictly less efficient than federated learning due to relying on a more basic infrastructure: only message passing and no cloud resources. In this empirical study, we examine this hypothesis and we present a systematic comparison of the two approaches. The experimental scenarios include a real churn trace collected over mobile phones, continuous and bursty communication patterns, different network sizes and different distributions of the training data over the devices. We also evaluate a number of additional techniques including a compression technique based on sampling, and token account based flow control for gossip learning. We examine the aggregated cost of machine learning in both approaches. Surprisingly, the best gossip variants perform comparably to the best federated learning variants overall, so they offer a fully decentralized alternative to federated learning. © 2020 The Author(s)","Decentralized machine learning; Federated learning; Gossip learning"
"Blockchain and AI amalgamation for energy cloud management: Challenges, solutions, and future directions","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085276236&doi=10.1016%2fj.jpdc.2020.05.004&partnerID=40&md5=7247a1dae14353e10ae10117edabf72c","In the recent years, the Smart Grid (SG) system faces various challenges like the ever-increasing energy demand, the enormous growth of renewable energy sources (RES) with distributed energy generation (EG), the extensive Internet of Things (IoT) devices adaptation, the emerging security threats, and the foremost goal of sustaining the SG stability, efficiency and reliability. To cope up these issues there exists, the energy cloud management (ECM) system, which combines the infrastructure for energy, with intelligent energy usage and value-added services as per consumers demand. To achieve these, efficient demand-side forecasting and secure data transmission are the key factors. The energy management issues pose extreme gravity in finding sustainable solutions by using the blockchain (BC) and Artificial Intelligence (AI). AI-based techniques support various services such as energy load prediction, classification of the consumer, load management, and analysis where the BC provides data immutability and trust mechanism for secure energy management. Therefore, this paper reviews several existing AI-based approaches along with the advantages and challenges of integrating the BC technology and AI in the ECM system. We presented a decentralized AI-based ECM framework for energy management using BC and validate it using a case study. It is shown that how BC and AI can be used to mitigate ECM with security and privacy issues. Finally, we highlighted the open research issues and challenges of the BC-AI-based ECM system. © 2020 Elsevier Inc.","AI; Blockchain; Energy cloud management; Smart grid"
"Arbitrarily large tomography with iterative algorithms on multiple GPUs using the TIGRE toolbox","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089412465&doi=10.1016%2fj.jpdc.2020.07.004&partnerID=40&md5=4b05fdf57f386700c652f38d46aa3007","3D tomographic imaging requires the computation of solutions to very large inverse problems. In many applications, iterative algorithms provide superior results, however, memory limits in available computing hardware restrict the size of problems that can be solved. For this reason, iterative methods are not normally used to reconstruct typical data sets acquired with lab based CT systems. We thus use state of the art techniques such as dual buffering to develop an efficient strategy to compute the required operations for iterative reconstruction. This allows the iterative reconstruction of volumetric images of arbitrary size using any number of GPUs, each with arbitrarily small memory. Strategies for both the forward and backprojection operators are presented, along with two regularization approaches that are easily generalized to other projection types or regularizers. The proposed improvement also accelerates reconstruction of smaller images on single or multiple GPU systems, providing faster code for time-critical applications. The resulting algorithm has been added to the TIGRE toolbox, a repository for iterative reconstruction algorithms for general CT, but this memory-saving and problem-splitting strategy can be easily adapted for use with other GPU-based tomographic reconstruction code. © 2020 Elsevier Inc.","Computed Tomography; Iterative reconstruction; multi-GPU; Software"
"A new energy-aware tasks scheduling approach in fog computing using hybrid meta-heuristic algorithm","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084947272&doi=10.1016%2fj.jpdc.2020.04.008&partnerID=40&md5=9e741fe9f8110f50b7bbcf66ad605641","In recent years, large computational problems have beensolved by the distributed environment in which applications are executed in parallel. Also, lately, fog computing or edge computing as a new environment is applied to collect data from the devices and preprocessing is done before sending for main processing in cloud computing. Since one of the crucial issues in such systems is task scheduling, this issue is addressed by considering reducing energy consumption. In this study, an energy-aware method is introduced by using the Dynamic Voltage and Frequency Scaling (DVFS) technique to reduce energy consumption. In addition, in order to construct valid task sequences, a hybrid Invasive Weed Optimization and Culture (IWO-CA) evolutionary algorithm is applied. The experimental results revealed that the proposed algorithm improves some current algorithms in terms of energy consumption. © 2020","DVFS; Energy consumption; Fog computing; Meta-heuristic Algorithm; Task scheduling"
"Management of geo-distributed intelligence: Deep Insight as a Service (DINSaaS) on Forged Cloud Platforms (FCP)","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097775339&doi=10.1016%2fj.jpdc.2020.11.009&partnerID=40&md5=58453b5f371d4d09a44b5448b47c5304","The recent advances in the cyber–physical domains, cloud and edge platforms along with the advanced communication technologies play a crucial role in connecting the globe more than ever, which is creating large volumes of data at astonishing rates and a tsunami of computation within hyper-connectivity. Data analytic tools are evolving rapidly to harvest these explosive increasing data volumes. Deriving meaningful insights from voluminous geo-distributed data of all kinds as a strategic asset is fuelling the innovation, facilitating e-commerce and revolutionizing the industry and businesses in the transition from digital to the intelligent way of doing business. In this perspective, in this study, a philosophical industrial and technological direction involving Deep Insight-as-a-Service (DINSaaS) on Forged Cloud Platforms (FCP) along with Advanced Insight Analytics (AIA), primarily motivated by the global benefit is systematically analysed within sophisticated theoretical knowledge, and consequently, a conceptual geo-distributed framework is proposed to (1) guide the national/international leading organizations, governments, cloud service providers and leading companies in order to establish a scalable framework within the hyperscale geo-distributed infrastructure in which exponentially increasing voluminous Big Data (BD) can be harvested effectively and efficiently, (2) inspire the transformation of BD into wiser abstract formats in Specialized Insight Domains (SID), (3) provide fusion and networking of insights rather than BD in order to obtain globally generated distributed intelligence and help make better decisions and near-real-time predictions, in particular for time-critical latency-sensitive applications, and (4) direct all the stakeholders to rivet the high-quality products and services within Automation of Everything (AoE) by exploiting continuously created and updated insights in dedicated taxonomic SID within large-scale geo-distributed datacenters. © 2020 Elsevier Inc.","Automation of Everything (AoE); Big data analytics; Cloud platform; Cyber–Physical Systems (CPS); Internet of Everything (IoE)"
"An empirical study of I/O separation for burst buffers in HPC systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096531399&doi=10.1016%2fj.jpdc.2020.10.007&partnerID=40&md5=c5fbee7d440d3a8e3eb715dda53f0a07","To meet the exascale I/O requirements for the High-Performance Computing (HPC), a new I/O subsystem, Burst Buffer, based on solid state drives (SSD), has been developed. However, the diverse HPC workloads and the bursty I/O pattern cause severe data fragmentation that requires costly garbage collection (GC) and increases the number of bytes written to the SSD. To address this data fragmentation challenge, a new multi-stream feature has been developed for SSDs. In this work, we develop an I/O Separation scheme called BIOS to leverage this multi-stream feature to group the I/O streams based on the user IDs. We propose a stream-aware scheduling policy based on burst buffer pools in the workload manager, and integrate the BIOS with the workload manager to optimize the I/O separation scheme in burst buffer. We evaluate the proposed framework with a burst buffer I/O traces from Cori Supercomputer including a diverse set of applications. Experimental results show that the BIOS could improve the performance by 1.44x on average and reduce the Write Amplification Factor (WAF) by up to 1.20x. These demonstrate the potential benefits of the I/O separation scheme for solid state storage systems. © 2020 Elsevier Inc.","Burst buffer; Evaluation; I/O separation; Multi-streamed SSD; Stream-aware"
"Blockchain-based eHealth system for auditable EHRs manipulation in cloud environments","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094202988&doi=10.1016%2fj.jpdc.2020.10.002&partnerID=40&md5=54787a32af0af9c72632879efc04c4ea","The development of cloud-assisted electronic health system effectively addresses the drawbacks of traditional medical management system. However, some challenging problems such as security and privacy in data storage and sharing cannot be ignored. First, it is difficult to ensure the integrity of electronic health records (EHRs) during the data outsourcing process. Second, it is difficult to guarantee the privacy and traceability of EHRs during the data sharing process. In this paper, a blockchain-based eHealth system called BCES is proposed to ensure that the manipulation of EHRs can be audited. In BCES, each legitimate query manipulation of data consumers, together with each legitimate outsourcing manipulation of hospitals, will be written into the blockchain as a transaction for permanent storage, which ensures the traceability. At the same time, the attributes-based proxy re-encryption is adopted to achieve fine-grained access control of medical data, and any behavior that threatens the integrity of EHRs will be discovered by the auditor. Due to the traceable and tamper-resistant characteristic of blockchain, any entity that had an illegal manipulation of EHRs will be held accountable to the evidence of our constructed Proof-Chain. Finally, security analysis and performance evaluation demonstrate that this scheme is secure and efficient. © 2020 Elsevier Inc.","Blockchain; Cloud computing; Data outsourcing and sharing; Electronic health records"
"Coordinated management of DVFS and cache partitioning under QoS constraints to save energy in multi-core systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086635020&doi=10.1016%2fj.jpdc.2020.05.006&partnerID=40&md5=c952c0295131119131ff0cfdd429574a","Reducing the energy expended to carry out a computational task is important. In this work, we explore the prospects of meeting Quality-of-Service requirements of tasks on a multi-core system while adjusting resources to expend a minimum of energy. This paper considers, for the first time, a QoS-driven coordinated resource management algorithm (RMA) that dynamically adjusts the size of the per-core last-level cache partitions and the per-core voltage–frequency settings to save energy while respecting QoS requirements of every application in multi-programmed workloads run on multi-core systems. It does so by doing configuration-space exploration across the spectrum of LLC partition sizes and Dynamic Voltage–Frequency Scaling (DVFS) settings at runtime at negligible overhead. We show that the energy of 4-core and 8-core systems can be reduced by up to 18% and 14%, respectively, compared to a baseline with even distribution of cache resources and a fixed mid-range core voltage–frequency setting. The energy savings can potentially reach 29% if the QoS targets are relaxed to 40% longer execution time. © 2020 The Author(s)","Cache partitioning; Dynamic voltage–frequency scaling (DVFS); Energy efficiency; Multi-core resource management; Quality of service (QoS)"
"Automatic blockchain whitepapers analysis via heterogeneous graph neural network","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086720033&doi=10.1016%2fj.jpdc.2020.05.014&partnerID=40&md5=99db3b5737095577f1661e309aaf153b","The blockchain whitepaper contains detailed technical and business information, so its analysis is important for blockchain text mining. Previous works focus on analyze homogeneous objects and relations. The main problem, however, is these works do not take into account the heterogeneity of information. This paper presents a new methodology for whitepapers analysis by designing heterogeneous graph neural network, named S-HGNN. In detail, this paper first builds a Heterogeneous Information Network (HIN) using heterogeneous objects and relationships extracted from the whitepaper to obtain similarity measures, then uses Graph Convolutional Network (GCN) and Graph Attention Network (GAT) to integrate both structural information and internal semantic into the whitepaper embedding. Compared with the previous models, this model improves 0.96%∼33.34% in terms of F1-score for classification task, and 4.94%∼14.14% in terms of purity for clustering task, and gets stable results on different tasks. The results show the effectiveness and robustness of this model for whitepapers analysis. © 2020 Elsevier Inc.","Blockchain; Classification; Clustering; Heterogeneous graph neural network; Heterogeneous information networks"
"Achieving high data reliability at low scrubbing cost via failure-aware scrubbing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086633489&doi=10.1016%2fj.jpdc.2020.05.007&partnerID=40&md5=59a21d32c0269ffad6cc04e44f051de5","Latent Sector Errors (LSEs) happen at a significant frequency in the field and can impose a huge risk to data reliability. Disk scrubbing is a background process that reads disks periodically to detect LSEs timely, thus shortening the window of vulnerability to data loss. Nowadays, proactive error prediction, using machine learning techniques, has been proposed to improve storage system reliability by increasing the scrubbing rate for disks with higher error rates. Unfortunately, the majority of works incur non-trivial scrubbing costs and overlook the relationship between complete disk failures and LSEs. In this paper, we attempt to maintain or improve data reliability at reduced scrubbing costs. In particular, we design a novel adaptive approach that enforces a lower scrubbing rate for healthy disks and a higher scrubbing rate for disks which are subject to LSEs. Besides LSEs that are specific to partial disk failures, we also adjust scrubbing rates according to complete disk failure rates, because disks typically develop LSEs before they finally fail. Moreover, a voting-based method that exploits the periodic characteristic of scrubbing is proposed to ensure prediction accuracy. Experimental results on a real-world field dataset have demonstrated the effectiveness of our proposed approach. Specifically, the results show that we can achieve the same level of reliability, in terms of Mean-Time-To-Detection (MTTD), as the traditional fixed-rate scrubbing scheme with almost 49% less scrubbing costs or we can improve the reliability by a factor of 2.4X without extra scrubbing costs. Compared with the state-of-the-art approaches, our method can achieve the same level of reliability with nearly 32% less scrubbing costs. © 2020 Elsevier Inc.","Hard disk; Latent sector error; Machine learning; Reliability; Scrubbing"
"Graph-Waving architecture: Efficient execution of graph applications on GPUs","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095447911&doi=10.1016%2fj.jpdc.2020.10.005&partnerID=40&md5=d3cf8bfe311df8ae5400d9f607debda1","Most existing graph frameworks for GPUs adopt a vertex-centric computing model where vertex to thread mapping is applied. When run with irregular graphs, we observe significant load imbalance within SIMD-groups using vertex to thread mapping. Uneven work distribution within SIMD-groups leads to low utilization of SIMD units and inefficient use of memory bandwidth. We introduce Graph-Waving (GW) architecture to improve support for many graph applications on GPUs. It uses vertex to SIMD-group mapping and Scalar-Waving as a mechanism for efficient execution. It also favors a narrow SIMD-group width with a clustered issue approach and reuse of instructions in the front-end. We thoroughly evaluate GW architecture using timing detailed GPGPU-sim simulator with several graph and non-graph benchmarks from a variety of benchmark suites. Our results show that GW architecture provides an average of 4.4x and a maximum of 10x speedup with graph applications, while it obtains 9% performance improvement with regular and 17% improvement with irregular benchmarks. © 2020 Elsevier Inc.","GPGPU; GPU microarchitecture; Graph application; Scalar waving; SIMD efficiency"
"Differential privacy in blockchain technology: A futuristic approach","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087330875&doi=10.1016%2fj.jpdc.2020.06.003&partnerID=40&md5=1be5f97cb7d40c816ce46da61e64dd73","Blockchain has received a widespread attention because of its decentralized, tamper-proof, and transparent nature. Blockchain works over the principle of distributed, secured, and shared ledger, which is used to record, and track data within a decentralized network. This technology has successfully replaced certain systems of economic transactions in organizations and has the potential to overtake various industrial business models in future. Blockchain works over peer-to-peer (P2P) phenomenon for its operation and does not require any trusted-third party authorization for data tracking and storage. The information stored in blockchain is distributed throughout the decentralized network and is usually protected using cryptographic hash functions. Since the beginning of blockchain technology, its use in different applications is increasing exponentially, but this increased use has also raised some questions regarding privacy and security of data being stored in it. Protecting privacy of blockchain data using data perturbation strategy such as differential privacy could be a novel approach to overcome privacy issues in blockchain. In this article, we cover the topic of integration of differential privacy in each layer of blockchain and in certain blockchain based scenarios. Moreover, we highlight some future challenges and application scenarios in which integration of differential privacy in blockchain can produce fruitful results. © 2020 Elsevier Inc.","Blockchain; Differential privacy; Privacy preservation"
"Lightweight collaborative anomaly detection for the IoT using blockchain","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087329897&doi=10.1016%2fj.jpdc.2020.06.008&partnerID=40&md5=9639ddde193d4cfc8103140f3f1948ad","Due to their rapid growth and deployment, the Internet of things (IoT) have become a central aspect of our daily lives. Unfortunately, IoT devices tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques, such as anomaly detection, can be used to secure these devices in a plug-and-protect manner. However, anomaly detection models must be trained for a long time in order to capture all benign behaviors. Furthermore, the anomaly detection model is vulnerable to adversarial attacks since, during the training phase, all observations are assumed to be benign. In this paper, we propose (1) a novel approach for anomaly detection and (2) a lightweight framework that utilizes the blockchain to ensemble an anomaly detection model in a distributed environment. Blockchain framework incrementally updates a trusted anomaly detection model via self-attestation and consensus among the IoT devices. We evaluate our method on a distributed IoT simulation platform, which consists of 48 Raspberry Pis. The simulation demonstrates how the approach can enhance the security of each device and the security of the network as a whole. © 2020 Elsevier Inc.","Anomaly detection; Blockchain; Collaborative security; IoT security; Markov-chain"
"DLHub: Simplifying publication, discovery, and use of machine learning models in science","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090565243&doi=10.1016%2fj.jpdc.2020.08.006&partnerID=40&md5=bced6bf3bbc5db2a80cbbf69c6b310fc","Machine Learning (ML) has become a critical tool enabling new methods of analysis and driving deeper understanding of phenomena across scientific disciplines. There is a growing need for “learning systems” to support various phases in the ML lifecycle. While others have focused on supporting model development, training, and inference, few have focused on the unique challenges inherent in science, such as the need to publish and share models and to serve them on a range of available computing resources. In this paper, we present the Data and Learning Hub for science (DLHub), a learning system designed to support these use cases. Specifically, DLHub enables publication of models, with descriptive metadata, persistent identifiers, and flexible access control. It packages arbitrary models into portable servable containers, and enables low-latency, distributed serving of these models on heterogeneous compute resources. We show that DLHub supports low-latency model inference comparable to other model serving systems including TensorFlow Serving, SageMaker, and Clipper, and improved performance, by up to 95%, with batching and memoization enabled. We also show that DLHub can scale to concurrently serve models on 500 containers. Finally, we describe five case studies that highlight the use of DLHub for scientific applications. © 2020 Elsevier Inc.","DLHub; Learning systems; Machine learning; Model serving"
"Expelliarmus: Semantic-centric virtual machine image management in IaaS Clouds","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089753647&doi=10.1016%2fj.jpdc.2020.08.001&partnerID=40&md5=61352ecf385aad6888fdedfd507ae863","Infrastructure-as-a-service (IaaS) Clouds concurrently accommodate diverse sets of user requests, requiring an efficient strategy for storing and retrieving virtual machine images (VMIs) at a large scale. The VMI storage management requires dealing with multiple VMIs, typically in the magnitude of gigabytes, which entails VMI sprawl issues hindering the elastic resource management and provisioning. Unfortunately, existing techniques to facilitate VMI management overlook VMI semantics (i.e at the level of base image and software packages), with either restricted possibility to identify and extract reusable functionalities or with higher VMI publishing and retrieval overheads. In this paper, we propose Expelliarmus, a novel VMI management system that helps to minimize VMI storage, publishing and retrieval overheads. To achieve this goal, Expelliarmus incorporates three complementary features. First, it models VMIs as semantic graphs to facilitate their similarity computation. Second, it provides a semantically-aware VMI decomposition and base image selection to extract and store non-redundant base image and software packages. Third, it assembles VMIs based on the required software packages upon user request. We evaluate Expelliarmus through a representative set of synthetic Cloud VMIs on a real test-bed. Experimental results show that our semantic-centric approach is able to optimize the repository size by 2.3−22 times compared to state-of-the-art systems (e.g. IBM's Mirage and Hemera) with significant VMI publishing and slight retrieval performance improvement. © 2020 The Author(s)","Semantic similarity; Storage optimization; Virtual machine image management; Virtual machine image publishing; Virtual machine image retrieval"
"Vulnerability assessment of fault-tolerant optical network-on-chips","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087700702&doi=10.1016%2fj.jpdc.2020.06.016&partnerID=40&md5=f87702119433f0caa8b82d7d536d5b9a","Multi/Many-core systems based on the traditional electrical network-on-chip are confronted with the limited bandwidth, high latency, and reliability challenges. The emerging optical solution of silicon photonics has promised to improve the design parameters of electrical interconnections for future multiprocessor system on chips. Although the optical network-on-chip has advantages in terms of reliability compared to the electrical ones, important phenomena such as crosstalk noise, process variation, and temperature fluctuations must be carefully considered. These physical challenges have substantial adverse effects on the correct functionality of on-chip optical devices such as microring resonator and the optical waveguide. Malfunction of these elements may cause the injection of faults that should be tolerated by the reliable system. In this paper, we have assessed the effect of the fault-tolerant design of optical routers on the reliability parameters through application mapping. We propose a fault-tolerant algorithm to modify optical routers to improve the reliability parameter. The vulnerability analysis of the proposed algorithm shows that besides obtaining the fault-tolerant capability in optical routers, only about 9.63% and 19.29% of SNR decrease for real-world applications and seven traditional optical routers are achieved in the case of a single fault and two faults injections, respectively. © 2020 Elsevier Inc.","Application mapping; Fault-tolerant; Mesh topology; Optical network on chip; Vulnerability assessment"
"FPC-BI: Fast Probabilistic Consensus within Byzantine Infrastructures","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090601605&doi=10.1016%2fj.jpdc.2020.09.002&partnerID=40&md5=d9da939ef2bad423b41dcbdcd9ef89f9","This paper presents a novel leaderless protocol (FPC-BI: Fast Probabilistic Consensus within Byzantine Infrastructures) with a low communicational complexity and which allows a set of nodes to come to a consensus on a value of a single bit. The paper makes the assumption that part of the nodes are Byzantine, and are thus controlled by an adversary who intends to either delay the consensus, or break it (this defines that at least a couple of honest nodes come to different conclusions). We prove that, nevertheless, the protocol works with high probability when its parameters are suitably chosen. Along this the paper also provides explicit estimates on the probability that the protocol finalizes in the consensus state in a given time. This protocol could be applied to reaching consensus in decentralized cryptocurrency systems. A special feature of it is that it makes use of a sequence of random numbers which are either provided by a trusted source or generated by the nodes themselves using some decentralized random number generating protocol. This increases the overall trustworthiness of the infrastructure. A core contribution of the paper is that it uses a very weak consensus to obtain a strong consensus on the value of a bit, and which can relate to the validity of a transaction. © 2020 Elsevier Inc.","Consensus; Decentralized cryptocurrency systems; Decentralized randomness; Voting"
"Real-time monitoring and operation of microgrid using distributed cloud–fog architecture","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088396418&doi=10.1016%2fj.jpdc.2020.06.006&partnerID=40&md5=c81697315b4f346b5c659a340dcf7455","In this paper, a new distributed multi-agent framework based on the three layers’ fog computing architecture is developed for real-time microgrid economic dispatch and monitoring. To this end, the changes of load at any time will be tracked by the proposed technique, considering unit sudden exits and entries. Moreover, to make the system more realistic, different renewable energies, including photovoltaics (PVs), wind turbines (WTs), fuel cells (FCs), and microturbines (MT) are considered in the proposed technique. To overcome the complexity of the problem, by using advantages of fog computing, a new fast consensus-based optimization algorithm is used, which is modified based on the fuzzy adaptive leader technique. Finally, the proposed technique is simulated and tested on microgrids with 6 and 14 buses, respectively. Simulation results demonstrate and validate the effectiveness of the proposed technique, as well as the capability to track the changes of load with the interactions in real-time and the fast convergence rate. © 2020 Elsevier Inc.","Consensus; Distributed optimization; Energy management; Fog computing; Microgrid"
"On achieving interactive consistency in real-world distributed systems","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092086535&doi=10.1016%2fj.jpdc.2020.09.010&partnerID=40&md5=43f1c018fa0d7e691000ed23bba7bd70","Interactive consistency is the problem in which n distinct nodes, each having its own private value, where up to t may be Byzantine, run an algorithm that allows all non-faulty nodes to infer the values of each other node. This problem is relevant to critical applications that rely on the combination of the opinions of multiple peers to provide a service. Examples include monitoring a content source to prevent equivocation or to track variability in the content provided, and resolving divergent state amongst the nodes of a distributed system. Previous works assume a fully synchronous system, where one can make strong assumptions such as negligible message delivery delays and/or detection of absent messages. However, practical, real-world systems are mostly asynchronous, i.e., they exhibit only some periods of synchrony during which message delivery is timely, thus requiring a different approach. In this paper, we present a thorough study of practical interactive consistency. We leverage the vast prior work on broadcast and Byzantine consensus algorithms to design, implement and evaluate a set of randomized algorithms, with only a single synchronization barrier and varying message complexities, that can be used to achieve interactive consistency in real-world distributed systems. We present formal proofs of correctness and message complexity of our proposed algorithms. We provide a complete, open-source implementation of each proposed interactive consistency algorithm by building a multi-layered software stack of algorithms that includes several broadcast algorithms, as well as a binary and a multi-valued consensus algorithm. Most of these algorithms have never been implemented and evaluated in a real system before. Finally, we analyze the performance of our suite of algorithms experimentally by testing both single instance and multiple parallel instances of each alternative and present a case study of achieving interactive consistency in a real-world distributed e-voting system. © 2020 Elsevier Inc.","Agreement; Asynchronous; Byzantine fault tolerance; Consensus; Interactive consistency"
"Towards autonomic data management for staging-based coupled scientific workflows","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089353434&doi=10.1016%2fj.jpdc.2020.07.002&partnerID=40&md5=e2582dedb91195e9c0e5ae58d9ebd4f2","Emerging scientific workflows running at extreme scale are composed of multiple applications that interact and exchange data at runtime. While staging-based approaches, e.g. in-situ/in-transit processing, are promising, dynamic behaviors (e.g. data volumes and distributions) in coupled applications and varying resource constraints at runtime make the efficient use of these techniques challenging. Addressing these challenges requires fundamental changes in the way that workflows are executed at runtime. Specifically, it is required to monitor the operating environment and running applications, and then adapt and tune the application behaviors and resource allocations at runtime while meeting the data management requirements and constraints. In this paper, we propose a policy-based autonomic data management (ADM) approach that can adaptively respond at runtime to dynamic data management requirements. We first formulate the schematic abstraction of this ADM approach including its conceptual model and system elements. Then, we explore the realization of ADM runtime and demonstrate how to achieve adaptations in a cross-layer manner with pre-defined autonomic policies. We also prototype our ADM approach and evaluate its performance on the Intrepid IBM-BlueGene and Titan Cray-XK7 systems using Chombo-based AMR applications and a visualization application. The experimental results demonstrate its effectiveness in meeting user defined objectives and accelerating overall scientific discovery. © 2020 Elsevier Inc.","Autonomic computing; Data management; Data staging; HPC workflow; In-situ"
"Enabling efficient and secure energy cloud using edge computing and 5G","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087335542&doi=10.1016%2fj.jpdc.2020.06.014&partnerID=40&md5=ecfd9ff4c902ed6956deda4e309ea6ad","Energy cloud systems continue to shape the future of the energy sector. The complexity of energy cloud systems stems from their widespread and distributed aspects such as renewable energy sources, energy storage, customers engagement, social media and the advancements in communication and computing technologies. The unprecedented large-scale growth of energy cloud systems requires a crucial and dramatic paradigm shift in managing and optimizing the available energy assets in order to satisfy the increasing customers’ requirements. This paper proposes and evaluates an edge computing based framework that aims to efficiently manage and optimize energy cloud systems while increasing their reliability, safety, and security. The proposed framework exploits the current expansion in computing capabilities of the edge computing and the Fifth Communication Generation (5G) technology. The evaluation of the proposed framework shows that an edge computing infrastructure improves the service efficiency by 22.6% compared with a cloud infrastructure. In addition, the latency is reduced by 69.1%. The proposed framework provides threat detection capability by using the edge layer as an extra layer for defense against energy cloud system attacks. However, this defense mechanism incurs 10.9% overhead and 9.6% extra delay per service request on average. © 2020 Elsevier Inc.","5G; Edge computing; Energy cloud; Energy cloud security; Reliability"
"Performance analysis of network-on-chip in many-core processors","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092153218&doi=10.1016%2fj.jpdc.2020.09.013&partnerID=40&md5=a6f34a8d4a5e855cd18248f7ff405507","Network-on-chip (NoC) is an integral part of many-core microprocessors. Performance analysis of network-on-chip directly affects the performance of the microprocessor. In this paper we propose a mathematical model to represent packet flow in an NoC as an open feed-forward queuing network. We study the performance of NoC by varying different parameters that includes packet injection rate, packet size, buffer size and number of virtual channels. We also discuss how different flow control algorithms, injection processes, traffic patterns can be incorporated in our model. Apart from the speedup achieved by our model, we also demonstrate that our model can be used to explore various configurations of NoC with minimal error. © 2020 Elsevier Inc.","Many-core processor; Network-on-chip; Performance analysis; Stochastic modelling"
"WP-SGD: Weighted parallel SGD for distributed unbalanced-workload training system","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088121216&doi=10.1016%2fj.jpdc.2020.06.011&partnerID=40&md5=9c58a0957d219c6583c292f1e5fe9519","Stochastic gradient descent (SGD) is a popular stochastic optimization method in machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel SGD (Zinkevich, 2010), often require all nodes to have the same performance or to consume equal quantities of data. However, these requirements are difficult to satisfy when the parallel SGD algorithms run in a heterogeneous computing environment; low-performance nodes will exert a negative influence on the final result. In this paper, we propose an algorithm called weighted parallel SGD (WP-SGD). WP-SGD combines weighted model parameters from different nodes in the system to produce the final output. WP-SGD makes use of the reduction in standard deviation to compensate for the loss from the inconsistency in performance of nodes in the cluster, which means that WP-SGD does not require that all nodes consume equal quantities of data. We also propose the methods of running two other parallel SGD algorithms combined with WP-SGD in a heterogeneous environment. The experimental results show that WP-SGD significantly outperforms the traditional parallel SGD algorithms on distributed training systems with an unbalanced workload. © 2020 Elsevier Inc.","Distributed system; SGD; SimuParallel SGD; Unbalanced workload"
"Detection resource allocation scheme for two-layer cooperative IDSs in smart grids","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092335087&doi=10.1016%2fj.jpdc.2020.09.011&partnerID=40&md5=afd24be147344a1c660427fb1f09cbc4","Although some existing collaborative intrusion detection schemes can increase the detection performance by dynamically allocating detection resources in smart grids, these related works fail to consider the optimization of resource allocation between IDSs under the condition of resource restriction. In this paper, considering the effect of resource restriction, we propose a resource allocation scheme for two-layer collaborative IDSs based on sharing strategies in smart grids. In the first layer of our scheme, we model the interaction between the IDSs and the attackers through a stochastic game based on sharing strategies, where we provide each IDS with two different options for its strategy updating at each stage in the stochastic game. Then the resource updating strategies of the IDSs are obtained through this proposed model. Further, in the second layer we quantify the effect of detection resource restriction, and we propose a resource allocation method under the condition of detection resource restriction, where each IDS can obtain its detection resources according to the results generated by our proposed stochastic game. Based on our experimental analysis, compared with other resource allocation schemes, our proposed scheme can more quickly achieve the Nash equilibrium between the IDSs and the attackers to make the IDSs obtain more rewards, and then can more rationally promote the IDSs to update their detection resources so that the IDSs obtain the optimal detection strategies under the condition of resource restriction. Our proposed scheme can achieve effective detection resource allocation between IDSs for the security of neighborhood area network in smart grids. © 2020 Elsevier Inc.","Intrusion detection system; Resource allocation; Sharing strategy; Smart grids"
"Improvement of recommendation algorithm based on Collaborative Deep Learning and its Parallelization on Spark","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094930344&doi=10.1016%2fj.jpdc.2020.09.014&partnerID=40&md5=06d8dc88c411dade476e1f7d884136da","Collaborative Deep Learning (CDL) utilizes the strong feature learning capability of neural network and the model fitting robustness to solve the problem that the performance of Recommender System drops dramatically when the data is sparse. However, it makes the model training become difficult to maintain when Recommender System faces a large amount of data, and a variety of unpredictable problems will arise. In order to solve the above problems, collaborative deep learning and its parallelization methods were studied in this study, and an improved model CDL-I (CDL with item private node) aiming at item content optimization based on collaborative deep learning was proposed, which improved SDAE on the basis of CDL, added private network nodes; in case of sharing the network parameters of the model, private bias terms were added for each item. As a result, the network may learn the item content parameters in a more targeted manner, thereby enhancing the detection performance of the model on item content in Recommender System. Furthermore, the algorithm was parallelized by splitting the model, and a parallel training CDL-I method was also proposed, which was transplanted to the Spark distributed cluster. The parameters of each part of the model were trained and optimized in parallel to enhance the scale and scalability of data that the model could process. The experiments on multiple real datasets have verified the effectiveness and efficiency of the proposed parallel CDL-I algorithm. © 2020 Elsevier Inc.","Collaborative Deep Learning (CDL); Parallel computing; Recommender System; Spark"
"Flow mapping on mesh-based deep learning accelerator","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086436418&doi=10.1016%2fj.jpdc.2020.04.011&partnerID=40&md5=1cf4919c4e861d5e86d14440fd011c19","Convolutional neural networks have been proposed as an approach for classifying data corresponding to a variety of datasets. Indeed, developments in data diversity and information technology have increased the complexity of deep learning algorithms. Numerous trained models have been proposed for supporting complex algorithms and data detachment with high accuracy. Convolutional operations increase when the convolution depth of neural networks increases. Thus, employing deep convolutional networks is challenging regarding energy consumption, bandwidth, memory requirements, and memory access. Different types of on-chip communication platforms and traffic distribution methods are effective in the improvement of memory access and energy consumption induced by data transfer. Also, dataflow mapping methods have an impressive effect on reducing or increasing delay and energy consumption caused by exchanging data between cores of a communication network. Different methods have been proposed to dataflow mapping on various networks for reducing total hop counts that led to improve performance and cost. Dataflow mapping approach can affect performance improvement of the inference phase in neural networks. This paper proposes various traffic patterns by considering different memory access mechanisms for traffic distribution of a trained AlexNet model on mesh topology. We propose a flow mapping method (FMM) on the mesh to determine the data flow efficiency of different traffic patterns on energy consumption. The FMM reduced energy and total flow by approximately 17.86% and 34.16%, respectively, using different traffic patterns. Thus, the FMM improved the performance of AlexNet traffic distribution while the impact on data flow reduced energy consumption. © 2020 Elsevier Inc.","Convolutional neural network (CNN); Flow mapping; Traffic distribution; Traffic pattern"
"Rendezvous algorithms for large-scale modeling and simulation","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091793457&doi=10.1016%2fj.jpdc.2020.09.001&partnerID=40&md5=3320613e1d01eb048795f6eddfe1c67f","Rendezvous algorithms encode a communication pattern that is useful when processors sending data do not know who the receiving processors should be, or vice versa. The idea is to define an intermediate decomposition where datums from different sending processors can ”rendezvous” to perform a computation, in a manner that both the senders and eventual receivers of the results can identify the appropriate rendezvous processor. Originally designed for interpolating between overlaid grids with independent parallel decompositions (Plimpton et al., 2004), we have recently found rendezvous algorithms useful for a variety of operations in particle- or grid-based simulation codes when running large problems on large numbers of processors. In particular, we show they can perform well when a load-balanced intermediate decomposition is randomized and not spatial, requiring all-to-all communication to move data between processors. In this case rendezvous algorithms leverage the large bisection communication bandwidths which parallel machines provide. We describe how rendezvous algorithms work in a scientific computing context and give specific examples for molecular dynamics and Direct Simulation Monte Carlo codes which result in dramatic performance improvements versus simpler algorithms which do not scale as well. We explain how a generic rendezvous algorithm can be implemented, and also point out similarities with the MapReduce paradigm popularized by Google and Hadoop. © 2020 Elsevier Inc.","MapReduce; Modeling and simulation; Parallel communication; Rendezvous algorithms"
"Communication optimization strategies for distributed deep neural network training: A survey","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097208289&doi=10.1016%2fj.jpdc.2020.11.005&partnerID=40&md5=dafa2fc92732d642f6a440871e764773","Recent trends in high-performance computing and deep learning have led to the proliferation of studies on large-scale deep neural network training. However, the frequent communication requirements among computation nodes drastically slow the overall training speeds, which causes bottlenecks in distributed training, particularly in clusters with limited network bandwidths. To mitigate the drawbacks of distributed communications, researchers have proposed various optimization strategies. In this paper, we provide a comprehensive survey of communication strategies from both an algorithm viewpoint and a computer network perspective. Algorithm optimizations focus on reducing the communication volumes used in distributed training, while network optimizations focus on accelerating the communications between distributed devices. At the algorithm level, we describe how to reduce the number of communication rounds and transmitted bits per round. In addition, we elucidate how to overlap computation and communication. At the network level, we discuss the effects caused by network infrastructures, including logical communication schemes and network protocols. Finally, we extrapolate the potential future challenges and new research directions to accelerate communications for distributed deep neural network training. © 2020 Elsevier Inc.","Communication optimization; Distributed deep learning; Network infrastructure; Parallel algorithms"
"Distributed approximate k-core decomposition and min–max edge orientation: Breaking the diameter barrier","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090850023&doi=10.1016%2fj.jpdc.2020.08.010&partnerID=40&md5=bb3d25442730ce201e360c074ce9dc33","We design distributed algorithms to compute approximate solutions for several related graph optimization problems. All our algorithms have round complexity being logarithmic in the number of nodes of the underlying graph and in particular independent of the graph diameter. By using a primal–dual approach, we develop a 2(1+ϵ)-approximation algorithm for computing the coreness values of the nodes in the underlying graph, as well as a 2(1+ϵ)-approximation algorithm for the min–max edge orientation problem, where the goal is to orient the edges so as to minimize the maximum weighted in-degree. We provide lower bounds showing that the aforementioned algorithms are tight both in terms of the approximation guarantee and the round complexity. Additionally, motivated by the fact that the densest subset problem has an inherent dependency on the diameter of the graph, we study a weaker version that does not suffer from the same limitation. Finally, we conduct experiments on large real-world graphs to evaluate the effectiveness of our algorithms. © 2020 Elsevier Inc.","Coreness; Distributed algorithms; Round complexity"
"Performance enhancement of a dynamic K-means algorithm through a parallel adaptive strategy on multicore CPUs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086986038&doi=10.1016%2fj.jpdc.2020.06.010&partnerID=40&md5=62c5273ce9c08d451b50a24147c31036","The K-means algorithm is one of the most popular algorithms in Data Science, and it is aimed to discover similarities among the elements belonging to large datasets, partitioning them in K distinct groups called clusters. The main weakness of this technique is that, in real problems, it is often impossible to define the value of K as input data. Furthermore, the large amount of data used for useful simulations makes impracticable the execution of the algorithm on traditional architectures. In this paper, we address the previous two issues. On the one hand, we propose a method to dynamically define the value of K by optimizing a suitable quality index with special care to the computational cost. On the other hand, to improve the performance and the effectiveness of the algorithm, we propose a strategy for parallel implementation on modern multicore CPUs. © 2020 Elsevier Inc.","Adaptive algorithm; K-means clustering; Multicore CPUs; Unsupervised learning"
"Cache simulation for irregular memory traffic on multi-core CPUs: Case study on performance models for sparse matrix–vector multiplication","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086381182&doi=10.1016%2fj.jpdc.2020.05.020&partnerID=40&md5=a6927a75c7378e17c4a48266bc8c668b","Parallel computations with irregular memory access patterns are often limited by the memory subsystems of multi-core CPUs, though it can be difficult to pinpoint and quantify performance bottlenecks precisely. We present a method for estimating volumes of data traffic caused by irregular, parallel computations on multi-core CPUs with memory hierarchies containing both private and shared caches. Further, we describe a performance model based on these estimates that applies to bandwidth-limited computations. As a case study, we consider two standard algorithms for sparse matrix–vector multiplication, a widely used, irregular kernel. Using three different multi-core CPU systems and a set of matrices that induce a range of irregular memory access patterns, we demonstrate that our cache simulation combined with the proposed performance model accurately quantifies performance bottlenecks that would not be detected using standard best- or worst-case estimates of the data traffic volume. © 2020 Elsevier Inc.","AMD Epyc; Cache simulation; Intel Xeon; Performance model; Sparse matrix–vector multiplication"
"FALCON-X: Zero-copy MPI derived datatype processing on modern CPU and GPU architectures","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085644081&doi=10.1016%2fj.jpdc.2020.05.008&partnerID=40&md5=faf8e8a4d872a09423f1fa2c0e54e110","This paper addresses the challenges of MPI derived datatype processing and proposes FALCON-X — A Fast and Low-overhead Communication framework for optimized zero-copy intra-node derived datatype communication on emerging CPU/GPU architectures. We quantify various performance bottlenecks such as memory layout translation and copy overheads for highly fragmented MPI datatypes and propose novel pipelining and memoization-based designs to achieve efficient derived datatype communication. In addition, we also propose enhancements to the MPI standard to address the semantic limitations. The experimental evaluations show that our proposed designs significantly improve the intra-node communication latency and bandwidth over state-of-the-art MPI libraries on modern CPU and GPU systems. By using representative application kernels such as MILC, WRF, NAS_MG, Specfem3D, and Stencils on three different CPU architectures and two different GPU systems including DGX-2, we demonstrate up to 5.5x improvement on multi-core CPUs and 120x benefits on DXG-2 GPU system over state-of-the-art designs in other MPI libraries. © 2020 Elsevier Inc.","CPU and GPU; Derived datatypes; HPC; MPI; NVIDIA DGX2"
"Matrix multiplication on batches of small matrices in half and half-complex precisions","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088025140&doi=10.1016%2fj.jpdc.2020.07.001&partnerID=40&md5=be85281bd26c7a0d5f282c8d297074a2","Machine learning and artificial intelligence (AI) applications often rely on performing many small matrix operations—in particular general matrix–matrix multiplication (GEMM). These operations are usually performed in a reduced precision, such as the 16-bit floating-point format (i.e., half precision or FP16). The GEMM operation is also very important for dense linear algebra algorithms, and half-precision GEMM operations can be used in mixed-precision linear solvers. Therefore, high-performance batched GEMM operations in reduced precision are significantly important, not only for deep learning frameworks, but also for scientific applications that rely on batched linear algebra, such as tensor contractions and sparse direct solvers. This paper presents optimized batched GEMM kernels for graphics processing units (GPUs) in FP16 arithmetic. The paper addresses both real and complex half-precision computations on the GPU. The proposed design takes advantage of the Tensor Core technology that was recently introduced in CUDA-enabled GPUs. With eight tuning parameters introduced in the design, the developed kernels have a high degree of flexibility that overcomes the limitations imposed by the hardware and software (in the form of discrete configurations for the Tensor Core APIs). For real FP16 arithmetic, performance speedups are observed against cuBLAS for sizes up to 128, and range between 1.5× and 2.5×. For the complex FP16 GEMM kernel, the speedups are between 1.7× and 7× thanks to a design that uses the standard interleaved matrix layout, in contrast with the planar layout required by the vendor's solution. The paper also discusses special optimizations for extremely small matrices, where even higher performance gains are achievable. © 2020 Elsevier Inc.","Batch linear algebra; GPU computing; Half precision; Matrix multiplication"
"Two Elementary Instructions make Compare-and-Swap","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088028765&doi=10.1016%2fj.jpdc.2020.06.005&partnerID=40&md5=df71242077312da5d1b5d1bf3c02e503","Herlihy showed that multiprocessors must support advanced atomic objects, such as compare-and-swap, to be able to solve any arbitrary synchronization task among any number of processes (Herlihy, 1991). Elementary objects such as read-write registers and fetch-and-add are fundamentally limited to at most two processes with respect to solving an arbitrary synchronization task. Later, it was also shown that simulating an advanced atomic object using elementary objects is impossible. However, Ellen et al. observed that the above impossibility assumes computation by synchronization objects instead of synchronization instructions applied on memory locations, which is how the actual multiprocessors compute (Ellen et al., 2016). Building on that observation, we show that two elementary instructions, such as max-write and half-max, can be much better than the advanced compare-and-swap instruction. Concretely, we show the following. • [1.] Half-max and max-write instructions are elementary, i.e., have consensus number one. • [2.] Half-max and max-write instructions can simulate compare-and-swap instruction in O(1) steps. • [3.] For a pipelined butterfly interconnect, concurrent throughput of half-max and max-write instructions exceeds the concurrent throughput of compare-and-swap by a factor n — the number of processes. • [4.] The family of instructions max-write-or-⊙ are also elementary, where ⊙ is a commutative and an associative operation. • [5.] It takes Ω(logn) steps to simulate max-write-or-add using compare-and-swap but O(1) steps to simulate compare-and-swap using max-write-or-add and half-max. © 2020 Elsevier Inc.","Compare-and-swap; Consensus numbers; Half-max; Max-write; Wait-free synchronization"
"A hybrid anomaly-based intrusion detection system to improve time complexity in the Internet of Energy environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087496960&doi=10.1016%2fj.jpdc.2020.06.012&partnerID=40&md5=eb66ce01a1cd464e14dc99b65c203944","The technological evolution of the smart grids is going to take its shape in the form of a new paradigm called the Internet of Energy (IoE); which is considered to be the convergence of internet, communication, and energy. Like other evolved technologies, the IoE inherits security vulnerabilities from its constituents that need to be addressed. Intrusion Detection Systems (IDS) have been used to counteract malicious attacks. Among the types of IDS, anomaly-based IDS that employ mostly machine learning algorithms are considered to be the promising one, owing to their capability of detecting zero-day attacks. However, using complex algorithms to detect attacks, the existing anomaly-based IDS designed for IoE require considerable amount of time. It is tempting to reduce the training and testing time in order to make the IDS feasible for the IoE architecture. In this paper, we propose a hybrid anomaly-based IDS that can be installed at any networked site of the IoE architecture, such as Advanced Metering Infrastructure (AMI), to counteract security attacks. Our proposed system reduces the overall classification time of detection compared to the existing hybrid methods. The proposed solution uses a combination of K-means and Support Vector Machine, where the K-means centroids are used in a unique training method that reduces the training and testing times of the Support Vector Machine without compromising classification performance. We choose the best value of “k” and fine-tuned the SVM for best anomaly detection. Our approach achieves the highest accuracy of 99.9% in comparison with the existing approaches. © 2020 Elsevier Inc.","Anomaly-based intrusion detection; Internet of Energy; Intrusion detection; Machine learning; Smart grid"
"Coalition formation for deadline-constrained resource procurement in cloud computing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096706416&doi=10.1016%2fj.jpdc.2020.10.004&partnerID=40&md5=04e86b099bc7066aa3bca8591289870d","To attract more customers, a cloud provider tends to give some discounts to a customer if he/she rents a plenty of resources. Under this situation, a group of customers who need homogeneous cloud instances with various deadlines are prone to purchasing resources in a collaborative manner, i.e., using a coalition game, to reduce purchase costs. It is essential to design a mechanism that enables all customers to voluntarily and happily collaborate while ensuring that each customer pays at the lowest cost possible. To address this issue, we propose a mechanism to show collaborative interactions between customers and determine the number of service programs purchased from each provider to charge each cloud customer a minimum cost. We establish a coalition game based on multi-customer resource procurement and prove that there exists a unique optimal solution in the coalition game, while satisfying individual stability and group stability. In addition, the optimal solution is a solution in which the selected service program of each coalition optimizes the cost per customer and maximizes resource utilization. We propose a heuristic Deadline-constrained Resource Coalition Allocation (DRCA) algorithm to calculate the near-optimal solution. A backtracking algorithm is proposed to calculate the pseudo maximum resource utilization of the provided programs by improving the rectangular packing. Extensive experiments are performed to verify the feasibility and effectiveness of the proposed algorithm. © 2020 Elsevier Inc.","Cloud computing; Coalition game; Rectangular packing; Resource procurement"
"Hybrid KNN-join: Parallel nearest neighbor searches exploiting CPU and GPU architectural features","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097793118&doi=10.1016%2fj.jpdc.2020.11.004&partnerID=40&md5=6733b3755e115ac67ff347e7983ce560","K Nearest Neighbor (KNN) joins are used in scientific domains for data analysis, and are building blocks of several well-known algorithms. KNN-joins find the KNN of all points in a dataset. This paper focuses on a hybrid CPU/GPU approach for low-dimensional KNN-joins, where the GPU may not yield substantial performance gains over parallel CPU algorithms. We utilize a work queue that prioritizes computing data points in high density regions on the GPU, and low density regions on the CPU, thereby taking advantage of each architecture's relative strengths. Our approach, HYBRIDKNN-JOIN, effectively augments a state-of-the-art multi-core CPU algorithm. We propose optimizations that (i) maximize GPU query throughput by assigning the GPU large batches of work; (ii) increase workload granularity to optimize GPU utilization; and, (iii) limit load imbalance between CPU and GPU architectures. We compare HYBRIDKNN-JOIN to one GPU and two parallel CPU reference implementations. Compared to the reference implementations, we find that the hybrid algorithm performs best on larger workloads (dataset size and K). The methods employed in this paper show promise for the general division of work in other hybrid algorithms. © 2020 Elsevier Inc.","GPGPU; Heterogeneous systems; In-memory database; Nearest neighbor search; Query optimization"
"IoT architecture for adaptation to transient devices","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092445191&doi=10.1016%2fj.jpdc.2020.09.012&partnerID=40&md5=fe3e56752e1b670076b4c4bb50941e89","IoT environments are continuously changing. Changes may come from the service, connectivity, or physical layers of the IoT architecture. Therefore, to function appropriately, the system needs to dynamically adapt to its environment. In previous work, we posited eight challenges to foster adaptation through all architecture layers of IoT systems. In this paper, we address the challenges to manage the inclusion of new devices and devices’ transient connection, by means of dynamic adaptations incorporated into our proposed software architecture for adaptive IoT systems. To manage dynamic adaptations, we extend the reference IoT architecture with our specialized components. In particular, we use (1) ontologies and instances to represent the domain knowledge; (2) a matching algorithm to pair services and IoT devices, taking into account their functional requirements, quality attributes and sensors properties; and (3) a match update algorithm used whenever sensors become (un)available. We evaluate the effectiveness of our solution with respect to the accuracy of matching services and IoT devices, and the response to environment changes. © 2020 Elsevier Inc.","Dynamic adaptation; Instance matching; Internet of Things; Transient systems"
"A blockchain-based Roadside Unit-assisted authentication and key agreement protocol for Internet of Vehicles","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096718844&doi=10.1016%2fj.jpdc.2020.11.003&partnerID=40&md5=82ada4aea9fc1be9babe7ff6c8dc1eaa","A fundamental layer of smart cities, the Internet of Vehicles (IoV) can significantly improve transportation efficiency, reduce energy consumption, and traffic accidents. However, because of the vehicle and the RoadSide Units (RSU) use wireless channels for communication, the risk of information being leaked or tampered is highly increased. Therefore, secure and reliable authentication and key agreement protocol is the masterpiece of IoV security. As most of the existing authentication protocols pertain to a centralized structure and single Trusted Authority (TA) network model, all vehicles involved can only perform mutual authentication with one TA through the intermediate node RSU, and thus, the efficiency of these centralized authentication protocols is easily affected by TA's communication and computing resource bottlenecks. In this article, a blockchain-based authentication and key agreement protocol is designed for the multi-TA network model, moving the computing load of TA down to the RSU to improve the efficiency of authentication. In addition, blockchain technology is used for multiple TAs to manage the ledger that stores vehicle-related information, which results in vehicles that can easily achieve cross-TA authentication. Both formal and informal security analysis and simulation results from ProVerif show that the proposed protocol is secure. Comparisons with other existing work show that the proposed protocol has less computational overhead, higher authentication efficiency, and can resist various common attacks. © 2020 Elsevier Inc.","Authentication; Cryptography; Internet of vehicles; Key agreement"
"Knowledge-driven machine learning based framework for early-stage disease risk prediction in edge environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088654909&doi=10.1016%2fj.jpdc.2020.07.003&partnerID=40&md5=b3ab6288d08cab56807f91b6c3ec6651","Early-stage disease risk prediction can be beneficial to improve the health of the mass and can reduce the economic burden of late treatment. Machine learning has played a pivotal role in predictive systems, which requires achieving a specific degree of accuracy for healthcare systems. Most recently researchers have found the necessity of bridging between epidemiology and machine learning classifications toward health risk prediction. This work proposes an epidemiology knowledge-driven unique model that follows the principle of association rule-based ontology to select features and classification techniques. The goal of this approach is to generalize a framework for future robust systems to predict the likelihood of diseases, which can be executed in the edge computing environment. The framework introduces epidemiological library and structured attribute set along with the library of precaution to derive the disease risk-prediction process. To investigate the adoption of the epidemiology knowledge-driven model, we considered a real dataset of early-stage likelihood prediction of diabetes and carried out a set of experiments for highlighting the significance of several epidemiological factors. The classification aspect of the framework is further compared with widely accepted approaches for machine learning based healthcare, which shows the novelty of the proposed model. © 2020 Elsevier Inc.","Disease likelihood; Epidemiology; Healthcare; Machine learning; Self-screening"
"CScript: A distributed programming language for building mixed-consistency applications","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086441831&doi=10.1016%2fj.jpdc.2020.05.010&partnerID=40&md5=54354dafc6a146dc0799618fc8ea595d","Current programming models only provide abstractions for sharing data under a homogeneous consistency model. It is, however, not uncommon for a distributed application to provide strong consistency for one part of the shared data and eventual consistency for another part. Because mixing consistency models is not supported by current programming models, writing such applications is extremely difficult. In this paper we propose CScript, a distributed object-oriented programming language with built-in support for data replication. At its core are consistent and available replicated objects. CScript regulates the interactions between these objects to avoid subtle inconsistencies that arise when mixing consistency models. Our evaluation compares a collaborative text editor built atop CScript with a state-of-the-art implementation. The results show that our approach is flexible and more memory efficient. © 2020 Elsevier Inc.","Consistency models; Distribution; Replicated data types"
"CIC-PIM: Trading spare computing power for memory space in graph processing","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091679135&doi=10.1016%2fj.jpdc.2020.09.008&partnerID=40&md5=f95d3b296000b5edd8d6cb51e103acc1","Shared-memory graph processing is usually more efficient than in a cluster in terms of cost effectiveness, ease of programming and runtime. However, the limited memory capacity of a single machine and the huge sizes of graphs restrains its applicability. Hence, it is imperative to reduce memory footprint. We observe that index compression holds promise and propose CIC-PIM, a lightweight encoding with chunked index compression, to reduce the memory footprint and the runtime of graph algorithms. CIC-PIM aims for significant space saving, real random-access support and high cache efficiency by exploiting the ubiquitous power-law and sparseness features of large scale graphs. The basic idea is to divide index structures into chunks of appropriate size and compress the chunks with our lightweight fixed-length byte-aligned encoding. After CIC-PIM compression, two-fold larger graphs are processed with all data fit in memory, resulting in speedups or fast in-memory processing unattainable previously. © 2020 Elsevier Inc.","Graph processing; Index compression; Parallel processing; Shared-memory"
"Evaluation of memory performance in NUMA architectures using Stochastic Reward Nets","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086383346&doi=10.1016%2fj.jpdc.2020.05.022&partnerID=40&md5=438619c1773dcae0371a1038a1465e07","Understanding memory performance in multi-core platforms is a prerequisite to perform optimizations. To this end, this paper presents analytical models based on Stochastic Reward Nets (SRNs) to model and evaluate the memory performance of Non-Uniform Memory Access (NUMA) multi-core architectures. The approach considers the details of the architecture and first proposes a monolithic SRN model that evaluates the memory performance in terms of the mean memory response time. Since the monolithic model incurs a state space explosion with an increasing number of cores and memory controllers, two approximate models are presented that are able to evaluate large-scale NUMA architectures. The SRNs are validated through measurements on two NUMA multi-core platforms, a 64-core AMD Opteron server and a 72-core Intel system. The results demonstrate the ability of the proposed models to accurately compute the mean memory response time on NUMA architectures. The results also provide valuable information that runtime systems and application designers can use to optimize execution of parallel applications on such architectures. © 2020 Elsevier Inc.","Approximate models; NUMA architectures; Performance modeling; Stochastic Reward Nets"
"Learning-based dynamic cache management in a cloud","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087402633&doi=10.1016%2fj.jpdc.2020.06.013&partnerID=40&md5=719d4f3f7fdaa90b70040c78079e2da5","Caches are an important component of modern computing systems given their significant impact on performance. In particular, caches play a key role in the cloud due to the nature of large-scale, data-intensive processing. One of the key challenges for the cloud providers is how to share the caching capacity among tenants, under the circumstance that each often requires a different degree of quality of service (QoS) with respect to data access performance. The invariant is that the individual tenants’ QoS requirements should be satisfied while the cache usage is optimized in a system-wide manner. In this paper, we introduce a learning-based approach for dynamic cache management in a cloud, which is based on the estimation of data access pattern of a tenant and the prediction of cache performance for the access pattern in question. We consider a variety of probability distributions to estimate the data access pattern, and examine a set of learning-based regression techniques to predict the cache hit rate for the access pattern. The predicted cache hit rate is then used to make a decision whether reallocating cache space is needed to meet the QoS requirement for the tenant. Our experimental results with an extensive set of synthetic traces and the YCSB benchmark show that the proposed method consistently optimizes the cache space while satisfying the QoS requirement. © 2020 Elsevier Inc.","Access pattern estimation; Cache size prediction; Cloud cache management; Dynamic caching; Machine learning"
"A decentralized algorithm to combine topology control with network coding","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098202473&doi=10.1016%2fj.jpdc.2020.12.001&partnerID=40&md5=b3dcb656460173763b1950acfc9020fd","Network coding and topology control techniques have been widely used to increase throughput and improve the lifetime of Wireless Sensor Networks (WSNs). This paper considers the simultaneous utilization of these techniques in a WSN and proposes convex non-linear programming. Since solving the problem for a large-scale and dynamic WSN is impractical and almost impossible, Lagrangian, sub-gradient and the decomposition methods are employed to provide a decentralized algorithm. In the proposed algorithm, a node makes the computations by acquiring local knowledge and information from its neighbors. This paper provides a mathematical language to build an analytic foundation for the design of a modularized and decentralized algorithm that provides transmission ranges and routes for a WSN. The simulation results show that increasing the number of sources, sinks, sensors, and traffic load leads to improving the lifetime which is acquired by the proposed algorithm. © 2020 Elsevier Inc.","Lifetime; Multicast; Network coding; Topology control; Wireless Sensor Network"
"Data placement in distributed data centers for improved SLA and network cost","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090134507&doi=10.1016%2fj.jpdc.2020.07.006&partnerID=40&md5=82e0514927dd543df6ab9f54827e9bd8","Large-scale data-intensive applications provide services to users by routing service requests to geographically distributed data centers interconnected by Internet links. In order to achieve good reliability and data access latency performance, cloud service providers often simultaneously place multiple copies of the data in different data centers. The network communication required for updating the multiple data copies incurs an operational cost. At the same time, the penalty incurred by the Service Level Agreement (SLA) violation for data access from the data centers also imposes an operational cost on the service providers. In this paper, we tackle the problem of data placement in distributed data centers with the aim to minimize the operational cost incurred by delay SLA violation penalty and inter-data center network communication, assuming each data has K data replicas. We propose a K-level Cluster-based Data Placement algorithm (K-CDP) for the problem. The algorithm solves the linear programming relaxation and dual programming problems corresponding to the problem of minimizing SLA violation penalty cost caused by placing a replica of each data in a data center. Based on the obtained solutions, the algorithm clusters the data so that the data with similar placeable data centers form a data cluster. For the data in each cluster, the algorithm selects K data centers to minimize the operational cost. We prove that algorithm K-CDP is 2-approximation to the data placement problem. Our simulation results demonstrate that the proposed algorithm can effectively reduce the penalty cost incurred by delay SLA violation, the network communication cost, and the operational cost of data centers. © 2020 Elsevier Inc.","Data placement; Latency; Network cost; Operational cost; SLA"
"A multi-GPU biclustering algorithm for binary datasets","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092107095&doi=10.1016%2fj.jpdc.2020.09.009&partnerID=40&md5=590d7e9d04c63fee81daadedc01a986f","Graphics Processing Units technology (GPU) and CUDA architecture are one of the most used options to adapt machine learning techniques to the huge amounts of complex data that are currently generated. Biclustering techniques are useful for discovering local patterns in datasets. Those of them that have been implemented to use GPU resources in parallel have improved their computational performance. However, this fact does not guarantee that they can successfully process large datasets. There are some important issues that must be taken into account, like the data transfers between CPU and GPU memory or the balanced distribution of workload between the GPU resources. In this paper, a GPU version of one of the fastest biclustering solutions, BiBit, is presented. This implementation, named gBiBit, has been designed to take full advantage of the computational resources offered by GPU devices. Either using a single GPU device or in its multi-GPU mode, gBiBit is able to process large binary datasets. The experimental results have shown that gBiBit improves the computational performance of BiBit, a CPU parallel version and an early GPU version, called ParBiBit and CUBiBit, respectively. gBiBit source code is available at https://github.com/aureliolfdez/gbibit. © 2020 Elsevier Inc.","Biclustering; Binary; CUDA; GPU"
"An Energy Efficient Adaptive Scheduling Scheme (EASS) for Mesh Grid Wireless Sensor Networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089845091&doi=10.1016%2fj.jpdc.2020.08.007&partnerID=40&md5=fab34b717a3489b1d0e278bab939f1bf","The technology-driven shift in distributed network systems, Internet technology and energy grid digitization have changed the traditional spoke-hub distribution models to the more flexible and giant network-to-networks paradigm known as Energy Cloud. This energy cloud paradigm has the ability to adapt to the large-scale distributed energy resources with dynamic demand–response associated with smart grid and smart homes applications, which allows customers/users to have connectivity of electrical devices with supply grid into one giant energy cloud network. If efficient policies are not adapted in the design and operations of such networks, sensor devices may sense and forward redundant data packets. Hence, valuable energy of the node will be depleted quickly and as a result; the network will be down before accomplishing its intended task. To address the issue of energy-efficiency in energy-cloud at embedded networks, this paper proposes a novel scheme, “Energy Efficient Adaptive Scheduling Scheme (EASS) for Mesh Grid Wireless Sensor Networks”. In EASS, a sensor node configures and schedules its functions/roles according to the contents of sensed data packets and frequency of generated traffic. Like energy cloud, tasks are uniformly distributed on all nodes in mesh-grid in four states – each state of a node is responsible for configuring its components on a specific energy level – which aims to avoid link disconnection and vanish redundant data packets generation. Simulation results show that EASS increases energy-efficiency by 50% due to the four-states model, 62.5% due to alive nodes, 92.60% due to minimized Cluster-Heads overhead and 38.11% due to the reduction in dead nodes ratio. © 2020 Elsevier Inc.","Adaptive; Energy-cloud; Energy-efficiency; Four-states; Mesh grid"
"QoS provision in hierarchical and non-hierarchical switch architectures","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096656835&doi=10.1016%2fj.jpdc.2020.10.009&partnerID=40&md5=b059d0d09488b4b7e0f9a02196e2a97c","Quality of service (QoS) provision has become an important aspect of high-performance computing interconnection networks. Proof of that is the inclusion of mechanisms targeted to the provision of QoS by the main interconnection technologies such as Gigabit Ethernet, Infiniband (IB) and Omni-Path (OPA). A key component of QoS provision is the output scheduling algorithm, which determines when a packet should be transmitted. An ideal scheduling algorithm should satisfy two main properties: good end-to-end latency and implementation simplicity. Table-based schedulers are able to provide these two properties, and because of this, IB and OPA have implemented this approach. In this paper, we present a comparative study in terms of QoS provision between these two dominating interconnection technologies. Those interconnection technologies are also two examples of non-hierarchical and hierarchical switch architectures, respectively, which gives the results of this study greater significance. In order to carry out the study, the Deficit Table scheduler (DTable) has been used. DTable is a table-based scheduling algorithm which offers a good balance between end-to-end latency and implementation cost. © 2020 Elsevier Inc.","Hierarchical and non-hierachical switches; Interconnection networks simulation; Performance evaluation; Quality of Service; Scheduling algorithms"
"PfTouch: Concurrent page-fault handling for Intel restricted transactional memory","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087496958&doi=10.1016%2fj.jpdc.2020.06.009&partnerID=40&md5=af4665af059be750b6ceb0e51aede0c4","Page faults occurring within transactions jeopardize concurrency in Intel Restricted Transactional Memory (RTM). To make progress in spite of page-fault-induced aborts, the program must resort to the non-speculative fallback path and re-execute the affected transaction. Since the atomicity of a non-speculative transaction is guaranteed by impeding the execution of any other speculative transactions until the former completes, taking the fallback path is particularly harmful for performance. Therefore, such page-fault-induced aborts currently lead to thread serialization during the potentially long period of time taken to resolve them. In this work we propose PfTouch, a simple extension to RTM that allows page-fault handling to be moved out of non-speculative transactional execution in mutual exclusion. Our proposal sidesteps taking the fallback path in these cases and thus avoids its associated performance loss, by triggering page faults in the abort handler while other speculative transactions can run concurrently. PfTouch requires minimal modifications in the Intel RTM specification and keeps the OS unaltered. Through full-system simulation, we show that PfTouch achieves average reductions in execution time of 7.7% (up to 24.4%) for the STAMP benchmarks, closely matching the performance of the more complex suspended transactional mode in the IBM Power ISA. © 2020 Elsevier Inc.","Fallback path; Intel TSX; Page fault; Serialization; Transactional memory"
"Compact self-stabilizing leader election for general networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086802506&doi=10.1016%2fj.jpdc.2020.05.019&partnerID=40&md5=664ad24694698fcfed60df9e3b84e64f","We present a self-stabilizing leader election algorithm for general networks, with space-complexity O(logΔ+loglogn) bits per node in n-node networks with maximum degree Δ. This space complexity is sub-logarithmic in n as long as Δ=no(1). The best space-complexity known so far for general networks was O(logn) bits per node, and algorithms with sub-logarithmic space-complexities were known for the ring only. To our knowledge, our algorithm is the first algorithm for self-stabilizing leader election to break the Ω(logn) bound for silent algorithms in general networks. Breaking this bound was obtained via the design of a (non-silent) self-stabilizing algorithm using sophisticated tools such as solving the distance-2 coloring problem in a silent self-stabilizing manner, with space-complexity O(logΔ+loglogn) bits per node. Solving this latter coloring problem allows us to implement a sub-logarithmic encoding of spanning trees — storing the IDs of the neighbors requires Ω(logn) bits per node, while we encode spanning trees using O(logΔ+loglogn) bits per node. Moreover, we show how to construct such compactly encoded spanning trees without relying on variables encoding distances or number of nodes, as these two types of variables would also require Ω(logn) bits per node. © 2020 Elsevier Inc.","Coloring; Leader election; Self-stabilisation; Spanning tree construction"
"BPS: A reliable and efficient pub/sub communication model with blockchain-enhanced paradigm in multi-tenant edge cloud","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085552783&doi=10.1016%2fj.jpdc.2020.05.005&partnerID=40&md5=c7e933e6987ef1492702a7827a30ee34","In recent years, with the rapid development of smart city, prevalent pub/sub (publish/subscribe) streaming systems have been increasingly employed as upstream middleware layer in multi-tenant edge clouds, and feed large volume of data gathered from IoT devices of different tenants into downstream systems (e.g., data analytics and warehouse). A shared tenancy model where multiple untrusted applications or tenants utilize the same pub/sub system is generally exploited in edge cloud, which poses crucial challenges including privacy-sensitive data/metadata access threat and critical metadata modification by unauthorized tenants. A centralized monitoring node is invariably adopted in existing security strategies (such as ACL, TLS), which causes the pub/sub streaming model vulnerable to external malicious attacks and single point failure. In this paper, inspired by outstanding features of blockchain including tamper-resistance, decentralization, strong consistency, and traceability, we propose BPS, a general and decentralized Blockchain-enhanced Pub/Sub communication model for multi-tenant edge cloud, to redesign pub/sub system internal security mechanisms. Specifically, by exploiting blockchain technology, BPS can detect the illegal operations and behaviors from both malicious tenants and untrusted publishers or subscribers. BPS directly leverages Merkel Hash Tree (MHT) of blockchain to verify the integrity of critical and confidential metadata. Regarding authorization, BPS introduces smart-contract-enabled fine-grained control over partition topic-classified messages by storing access control list (ACL) into an append-only blockchain ledger. Additionally, an incentive mechanism is employed in BPS to reward honest publishers and subscribers. We implement BPS prototype based on Kafka and EoS blockchain. Our security analysis and extensive experiments demonstrate that BPS outperforms the state-of-the-art pub/sub streaming system Kafka in security with minimal performance overhead. © 2020 Elsevier Inc.","Blockchain; Edge cloud; Pub/sub system; Security; Smart city"
"Effective replica management for improving reliability and availability in edge-cloud computing environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085274390&doi=10.1016%2fj.jpdc.2020.04.012&partnerID=40&md5=cc07711f8ede5e2ece1961a21bed86d9","The multi-replica strategy can create multiple data replicas for the edge cloud system and store them in different DataNodes, which improves data availability and data service quality. However, the storage resources of DataNodes are limited and the user demand for data is time-varying, the unreasonable number of data replicas will cause a high storage burden on the file system or low data service quality. Therefore, the number of data replicas needs to be dynamically adjusted according to the actual situation. Based on this, a dynamic replica creation strategy based on the gray Markov chain is proposed. If the number of replicas needs to be increased, the newly added replicas need to be placed on the DataNodes. Considering the problem of load balancing of the DataNode during replica placement, this paper proposes a replica placement strategy based on the Fast Non-dominated Sorting Genetic algorithm. In addition, considering the problem of data replica synchronization and the data recovery of failed DataNodes in the edge cloud system, this paper proposes a delay-adaptive replica synchronization strategy and a load-balancing based replica recovery strategy. Finally, the experiments prove the effectiveness of the proposed strategies. © 2020 Elsevier Inc.","Edge-cloud; Replica placement; Replica synchronization"
"Leveraging InfiniBand controller to configure deadlock-free routing engines for Dragonflies","2021","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090122934&doi=10.1016%2fj.jpdc.2020.07.010&partnerID=40&md5=f252773326c6dcae10ced8dd8396ec0a","The Dragonfly topology is currently one of the most popular network topologies in high-performance parallel systems. The interconnection networks of many of these systems are built from components based on the InfiniBand specification. However, due to some constraints in this specification, the available versions of the InfiniBand network controller (OpenSM) do not include routing engines based on some popular deadlock-free routing algorithms proposed theoretically for Dragonflies, such as the one proposed by Kim and Dally based on Virtual-Channel shifting. In this paper we propose a straightforward method to integrate this routing algorithm in OpenSM as a routing engine, explaining in detail the configuration required to support it. We also provide experiment results, obtained both from a real InfiniBand-based cluster and from simulation, to validate the new routing engine and to compare its performance and requirements against other routing engines currently available in OpenSM. © 2020 Elsevier Inc.","Deadlock freedom; Dragonfly; InfiniBand; Routing"
"Blockchain-based verification framework for data integrity in edge-cloud storage","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088524855&doi=10.1016%2fj.jpdc.2020.06.007&partnerID=40&md5=65888301452f6799bcad0165c17c9420","With the popularity of the Internet of Things (IoT), data integrity verification in the edge cloud storage attracts attentions from many researchers. Due to the over dependence of the Third Party Auditor (TPA) and the dynamical nature of the IoT data, the traditional data integrity verification framework for cloud storage can hardly work. To satisfy the characteristics of the IoT and avoid the over dependence of the TPA, we propose a blockchain-based framework without TPA for data integrity verification in a decentralized edge-cloud storage (ECS) scenario in this paper. In our framework, we employ the Merkle tree with random challenging numbers for data integrity verification and analyze different Merkle tree structures to optimize the system performance. To solve the problem of limited resources and high real-time requirements, we further propose sampling verification and develop rational sampling strategies to make sampling verification more effective. The overhead and precision of the verification in ECS are studied by an optimal sample size strategy. Finally, a prototype system is implemented based on our framework. We conduct a series of experiments to evaluate the effectiveness of the proposed schemes. The experimental results show that our schemes can effectively improve the performance of data integrity verification. © 2020 Elsevier Inc.","Blockchain; Data integrity verification; Edge-cloud storage; Merkle trees; Sampling"
"Proof of witness presence: Blockchain consensus for augmented democracy in smart cities","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.06.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087621573&doi=10.1016%2fj.jpdc.2020.06.015&partnerID=40&md5=dc5b2d02f61b6067b1353e954b79819b","Smart Cities evolve into complex and pervasive urban environments with a citizens’ mandate to meet sustainable development goals. Repositioning democratic values of citizens’ choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens’ trust and overcome security vulnerabilities of GPS localization. Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens’ choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities, which are compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined. © 2020 Elsevier Inc.","Augmented democracy; Blockchain; Consensus mechanism; Smart City; Witness presence"
"Efficient convolution pooling on the GPU","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078126305&doi=10.1016%2fj.jpdc.2019.12.006&partnerID=40&md5=ae111a9a0408d9382d432d45921638a4","The main contribution of this paper is to show efficient implementations of the convolution-pooling in the GPU, in which the pooling follows the multiple convolution. Since the multiple convolution and the pooling operations are performed alternately in earlier stages of many Convolutional Neural Networks (CNNs), it is very important to accelerate the convolution-pooling. Our new GPU implementation uses two techniques, (1) convolution interchange with direct sum, and (2) conversion to matrix multiplication. By these techniques, the computational and memory access cost are reduced. Further the convolution interchange is converted to matrix multiplication, which can be computed by cuBLAS very efficiently. Experimental results using Tesla V100 GPU show that our new GPU implementation compatible with cuDNN for the convolution-pooling is expected 2.90 times and 1.43 times faster for fp32 and fp16 than the multiple convolution and then the pooling by cuDNN, respectively. the most popular library of primitives to implement the CNNs in the GPU. © 2019 Elsevier Inc.","Average pooling; Convolution; Deep learning; GPU; Neural Networks"
"Load balancing of energy cloud using wind driven and firefly algorithms in internet of everything","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083355784&doi=10.1016%2fj.jpdc.2020.02.010&partnerID=40&md5=ab1d2b3f25b14c7e1b90ee09eb8703a8","The smart applications dominating the planet in the present day and age, have innovatively progressed to deploy Internet of Things (IoT) based systems and related infrastructures in all spectrums of life. Since, variety of applications are being developed using this IoT paradigm, there is an immense necessity for storing data, processing them to get meaningful information and render suitable services to the end-users. The “thing” in this decade is not only a smart sensor or a device; it can be any physical or household object, a smart device or a mobile. With the ever increasing rise in population and smart device usage in every sphere of life, when all of such “thing”s generates data, there is a chance of huge data traffic in the internet. This could be handled only by integrating “Internet of Everything (IoE)” paradigm with a completely diversified technology — Cloud Computing. In order to handle this heavy flow of data traffic and process the same to generate meaningful information, various services in the global environment are utilized. Hence the primary focus revolves in integrating these two diversified paradigm shifts to develop intelligent information processing systems. Energy Efficient Cloud Based Internet of Everything (EECloudIoE) architecture is proposed in this study, which acts as an initial step in integrating these two wide areas thereby providing valuable services to the end users. The utilization of energy is optimized by clustering the various IoT network using Wind Driven Optimization Algorithm. Next, an optimized Cluster Head (CH) is chosen for each cluster, using Firefly Algorithm resulting in reduced data traffic in comparison to other non-clustering schemes. The proposed clustering of IoE is further compared with the widely used state of the art techniques like Artificial Bee Colony (ABC) algorithm, Genetic Algorithm (GA) and Adaptive Gravitational Search algorithm (AGSA). The results justify the superiority of the proposed methodology outperforming the existing approaches with an increased life-time and reduction in traffic. © 2020 Elsevier Inc.","Energy cloud; Firefly algorithm; Green communication; Internet of everything (IoE); Wind driven algorithm"
"Scalable and energy efficient wireless inter chip interconnection fabrics using THz-band antennas","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079894811&doi=10.1016%2fj.jpdc.2020.02.002&partnerID=40&md5=65204d41ddc7b87054d260af110e3308","Computing platforms ranging from embedded systems to server blades comprise of multiple Systems-on-Chips (SoCs). Conventionally, communication between chips in these multichip platforms are realized using high-speed I/O modules over metal traces on a substrate. Due to the high-power consumption of I/O modules and non-scalable pitch of pins or solder bumps their bandwidth density and power consumption becomes bottleneck for multichip systems. Wireless chip-to-chip communication is emerging as an alternative solution to the traditional interconnection challenges of multichip systems. Novel devices based on graphene structures capable of establishing wireless links are explored in recent literature to provide high bandwidth THz links. In this work, we propose to utilize graphene-based wireless links to enable energy-efficient, multi-modal chip-to-chip communication protocol to create toroidal folding based interconnection architectures for multichip systems. With cycle-accurate simulations we demonstrate that such designs can outperform state-of-the-art wireline multichip systems. © 2020 Elsevier Inc.","Graphene-based antenna; Multichip system; THz wireless; Wireless interconnects"
"Decomposition of augmented cubes into regular connected pancyclic subgraphs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083025830&doi=10.1016%2fj.jpdc.2020.03.017&partnerID=40&md5=02c16aaaf4b3e5d56c0c21c0a0682981","In this paper, we consider the problem of decomposing the augmented cube AQn into two spanning, regular, connected and pancyclic subgraphs. We prove that for n≥4 and 2n−1=n1+n2 with n1,n2≥2,AQn can be decomposed into two spanning subgraphs H1 and H2 such that Hi is ni-regular and ni-connected for i=1,2. Moreover, Hi is 4-pancyclic if ni≥3. © 2020 Elsevier Inc.","Augmented cube; Hypercube; n-connected; r-pancyclic; Spanning subgraphs"
"Parallel implementation of the Image Block Representation using OpenMP","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075727725&doi=10.1016%2fj.jpdc.2019.11.006&partnerID=40&md5=7b377985f31bd079c2a16e10d97a617a","Herein, a parallel implementation in OpenMP of the Image Block Representation (IBR) for binary imagesis investigated. The IBR is a region-based image representation scheme that represents the binary image as a set of non-overlapping rectangular areas with object level, called blocks. The IBR permits the execution of operations on image areas instead of image points and therefore leads to a substantial reduction of the required computational complexity. The experimental and the analytically derived results from parallel implementation in OpenMP, on a multicore computer, proved that a very good overall performance can be achieved. © 2019 Elsevier Inc.","Image Block Representation; Karp–Flatt metric; OpenMP; Parallel algorithms; Parallel computing"
"Blockchain 3.0 applications survey","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077736981&doi=10.1016%2fj.jpdc.2019.12.019&partnerID=40&md5=d1f00cd0947a54dba91b19bbb00eb628","In this paper we survey a number of interesting applications of blockchain technology not related to cryptocurrencies. As a matter of fact, after an initial period of application to cryptocurrencies and to the financial world, blockchain technology has been successfully exploited in many other different scenarios, where its unique features allowed the definition of innovative and sometimes disruptive solutions. In particular, this paper takes into account the following application scenarios: end-to-end verifiable electronic voting, healthcare records management, identity management systems, access control systems, decentralized notary (with a focus on intellectual property protection) and supply chain management. For each of these, we firstly analyse the problem, the related requirements and the advantages the adoption of blockchain technology might bring. Then, we present a number of relevant solutions proposed in the literature both by academia and companies. © 2020 Elsevier Inc.","Blockchain; Distributed Applications; DLT; Smart contracts"
"Efficient task pruning mechanism to improve robustness of heterogeneous computing systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083516237&doi=10.1016%2fj.jpdc.2020.03.018&partnerID=40&md5=aa6b4dd69ed47973db795b23cd0c07f5","In heterogeneous distributed computing (HC) systems, diversity can exist in both computational resources and arriving tasks. In an inconsistently heterogeneous computing system, task types have different execution times on heterogeneous machines. A method is required to map arriving tasks to machines based on machine availability and performance, maximizing the number of tasks meeting deadlines (defined as robustness). For tasks with hard deadlines (e.g., those in live video streaming), tasks that miss their deadlines are dropped. The problem investigated in this research is maximizing the robustness of an oversubscribed HC system. A way to maximize this robustness is to prune (i.e., defer or drop) tasks with low probability of meeting their deadlines to increase the probability of other tasks meeting their deadlines. In this paper, we first provide a mathematical model to estimate a task's probability of meeting its deadline in the presence of task dropping. We then investigate methods for engaging probabilistic dropping. We propose methods to dynamically determine task dropping and deferring threshold probabilities. Next, we develop a pruning system and a pruning-aware mapping heuristic, which we extend to engender fairness across various task types. We present the pruning mechanism as an independent component that can be applied to any mapping heuristic to improve the system robustness. To reduce overhead of the pruning mechanism, we propose approximation methods that remarkably reduce the number of mathematical calculations and improve the practicality of deploying the mechanism in heterogeneous or even homogeneous computing systems. We show the cost and energy gains of the pruning mechanism. Simulation results, harnessing a selection of mapping heuristics, show efficacy of the pruning mechanism in improving robustness (on average by ≃22%) and cost in an oversubscribed HC system by up to ≃33%. © 2020 Elsevier Inc.","Heterogeneous computing (HC); Mapping heuristic; Probabilistic pruning; Robustness"
"Hybridization of firefly and Improved Multi-Objective Particle Swarm Optimization algorithm for energy efficient load balancing in Cloud Computing environments","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083422873&doi=10.1016%2fj.jpdc.2020.03.022&partnerID=40&md5=50a1edd6d970d196e8e61bc78b8560db","Load balancing, in Cloud Computing (CC) environment, is defined as the method of splitting workloads and computing properties. It enables the enterprises to manage workload demands or application demands by distributing the resources among computers, networks or servers. In this research article, a new load balancing algorithm is proposed as a hybrid of firefly and Improved Multi-Objective Particle Swarm Optimization (IMPSO) technique, abbreviated as FIMPSO. This technique deploys Firefly (FF) algorithm to minimize the search space where as the IMPSO technique is implemented to identify the enhanced response. The IMPSO algorithm works by selecting the global best (gbest) particle with a small distance of point to a line. With the application of minimum distance from a point to a line, the gbest particle candidates could be elected. The proposed FIMPSO algorithm achieved effective average load for making and enhanced the essential measures like proper resource usage and response time of the tasks. The simulation outcome showed that the proposed FIMPSO model exhibited an effective performance when compared with other methods. From the simulation outcome, it is understood that the FIMPSO algorithm yielded an effective result with the least average response time of 13.58ms, maximum CPU utilization of 98%, memory utilization of 93%, reliability of 67% and throughput of 72% along with a make span of 148, which was superior to all the other compared methods. © 2020 Elsevier Inc.","Cloud computing; Firefly; IMPSO; Load balancing; Task scheduling"
"Structured multi-block grid partitioning using balanced cut trees","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077682463&doi=10.1016%2fj.jpdc.2019.12.010&partnerID=40&md5=0aa9ccf1a2fcf359f2af2c24e57e9587","An algorithm to partition structured multi-block hexahedral grids for a load balanced assignment of the partitions to a given number of bins is presented. It uses a balanced hierarchical cut tree data structure to partition the structured blocks into structured partitions. The refinement of the cut tree attempts to generate equally shaped partitions with a low amount of additional surface. A multi-block load balancing approach is presented that guarantees to satisfy an upper bound of load imbalance. The partition quality of the algorithm is compared to established recursive edge bisection approaches and an unstructured partitioning using METIS. Two generic and two turbomachinery test cases demonstrate the superior quality and fast runtime of the present algorithm at generating load balanced structured partitions. © 2020 Elsevier Inc.","Grid partitioning; Load balancing; Parallel computing; Structured multi-block grids"
"A protection routing with secure mechanism in Möbius cubes","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080934132&doi=10.1016%2fj.jpdc.2020.02.007&partnerID=40&md5=9ff826173187aa9e1dd1fcb2346058d8","The protection routing uses the multi-paths technique for integrating route discovery and route maintenance mechanisms in a network, and thus it can tolerate the failure of one component (including a node or a link). Tapolcai (2013) proposed a method showing that a network possessing two completely independent spanning trees (CISTs for short) suffices to configure a protection routing. However, it is well-known that the problem of determining whether there exist two CISTs in a graph (or network) is NP-complete. In this paper, we extend Tapolcai's method such that the protection routing is configured by a combination of multiple CISTs. The first application of such an extension is that it can be used to deal with the problem of security in transmission, which we call the secure-protection routing scheme (SPR-scheme for short). Thus, a network transmission using the SPR-scheme ensures that no node other than the destination can receive the complete message. Moreover, we show that if a message M is transmitted in a network G using the SPR-scheme configured by a combination of n CISTs, then each intermediate node of G can receive a maximum of 2∕n ratio of M. From a similar idea of the extension, another application is the so-called multiple-protection routing scheme (MPR-scheme for short) which can be used to increase the capability of fault-tolerance. For assessing the performance of routing using MPR-scheme, we first propose a construction of three CISTs in the two types of Möbius cubes, which are hypercube-variant networks and are superior to hypercubes due to having a smaller diameter. For the n-dimensional Möbius cube, the diameters of CISTs we constructed are at most 14 when n=6 and at most 2n+1 when n⩾7. So, we configure the desired protection routing in the Möbius cubes via the three CISTs. Then, we provide simulation results to experimentally evaluate the performance of the newly proposed MPR-scheme in the n-dimensional Möbius cubes for 6⩽n⩽10. As an important point, our results show that the adoption of routing using MPR-scheme will result in a significant slowdown in the transmission failure rate. © 2020 Elsevier Inc.","Completely independent spanning trees; Fault-tolerant routing; Multiple-protection routing scheme; Möbius cubes; Secure-protection routing scheme"
"Blockchain-based accountability for multi-party oblivious RAM","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076828750&doi=10.1016%2fj.jpdc.2019.10.005&partnerID=40&md5=35cd741fac3165c700afd6cea9560792","Recently, oblivious random access machine (ORAM) has been widely used to prevent privacy leakage from user's access pattern. However, in multi-user scenarios, the obliviousness property of ORAM facilitates the malicious data modification by unauthorized users, which brings a new security challenge of user accountability to ORAM applications. Moreover, based on our observations, existing user accountability schemes for multi-user ORAM induce the extremely unacceptable overhead in both time and storage. What is worse, it is still inherent the traditional cloud accountability problem that the untrusted cloud server may have misbehavior on storing the outsourced data. In this paper, we focus on the issue that how to do accountability for both malicious users and untrusted cloud server without the independent trusted third party server. To address the above problem, we design and implement a Traceable Oblivious RAM, or T-ORAM for short, a cryptographic system that protects the privacy of users and the integrity of outsourced data based on group signatures. It can detect malicious users quickly by utilizing the traceability property of group signatures, and cost less storage overhead comparing with the existing solutions. Then, we further propose a more secure solution of Blockchain-based Traceable Oblivious RAM (BT-ORAM). Specifically, by introducing the blockchain technology, BT-ORAM can detect the malicious behavior from both malicious users and untrusted cloud server. BT-ORAM is the first accountability work for multi-user ORAM that deal with both malicious users and the untrusted cloud server. Finally, security analysis and experimental results show that our method outperforms the state-of-the-art accountability work for oblivious RAM, S-GORAM, in both security and performance. © 2019 Elsevier Inc.","Access control; Accountability; Blockchain; Group signature; Oblivious RAM"
"Algorithmic and language-based optimization of Marsa-LFIB4 pseudorandom number generator using OpenMP, OpenACC and CUDA","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076835716&doi=10.1016%2fj.jpdc.2019.12.004&partnerID=40&md5=ce98a7446e869f4b54c41008669f0f2f","The aim of this paper is to present new high-performance implementations of Marsa-LFIB4 which is an example of high-quality multiple recursive pseudorandom number generators. We propose an algorithmic approach that combines language-based vectorization techniques together with a new divide-and-conquer parallel method that exploits a special sparse structure of the matrix obtained from the recursive formula that defines the generator. Our portable OpenACC implementation achieves the performance comparable to those achieved by our CUDA-based and OpenMP-based implementations on GPUs and multicore CPUs, respectively. © 2019 The Author","Algorithmic approach; OpenMP, OpenACC and CUDA; Pseudorandom numbers; Recursive generators; Vectorization"
"High-efficiency parallelism solution for a Multiview High-Efficiency Video Coding decoder","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083641040&doi=10.1016%2fj.jpdc.2020.03.008&partnerID=40&md5=c006c1340fe5824b630facc230eee13c","Thus far, Multiview High-Efficiency Video Coding (MV-HEVC) can only use a central processing unit (CPU) to perform decompression on a personal computer (PC) or workstation. Because MV-HEVC is much more complex than High-Efficiency Video Coding (HEVC), decompressors need higher parallelism to decompress in real time. Therefore, this study presents a parallel method based on MV-HEVC. Inter-view complete parallelism is realized according to the dependent relationship between other MV-HEVC views, and a search range is not required to avoid the data dependence between frames. Based on the dependencies of each task in MV-HEVC, an advanced wavefront parallel processing method is proposed to achieve higher intra-frame parallelism. The parallel structure of the proposed method is compatible with that of the single-instruction multiple-data acceleration method. The results showed that the proposed method can decompress MV-HEVC with 20 threads in real time for 1088p video with three views. © 2020 Elsevier Inc.","Inter-view; Multithread; MV-HEVC; Parallel processing; Wavefront parallel processing"
"Designing an efficient parallel spectral clustering algorithm on multi-core processors in Julia","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078150424&doi=10.1016%2fj.jpdc.2020.01.003&partnerID=40&md5=b838252ae0314860c0c5be50146dbd93","Spectral clustering is widely used in data mining, machine learning and other fields. It can identify the arbitrary shape of a sample space and converge to the global optimal solution. Compared with the traditional k-means algorithm, the spectral clustering algorithm has stronger adaptability to data and better clustering results. However, the computation of the algorithm is quite expensive. In this paper, an efficient parallel spectral clustering algorithm on multi-core processors in the Julia language is proposed, and we refer to it as juPSC. The Julia language is a high-performance, open-source programming language. The juPSC is composed of three procedures: (1) calculating the affinity matrix, (2) calculating the eigenvectors, and (3) conducting k-means clustering. Procedures (1) and (3) are computed by the efficient parallel algorithm, and the COO format is used to compress the affinity matrix. Two groups of experiments are conducted to verify the accuracy and efficiency of the juPSC. Experimental results indicate that (1) the juPSC achieves speedups of approximately 14×∼18× on a 24-core CPU and that (2) the serial version of the juPSC is faster than the Python version of scikit-learn. Moreover, the structure and functions of the juPSC are designed considering modularity, which is convenient for combination and further optimization with other parallel computing platforms. © 2020 Elsevier Inc.","Clustering algorithm; Julia language; Multi-core processors; Parallel algorithm; Spectral clustering"
"Parallel quicksort algorithm on OTIS hyper hexa-cell optoelectronic architecture","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083037766&doi=10.1016%2fj.jpdc.2020.03.015&partnerID=40&md5=5785a2bc9535d52997d92318c8715117","In the last two decades, widespread attention has been paid in parallelizing algorithms, such as sorting and searching, for computationally intensive applications. Several interconnection networks were demonstrated for that purpose; such as hypercube and Hyper Hexa-Cell (HHC). However, the leverage of optical links in Optical Transpose Interconnection Systems (OTIS) stimulates the researchers toward developing optoelectronic architectures that utilize the optical links in addition to the electronic links; such as, OTIS-hypercube and OTIS-HHC. In this paper, we introduced a parallel quicksort algorithm for the OTIS-HHC optoelectronic architecture. This algorithm has been evaluated analytically and by simulation in terms of run time, speedup, and efficiency, where a set of simulation runs were carried out on different input data distributions types with various sizes. Thus, simulation results supported the analytical evaluation and met the expectations in which they showed good performance in terms of speedup and efficiency. © 2020 Elsevier Inc.","Interconnection network; Optoelectronic architecture; OTIS; Parallel quicksort; Parallel sorting"
"FFT, FMM, and multigrid on the road to exascale: Performance challenges and opportunities","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074439422&doi=10.1016%2fj.jpdc.2019.09.014&partnerID=40&md5=99bce502f10929eed6fb5807861f1aa8","FFT, FMM, and multigrid methods are widely used fast and highly scalable solvers for elliptic PDEs. However, emerging large-scale computing systems are introducing challenges in comparison to current petascale computers. Recent efforts (Dongarra et al. 2011) have identified several constraints in the design of exascale software that include massive concurrency, resilience management, exploiting the high performance of heterogeneous systems, energy efficiency, and utilizing the deeper and more complex memory hierarchy expected at exascale. In this paper, we perform a model-based comparison of the FFT, FMM, and multigrid methods in the context of these projected constraints. In addition we use performance models to offer predictions about the expected performance on upcoming exascale system configurations based on current technology trends. © 2019 Elsevier Inc.","Exascale; Fast Fourier transform; Fast multipole method; Multigrid; Performance modeling"
"Securing instruction interaction for hierarchical management","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075511190&doi=10.1016%2fj.jpdc.2019.10.010&partnerID=40&md5=ecef18a78b18b01592b10dd239e50fa6","Hierarchical management is a typical approach to managing complex information networks (CINs). In hierarchical management, as the only avenue to control information storage, transmission and usage, instruction interaction is often exploited by attackers to threaten CINs security. Now it has become a major challenge to ensure the security of instruction interaction. To address this challenge, in this paper, we propose a security model to manage instruction interaction for the hierarchical management in CINs. First, considering instruction lifecycle, we design five basic instruction operations (i.e., instruction generation, distribution, decomposition, execution, and execution outcome feedback) and formally define their semantics. Then a series of security rules for the basic operations are proposed to monitor and control the instruction interactions. We prove that, through these rules, both controllability and confidentiality can be provided. Finally, case studies demonstrate the feasibility of the proposed model. © 2019 Elsevier Inc.","Complex information network; Hierarchical multi-domain access; Instruction control; Security model"
"Subgraph fault tolerance of distance optimally edge connected hypercubes and folded hypercubes","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077926539&doi=10.1016%2fj.jpdc.2019.12.009&partnerID=40&md5=137de01af6711c459ed6f11c7ecfed23","Hypercube and folded hypercube are the most fundamental interconnection networks for the attractive topological properties. We assume for any distinct vertices u,v∈V,κ(u,v) defined as local connectivity of u and v, is the maximum number of independent (u,v)-paths in G. Similarly, λ(u,v) is local edge connectivity of u,v. For some t∈[1,D(G)],∀u,v∈V,u≠v, and d(u,v)=t, if κ(u,v)(orλ(u,v))=min{d(u),d(v)}, then G is t-distance optimally (edge) connected, where D(G) is the diameter of G and d(u) is the degree of u. For all integers 0<k≤t, if G is k-distance optimally connected, then we call G is t-distance local optimally connected. Similarly, we have the definition of t-distance local optimally edge connected. In this paper, we show that after deleting Qk(k≤n−1), Qn−Qk and FQn−Qk are 2-distance local optimally edge connected. © 2019 Elsevier Inc.","Distance; Fault-tolerance; Interconnection networks; Path"
"Variational approach for privacy funnel optimization on continuous data","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075002724&doi=10.1016%2fj.jpdc.2019.09.010&partnerID=40&md5=ba1c49b8d77dd8e206c43073ccfdb10a","Here we consider a common data encryption problem encountered by users who want to disclose some data to gain utility but preserve their private information. Specifically, we consider the inference attack, in which an adversary conducts inference on the disclosed data to gain information about users’ private data. Following privacy funnel (Makhdoumi et al., 2014), assuming that the original data X is transformed into Z before disclosing and the log loss is used for both privacy and utility metrics, then the problem can be modeled as finding a mapping X→Z that maximizes mutual information between X and Z subject to a constraint that the mutual information between Z and private data S is smaller than a predefined threshold ϵ. In contrast to the original study (Makhdoumi et al., 2014), which only focused on discrete data, we consider the more general and practical setting of continuous and high-dimensional disclosed data (e.g., image data). Most previous work on privacy-preserving representation learning is based on adversarial learning or generative adversarial networks, which has been shown to suffer from the vanishing gradient problem, and it is experimentally difficult to eliminate the relationship with private data Y when Z is constrained to retain more information about X. Here we propose a simple but effective variational approach that does not rely on adversarial training. Our experimental results show that our approach is stable and outperforms previous methods in terms of both downstream task accuracy and mutual information estimation. © 2019 Elsevier Inc.","Data security; Privacy; Representation Learning"
"COMITMENT: A Fog Computing Trust Management Approach","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074826255&doi=10.1016%2fj.jpdc.2019.10.006&partnerID=40&md5=7e9238b8c6e06a9f107a2962834d8b61","As an extension of cloud computing, fog computing is considered to be relatively more secure than cloud computing due to data being transiently maintained and analyzed on local fog nodes closer to data sources. However, there exist several security and privacy concerns when fog nodes collaborate and share data to execute certain tasks. For example, offloading data to a malicious fog node can result into an unauthorized collection or manipulation of users’ private data. Cryptographic-based techniques can prevent external attacks, but are not useful when fog nodes are already authenticated and part of a networks using legitimate identities. We therefore resort to trust to identify and isolate malicious fog nodes and mitigate security, respectively. In this paper, we present a fog COMputIng Trust manageMENT (COMITMENT) approach that uses quality of service and quality of protection history measures from previous direct and indirect fog node interactions for assessing and managing the trust level of the nodes within the fog computing environment. Using COMITMENT approach, we were able to reduce/identify the malicious attacks/interactions among fog nodes by approximately 66%, while reducing the service response time by approximately 15 s. © 2019 Elsevier Inc.","Fog computing; Quality of protection; Trust"
"Selective bypassing and mapping for heterogeneous applications on GPGPUs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084476752&doi=10.1016%2fj.jpdc.2020.04.003&partnerID=40&md5=8c14dcd4f69cfe060f0305c2ce880028","Modern GPGPU supports executing multiple tasks with different run time characteristics and resource utilization. Having an efficient execution and resource management policy has been shown to be a critical performance factor when handling the concurrent execution of tasks with different run time behavior. Previous policies either assign equal resources to disparate tasks or allocate resources based on static or standalone behavior profiling. Treating tasks equally cannot efficiently utilize the system resources, while the standalone profiling ignores the correlated impact when running tasks concurrently and could hint incorrect task behavior. This paper addresses the above drawbacks and proposes a heterogeneity aware Selective Bypassing and Mapping (SBM) to manage both computing and cache resources for multiple tasks in a fine-grain manner. The light-weight run time profiling of SBM properly characterizes the disparate behavior of the concurrently executed multiple tasks, and selectively applies suited cache management and workgroup mapping policies to each task. When compared with the previous coarse-grained policies, SBM can achieve an average of 138% and up to 895% performance enhancement. When compared with the state-of-art fine-grained policy, SBM can achieve an average of 58% and up to 378% performance enhancement. © 2020 Elsevier Inc.","Dynamic scheduling; GPGPU; Heterogeneous applications; Manycore architectures; OpenCL"
"On the Monotonic Lagrangian Grid as Antecedent to the Neighborhood Grid","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083329804&doi=10.1016%2fj.jpdc.2020.04.001&partnerID=40&md5=95b65feeb3b479c780cb7409c7871b45","We note that a recently described data structure, the Neighborhood Grid, is equivalent to a data structure developed in the mid-1980s called the Monotonic Lagrangian Grid (MLG). The MLG was originally developed to support high-performance molecular and fluid dynamics simulations on both supercomputer and vector processing architectures and still finds use in those and other areas. In this paper we emphasize that the rediscovery of the MLG offers benefits to users of the Neighborhood Grid in the form of an existing literature with results relevant to its efficient implementation in various contexts while users of the MLG similarly benefit from new theoretical results obtained for the Neighborhood grid. © 2020 Elsevier Inc.","MLG; Monotonic Lagrangian grid; Near-neighbor algorithms; Neighborhood Grid; Spatial data structures"
"Practical concurrent unrolled linked lists using lazy synchronization","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079616481&doi=10.1016%2fj.jpdc.2019.11.005&partnerID=40&md5=e0ec35febbb8235b937858e77ed22ece","Linked lists and other list-based sets are some of the most ubiquitous data structures in computer science. They are useful in their own right and are frequently used as building blocks in other data structures. A linked list can be “unrolled” to combine multiple keys in each node; this improves storage density and overall performance. This organization also allows an operation to skip over nodes which cannot contain a key of interest. This work introduces a new high-performance concurrent unrolled linked list with a lazy synchronization strategy. Most write operations under this strategy can complete by locking a single node. Experiments show up to 300% improvement over other concurrent list-based sets. © 2019","Concurrent data structures; Lazy synchronization; Linked lists; Unrolling"
"Efficient OpenMP parallelization to a complex MPI parallel magnetohydrodynamics code","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079663191&doi=10.1016%2fj.jpdc.2020.02.004&partnerID=40&md5=8ced3a91139f1e5135acb31929f33d3e","The state-of-the-art finite volume/difference magnetohydrodynamics (MHD) code Block Adaptive Tree Solarwind Roe Upwind Scheme (BATS-R-US) was originally designed with pure MPI parallelization. The maximum problem size achievable was limited by the storage requirements of the block tree structure. To mitigate this limitation, we have added multi-threaded OpenMP parallelization to the previous pure MPI implementation. We opted to use a coarse-grained approach by making the loops over grid blocks multi-threaded and have succeeded in making BATS-R-US an efficient hybrid parallel code with modest changes in the source code while preserving the performance. Good weak scalings up to hundreds of thousands of cores were achieved both for explicit and implicit time stepping schemes. This parallelization strategy greatly extended the possible simulation scale from 16,000 cores to more than 500,000 cores with 2GB/core memory on the Blue Waters supercomputer. Our work also revealed significant performance issues for some of the compilers when the code is compiled with the OpenMP library, probably related to the less efficient optimization of a complex multi-threaded region. © 2020 Elsevier Inc.","MHD; MPI; OpenMP; Parallel scaling"
"sLASs: A fully automatic auto-tuned linear algebra library based on OpenMP extensions implemented in OmpSs (LASs Library)","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077801645&doi=10.1016%2fj.jpdc.2019.12.002&partnerID=40&md5=405d634d4716b6a5b8ce487cfad57b13","In this work we have implemented a novel Linear Algebra Library on top of the task-based runtime OmpSs-2. We have used some of the most advanced OmpSs-2 features; weak dependencies and regions, together with the final clause for the implementation of auto-tunable code for the BLAS-3 TRSM routine and the LAPACK routines NPGETRF and NPGESV. All these implementations are part of the first prototype of sLASs library, a novel library for auto-tunable codes for linear algebra operations based on LASs library. In all these cases, the use of the OmpSs-2 features presents an improvement in terms of execution time against other reference libraries such as, the original LASs library, PLASMA, ATLAS and Intel MKL. These codes are able to reduce the execution time in about 18% on big matrices, by increasing the IPC on GEMM and reducing the time of task instantiation. For a few medium matrices, benefits are also seen. For small matrices and a subset of medium matrices, specific optimizations that allow to increase the degree of parallelism in both, GEMM and TRSM tasks, are applied. This strategy achieves an increment in performance of up to 40%. © 2019 Elsevier Inc.","Auto-tuning; LASs; Linear algebra; OmpSs"
"ARVMEC: Adaptive Recommendation of Virtual Machines for IoT in Edge–Cloud Environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083078764&doi=10.1016%2fj.jpdc.2020.03.006&partnerID=40&md5=25a81cb3170ff71257b8b98d2d76f1df","Edge–cloud services provide heterogeneous virtual machine types to run various IoT workloads. Choosing the appropriate VM configuration for each workload can effectively improve performance and reduce costs. This article proposes ARVMEC, Adaptive Recommendation of Virtual Machines for IoT in Edge-Cloud Environment, which can always provide users with the best VM recommendation according to their own budget or deadline constraints. ARVMEC uses a tree-based ensemble learning algorithm to make accurate predictions on workload performance for all VM types. It can abstract user purposes in a more flexible and general mode, thus offer reasonable recommendations accordingly. Compared to state-of-art methods, ARVMEC can make better predictions with a 15% improvement in accuracy. © 2020 Elsevier Inc.","Cloud computing; Edge computing; Ensemble learning; Internet of Things; VM recommendation"
"Towards secure big data analytic for cloud-enabled applications with fully homomorphic encryption","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076495564&doi=10.1016%2fj.jpdc.2019.10.008&partnerID=40&md5=7c311b33fdaf3754c1ee8830f23fe7ab","Cloud computing empowers enterprises to efficiently manage big data and discovery of useful information which are the most fundamental challenges for big data enabled applications. The cloud offered unlimited resources that can store, manage and analyze massive and heterogeneous data to improve the quality assurance of application services. Nevertheless, cloud computing is exposed to enormous external and internal privacy breaches and leakage threats. In this paper, we introduce a privacy-preserving distributed analytics framework for big data in cloud. Fully Homomorphic Encryption (FHE) is used as an emerging and powerful cryptosystem that can carry out analysis tasks on encrypted data. The developed distributed approach has the scalability to partition both data and analysis computations into subset cloud computing nodes that can be run independently. This rapidly accelerates the performance of encrypted data processing while preserving a high level of analysis accuracy. Our experimental evaluation demonstrates the efficiency of the proposed framework, in terms of both analysis performance and accuracy, for building a secure analytics cloud-enabled application. © 2019 Elsevier Inc.","Big data; Cloud computing; Distributed analytic; Fully homomorphic encryption"
"Adaptive data and verified message disjoint security routing for gathering big data in energy harvesting networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073033399&doi=10.1016%2fj.jpdc.2019.08.012&partnerID=40&md5=2856ea65c92d6cb3c6b9f6934f0a4797","To improve the data arrival ratio and the transmission delay and considering that the capacity for determining malicious nodes and energy are limited, a security disjoint routing-based verified message (SDRVM) scheme is proposed. The main contributions of SDRVM are as follows: (a) two connected dominating sets (a data CDS and a v-message CDS) are created for disseminating data and verified messages (v-messages), respectively, based on the remaining energy of nodes. (b) Nodes record the ID information in data packets with a specified probability, namely, the marking probability, which is adjusted according to the remaining energy of the nodes. (c) The duty cycle of the nodes is adjusted, and the energy of the nodes is divided into three levels. In the data CDS, the duty cycle of the sensor nodes is the longest and the duty cycle of the nodes that do not belong to either of the CDSs is the shortest. (d) If the energy of the sensor nodes is sufficient, data packets are transmitted several times and the v-messages that are stored in the nodes are transmitted to the destination nodes. The proposed scheme has been evaluated using different parameters where the results obtained prove its effectiveness in comparison to the existing solutions. © 2019 Elsevier Inc.","Disjoint routing; Energy harvesting networks; Marking probability; Network lifetime; Security"
"Artificial intelligence inspired energy and spectrum aware cluster based routing protocol for cognitive radio sensor networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104936752&doi=10.1016%2fj.jpdc.2020.04.007&partnerID=40&md5=74e7f6b3eb9374841ecc3fe6e987bb0e","A Cognitive Radio Sensor Network (CRSN) is a distributed network of sensor nodes, which senses event signals and collaboratively communicates over dynamically available spectrum bands in a multi-hop mode. All nodes participating in CRSN have to be cognitive of the network environment and autonomous in decision making for resolving issues related to throughput maximization, delay, and energy minimization. Clustering in CRSN is proven to tackle such issues and enlarges the network's lifetime. However, the existing clustering algorithms designed for WSNs do not consider the CR functionalities and challenges, and CR based networks work on the assumption of unlimited energy. This paper proposes an energy and spectrum aware unequal cluster based routing (ESUCR) protocol intending to resolve the issues of clustering and routing in CRSN. In ESUCR, cluster formation is mainly performed considering the residual energy of the secondary users (SUs) and relative spectrum awareness such that the common data channels for clusters are selected based on the appearance probability of PUs. ESUCR performs energy-efficient channel sensing by deciding the channel state with the statistic previous channel states. The premature death of cluster heads (CHs) is minimized by selecting and rotating the CHs based on intra-cluster channel stability, energy, distance, and neighbor connectivity. During event detection, ESUCR performs energy-efficient data routing towards the sink node by employing hop by hop forwarding through the CHs and primary/secondary gateways. The performance of the proposed ESUCR protocol is proved through extensive simulations and compared to those of the state-of-the-art protocols under a dynamic spectrum-aware data transmission environment. © 2020 Elsevier Inc.","Clustering; Cognitive radio sensor networks; Energy efficiency; Event driven; Spectrum sensing"
"Enhancing and simplifying data security and privacy for multitiered applications","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079538906&doi=10.1016%2fj.jpdc.2020.01.006&partnerID=40&md5=3695f0ca28e5707c2c273df3f1ff205c","While databases provide capabilities to enforce security and privacy policies, two major issues still prevent applications from safely delegating such policies to the database. The first one is the loss of user identity in multitiered environments which renders the database security features of little to no value. The second issue is the unsafe coexistence between the security capabilities and fundamental database tenets which creates data leakage vulnerabilities. This paper proposes extensions to database systems to allow applications, such as those used in managing the operations of energy clouds, to safely delegate the security and privacy policies to the database. This delegation reduces complexity for applications and improves overall data security and privacy. Our performance evaluation shows that almost all the TPC-H queries perform the same or better when the security policy is enforced by the database. For the set of queries that performed better, the improvement observed ranges from 8 to 68%. © 2020 Elsevier Inc.","Applications; Databases; Energy cloud; Privacy; Security"
"A general purpose contention manager for software transactions on the GPU","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078908433&doi=10.1016%2fj.jpdc.2019.12.018&partnerID=40&md5=b02a6b602859dabe5b3eeebd96aa7a45","The Graphics Processing Unit (GPU) is now used extensively for general purpose GPU programming (GPGPU), allowing for greater exploitation of the multi-core model across many application domains. This is particularly true in cloud/edge/fog computing, where multiple GPU enabled servers support many different end user services. This move away from the naturally parallel domain of graphics can incur significant performance issues. Unlike the CPU, code that is hindered from execution due to blocking/waiting on the GPU can affect thousands of threads, rendering the advantages of a GPU irrelevant and reducing a highly parallel environment down to a serial one in the worst case. In this paper we present a solution that minimises blocking/waiting in GPGPU computing using a contention manager that offsets memory conflicts across threads through thread re-ordering. We consider conflicts of memory not only to avoid corruption (standard for transactional memory) but also in the semantic layer of application logic (e.g., enforcing ordering to ensure money drawn from bank account occurs after all deposits). We demonstrate how our approach is successful across a number of industry benchmarks and compare our approach to the only other related solution. We also demonstrate that our approach is scalable in terms of thread numbers (a key requirement on the GPU). We believe this is the first work of its kind demonstrating a generalised conflict and semantic contention manager suitable for the scale of parallel execution found on a GPU. © 2020 Elsevier Inc.","GPU; High performance computing; Parallel processing"
"A precise non-asymptotic complexity analysis of parallel hash functions without tree topology constraints","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077044859&doi=10.1016%2fj.jpdc.2019.10.002&partnerID=40&md5=d46954ce32f2c6ea25610840563b8e18","A recent work shows how we can optimize a tree based mode of operation for a hash function where the sizes of input message blocks and digest are the same, subject to the constraint that the involved tree structure has all its leaves at the same depth. In this work, we show that we can further optimize the running time of such a mode by using a tree having leaves at all its levels. We make the assumption that the input message block has a size a multiple of that of the digest and denote by d the ratio block size over digest size. The running time is evaluated in terms of number of operations performed by the hash function, i.e. the number of calls to its underlying function. It turns out that a digest can be computed in ⌈logd+1(l∕2)⌉+2 evaluations of the underlying function using ⌈l∕2⌉ processors, where l is the number of blocks of the message. Other results of interest are discussed, such as the optimization of the parallel running time for a tree of restricted height. © 2019 Elsevier Inc.","Hash functions; Merkle trees; Parallel algorithms; Prefix-free Merkle–Damgård; Sponge functions"
"Accelerating fingerprint identification using FPGA for large-scale applications","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082807786&doi=10.1016%2fj.jpdc.2020.03.007&partnerID=40&md5=b44db3830cf7f77cf56fe82b3b1b1b4c","Fingerprint-based human authentication has shown great potential for civil, forensic and corporate security applications in recent years. For large-scale databases, the complexity of the identification system increases and implementing these systems on general-purpose sequential computing devices becomes challenging. Field Programmable Gate Array (FPGA) has demonstrated to be an efficient tool for the acceleration of computationally challenging applications by utilizing parallelism in the computations. In this study, an FPGA-based hardware accelerator is exploited to propose a fast and robust fingerprint identification solution that is based on a generalized minutiae neighbor based encoding and matching algorithm. The proposed FPGA implementation employs the Distributed RAM resources efficiently by using them as look-up tables for matching the encoded minutiae features. The proposed FPGA-based fingerprint matching system has the potential to match 2.75 million fingerprints per second while maintaining a low error rate. The proposed system can be deemed as an effective solution for Automated Fingerprint Identification Systems (AFIS) for large-scale applications. © 2020 Elsevier Inc.","Biometrics; Fingerprint identification; FPGA; Parallel processing"
"A Hitchhiker's Guide On Distributed Training Of Deep Neural Networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075295079&doi=10.1016%2fj.jpdc.2019.10.004&partnerID=40&md5=cfd4b6e710c02caeee57aa0fdaf88eef","Deep learning has led to tremendous advancements in the field of Artificial Intelligence. One caveat, however, is the substantial amount of compute needed to train these deep learning models. Training a benchmark dataset like ImageNet on a single machine with a modern GPU can take up to a week and distributing training on multiple machines has been observed to drastically bring this time down. Recent work has brought down ImageNet training time to as low as 4 min by using a cluster of 2048 GPUs. This paper surveys the various algorithms and techniques used in distributed training and presents the current state of the art for a modern distributed training framework. More specifically, we explore the synchronous and asynchronous variants of distributed Stochastic Gradient Descent, various All Reduce gradient aggregation strategies and best practices for obtaining higher throughput and lower latency over a cluster such as mixed precision training, large batch training, and gradient compression. © 2019 Elsevier Inc.","Deep learning; Deep neural networks; Distributed training; High Performance Computing"
"How fast can one resize a distributed file system?","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082586754&doi=10.1016%2fj.jpdc.2020.02.001&partnerID=40&md5=a209347b5a5519d60cb7109d12756a6b","Efficient resource utilization becomes a major concern as large-scale distributed computing infrastructures keep growing in size. Malleability, the possibility for resource managers to dynamically increase or decrease the amount of resources allocated to a job, is a promising way to save energy and costs. However, state-of-the-art parallel and distributed storage systems have not been designed with malleability in mind. The reason is mainly the supposedly high cost of data transfers required by resizing operations. Nevertheless, as network and storage technologies evolve, old assumptions about potential bottlenecks can be revisited. In this study, we evaluate the viability of malleability as a design principle for a distributed storage system. We specifically model the minimal duration of the commission and decommission operations. To show how our models can be used in practice, we evaluate the performance of these operations in HDFS, a relevant state-of-the-art distributed file system. We show that the existing decommission mechanism of HDFS is good when the network is the bottleneck, but can be accelerated by up to a factor 3 when storage is the limiting factor. We also show that the commission in HDFS can be substantially accelerated. With the highlights provided by our model, we suggest improvements to speed both operations in HDFS. We discuss how the proposed models can be generalized for distributed file systems with different assumptions and what perspectives are open for the design of efficient malleable distributed file systems. © 2020 Elsevier Inc.","Commission; Decommission; Elastic storage; Malleable distributed file system; Model"
"On the performance difference between theory and practice for parallel algorithms","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078116479&doi=10.1016%2fj.jpdc.2019.12.020&partnerID=40&md5=023f8647a6e6362062d546891d4a797f","The performance of parallel algorithms is often inconsistent with their preliminary theoretical analyses. Indeed, the difference is increasing between the ability to theoretically predict the performance of a parallel algorithm and the results measured in practice. This is mainly due to the accelerated development of advanced parallel architectures, whereas there is still no agreed model for parallel computation, which has implications for the design of parallel algorithms and for the manner in which parallel programming should be taught. In this study, we examined the practical performance of Cormen's Quicksort parallel algorithm. We determined the performance of the algorithm with different parallel programming approaches and examine the capacity of theoretical performance analyses of the algorithm for predicting the actual performance. This algorithm is used for teaching theoretical and practical aspects of parallel programming to undergraduate students. We considered the pedagogic implications that may arise when the algorithm is used as a learning resource for teaching parallel programming. © 2020 Elsevier Inc.","Performance modeling; Python; Quicksort; Teaching parallel programming"
"Joker: Elastic stream processing with organic adaptation","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076826035&doi=10.1016%2fj.jpdc.2019.10.012&partnerID=40&md5=6ab190475e9aaf5a74d3d28b326ab92b","This paper addresses the problem of auto-parallelization of streaming applications. We propose an online parallelization optimization algorithm that adjusts the degree of pipeline and data parallelism in a joint manner. We define an operator development API and a flexible parallel execution model to form a basis for the optimization algorithm. The operator interface unifies the development of different types of operators and makes operator properties visible in order to enable safe optimizations. The parallel execution model splits a data flow graph into regions. A region contains the longest sequence of compatible operators that are amenable to data parallelism as a whole and can be further parallelized with pipeline parallelism. We also develop a stream processing run-time, named Joker, to scale the execution of streaming applications in a safe, transparent, dynamic, and automatic manner. This ability is called organic adaptation. Joker implements the runtime machinery to execute a data flow graph with any parallelization configuration and most importantly change this configuration at run-time with low cost in the presence of partitioned stateful operators, in a way that is transparent to the application developers. Joker continuously monitors the run-time performance, and runs the optimization algorithm to resolve bottlenecks and scale the application by adjusting the degree of pipeline and data parallelism. The experimental evaluation based on micro-benchmarks and real-world applications showcase that our solution accomplishes elasticity by finding an effective parallelization configuration. © 2019 Elsevier Inc.","Elasticity; Parallelization; Stream processing"
"Scheduling directed acyclic graphs with optimal duplication strategy on homogeneous multiprocessor systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077739073&doi=10.1016%2fj.jpdc.2019.12.012&partnerID=40&md5=5864a943258c8284ea1433ff045e0332","Modern applications generally need a large volume of computation and communication to fulfill the goal. These applications are often implemented on multiprocessor systems to meet the requirements in computing capacity and communication bandwidth, whereas, how to obtain a good or even the optimal performance on such systems remains a challenge. When tasks of the application are mapped onto different processors for execution, inter-processor communications become inevitable, which delays some tasks’ execution and deteriorates the schedule performance. To mitigate the overhead incurred by inter-processor communications and improve the schedule performance, task duplication strategy has been employed in the schedule. Most available techniques for the duplication-based scheduling problem utilize heuristic strategies to produce sub-optimal solutions, however, how to find the optimal duplication-based solution with the minimal schedule makespan remains an unsolved issue. To fill in this gap, this paper proposes a novel Mixed Integer Linear Programming (MILP) formulation for this problem, together with a set of key theorems which enable and simplify the MILP formulation. The proposed MILP formulation can optimize the duplication strategy, serialize the execution of task instances on each processor and determine data precedences among different task instances, thus producing the optimal solution. The proposed method is tested on a set of synthesized applications and platforms and compared with the well-known algorithm. The experimental results demonstrate the effectiveness of the proposed method. © 2019 Elsevier Inc.","Makespan; Mixed Integer Linear Programming; Multiprocessor; Schedule; Task duplication"
"Improving the performance of physics applications in atom-based clusters with rCUDA","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076105797&doi=10.1016%2fj.jpdc.2019.11.007&partnerID=40&md5=cd17e51e7a45845296c7f4c7bbe58266","Traditionally, High-Performance Computing (HPC) has been associated with large power requirements. The reason was that chip makers of the processors typically employed in HPC deployments have always focused on getting the highest performance from their designs, regardless of the energy their processors may consume. Actually, for many years only heat dissipation was the real barrier for achieving higher performance, at the cost of higher energy consumption. However, a new trend has recently appeared consisting on the use of low-power processors for HPC purposes. The MontBlanc and Isambard projects are good examples of this trend. These proposals, however, do not consider the use of GPUs. In this paper we propose to use GPUs in this kind of low-power processor based HPC deployments by making use of the remote GPU virtualization mechanism. To that end, we leverage the rCUDA middleware in a hybrid cluster composed of low-power Atom-based nodes and regular Xeon-based nodes equipped with GPUs. Our experiments show that, by making use of rCUDA, the execution time of applications belonging to the physics domain is noticeably reduced, achieving a speed up of up to 140x with just one remote NVIDIA V100 GPU with respect to the execution of the same applications using 8 Atom-based nodes. Additionally, a rough energy consumption estimation reports improvements in energy demands of up to 37x. © 2019 Elsevier Inc.","GPU virtualization; InfiniBand; Low-power processors; Physics applications; rCUDA"
"A secure and efficient outsourced computation on data sharing scheme for privacy computing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073539771&doi=10.1016%2fj.jpdc.2019.09.008&partnerID=40&md5=a6a8b029be713b2b29c8f601920092e2","With the development of computer technology, it makes malicious users more easily get data stored in cloud. However, these data are always related to users’ privacy, and it is harmful when the data are acquired by attackers. Ciphertext-policy attribute-based encryption (CP-ABE) is suitable to achieve privacy and security in cloud. In this paper, we put forward a secure and efficient outsourced computation algorithm on data sharing scheme for privacy computing. Existing schemes only outsource decryption computation to the cloud, users still have heavy burden in encryption. In order to reduce the computing burden of users, most encryption and decryption computations are outsourced to the cloud service provider in our construction. At the same time, to apply in practice, we propose efficient user and attribute revocation. Finally, the security analysis and simulation results show that our scheme is secure and efficient compared with existing schemes. © 2019 Elsevier Inc.","CP-ABE; Outsourced computation; Privacy computing; Revocation"
"A practical group blind signature scheme for privacy protection in smart grid","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073981522&doi=10.1016%2fj.jpdc.2019.09.016&partnerID=40&md5=c5ddd658eeeddb49ba9fdbfa844ed09d","The leakage of privacy is one of the key factors to restrict the development of smart grid. Currently, research works of protecting privacy in smart grid mainly focused on two aspects: (1) data aggregation based on the mathematical model and algorithm and (2) user anonymous authentication. However, data aggregation is at cost of obtaining fine-grained electricity consumption information to protect privacy. Anonymous authentication cannot identify the malicious user in previous studies. Hence, in this work, we propose a group blind signature scheme in smart grid to accomplish conditional anonymity. Furthermore, the integrity of consumption data can be verified by homomorphic encryption (HE) which can decrease the communication overhead between control center and smart meter remarkably. From the security analysis and experiment simulation, the results show that our scheme is safe and efficient. © 2019 Elsevier Inc.","Anonymous authentication; Data integrity verification; Industry 4.0/5.0 for security; Smart grid; Traceability"
"Utilization-prediction-aware virtual machine consolidation approach for energy-efficient cloud data centers","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079666795&doi=10.1016%2fj.jpdc.2019.12.014&partnerID=40&md5=f9d04cfe23844d69aac589f6a3081f09","In the age of the information explosion, the energy demand for cloud data centers has increased markedly; hence, reducing the energy consumption of cloud data centers is essential. Dynamic virtual machine VM consolidation, as one of the effective methods for reducing energy energy consumption is extensively employed in large cloud data centers. It achieves the energy reductions by concentrating the workload of active hosts and switching idle hosts into low-power state; moreover, it improves the resource utilization of cloud data centers. However, the quality of service (QoS) guarantee is fundamental for maintaining dependable services between cloud providers and their customers in the cloud environment. Therefore, reducing the power costs while preserving the QoS guarantee are considered as the two main goals of this study. To efficiently address this problem, the proposed VM consolidation approach considers the current and future utilization of resources through the host overload detection (UP-POD) and host underload detection (UP-PUD). The future utilization of resources is accurately predicted using a Gray-Markov-based model. In the experiment, the proposed approach is applied for real-world workload traces in CloudSim and were compared with the existing benchmark algorithms. Simulation results show that the proposed approaches significantly reduce the number of VM migrations and energy consumption while maintaining the QoS guarantee. © 2020 Elsevier Inc.","Cloud computing; Cloud data centers; Dynamic virtual machine (VM) consolidation; Utilization prediction model"
"MalFCS: An effective malware classification framework with automated feature extraction based on deep convolutional neural networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082975727&doi=10.1016%2fj.jpdc.2020.03.012&partnerID=40&md5=ead81ea30c51c300b929f23dbd9afe18","Identifying the family of malware can determine their malicious intent and attack patterns, which helps to efficiently analyze large numbers of malware variants. Methods based on traditional machine learning often require a lot of time and resources in feature engineering. Virtually all existing static analysis methods based on malware visualization are derived from grayscale images, while a single low-order feature representation may be detrimental to discovering hidden features in a malware family. Based on these problems, this paper proposes an effective malware classification framework (MalFCS) based on malware visualization and automated feature extraction. MalFCS includes mainly three modules: malware visualization, feature extraction, and classification. First, we visualize malware binaries as entropy graphs based on structural entropy. Second, we present a feature extractor based on deep convolutional neural networks to extract patterns shared by a family from entropy graphs automatically. Finally, we propose an SVM classifier to classify malware based on the extracted features. We evaluate the proposed MalFCS over two widely studied benchmark datasets, i.e., Malimg and Microsoft. Experimental results show that compared with the state-of-the-art methods, MalFCS can obtain excellent classification performance with accuracy of 0.997 and 1, respectively, achieving the state-of-the-art performance. © 2020 Elsevier Inc.","Deep learning; Feature extraction; Information security; Malware classification; Malware visualization"
"Efficient AES implementation on Sunway TaihuLight supercomputer: A systematic approach","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077933059&doi=10.1016%2fj.jpdc.2019.12.013&partnerID=40&md5=42b03d4fdc553b09322f9e052b514e48","Encryption is an important technique to improve information security for many real-world applications. The Advanced Encryption Standard (AES) is a widely-used efficient cryptographic algorithm. Although AES is fast both in software and hardware, it is time-consuming to do data encryption especially for large amount of data. Therefore, it is a lasting effort to accelerate AES operations. This paper presents SW-AES, a parallel AES implementation on the Sunway TaihuLight, one of the fastest supercomputers in the world that takes the SW26010 processor as the basic building block. According to the architectural features of SW26010, SW-AES exploits parallelism from different levels, including (1) inter-CPE (Computing Processing Element) data parallelism that distributes tasks among the 256 on-chip CPEs, (2) intra-CPE data parallelism enabled by the Single-Instruction Multiple-Data (SIMD) instructions inside each CPE, and (3) instruction-level parallelism that pipelines memory access and the computation. In addition, corresponding to the two application scenarios, SW-AES presents scalable ways to efficiently run AES on many nodes. As a result, SW-AES can gain a maximum throughput of 13.50 GB/s on a single SW26010 node, which is 216.23× higher than the latest parallel AES implementation on the Sunway TaihuLight, and about 37.3% higher than the latest AES implementation on the GTX 480 GPU. When running on 1024 computing nodes with each one processing 1 GB data, SW-AES can achieve a throughput of 13819.25 GB/s. On the contrast, only a throughput of 63.91 GB/s can be achieved by the latest related work on the Sunway TaihuLight. © 2019 Elsevier Inc.","AES algorithm; High-performance computing; Parallelism; Supercomputer; Vectorization"
"PMSMC: Priority-based Multi-requestor Scheduler for Embedded System Memory Controller","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079886612&doi=10.1016%2fj.jpdc.2020.01.001&partnerID=40&md5=464b96cd33803e05a4e46b3e33292906","Modern Multi-Processor System-On-Chips (MPSOC) are widely used especially in real-time embedded systems due to their high throughput and low per unit cost. However, bounded latency is vital to guarantee fast response as well as fairness for applications running on multicore processors. In this paper, a new Priority-base Memory Controller for Embedded Systems (PMSMC) that prioritizes concurrently running applications by assigning uneven quota for each requestor is proposed. Each requestor quota is accompanied by a timer to control the dispatch rate to prevent starvation. Moreover, PMSMC can monitor the real-time application memory activity to assist the request scheduling to achieve efficient utilization of the shared DRAM resource while keeping the timing bounded. Hence, PMSMC can serve both multimedia real-time applications and hard real-time applications concurrently. For 8-core processors, PMSMC is able to achieve an overall performance speedup of 24% and 16% compared to the recently proposed WCAD and TRB-SP memory controllers, respectively. For the Energy-Delay Product (EDP) metric which combines both performance and energy consumption, PMSMC achieves lower EDPs of 25% and 60% compared to the recently proposed WCAD and TRB-SP memory controllers, respectively. © 2020 Elsevier Inc.","Memory controller; Multicore processors; Priority scheduling; Real-time systems; Request scheduling"
"TSpoon: Transactions on a stream processor","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082618808&doi=10.1016%2fj.jpdc.2020.03.003&partnerID=40&md5=bd542b0f902f56ed685d0563604ad531","Stream processing systems are increasingly becoming a core element in the data processing stack of many large companies, where they complement data management frameworks to build comprehensive solutions for processing, storage, and query. The adoption of separate tools leads to complex architectures that leave developers with the difficult task of writing application-specific code that ensures integration correctness. This hinders design, implementation, maintenance, and evolution. We address this problem with a new model that seamlessly integrates data management capabilities within a distributed stream processor. The model makes the state of stream processing operators externally visible and queryable, providing transactional guarantees for state accesses and updates. It enables developers to configure transactions obtaining strong guarantees when needed and relaxing them for higher performance when possible. We introduce the new model and formalize the transactional guarantees it offers. We discuss the implementation of the model into the TSpoon tool and experiment different algorithms to enforce transactional behavior. We evaluate the performance of TSpoon with real world case studies and synthetic workloads, compare it with state-of-the-art tools for distributed in-memory stream processing and data management, and analyze in detail the cost to ensure various transactional semantics. © 2020 Elsevier Inc.","Data databases; Distributed Stream processing; Queryable state; Transactions; TSpoon"
"HDOT — An approach towards productive programming of hybrid applications","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075561021&doi=10.1016%2fj.jpdc.2019.11.003&partnerID=40&md5=d6e3ccff02cac2234e22657b7c1eb9dd","A wealth of important scientific and engineering applications are configured for use on high performance computing architectures using functionality found in the MPI specification. This specification provides application developers with a straightforward means for implementing their ideas for execution on distributed-memory parallel processing computers. OpenMP directives provide a means for operating on shared-memory regions of those computers. With the advent of machines composed of many-core processors, the strict synchronisation required by the bulk synchronous parallel (BSP) communication model can hinder performance increases. This is due to the complexity to handle load imbalances, to reduce serialisation imposed by blocking communication patterns, to overlap communication with computation and, finally, to deal with increasing memory overheads. The MPI specification provides advanced features such as non-blocking calls or shared memory to mitigate some of these factors. However, applying these features efficiently usually requires significant changes on the application structure. Task parallel programming models are being developed as a means of mitigating the abovementioned issues but without requiring extensive changes on the application code. In this work, we present a methodology to develop hybrid applications based on tasks called hierarchical domain over-decomposition with tasking (HDOT). This methodology overcomes most of the issues found on MPI-only and traditional hybrid MPI+OpenMP applications. However, by emphasising the reuse of data partition schemes from process-level and applying them to task-level, it enables a natural coexistence between MPI and shared-memory programming models. The proposed methodology shows promising results in terms of programmability and performance measured on a set of applications. © 2019 Elsevier Inc.","Hybrid programming; MPI; OmpSs-2; OpenMP; Parallel programming"
"Multiple pattern matching for network security applications: Acceleration through vectorization","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075265689&doi=10.1016%2fj.jpdc.2019.10.011&partnerID=40&md5=546a2e16025892579e0817633d570c5c","As both new network attacks emerge and network traffic increases in volume, the need to perform network traffic inspection at high rates is ever increasing. The core of many security applications that inspect network traffic (such as Network Intrusion Detection) is pattern matching. At the same time, pattern matching is a major performance bottleneck for those applications: indeed, it is shown to contribute to more than 70% of the total running time of Intrusion Detection Systems. Although numerous efficient approaches to this problem have been proposed on custom hardware, it is challenging for pattern matching algorithms to gain benefit from the advances in commodity hardware. This becomes even more relevant with the adoption of Network Function Virtualization, that moves network services, such as Network Intrusion Detection, to the cloud, where scaling on commodity hardware is key for performance. In this paper, we tackle the problem of pattern matching and show how to leverage the architecture features found in commodity platforms. We present efficient algorithmic designs that achieve good cache locality and make use of modern vectorization techniques to utilize data parallelism within each core. We first identify properties of pattern matching that make it fit for vectorization and show how to use them in the algorithmic design. Second, we build on an earlier, cache-aware algorithmic design and show how we apply cache-locality combined with SIMD gather instructions to pattern matching. Third, we complement our algorithms with an analytical model that predicts their performance and that can be used to easily evaluate alternative designs. We evaluate our algorithmic design with open data sets of real-world network traffic: Our results on two different platforms, Haswell and Xeon-Phi, show a speedup of 1.8x and 3.6x, respectively, over Direct Filter Classification (DFC), a recently proposed algorithm by Choi et al. for pattern matching exploiting cache locality, and a speedup of more than 2.3x over Aho–Corasick, a widely used algorithm in today's Intrusion Detection Systems. Finally, we utilize highly parallel hardware platforms, evaluate the scalability of our algorithms and compare it to parallel implementations of DFC and Aho–Corasick, achieving processing throughput of up to 45Gbps and close to 2 times higher throughput than Aho–Corasick. © 2019 Elsevier Inc.","Gather; Pattern matching; SIMD; Vectorization"
"CFSec: Password based secure communication protocol in cloud-fog environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080994991&doi=10.1016%2fj.jpdc.2020.02.005&partnerID=40&md5=8c9cfb603b6de93ecf1cea6b18fbe731","With the growing needs of data across the world, it is almost hard to live without data a day. The fog computing concept is aiming to change the scenarios created by Cloud Computing environments and also to make the data-centric clouds decentralized and localized. Fog devices which aim to be at a shorter distance with the user, access data from the cloud itself. Hence, exchanging data between the cloud and fog device(s) is required and in this context security and privacy come into the picture to provide data confidentiality. This paper first shows an architecture for the data flow model between the cloud and fog computing and then designs an authentication protocol with proper key establishment between the cloud, fog, and user. We have simulated the proposed protocol using a popular simulator i.e., Scyther and proved that the parameters used in our protocol are strongly protected during protocol execution. Besides, our informal security analysis also confirms the robustness of the protocol. Our protocol achieves quick responses in comparison with state-of-the-art because of less computation overhead. © 2020 Elsevier Inc.","Cloud computing; Fog Devices; Scyther Tool; Secure communication"
"Batched transpose-free ADI-type preconditioners for a Poisson solver on GPGPUs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075774593&doi=10.1016%2fj.jpdc.2019.11.004&partnerID=40&md5=ab7911d4a976f455a8d3cc44f231b602","We investigate the iterative solution of a symmetric positive definite linear system involving the shifted Laplacian as the system matrix on General Purpose Graphics Processing Units (GPGPUs). We consider in particular the Chebyshev iteration for its reduced global communication. The ADI-type preconditioner involves solving multiple (batched) symmetric positive tridiagonal Toeplitz systems along each coordinate direction. We investigate several variants how to solve these tridiagonal systems, the Thomas algorithm, the Thomas combined with the SPIKE algorithm, and a polynomial approximation of the inverse. We test the various implementations numerically by means of two- and three-dimensional examples. It turns out that a combination of the Thomas algorithm and the approximate inverse leads to a solution that does not need either tiling or transpositions. As such none of the kernels uses an extensive amount of shared memory which yields a very high GPU utilization and more importantly optimal coalesced global memory access patterns. © 2019 Elsevier Inc.","ADI preconditioner; Batched triangular systems; General Purpose Graphical Processing Unit (GPGPU); Shifted Poisson problem"
"Reordering sparse matrices into block-diagonal column-overlapped form","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082841318&doi=10.1016%2fj.jpdc.2020.03.002&partnerID=40&md5=dafb59895869563c1e8639a7e80aac81","Many scientific and engineering applications necessitate computing the minimum norm solution of a sparse underdetermined linear system of equations. The minimum 2-norm solution of such systems can be obtained by a recent parallel algorithm, whose numerical effectiveness and parallel scalability are validated in both shared- and distributed-memory architectures. This parallel algorithm assumes the coefficient matrix in a block-diagonal column-overlapped (BDCO) form, which is a variant of the block-diagonal form where the successive diagonal blocks may overlap along their columns. The total overlap size of the BDCO form is an important metric in the performance of the subject parallel algorithm since it determines the size of the reduced system, solution of which is a bottleneck operation in the parallel algorithm. In this work, we propose a hypergraph partitioning model for reordering sparse matrices into BDCO form with the objective of minimizing the total overlap size and the constraint of maintaining balance on the number of nonzeros of the diagonal blocks. Our model makes use of existing partitioning tools that support fixed vertices in the recursive bipartitioning paradigm. Experimental results validate the use of our model as it achieves small overlap size and balanced diagonal blocks. © 2020 Elsevier Inc.","Block-diagonal form; Column overlap; Hypergraph partitioning; Sparse matrices; Underdetermined systems"
"Managing renewable energy and carbon footprint in multi-cloud computing environments","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073558593&doi=10.1016%2fj.jpdc.2019.09.015&partnerID=40&md5=71ccf767cd494a5bd877e8780f86b1ab","Cloud computing offers attractive features for both service providers and customers. Users benefit from the pay-as-you-go model by saving expenditures and service providers are deploying their services to cloud data centers to reduce their maintenance efforts. However, due to the fast growth of cloud data centers, the energy consumed by the data centers can lead to a huge amount of carbon emission with environmental impacts, and the carbon intensity of different locations are varied among different power plants according to the sources of energy. Thus, in this paper, to address the carbon emission problem of data centers, we consider shifting the workloads among multi-cloud located in different time zones. We also formulate the energy usage and carbon emission of data centers and model the solar power corresponding to the locations. This helps to reduce the usage of brown energy and maximize the utilization of renewable energy at different locations. We propose an approach for managing carbon footprint and renewable energy for multiple data centers at California, Virginia, and Dublin, which are in different time zones. The results show that our proposed approaches that apply workload shifting can reduce around 40% carbon emission in comparison to the baseline while ensuring the average response time of user requests. © 2019 Elsevier Inc.","Brownout; Carbon footprint; Cloud data centers; Renewable energy; Workload shifting"
"A Parallel Multilevel Feature Selection algorithm for improved cancer classification","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077515030&doi=10.1016%2fj.jpdc.2019.12.015&partnerID=40&md5=34b4cb14f7290334d999a66e66586722","Biological data is prone to grow exponentially, which consumes more resources, time and manpower. Parallelization of algorithms could reduce overall execution time. There are two main challenges in parallelizing computational methods. (1) Biological data is multi-dimensional in nature. (2). Parallel algorithms reduce execution time, but with the penalty of reduced prediction accuracy. This research paper targets these two issues and proposes the following approaches. (1) Vertical partitioning of data along feature space and horizontal partitioning along samples in order to ease the task of data parallelism. (2) Parallel Multilevel Feature Selection (M-FS) algorithm to select optimal and important features for improved classification of cancer sub-types. The selected features are evaluated using parallel Random Forest on Spark, compared with previously reported results and also with the results of sequential execution of same algorithms. The proposed parallel M-FS algorithm was compared with existing parallel feature selection algorithms in terms of accuracy and execution time. The results reveal that parallel multilevel feature selection algorithm improved cancer classification resulting into prediction accuracy ranging from ∼85% to ∼99% with very high speed up in terms of seconds. On the other hand, existing sequential algorithms yielded prediction accuracy of ∼65% to ∼99% with execution time of more than 24 hours. © 2019 Elsevier Inc.","Classification accuracy; Horizontal & Vertical partition; Oncogenes and proteins; Parallel Multilevel Feature Selection; Parallel Random Forest"
"Fault-tolerant least squares solvers for wireless sensor networks based on gossiping","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074450660&doi=10.1016%2fj.jpdc.2019.09.006&partnerID=40&md5=2c11ccc6f905b5dd1d9d9ee84adb16a7","Many applications in large loosely connected distributed networks (such as wireless sensor networks) require the distributed solution of linear least squares (dLLS) problems. Ideally, a truly distributed algorithm should require very little coordination between the nodes. This favours algorithms which do not require a fusion centre, cluster heads or any multi-hop communication. We present the novel dLLS solver GLS-IR for overdetermined linear systems. We investigate two variants of our novel solver, one of them based on the semi-normal equations, the other based on the normal equations. Both are combined with iterative refinement in mixed precision, which not only stabilises the methods but also decreases the communication cost. In GLS-IR, all communication between nodes is contained within a gossip-based algorithm for distributed aggregation, which limits the communication of each node to its immediate neighbourhood. Therefore, GLS-IR benefits directly from efficient and fault-tolerant algorithms for distributed aggregation. We use a fault-tolerant alternative to the push-sum method, the push-flow algorithm, which is able to recover from silent message loss and temporary or permanent link failures. We analytically compare the communication cost of GLS-IR to existing truly distributed algorithms. Since the theoretical analysis contains problem-dependent parameters, numerical experiments are needed in order to get a complete picture. Our simulation experiments illustrate a significantly reduced communication cost of GLS-IR compared to other existing truly distributed least squares solvers. We also illustrate that due to the properties of iterative refinement and push-flow, GLS-IR can achieve a result accurate to machine precision even if a high amount of message loss occurs. © 2019 Elsevier Inc.","Algorithmic fault tolerance; Distributed least squares solver; Gossip-based aggregation; Message loss; Mixed precision iterative refinement"
"Budgeted video replacement policy in mobile crowdsensing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073967371&doi=10.1016%2fj.jpdc.2019.10.003&partnerID=40&md5=676f66b8c81d78ab4ec1228cccdf4ff7","Mobile crowdsensing offers a new platform that recruits a suitable set of users to collectively complete an information collection/sensing task through users’ equipped devices. As a special case, video crowdsensing is to collect different video segments of the same event that are taken separately by the built-in cameras of mobile devices, and then combine them into a complete video. Mobile crowdsensing has attracted considerable attention recently due to the rich information that can be provided by videos. However, because of the limited caching space, a suitable video replacement policy is necessary. In this paper, we propose a Budgeted Video replaCement policy in mobile Video crowdsensing (BVCV), which first determines a video segment's value according to its caching situation and natural attributes. Then, we formulate the video caching problem as a budgeted maximum coverage problem, which is a well-known NP-hard problem. Finally, we propose a practical greedy solution and also infer the approximate ratio, which could be regarded as the lower bound of BVCV to the optimal solution. Our experiments with the real mobility datasets (StudentLife dataset, Buffalo/phonelab-wifi dataset) show that, the proposed budgeted video replacement policy achieves a longer successfully delivered video length, compared with other general replacement policies. © 2019 Elsevier Inc.","Budgeted; Mobile video crowdsensing; NP-hard; Replacement policy"
"Kokkos implementation of an Ewald Coulomb solver and analysis of performance portability","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077515556&doi=10.1016%2fj.jpdc.2019.12.003&partnerID=40&md5=91bb296f82ebd3a640bc422508294df1","We have implemented the computation of Coulomb interactions in particle systems using the performance portable C++ framework Kokkos. For the computation of the electrostatic interactions in particle systems we used an Ewald summation. This implementation we consider as a basis for a performance portability study. As target architectures we used Intel CPUs, including Intel Xeon Phi, as well as Nvidia GPUs. To provide a measure for performance portability we compute the number of needed operations and required cycles, i.e. runtime, and compare these with the measured runtime. Results indicate a similar quality of performance portability on all investigated architectures. © 2019 Elsevier Inc.","Electrostatics; Kokkos; Particle algorithms; Performance modelling; Performance portability"
"A novel routing verification approach based on blockchain for inter-domain routing in smart metropolitan area networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084170896&doi=10.1016%2fj.jpdc.2020.04.005&partnerID=40&md5=7177dfa0053f3d36dbe182992129cfe5","In recent years, with the continuous expansion of metropolitan area networks, the routing security problem has become more and more serious. In particular, promise-violating attack to inter-domain routing protocol is one of the most difficult attacks to defend, which always leads to serious consequences, such as maliciously attracting traffic and disrupting the network. To deal with such attack, current research generally adopts routing verification. However, it can only detect attacks violating a specific routing policy triggered by one malicious node, and no research has yet solved the problem caused by multiple collusion nodes. In this paper, we propose BRVM, a blockchain-based routing verification model, to address the issue that violating the shortest AS Path policy. The main idea of BRVM is to construct a route proof chain to verify whether a route violates the policy with the help of the blockchain technology. The precondition that avoiding the collusion attack is that the proportion of the malicious verification nodes is lower than the fault tolerance rate of the consensus algorithm. Then, we prove the correctness of BRVM in theory, and implement a prototype based on Quagga and Hyperledger Fabric. Some experiments on this prototype show that BRVM can indeed solve the promise-violating problem caused by multiple collusion nodes, and about 15.5% faster in performance compared with SPIDeR. © 2020 Elsevier Inc.","Blockchain; Inter-domain routing; Privacy; Routing verification; Security"
"Distributed Bayesian optimization of deep reinforcement learning algorithms","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079395340&doi=10.1016%2fj.jpdc.2019.07.008&partnerID=40&md5=66745ef0c155647cc476114d3207fb54","Significant strides have been made in supervised learning settings thanks to the successful application of deep learning. Now, recent work has brought the techniques of deep learning to bear on sequential decision processes in the area of deep reinforcement learning (DRL). Currently, little is known regarding hyperparameter optimization for DRL algorithms. Given that DRL algorithms are computationally intensive to train, and are known to be sample inefficient, optimizing model hyperparameters for DRL presents significant challenges to established techniques. We provide an open source, distributed Bayesian model-based optimization algorithm, HyperSpace, and show that it consistently outperforms standard hyperparameter optimization techniques across three DRL algorithms. © 2019 The Author","Bayesian optimization; Deep reinforcement learning"
"Improved GPU near neighbours performance for multi-agent simulations","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075301973&doi=10.1016%2fj.jpdc.2019.11.002&partnerID=40&md5=4c2a9a77a46e9d8f043530b0e203ea5e","Complex systems simulations are well suited to the SIMT paradigm of GPUs, enabling millions of actors to be processed in fractions of a second. At the core of many such simulations, fixed radius near neighbours (FRRN) search provides the actors with spatial awareness of their neighbours. The FRNN search process is frequently the limiting factor of performance, due to the disproportionate level of scattered memory reads demanded by the query stage, leading to FRNN search runtimes exceeding that of simulation logic. In this paper, we propose and evaluate two novel optimisations (Strips and Proportional Bin Width) for improving the performance of uniform spatially partitioned FRNN searches and apply them in combination to demonstrate the impact on the performance of multi-agent simulations. The two approaches aim to reduce latency in search and reduce the amount of data considered (i.e. more efficient searching), respectively. When the two optimisations are combined, the peak obtained speedups observed in a benchmark model are 1.27x and 1.34x in two and three dimensional implementations, respectively. Due to additional non FRNN search computation, the peak speedup obtained when applied to complex system simulations within FLAMEGPU is 1.21x. © 2019 The Authors","Complex systems; CUDA; GPU; Parallel algorithms"
"Hybrid-DCA: A double asynchronous approach for stochastic dual coordinate ascent","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084817590&doi=10.1016%2fj.jpdc.2020.04.002&partnerID=40&md5=4722ab7cd1616591574c50bcedab971e","In prior works, stochastic dual coordinate ascent (SDCA) has been parallelized in a multi-core environment where the cores communicate through shared memory, or in a multi-processor distributed memory environment where the processors communicate through message passing. In this paper, we propose a hybrid SDCA framework for multi-core clusters, the most common high performance computing environment that consists of multiple nodes each having multiple cores and its own shared memory. We distribute data across nodes where each node solves a local problem in an asynchronous parallel fashion on its cores, and then the local updates are aggregated via an asynchronous across-node update scheme. The proposed double asynchronous method converges to a global solution for L-Lipschitz continuous loss functions, and at a linear convergence rate if a smooth convex loss function is used. Extensive empirical comparison has shown that our algorithm scales better than the best known shared-memory methods and runs faster than previous distributed-memory methods. Big datasets, such as one of 280 GB from the LIBSVM repository, cannot be accommodated on a single node and hence cannot be solved by a parallel algorithm. For such a dataset, our hybrid algorithm takes less than 30 s to achieve a duality gap of 10−5 on 16 nodes each using 12 cores, which is significantly faster than the best known distributed algorithms, such as CoCoA+, that take more than 160 s on 16 nodes. © 2020 The Authors","Distributed computing; Dual coordinate descent; Optimization"
"DQPFS: Distributed quadratic programming based feature selection for big data","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076969658&doi=10.1016%2fj.jpdc.2019.12.001&partnerID=40&md5=0897539cf9ea9c80f77139e146e9b2e0","With the advent of the Big data, the scalability of the machine learning algorithms has become more crucial than ever before. Furthermore, Feature selection as an essential preprocessing technique can improve the performance of the learning algorithms in confront with large-scale dataset by removing the irrelevant and redundant features. Owing to the lack of scalability, most of the classical feature selection algorithms are not so proper to deal with the voluminous data in the Big Data era. QPFS is a traditional feature weighting algorithm that has been used in lots of feature selection applications. By inspiring the classical QPFS, in this paper, a scalable algorithm called DQPFS is proposed based on the novel Apache Spark cluster computing model. The experimental study is performed on three big datasets that have a large number of instances and features at the same time. Then some assessment criteria such as accuracy, execution time, speed-up and scale-out are figured. Moreover, to study more deeply, the results of the proposed algorithm are compared with the classical version QPFS and the DiRelief, a distributed feature selection algorithm proposed recently. The empirical results illustrate that proposed method has (a) better scale-out than DiRelief, (b) significantly lower execution time than DiRelief, (c) lower execution time than QPFS, (d) better accuracy of the Naïve Bayes classifier in two of three datasets than DiRelief. © 2019 Elsevier Inc.","Apache Spark; Big data; Feature ranking; Feature selection; Quadratic programming"
"Privbus: A privacy-enhanced crowdsourced bus service via fog computing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073571150&doi=10.1016%2fj.jpdc.2019.09.007&partnerID=40&md5=278c97e7b51ce15163abb101241f3633","Crowdsourced bus service provides the customized bus for a group of users with similar itineraries by designing the route based on the users’ trip plans. With crowdsourced bus service, the users with similar trips can enjoy the customized bus route efficiently and inexpensively. However, serious privacy concerns (e.g., the exposure of users’ current and future locations) have become a major barrier. To protect users’ itineraries, we propose Privbus, a privacy-enhanced crowdsourced bus service without hampering the functionality of bus route planning. Specifically, Privbus improves the performance of clustering itineraries due to the assistance of fogs. Then, Privbus executes the fog-assisted density peaks clustering operations on ciphertexts of users’ travel plans to protect the users’ trips. By doing so, the clustering operation is removed from users’ smart devices to fog nodes, so as to enable the users to be offline after they submit their travel plans. According to the clustering results, Privbus uses a route planning method to optimize the bus routes. The optimization reduces the time cost on travel of users, while guaranteeing the good profit and the wide coverage of crowdsourced bus service. Finally, through the performance evaluation and extensive experiments, we demonstrate that Privbus has the advantage of low computational and communication overhead, while providing high security and precision guarantees. © 2019 Elsevier Inc.","Crowdsourced bus service; Data clustering; Fog computing; Privacy preservation"
"A distributed algorithm for a maximal 2-packing set in Halin graphs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083757662&doi=10.1016%2fj.jpdc.2020.03.016&partnerID=40&md5=16ea92c638b7eed7b932b921c35d89f1","In this work, we propose MAXIMAL-2-PACKING-HALIN, a distributed algorithm that finds a maximal 2-packing set in undirected non-geometric Halin graphs of order n in linear time. First, this algorithm finds an external face of the input graph through the application of graph-reduction rules. Second, each vertex determines if it belongs to a maximal 2-packing set by applying a set of vertex-coloring rules. © 2020 Elsevier Inc.","Distributed algorithm; Halin graph; Maximal 2-packing set"
"Cost-effective deployment of certified cloud composite services","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073719046&doi=10.1016%2fj.jpdc.2019.09.003&partnerID=40&md5=b0ed3f57c7b8236da71c96fed5cc7acc","The advent of cloud computing has radically changed the concept of distributed environments, where services can now be composed and reused at high rates. Today, service composition in the cloud is driven by the need of providing stable QoS, where non-functional properties of composite services are proven over time and composite services continuously adapt to both functional and non-functional changes of the component services. This scenario introduces substantial costs on the cloud providers that go beyond the cost of deploying component services, and require to consider the costs of continuously verifying non-functional properties of composite and component services. In this paper, we propose a cost-effective approach to certification-based cloud service composition. This approach is based, on one side, on a portable certification process for the cloud evaluating non-functional properties of composite services and, on the other side, on a cost-evaluation methodology aimed to produce the service composition that minimizes the total cost paid by the cloud providers, taking into account both deployment and certification/verification costs. Our service composition approach is driven by certificates awarded to single services and by a fuzzy-based cost evaluation methodology, and assumes certified properties as must-have requirements for service selection and composition. © 2019 Elsevier Inc.","Certification; Cloud; Cost optimization; Security; Service composition"
"CHAMELEON: Reactive Load Balancing for Hybrid MPI+OpenMP Task-Parallel Applications","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077513415&doi=10.1016%2fj.jpdc.2019.12.005&partnerID=40&md5=dd8030737b8d6c3266f83183db239222","Many applications in high performance computing are designed based on underlying performance and execution models. While these models could successfully be employed in the past for balancing load within and between compute nodes, modern software and hardware increasingly make performance predictability difficult if not impossible. Consequently, balancing computational load becomes much more difficult. Aiming to tackle these challenges in search for a general solution, we present a novel library for fine-granular task-based reactive load balancing in distributed memory based on MPI and OpenMP. With our approach, individual migratable tasks can be executed on any MPI rank. The actual executing rank is determined at run time based on online performance data. We evaluate our approach under an enforced power cap and under enforced clock frequency changes for a synthetic benchmark and show its robustness for work-induced imbalances for a realistic application. Our experiments demonstrate speedups of up to 1.31X. © 2019 The Authors","Hybrid MPI+openMP; Load balancing; Reactivity; Task migration; Tasking"
"MiniChain: A lightweight protocol to combat the UTXO growth in public blockchain","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084943366&doi=10.1016%2fj.jpdc.2020.05.001&partnerID=40&md5=317c5a7997966aab93406969a01a5590","The current UTXO-based blockchains require the validators to keep the entire ever-growing UTXO set to verify transactions, which is unsuitable for ordinary machines since they occupy a large size of RAM in the long run, resulting in network centralizations. Recently, stateless blockchain technology has been proposed which uses the accumulator to combine the large UTXO set into one short, constant-sized commitment. However, the UTXO commitments in these methods are inefficient since the UXTO set is required dynamic addition and removal of elements as transactions are processed. In this work, we propose MiniChain, which replaces the UTXO set with two append-only data structures: STXO (Spent Transaction Outputs) set and TXO (Transaction Outputs) set. Thus, a valid UTXO must belong to the TXO set, but not in STXO set. Then, we construct a novel STXO commitment and TXO commitment by using a trapdoor-less RSA accumulator and a Merkle Mountain Range (MMR) respectively, greatly increasing the efficiency of accumulator. Besides, we introduce a cache mechanism, by storing the STXOs of latest N blocks, the transaction proof can be kept alive for a period of time, avoiding constantly recomputing proofs for unaccepted transactions. Our evaluation shows that (i) MiniChain only needs a fixed-size RAM and the disk usage grows very slow since only the block headers are stored; (ii) comparing to the state-of-the-art work, the performance of the accumulator update has been improved from O(n2) to O(n), enabling MiniChain to support a higher TPS. © 2020 Elsevier Inc.","RSA accumulator; Stateless blockchain; STXO commitment; TXO commitment; UTXO growth"
"Prime clock: Encoded vector clock to characterize causality in distributed systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081036477&doi=10.1016%2fj.jpdc.2020.02.008&partnerID=40&md5=2a91a1e124109892fe9e9698d8231974","The vector clock is a fundamental tool for tracking causality in distributed applications. Unfortunately, it does not scale well to large systems because each process needs to maintain a vector of size n, where n is the total number of processes in the system. To address this problem, we propose the prime clock, which is based on the encoding of the vector clock using prime numbers and uses a single number to represent vector time. We propose the operations on the encoded vector clock (EVC). We then show how to timestamp global states and how to perform operations on the global states using the EVC. Using a theoretical analysis and a simulation model, we evaluate the growth rate of the size of the EVC. The EVC is seen to grow very fast and hence it does not appear to offer a general purpose practical replacement of vector clocks. To address this drawback, we propose several scalability techniques for the EVC that can allow the use of the EVC in practical applications. We then present two case studies of detecting memory consistency errors in MPI one-sided applications and of dynamic race detection in multi-threaded environments, that use a combination of two of these scalability techniques. The results show that the EVC is not just a theoretical concept, but it is applicable to practical problems and can compete in terms of both space and time requirements with other known protocols. © 2020 Elsevier Inc.","Causality; Encoding; Happened-before relation; Prime numbers; Vector clock"
"Cost optimization of secure routing with untrusted devices in software defined networking","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084605085&doi=10.1016%2fj.jpdc.2020.03.021&partnerID=40&md5=96d730ca374a72453ab39814a1682476","Over the years, switches and network routers have been compromised frequently, and a lot of vulnerabilities have occurred in network infrastructure. Secure routing (SR) is one of the challenges that currently exists in computer networks. Software-defined networks (SDN) are designed by assuming that routers or switches are trustworthy. In SDN, untrusted devices have resulted in security issues such as traffic analysis, failure to receive information, packet change and removal, inaccurate routing, and network downtime. Using encryption methods is a possible solution to deal with some of these problems, but it requires additional infrastructure as well as significant overhead at runtime. One of the most trusted routing methods is through replicated devices (switches or routers). Recently we have seen less attention given to the number of replicated devices in SR. In this paper, the problem of SR is converted into a multi-objective optimization problem, considering the reliability of different manufacturers and SR with untrusted devices is performed. To this end, a mathematical model is provided to study the objectives of maximum reliability and cost minimization. The NSGA-II algorithm is applied to determine the optimal number of replicated devices in order to minimize the cost of implementing SR in spite of the presence of untrusted devices in SDN. Our simulation results show that our proposed method compared to the base method (without considering optimization) decreases implementation cost by 27% and increases the reliability from 70% to 93.2%. © 2020 Elsevier Inc.","Cost optimization; Multi-objective optimization; Network security; SDN; Secure routing"
"Intrusion detection in Edge-of-Things computing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077079190&doi=10.1016%2fj.jpdc.2019.12.008&partnerID=40&md5=06d5bd3729024db0f4675eb20f806579","Edge-of-Things (EoT) is a new evolving computing model driven by the Internet of Things (IoT). It enables data processing, storage, and service to be shifted from the Cloud to nearby Edge devices/systems such as smartphones, routers, and base stations on the IoT paradigm. However, this architectural shift causes the security and privacy issues to migrate to the different layers of the Edge architecture. Therefore, detecting intrusion in such a distributed environment is difficult. In this scenario, an Intrusion Detection Systems is necessary. Here, we propose an approach to quickly and accurately detect intrusive activities in the EoT network, to realize the full potential of the IoT. Specifically, we propose a deep belief network (DBN) based on an advanced intrusion detection approach. We studied different detection models, by using different structures of DBNs, and compared them with existing detection techniques. Test results show that the proposed methodology performs essentially superior to the current state-of-the-art approaches. © 2019 Elsevier Inc.","Deep belief network; Deep learning; Edge computing; Internet-of-Things; Intrusion detection"
"Dynamic memory-aware scheduling in spark computing environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083082656&doi=10.1016%2fj.jpdc.2020.03.010&partnerID=40&md5=05fca91fce549d475fbda36e3f562546","Scheduling plays an important role in improving the performance of big data-parallel processing. Spark is an in-memory parallel computing framework that uses a multi-threaded model in task scheduling. Most Spark task scheduling processes do not take the memory into account, but the number of concurrent task threads determined by the user. It emerges as a potential limitation for the performance. To overcome the limitations in the Spark-core source code, this paper proposes a dynamic Spark memory-aware task scheduler (DMATS), which not only treats memory and network I/O as a computational resource but also dynamically adjusts concurrency when scheduling tasks. Specifically, we first analyze the RDD based Spark execution engine to obtain the amount of task processing data and propose an algorithm for estimating the initial adaptive task concurrency, which is integrated with the known task input information and the executor memory. Then, a dynamic adjustment algorithm is proposed to change the concurrency dynamically through feedback information to optimally utilize the limited memory resources. We implement a dynamic memory-aware task scheduling (DMATS) in Spark 2.3.4 and evaluate performance with two typical benchmarks, shuffle-light and shuffle-heavy. The results show that the algorithm not only reduces the execution time by 43.64%, but also significantly improves resource utilization. Experiments also show that our proposed method has advantages compared with other similar works such as WASP. © 2020 Elsevier Inc.","Concurrency; Dynamic adjustment; Memory resource; Spark; Task scheduling"
"Design and implementation of multiple-precision BLAS Level 1 functions for graphics processing units","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081030374&doi=10.1016%2fj.jpdc.2020.02.006&partnerID=40&md5=3fb6af8cf102f226fc480b34af80a122","Basic Linear Algebra Subprograms (BLAS) are the building blocks for various numerical algorithms and are widely used in scientific computations. However, some linear algebra applications need more precision than the standard double precision available in most existing BLAS libraries. In this paper, we implement and evaluate multiple-precision scalar and vector BLAS functions on graphics processing units (GPUs). We use the residue number system (RNS) to represent arbitrary length floating-point numbers. The non-positional nature of RNS enables parallelism in multiple-precision arithmetic and makes RNS a good tool for high-performance computing applications. We first present new data-parallel algorithms for multiplying and adding RNS-based floating-point representations. Next, we suggest algorithms for multiple-precision vectors specially designed for parallel computations on GPUs. Using these algorithms, we develop and evaluate four GPU-accelerated multiple-precision BLAS functions, ASUM, DOT, SCAL, and AXPY. It is shown through experiments that in many cases, the implemented functions achieve significantly better performance compared to existing multiple-precision software for CPU and GPU. © 2020 Elsevier Inc.","BLAS; High-performance computing; Multiple-precision arithmetic; Parallel processing; Residue number system"
"BTNC: A blockchain based trusted network connection protocol in IoT","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084578187&doi=10.1016%2fj.jpdc.2020.04.004&partnerID=40&md5=2a52a86c1709dffa1f347eb5d34ac878","Along with the rapid growth of the size and complexity of Internet of Things (IoT), the security of terminal devices has increasingly become a focus. In order to ensure the security of terminals, the trusted network connect (TNC) could realize not only the user authentication but also the platform attestation during the network access process. However, the existing TNC infrastructure is based on a centralized architecture, which is not suitable for distributed services. To address this problem, we present a blockchain-based TNC protocol named BTNC to ensure the reliability of terminals in IoT. Due to the decentralization, trustlessness, trackability, and immutability features of blockchain, BTNC is able to verify the security of terminal devices in IoT networks. First, we come up with some threats, including unauthorized user, illegal platform and platform replacement attack, then correspondingly define the security goals of our scheme. Second, combining key exchange protocol based on blockchain and D–H PN protocol included in TNC specification, we propose a blockchain-based trusted network connection protocol, which realizes mutual user authentication, platform attestation and trust network access by cryptography among terminals in IoT. Third, we make a security analysis in the PCL mode and conclude that our protocol can resist the attacks above. Finally, the performance overheads caused by our scheme are evaluated and the experiments show that it is efficient and feasible for different kinds of terminals in IoT. © 2020 Elsevier Inc.","Blockchain; IoT; Key exchange; Platform measurement; Trusted network connection"
"Performance evaluation of decision making under uncertainty for low power heterogeneous platforms","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075754909&doi=10.1016%2fj.jpdc.2019.11.009&partnerID=40&md5=dc3c2c417ebcd95b05f505b18dc3d44b","Value Iteration (VI) is a core method to find optimal policies, allowing a robot to act autonomously in environments where the effects of its actions are not deterministic. Although there are extensive studies on VI's theoretical properties and computational cost, its energy performance — an essential indicator for its use in the physical world — has not been evaluated. In this paper, we explore both the energy and runtime performance of five parallel implementation strategies of VI on representative low-power heterogeneous computing platforms that integrate CPUs and GPUs, for the use-case scenario of indoor autonomous robot navigation. We provide a statistical analysis of their performance depending on the problem size, parallel implementation and computing platform. Our study shows that CPU–GPU heterogeneous strategies reduce computation time and energy considerably, given large enough problem sizes, regardless of the computation platform. This work also provides practical guidelines to assist in the application of the most efficient implementation, either in terms of energy consumption or time, to a low-power heterogeneous platform. © 2019 Elsevier Inc.","Low-power heterogeneous architectures; Markov Decision Processes; Mobile robot navigation; Sequential decision making under uncertainty; Value Iteration"
"Multicore and manycore parallelization of cheap synchronizing sequence heuristics","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081034966&doi=10.1016%2fj.jpdc.2020.02.009&partnerID=40&md5=373baf7b9bbd66f4115f839537033123","An important concept in finite state machine based testing is synchronization which is used to initialize an implementation to a particular state. Usually, synchronizing sequences are used for this purpose and the length of the sequence used is important since it determines the cost of the initialization process. Unfortunately, the shortest synchronization sequence problem is NP-Hard. Instead, heuristics are used to generate short sequences. However, the cubic complexity of even the fastest heuristic algorithms can be a problem in practice. In order to scale the performance of the heuristics for generating short synchronizing sequences, we propose algorithmic improvements together with a parallel implementation of the cheapest heuristics existing in the literature. To identify the bottlenecks of these heuristics, we experimented on random and slowly synchronizing automata. The identified bottlenecks in the algorithms are improved by using algorithmic modifications. We also implement the techniques on multicore CPUs and Graphics Processing Units (GPUs) to take benefit of the modern parallel computation architectures. The sequential implementation of the heuristic algorithms are compared to our parallel implementations by using a test suite consisting of 1200 automata. The speedup values obtained depend on the size and the nature of the automaton. In our experiments, we observe speedup values as high as 340x by using a 16-core CPU parallelization, and 496x by using a GPU. Furthermore, the proposed methods scale well and the speedup values increase as the size of the automata increases. © 2020 Elsevier Inc.","Finite state automata; Graphics processing units; Parallel algorithms; Software testing; Synchronizing sequences"
"GuardHealth: Blockchain empowered secure data management and Graph Convolutional Network enabled anomaly detection in smart healthcare","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083378724&doi=10.1016%2fj.jpdc.2020.03.004&partnerID=40&md5=4e20dc6837fb031280d6c816542a5153","The paradox between the dramatic development of medical data privacy demand and years of bureaucratic regulation has slowed innovation for electronic medical records (EMRs). We are at a historical point for such innovation to prompt patients data autonomy. In this paper, we propose GuardHealth: an efficient, secure and decentralized Blockchain system for data privacy preserving and sharing. GuardHealth manages confidentiality, authentication, data preserving and data sharing when handling sensitive information. We exploit consortium Blockchain and smart contract to achieve secure data storage and sharing, which prevents data sharing without permission. A trust model is utilized for precisely managing trust of users with the implementation of the state-of-art Graph Neural Network (GNN) for malicious node detection. Security analysis and experiment results show that the proposed scheme is applicable for smart healthcare system. © 2020 Elsevier Inc.","Blockchain; Graph Convolutional Network; Security and privacy; Smart healthcare; Trust assessment"
"An efficient fault tolerant workflow scheduling approach using replication heuristics and checkpointing in the cloud","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073981275&doi=10.1016%2fj.jpdc.2019.09.004&partnerID=40&md5=55f8dee5bd8724cc47a23bc6af6376f3","Scientific workflows have been predominantly used for complex and large scale data analysis and scientific computation/automation and the need for robust workflow scheduling techniques has grown considerably. But, most of the existing workflow scheduling algorithms do not provide the required reliability and robustness. In this paper, a new fault tolerant workflow scheduling algorithm that learns replication heuristics in an unsupervised manner has been proposed. Furthermore, the use of light weight synchronized checkpointing enables efficient resubmission of failed tasks and ensures workflow completion even in precarious environments. The proposed technique improves upon metrics like Resource Wastage and Resource Usage in comparison to the Replicate-All algorithm, while maintaining an acceptable increase in Makespan as compared to the vanilla Heterogeneous Earliest Finish Time (HEFT). © 2019 Elsevier Inc.","Checkpointing; Clustering; Clustering; Resubmission; Workflow Scheduling"
"IoTSim-SDWAN: A simulation framework for interconnecting distributed datacenters over Software-Defined Wide Area Network (SD-WAN)","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084434126&doi=10.1016%2fj.jpdc.2020.04.006&partnerID=40&md5=df6505dff576488cbdbd0aaa4727ac73","Software-defined networking (SDN) has evolved as an approach that allows network administrators to program and initialize, control, change and manage networking components (mostly at L2-L3 layers) of the OSI model. SDN is designed to address the programmability shortcomings of traditional networking architectures commonly used in cloud datacenters (CDC). Deployment of SDN solutions have demonstrated significant improvements in areas such as flow optimization and bandwidth allocation in a CDC. However, the benefits are significantly less explored when considering Software-Defined Wide Area Networks (SD-WAN) architectures in the context of delivering solutions by networking multiple CDCs. To support the testing and bench-marking of data-driven applications that rely on data ingestion and processing (e.g., Smart Energy Cloud, Content Delivery Networks) across multiple cloud datacenters, this paper presents the simulator, IoTSim-SDWAN. To the best of our knowledge, IoTSim-SDWAN is the first simulator that facilitates the modeling, simulating, and evaluating of new algorithms, policies, and designs in the context of SD-WAN ecosystems and SDN-enabled multiple cloud datacenters. Finally, IoTSim-SDWAN simulator is evaluated for network performance and energy to illustrate the difference between classical WAN and SD-WAN environments. The obtained results show that SD-WAN surpasses the classical WAN in terms of accelerating traffic flows and reducing power consumption. © 2020 Elsevier Inc.","Classical WAN; Internet of Things (IoT); Software-Defined Network (SDN); Software-Defined Wide Area Network (SD-WAN)"
"Predicting and reining in application-level slowdown on spatial multitasking GPUs","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083166448&doi=10.1016%2fj.jpdc.2020.03.009&partnerID=40&md5=483019f89c1a0b62b7e8a7c7da79d944","Predicting performance degradation of a GPU application at co-location on a spatial multitasking GPU without prior application knowledge is essential in public Clouds. Prior work mainly targets CPU co-location, and is inaccurate and/or inefficient for predicting performance of applications at co-location on spatial multitasking GPUs. Our investigation shows that hardware event statistics caused by co-located applications strongly correlate with their slowdowns. Based on this observation, we present Themis with a kernel slowdown model (Themis-KSM), which performs precise and efficient online application slowdown prediction without prior application knowledge. The kernel slowdown model is trained offline. When new applications co-run, Themis-KSM collects event statistics and predicts their slowdowns simultaneously. In addition, we also propose a two-stage slowdown prediction mechanism (Themis-TSP) for real-system GPUs without any hardware modification. Our evaluation shows that Themis has negligible runtime overhead, and both Themis-KSM and Themis-TSP can precisely predict application-level slowdown with prediction error smaller than 9.5% and 12.8%, respectively. Based on Themis, we also implement an SM allocation engine to rein in application slowdown at co-location. Case studies show that the engine successfully enforces fair sharing and QoS. © 2020 Elsevier Inc.","Co-location; Performance prediction; Sharing GPU; Slowdown prediction; Spatial Multitasking GPU"
"A blockchain based decentralized data security mechanism for the Internet of Things","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082858215&doi=10.1016%2fj.jpdc.2020.03.005&partnerID=40&md5=d9e831f08b24cac64db95cad31d957ec","The Internet of Things (IoT) is the core infrastructure of the smart city information system. With the explosive growth of IoT devices, how to securely maintain the important data generated by IoT devices has become an important issue. In the conventional IoT-cloud based infrastructure, the sensitive IoT data was stored in a third cloud service provider. However, in such a manner, the private IoT data may be disclosed by the cloud server since the cloud server knows all the data stored in it. This paper, for the first time, proposes a decentralized secure mechanism based on the blockchain technique to store the important data generated in the IoT system. This mechanism effectively solves the data reliability, security and privacy issues that may be encountered in the conventional IoT-cloud system. Considering the defects of simplified payment verification used by light nodes in blockchain networks, this paper proposes an Unspent Transaction Output (UTXO) verification mechanism based on the RSA accumulator, which makes the computational complexity of light nodes to generate and verify the UTXO proof to be constant. The proposed mechanism not only provides proof of inclusion but also supports efficient proof of exclusion for a lightweight node. Our experiment results indicate that the proposed scheme is practical and efficient. © 2020 Elsevier Inc.","Accumulator; Blockchain; IoT; UTXO"
"Efficient and secure flash-based gaming CAPTCHA","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083399023&doi=10.1016%2fj.jpdc.2020.03.020&partnerID=40&md5=b3a5f6bef45299b9f34e6a4b39b4b1d5","With the growth of connectivity to smart grids, new applications, and the changing interaction between customer and energy clouds, clouds are more vulnerable to denial-of-service attacks. Efficient detection methods are required to authenticate, detect and control attackers. Completely Automated Public Turing test to tell Computers and Humans Apart, CAPTCHA, is one efficient tool to thwart denial of service attacks. The server presents the user with a client puzzle to solve in order to gain access to the service or website. The puzzle should be hard enough for computers, but easy for humans to solve. Several methods have been suggested including the popular image-based, as well as video-based, and text-based CAPTCHAs. In this paper, we present a new Flash-based gaming CAPTCHA to differentiate bots from humans. We propose a drag and drop client puzzle where the user will play a simple game to answer a visual question. Our method turns out to be convenient, easy for users and challenging for bots. Additionally, it has gaming aspect, which makes it interesting to users of all age groups. © 2020 Elsevier Inc.","Client puzzles; Energy cloud; Flash CAPTCHA; Gaming CAPTCHA; Turing test"
"Parallel tiled cache and energy efficient codes for O(n4) RNA folding algorithms","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076881312&doi=10.1016%2fj.jpdc.2019.12.007&partnerID=40&md5=ab11b0169c88cb5774eab6def5a022f9","In this paper, we consider two O(n4) RNA folding algorithms, Zuker's recurrence and the maximum expected accuracy prediction (MEA), which are challenging dynamic programming tasks to optimize because they are computationally intensive and have a large number of non-uniform dependences. We apply our previously published approach to automatically tile and parallelize each loop in the studied algorithms by means of the polyhedral model. First, for each loop nest statement, rectangular tiles are formed within the iteration space of the loop nest. Then, those tiles are corrected to honor all dependences exposed for the original loop nest. Correction is based on applying the exact transitive closure of a dependence graph. We implemented our approach as a part of the source-to-source TRACO compiler, generated target code, and compare the performance and energy consumption of generated code with those of code obtained with the state-of-the-art PluTo compiler based on the affine transformation framework as well as with those of code generated by means of the manual cache-efficient Transpose method. Experiments were carried out on a modern multi-core processor to achieve the significant locality improvement and energy saving for generated code. © 2019 Elsevier Inc.","Energy consumption; High-performance computing; Loop tiling; RNA folding; Zuker and MEA algorithms"
"QuickDedup: Efficient VM deduplication in cloud computing environments","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078964465&doi=10.1016%2fj.jpdc.2020.01.002&partnerID=40&md5=10c1fc0c0a74b2f24fd88f07989990d2","Deduplication is one of the major storage optimisation techniques for Virtual Machines (VMs) in cloud environment. Usually, hashing of blocks helps in identifying duplicate data blocks. This paper proposes a novel deduplication approach, QuickDedup that reduces the overall deduplication time, metadata overhead and the number of hash computations, and subsequent comparisons for the VM disk images. In addition to minimising the deduplication related metadata, which is a necessary by-product useful in checking deduplication, QuickDedup, follows novel byte comparison scheme to prepare various block classes. This way, QuickDedup eliminates or minimises the need for hash calculation and subsequent comparisons. QuickDedup performs the calculation and comparisons of hashes within the respective categories only. QuickDedup saves the space required for hash storage during deduplication and makes deduplication of VM disk images much faster. We conducted a detailed evaluation of QuickDedup on various metrics with different kinds and sizes of VM images taken from publicly available datasets. The evaluation results show a substantial improvement of up to 96% in the overall deduplication time required to deduplicate VM images apart from significant savings in metadata and storage overhead. © 2020 Elsevier Inc.","Deduplication; Hashing performance; Storage; VM disk image"
"On demand clock synchronization for live VM migration in distributed cloud data centers","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077050651&doi=10.1016%2fj.jpdc.2019.11.012&partnerID=40&md5=3b94f70d9964c561d0cc128ee0e0e52c","Live migration of virtual machines (VMs) has become an extremely powerful tool for cloud data center management and provides significant benefits of seamless VM mobility among physical hosts within a data center or across multiple data centers without interrupting the running service. However, with all the enhanced techniques that ensure a smooth and flexible migration, the down-time of any VM during a live migration could still be in a range of few milliseconds to seconds. But many time-sensitive applications and services cannot afford this extended down-time, and their clocks must be perfectly synchronized to ensure no loss of events or information. In such a virtualized environment, clock synchronization with minute precision and error boundedness are one of the most complex and tedious tasks for system performance. In this paper, we propose enhanced DTP and wireless PTP based clock synchronization algorithms to achieve high precision at intra and inter-cloud data center networks. We thoroughly analyze the performance of the proposed algorithms using different clock measurements. Through simulation and real-time experiments, we also show the effect of various performance parameters on the data center networking architectures. © 2019 Elsevier Inc.","Cloud data centers; Data center networks; Data center Time Protocol (DTP); Live VM migration; Precision Time Protocol (PTP)"
"Efficient authentication protocol with anonymity and key protection for mobile Internet users","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076235454&doi=10.1016%2fj.jpdc.2019.11.010&partnerID=40&md5=f3df3850a1edc9aaf3c7c4060864c9bd","To preserve user privacy and guarantee data confidentiality on the mobile Internet, it is crucial to secure communication between the mobile devices held by users and a remote server. In real applications, a serious threat against communication security is exposure of secret keys, due to the compromise of the mobile devices storing the key. One method of preserving key exposure is to use protected hardware or smart-cards, but they are costly and impractical. Another method is to utilize secret sharing to share secret key across multiple devices. Nevertheless, secret sharing schemes guarantee security only if the adversary cannot access at least one share in its entirety. In this paper, we present a remote authentication protocol, which resists key exposure. Further, we present a zero-knowledge protocol based on SDH assumption that can achieve anonymity. We formally prove our proposed solution is secure under the decision linear assumption and the qs-mSDH assumption in the random oracle model. Finally, we show our solution can achieve higher efficiency and stronger anonymity comparing with existing schemes, and thus the proposed solution is more suitable for real-world environments. © 2019 Elsevier Inc.","Anonymity; Key protection; Mobile Internet; Zero-knowledge protocols"
"Profit-aware application placement for integrated Fog–Cloud computing environments","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073533475&doi=10.1016%2fj.jpdc.2019.10.001&partnerID=40&md5=a1ed519926d1bc3c2540b8c8b78dbac0","The marketplace for Internet of Things (IoT)-enabled smart systems is rapidly expanding. The integration of Fog and Cloud paradigm aims at harnessing both edge device and remote datacentre-based computing resources to meet Quality of Service (QoS) requirements of these smart systems. Due to lack of instance pricing and revenue maximizing techniques, it becomes difficult for service providers to make comprehensive profit from such integration. This problem further intensifies when associated expenses and allowances are charged from the revenue. Conversely, the rigid revenue maximizing intention of providers affects user's budget and system's service quality. To address these issues, we propose a profit-aware application placement policy for integrated Fog–Cloud environments. It is formulated using constraint Integer Linear Programming model that simultaneously enhances profit and ensures QoS during application placement on computing instances. Furthermore, it provides compensation to users for any violation of Service Level Agreement (SLA) and sets the price of instances according to their ability of reducing service delivery time. The performance of proposed policy is evaluated in a simulated Fog–Cloud environment using iFogSim and the results demonstrate that it outperforms other placement policies in concurrently increasing provider's profit and user's QoS satisfaction rate. © 2019 Elsevier Inc.","Application placement; Fog–Cloud integration; Internet of Things; Pricing model; Profit-awareness"
"Malware detection in mobile environments based on Autoencoders and API-images","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075060873&doi=10.1016%2fj.jpdc.2019.11.001&partnerID=40&md5=089239033998151930657bdb91a730eb","Due to their open nature and popularity, Android-based devices represent one of the main targets for malware attacks that may adversely affect the privacy of their users. Considering the huge Android market share, it is necessary to build effective tools able to reliably detect zero-day malware on these platforms. Therefore, several static and dynamic analysis methods based on Neural Networks and Deep Learning have been proposed in the literature. Despite machine learning can be considered the most promising approach for classifying applications into malware or legitimate ones, its success strongly depends on the choice of the right features used for building the detection model. This is definitely not an easy task that requires a systematic solution. Accordingly, this work represents the sequences of API calls invoked by apps during their execution as sparse matrices looking like images (API-images), which can be used as fingerprints of the apps’ behavior over time. We also used autoencoders to autonomously extract the most representative and discriminating features from these matrices, that, once provided to an artificial neural network-based classifier have shown to be effective in detecting malware, also when the network is trained on a reduced number of samples. Experimental results show that the resulting framework is able to outperform more complex and sophisticated machine learning approaches in malware classification. © 2019 Elsevier Inc.","Android; Autoencoders; Deep learning; Dynamic analysis; Malware"
"Blockchain for secure location verification","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074527923&doi=10.1016%2fj.jpdc.2019.10.007&partnerID=40&md5=f15e79ad4e84279ad5179cd90a4e3b13","In location-sensitive applications, dishonest users may submit fake location claims to illegally access a service or obtain benefit. To address this issue, a number of location proof mechanisms have been proposed in literature. However, they confront various security and privacy challenges, including Prover–Prover collusions (Terrorist Frauds), Prover–Witness collusions, and location privacy threats. In this paper, we utilize the unique features of the blockchain technology to design a decentralized scheme for location proof generation and verification. In the proposed scheme, a user who needs a location proof (called a prover) broadcasts a request to the neighbor devices through a short-range communication interface, e.g. Bluetooth. Those neighbor devices that decide to respond (called witnesses) start to authenticate the requesting user. We integrate an incentive mechanism into the proposed scheme to reward such witnesses. Upon successful authentication, a transaction is generated as a location proof and is broadcast onto a peer-to-peer network where it can be picked up by verifiers for final verification. Our security analysis shows that the proposed scheme achieves a reliable performance against Prover–Prover and Prover–Witness collusions. Moreover, our prototype implementation on the Android platform shows that the proposed scheme outperforms other currently deployed location proof schemes. © 2019 Elsevier Inc.","Distance bounding; Location privacy; Location proof; Location-based services"
"Cost effective dynamic data placement for efficient access of social networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083082398&doi=10.1016%2fj.jpdc.2020.03.013&partnerID=40&md5=abcd73b758a36cc3525229c163cff731","Social networks boast a huge number of worldwide users who join, connect, and publish various content, often very large, e.g. videos, images etc. For such very large-scale data storage, data replication using geo-distributed cloud services with virtually unlimited capabilities are suitable to fulfill the users’ expectations, such as low latency when accessing their and their friends’ data. However, service providers ideally want to spend as little as possible on replicating users’ data. Moreover, social networks have a dynamic nature and thus replicas need to be adaptable according to the environment, users’ behaviors, social network topology, and workload at runtime. Hence, it is not only crucial to have an optimized data placement and request distribution – meeting individual users’ acceptable latency requirements while incurring minimum cost for service providers – but the data placement must be adapted based on changes in the social network to keep it efficient and effective over time. In this paper, we model data placement as a dynamic set cover problem and propose a novel approach to solve this problem. We have run several experiments using two large-scale, open Facebook and Gowala datasets and real latencies derived from Amazon cloud datacenters to demonstrate our novel strategy's efficiency and effectiveness. © 2020 Elsevier Inc.","Access latency; Cost optimization; Data placement; Data replication; Social networks"
"Scalable energy-efficient parallel sorting on a fine-grained many-core processor array","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077239644&doi=10.1016%2fj.jpdc.2019.12.011&partnerID=40&md5=7ebacc93aa87b92a1c903a52c0f8cd99","Three parallel sorting applications and two list output protocols for the first phase of an external sort execute on a fine-grained many-core processor array that contains no algorithm-specific hardware acting as a co-processor with a variety of array sizes. Results are generated using a cycle-accurate model based on measured data from a fabricated many-core chip, and simulated for different processor array sizes. The data shows most energy efficient first-phase many-core sort requires over 65× lower energy than GNU C++ standard library sort performed on an Intel laptop-class processor and over 105× lower energy than a radix sort running on an Nvidia GPU. In addition, the highest first-phase throughput many-core sort is over 9.8× faster than the std::sort and over 14× faster than the radix sort. Both phases of a 10 GB external sort require 6.2× lower energy× time energy delay product than the std::sort and over 13× lower energy×time than the radix sort. © 2019 Elsevier Inc.","External sorting; Fine-grained many-core; Parallel processing; Processor array; Scalable sorting"
"En-ABC: An ensemble artificial bee colony based anomaly detection scheme for cloud environment","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073953051&doi=10.1016%2fj.jpdc.2019.09.013&partnerID=40&md5=b6442a603b63a054331930ec748ac2f5","With an exponential increase in the usage of different types of services and applications in cloud computing environment, the identification of malicious behavior of different nodes becomes challenging due to the diversity of traffic patterns generated from various services and applications. Most of the existing solutions reported in the literature are restricted with respect to the usage of a specific technique applicable to single class datasets. But in real life scenarios, applications and services especially in cloud environment may have multi-class datasets. Moreover, non-linear behavior among the dataset attributes generates additional challenges for identification of nodes behavior, and it has not been exploited to its full potential in the existing solutions. This can lead to performance bottlenecks with respect to the identification of malicious behavior of different nodes. Motivated from these facts, this paper proposes an Ensemble Artificial Bee Colony based Anomaly Detection Scheme (En-ABC) for multi-class datasets in cloud environment. En-ABC has following components for identification of malicious behavior of nodes-(i) feature selection and optimization, (ii) data clustering, and (iii) identification of anomalous behavior of nodes. The feature selection and optimization model in En-ABC has been built using Restricted Boltzmann Machine and Unscented Kalman Filter (to handle the non-linear behavior of dataset attributes) respectively. Moreover, Artificial Bee Colony-based Fuzzy C-means clustering technique is used to obtain an optimal clustering based on two objective functions, i.e., Mean Square Deviation and Dunn Index (to handle the participation of attributes in multiple clustered datasets). Then, a profile of normal/abnormal behavior has been built using clustering results for detection of the anomalies. Finally, the performance of the proposed scheme has been compared with the existing schemes (CM, SVM, ML-IDS and MSADA) using various parameters such as-detection, false alarm, and accuracy rates. Experimental results on benchmark (NSL-KDD, NAB and IBRL) and synthetic datasets validate the effectiveness of the proposed scheme. © 2019 Elsevier Inc.","Anomaly detection; Artificial bee colony algorithm; Cloud computing; Fuzzy C-means clustering; Restricted Boltzmann machine; Unscented Kalman filter"
"A semantic-based methodology for digital forensics analysis","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077796899&doi=10.1016%2fj.jpdc.2019.12.017&partnerID=40&md5=60de91ebb921dbd7f408f0f723fd0509","Nowadays, more than ever, digital forensics activities are involved in any criminal, civil or military investigation and represent a fundamental tool to support cyber-security. Investigators use a variety of techniques and proprietary software forensics applications to examine the copy of digital devices, searching hidden, deleted, encrypted, or damaged files or folders. Any evidence found is carefully analysed and documented in a “finding report” in preparation for legal proceedings that involve discovery, depositions, or actual litigation. The aim is to discover and analyse patterns of fraudulent activities. In this work, a new methodology is proposed to support investigators during the analysis process, correlating evidence found through different forensics tools. The methodology was implemented through a system able to add semantic assertion to data generated by forensics tools during extraction processes. These assertions enable more effective access to relevant information and enhanced retrieval and reasoning capabilities. © 2019 Elsevier Inc.","Correlation; Cybersecurity; Digital forensics; Log analysis; Text analysis"
"CloudFNF: An ontology structure for functional and non-functional features of cloud services","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083359296&doi=10.1016%2fj.jpdc.2020.03.019&partnerID=40&md5=0cd6c049b4784aeab47e9ef1aeb95522","Recently, cloud computing becomes one of the main orientations of many researchers and companies in the IT area. Therefore, a huge number of cloud services have been developed. Because of the diversity and heterogeneity in providing these services, it is urgently needed to develop a unified cloud ontology. Such ontology can classify these services appropriately and participate as a mapping layer to present such services in a unified description format. Although many studies have been conducted to build cloud ontologies, they have adopted the cloud service layers-based structure. On the other hand, the existing cloud services may involve functionalities from different layers due to the continual increase in the complexity of customer demands. Unfortunately, there are no clear relations to organize the interventions among these layers. Therefore, such services may be difficult to be classified into a specific cloud service layer. Additionally, the layers-based structure of ontologies represents an obstacle to address important issues, such as cloud service recommendation. Despite there are few cloud service functionality-based cloud ontologies, they suffer from many overlaps, lack of semantic relations, or poor granularity of concepts. Also, all the existing cloud ontologies (i.e., layers-based and functionality-based) lack important criteria, such as completeness, consistency, conciseness, clarity, preciseness, and granularity. In this paper, a comprehensive cloud ontology called CloudFNF has been proposed to overcome such drawbacks. According to the structure of the proposed ontology, cloud services are classified as functionality-based instead of layers-based. Also, non-functional features of cloud services (i.e., configuration and QoS features) are considered to enable services of the same functionalities to be ranked efficiently. Based on our previously suggested cloud ontology evaluation framework, our proposed cloud ontology has been evaluated compared to the most related cloud ontologies. The evaluation results show that the proposed CloudFNF ontology outperforms the other ontologies. © 2020 Elsevier Inc.","Cloud computing; Cloud ontology; Cloud service recommendation; Cloud taxonomy; QoS"
"Accelerated serverless computing based on GPU virtualization","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079343382&doi=10.1016%2fj.jpdc.2020.01.004&partnerID=40&md5=3eaacf73d6dbd03844b1ca8ed55c7054","This paper introduces a platform to support serverless computing for scalable event-driven data processing that features a multi-level elasticity approach combined with virtualization of GPUs. The platform supports the execution of applications based on Docker containers in response to file uploads to a data storage in order to perform the data processing in parallel. This is managed by an elastic Kubernetes cluster whose size automatically grows and shrinks depending on the number of files to be processed. To accelerate the processing time of each file, several approaches involving virtualized access to GPUs, either locally or remote, have been evaluated. A use case that involves the inference based on deep learning techniques on transthoracic echocardiography imaging has been carried out to assess the benefits and limitations of the platform. The results indicate that the combination of serverless computing and GPU virtualization introduce an efficient and cost-effective event-driven accelerated computing approach that can be applied for a wide variety of scientific applications. © 2020 Elsevier Inc.","GPU virtualization; GPUs; Serverless computing"
"Distributed on-demand clustering algorithm for lifetime optimization in wireless sensor networks","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083320202&doi=10.1016%2fj.jpdc.2020.03.014&partnerID=40&md5=3ba47b0913c7b1574c89352111405398","Wireless Sensor Networks (WSNs) play a significant role in Internet of Things (IoT) to provide cost effective solutions for various IoT applications, e.g., wildlife habitat monitoring, but are often highly resource constrained. Hence, preserving energy (or, battery power) of sensor nodes and maximizing the lifetime of WSNs is extremely important. To maximize the lifetime of WSNs, clustering is commonly considered as one of the efficient technique. In a cluster, the role of individual sensor nodes changes to minimize energy consumption, thereby prolonging network lifetime. This paper addresses the problem of lifetime maximization in WSNs by devising a novel clustering algorithm where clusters are formed dynamically. Specifically, we first analyze the network lifetime maximization problem by balancing the energy consumption among cluster heads. Based on the analysis, we provide an optimal clustering technique, in which the cluster radius is computed using alternating direction method of multiplier. Next, we propose a novel On-demand, oPTImal Clustering (OPTIC) algorithm for WSNs. Our cluster head election procedure is not periodic, but adaptive based on the dynamism of the occurrence of events. This on-demand execution of OPTIC aims to significantly reduce computation and message overheads. Experimental results demonstrate that OPTIC improves the energy balance by more than 18% and network lifetime by more than 19% compared to a non-clustering and two clustering solutions in the state-of-the-art. © 2020 Elsevier Inc.","Energy balance; Linear programming; Network lifetime; On-demand clustering; Wireless sensor network"
"KASLR-MT: Kernel Address Space Layout Randomization for Multi-Tenant cloud systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075494023&doi=10.1016%2fj.jpdc.2019.11.008&partnerID=40&md5=97f580dfac63f62172e773edc62c29cf","Cloud computing has completely changed our lives. This technology dramatically impacted on how we play, work and live. It has been widely adopted in many sectors mainly because it reduces the cost of performing tasks in a flexible, scalable and reliable way. To provide a secure cloud computing architecture, the highest possible level of protection must be applied. Unfortunately, the cloud computing paradigm introduces new scenarios where security protection techniques are weakened or disabled to obtain a better performance and resources exploitation. Kernel ASLR (KASLR) is a widely adopted protection technique present in all modern operating systems. KASLR is a very effective technique that thwarts unknown attacks but unfortunately its randomness have a significant impact on memory deduplication savings. Both techniques are very desired by the industry, the first one because of the high level of security that it provides and the latter to obtain better performance and resources exploitation. In this paper, we propose KASLR-MT, a new Linux kernel randomization approach compatible with memory deduplication. We identify why the most widely and effective technique used to mitigate attacks at kernel level, KASLR, fails to provide protection and shareability at the same time. We analyze the current Linux kernel randomization and how it affects to the shared memory of each kernel region. Then, based on the analysis, we propose KASLR-MT, the first effective and practical Kernel ASLR memory protection that maximizes the memory deduplication savings rate while providing a strong security. Our tests reveal that KASLR-MT is not intrusive, very scalable and provides strong protection without sacrificing the shareability. © 2019 Elsevier Inc.","Cloud; Memory deduplication; Operating systems; Security; Virtualization"
"Modeling I/O performance variability in high-performance computing systems using mixture distributions","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079619218&doi=10.1016%2fj.jpdc.2020.01.005&partnerID=40&md5=9c53eade5e6b6368fbd102bbc499a351","Performance variability is an important factor of high-performance computing (HPC) systems. HPC performance variability is often complex because its sources interact and are distributed throughout the system stack. For example, the performance variability of I/O throughput can be affected by factors such as CPU frequency, the number of I/O threads, file size, and record size. In this paper, we focus on the I/O throughput variability across multiple executions of a benchmark program. For a given system configuration, the distribution of throughputs from run to run is of interest. We conduct large-scale experiments and collect a massive amount of data to study the distribution of I/O throughput under tens of thousands of system configurations. Despite normality often being assumed in the literature, our statistical analysis reveals that the performance variability is not normally distributed under most system configurations. Instead, multimodal distributions are common for many system configurations. We propose the use of mixture distributions to describe the multimodal behavior. Various underlying parametric distributions such as normal, gamma, and the Weibull are considered. We apply an expectation–maximization (EM) algorithm for parameter estimation and use the Bayesian information criterion (BIC) for parametric model selections. We also illustrate how to use the estimated mixture distribution to calculate the number of runs needed for future experiments on variability analysis. The paper provides a useful tool set in studying the behavior of performance variability. © 2020 Elsevier Inc.","EM algorithm; I/O variability; Mixture model; Performance analysis; Uncertainty quantification"
"Edge-based differential privacy computing for sensor–cloud systems","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074794284&doi=10.1016%2fj.jpdc.2019.10.009&partnerID=40&md5=612746e7c0b9ffc80043bde5ec5a40aa","In sensor–cloud systems, with more personal data being hosted in cloud, privacy leakage is becoming one of the most serious concerns. Privacy computing is emerging as a paradigm to systematically enhance privacy protection. In other words, the new paradigm requests us to improve the computing model to provide a general privacy protection service. In this paper, we propose an edge-based model for data collection, in which the raw data from wireless sensor networks (WSNs) is differentially processed by algorithms on edge servers for privacy computing. A small quantity of the core data is stored on edge and local servers while the rest is transmitted to cloud for storage. In this way, the benefits are twofold. First, the data privacy is preserved since the original data cannot be retrieved even if the data stored in the cloud is leaked. Second, implemented by a differential storage method, compared to the state of the art, the edge-based model sends less data to the cloud and reduces the cost of communication and storage. Both theoretical analyses and extensive experiments validate our proposed method. © 2019 Elsevier Inc.","Data collection; Edge-based model; Privacy computing; Privacy protection; Sensor–cloud"
"High level programming abstractions for leveraging hierarchical memories with micro-core architectures","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.11.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077758182&doi=10.1016%2fj.jpdc.2019.11.011&partnerID=40&md5=709ba4e67787ee62a550ebac1cd14112","Micro-core architectures combine many low memory, low power computing cores together in a single package. These are attractive for use as accelerators but due to limited on-chip memory and multiple levels of memory hierarchy, the way in which programmers offload kernels needs to be carefully considered. In this paper we use Python as a vehicle for exploring the semantics and abstractions of higher level programming languages to support the offloading of computational kernels to these devices. By moving to a pass by reference model, along with leveraging memory kinds, we demonstrate the ability to easily and efficiently take advantage of multiple levels in the memory hierarchy, even ones that are not directly accessible to the micro-cores. Using a machine learning benchmark, we perform experiments on both Epiphany-III and MicroBlaze based micro-cores, demonstrating the ability to compute with data sets of arbitrarily large size. To provide context of our results, we explore the performance and power efficiency of these technologies, demonstrating that whilst these two micro-core technologies are competitive within their own embedded class of hardware, there is still a way to go to reach HPC class GPUs. © 2019 Elsevier Inc.","Hardware accelerators; Interpreters; Neural networks; Parallel programming languages; Runtime environments"
"Extending the limits for big data RSA cracking: Towards cache-oblivious TU decomposition","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077514130&doi=10.1016%2fj.jpdc.2019.12.016&partnerID=40&md5=752bc654d82f662f553947e158db45c1","Nowadays, Big Data security processes require mining large amounts of content that was traditionally not typically used for security analysis in the past. The RSA algorithm has become the de facto standard for encryption, especially for data sent over the internet. RSA takes its security from the hardness of the Integer Factorisation Problem. As the size of the modulus of an RSA key grows with the number of bytes to be encrypted, the corresponding linear system to be solved in the adversary integer factorisation algorithm also grows. In the age of big data this makes it compelling to redesign linear solvers over finite fields so that they exploit the memory hierarchy. To this end, we examine several matrix layouts based on space-filling curves that allow for a cache-oblivious adaptation of parallel TU decomposition for rectangular matrices over finite fields. The TU algorithm of Dumas and Roche (2002) requires index conversion routines for which the cost to encode and decode the chosen curve is significant. Using a detailed analysis of the number of bit operations required for the encoding and decoding procedures, and filtering the cost of lookup tables that represent the recursive decomposition of the Hilbert curve, we show that the Morton-hybrid order incurs the least cost for index conversion routines that are required throughout the matrix decomposition as compared to the Hilbert, Peano, or Morton orders. The motivation lies in that cache efficient parallel adaptations for which the natural sequential evaluation order demonstrates lower cache miss rate result in overall faster performance on parallel machines with private or shared caches and on GPU's. © 2019 Elsevier Inc.","Cache-oblivious algorithms; Exact linear algebra; Morton-hybrid order; Space-filling curves"
"On-GPU thread-data remapping for nested branch divergence","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2020.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079824306&doi=10.1016%2fj.jpdc.2020.02.003&partnerID=40&md5=ddced10e6b3bef574df1c61c5c1b1b11","Nested branches are common in applications with decision trees. The more layers in the branch nest, the larger slowdown is caused by nested branch divergence on GPU. Since inner branches are impractical to evaluate on host end, thread-data remapping via GPU shared memory is so far the most suitable solution. However, existing solution cannot handle inner branches directly due to undefined behavior of GPU barrier function when executed within branch statements. Race condition needs to be prevented without using barrier function. Targeting nested divergence, we propose NeX as a nested extension scheme featuring an inter-thread protocol that supports sub-workgroup synchronization. We further exploit the on-the-fly nature of Head-or-Tail (HoT) algorithm and propose HoT2 with enhanced flexibility of wavefront scheduling. Evaluated on four GPU models including NVIDIA Volta and Turing, HoT2 confirms to be more efficient. For benchmarks with branch nests up to five-layer-deep, NeX further boosts performance by up to 1.56x. © 2020 Elsevier Inc.","Branch divergence; GPGPU; Race condition; SIMD"
"Detection of ships in inland river using high-resolution optical satellite imagery based on mixture of deformable part models","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066401240&doi=10.1016%2fj.jpdc.2019.04.013&partnerID=40&md5=0ccd4e1e266673d3a7a2cb057e9ecc62","Ship detection using optical satellite imagery is of great significance in many applications such as traffic surveillance, pollution monitoring, etc. So far, a lot of ship detection methods have been developed for images covering open sea, offshore area and harbors. Compared to the ship detection in sea and offshore area, it is more difficult to detect ships in inland river due to several challenges. First of all, many ships in inland river are clustered together and hard to be separated from each other. Secondly, ships lying alongside the pier are very likely to be recognized as part of the pier. Thirdly, ships in inland river is usually smaller than those in the sea. A hierarchical method is proposed to detect the ships in inland river in this paper. The Regions of Interest (ROIs) are firstly extracted based on water–land segmentation using multi-spectral information. Then two kinds of ship candidates are extracted based on the panchromatic band. The isolated ships are detected by analyzing the shape of connected components and the clustered ships are detected by using mixtures multi-scale Deformable Part Models (DPM) and Histogram of Oriented Gradient (HOG). At last, a Back Propagation Neural Network (BPNN) is trained to classify the ship candidates using the multi-spectral bands. The experiments using Quickbird satellite images show that our approach is effective in ship detection and performs particularly well in separating the ships clustered together and staying alongside the pier. © 2019 Elsevier Inc.","Deformable part model; Inland river; Optical satellite imagery; Ship detection"
"Person re-identification with multiple similarity probabilities using deep metric learning for efficient smart security applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044716579&doi=10.1016%2fj.jpdc.2017.11.009&partnerID=40&md5=acc07f7d9fbdd372c07ef7e1eca70915","Surveillance video analysis plays a vital role in the daily operations of smart cities, which increasingly relies on person re-identification technology to sustain smart security applications. However, research challenges of re-identification remain especially in terms of recognizing the different appearances of the same person in a harsh real-world environment: (1) the adaptability of the selected features to the dynamic environment cannot be guaranteed, and (2) existing methods rooted from metric learning aim to find a single metric function, and they lack the ability to measure the different appearances of the same person. To address these problems, this study proposes a multiple deep metric learning method empowered by the functionality of person similarity probability measurement. The proposed method exploits multiple stacked auto-encoder networks and classification networks to quantify pedestrians’ similarity relations. The stacked auto-encoder networks directly recognize persons from surveillance images at the pixel level. The classification networks are equipped with the Softmax regression models and produce multiple similarity probabilities to characterize different appearances belonging to the same person. An Adaboost-like model is designed to fuse the probabilities corresponding to multiple metrics, which ensures a high accuracy of recognition. Experimental results on two public datasets (VIPeR and CUHK-01) indicate that the proposed method outperforms existing algorithms by 2%–10% at rank 1. Based on the similarity probabilities learned by the proposed model, the algorithm for matching the person pair can achieve a time complexity as low as O(n), which can be deployed at a large scale on the distributed intelligent surveillance network, with each node maintaining limited computing capabilities. © 2017 Elsevier Inc.","Deep metric learning; Person re-identification; Similarity probability; Smart security; Surveillance video analysis"
"Accountable privacy preserving attribute based framework for authenticated encrypted access in clouds","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072582323&doi=10.1016%2fj.jpdc.2019.08.014&partnerID=40&md5=9d54fee1186a1398a0ff7445fd5df0dc","In this paper, we propose an accountable privacy preserving attribute-based framework, called Ins-PAbAC, that combines attribute based encryption and attribute based signature techniques for securely sharing outsourced data contents via public cloud servers. The proposed framework presents several advantages. First, it provides an encrypted access control feature, enforced at the data owner's side, while providing the desired expressiveness of access control policies. Second, Ins-PAbAC preserves users’ privacy, relying on an anonymous authentication mechanism, derived from a privacy preserving attribute based signature scheme that hides the users’ identifying information. Furthermore, our proposal introduces an accountable attribute based signature that enables an inspection authority to reveal the identity of the anonymously-authenticated user if needed. Third, Ins-PAbAC is provably secure, as it is resistant to both curious cloud providers and malicious users adversaries. Finally, experimental results, built upon OpenStack Swift testbed, point out the applicability of the proposed scheme in real world scenarios. © 2019 Elsevier Inc.","Accountability; Attribute based encryption; Attribute based signature; Cloud data sharing; Privacy"
"Brokering in interconnected cloud computing environments: A survey","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053385874&doi=10.1016%2fj.jpdc.2018.08.001&partnerID=40&md5=d38ca2f23072d8b76c2c404bde21ea28","Cloud computing provides computing platforms and facilitates to optimize utilization of infrastructure resources, reduces deployment time and increases flexibility. The popularity of cloud computing led to development of interconnected cloud computing environments (ICCE) such as hybrid cloud, inter-cloud, multi-cloud, and federated cloud, enabling the possibilities to share resources among individual clouds. However, individual proprietary technologies and access interfaces employed by cloud service providers made it difficult to share resources. Interoperability and portability are two of the major challenges to be addressed to ensure seamless access and sharing of resources and services. Many cloud service providers have similar service offerings but different access patterns. It is difficult and time consuming for a cloud user to select an appropriate cloud service as per the application's requirement. Cloud user has to gather information from various cloud service providers and analyze them. Cloud broker has been proposed to address the challenge of cloud users to get best out of cloud provider. Cloud broker is an entity which works as an independent third party between cloud users and cloud providers. Cloud broker negotiates with several cloud providers as per user's requirements and tries to select the best services. Cloud broker coordinates the sharing of resources and provides interoperability and portability with other cloud providers. In this paper, a comprehensive survey of cloud brokering in interconnected cloud computing environments has been provided. The need and importance of cloud broker has been discussed. The existing architectures and frameworks of Cloud Brokering are reviewed. A comprehensive literature survey of various Cloud Brokering techniques is presented. A taxonomy of Cloud Brokering techniques has been presented and analyzed on the basis of their strengths and weaknesses/limitations. The taxonomy includes pricing, multi-criteria, quality of services, optimization and trust techniques. The techniques are analyzed on various performance metrics. Research challenges and open problems are identified from reviewed techniques. A model for cloud broker is proposed to address identified challenges. We hope that our work will enable researchers to launch and dive deep into Cloud Brokering challenges in interconnected cloud computing environments. © 2018 Elsevier Inc.","Cloud broker; Cloud computing; Federated cloud; Hybrid cloud; Inter-cloud; Multi-cloud"
"Topic-based rank search with verifiable social data outsourcing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069854914&doi=10.1016%2fj.jpdc.2019.07.003&partnerID=40&md5=b20bc929724ac3b1e15639593d1885df","As the explosive development of social network sites, social data has successfully captured either individuals’ or entities’ attention due to having tremendous commercial value. The initial step to my various insights from social data is how to obtain authentic social data. Social data outsourcing as a new paradigm has been pervasive, in which a social data provider collects integrity social data from different social network sites and resells to data consumers on demand. However, some dishonest activities, like adding fake data, deleting/modifying raw data, drive us to consider verifiable topic-based rank search problem on social data outsourcing scenario. To guarantee the authenticity of social data, we propose two schemes. In our basic scheme, social network sites generate unforgeable auxiliary information and outsource to the social data provider as well as social data. Data consumers can probabilistically verify the correctness and completeness of the query results with the help of verification objects derived from auxiliary information. To reduce the number of the most related topic labels, we propose an enhanced scheme, in which the social network sites first cluster similar topics together, and then generate auxiliary information. Rigorous security and performance analyses prove that our proposed schemes are safe and effective. In addition, our experimental results built on a real Twitter dataset which demonstrates the efficacy and efficiency of our schemes. © 2019 Elsevier Inc.","Authenticity verification; Latent Dirichlet allocation; Topic-based rank search; Topics graph; Verifiable social data outsourcing"
"Speeding up exascale interconnection network simulations with the VEF3 trace framework","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068582551&doi=10.1016%2fj.jpdc.2019.06.013&partnerID=40&md5=fc0975d88920d0a7c1f42de902db7658","Simulation is used to evaluate and validate the behavior and performance of computing systems, in particular the interconnection network in the context of high-performance computing. For the simulation to be performed, the simulator program must be provided with a mechanism that generates network traffic or workload. Although synthetic traffic has been widely used, communication from real applications is a better and more representative workload. With this kind of network workload, the simulations can become slower, especially when simulating Exascale systems. In this paper, we extend the VEF trace framework, originally designed for feeding off-chip networks with MPI traffic, including new functionality related to the on-chip communications and introducing improvements to speed up the simulations. This way, the VEF framework allows to study the behavior of Exascale interconnection networks with realistic traffic and in reasonably short times. © 2019 Elsevier Inc.","Exascale workloads; Interconnection networks; Modeling and simulation tool; Performance evaluation"
"A unified view of parallel multi-objective evolutionary algorithms","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047618067&doi=10.1016%2fj.jpdc.2018.04.012&partnerID=40&md5=7399d26fc0e8253daf9f1f8e7e354654","This paper describes a unified view of parallel evolutionary algorithms for multi-objective optimization problems. The parallel optimization algorithms are detailed from both design and implementation aspects. The proposed taxonomy is based on three hierarchical parallel models. Moreover, various parallel architectures are taken into account. The performance assessment issue of parallel multi-objective evolutionary algorithms (MOEA) is also presented. This work can be extended to any population-based metaheuristics such as particle swarm and scatter search. © 2018","Metaheuristics; Multi-objective optimization; Parallel evolutionary algorithms"
"QoS-aware service recommendation based on relational topic model and factorization machines for IoT Mashup applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046797359&doi=10.1016%2fj.jpdc.2018.04.002&partnerID=40&md5=2d2b16c3d714ce633b9d62425a86a469","IoT Mashup applications allow developer to compose existing Web APIs to create value-added composite Web services. The rapid growth of large-scale and complex services makes it difficult to find suitable Web APIs to build IoT Mashup applications for developers. Even if the existing service recommendation methods show improvements in service discovery, the accuracy of them can be significantly improved due to overlooking the impact of sparsity and multiple-dimension information of QoS between Mashup and services on recommendation accuracy. In this paper, we propose a QoS-aware service recommendation based on relational topic model and factorization machines for IoT Mashup applications. This method first uses relational topic model to characterize the relationships among Mashup, services, and their links, and mine the latent topics derived by the relationships. Second, it exploits factorization machines to train the latent topics for predicting the link relationship among Mashup and services to recommend adequate relevant top-k Web APIs for target IoT Mashup creation. Finally, we conduct a comprehensive evaluation to measure performance of our method. Compared with other existing recommendation approaches, experimental results show that our approach achieves a significant improvement in terms of precision, recall, and F-measure. © 2018 Elsevier Inc.","Factorization machines; IoT Mashup applications; QoS; Relational topic model; Service recommendation"
"FOCAN: A Fog-supported smart city network architecture for management of applications in the Internet of Everything environments","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050228043&doi=10.1016%2fj.jpdc.2018.07.003&partnerID=40&md5=4cd1ce02f05200ca736b10aef80ad98d","Smart city vision brings emerging heterogeneous communication technologies such as Fog Computing (FC) together to substantially reduce the latency and energy consumption of Internet of Everything (IoE) devices running various applications. The key feature that distinguishes the FC paradigm for smart cities is that it spreads communication and computing resources over the wired/wireless access network (e.g., proximate access points and base stations) to provide resource augmentation (e.g., cyberforaging) for resource- and energy-limited wired/wireless (possibly mobile) things. Motivated by these considerations, this paper presents a Fog-supported smart city network architecture called Fog Computing Architecture Network (FOCAN), a multi-tier structure in which the applications are running on things thatjointly compute, route, and communicate with one another through the smart city environment. FOCAN decreases latency and improves energy provisioning and the efficiency of services among things with different capabilities. In particular, three types of communications are defined between FOCAN devices – interprimary, primary, and secondary communication –to manage applications in a way that meets the quality of service standards for the Internet of Everything. One of the main advantages of the proposed architecture is that the devices can provide the services with low energy usage and in an efficient manner. Simulation results for a selected case study demonstrate the tremendous impact of the FOCAN energy-efficient solution on the communication performance of various types of things in smart cities. © 2018 Elsevier Inc.","Computing and communication; Fog computing; Internet of Everything (IoE); Routing algorithm; Smart city"
"Optimistic scheduling with service guarantees","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072184672&doi=10.1016%2fj.jpdc.2019.04.010&partnerID=40&md5=7904189163f125cece57c5ed046ab809","Data centers form the core of current information and commercial enterprise. At current scale, any improvement in data center resource utilization leads to substantial savings. We focus on the problem of scheduling jobs in distributed execution environments to improve resource utilization. Cluster schedulers like YARN and Mesos base their scheduling decisions on resource requirements provided by end users. It is hard for end-users to predict the exact amount of resources required for a task/job, especially since resource utilization can vary significantly over time and across tasks. In practice, users make highly conservative estimates of peak utilization across all tasks of a job to ensure job completion, leading to resource fragmentation and severe under utilization in production clusters. We present UBIS, a utilization-aware approach to cluster scheduling, to address resource fragmentation, and to improve cluster utilization and job throughput. UBIS considers actual usage of running tasks and schedules opportunistic work on under-utilized nodes. UBIS monitors resource usage on these nodes and preempts opportunistic containers in the event this over-subscription becomes untenable. In doing so, UBIS effectively utilizes wasted resources, while minimizing adverse effects on regularly scheduled tasks. Our implementation of UBIS on YARN demonstrates improvements of up to 30% in makespan for a representative workload and 25% in individual job durations. © 2019 Elsevier Inc.","Cluster scheduling; Distributed computing"
"Improving hardware transactional memory parallelization of computational geometry algorithms using privatizing transactions","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065590869&doi=10.1016%2fj.jpdc.2019.04.018&partnerID=40&md5=3b0ff4055e6e7a78c6cf16e7fffcde99","Hardware transactional memory is a new parallel programming paradigm supported by current commercial multiprocessors. This paradigm provides optimistic concurrency and overcomes some of the problems associated with classical lock-based synchronization, such as deadlock and serialization. Certain algorithms of computational geometry are found to be good candidates for parallelization with this paradigm. However, hardware transactional approaches to these algorithms lead to poor performance as the resulting transactions are too large for the underlying hardware to deal with. Large transactions overflow hardware resources serializing the execution. In this paper, we propose using privatizing transactions to parallelize two computational geometry algorithms: Lee's algorithm, which solves the shortest-route problem, and Ruppert's algorithm for Delaunay/Voronoi mesh refinement. Privatizing transactions are based on commercial hardware transactional memory extensions, and their goal is to reduce transaction footprint by means of a non-transactional private execution section. This results in effective smaller transactions. Our implementation is able to further reduce the transaction size as we propose a reduced validation set for privatizing transactions. Programming complexity of these implementations is discussed. Results show that our privatizing transaction implementations indeed enhance performance comparing with existing hardware transactional memory versions. Experiments with Intel's transactional memory extensions yield speedups ranging from 2× to 3.5× with four threads. © 2019 Elsevier Inc.","Computational geometry; Delaunay triangulation; Hardware transactional memory; Lee's algorithm; Privatizing transactions; Ruppert's algorithm"
"Parallel fractal decomposition based algorithm for big continuous optimization problems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048931268&doi=10.1016%2fj.jpdc.2018.06.002&partnerID=40&md5=79cc4fc45e5524f383965159bb7097fe","Fractal Decomposition Algorithm (FDA) is a metaheuristic that was recently proposed to solve high dimensional continuous optimization problems. This approach is based on a geometric fractal decomposition which divides the search space while looking for the optimal solution. While FDA and its fractal decomposition has shown to be an effective optimization algorithm, its running time grows significantly as the problems dimension increases. To overcome this expensive computational time, a parallelized version of FDA, called Parallel Fractal Decomposition Algorithm (PFDA) is proposed. The focus was on parallelizing the exploration and exploitation phases of the original algorithm on a multi-threaded environment. The performances of PFDA were evaluated on the same Benchmark used to illustrate FDA efficiency, the SOCO 2011. It is composed of 19 functions with dimensions going from 50 to 5000. Results show that PFDA reaches similar performances as the original version with a significantly reduced computational time. © 2018 Elsevier Inc.","Continuous optimization; Geometric fractal decomposition; Local search; Metaheuristics; Very-large-scale optimization"
"Towards a real-time production of immersive spatial audio of high individuality with an RBF neural network","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065602809&doi=10.1016%2fj.jpdc.2019.04.020&partnerID=40&md5=25e7f4a83c18e9328955814184c0fbc9","Immersion perception plays a critical role in the tremendous success of the recent development of augment/virtual reality applications, in which high-quality spatial audio is mandatory. However, because of the high individuality of numerous anthropometric parameters in connection with listeners, deriving the proper acoustic perturbation characteristics in the process of producing immersive spatial audio via loudspeakers, in which speed and precision are both important, has long been a research challenge. This study first adopts gain vectors for loudspeakers (GVL) to represent the acoustic perturbations, which are sensitive to both the frequency bands and the anthropometric parameters of an individual. The radial base function neural network then maps the parameter sets to the corresponding GVLs. A parallel convolution algorithm guides the GVLs to convolve with the source signals, and the outputs drive the loudspeakers to produce the designated spatial audio of high individuality. Experimental results indicate the following: (1) the binaural cues deviation decrease by 12.21% on average, and the subjective score of the listener increases by 27.24%, and (2) the ratio of time consumed by parallel convolution based on six threads to a general convolution is 0.373, demonstrating that immersive spatial audio of high individuality can be produced in real time. © 2019 Elsevier Inc.","Parallel convolution; Radial base function neural network; Spatial audio; Vector-based amplitude panning; Virtual sound"
"FPGA design space exploration for scientific HPC applications using a fast and accurate cost model based on roofline analysis","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021086757&doi=10.1016%2fj.jpdc.2017.05.014&partnerID=40&md5=2ea39d57393af73fa1a3cba4f4a55e6c","High-performance computing on heterogeneous platforms in general and those with FPGAs in particular presents a significant programming challenge. We contend that compiler technology has to evolve to automatically optimize applications by transforming a given original program. We are developing a novel methodology based on type transformations on a functional description of a given scientific kernel, for generating correct-by-construction design variants. An associated lightweight costing mechanism for evaluating these variants is a cornerstone of our methodology, and the focus of this paper. We discuss our use of the roofline model to work with our optimizing compiler to enable us to quickly derive accurate estimates of performance from the design's representation in our custom intermediate language. We show results confirming the accuracy of our cost model by validating it on different scientific kernels. A case study is presented to demonstrate that a solution created from our optimizing framework outperforms commercial high-level synthesis tools both in terms of throughput and power efficiency. © 2017 The Authors","Cost model; FPGA; High-level programming; High-level synthesis; High-performance computing; Performance model; Roofline model"
"Optimal LEACH protocol with modified bat algorithm for big data sensing systems in Internet of Things","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040786247&doi=10.1016%2fj.jpdc.2017.12.014&partnerID=40&md5=289f14545118fc83cb3873877d4bd378","Big data sensing system (BDSS) plays an important role in the Internet of Things, in which how to reduce power consumption is one crucial problem. Currently, low energy adaptive clustering hierarchy (LEACH) protocol is one well-known algorithm used in BDSS with low energy cost. In this paper, a new variant of bat algorithm combined with centroid strategy is introduced. Three different centroid strategies with six different designs are introduced. In addition, the velocity inertia-free update equation is also provided. The optimization performance is verified by CEC2013 benchmarks in those designs against standard BA. Simulation results prove that the bat algorithm with weighted harmonic centroid (WHCBA) strategy is superior to other algorithms. By integrating WHCBA into LEACH protocol, we develop a two-stage cluster-head node selection strategy and can save more energy compared to the standard LEACH protocol. © 2017 Elsevier Inc.","Bat algorithm with centroid strategy; Big data sensing systems; Inertia-free; LEACH protocol"
"Task Packing: Efficient task scheduling in unbalanced parallel programs to maximize CPU utilization","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070676316&doi=10.1016%2fj.jpdc.2019.08.003&partnerID=40&md5=e94d92a091ce2b7eaae7513edf4facfa","Load imbalance in parallel systems can be generated by external factors to the currently running applications like operating system noise or the underlying hardware like a heterogeneous cluster. HPC applications working on irregular data structures can also have difficulties to balance their computations across the parallel tasks. In this article we extend, improve and evaluate more deeply the Task Packing mechanism proposed in a previous work. The main idea of the mechanism is to concentrate the idle cycles of unbalanced applications in such a way that one or more CPUs are freed from execution. To achieve this, CPUs are stressed with just useful work of the parallel application tasks, provided performance is not degraded. The packing is solved by an algorithm based on the Knapsack problem, in a minimum number of CPUs and using oversubscription. We design and implement a more efficient version of such mechanism. To that end, we perform the Task Packing “in place”, taking advantage of idle cycles generated at synchronization points of unbalanced applications. Evaluations are carried out on a heterogeneous platform using FT and miniFE benchmarks. Results showed that our proposal generates low overhead. In addition the amount of freed CPUs are related to a load imbalance metric which can be used as a prediction for it. © 2019 Elsevier Inc.","HPC; Knapsack algorithm; Load balancing; MPI; Oversubscription"
"Privacy-preserving k nearest neighbor query with authentication on road networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070282079&doi=10.1016%2fj.jpdc.2019.07.013&partnerID=40&md5=08abc51b995ef4be00fdd369d029538d","k nearest neighbor (kNN) queries are frequently used in location-based services (LBSs), by which we wish to get k closest points of interest (POIs) given a certain point. Since the cloud computing is developing fast, LBS providers are tended to outsource spatial databases to the cloud. However, cloud servers are often untrusty, so that ensuring the spatial query integrity as well as the spatial query privacy is critical. We present a verifiable privacy-preserving kNN query scheme, which can be used on road networks. Our work makes use of the network Voronoi diagram and several cryptographic primitives including pseudo-random functions, Paillier cryptosystem, condensed RSA digital signature, and so on. It can simultaneously preserve the privacy of spatial data and kNN queries, and verify the reliability of query results. The effectiveness and practicability of our scheme are validated by our experimental results. We further analyzed the security of our scheme under the adaptive chosen-query attack via rigorous proof. © 2019 Elsevier Inc.","Authentication; Cloud computing; Graph encryption; kNN query; Privacy"
"On construction of a big data warehouse accessing platform for campus power usages","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067921421&doi=10.1016%2fj.jpdc.2019.05.011&partnerID=40&md5=6b324a78ea4b9a67285d3035b4bf5670","With the emerging of the Internet of Things (IoT) technology, we can analyze and real-time monitoring the power consumption data of buildings or equipment. However, over time, many power data will accumulate to even the terabyte level. Traditional data processing techniques will not handle. It is a challenge to monitor and process the data in a reasonable time. In this paper, we construct an efficient and real-time power monitoring platform, based on open source techniques. The power-data of buildings or equipment is provided through smart meters equipped inside campus buildings. The technologies of data collected and stored on the server side were handled by the Hadoop ecosystem and the data process to Hive data warehouse is executed by the Spark. On the particulars of system evaluation, we make some experiments to compare with the performance of record query, use three different technologies separately: Apache Hive, Apache Spark, and Impala had been evaluated regarding query-response performance. Moreover, for the performance estimation on the data ETL processing and SQL functions. The relevant experiments have been conducted on the same three software modules as well. © 2019 Elsevier Inc.","Big data warehouse; Data ETL; Internet of Things; Real-time processing; Smart meter"
"A multi-staged niched evolutionary approach for allocating parallel tasks with joint optimization of performance, energy, and temperature","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070914245&doi=10.1016%2fj.jpdc.2019.05.009&partnerID=40&md5=3a3a7a769c311a01d171790a29d041bf","This paper presents a multi-stage multi-objective evolutionary approach (MS-MOEA) for allocating parallel computations on multi-core processors by joint optimizing performance (P), energy (E), and temperature (T). Evolutionary techniques have been shown to be effective for solving optimization problems, including our own previous work on solving the PET-optimized scheduling (PETOS) problem. There have long been a great many debates and rivalries between various evolutionary approaches, such as the SPEA or NSGA, with regard to their relative matters. The novelty of the proposed MS-MOEA approach is its amalgamation of the basic evolutionary algorithms that are already shown to be highly effective, thereby creating a niche of these techniques. The niche takes advantages of the strengths of each baseline technique for achieving additional enhancement in the precision of the optimization. We propose six multi-stage hybrids, each designed with either niched fitness assignment strategy, or combining populations from multiple MOEAs, or incorporating the problem knowledge into the conventional technique. The experimental results measure the quality of resulting Pareto fronts and demonstrate that the proposed MS-MOEAs yield better optimization for the PETOS problem in achieving three-objective in parallel task-to-core mapping. © 2019 Elsevier Inc.","Energy-efficient computing; Evolutionary algorithms; Static scheduling; Task allocation; Task graphs"
"Extending the lifetime of wireless sensor networks through mitigating the hot spot problem","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069685090&doi=10.1016%2fj.jpdc.2019.06.007&partnerID=40&md5=3301ed340d6678a563228d4b28ba764d","Wireless sensor networks (WSNs) are used to retrieve information in hostile environments that are not always accessible to human beings. Once they are deployed, the sensors are considered autonomous. Their lifetime is equivalent to the lifetime of their battery. The energy factor is the center of all the concerns of the sensors: economic routing protocols, adapted wireless technology, etc. Energy costs must be minimized because energy is a key constraint in sensor networks. This paper is part of the study of the routing problem in WSNs. Indeed, the problem consists of the nodes located around the base station (BS), which transmit data to other nodes, their energy is depleted more quickly. This causes energy holes and areas called hot spot. To address this problem, we used the unequal clustering mechanism in order to solve the network hot spot problem. Clusters closer to the BS have smaller sizes than those farther from the BS to overcome the energy over-consumption around the BS. The evaluation of our solution will be carried out using the platform Omnet++/Castalia. The simulation results obtained allow a glimpse of the energy gained by the proposed protocol compared to the protocol Eeuc. Energy efficiency is achieved thanks to the use of a network management policy which enables to balance the energy levels of different nodes, consequently, extends the lifetime of the network. © 2019 Elsevier Inc.","Castalia; Energy consumption; Hot spot problem; Routing; WSN"
"ForestLayer: Efficient training of deep forests on distributed task-parallel platforms","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067260545&doi=10.1016%2fj.jpdc.2019.05.001&partnerID=40&md5=088d6aecb6157894c14eb838b63619d5","Most of the existing deep models are deep neural networks. Recently, the deep forest opens a door towards an alternative to deep neural networks for many tasks and has attracted more and more attention. At the same time, the deep forest model becomes widely used in many real-world applications. However, the existing deep forest system is inefficient and lacks scalability. In this paper, we present ForestLayer, which is an efficient and scalable deep forest system built on distributed task-parallel platforms. First, to improve the computing concurrency and reduce the communication overhead, we propose a fine-grained sub-forest based task-parallel algorithm. Next, we design a novel task splitting mechanism to reduce the training time without decreasing the accuracy of the original method. To further improve the performance of ForestLayer, we propose three system-level optimization techniques, including lazy scan, pre-pooling, and partial transmission. Besides the systematic optimization, we also propose a set of high-level programming APIs to improve the ease-of-use of ForestLayer. Finally, we have implemented ForestLayer on the distributed task-parallel platform Ray. The experimental results reveal that ForestLayer outperforms the existing deep forest system gcForest with 7× to 20.9× speedup on a range of datasets. In addition, ForestLayer outperforms TensorFlow-based implementation on most of the datasets, while achieving better predictive performance. Furthermore, ForestLayer achieves good scalability and load balance. © 2019 Elsevier Inc.","Deep forest; Distributed computing; Random forest; Ray; Task-parallel"
"CLASSIC: A cortex-inspired hardware accelerator","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071938679&doi=10.1016%2fj.jpdc.2019.08.009&partnerID=40&md5=8aa6b2f634dd5f4aa7f17c67fb9a9e0b","This work explores the feasibility of specialized hardware implementing the Cortical Learning Algorithm (CLA) in order to fully exploit its inherent advantages. This algorithm, which is inspired by the current understanding of the mammalian neo-cortex, is the basis of the Hierarchical Temporal Memory (HTM). In contrast to other machine learning (ML) approaches, the structure is not application dependent and relies on fully unsupervised continuous learning. We hypothesize that a hardware implementation will be able not only to extend the existing practical uses of these ideas to broader scenarios but also to exploit CLA's hardware-friendly characteristics. The architecture proposed will enable the system size to be scaled up compared to a state-of-the-art CLA software implementation. It may be possible to improve performance by 4 orders of magnitude and energy efficiency by up to 8 orders of magnitude. Given the problem's complex nature, we found that the most demanding issue, from a scalability standpoint, is the massive degree of connectivity required. We propose to use a packet-switched network to tackle this. The paper addresses the fundamental issues of such an approach, proposing solutions to achieve a scalable proposal. We will analyze cost and performance when using well-known architectural techniques and tools. The results obtained suggest that even with CMOS technology, under constrained cost, it might be possible to implement a large-scale system. We found that the proposed solutions enable a saving of ∼90% of the original communication costs running either synthetic or realistic workloads. © 2019 Elsevier Inc.","Computer architecture; Cortex; Cortical learning algorithm; Neuroscience; Packet-switched network"
"Intelligent route planning on large road networks with efficiency and privacy","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068414901&doi=10.1016%2fj.jpdc.2019.06.012&partnerID=40&md5=0d5ed205db3b0c1259de32421f5567a5","When using Location-Based Services (LBS), intelligent route planning becomes crucial to improve service quality and user experience. The state-of-the-art G-tree structure enables efficient route planning on large road networks, but lacks usability and intelligence. In this paper, we first propose a comprehensive service framework, called BCloud-IFog, which consists of blind cloud servers and intelligent fog servers. Then, we propose an Outsourced Real-time Route Planning (OR2P) scheme, where the search index is built as a G*-tree structure and each G*-tree leaf node is split into a set of non-confidential outsourced graphs. Compared with the G-tree structure, our work has the following advantages: (1) Higher usability. Unlike the G-tree structure requiring the user to perform all calculations in the search process, it delegates the most of computations to cloud servers. (2) Privacy Protection. Unlike the straightforward solution that directly outsource the G-tree structure, it outsources only the non-confidential graphs such that cloud servers cannot infer the original road network or user trajectory. (3) Better intelligence. Unlike the G-tree structure handles only static route planning, it allows fog servers to make route plans based on the dynamic real-time traffic status. Extensive experiments on real data sets demonstrate the effectiveness of our work. © 2019 Elsevier Inc.","Intelligence; Privacy; Road networks; Route planning; Usability"
"An optimized RGA supporting selective undo for collaborative text editing systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066064570&doi=10.1016%2fj.jpdc.2019.05.005&partnerID=40&md5=dc6543ca6483f1da6fa9359ac70b236e","Collaboration plays a key role in distributed applications. As a fundamental vehicle for collaboration, collaborative text editing systems have been an important field within CSCW. More recently, with the increasing popularity of cloud computing, collaborative text editing systems move towards large-scale collaborations based on the cloud computing/cloud platform. The computing performance is the key factor of success for large-scale collaborations. CRDT algorithms have been proved to outperform traditional algorithms in publications. However, how to support selective undo has been a challenging issue for existing CRDT algorithms. This paper proposes an efficient CRDT algorithm called ORGAU that provides integrated do and selective undo efficiently. The correctness and operation intentions preserving of the proposed algorithm under an integrated do/undo framework are formally proved. Compared with the typical CRDT algorithms, the proposed algorithm has better computing performance both in theoretical analysis and experimental evaluation while keeping the same space complexity. © 2019 Elsevier Inc.","Collaborative text editing; CRDT(Commutative Replicated Data Types); CSCW (Computer Supported Cooperative Work); RGA(Replicated Growable Array); Selective undo"
"Differential privacy for renewable energy resources based smart metering","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065540984&doi=10.1016%2fj.jpdc.2019.04.012&partnerID=40&md5=2cae5f63c5a775776b8dda7614f8dc00","The increasing energy costs and increase in losses in traditional power grid system triggered the integration of Renewable Energy Resources (RERs)in smart homes. The global desire of consumers to rely on RERs such as solar energy, and wind energy has increased dramatically. Similarly, the IT technologies are also playing their part in smart grid development, such as real time data monitoring. On the other hand, with the advancement of these IT technologies in smart meters, the privacy of customers is also at risk Smart grid utility knows the exact generation of any specific renewable resource in a specific interval of time. Utility need to monitor this real time data for load forecasting and implementation of demand response scenarios. However, the utility may misuse the data and may increase the prices for specific time slots when RERs are not present. Similarly, real time monitoring of data can lead to estimation of life routines of users such as sleeping habits, time of usage of heavy appliances, and lifestyle. In this paper, a Differential Privacy based real time Load Monitoring approach (DPLM)is proposed that preserve the privacy of users by masking the values of load in such a way that utility will not be able to judge the usage of specific RER and the daily routine of any smart meter user. We compare our scheme with Gaussian Noise Differential Privacy (GNDP)strategy. Experimental results validate that our DPLM approach provides a desirable solution to protect smart grid user's privacy by efficient noise addition and peak value protection along with having an error rate of only 1.5%. © 2019 Elsevier Inc.","Differential privacy (DP); Privacy preservation; Renewable energy resources (RERs); Smart grid (SG)"
"Preserving SSD lifetime in deep learning applications with delta snapshots","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068189042&doi=10.1016%2fj.jpdc.2019.06.011&partnerID=40&md5=9da3a3dfd8e57aa68a6cc49a61ba918a","In large-scale deep learning applications, SSDs (Solid State Drives) have been widely adopted to speed up the training. However, a snapshot process is periodically performed in a deep learning application, by which a great number of training parameters (at the TB level) need to be written to SSDs; thus, it poses serious challenges for the lifetime of SSDs. In this paper, we for the first time design a mechanism, called delta snapshot, that can effectively reduce the overall amount of data written to an SSD during the training phase so as to preserve its lifetime. Delta snapshot exploits the redundant information between snapshots. Specifically, we observe that the exponent part and the most significant bits of the mantissa change very little between two consecutive snapshots. Based on this, we develop effective mechanism to compress the redundant bits of snapshots to reduce the size of the written data. Experimental results showed that our technique can reduce the overall amount of written data by 31% and the erase operations by 27%, with a negligible time overhead in the training phase. © 2019 Elsevier Inc.","Deep learning; Snapshots; Solid state disk"
"Ada-Things: An adaptive virtual machine monitoring and migration strategy for internet of things applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049953016&doi=10.1016%2fj.jpdc.2018.06.009&partnerID=40&md5=9b07bff64c19bf0f6d5448e2ec6e1336","Internet of Things (IoT) applications running on mobile devices are subject to the low storage capacity and short battery lifetime. Edge clouds (EC) provide an approach to offload computation tasks and reduce network latency for these applications. The main challenge in such ecosystems is how to efficiently monitor and allocate VM resources to realize load balancing among edge clouds. In this paper, we propose Ada-Things, an adaptive VM monitoring and live migration Strategy for IoT applications in edge cloud architecture. The basic idea of Ada-Things is that the migration method of a VM should be determined by its workload characteristics. Specifically, based on the variation of current memory dirty page rate in IoT applications, Ada-Things can adaptively select the most appropriate migration method to copy memory pages, thus addressing the two limitations (application generality and performance imbalance) of existing VM migration methods in edge cloud. Evaluation results show, compared with traditional methods, Ada-Things can significantly reduce the total migration time by 21%, the VM downtime by 38% and the amount of pages transferred by 29% in average. © 2018 Elsevier Inc.","Cloud computing; Internet of things; Live migration; Memory copy strategy; Virtual machine"
"A communication-avoiding 3D algorithm for sparse LU factorization on heterogeneous systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065564574&doi=10.1016%2fj.jpdc.2019.03.004&partnerID=40&md5=9d20f723ea4c79634bfff461a4e79e04","We propose a new algorithm to improve the strong scalability of right-looking sparse LU factorization on distributed memory systems. Our 3D algorithm for sparse LU uses a three-dimensional MPI process grid, exploits elimination tree parallelism, and trades off increased memory for reduced per-process communication. We also analyze the asymptotic improvements for planar graphs (e.g., those arising from 2D grid or mesh discretizations) and certain non-planar graphs (specifically for 3D grids and meshes). For a planar graph with n vertices, our algorithm reduces communication volume asymptotically in n by a factor of Ologn and latency by a factor of Ologn. For non-planar cases, our algorithm can reduce the per-process communication volume by 3× and latency by On1/3 times. In all cases, the memory needed to achieve these gains is a constant factor. We implemented our algorithm by extending the 2D data structure used in SUPERLU_DIST. Our new 3D code achieves empirical speedups up to 27× for planar graphs and up to 3.3× for non-planar graphs over the baseline 2D SUPERLU_DIST when run on 24,000 cores of a Cray XC30. We extend the 3D algorithm for heterogeneous architectures by adding the Highly Asynchronous Lazy Offload (HALO) algorithm for co-processor offload [44]. On 4096 nodes of a Cray XK7 with 32,768 CPU cores and 4096 Nvidia K20x GPUs, the 3D algorithm achieves empirical speedups up to 24× for planar graphs and 3.5× for non-planar graphs over the baseline 2D SUPERLU_DIST with co-processor acceleration. © 2019 Elsevier Inc.",""
"Exploiting multi-core and GPU hardware to speed up the registration of range images by means of Differential Evolution","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050408101&doi=10.1016%2fj.jpdc.2018.07.002&partnerID=40&md5=573ec6a9a49df0b05dcf810545a38478","Within this paper a general-purpose distributed evolutionary algorithm is presented, and is applied to the pair-wise registration of range images. Registration is carried out by utilizing the Grid Closest Point (GCP) for the graphical registration operations and the distributed algorithm to search for the best possible transformation of a scene image that, merged with the model image, yields a 3D reconstruction of the original object. The evolutionary algorithm is a distributed Differential Evolution algorithm that exploits an asynchronous migration mechanism and a multi-population recombination information exchange. Such an algorithm is provided with an adaptive updating scheme based on chaotic features for dynamically updating the control parameters. The scope of the paper is to speed up the registration process by using processor specialized to handle graphical operations and multi-core platforms. On the one hand, we investigate the use of either Graphic Processing Units (GPUs) or multi-core architectures to lower the execution time of the GCP procedure. On the other hand, we evaluate the performance of the distributed evolutionary algorithm in terms of solution quality by examining different multi-core architectures. Experimental results on a set of publicly available images show that, to perform the GCP, reductions in the execution times by one order of magnitude are obtained by harnessing the computational power of GPU and multi-core platforms with respect to the execution on a CPU-based framework. Furthermore, a comparison with the state-of-the-art sequential evolutionary algorithm for range image registration reveals that the adaptive distributed Differential Evolution algorithm allows attaining integral 3D models from 3D scan datasets that are better in terms of both quality and robustness. © 2018 Elsevier Inc.","Distributed computing; Heuristics; Range image registration"
"Performance evaluation of FIWARE: A cloud-based IoT platform for smart cities","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066430784&doi=10.1016%2fj.jpdc.2018.12.010&partnerID=40&md5=63ae9f9e5451630b9281aa9e6e34f4fd","As the Internet of Things (IoT) becomes a reality, millions of devices will be connected to IoT platforms in smart cities. These devices will cater to several areas within a smart city such as healthcare, logistics, and transportation. These devices are expected to generate significant amounts of data requests at high data rates, therefore, necessitating the performance benchmarking of IoT platforms to ascertain whether they can efficiently handle such devices. In this article, we present our results gathered from extensive performance evaluation of the cloud-based IoT platform, FIWARE. In particular, to study FIWARE's performance, we developed a testbed and generated CoAP and MQTT data to emulate large-scale IoT deployments, crucial for future smart cities. We performed extensive tests and studied FIWARE's performance regarding vertical and horizontal scalability. We present bottlenecks and limitations regarding FIWARE components and their cloud deployment. Finally, we discuss cost-efficient FIWARE deployment strategies that can be extremely beneficial to stakeholders aiming to deploy FIWARE as an IoT platform for smart cities. © 2019","Benchmarking; Cloud computing; Internet of things; Middleware; Quality of service; Smart cities"
"SIMD programming using Intel vector extensions","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072852235&doi=10.1016%2fj.jpdc.2019.09.012&partnerID=40&md5=3fb531147efa35407cc5cda52c7da889","Single instruction multiple data (SIMD) extensions are one of the most significant capabilities of recent General Purpose Processors (GPPs) which improves the performance of applications with less hardware modification. Each GPP vendor such as HP, Sun, Intel, and AMD has its particular Instruction Set Architecture (ISA) and SIMD micro-architecture with different perspectives. Intel expanded SIMD technologies from hardware and software point of view. It has introduced SIMD technologies such as MultiMedia eXtensions (MMX), Streaming SIMD Extensions (SSE), Advanced Vector eXtensions (AVX), Fused Multiply Add (FMA) and AVX-512 sets. During micro-processors developments path, register width has been extended from 64 bits to 512 bits and number of vector registers has been increased from 8 to 32. Wider registers provide more parallelism ways and more registers reduce extra data movement to the cache memory. In order to gain the advantages of SIMD extensions, many programming approaches have been developed. Compiler Automatic Vectorization (CAV) as an implicit vectorization approach, provides simple and easy SIMDization tools. While, performance improvement of CAV is not always granted, most compilers auto-vectorize simple loops. On the other hand, for explicit vectorization, Intrinsic Programming Model (IPM) provides low-level access to vector registers for SIMDizing. However, programming with IPM requires great amount of expertise especially in low-level architecture feature, thus, choosing the suitable instructions and vectorization methodology for mapping to a certain algorithm is important. Moreover, portability, compatibility, scalability and compiler optimization might limit the advantage of IPM. Our goal in this paper is as follows. First, we provide a review of SIMD technology in general and Intel's SIMD extensions in particular. Second, some SIMD features of Intel SIMD technologies, MMX, SSEs, AVX, and FMA in terms of ISA, vector width, and SIMD programming tools are comparatively discussed. Third, in order to compare the performance of different auto-vectorizers and IPM approaches using Intel C++ compiler (ICC), GNU Compiler Collection (GCC) and Low Level Virtual Machine (LLVM), we map and implement some representative multimedia kernels on AVX and AVX2 extensions. Finally, our experimental results show that although the performance improvement using IPM approach is higher than CAVs, programmer needs more programming efforts and knows different mapping strategists. Therefore, extending auto-vectorizers abilities to generate more efficient vectorized codes is an important issue in different compilers. © 2019 Elsevier Inc.","AVX; AVX-512; Intel; SIMD; Vectorization"
"A privacy preserved and credible network protocol","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067390897&doi=10.1016%2fj.jpdc.2019.06.002&partnerID=40&md5=922c9f004d67c05037f126d40bea43c0","The identities of packet senders and receivers are treated as important privacy information in communication networks. Any packet can be attributed to its sender for evaluating its credibility. Existing studies mainly rely on third-party agents that contain the packet sender's identity to ensure the sender's privacy preservation and credibility. In this case, packet senders run the risk that their privacy might be leaked by the agent. To this end, this paper proposes a Privacy Preserved and Credible Network Protocol (PCNP), which authorizes the agent to hide the identities of senders and receivers, while guaranteeing the credibility of a packet. The feasibility of the PCNP deployment is analyzed, and its performance is evaluated through extensive experiments. © 2019 Elsevier Inc.","Accountability; Credibility; Network protocol; Privacy protection"
"GPU based parallel genetic algorithm for solving an energy efficient dynamic flexible flow shop scheduling problem","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052740364&doi=10.1016%2fj.jpdc.2018.07.022&partnerID=40&md5=b36a68a8f4c44a44c3de6289c7b96e03","Due to new government legislation, customers’ environmental concerns and continuously rising cost of energy, energy efficiency is becoming an essential parameter of industrial manufacturing processes in recent years. Most efforts considering energy issues in scheduling problems have focused on static scheduling. But in fact, scheduling problems are dynamic in the real world with uncertain new arrival jobs after the execution time. This paper proposes an energy efficient dynamic flexible flow shop scheduling model using the peak power value with consideration of new arrival jobs. As the problem is strongly NP-hard, a priority based hybrid parallel Genetic Algorithm with a predictive reactive complete rescheduling strategy is developed. In order to achieve a speedup to meet the short response in the dynamic environment, the proposed method is designed to be highly consistent with the NVIDIA CUDA software model. Finally, numerical experiments are conducted and show that our approach can not only solve the problem flexibly, but also gain competitive results and reduce time requirements dramatically. © 2018 Elsevier Inc.","Dynamic scheduling; Energy efficiency; Flexible flow shop; GPU Computing; Hybrid parallel genetic algorithm"
"Scalable clustering and mapping algorithm for application distribution on heterogeneous and irregular FPGA clusters","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044845564&doi=10.1016%2fj.jpdc.2018.02.033&partnerID=40&md5=d96075bf2a791c91fff7b8344b47fa12","The high flexibility of FPGAs predestines them for emulation and prototyping of ASIC designs. To increase the available amount of resources or to reduce costs, multiple low cost mainstream FPGA boards can be combined into one cluster. This paper presents a new algorithm for the distribution of application tasks into a cluster of FPGAs. This algorithm focuses on both clustering and mapping in one single step, which can be split into two phases. The first phase uses load balancing techniques, to achieve scalability for the number of tasks in the application and the number of FPGAs in the cluster. In the second phase different heuristic search techniques are used, to solve optimization problems, like the reduction of the maximum dilation and the maximum capacity utilization. In order to be applicable to many different topologies, the algorithm supports heterogeneous and irregular structures for FPGA clusters. © 2018 Elsevier Inc.","Algorithm design and analysis; Application distribution; Clustering & mapping algorithms; Field programmable gate arrays; Heterogeneity; Heuristic search techniques; Irregular graphs; Workload balancing"
"Constructing node-independent spanning trees on the line graph of the hypercube by an independent forest scheme","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071428643&doi=10.1016%2fj.jpdc.2019.08.006&partnerID=40&md5=679ca8aba15793f355b0c81810524f81","Due to the application in reliable communication, reliable broadcasting, secure message distribution, etc., node/edge-independent spanning trees (ISTs) have attracted much attention in the past twenty years. However, node/edge conjecture is still open for networks with node/edge-connectivity ≥ 5. So far, results have been obtained on a lot of special networks, but only a few results are reported on the line graphs of them. Hypercubes play important roles in parallel computing systems, and the line graphs of which have been recently adopted for the architectures of data center networks. Since the line graph of n-dimensional hypercube Qn, L(Qn), is (2n−2)-regular, whether there exist 2n−2 node-ISTs rooted at any node on L(Qn) is an open question. In this paper, we focus on the problem of constructing node-ISTs on L(Qn). Firstly, we point out that L(Qn) can be partitioned into 2n−1 complete graphs. Then, based on the complete graphs and n−1 node-ISTs rooted at 0 on Qn−1 0, we obtain an “independent forest” containing 2n−2 trees on L(Qn). Furthermore, we present an O(N) time algorithm to construct 2n−2 node-ISTs rooted at node [0, 2n−1] isomorphic to each other on L(Qn) based on the independent forest, where N=n×2n−1 is the number of nodes on L(Qn). In addition, we point out that the 2n−2 node-ISTs on L(Qn) is a new method to prove the node/edge-connectivity and the upper bound of (2n−2)-node/edge-wide-diameter of L(Qn). © 2019 Elsevier Inc.","Independent forest; Line graph; Network; Node-disjoint paths; Node-independent spanning trees"
"RedSync: Reducing synchronization bandwidth for distributed deep learning training system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067865405&doi=10.1016%2fj.jpdc.2019.05.016&partnerID=40&md5=bcc04981725e266c2b448d482f73cb6a","Data parallelism has become a dominant method to scale Deep Neural Network (DNN) training across multiple nodes. Since the bandwidth requirement of synchronizing the gradients of the local model can be a bottleneck for large-scale distributed training, compressing communication traffic has gained widespread attention recently. Among several recent proposed compression algorithms, Residual Gradient Compression (RGC) is one of the most successful approaches—it can significantly compress the transmitting message size (0.1% of the gradient size) of each node and still achieve correct accuracy and the same convergence speed. However, the literature on compressing deep networks focuses almost exclusively on achieving good theoretical compression rate, while the efficiency of RGC in real implementation has been less investigated. In this paper, we develop an RGC method that is able to reduce the end-to-end training time on real-world multi-GPU systems. Our proposed RGC system design called RedSync, introduces a set of optimizations to reduce communication bandwidth while introducing limited overhead. We examine the performance of RedSync on two different multiple GPU platforms, including 128 GPUs of a supercomputer and an 8-GPU server. Our test cases include image classification on Cifar10 and ImageNet, and language modeling tasks on Penn Treebank and Wiki2 datasets. For DNNs featured with high communication to computation ratio, which has long been considered with poor scalability, RedSync shows significant performance improvement. © 2019 Elsevier Inc.","Data parallelism; Deep learning; Gradient compression; Graphics processing unit (GPU); MPI; Quantization; Sparsification"
"Quality of Experience (QoE)-aware placement of applications in Fog computing environments","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044848033&doi=10.1016%2fj.jpdc.2018.03.004&partnerID=40&md5=239e44affb60efc1a3d05c5a69cbb95a","Fog computing aims at offering Cloud like services at the network edge for supporting Internet of Things (IoT) applications with low latency response requirements. Hierarchical, distributed and heterogeneous nature of computational instances make application placement in Fog a challenging task. Diversified user expectations and different features of IoT devices also intensify the application placement problem. Placement of applications to compatible Fog instances based on user expectations can enhance Quality of Experience (QoE) regarding the system services. In this paper, we propose a QoE-aware application placement policy that prioritizes different application placement requests according to user expectations and calculates the capabilities of Fog instances considering their current status. In Fog computing environment, it also facilitates placement of applications to suitable Fog instances so that user QoE is maximized in respect of utility access, resource consumption and service delivery. The proposed policy is evaluated by simulating a Fog environment using iFogSim. Experimental results indicate that the policy significantly improves data processing time, network congestion, resource affordability and service quality. © 2018 Elsevier Inc.","Application placement; Fog computing; Fuzzy logic; Quality of experience; User expectation"
"Preparing opportunistic networks for smart cities: Collecting sensed data with minimal knowledge","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072615079&doi=10.1016%2fj.jpdc.2019.09.005&partnerID=40&md5=7a1bd4a8eae47e264349c6a01fa162cf","Opportunistic Networks exploit portable handheld devices to collect delay-tolerant data from sensors to gateways for realizing various Smart City applications. To obtain knowledge for determining suitable routing paths as users go about their daily routine, nodes maintain history of every encounter and exchange the information through summary vectors. Due to large node populations, the size of summary vectors makes it challenging to implement real-world city-scale applications with the technology. In this paper, we take the technology a step towards real-world implementation by proposing a set of adaptive and privacy-preserving mechanisms that can be incorporated into existing encounter-based routing protocols to reduce summary vector sizes without compromising delivery guarantees. We validate our proposals with real-world human movement traces and simulation experiments. In terms of network performance, our proposals reduce the average summary vector size by 75% to achieve up to 21% less energy consumption with about 28% improvement in throughput. © 2019 Elsevier Inc.","Contact information overhead; Opportunistic networks; Sensed Data Collection; Smart City; Wireless sensors"
"Configurable cost-quality optimization of cloud-based VoIP","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050162601&doi=10.1016%2fj.jpdc.2018.07.001&partnerID=40&md5=d49d3747fd1fd1a56a2ebe731b227ab5","In this paper, we formulate configurable cloud-based VoIP call allocation problem as a special case of dynamic multi-objective bin-packing. We consider voice quality influenced by CPU stress, cost contributed by the number of billing hours for Virtual Machines (VMs) provisioning, and calls placed on hold due to under-provisioning resources. We distinguish call allocation strategies by the type and amount of information used for allocation: knowledge-free, utilization-aware, rental-aware, and load-aware. We propose and study a variety of strategies with static and dynamic policies of VM provisioning. To study realistic scenarios, we consider startup delays for VM provisioning, and three configurable parameters: utilization threshold, rental threshold, and prediction interval. They can be configured and dynamically adapted to cope with different objective preferences, workloads, and cloud properties. We conduct comprehensive simulation on the real workload of the MIXvoip company and show that the proposed strategies outperform ones currently in-use. © 2018 Elsevier Inc.","Bin packing; Call allocation; Cloud computing; Cloud voice over IP; Quality of service; Scheduling"
"Cooperative unmanned aerial vehicles with privacy preserving deep vision for real-time object identification and tracking","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065830000&doi=10.1016%2fj.jpdc.2019.04.009&partnerID=40&md5=03d3d548ae921a74bcd02307f94486ed","Human tracking is an important challenge in a wide variety of applications, including but not limited to, surveillance, military operations, and disaster relief services. Unmanned Aerial Vehicles (UAVs) allow the surveying of dangerous or impassable areas from a safe distance. They also provide a machine-based capability, which may not only solve resource constraint issues, but can also improve effectiveness and efficiency in the tracking task. The effectiveness of tracking is directly related to the angle of view and degree of freedom of the camera system. In this paper, we introduce a decentralized, distributed deep learning algorithm for Real-Time Privacy-preserving Target Tracking Re-Identification (RPTT-ReID) used by cooperative UAVs in complex and adversarial environments involving motion, crowded scenes, and varied camera angles. The efficiency of RPTT-ReID makes it amenable to edge computing applications. The proposed algorithmic approach resolves shortfalls with current tracking algorithms, specifically challenges in maintaining tracking when subjects cross paths, switch identity, or are occluded in a frame of view. We demonstrate the power of our approach both in single and multi-UAV scenarios to track movable targets by extracting the facial embedding information in crowds, in order to ensure the privacy of individuals captured by the UAVs without compromising the capability for target re-identification. We validate RPTT-ReID on a challenging video dataset of crowded scenes. Our experimental evaluation shows that the proposed approach is capable of tracking and re-identifying people in crowds despite blended trajectories with minimum and maximum accuracy of 79.91 ± 0.2% and 93.27 ± 0.1% respectively. The proposed approach is 18% faster than previous methods for tracking in crowded urban environments. © 2019 Elsevier Inc.","Deep learning; Facial recognition; Object tracking; Re-identification; Unmanned aerial vehicles"
"MIDAS: Multilinear detection at scale","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068112273&doi=10.1016%2fj.jpdc.2019.04.006&partnerID=40&md5=43f6b9ec25cce452dbec8e06377a7706","We focus on two classes of problems in graph mining: (1) finding trees and (2) anomaly detection in complex networks using scan statistics. These are fundamental problems in a broad class of applications. Most of the parallel algorithms for such problems are either based on heuristics, which do not scale very well, or use techniques like color coding, which have a high memory overhead. In this paper, we develop a novel approach for parallelizing both these classes of problems, using an algebraic representation of subgraphs as monomials—this methodology involves detecting multilinear terms in multivariate polynomials. Our algorithms show good scaling over a large regime, and they run on networks with close to half one billion edges. The resulting parallel algorithm for trees is able to scale to subgraphs of size 18, which has not been done before, and it significantly outperforms the best prior color coding based method (FASCIA) by more than two orders of magnitude. Our algorithm for network scan statistics is the first such parallelization, and it is able to handle a broad class of scan statistics functions with the same approach. © 2019","Distributed graph algorithms; Graph scan statistics; Multilinear detection; Parameterized complexity; Subgraph isomorphism"
"A fine-grained authorized keyword secure search scheme with efficient search permission update in cloud computing","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072748458&doi=10.1016%2fj.jpdc.2019.09.011&partnerID=40&md5=d7e12863051139cc5f985634b47162a3","With the rapid development of cloud computing, secure search has become a hot research spot, which is a promising technique that allows a data user to perform privacy-preserving keyword-based search over encrypted cloud data. In this paper, we further consider the secure search problem based on a practical application scenario that a data owner needs to grant different keyword query permissions for different data users to achieve flexible access control on outsourced encrypted data in the cloud computing environment. To address this problem, we propose a fine-grained authorized keyword secure search scheme by leveraging the ciphertext policy attribute-based encryption (ABE), which not only supports privacy-preserving keyword-based search over encrypted data, but also inherits flexible and fine-grained data privilege control properties of ABE. Moreover, our proposed scheme is able to achieve fine-grained search permission update with very small communication and computation cost. By running the attribute revocation sub-protocol and attribute addition sub-protocol, the data owner can flexibly and efficiently update a data user's keyword search permissions when the data user's system role changes. We provide detailed performance analysis and rigorous security proof for our scheme. Extensive experiments demonstrate the correctness and practicality of the proposed scheme. © 2019 Elsevier Inc.","Attribute-based encryption; Authorized keyword search; Cloud computing; Search permission update; Secure search"
"Vertical elasticity on Marathon and Chronos Mesos frameworks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060948764&doi=10.1016%2fj.jpdc.2019.01.002&partnerID=40&md5=18afc7d8a05d019eaad14e856e9416bb","Marathon and Chronos are two popular Mesos frameworks widely used for deploying fault-tolerant services and periodic batch jobs. Marathon and Chronos provide mechanisms for horizontal elasticity, scaling up and down the number of job and service instances. Horizontal elasticity is appropriate when the problems that are solved are inherently parallel. However, when the problem cannot benefit from an increase in the resources, vertical elasticity must be considered. This work implements a mechanism on top of Marathon and Chronos Mesos frameworks to vary the resources assigned to an application dynamically according to its progress and considering a specific Quality of Service (QoS). The mechanism developed provides a wrapper executable and a service that decides to increase or decrease the resources assigned to different Chronos iterations or a long-living Marathon application. The mechanism makes use of checkpointing techniques to preserve the execution of Marathon applications and leverages OpenStack Monasca for the monitoring. © 2019 Elsevier Inc.","Cloud computing; Mesos frameworks; Quality of Service; Vertical elasticity"
"Troodon: A machine-learning based load-balancing application scheduler for CPU–GPU system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067235743&doi=10.1016%2fj.jpdc.2019.05.015&partnerID=40&md5=50e8ba5a9c229370608c8e630d9ddc6f","Heterogeneous computing machines consisting of a CPU and one or more GPUs are increasingly being used today because of their higher performance-cost ratio and lower energy consumption. To program such heterogeneous systems, OpenCL has become an industry standard due to the portability across various computing architectures. To exploit the computing capabilities of heterogeneous systems, application developers are porting their cluster and Cloud applications using OpenCL. With the increasing number of such applications, the use of shared accelerating computing devices (such as CPUs and GPUs) should be managed using an efficient load-balancing scheduling heuristic capable of reducing execution time, increasing throughput with high device utilization. Mostly, the OpenCL applications are suited (execute faster) on a specific computing device (CPU or GPU) and with varying data-sizes the speedup obtained by an application on the suitable device varies too. Applications’ mapping to computing devices without considering device suitability and obtainable speedup on a suitable device leads to sub-optimal execution time, lower throughput and load imbalance. Therefore, an application scheduler should consider both the device-suitability and speedup variation for scheduling decisions leading to a reduction in execution time and an increase in throughput. In this paper, we present a novel load-balancing scheduling heuristic named as Troodon that considers machine-learning based device-suitability model that classify OpenCL applications into either CPU suitable or GPU suitable. Moreover, a speedup predictor that predicts the amount of speedup that jobs will obtain when executed on a suitable device is also part of the Troodon. Troodon incorporates the E-OSched scheduling mechanism to map jobs on CPU and GPUs in a load balanced way. This results in reduced applications execution time, increased system throughput, and improved device utilization. We evaluate the proposed scheduler using a large number of data-parallel applications and compared with several other state-of-the-art scheduling heuristics. The experimental evaluation has demonstrated that the proposed scheduler outperformed the existing heuristics and reduced the application execution time up to 38% with increased system throughput and device utilization. © 2019 Elsevier Inc.","Device suitability; Heterogeneous system; Load-balancing; Machine learning; Scheduling"
"HyPar: A divide-and-conquer model for hybrid CPU–GPU graph processing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066739869&doi=10.1016%2fj.jpdc.2019.05.014&partnerID=40&md5=e818277a9a337c13749a7a01a9d6442f","Efficient processing of graph applications on heterogeneous CPU–GPU systems require effectively harnessing the combined power of both the CPU and GPU devices. This paper presents HyPar, a divide-and-conquer model for processing graph applications on hybrid CPU–GPU systems. Our strategy partitions the given graph across the devices and performs simultaneous independent computations on both the devices. The model provides a simple and generic API, supported with efficient runtime strategies for hybrid executions. The divide-and-conquer model is demonstrated with five graph applications and using experiments with these applications on a heterogeneous system it is shown that our HyPar strategy provides equivalent performance to the state-of-art, optimized CPU-only and GPU-only implementations of the corresponding applications. When compared to the prevalent BSP approach for multi-device executions of graphs, our HyPar method yields 74%–92% average performance improvements. © 2019 Elsevier Inc.","Divide-and-conquer; Graph algorithms; Hybrid CPU–GPU"
"Differential privacy-based trajectory community recommendation in social network","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068860757&doi=10.1016%2fj.jpdc.2019.07.002&partnerID=40&md5=57389293a5cd8fa20e01528c2abd3d0b","Trajectory community recommendation (TCR) is a location-based social network (LBSN) service whereby LBSN server recommends a user a community in which the trajectories have similar movement patterns with the user's trajectory. Due to privacy concerns, the trajectory should be protected. However, the data availability of traditional privacy-preserving schemes is limited, and previous differential privacy (DP) methods cannot achieve high data utility in TCR and rely on a fully trusted third party. In this paper, we propose a DP-based trajectory community recommendation (DPTCR) scheme to perform effective TCR service while protecting trajectory privacy by the user himself. First, DPTCR transits the actual trajectory's locations into noisy feature locations based on private semantic expectation method, which ensures the semantic similarity between noisy locations and the actual locations. Second, DPTCR uses a private geographical distance method to construct a noisy trajectory that has the smallest geographical distance with the actual trajectory. Finally, DPTCR uses a semantic-geographical distance model to cluster a community in which the trajectories have high similarity with the constructed noisy trajectory. Security analysis proves that our DPTCR scheme achieves ϵ-DP, and experimental results show that our scheme achieves high efficiency and data utility. © 2019 Elsevier Inc.","Data utility; Differential privacy; Location-based social network; Semantic-geographical distance"
"Operating cost and quality of service optimization for multi-vehicle-type timetabling for urban bus systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042618593&doi=10.1016%2fj.jpdc.2018.01.009&partnerID=40&md5=935c2208bbc17f30b8d8081b992db46f","In this paper, we propose a timetable optimization method based on a Multiobjective Cellular genetic algorithm to tackle the multiple vehicle-type problems. The objective is to determine bus assignment in each time period to optimize a quality of service and transport operating cost. The quality of service, represented by the unsatisfied user demand, guarantees a good experience in terms of comfort, safety, availability, improving effects on how passengers perceive wait times. The operational cost contributes to reducing the traffic jams, the flux of unfilled vehicles and fuel consumption, helping to diminish the negative environmental impact. With the operation data of Los Angeles bus route 217 northbound, at peak and off-peak hours, we obtain a set of non-dominated solutions that represent different assignments of vehicles covering a given set of trips in a defined route. The experimental analysis based on several quality indicators, like Hypervolume, Spread, ε-Indicator, and Set Coverage, indicates that our algorithm is a competitive technique comparing with well-known techniques presented in the literature. © 2018 Elsevier Inc.","Evolutionary algorithms; Metaheuristics; Multiobjective optimization; Multiple vehicle types; Public transport; Smart cities"
"Fast neural network training on a cluster of GPUs for action recognition with high accuracy","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071947285&doi=10.1016%2fj.jpdc.2019.07.009&partnerID=40&md5=a983ec40047bdf0d508f636e0100d565","We propose algorithms and techniques to accelerate training of deep neural networks for action recognition on a cluster of GPUs. The convergence analysis of our algorithm shows it is possible to reduce communication cost and at the same time minimize the number of iterations needed for convergence. We customize the Adam optimizer for our distributed algorithm to improve efficiency. In addition, we employ transfer-learning to further reduce training time while improving validation accuracy. For the UCF101 and HMDB51 datasets, the validation accuracies achieved are 93.1% and 67.9% respectively. With an additional end-to-end trained temporal stream, the validation accuracies achieved for UCF101 and HMDB51 are 93.47% and 81.24% respectively. As far as we know, these are the highest accuracies achieved with the two-stream approach using ResNet that does not involve computationally expensive 3D convolutions or pretraining on much larger datasets. © 2019 Elsevier Inc.","Distributed training; GPU; Machine learning; Transfer learning; Video analytics"
"Fast and green parallel isogeometric analysis computations for multi-objective optimization of liquid fossil fuel reserve exploitation with minimal groundwater contamination","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071029583&doi=10.1016%2fj.jpdc.2019.06.010&partnerID=40&md5=d12a6393572c0aeee07c2eadc9cefa8f","We present a general optimization technique to minimize the execution time, energy consumption, and numerical error of a parallel isogeometric finite element method (IGA-FEM) solver for time-dependent problems. The IGA-FEM solver is called upon multiple times during inverse problem computations. It is used to evaluate the inverse problem parameters. As an exemplary challenging problem, we consider the IGA-FEM solver as a primal solver for the inverse optimization of hydraulic fracking. Liquid Fossil Fuel Extraction Problem (LFFEP) is defined to maximize extraction and minimize contamination for the hydraulic fracking process. Solving the LFFEP problem requires hundreds of computationally demanding calls to the IGA-FEM solver. Thus, we research the optimal configuration of the IGA-FEM solver to make computations more efficient; i.e., to reduce computation time and energy consumption where the input parameters are the number of utilized CPUs in a parallel environment, mesh size, and B-spline order. The algorithm presented in this paper can be used to optimize any similar time-dependent IGA-FEM simulation. © 2019 Elsevier Inc.","Alternating directions parallel solver; Fossil fuel extraction; Isogeometric analysis; Isogeometric analysis; Multi-objective optimization"
"Analysis of heterogeneous computing approaches to simulating heat transfer in heterogeneous material","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067390249&doi=10.1016%2fj.jpdc.2019.06.004&partnerID=40&md5=7da2a42047a9fa84fa6b12fa4e9ce147","The simulation of heat flow through heterogeneous material is important for the design of structural and electronic components. Classical analytical solutions to the heat equation PDE are not known for many such domains, even those having simple geometries. The finite element method can provide approximations to a weak form continuum solution, with increasing accuracy as the number of degrees of freedom in the model increases. This comes at a cost of increased memory usage and computation time; even when taking advantage of sparse matrix techniques for the finite element system matrix. We summarize recent approaches in solving problems in structural mechanics and steady state heat conduction which do not require the explicit assembly of any system matrices, and adapt them to a method for solving the time-depended flow of heat. These approaches are highly parallelizable, and can be performed on graphical processing units (GPUs). Furthermore, they lend themselves to the simulation of heterogeneous material, with a minimum of added complexity. We present the mathematical framework of assembly-free FEM approaches, through which we summarize the benefits of GPU computation. We discuss our implementation using the OpenCL computing framework, and show how it is further adapted for use on multiple GPUs. We compare the performance of single and dual GPUs implementations of our method with previous GPU computing strategies from the literature and a CPU sparse matrix approach. The utility of the novel method is demonstrated through the solution of a real-world coefficient inverse problem that requires thousands of transient heat flow simulations, each of which involves solving a 1 million degree of freedom linear system over hundreds of time steps. © 2019 Elsevier Inc.","Heat conduction simulation; Heterogeneous computing; OpenCL"
"LFRic: Meeting the challenges of scalability and performance portability in Weather and Climate models","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066413388&doi=10.1016%2fj.jpdc.2019.02.007&partnerID=40&md5=6b171dd627bec017113773f95f114054","This paper describes LFRic: the new weather and climate modelling system being developed by the UK Met Office to replace the existing Unified Model in preparation for exascale computing in the 2020s. LFRic uses the GungHo dynamical core and runs on a semi-structured cubed-sphere mesh. The design of the supporting infrastructure follows object-oriented principles to facilitate modularity and the use of external libraries where possible. In particular, a ‘separation of concerns’ between the science code and parallel code is imposed to promote performance portability. An application called PSyclone, developed at the STFC Hartree centre, can generate the parallel code enabling deployment of a single source science code onto different machine architectures. This paper provides an overview of the scientific requirement, the design of the software infrastructure, and examples of PSyclone usage. Preliminary performance results show strong scaling and an indication that hybrid MPI/OpenMP performs better than pure MPI. © 2019","Domain specific language; Exascale; Numerical weather prediction; Separation of concerns"
"Privacy preserving classification on local differential privacy in data centers","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072698096&doi=10.1016%2fj.jpdc.2019.09.009&partnerID=40&md5=ac94396fbba719de4ecc3736162a82c8","With the rise of cloud service providers and the continuous virtualization of data centers, data center networks are also developing rapidly. As data centers become more and more complex, the demand for security increases dramatically. This paper discusses the privacy inherent in data centers. However, there is no general solution to the privacy problem in data centers due to the device heterogeneity. In this paper, we proposed a local differential privacy-based classification algorithm for data centers. In data mining of data centers, the differential privacy protection mechanism is added to deal with Laplace noise of sensitive information in the pattern mining process. We designed a method to quantify the quality of privacy protection through strict mathematical proof. Experiments demonstrated that the differential privacy-based classification algorithm proposed in this paper has higher iteration efficiency, better security and feasible accuracy. On the premise of ensuring availability, the algorithm has reliable privacy protection characteristics and excellent timeliness. © 2019 Elsevier Inc.","Classification model; Data center networks; Data mining; Local Differential privacy"
"Accelerating the similarity self-join using the GPU","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068447185&doi=10.1016%2fj.jpdc.2019.06.005&partnerID=40&md5=cd16fc7fe1a26e6d3347716ccca1e9a9","The self-join finds all objects in a dataset within a threshold of each other defined by a similarity metric. As such, the self-join is a fundamental building block for the field of databases and data mining. In low dimensionality, there are several challenges associated with efficiently computing the self-join on the graphics processing unit (GPU). Low dimensional data results in higher data densities, causing a significant number of distance calculations and a large result set, and as dimensionality increases, index searches become increasingly exhaustive. We propose several techniques to optimize the self-join using the GPU that include a GPU-efficient index that employs a bounded search, a batching scheme to accommodate large result sets, and duplicate search removal with low overhead. Furthermore, we propose a performance model that reveals bottlenecks related to the result set size and enables us to choose a batch size that mitigates two sources of performance degradation. Our approach outperforms the state-of-the-art on most scenarios. © 2019 Elsevier Inc.","GPGPU; In-memory database; Query optimization; Self-join"
"Reco-Pi: A reconfigurable Cryptoprocessor for π-Cipher","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020640917&doi=10.1016%2fj.jpdc.2017.05.012&partnerID=40&md5=ed76e9661e9b2bd9c26c7cab70ac3de0","This paper presents the reconfigurable hardware design of an Encryptor and a Decryptor of the 16-, 32-, and 64-bit versions of π-Cipher called “Reco-Pi”, one of candidate designs for the Competition for Authenticated Encryption: Security, Applicability, and Robustness. π-Cipher is a nonce-based authenticated encryption engine with associated data. The security of π-Cipher depends on using Add, Rotate, XOR operations. Furthermore, in this paper, a Cryptoprocessor for the 16-, 32-, and 64-bit versions are introduced. The functionality of the three different bit versions of the π-Cipher were verified on the Xilinx Virtex-7. Our results show an improvement in the design speed by 45% at the cost of increase in 17% of the total area. Also, we observe an improvement in the design's throughput of 76% compared to our previous work of the 16-bit Encryptor. © 2017 Elsevier Inc.","Authenticated encryption; CAESAR; Cryptoprocessor; Dynamic circuit specialization; FPGA; Micro-reconfiguration; TLUT; π-Cipher"
"SMCA: An efficient SOAP messages compression and aggregation technique for improving web services performance","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069565876&doi=10.1016%2fj.jpdc.2019.07.001&partnerID=40&md5=cb4e94c997efa64fe11aefab3a2e56b4","The Simple Object Access Protocol (SOAP) is an eXtensible Markup Language (XML) based messaging protocol, which is widely used over the Internet. It supports interoperability by creating access between users and their service providers from the same or different platforms. However, the huge number and the large size of exchanged SOAP messages cause congestions and bottlenecks. Existing techniques based on grouping of XML messages have shown some shortcomings in terms of execution time and compression ratio. Therefore, in this paper, we propose a new technique called SMCA for efficiently compressing and aggregating the SOAP messages. Technically, the proposed technique requires only one passage on all the XML messages to perform aggregation and compression processes. Based on the SMCA technique, the XML data of the same paths are regrouped in one container. The experimental results on real XML dataset verify the efficiency and the effectiveness of the proposed technique. © 2019 Elsevier Inc.","Aggregation; Compression; SOAP; Web services; XML"
"Automatic social signal analysis: Facial expression recognition using difference convolution neural network","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065591354&doi=10.1016%2fj.jpdc.2019.04.017&partnerID=40&md5=0d43a1d74e78f0a6c0b1e957801f4daa","Facial expression is one of the most powerful social signals for human beings to convey emotion and intention, hence automatic facial expression recognition (FER) has wide applications in human–computer interaction and affective computing, it has attracted an increasing attention recently. Researches in this field have made great progress especially with the development of deep learning method. However, FER remains a challenging task due to individual differences. To address the issue, we propose a two-stage framework based on Difference Convolution Neural Network (DCNN) inspired by the facial expression's nonstationary nature. In the first stage, the neutral expression frame and fully expression frame are automatically picked out from the facial expression sequences using a binary Convolution Neural Network (CNN). Then in the second stage, an end-to-end DCNN is proposed to classify the six basic facial expressions using the difference information between the neutral expression frame and the fully expression frame. Experiments have been conducted on the CK+ and BU-4DFE datasets, and the results show that the proposed framework delivers a promising performance (95.4% on the CK+ dataset and 77.4% on the BU-4DFE). Moreover, the proposed method is also successfully applied to analyze the student's affective state in an E-learning environment which suggests that it has strong potential to analyze nonstationary social signals. © 2019 Elsevier Inc.","Deep learning; Difference convolution neural network; Facial expression recognition; Social signal analysis"
"Interval stabbing on the Automata Processor","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042662088&doi=10.1016%2fj.jpdc.2018.01.010&partnerID=40&md5=70adc2010e77256bf3769efed72390e5","The Automata Processor (AP) was designed for string-pattern matching. In this paper, we showcase its use to execute integer and floating-point comparisons and apply the same to accelerate interval stabbing queries. An interval stabbing query determines which of the intervals in a set overlap a query point. Such queries are often used in computational geometry, pattern matching, database management systems, and geographic information systems. The check for each interval is programmed as a single automaton and multiple automata are executed in parallel to provide significant performance gains. While handling 32-bit integers or single-precision floating-point numbers, up to 2.75 trillion comparisons can be executed per second, whereas 0.79 trillion comparisons per second can be completed for 64-bit integers or double-precision floating-point numbers. Additionally, our solution leaves the intervals in the set unordered allowing addition or deletion of an interval in constant time. This is not possible for contemporary solutions wherein the intervals are ordered, making update of intervals complex. Our automata designs are modular allowing them to become constituent parts of larger automata, where the numerical comparisons are part of the overall pattern matching operation. We have validated the designs on the hardware, and the routines to generate the necessary automata and execute them on the AP will be made available as software libraries shortly. © 2018 Elsevier Inc.","Automata Processor; Finite automata; Interval stabbing"
"A Mobile Code-driven Trust Mechanism for detecting internal attacks in sensor node-powered IoT","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072240948&doi=10.1016%2fj.jpdc.2019.08.013&partnerID=40&md5=cb46746f8cb257b17b9f476755b678a2","The ubiquitous use of Internet-of-Things (IoT) is enabling a new era of wireless Sensor Nodes (SNs) that can be subject to attacks like any other piece of hardware and software. Unfortunately, an open and challenging issue is to what extent legitimate SNs can be trusted. This paper presents an energy-efficient, software-defined-network-based Mobile Code-driven Trust Mechanism (MCTM) for addressing this issue by assessing trust of SNs based on their forwarding behaviors. MCTM uses mobile code to visit the SNs based on pre-defined itineraries while collecting necessary details about these SNs in preparation for assessing their trust. The results gained from the experiments demonstrate a superior performance over a state-of-art technique that is energy-efficient management based on Software-Defined Network (SDN) for SNs. Message overhead is reduced by approximately 50%, which results in consuming less energy when detecting malicious SNs. © 2019 Elsevier Inc.","Energy; Internet of Things; Mobile code; Sensor node; Trust"
"Efficient low-latency packet processing using On-GPU Thread-Data Remapping","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067975117&doi=10.1016%2fj.jpdc.2019.06.009&partnerID=40&md5=74245dc5ea692b0974c3ef59e21da0fd","Graphics processing units are widely-used for packet processing acceleration in both physical and virtual networks. However, real-life packets come in highly-divergent sizes, causing severe GPU control flow divergence. Previous solutions rely on CPU preprocessing to reduce divergence, but it forbids the more efficient NIC–GPU packet streaming as packet batches have to stop completely at host machine. To fully utilize both GPU and PCIe resources, we propose Blink as a GPU modular software router. Instead of CPU pre-processing, the Blink router uses On-GPU Thread-Data Remapping to reduce divergence, and our novel Cross-Iteration Thread Event Signaling mechanism filters unnecessary inter-thread synchronization, doubling the performance gain achieved by traditional solution. Serving as a TCP/IP router with Deep Packet Inspection (DPI) firewall, Blink can sustain processing throughput of 31.5 GBit/s over a PCIe bandwidth of 32 GBit/s. Given a certain bandwidth, Blink reduces processing latency at least by half compared with other works. © 2019 Elsevier Inc.","GPU control flow divergence; Packet processing; SIMD; Software router"
"High-dimensional image descriptor matching using highly parallel KD-tree construction and approximate nearest neighbor search","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067227870&doi=10.1016%2fj.jpdc.2019.06.003&partnerID=40&md5=0cb0c35b767ba8aa0670b5b023d46fab","To overcome the high computational cost associated with the high-dimensional digital image descriptor matching, this paper presents a set of integrated parallel algorithms for the construction of K-dimensional tree (KD-tree) and P approximate nearest neighbor search (P-ANNS) on the modern massively parallel architectures (MPA). To improve the runtime performance of the P-ANNS, we propose an efficient sliding window for a parallel buffered P-ANNS on KD-tree to mitigate the high cost of global memory accesses. When applied to high dimensional real-world image descriptor datasets, the proposed KD-tree construction and the buffered P-ANNS algorithms are of comparable matching quality to the traditional sequential counterparts on CPU, while outperforming their serial CPU counterparts by speedup factors of up to 17and 163, respectively. The algorithms are also studied for the performance impact factors to obtain the optimal runtime configurations for various datasets. Moreover, we verify the features of the parallel algorithms on typical 3D image matching scenarios. With the classical local image descriptor signature of histograms of orientations (SHOT) datasets, the parallel KD-tree construction and image descriptor matching can achieve up to 11 and 138-fold speedups, respectively. © 2019 Elsevier Inc.","Approximate nearest neighbor search; CUDA; GPU; Image matching; KD-tree; Parallel algorithm; Pattern recognition"
"LSB: A Lightweight Scalable Blockchain for IoT security and anonymity","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072261125&doi=10.1016%2fj.jpdc.2019.08.005&partnerID=40&md5=22361bc394943566d817c568b16530f7","In recent years, Blockchain has attracted tremendous attention due to its salient features including auditability, immutability, security, and anonymity. Resulting from these salient features, blockchain has been applied in multiple non-monetary applications including the Internet of Things (IoT). However, blockchain is computationally expensive, has limited scalability and incurs significant bandwidth overheads and delays which are not suited for most IoT applications. In this paper, we propose a Lightweight Scalable blockchain (LSB) that is optimized for IoT requirements while also providing end-to-end security. Our blockchain instantiation achieves decentralization by forming an overlay network where high resource devices jointly manage the blockchain. The overlay is organized as distinct clusters to reduce overheads and the cluster heads are responsible for managing the public blockchain. We propose a Distributed Time-based Consensus algorithm (DTC) which reduces the mining processing overhead and delay. Distributed trust approach is employed by the cluster heads to progressively reduce the processing overhead for verifying new blocks. LSB incorporates a Distributed Throughput Management (DTM) algorithm which ensures that the blockchain throughput does not significantly deviate from the cumulative transaction load in the network. We explore our approach in a smart home setting as a representative example for broader IoT applications. Qualitative arguments demonstrate that our approach is resilient to several security attacks. Extensive simulations show that packet overhead and delay are decreased and blockchain scalability is increased compared to relevant baselines. © 2019 Elsevier Inc.","Blockchain; Internet of Things; Security; Smart home"
"Security-aware multi-objective optimization of distributed reconfigurable embedded systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044113735&doi=10.1016%2fj.jpdc.2018.02.015&partnerID=40&md5=ceed4dc12d6c1a4e6153499bbb2f10af","Distributed embedded systems are increasingly prevalent in numerous applications, and with pervasive network access within these systems, security is also a critical design concern. We present a modeling and optimization framework for distributed embedded systems incorporating heterogeneous resources, including single core processor, asymmetric multicore processors, and FPGAs. A dataflow-based modeling framework for streaming applications integrates models for computational latency, cryptographic security levels, communication latency, and power consumption. We utilize a multi-objective genetic optimization algorithm to optimize security subject to constraints for energy consumption and minimum security level. The presented methodology is evaluated using a video-based object detection and tracking application considering several distributed heterogeneous embedded systems architectures. © 2018 Elsevier Inc.","Co-design modeling; Design space exploration; Distributed embedded systems; Dynamic optimization; Penalty functions; Security"
"Towards comprehensive dependability-driven resource use and message log-analysis for HPC systems diagnosis","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067225322&doi=10.1016%2fj.jpdc.2019.05.013&partnerID=40&md5=2e06cfd3b8928a4128f64f5de4344150","Failure analysis plays an important role in the reliability of data centers and high-performance computing (HPC) systems. Recent work have shown that both resource use data and failure logs can, separately and together, be used to detect system failure-inducing errors and diagnose system failures; the result of error propagation and (unsuccessful) execution of error recovery mechanisms. For more accurate and detailed failure diagnosis, knowledge of error propagation patterns and unsuccessful error recovery is important. To improve system reliability, knowledge of recovery protocols deployment is important. This paper describes and demonstrates application of a new diagnostics framework (CORRMEXT). CORRMEXT analyzes and reports error propagation patterns and degrees of success and failure of error recovery protocols. The steps in the framework are correlations of resource use metrics and error messages, and identification of the earliest times of change of system behaviour. The framework is illustrated with analyses of resource use data and message logs for three HPC systems operated by the Texas Advanced Computing Center (TACC). The illustrations are focused on groups of resource use counters and groups of errors; they reveal many interesting insights into patterns of: (i) network data and software errors, (ii) Lustre file-system and Linux operating system process errors, and (iii) memory and storage errors. We also confirm that: (i) correlations of resource use and errors can only be identified by applying different correlation algorithms, and (ii) the earliest times of change in system behaviour can only be identified by analyzing both the correlated resource use counters and correlated errors. We believe CORRMEXT is the first tool that have diagnosed error propagation paths and error recovery attempts on three different HPC systems. CORRMEXT will be put on the public domain to support systems administrators in diagnosing HPC system failures, on August 2018. © 2019 Elsevier Inc.","Cluster log-data; Correlation; Error propagation and recovery; Large HPC systems; Variance extraction"
"A framework for real time end to end monitoring and big data oriented management of smart environments","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057525900&doi=10.1016%2fj.jpdc.2018.10.015&partnerID=40&md5=045d316fca713550c688c8b7bbd9def3","Nowadays, the success of Internet of Things (IoT) applications depends on the intelligence of tools and techniques that can monitor, manage, and verify the correct operations of smart ecosystems including sensors and big data analytics tools, typically deployed in Cloud and Edge computing datacenters. In this paper, we propose a framework for the monitoring and management of IoT system that integrates the AllJoyn functionalities, useful to interconnect IoT devices, MongoDB, to implement Big Data storage, and Storm, to run real-time data analytics. We implemented the proposed framework and we tested its main functionalities in a smart home application scenario. In our experimentation, we investigated three different data patterns, i.e., regular, event-based, and automated, in order to evaluate performance of our framework in terms of response time under different operational conditions. Experimental results show that the latency of the monitoring and service strongly depends on the type of management application running in the system, whereas it is lightly affected by the data patterns. © 2018 Elsevier Inc.","Big data; Cloud computing; Edge computing; IoT; Management; Monitoring"
"Locality optimized unstructured mesh algorithms on GPUs","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070713624&doi=10.1016%2fj.jpdc.2019.07.011&partnerID=40&md5=9ea01db51183a930c4e870002c2b9300","Unstructured-mesh based numerical algorithms such as finite volume and finite element algorithms form an important class of applications for many scientific and engineering domains. The key difficulty in achieving higher performance from these applications is the indirect accesses that lead to data-races when parallelized. Current methods for handling such data-races lead to reduced parallelism and suboptimal performance. Particularly on modern many-core architectures, such as GPUs, that has increasing core/thread counts, reducing data movement and exploiting memory locality is vital for gaining good performance. In this work we present novel locality-exploiting optimizations for the efficient execution of unstructured-mesh algorithms on GPUs. Building on a two-layered coloring strategy for handling data races, we introduce novel reordering and partitioning techniques to further improve efficient execution. The new optimizations are then applied to several well established unstructured-mesh applications, investigating their performance on NVIDIA's latest P100 and V100 GPUs. We demonstrate significant speedups (1.1–1.75×) compared to the state-of-the-art. A range of performance metrics are benchmarked including runtime, memory transactions, achieved bandwidth performance, GPU occupancy and data reuse factors and are used to understand and explain the key factors impacting performance. The optimized algorithms are implemented as an open-source software library and we illustrate its use for improving performance of existing or new unstructured-mesh applications. © 2019 Elsevier Inc.","Finite element; Finite volume; GPU; Race condition"
"Optimal deterministic distributed algorithms for maximal independent set in geometric graphs","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066794849&doi=10.1016%2fj.jpdc.2019.05.012&partnerID=40&md5=34cd62ab52ee126b15b98c581c4f34a2","Finding a maximal independent set (MIS) in a graph is one of the fundamental problems in distributed computing. Researchers in this community are trying to close the time complexity gap of computing a MIS in a graph, way back from Luby's [STOC’85] O(logn) time (randomized) algorithm and Linial's [SICOMP’92] Ω(log∗n) lower bound result (n is the number of nodes of the graph). Since then, an extensive research has been done on this problem, however, most of the results are randomized and we are lack of efficient deterministic solution. In fact, no polylogarithmic (in n) time deterministic algorithm is known so far in general graphs — an open question raised by Linial in 1993 [Comb. Probab. Comput.’93]. In this paper, we study the MIS problem in geometric graphs, a class of graphs which has both theoretical and practical importance. We present O(1)-time deterministic algorithms (hence, optimal) for computing MIS in unit interval graphs, unit square graphs and unit disk graphs. The same idea applies to develop optimal MIS algorithm for higher dimensional unit balls and unit hypercubes. We further extend the MIS algorithms to compute approximate maximum independent set (MaxIS) in the above geometric graph classes. The theoretical results are corroborated through extensive experiments to show the effectiveness and efficiency of our algorithms. Our algorithms are fully decentralized, scalable, and robust. In general, nodes exchange only O(logn) bits message through an edge per communication. Moreover, being local, our algorithms are also suitable in failure model, self-stabilizing and appropriate for dynamic environment. © 2019 Elsevier Inc.","Distributed algorithm; Geometric graphs; Local algorithm; Maximal and maximum independent set; Unit interval graphs"
"Comparison of the capabilities of GPU clusters and general-purpose supercomputers for solving 3D inverse problems of ultrasound tomography","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068236968&doi=10.1016%2fj.jpdc.2019.06.008&partnerID=40&md5=a56b58b701db8f3fc7480dcbfb7fb572","This study focuses on developing the algorithms for solving 3D inverse problems of ultrasound tomography using GPU clusters and general-purpose supercomputers. The computing capabilities of these supercomputer architectures in application to wave tomography are compared via numerical simulations. Parallel gradient-based iterative algorithms designed to solve coefficient inverse problems for the wave equation are proposed. We show that layer-by-layer wave tomography can be efficiently performed using both CPU- and GPU-based supercomputers. Solving 3D coefficient inverse problems is much more computationally expensive. A general-purpose (CPU-based) system capable of reconstructing 3D images should be equipped with either a sufficient number of memory access channels or a large number of computing cores with large cache memory and high-speed communication network. Such a system would be very large and expensive. Therefore, the most promising solution is to develop dedicated GPU-based supercomputers. From the applications perspective, this study focuses on problems of developing ultrasound tomography devices for breast cancer diagnosis. Computer simulations show that a GPU cluster capable of performing 3D image reconstruction within reasonable time fits in a single rack and can be incorporated into medical ultrasound tomography facilities. © 2019 Elsevier Inc.","GPU clusters; Scalable parallel computing; Supercomputer technologies; Ultrasonic tomography; Wave equation"
"Privacy-preserving big data analytics a comprehensive survey","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072297690&doi=10.1016%2fj.jpdc.2019.08.007&partnerID=40&md5=79fb7bcbcc04f642a2921280eccb11d5","In this paper, we present a comprehensive survey of privacy-preserving big data analytics. We introduce well-designed taxonomies which offer both systematic views and a detailed classification of this challenging research field. We give insights into recent studies on existing active topics in the field. Furthermore, we identify open future research directions for privacy-preserving big data analytics. This survey can serve as a good reference source for the development of modern privacy-preserving techniques to address various privacy-related scenarios to be encountered in practice. © 2019 Elsevier Inc.","Anonymity; Privacy-preserving big data analytics; Private learning; Secure outsourcing; Social networks"
"Rotorcraft virtual sensors via deep regression","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072872455&doi=10.1016%2fj.jpdc.2019.08.008&partnerID=40&md5=49e429c9d4b2c791f14cac3ec1f8b07c","Raw sensor data containing high-fidelity information is highly desirable for valuable post-processing. We developed a machine learning model that performs deep regression to infer rotorcraft component vibration spectra from a few flight conditional indicators (CI). The model consists of a deep neural network of fully connected layers (DNN) that performs high-dimensional and non-linear multivariate regression to reconstruct raw accelerometer data. The network architecture hyperparameters were optimized using an evolutionary genetic algorithm (GA) that was more effective than random and manual search methods. The best GA design was further tuned to achieve spectrum reconstruction accuracies above 95% on validation datasets. An automated model generator workflow was developed to train and evaluate thousands of DNN designs using parallel asynchronous execution on a Cray XC50, which were monitored and studied. Finally, as a verification step of the DNN inference model operation and performance, a detailed sensitivity analysis was performed using a modified Sobol sampling technique to understand response behavior and limitations. The sensitivity analysis method utilized Dask-distributed across multiple nodes on our HPC to evaluate millions of generated samples in parallel. © 2019","Deep Learning; Deep Neural Networks; Evolutionary Optimization; High Performance Computing; Virtual Sensors"
"Parallel evolutionary approaches for game playing and verification using Intel Xeon Phi","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051720494&doi=10.1016%2fj.jpdc.2018.07.010&partnerID=40&md5=3ddfbf55422d2fa2d7bde9439843f8d9","Automatic generation of artificial players is an important subject for the videogames industry. Different strategies have been proposed to implement realistic and intelligent agents for gameplaying and verification. This article presents a parallel evolutionary approach for the automation of computer player generation for video games. A learning pipeline model is defined to study the generation problem for Nintendo Entertainment System games composed of three stages: objective inference, objective refinement and artificial intelligence generation. Two case studies based on the defined pipeline are presented: an evolutionary algorithm to learn how to play the game Pinball, offloading the evaluation of the fitness function to a Xeon Phi coprocessor, and a full pipeline implementation that uses neuroevolution to generate RNNs that can play different games successfully. Results show that the proposed pipeline can be applied for the automatic generation of artificial players for the studied games. © 2018 Elsevier Inc.","NES; Neuroevolution; Parallel evolutionary algorithms; Xeon Phi"
"A distributed approximate nearest neighbors algorithm for efficient large scale mean shift clustering","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071951827&doi=10.1016%2fj.jpdc.2019.07.015&partnerID=40&md5=e666c471ba4e1d0474caa766dd4c3f26","Mean Shift clustering, as a generalization of the well-known k-means clustering, computes arbitrarily shaped clusters as defined as the basins of attraction to the local modes created by the density gradient ascent paths. Despite its potential for improved clustering accuracy, the Mean Shift approach is a computationally expensive method for unsupervised learning. We introduce two contributions aiming to provide approximate Mean Shift clustering, based on scalable procedures to compute the density gradient ascent and cluster labeling, with a linear time complexity, as opposed to the quadratic time complexity for the exact clustering. Both propositions are based on Locality Sensitive Hashing (LSH) to approximate nearest neighbors. When implemented on a serial system, these approximate methods can be used for moderate sized datasets. To facilitate the analysis of Big Data, a distributed implementation, written for the Spark/Scala ecosystem is proposed. An added benefit is that our proposed approximations of the density gradient ascent, when used as a pre-processing step in other clustering methods, can also improve the clustering accuracy of the latter. We present experimental results illustrating the effect of tuning parameters on cluster labeling accuracy and execution times, as well as the potential to solve concrete problems in Big Clustering. © 2019 Elsevier Inc.","Clustering; Gradient ascent; MapReduce; Nearest neighbors; Spark"
"Membership overlay design optimization with resource constraints (accelerated on GPU)","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051016981&doi=10.1016%2fj.jpdc.2018.07.009&partnerID=40&md5=24ab7e66fbb85983d0a88a2aed40ae9c","Centralized overlay network management, such as for Service Overlay Networks, is an important topicfor Internet based services. The computational efficiency of the central controller node is of paramount importance to guarantee the quality of the service. The paper considers the problem where the network can be also asymmetric and each node requires to be connected with nodes which provide some given resources. This work proposes a comparison of alternative approaches to the parallelization of a dynamic overlay network reconfiguration algorithm, implemented as a distributed Lagrangian algorithm. The proposed approach is based on a subgradient algorithm which makes use of the quasi-constant step-size rule specifically studied for a parallel/distributed implementation. The method is implemented using MPI, OpenMP and CUDA on GPU and was focussed on the membership overlay reconfiguration for test instances up to 1000 nodes. © 2018 Elsevier Inc.","GPU; Network overlay; Optimization; Peer to peer"
"Dynamic self-reconfiguration of a MIPS-based soft-core processor architecture","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032939517&doi=10.1016%2fj.jpdc.2017.09.013&partnerID=40&md5=05b576708f93f895ba770c8adf1a5c06","The rising demands for computational performance are a permanent trend in our increasingly digital world. Consistently addressing this trend poses a challenge for every embedded processor system. This paper proposes the use of reconfigurable processor architectures to increase “on demand” processing performance while running a specific target application. The reconfiguration is used to interchange specialized co-processors attached to a static soft-core processor during run-time. Different self-optimization software–hardware substitution mechanisms, inspired by the field of organic computing, are implemented and evaluated using two different synthetic benchmarks and an exemplary application from the field of parallel robotics. An efficient self-optimization can be reached by combining a speed-up-based replacement strategy for scheduling the reconfigurable co-processors and a least mean square optimization algorithm without requiring any a-priori application profiling. For a reduced number of reconfigurable co-processors, the results show that the proposed software–hardware reconfiguration strategy provides, in general, better trade-offs between the required hardware resources and performance improvement when compared to the equivalent soft-core processor with the same number of static co-processors. © 2017 Elsevier Inc.","Dynamic partial reconfiguration; Reconfigurable computing; Self-optimizing system; Soft-core processor"
"Towards an improved Adaboost algorithmic method for computational financial analysis","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070903370&doi=10.1016%2fj.jpdc.2019.07.014&partnerID=40&md5=a4cbc6fee830d026596edc91dfb0c87c","Machine learning can process data intelligently, perform learning tasks and predict possible outputs in time series. This paper presents the use of our proposed machine learning algorithm; an Adaptive Boosting (Adaboost) algorithm, in analyzing and forecasting financial nonstationary data, and demonstrating its feasibility in financial trading. The data of future contracts are used in our analysis. The future used to test the Adaboost algorithm is a contract chosen to study future IF1711, which is combined by “HS300 index and Rb”, the deformed steel bar future in Chinese stock market. The predicted data is compared with real world data to calculate accuracy and efficiency. The Adaboost algorithm is combined with an Average True Range–Relative Strength Index (ATR–RSI) strategy, so that it can be applied in automatic trading and therefore demonstrate its practical application We develop three additional algorithms to enable optimization, large sale simulations and comparing both the predicted and actual pricing values. We performed experiments and large scale simulations to justify our work. We have tested the accuracy and validity of our approach to improve its quality. In summary, our analysis and results show that our improved Adaboost algorithms may have useful and practical implications in nonstationary data analysis. © 2019 Elsevier Inc.","Adaptive boosting algorithm; ATR–RSI strategy; Back-propagation algorithm; Machine learning; Nonstationary data; Nonstationary time series"
"A provably secure and anonymous message authentication scheme for smart grids","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039063627&doi=10.1016%2fj.jpdc.2017.11.008&partnerID=40&md5=30b3286094b451fb87153d8b89aaa349","Smart grid (SG) is an agenda of interest to governments and researchers, as a successful cyberattack against smart grids (e.g. compromise of smart meters) can have devastating real-world consequences, ranging from financial loss to fatalities. In this paper, we present a novel and secure message authentication scheme, which provides mutual authentication and key establishment for smart grid. The scheme is also designed to preserve the identities of the gateways during message transmission. We then prove the security of the scheme, as well as verifying the security properties using Proverif and demonstrating the utility of the scheme using simulations. © 2017 Elsevier Inc.","Anonymity; Formal proof; Formal verification; Key agreement; Mutual authentication; Smart grid"
"Exploiting Docker containers over Grid computing for a comprehensive study of chromatin conformation in different cell types","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071569336&doi=10.1016%2fj.jpdc.2019.08.002&partnerID=40&md5=71b42394796a466d86e840b72b9d839c","Many bioinformatic applications require to exploit the capabilities of several computational resources to effectively access and process large and distributed datasets. In this context, Grid computing has been largely used to face unprecedented challenges in Computational Biology, at the cost of complex workarounds needed to make applications successfully running. The Grid computing paradigm, in fact, has always suffered from a lack of flexibility. Although this has been partially solved by Cloud computing, the on-demand approach is way distant from the original idea of volunteering computing that boosted the Grid paradigm. A solution to outpace the impossibility of creating custom environments for running applications in Grid is represented by the containerization technology. In this paper, we describe our experience in exploiting a Docker-based approach to run in a Grid environment a novel, computationally intensive, bioinformatic application, which models the DNA spatial conformation inside the nucleus of eukaryotic cells. Results assess the feasibility of this approach in terms of performance and efforts to run large experiments. © 2019 Elsevier Inc.","Chromatin conformation; Computational Biology; Data modelling; Docker containers; Grid computing"
"Disconnected components detection and rooted shortest-path tree maintenance in networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066304924&doi=10.1016%2fj.jpdc.2019.05.006&partnerID=40&md5=17e9e47a6c6cdc888602852924608c2f","Many articles deal with the problem of maintaining a rooted shortest-path tree. However, after some edge deletions, some nodes can be disconnected from the connected component Vr of some distinguished node r. In this case, an additional objective is to ensure the detection of the disconnection by the nodes that no longer belong to Vr. We present a detailed analysis of a silent self-stabilizing algorithm. We prove that it solves this more demanding task in anonymous weighted networks with the following additional strong properties: it runs without any knowledge on the network and under the unfair daemon, that is without any assumption on the asynchronous model. Moreover, it terminates in less than 2n+D rounds for a network of n nodes and hop-diameter D. © 2019","Disconnected network; Routing algorithm; Self-stabilization; Shortest-path"
"The generalized 3-connectivity of some Regular Networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067481026&doi=10.1016%2fj.jpdc.2019.06.006&partnerID=40&md5=02bb8f7b8d495bb1cf2b338e715c6d37","For a vertex set S with cardinality at least two, we need a tree to connect them, where this tree is usually called an S-Steiner tree (or a tree connecting S). Two S-Steiner trees T and T′ are said to be internally disjoint if E(T)∩E(T′)=0̸ and V(T)∩V(T′)=S. Let κG(S) denote the maximum number r of internally disjoint S-Steiner trees in G. For an integer k with 2≤k≤n, the generalized k -connectivity of a graph G is defined as κk(G)= min{κG(S)||S⊆V(G) and |S|=k}. It is proved NP-complete to determine κk(G) for a general graph G. So far, the exact values of κk(G) are known for small classes of graphs and most of them are about k=3. In this paper, we introduce a family of m-regular and m-connected graph Gn which are constructed recursively and contains many important interconnection networks such as the alternating group graph AGn, the k-ary n-cube Qn k, the split-star network Sn 2 and the bubble-sort-star graph BSn. We study the generalized 3-connectivity of Gn and show that κ3(Gn)=m−1, which attains the upper bound of κ3(G) given by Li et al. for G=Gn. As applications, the generalized 3-connectivity of AGn, Qn k, Sn 2 and BSn etc., can be obtained directly. © 2019 Elsevier Inc.","Fault-tolerance; Generalized connectivity; Interconnection network; Regular Network"
"OpenACC acceleration for the PN–PN-2 algorithm in Nek5000","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066835225&doi=10.1016%2fj.jpdc.2019.05.010&partnerID=40&md5=e387f15ed21f5d3d7cabdb14fda195be","Due to its high performance and throughput capabilities, GPU-accelerated computing is becoming a popular technology in scientific computing, in particular using programming models such as CUDA and OpenACC. The main advantage with OpenACC is that it enables to simply port codes in their “original” form to GPU systems through compiler directives, thus allowing an incremental approach. An OpenACC implementation is applied to the CFD code Nek5000 for simulation of incompressible flows, based on the spectral-element method. The work follows up previous implementations and focuses now on the PN−PN−2 method for the spatial discretization of the Navier–Stokes equations. Performance results of the ported code show a speed-up of up to 3.1 on multi-GPU for a polynomial order N>11. © 2019 Elsevier Inc.","GPU programming; High performance computing; Nek5000; OpenACC; Spectral element method"
"Optimal placement for repair-efficient erasure codes in geo-diverse storage centres","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072850255&doi=10.1016%2fj.jpdc.2019.08.010&partnerID=40&md5=ea4ac46cbbe840217d5d6ebd1dfe280e","Erasure codes are increasingly being used by storage providers to reduce the cost of reliably storing large volumes of data. As compared to the default mechanism of triple replication, erasure codes result in optimal storage efficiency, but require significant network and disk usage during repair of failed data. The repair process is particularly complicated for storage clusters with data centres spread across a wide geographical area. For such geo-diverse clusters, the recovery performance of the code used depends more on the network throughput and latency than on the computations required for decoding. Hence, the recovery performance of most erasure codes can be improved if the surviving blocks for effecting a node repair are placed optimally. This article affirms the idea by proposing an optimization framework for placement of blocks in geo-distributed storage clusters, addressing the open problem posed by Dimakis et al. in their celebrated paper on network coding. To this end, a signomial program is formulated that yields the optimal placement minimizing the average single-block repair cost over large number of files. Though non-convex, the structure of the problem allows us to use a monomial approximation to solve the problem efficiently. MATLAB simulation results and equivalent translation to implementation with popular codes used in Hadoop storage setting are presented, that validate our framework. The idea could be applied to any coded geo-diverse storage system to achieve significant benefits in repair performance during node failures. © 2019 Elsevier Inc.","Block placement; Distributed storage; Erasure codes; Geometric optimization; Node failure and repair"
"Blockchain based privacy-preserving software updates with proof-of-delivery for Internet of Things","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067258270&doi=10.1016%2fj.jpdc.2019.06.001&partnerID=40&md5=d2d6d0440abf771ebd5118d2deee0d4f","A large number of IoT devices are connected via the Internet. However, most of these IoT devices are generally not perfect-by-design even have security weaknesses or vulnerabilities. Thus, it is essential to update these IoT devices securely, patching their vulnerabilities and protecting the safety of the involved users. Existing studies deliver secure and reliable updates based on blockchain network which serves as the transmission network. However, these approaches could compromise users privacy when updating the IoT devices. In this paper, we propose a new blockchain based privacy-preserving software update protocol, which delivers secure and reliable updates with an incentive mechanism while protects the privacy of involved users. A vendor delivers the updates and makes a commitment by using smart contract to provide financial incentive to the transmission nodes who deliver the updates to its IoT devices. A transmission node can get financial incentive by providing a proof-of-delivery. In order to obtain the proof-of-delivery, the transmission node uses double authentication preventing signature (DAPS) to carry out fair exchange. Specifically, the transmission node uses the DAPS to exchange an attribute-based signature (ABS) of one IoT device. Then, it uses the ABS as proof-of-delivery to receive financial incentives. Generally, to generate an ABS, the IoT device has to execute complex computations which is intolerable for resource limited devices. We propose a concrete outsourced attribute-based signature (OABS) scheme to overcome the weakness. Then, we prove the security of the proposed OABS and the protocol. Finally, we implement smart contract in Solidity to demonstrate the validity of the proposed protocol. © 2019 Elsevier Inc.","Attribute-based signatures; Blockchain; IoT; Privacy-preserving; Software update"
"A risk defense method based on microscopic state prediction with partial information observations in social networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065907548&doi=10.1016%2fj.jpdc.2019.04.007&partnerID=40&md5=6bea67dffb191227ad163215d6240e19","The development of network science has led to an increase in the size and user number of social networks. Messages (e.g., rumors, leaked user information) will quickly spread to social networks and lead to terrible results. Researchers have proposed a number of protection methods in risks’ propagation process, such as blocking pivotal topological nodes, controlling the bridges between social communities, etc. However, these methods mainly focus on static topological characteristics of the networks and rarely take the spatio-temporal diffusion dynamic of risks into consideration. In fact, if the selected controlled nodes or bridges are far enough away from the risk source or have already undergone the risks before, they cannot actually affect the risk propagation process at current time. To solve this problem, we propose a microscopic risk diffusion model and aim to defend against network risks and threats by predicting their dynamic propagation from the microscopic probability perspective and collecting the infection boundary nodes that are currently most likely to be contagious state. Meanwhile, in real life, we often fail to obtain the monitoring data of all network nodes, so we use the sensor observation and assume that there are some short propagation paths that are clear to us. We experimentally demonstrate that the estimations of proposed microscopic diffusion model fairly accurately predict the propagation behaviors of the network risks. Moreover, on average, the proposed risk elimination solution based on microscopic state prediction with partial observations outperforms acquaintance immunization and targeted immunization approaches in terms of defense effects and by approximately 30% in terms of defense cost. © 2019 Elsevier Inc.","Boundary users; Risk defense; Security and privacy; Sensor observation; social network threats"
"Scheduling parallel identical machines to minimize makespan:A parallel approximation algorithm","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048320594&doi=10.1016%2fj.jpdc.2018.05.008&partnerID=40&md5=3a82dce3908a9dc787e2e037a866e94e","Approximation algorithms for scheduling parallel machines have been studied for decades, leading to significant progress in terms of their approximation guarantees. The algorithms that provide near optimal performance are not feasible to use in practice due to their huge execution time requirements, thus underscoring the importance of developing efficient parallel approximation algorithms with near-optimal performance guarantees that are suitable for execution on current parallel systems, such as multi-core systems. We present the design and analysis of a parallel approximation algorithm for the problem of scheduling jobs on parallel identical machines to minimize makespan. The design of the parallel approximation algorithm is based on the best existing polynomial-time approximation scheme (PTAS) for the problem. To the best of our knowledge, this is the first practical parallel approximation algorithm for the minimum makespan scheduling problem that maintains the approximation guarantees of the sequential PTAS and it is specifically designed for execution on shared-memory parallel machines. We implement and run the algorithm on a large multi-core system and perform an extensive experimental analysis on data generated from realistic probability distributions. The results show that our proposed parallel approximation algorithm achieves significant speedup with respect to both the sequential PTAS and the CPLEX-based solver that solves the mixed integer program formulation of the problem. © 2018 Elsevier Inc.","Approximation algorithms; Parallel algorithms; Scheduling"
"Load-balancing distributed outer joins through operator decomposition","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066853800&doi=10.1016%2fj.jpdc.2019.05.008&partnerID=40&md5=4ce35d5d512c0b7cf83a3cb154dc4534","High-performance data analytics largely relies on being able to efficiently execute various distributed data operators such as distributed joins. So far, large amounts of join methods have been proposed and evaluated in parallel and distributed environments. However, most of them focus on inner joins, and there is little published work providing the detailed implementations and analysis of outer joins. In this work, we present POPI (Partial Outer join & Partial Inner join), a novel method to load-balance large parallel outer joins by decomposing them into two operations: a large outer join over data that does not present significant skew in the input and an inner join over data presenting significant skew. We present the detailed implementation of our approach and show that POPI is implementable over a variety of architectures and underlying join implementations. Moreover, our experimental evaluation over a distributed memory platform also demonstrates that the proposed method is able to improve outer join performance under varying data skew and present excellent load-balancing properties, compared to current approaches. © 2019 Elsevier Inc.","Data skew; Distributed join; Load balancing; Outer join; Spark"
"CTS: An operating system CPU scheduler to mitigate tail latency for latency-sensitive multi-threaded applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048485711&doi=10.1016%2fj.jpdc.2018.04.003&partnerID=40&md5=4c638ad6c1e3df4fcfaddd2e0bdb9984","Large-scale interactive Web services break a user's request to many sub-requests and send them to a large number of independent servers so as to consult multi-terabyte datasets instantaneously. Service responsiveness hinges on the slowest server, making the tail of the latency distribution of individual servers a matter of great concern. A large number of latency-sensitive applications hosted on individual servers use thread-driven concurrency model wherein a thread is spawned for each user connection. Threaded applications rely on the operating system CPU scheduler for determining the order of thread execution. Our experiments show that the default Linux scheduler (CFS) idiosyncrasies result in LCFS (Last Come First Served) scheduling of threads belonging to the same application. On the other hand, studies have shown that FCFS (First Come First Served) scheduling yields the lowest response time variability and tail latency, making the default scheduler of Linux a source of long tail latency for multi-threaded applications. In this paper, we present CTS, an operating system CPU scheduler to trim the tail of the latency distribution for latency-sensitive multi-threaded applications while maintaining the key characteristics of the default Linux scheduler (e.g., fairness). By adding new data structures to the Linux kernel, CTS tracks threads belonging to an application in a timely manner and schedules them in FCFS manner, mitigating the tail latency. To keep the existing features of the default Linux scheduler intact, CTS keeps CFS responsible for system-wide load balancing and core level process scheduling; CTS merely schedules threads of the CFS chosen process in FCFS order, ensuring tail latency mitigation without sacrificing the default Linux scheduler properties. Experiments with a prototype implementation of CTS in the Linux kernel demonstrate that CTS significantly outperforms the Linux default scheduler. For example, CTS mitigates the tail latency of a Null RPC server by up to 96%, a Thrift server by up to 90% and an Apache Web server by up to 51% at 99.9th percentile. © 2018 Elsevier Inc.","CPU scheduling; Linux CFS scheduler; Multi-threaded applications; Tail latency"
"Cross-state events: A new approach to parallel discrete event simulation and its speculative runtime support","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066862371&doi=10.1016%2fj.jpdc.2019.05.003&partnerID=40&md5=7f79782d16cfdfce479377b364cc8af1","We present a new approach to Parallel Discrete Event Simulation (PDES), where we enable the execution of so-called cross-state events. During their processing, the state of multiple concurrent simulation objects can be accessed in read/write mode, as opposed to classical partitioned accesses. This is done with no pre-declaration of this type of access by the programmer, hence also coping with non-determinism. In our proposal, cross-state events are supported by a speculative runtime environment fully transparently to the application code. This is done through an ad-hoc memory management architecture and an extension of the classical Time Warp synchronization protocol. This extension, named Event and Cross-State (ECS) synchronization, ensures causally-consistent speculative parallel execution of discrete event applications by allowing all events to observe the snapshot of the model execution trajectory that would have been observed in a timestamp-ordered execution of the same model. An experimental assessment of our proposal shows how it can significantly reduce the application development complexity, while also providing advantages in terms of performance. © 2019 Elsevier Inc.","Discrete event simulation; Multicore computing; Parallelization techniques; Synchronization transparency"
"Analyzing the performance/power tradeoff of the rCUDA middleware for future exascale systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066232741&doi=10.1016%2fj.jpdc.2019.04.021&partnerID=40&md5=af100f11b93d2a902f7cc089b4a73f5d","The computing power of supercomputers and data centers has noticeably grown during the last decades at the cost of an ever increasing energy demand. The need for energy (and power) of these facilities has finally limited the evolution of high performance computing, making that many researchers are concerned not only about performance but also about energy efficiency. However, despite the many concerns about energy consumption, the search for computing power continues. In this regard, the research on exascale systems, able to deliver 1018 floating point operations per second, has reached a widely consensus that these systems should operate within a maximum power budget of 20 megawatts. Many efficiency improvements are necessary for achieving this goal. One of these improvements is the usage of ARM low-power processors, as the Mont-Blanc project proposes. In this paper we analyze the combined use of ARM processors with the rCUDA remote GPU virtualization middleware as a way to improve efficiency even more. Results show that it is possible to speed up applications by almost 8x while reducing energy consumption up to 35% when rCUDA is used to access high-end GPUs. These improvements are achieved while maintaining a feasible average power consumption level for future exascale systems. © 2019 Elsevier Inc.","Energy; Exascale; GPU virtualization; HPC"
"Non-clairvoyant scheduling of independent parallel tasks on single and multiple multicore processors","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049387052&doi=10.1016%2fj.jpdc.2018.06.001&partnerID=40&md5=fd091c210d585baedd48af140dc19f2a","We investigate the problem of non-clairvoyant scheduling of independent parallel tasks on single and multiple multicore processors. For a single multicore processor, we derive an asymptotic worst-case performance bound for a non-clairvoyant offline scheduling algorithm called largest task first (LTF). The result improves our previous result on a single parallel computing system. For multiple multicore processors, we derive an asymptotic worst-case performance bound for the LTF algorithm. To the best of our knowledge, there has been little result on scheduling parallel tasks on multiple parallel computing systems. For multiple multicore processors, we also derive an asymptotic average-case performance bound for a non-clairvoyant online scheduling algorithm called random task first (RTF). The result extends our earlier result on a single parallel computing system. Extensive simulation results are also demonstrated. © 2018 Elsevier Inc.","Multicore processor; Non-clairvoyant scheduling; Online scheduling; Parallel task; Performance bound; Task scheduling"
"A distributed message-optimal assignment on rings","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066309307&doi=10.1016%2fj.jpdc.2019.05.007&partnerID=40&md5=d71aba999c0dfbf6fe21e43471c0f89f","Consider a set of items and a set of m colors, where each item is associated to one color. Consider also n computational agents connected by a ring. Each agent holds a subset of the items and items of the same color can be held by different agents. We analyze the problem of distributively assigning colors to agents in such a way that (a) each color is assigned to one agent only and (b) the number of different colors assigned to each agent is minimum. Since any color assignment requires the items be distributed according to it (e.g. all items of the same color are to be held by only one agent), we define the cost of a color assignment as the amount of items that need to be moved, given an initial allocation. We first show that any distributed algorithm for this problem requires a message complexity of Ω(n⋅m) and then we exhibit an optimal message complexity algorithm for synchronous and asynchronous rings that in polynomial time determines a color assignment with cost at most three times the optimal. We show that the approximation is tight and how to get a better cost solution at the expenses of either the message or the time complexity. Finally, we present some experiments showing that, in practice, our algorithm performs much better than the theatrical worst case scenario. © 2019 Elsevier Inc.","Algorithms; Distributed computing; Leader election; Ring"
"A generic formal model for the comparison and analysis of distributed job-scheduling algorithms in grid environment","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066152222&doi=10.1016%2fj.jpdc.2019.05.002&partnerID=40&md5=6fbd8a87142d17df437e1a112411daf6","Nowadays, high-end Systems consist of thousands of individual devices which are in general heterogeneous. Grid computing environments are an example of high-end systems which are composed of many diverse and heterogeneous resources distributed within multiple geographical areas. The performance of such systems depends considerably on job scheduling and resource allocation algorithms. Indeed, to improve the global throughput of these environments, effective and efficient load balancing algorithms are fundamentally important. In this paper, we propose a completely distributed formal model for the description of grid architecture. Our model, is then parameterized to describe different job-scheduling algorithms in a completely parallel architecture. In particular, we focus on the modeling and description of distributed load-balancing algorithms. To reach this purpose, we define a set of parallel schedulers communicating together to achieve a given load-balancing policy. To show the applicability of our approach, we propose to specify and compare different well-known job-scheduling policies in grid environments. The formal verification of different properties of the studied protocols has been performed automatically using Model-checking and a set of performance analysis results are also provided. © 2019 Elsevier Inc.","Distributed scheduling; Formal verification; Grid computing environment; Load balancing algorithms"
"Privacy-preserving range query over multi-source electronic health records in public clouds","2020","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073028035&doi=10.1016%2fj.jpdc.2019.08.011&partnerID=40&md5=5820a1a05bf6840f64533ba6f7058ee1","Range query is an important data search technique in cloud-based electronic healthcare (eHealth) systems. It enables authorized doctors to retrieve target electronic health records (EHRs) that are generated and outsourced by patients from the cloud server. In reality, patients always encrypt their EHRs before outsourcing, making the range query impossible. In this paper, we identify three threats in real cloud-based eHealth systems, i.e., privacy leakage, frequency analysis, and identical data inference. To capture the security properties that resist these threats, we define a security notion of indistinguishability under multi-source ordered chosen plaintext attack (IND-MSOCPA). Then, we propose a multi-source order-preserving encryption (MSOPE) scheme for cloud-based eHealth systems to enable range queries over encrypted EHRs from multiple patients. Security analysis proves that the MSOPE scheme is IND-MSOCPA secure. We also conduct comprehensive performance evaluations, which demonstrate the high efficiency of the MSOPE scheme. © 2019 Elsevier Inc.","Electronic health record; Order-preserving encryption; Privacy-preserving range query"
"Symmetric rendezvous with advice: How to rendezvous in a disk","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070104019&doi=10.1016%2fj.jpdc.2019.07.006&partnerID=40&md5=5a4cf04a720791659219c35c8dd42928","In the classic Symmetric Rendezvous problem on a Line (SR-LINE), two robots at known distance 2 but unknown direction execute the same randomized algorithm trying to minimize the expected rendezvous time. A long standing conjecture is that the best possible rendezvous time is 4.25 with known upper and lower bounds being very close to that value. We introduce and study a geometric variation of SR-LINE that we call Symmetric Rendezvous in a Disk (SR-DISK) where two robots at distance 2 have a common reference point at distance ρ. We show that even when ρ is not too small, the two robots can meet in expected time that is less than 4.25. Part of our contribution is that we demonstrate how to adjust known, even simple and provably non-optimal, algorithms for SR-LINE, effectively improving their performance in the presence of a reference point. Special to our algorithms for SR-DISK is that, unlike in SR-LINE, for every fixed ρ the worst case distance traveled in our algorithms is finite. In particular, we show that the worst case distance of our algorithms is Oρ2, while we also explore average–worst case tradeoffs, concluding that one may be efficient both with respect to average and worst case, with only a minor compromise on the optimal termination time. © 2019 Elsevier Inc.","Average case analysis; Disk; Rendezvous problem; Symmetric algorithms; Worst case analysis"
"A memory-distributed quasi-Newton solver for nonlinear programming problems with a small number of general constraints","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057978957&doi=10.1016%2fj.jpdc.2018.10.009&partnerID=40&md5=08a4cc68476a5f3a89c37ed9140b094e","We address the problem of parallelizing state-of-the-art nonlinear programming optimization algorithms. In particular, we focus on parallelizing quasi-Newton interior-point methods that use limited-memory secant Hessian approximations. Such interior-point methods are known to have better convergence properties and to be more effective on large-scale problems than gradient-based and derivative-free optimization algorithms. We target nonlinear and potentially nonconvex optimization problems with an arbitrary number of bound constraints and a small number of general equality and inequality constraints on the optimization variables. These problems occur for example in the form of optimal control, optimal design, and inverse problems governed by ordinary or partial differential equations, whenever they are expressed in a “reduced-space” optimization approach. We introduce and analyze the time and space complexity of a decomposition method for solving the quasi-Newton linear systems that leverages the fact that the quasi-Newton Hessian matrix has a small number of dense blocks that border a low-rank update of a diagonal matrix. This enables an efficient parallelization on memory-distributed computers of the iterations of the optimization algorithm, a state-of-the-art filter line-search interior-point algorithm by Wächter et. al. We illustrate the efficiency of the proposed method by solving structural topology optimization problems on up to 4608 cores on a parallel machine. © 2018 Elsevier Inc.","Parallel interior-point; Parallel optimization; Quasi-Newton"
"Parallelizing a multi-objective optimization approach for extractive multi-document text summarization","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072245077&doi=10.1016%2fj.jpdc.2019.09.001&partnerID=40&md5=b7ef6a7766dbc792f5c47e0a250dbaa1","Currently, automatic multi-document text summarization is an important task in many fields of knowledge, due to the continuous exponential growth of information on the Internet. Nevertheless, this task is computationally demanding. In the last years, automatic text summarization has been addressed by using multi-objective optimization approaches. In particular, recently, the Multi-Objective Artificial Bee Colony (MOABC) algorithm has obtained very good results. This work focuses on the parallelization of this approach. Several steps have been carried out for this goal. After a time profiling of the algorithm, a runtime comparison has been performed between the use of different random number generators within the algorithm. Then, a parallel implementation of the MOABC algorithm has been designed following its original scheme, in which the main steps are parallelized, and different parallel schedules have been studied and compared. Finally, a second design based on the asynchronous behavior of the bee colony in nature has been implemented and compared. Experiments have been carried out with datasets from Document Understanding Conference (DUC). The results show that the asynchronous design improves greatly the parallel design, being more than 55 times faster with 64 threads than the standard design. An efficiency of 86.72% has been reported for 64 threads. © 2019 Elsevier Inc.","Artificial bee colony; Multi-document; Multi-objective optimization; Parallel computing; Text summarization"
"ACOR: Adaptive congestion-oblivious routing in dragonfly networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065905499&doi=10.1016%2fj.jpdc.2019.04.022&partnerID=40&md5=5a331fd3d12d9279f1a7beecd2ca22bb","Low-diameter network topologies require non-minimal routing to avoid network congestion, such as Valiant routing. This increases base latency but avoids congestion issues. Optimized restricted variants focus on reducing path length. However, these optimizations only reduce paths for local traffic, where source and destination of each packet belong to the same partition of the network. This paper introduces ACOR: Adaptive Congestion-Oblivious Routing. ACOR leverages the restricted and recomputation mechanisms to reduce path length for local and global traffic, and extends it when the network conditions are adverse. ACOR relies on a sequence of misrouting policies ordered by path length. A hysteresis mechanism improves performance and avoids variability in the results. The ACOR mechanism can be combined with other non-minimal routing mechanism such as Piggyback. Results show that ACOR improves base latency in all cases, up to 28% standalone and up to 25.5% when combined with Piggyback, while requiring a simple implementation. © 2019 Elsevier Inc.","ACOR; Dragonfly network; Piggyback routing; Restricted Valiant routing; Routing Recomputation mechanism"
"IoT-CANE: A unified knowledge management system for data-centric Internet of Things application systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065831540&doi=10.1016%2fj.jpdc.2019.04.016&partnerID=40&md5=8f2cadbabd8b3a4936b2abb719f86c58","Identifying a suitable configuration of devices, software and infrastructures in the context of user requirements is fundamental to the success of delivering IoT applications. As possible configurations could be large in number and not all configurations are valid, a configuration knowledge representation model can provide ready-made configurations based on IoT requirements. Combining such a model within the context of a given user-oriented scenario, it is possible to automate the recommendation of solutions for deployment and long-time evolution of IoT applications. However, in the context of Cloud/Edge technologies, that may themselves exhibit significant configuration possibilities that are also dynamic in nature, a more unified approach is required. We present IoT-CANE (Context Aware recommendatioN systEm) as such a unified approach. IoT-CANE embodies a unified conceptual model capturing configuration, constraint and infrastructure features of Cloud/Edge together with IoT devices. The success of IoT-CANE is evaluated through an end-user case study. © 2019 Elsevier Inc.","Configuration management; Internet of Things; Knowledge representation; Recommender system; Ripple Down Rules"
"Communication-free massively distributed graph generation","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066104913&doi=10.1016%2fj.jpdc.2019.03.011&partnerID=40&md5=d74951556d0d8f427ca0f93ab539093d","Analyzing massive complex networks yields promising insights about our everyday lives. Building scalable algorithms to do so is a challenging task that requires a careful analysis and an extensive evaluation. However, engineering such algorithms is often hindered by the scarcity of publicly available datasets. Network generators serve as a tool to alleviate this problem by providing synthetic instances with controllable parameters. However, many network generators fail to provide instances on a massive scale due to their sequential nature or resource constraints. Additionally, truly scalable network generators are few and often limited in their realism. In this work, we present novel generators for a variety of network models that are frequently used as benchmarks. By making use of pseudorandomization and divide-and-conquer schemes, our generators follow a communication-free paradigm. The resulting generators are thus embarrassingly parallel and have a near optimal scaling behavior. This allows us to generate instances of up to 243 vertices and 247 edges in less than 22 min on 32768 cores. Therefore, our generators allow new graph families to be used on an unprecedented scale. © 2019 Elsevier Inc.","Communication-free; Distributed algorithms; Graph generation"
"Estimation of energy consumption in machine learning","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071032788&doi=10.1016%2fj.jpdc.2019.07.007&partnerID=40&md5=be8725ef705d83dcf045879642f8df80","Energy consumption has been widely studied in the computer architecture field for decades. While the adoption of energy as a metric in machine learning is emerging, the majority of research is still primarily focused on obtaining high levels of accuracy without any computational constraint. We believe that one of the reasons for this lack of interest is due to their lack of familiarity with approaches to evaluate energy consumption. To address this challenge, we present a review of the different approaches to estimate energy consumption in general and machine learning applications in particular. Our goal is to provide useful guidelines to the machine learning community giving them the fundamental knowledge to use and build specific energy estimation methods for machine learning algorithms. We also present the latest software tools that give energy estimation values, together with two use cases that enhance the study of energy consumption in machine learning. © 2019 The Authors","Deep learning; Energy consumption; GreenAI; High performance computing; Machine learning"
"An automatic performance model-based scheduling tool for coupled climate system models","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041723839&doi=10.1016%2fj.jpdc.2018.01.002&partnerID=40&md5=2a089a4eb11f1f2dc733c262dc11e04a","The prediction ability of the climate system is highly depended on the efficient integration of observations and simulations of the Earth, which is regarded as a canonical example of the cyber–physical system. The climate system model, the simulation engine in this cyber–physical system, is one of most challenging applications in scientific computing. It utilizes the multi-physics simulation that couples multiple components, conducts decadal to millennium simulations, and has long been an important application on supercomputers. However, current climate system models suffer from the inefficient task scheduling methods resulting in an intolerable simulation time. Take the Community Earth System Model (CESM), the most widely used climate system model, as an example, one major reason that CESM suffers from bad performances is the huge overhead to rationally distribute processes among the coupled heterogeneous components. According to the report of NCAR, every percent improvement in CESM performance frees up to the equivalent of $250,000 in computing resources in their scientific experiments. To address such challenge, our paper first constructs a lightweight and accurate performance model for effectively capturing and predicting the heterogeneous time-to-solution performance of end-to-end CESM components with a given simulation configuration. Then, based on the performance model, we further propose an efficient scheduling strategy based on rectangular packing method to determine the best process layout among different components, and the process numbers assigned to each component. Our evaluations show that we can achieve 58% average run time reductions on CESM comparing to the widely used sequential process layout for a scale of 144–480 cores on typical CPU clusters. And we can save 4 million CPU hours when we conduct one standard scientific experiment (a 2870-year simulation), which equals to save $40,089 with a charge of $0.01 per CPU hour. Meanwhile, 26% extra performance improvements also could be gained in our methods comparing to the heuristic branch and bound algorithm with the guidance of the known curve-fitting performance model. © 2018 Elsevier Inc.","Automatic tool; Cyber–Physical system; Performance model; Scheduling; Time-to-solution"
"Large-scale performance of a DSL-based multi-block structured-mesh application for Direct Numerical Simulation","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065618798&doi=10.1016%2fj.jpdc.2019.04.019&partnerID=40&md5=c610e8e8ff73fa9906d4698a1791f856","SBLI (Shock-wave/Boundary-layer Interaction) is a large-scale Computational Fluid Dynamics (CFD) application, developed over 20 years at the University of Southampton and extensively used within the UK Turbulence Consortium. It is capable of performing Direct Numerical Simulations (DNS) or Large Eddy Simulation (LES) of shock-wave/boundary-layer interaction problems over highly detailed multi-block structured mesh geometries. SBLI presents major challenges in data organization and movement that need to be overcome for continued high performance on emerging massively parallel hardware platforms. In this paper we present research in achieving this goal through the OPS embedded domain-specific language. OPS targets the domain of multi-block structured mesh applications. It provides an API embedded in C/C++ and Fortran and makes use of automatic code generation and compilation to produce executables capable of running on a range of parallel hardware systems. The core functionality of SBLI is captured using a new framework called OpenSBLI which enables a developer to declare the partial differential equations using Einstein notation and then automatically carryout discretization and generation of OPS (C/C++) API code. OPS is then used to automatically generate a wide range of parallel implementations. Using this multi-layered abstractions approach we demonstrate how new opportunities for further optimizations can be gained, such as fine-tuning the computation intensity and reducing data movement and apply them automatically. Performance results demonstrate there is no performance loss due to the high-level development strategy with OPS and OpenSBLI, with performance matching or exceeding the hand-tuned original code on all CPU nodes tested. The data movement optimizations provide over 3× speedups on CPU nodes, while GPUs provide 5× speedups over the best performing CPU node. The OPS generated parallel code also demonstrates excellent scalability on nearly 100K cores on a Cray XC30 (ARCHER at EPCC) and on over 4K GPUs on a CrayXK7 (Titan at ORNL). © 2019 Elsevier Inc.","DSLs; Finite difference methods; Multi-block structured mesh applications; OPS; SBLI"
"Improving resilience of scientific software through a domain-specific approach","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062497016&doi=10.1016%2fj.jpdc.2019.01.015&partnerID=40&md5=ff6ea5a06ec630045f7f1c3166ee4344","In this paper we present research on improving the resilience of the execution of scientific software, an increasingly important concern in High Performance Computing (HPC). We build on an existing high-level abstraction framework, the Oxford Parallel library for Structured meshes (OPS), developed for the solution of multi-block structured mesh-based applications, and implement an algorithm in the library to carry out checkpointing automatically, without the intervention of the user. The target applications are a hydrodynamics benchmark application from the Mantevo Suite, CloverLeaf 3D, the sparse linear solver proxy application TeaLeaf, and the OpenSBLI compressible Navier–Stokes direct numerical simulation (DNS) solver. We present (1) the basic algorithm that OPS relies on to determine the optimal checkpoint in terms of size and location, (2) improvements that supply additional information to improve the decision, (3) techniques that reduce the cost of writing the checkpoints to non-volatile storage, (4) a performance analysis of the developed techniques on a single workstation and on several supercomputers, including ORNL's Titan. Our results demonstrate the utility of the high-level abstractions approach in automating the checkpointing process and show that performance is comparable to, or better than the reference in all cases. © 2019","Checkpointing; Domain specific language; High performance computing; Parallel I/O; Resilience"
"Dynamic replication and migration of data objects with hot-spot and cold-spot statuses across storage data centers","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059871491&doi=10.1016%2fj.jpdc.2018.12.003&partnerID=40&md5=4d8c9a7b6e565753f36b86276a909616","Cloud Storage Providers (CSPs) offer geographically dispersed data stores providing several storage classes with different prices. A vital problem faced by application providers is how to exploit price differences across data stores to minimize monetary cost of applications that include hot-spot objects that are accessed frequently and cold-spot objects that are often accessed far less. This monetary cost consists of replica creation, storage, Put, Get, and potential migration costs. To optimize such costs, we first propose the optimal solution that leverages dynamic and linear programming techniques with the assumption that the workload on objects is known in advance. We also propose a lightweight heuristic solution, inspired from an approximate algorithm for the Set Covering Problem, which does not make any assumption on the object workload. This solution jointly determines object replicas location, object replicas migration times, and redirection of Get (read) requests to object replicas so that the monetary cost of data storage management is optimized while the user-perceived latency is satisfied. We evaluate the effectiveness of the proposed lightweight algorithm in terms of cost savings via extensive simulations using CloudSim simulator and traces from Twitter. In addition, we have built a prototype system running over Amazon Web Service (AWS) and Microsoft Azure to evaluate the duration of objects migration within and across regions. © 2018 Elsevier Inc.","Cold-spot data; Data migration; Data replication; Hot-spot data; Storage data center"
"Limiting the memory footprint when dynamically scheduling DAGs on shared-memory platforms","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061705964&doi=10.1016%2fj.jpdc.2019.01.009&partnerID=40&md5=50c4479fc6bacf5893e961e5efc238d8","Scientific workflows are frequently modeled as Directed Acyclic Graphs (DAGs) of tasks, which represent computational modules and their dependences in the form of data produced by a task and used by another one. This formulation allows the use of runtime systems which dynamically allocate tasks onto the resources of increasingly complex computing platforms. However, for some workflows, such a dynamic schedule may run out of memory by processing too many tasks simultaneously. This paper focuses on the problem of transforming such a DAG to prevent memory shortage, and concentrates on shared memory platforms. We first propose a simple model of DAGs which is expressive enough to emulate complex memory behaviors. We then exhibit a polynomial-time algorithm that computes the maximum peak memory of a DAG, that is, the maximum memory needed by any parallel schedule. We consider the problem of reducing this maximum peak memory to make it smaller than a given bound. Our solution consists in adding new fictitious edges, while trying to minimize the critical path of the graph. After proving that this problem is NP-complete, we provide an ILP solution as well as several heuristic strategies that are thoroughly compared by simulation on synthetic DAGs modeling actual computational workflows. We show that on most instances we are able to decrease the maximum peak memory at the cost of a small increase in the critical path, thus with little impact on the quality of the final parallel schedule. © 2019 Elsevier Inc.","Bounded memory; Scheduling; Task graph"
"Deadlock prevention for service orchestration via controlled Petri nets","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055739940&doi=10.1016%2fj.jpdc.2018.09.010&partnerID=40&md5=6a347fceea4d07e823f66e42c44c84f0","In multi-party Web service composition, the non-local choice mismatch is one of the most important problems. To solve it, the reachability graph-based method is widely adopted to check deadlock-freeness by generating adaptors. However, this method is inefficient due to the neglect of future deadlock states and requirements of many possible interactions. This work proposes an abnormity prevention strategy and an optimal controller for service collaboration. To overcome drawbacks of previous studies, this work describes service choreography by using service workflow nets. Moreover, this work formulates a novel reachability graph by combining structures and reachability analysis. To present abnormity, a maximally permissive state feedback control policy is then proposed. Furthermore, to avoid deadlocks in service orchestration, this work constructs an optimal controller for administrators of service composition. Finally, experiments demonstrate the advantage of the proposed method via a realistic example. © 2018 Elsevier Inc.","Compatibility enforcement; Controlled Petri net; Deadlock prevention; Service orchestration"
"Statistically managing cloud operations for latency-tail-tolerance in IoT-enabled smart cities","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043993850&doi=10.1016%2fj.jpdc.2018.02.016&partnerID=40&md5=a75183b3980decb5083d1f08f215333f","Smart City is typically large scale and IoT-enabled online services for huge amount of streaming data generated by sensors, and the services are often deployed in clouds, where infrastructure and services need to be maintained and optimised to achieve the best quality of service by using various cloud operations that are running in the background of Smart City business. However, on one hand the background operations inevitably press a negative impact on the latency of normal requests to the services, while on the other hand to maintain the short latency of requests usually results in unreasonably long latency of background operations. In this paper, on the architecture of IoT-enabled Smart City services deployed in clouds, our motivation is to find the best management policy of operations for normal request traffic, which is stable and stationary and to which operations are inserted. We focus on the system environment in which administrators are dealing with in most time other than discuss the special cases, such as outages or rush hours, and take the assumptions e.g. predictability and periodicity. The suitable management techniques for us are a class of online management policies, which are simple and applicable in practice. These policies harmonise the requests and the operations by giving higher priority to the normal requests other than forcing the background operations to wait unreasonably. We also propose a statistical policy, which is asymptotically optimal in the class. In order to compute the numeric solution, we employ a queueing model to analyse our policy. The evaluation indicates that our technique can effectively maintain the short latency of the requests, and meanwhile the latency of operations is not unreasonably long in comparison to the competitors. © 2018 Elsevier Inc.","Asymptotical optimality; Cloud operation; IoT; Latency; Smart city; Statistical policies"
"Big vs little core for energy-efficient Hadoop computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046135916&doi=10.1016%2fj.jpdc.2018.02.017&partnerID=40&md5=ea95c0400f145bf6928f91862b714fda","Emerging big data applications require a significant amount of server computational power. However, the rapid growth in the data yields challenges to process them efficiently using current high-performance server architectures. Furthermore, physical design constraints, such as power and density, have become the dominant limiting factor for scaling out servers. Heterogeneous architectures that combine big Xeon cores with little Atom cores have emerged as a promising solution to enhance energy-efficiency by allowing each application to run on an architecture that matches resource needs more closely than a one-size-fits-all architecture. Therefore, the question of whether to map the application to big Xeon or little Atom in heterogeneous server architecture becomes important. In this paper, through a comprehensive system level analysis, we first characterize Hadoop-based MapReduce applications on big Xeon and little Atom-based server architectures to understand how the choice of big vs little cores is affected by various parameters at application, system and architecture levels and the interplay among these parameters. Second, we study how the choice between big and little core changes across various phases of MapReduce tasks. Furthermore, we show how the choice of most efficient core for a particular MapReduce phase changes in the presence of accelerators. The characterization analysis helps guiding scheduling decisions in future cloud-computing environment equipped with heterogeneous multicore architectures and accelerators. We have also evaluated the operational and the capital cost to understand how performance, power and area constraints for big data analytics affect the choice of big vs little core server as a more cost and energy efficient architecture. © 2018 Elsevier Inc.","Big and little cores; Energy and cost efficiency; Hadoop; Heterogeneous architectures; MapReduce; Scheduling"
"Performance portability study for massively parallel computational fluid dynamics application on scalable heterogeneous architectures","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063080601&doi=10.1016%2fj.jpdc.2019.02.005&partnerID=40&md5=daf1b6a92ec3066c493c9c2df560ed43","Patient-specific hemodynamic simulations have the potential to greatly improve both the diagnosis and treatment of a variety of vascular diseases. Portability will enable wider adoption of computational fluid dynamics (CFD) applications in the biomedical research community and targeting to platforms ideally suited to different vascular regions. In this work, we present a case study in performance portability that assesses (1) the ease of porting an MPI application optimized for one specific architecture to new platforms using variants of hybrid MPI＋X programming models; (2) performance portability seen when simulating blood flow in three different vascular regions on diverse heterogeneous architectures; (3) model-based performance prediction for future architectures; and (4) performance scaling of the hybrid MPI＋X programming on parallel heterogeneous systems. We discuss the lessons learned in porting HARVEY, a massively parallel CFD application, from traditional multicore CPUs to diverse heterogeneous architectures ranging from NVIDIA/AMD GPUs to Intel MICs and Altera FPGAs. © 2019 Elsevier Inc.","Computational fluid dynamics; Heterogeneous architectures; Lattice Boltzmann method; OpenACC; Patient-specific hemodynamics; Performance portability; Performance prediction"
"Anti-spoofing cloud-based multi-spectral biometric identification system for enterprise security and privacy-preservation","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056209866&doi=10.1016%2fj.jpdc.2018.10.005&partnerID=40&md5=71a07747413f21485f3f34cf35351032","Nowadays, cloud computing has provided enterprises and users with several capabilities to process and store their data in various cloud data centers. Storing and processing these sensitive data with better protection and management are a big challenge. Therefore, there is a need for maintaining the confidentiality and integrity of data in the cloud without any information leakage. Recently, biometric recognition has achieved significant advancements in the identification of individuals for the purpose of privacy-preservation in the cloud computing. Only few works have used a face as a typical biometric trait for cloud and cross-enterprise identification in the last recent years. However, current cloud-based biometric identification systems and approaches have some limitations such as noisy data, inter and intra class variations, high time cost, inaccurate, non-universality and spoofing attack. This paper proposes a new anti-spoof multispectral biometric cloud-based identification approach for privacy and security of cloud computing. The approach offers the solution using multi-spectral palmprint as a typical biometric trait between two main phases: offline enrollment phase and online identification phase. This work is considered the first approach of privacy-preservation in cloud computing using encrypted multi-spectral palmprint features without any information leakage and disclosure possibility. The experimental results show that the proposed approach can accurately and efficiently provide the privacy and security of cloud data. © 2018 Elsevier Inc.","Cloud computing; Enterprise security and privacy-preservation; Multi-spectral biometric identification; Palmprint biometric trait; RSA algorithm"
"CBase: Fast Virtual Machine storage data migration with a new data center structure","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055877793&doi=10.1016%2fj.jpdc.2018.10.001&partnerID=40&md5=b2f0a38a235884af92b12a30541a298f","Live Virtual Machine (VM) migration within a data center is an important technology for cloud management, and has brought many benefits to both cloud providers and users. With the development of cloud computing, across-data-center VM migration is also desired. Normally, there is no shared storage system between data centers, hence the storage data (disk image) of a VM will be migrated to the destination data center as well. However, the slow network speed of the Internet and the comparatively large size of VM disk image make VM storage data migration become a bottleneck for live VM migration across data centers. In this paper, based on a detailed analysis of VM deployment models and the nature of VM image data, we design and implement a new migration system, called CBase. The key concept of CBase is a newly introduced central base image repository for reliable and efficient data sharing between VMs and data centers. With this central repository, further performance optimizations to VM storage data migration are made possible. Two migration mechanisms (data deduplication and Peer-to-Peer (P2P) file sharing) are utilized to accelerate base image migration, and a strategy is designed to elevate the synchronization of newly-written disk blocks. The results from an extensive experiment show that CBase significantly outperforms existing migration mechanisms under different conditions regarding total migration time and total network traffic. In particular, CBase with data deduplication is better than P2P file sharing for base image migration in our experimental environment. © 2018 Elsevier Inc.","Cloud computing; Data center; Data deduplication; Live VM migration; P2P file sharing; Storage data migration"
"Communicating with beeps","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064322708&doi=10.1016%2fj.jpdc.2019.03.020&partnerID=40&md5=1d9119f30be6554312157df59bbb5646","The beep model is a very weak communications model in which devices in a network can communicate only via beeps and silence. As a result of its weak assumptions, it has broad applicability to many different implementations of communications networks. This comes at the cost of a restrictive environment for algorithm design. Despite being only recently introduced, the beep model has received considerable attention, in part due to its relationship with other communication models such as that of ad-hoc radio networks. However, there has been no definitive published result for several fundamental tasks in the model. We aim to rectify this with our paper. We present algorithms and lower bounds for a variety of fundamental global communications tasks in the model. © 2019 Elsevier Inc.","Beep model; Communication protocols; Wireless networks"
"Parallel multi-core and multi-processor methods on point-value multiresolution algorithms for hyperbolic conservation laws","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055048687&doi=10.1016%2fj.jpdc.2018.09.016&partnerID=40&md5=11e5f89ec0df0c22fa10ae7fdb2b3df6","The underlying sequential behavior of the multiresolution (MR) method has been exploited for parallel computing by introducing a concept of multiresolution forest structures (MFS) along with two new load-balancing algorithms. Another easy-to-implement multithreading approach has also been introduced for the multicore architectures. Tests were conducted using an Euler solver based on a fifth-order shock capturing WENO scheme and a third-order Runge–Kutta algorithm. The methods have been rigorously analyzed in terms of speedup ratio and parallel efficiency to bring forth their benefits as well as limitations. The performance yielded through these methods indicates that the MFS is a new headway for the MR method in parallel computing that has a potential to harness better scalability. © 2018 Elsevier Inc.","Finite differences; Load balancing algorithms; Multiresolution method; Parallel computing"
"A comparative analysis of adaptive consistency approaches in cloud storage","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063610408&doi=10.1016%2fj.jpdc.2019.03.006&partnerID=40&md5=beb264cd80b76e37d5071abe1d252ab2","NoSQL storage systems are used extensively by web applications and provide an attractive alternative to conventional databases due to their high security and availability with a low cost. High data availability is achieved by replicating data in different servers in order to reduce access time lag, network bandwidth consumption and system unreliability. Hence, the data consistency is a major challenge in distributed systems. In this context, strong consistency guarantees data freshness but affects directly the performance and availability of the system. In contrast, weaker consistency enhances availability and performance but increases data staleness. Therefore, an adaptive consistency strategy is needed to tune, during runtime, the consistency level depending on the criticality of the requests or data items. Although there is a rich literature on adaptive consistency approaches in cloud storage, there is a need to classify as well as regroup the approaches based on their strategies. This paper will establish a set of comparative criteria and then make a comparative analysis of existing adaptive consistency approaches. A survey of this kind not only provides the user/researcher with a comparative performance analysis of the approaches but also clarifies the suitability of these for candidate cloud systems. © 2019 Elsevier Inc.","Adaptive consistency; Adaptive policy; Big data; Cloud storage"
"ReD: A reuse detector for content selection in exclusive shared last-level caches","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058186283&doi=10.1016%2fj.jpdc.2018.11.005&partnerID=40&md5=cc869f0b27028837c90f3bb9c7d62b61","The reference stream reaching a chip multiprocessor Shared Last-Level Cache (SLLC) shows poor temporal locality, making conventional cache management policies inefficient. Few proposals address this problem for exclusive caches. In this paper, we propose the Reuse Detector (ReD), a new content selection mechanism for exclusive hierarchies that leverages reuse locality at the SLLC, a property that states that blocks referenced more than once are more likely to be accessed in the near future. Being placed between each L2 private cache and the SLLC, ReD prevents the insertion of blocks without reuse into the SLLC. It is designed to overcome problems affecting similar recent mechanisms (low accuracy, reduced visibility window and detector thrashing). ReD improves performance over other state-of-the-art proposals (CHAR, Reuse Cache and EAF cache). Compared with the baseline system with no content selection, it reduces the SLLC miss rate (MPI) by 10.1% and increases harmonic IPC by 9.5%. © 2018 Elsevier Inc.","Bypass; Chip multiprocessor; Content selection; Exclusion; Reuse; Shared last-level cache"
"Performance considerations for scalable parallel tensor decomposition","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034565209&doi=10.1016%2fj.jpdc.2017.10.013&partnerID=40&md5=4785e01d9f5c5cbc59d3d8731acf5f5c","Tensor decomposition, the higher-order analogue to singular value decomposition, has emerged as a useful tool for finding relationships in large, sparse, multidimensional data. As this technique matures and is applied to increasingly larger data sets, the need for high performance implementations becomes critical. A better understanding of the performance characteristics of tensor decomposition on large and sparse tensors can help drive the development of such implementations. In this work, we perform an objective empirical evaluation of three state of the art parallel tools that implement the Canonical Decomposition/Parallel Factorization tensor decomposition algorithm using alternating least squares fitting (CP-ALS): SPLATT, DFacTo, and ENSIGN. We conduct performance studies across a variety of data sets and evaluate the tools with respect to total memory required, processor stall cycles, execution time, data distribution, and communication patterns. Furthermore, we investigate the performance of the implementations on tensors with up to 6 dimensions and when executing high rank decompositions. We find that tensor data structure layout and distribution choices can result in differences as large as 14.6x with respect to memory usage and 39.17x with respect to execution time. We provide an outline of a distributed heterogeneous CP-ALS implementation that addresses the performance issues we observe. © 2017","Alternating least squares; Canonical decomposition/parallel factorization; Decomposition; Parallel; Performance analysis; Sparse tensors"
"Multi-level multi-secret sharing scheme for decentralized e-voting in cloud computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064317363&doi=10.1016%2fj.jpdc.2019.04.003&partnerID=40&md5=cf2b9feb9c5997c5ee0d1b8b111d9165","The cryptosystem-based data privacy preserving methods employ high computing power of cloud servers, where the main feature is to allow resource sharing and provide multi-user independent services. Therefore, to achieve the rapid allocation and release of resource sharing in cloud computing, decentralized cryptographic protocols need to be proposed for multi-user consensus systems. In this work, we first present a multi-secret sharing scheme with multi-level access structure, where the secret reconstruction algorithm satisfies the additive homomorphism. The secret sharing scheme needs no trusted third parties and any user can play the role of dealer. In the designing, multiple target secrets are independently shared, where each subset of users forms a sub-access structure and shares one target secret only with a short secret share. This scheme is efficient and unconditionally secure. Furthermore, based on the multi-level access structures, a decentralized multi-role e-voting protocol is designed using Chinese Remainder Theorem, where each role's election is associated with one sub-access structure. The voters employ a shared parameter to blind the sum of ballot values. Meanwhile, the e-voting scheme supports a public verification for the final election results. Compared with the existing e-voting protocols, our e-voting system does not require any authority center and the cloud server runs vote counting. And our e-voting scheme does not need any high-complexity computational cost operation such as module exponential operation, etc. Finally, the common feature of Blockchain and Ad Hoc networks is decentralized. Thus the main idea of this protocol without a trusted third party can be used to achieve a secure consensus among multiple nodes in Blockchain and Ad Hoc network, meanwhile, the consensus results can be verified. © 2019 Elsevier Inc.","Cloud computing; Decentralized system; Multi-role e-voting; Multi-secret sharing"
"Efficient parallel optimizations of a high-performance SIFT on GPUs","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056815706&doi=10.1016%2fj.jpdc.2018.10.012&partnerID=40&md5=6f7d38daaa77c9f9bf86a21633e8f0c5","Stable local image feature detection is a fundamental problem in computer vision and is critical for obtaining the corresponding interest points among images. As a popular and robust feature extraction algorithm, the scale invariant feature transform (SIFT) is widely used in various domains, such as image stitching and remote sensing image registration. However, the computational complexity of SIFT is extremely high, which limits its application in real-time systems and large-scale data processing tasks. Thus, we propose several efficient optimizations to realize a high-performance SIFT (HartSift) by exploiting the computing resources of CPUs and GPUs in a heterogeneous machine. Our experimental results show that HartSift processes an image within 3.07∼7.71 ms, which is 55.88∼121.99 times, 5.17∼6.88 times, and 1.25∼1.79 times faster than OpenCV SIFT, SiftGPU, and CudaSift, respectively. © 2018 Elsevier Inc.","Feature extraction; GPU; HartSift; High performance; SIFT"
"Joint optimization of data placement and scheduling for improving user experience in edge computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058015077&doi=10.1016%2fj.jpdc.2018.11.006&partnerID=40&md5=93910dc575e7117c6efd5e883e0f1765","In recent years, edge computing becomes an increasingly popular alternative. Edge computing allows the computation is implemented in the edge of network, in which the data are stored in the edge of network, to improve the efficiency of data process. However, some resource management techniques in cloud or distributed system cannot better suit for edge computing. Therefore, there exist some challenges on the performance improvement of edge computing. In this paper, the main purpose is to combine the optimal placement of data blocks and the optimal scheduling of tasks to reduce the computation delay and response time for the submitted tasks and improve user experience in edge computing. In optimal placement of data blocks, the value of the data blocks considers not only the popularity of the data blocks, but the data storage capacity and replacement ratios of an edge server that will store those data blocks. Furthermore, the replacement cost for placed data blocks is regarded as an important component of data block placement. This optimal placement scheme can avoid replacing the placed data blocks repeatedly so that the bandwidth overhead is reduced. In optimal scheduling of tasks, the containers are taken as the lightweight resource unit for the services for user requests to make full use of data storage in edge servers and improve the services performance of edge servers. Finally, extensive experiments are conducted to value the performance of task scheduling strategy. The results show that the performance of the proposed task scheduling algorithm is better than that of the compared algorithms. © 2018 Elsevier Inc.","Data placement; Edge computing; Task scheduling; User experience"
"A fault-tolerant last level cache for CMPs operating at ultra-low voltage","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057799088&doi=10.1016%2fj.jpdc.2018.10.010&partnerID=40&md5=5276e215261f0386c889a87483be10a7","Voltage scaling to values near the threshold voltage is a promising technique to hold off the many-core power wall. However, as voltage decreases, some SRAM cells are unable to operate reliably and show a behavior consistent with a hard fault. Block disabling is a micro-architectural technique that allows low-voltage operation by deactivating faulty cache entries, at the expense of reducing the effective cache capacity. In the case of the last-level cache, this capacity reduction leads to an increase in off-chip memory accesses, diminishing the overall energy benefit of reducing the voltage supply. In this work, we exploit the reuse locality and the intrinsic redundancy of multi-level inclusive hierarchies to enhance the performance of block disabling with negligible cost. The proposed fault-aware last-level cache management policy maps critical blocks, those not present in private caches and with a higher probability of being reused, to active cache entries. Our evaluation shows that this fault-aware management results in up to 37.3% and 54.2% fewer misses per kilo instruction (MPKI) than block disabling for multiprogrammed and parallel workloads, respectively. This translates to performance enhancements of up to 13% and 34.6% for multiprogrammed and parallel workloads, respectively. © 2018 Elsevier Inc.","Cache management; Fault-tolerance; Near-threshold voltage; On-chip caches; SRAM reliability"
"An algorithm for computing short-range forces in molecular dynamics simulations with non-uniform particle densities","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063869179&doi=10.1016%2fj.jpdc.2019.03.008&partnerID=40&md5=dfda322c1d2a9a687019bfaef2a6bffa","We present projection sorting, an algorithmic approach to determining pairwise short-range forces between particles in molecular dynamics simulations. We show it can be more effective than the standard approaches when particle density is non-uniform. We implement tuned versions of the algorithm in the context of a biophysical simulation of chromosome condensation, for the modern Intel Broadwell and Knights Landing architectures, across multiple nodes. We demonstrate up to 5× overall speedup and good scaling to large problem sizes and processor counts. © 2019 The Author(s)","Algorithms; ARCHER; Many-core; Molecular dynamics; MPI; Simulation"
"Parallel cosine nearest neighbor graph construction","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039453366&doi=10.1016%2fj.jpdc.2017.11.016&partnerID=40&md5=adbea479a49bcf755765e7b565833205","                             The nearest neighbor graph is an important structure in many data mining methods for clustering, advertising, recommender systems, and outlier detection. Constructing the graph requires computing up to n                             2                              similarities for a set of n objects. This high complexity has led researchers to seek approximate methods, which find many but not all of the nearest neighbors. In contrast, we leverage shared memory parallelism and recent advances in similarity joins to solve the problem exactly. Our method considers all pairs of potential neighbors but quickly filters pairs that could not be a part of the nearest neighbor graph, based on similarity upper bound estimates. The filtering is data dependent and not easily predicted, which poses load balance challenges in parallel execution. We evaluated our methods on several real-world datasets and found they work up to two orders of magnitude faster than existing methods, display linear strong scaling characteristics, and incur less than 1% load imbalance during filtering.                          © 2017 Elsevier Inc.","All-pairs; Bounded similarity graph; Cosine similarity; Nearest neighbors; Neighborhood graph construction; Shared memory parallel; Similarity search"
"Two-dimensional batch linear programming on the GPU","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060343851&doi=10.1016%2fj.jpdc.2019.01.001&partnerID=40&md5=093ddee18805dc0236858e9fa1fd9934","This paper presents a novel, high-performance, graphical processing unit-based algorithm for efficiently solving two-dimensional linear programs in batches. The domain of two-dimensional linear programs is particularly useful due to the prevalence of relevant geometric problems. Batch linear programming refers to solving numerous different linear programs within one operation. By solving many linear programs simultaneously and distributing workload evenly across threads, graphical processing unit utilization can be maximized. Speedups of over 22 times and 63 times are obtained against state-of-the-art graphics processing unit and CPU linear program solvers, respectively. © 2019","Cooperative thread array; GPU-computing; Graphics processing unit; Incremental linear programming"
"Understanding the latency distribution of cloud object storage systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062065626&doi=10.1016%2fj.jpdc.2019.01.008&partnerID=40&md5=ff6e1b63f4c51030d669e8031ecefdc8","As a fundamental cloud service, the cloud object storage system stores and retrieves millions or even billions of read-heavy data objects. Serving for a massive amount of requests each day makes the response latency be a vital component of user experiences. Timeout is also a key issue as it has a great impact on the response latency. Due to the lack of suitable understanding on the distribution of the response latency and the occurrence of timeouts, current practice is to use overprovision resources to meet a Service Level Agreement (SLA) on response latency. Hence, firstly, we build a performance model for the cloud object storage system, which assumes no timeout occurring. Our model predicts the percentage of requests meeting an SLA, in the context of complicated disk operations, event-driven programming model and requests waiting for being accept-ed. Secondly, we propose a method that determines whether or not our model is applicable by predicting the occurrence of timeouts. We evaluate our model with a production system using a real-world trace. In a variety of scenarios, our model reduces the prediction errors by up to 90% compared with baseline models, and its overall average error is 2.63%. Moreover, we could also accurately predict the applicability of our model. © 2019 Elsevier Inc.","Cloud object storage; Latency distribution; Performance modeling; Queueing theory"
"Efficient and secure multi-dimensional geometric range query over encrypted data in cloud","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065442941&doi=10.1016%2fj.jpdc.2019.04.015&partnerID=40&md5=224af5155d91d123ff7c0f5d6fe6b851","Secure geometric range query, which aims to retrieve data points within a given geometric range from an encrypted dataset in the cloud, attracts more and more attention due to its wide applications. Up to now, several secure geometric range query schemes have been put forward. However, the existing schemes still suffer from various disadvantages, such as they are of low efficiency, cannot support multi-dimensional data and general range query, or even have security flaws. In this paper, we study secure geometric range query on encrypted dataset in cloud. First, we show the security problem of the state-of-the-art scheme by proposing an efficient attack method. Then, we propose a new secure solution for general multi-dimensional range query, which is secure under known-background model, and leverage R-tree index to achieve sub-linear search efficiency. Finally, through theoretical analysis and extensive experiments, we demonstrate the effectiveness and efficiency of our proposed approaches. © 2019 Elsevier Inc.","Cloud computing; Index; Privacy; Range query"
"GPU-based acceleration of the Linear Complexity Test for random number generator testing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062856680&doi=10.1016%2fj.jpdc.2019.01.011&partnerID=40&md5=f649e972ab0fd9f3e34ca831310d7667","The Linear Complexity Test is a statistical test for verifying the randomness of a binary sequence produced by a random number generator (RNG). It is the most time-consuming test in the widely used randomness testing suite that was published by the National Institute of Standards and Technology (NIST). The slow performance of the original Linear Complexity Test implementation is one of the major hurdles in the RNG testing process. In this work, we present a parallelized implementation of the Linear Complexity Test for GPU computation. We incorporate two levels of parallelism and various design optimization approaches to accelerate the test execution on modern GPU architectures. To further enhance the performance, we also create a hybrid computation approach that uses both CPU and GPU simultaneously. We achieve a speedup of more than 4000 times over the original Linear Complexity Test implementation from NIST (27 times over the previous best implementation of the test). © 2019 Elsevier Inc.","Berlekamp–Massey algorithm; GPU; Linear complexity; Parallelization; Random number generator; Randomness"
"Evaluating distributed IoT databases for edge/cloud platforms using the analytic hierarchy process","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055966425&doi=10.1016%2fj.jpdc.2018.10.008&partnerID=40&md5=e1bc54aadcc0b47dbb8de043d5896e52","Decision-making is not a trivial process. It involves studying and analyzing different alternatives. In addition, it requires defining criteria for evaluating the alternatives. A problem arises when evaluating criteria that conflict or when dealing with qualitative criteria. The analytic hierarchy process (AHP) is a multi-criteria decision tool that simplifies the decision-making process. It can evaluate both qualitative and quantitative criteria. Moreover, AHP justifies the final decision by providing the mathematical reasoning behind the judgment. The aim of this research is to evaluate available Internet of Things (IoT) databases in an edge/cloud platform by applying AHP and to suggest a suitable approach for developing a database application. In this study, four alternative database development tools are evaluated: DaDaBIK, DataFlex, Oracle Application Express, and FileMaker. We define our criteria, explain why they are selected, and assign each a weight based on its importance. We then evaluate the candidates using the weighted criteria. FileMaker is found to be the best choice because it offers the best usability, portability, and supportability for IoT scenarios. © 2018 Elsevier Inc.","Analytical hierarchy process; Distributed IoT database; Edge/cloud; Evaluation criteria"
"An open modal stabilizing m-wave algorithm for arbitrary networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061446999&doi=10.1016%2fj.jpdc.2019.01.014&partnerID=40&md5=769c7bb1cacf300762d01448ef6b0c6a","A system that operates in multiple input and corresponding operationalmodes depending on external input is referred to as an open modal (multi-mode) system. An input mode refers to a particular type and pattern of input whereas an operational mode refers to the operational semantics or the specification implemented by an algorithm. An open modal stabilizing system can operate in multiple operational modes and, eventually enters an operational mode depending on its current input mode. Such a system can be viewed as a generalization of the closed single-mode stabilizing systems that do not receive external input and implement a single operational mode upon entering a legitimate configuration. A wave is a distributed execution, often made up of a broadcast phase followed by a feedback phase, requiring the participation of all the system processors before a particular event called decision is taken. Wave algorithms with one initiator such as the 1-wave algorithm (Bui et al., 1999) have been shown to be very efficient for broadcasting messages in networks. However as the network size increases, having a single initiator adversely affects the message delivery times to nodes further away from the initiator. As a remedy, broadcast waves can be initiated by multiple initiator nodes forming a collection of waves covering the entire network to reduce the completion times of broadcasts. Solutions to global-snapshots, distributed broadcast and various synchronization problems can be solved efficiently using waves with multiple concurrent initiators (Prakash and Singhal, 1994). In this paper, we propose the first open modal stabilizing m-wave (multi-initiator wave) algorithm implementing concurrent waves started by multiple initiator processors. An m-wave is an execution in which a (non-empty) set of processors initiate broadcasts such that each processor in the network receives a broadcast that was initiated in the current m-wave. A broadcast is said to be initiated by a processor, if the processor has received an input and started the broadcast by informing others of this event. Since the algorithm is a modal algorithm, it is capable of operating in multiple operational and input modes, where the operational mode is determined by the input mode. Due to being stabilizing, the algorithm eventually behaves according to its specification, that is, after a delay and possible attempts to complete waves, every initiated m-wave is completed as per its specification. In addition, after stabilization, if the input mode changes, the system remains in a legitimate configuration, however, enters a new operational mode after a delay in which the corresponding broadcast semantics is implemented. © 2019","m-wave algorithms; Modal systems; Multi-node broadcast; Open systems; Stabilization; Wave algorithms"
"Hardware Transactional Memory meets memory persistency","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064324613&doi=10.1016%2fj.jpdc.2019.03.009&partnerID=40&md5=0b4ccf575707617ccda690bcd21ce186","Persistent Memory (PM) and Hardware Transactional Memory (HTM) are two recent architectural developments whose joint usage promises to drastically accelerate the performance of concurrent, data-intensive applications. Unfortunately, combining these two mechanisms using existing architectural supports is far from being trivial. This paper presents NV-HTM, a system that allows the execution of transactions over PM using unmodified commodity HTM implementations. NV-HTM exploits a hardware–software co-design technique, which is based on three key ideas: (i) relying on software to persist transactional modifications after they have been committed via HTM; (ii) postponingthe externalization of commit events to applications until it is ensured, via software, that any data version produced and observed by committed transactions is first logged in PM; (ii) pruning the commit logs via checkpointing schemes that not only bound the log space and recovery time, but also implement wear leveling techniques to enhance PM's endurance. By means of an extensive experimental evaluation, we show that NV-HTM can achieve up to 10× speed-ups and up to 11.6× reduced flush operations with respect to state of the art solutions, which, unlike NV-HTM, require custom modifications to existing HTM systems. © 2019 Elsevier Inc.","Hardware; Memory; Persistent; System; Transaction"
"Design and automation of VLSI architectures for bidirectional scan based fault localization approach in FPGA fabric aware cellular automata topologies","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064404046&doi=10.1016%2fj.jpdc.2019.03.021&partnerID=40&md5=bc73c50c46e5a4fe3831ff8b73266102","Cellular automata (CA) have received significant attention in VLSI design for the inherent architectural advantages of modularity, cascadability, simplicity and localized interconnections. In this paper, we have designed FPGA fabric aware CA circuit topologies with a built-in bidirectional scan chain to facilitate fine-grained fault localization of any faulty logic element configured for circuit realization, without increase in logic resources or critical path delay. The scan path arrangement may also be used for seeding the CA with the desired initial state. The generation of circuit description files has been completely automated which further facilitates to single out the exact faulty logic element (if any) on which the circuit has been configured. The proposed architectures outperform the state-of-the-art error detection and fault localization techniques tailored for FPGA implementations both in terms of area and speed. © 2019 Elsevier Inc.","Bidirectional scan; Cellular automata; Design automation; Fault localization; FPGA; Placement; Primitive instantiation"
"Data-flow analysis and optimization for data coherence in heterogeneous architectures","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064391437&doi=10.1016%2fj.jpdc.2019.04.004&partnerID=40&md5=09e8bce827663bcd2f368abd2145f4a0","Although heterogeneous computing has enabled developers to achieve impressive program speed-ups, the cost of moving and keeping data coherent between host and device may easily eliminate any performance gains achieved by acceleration. To deal with this problem, this paper introduces DCA: a pair of two data-flow analyses that determine how variables are used by host/device at each program point. It also introduces DCO, a code optimization technique that uses DCA information to: (a) allocate OpenCL shared buffers between host and devices; and (b) insert appropriate OpenCL function calls into program points so as to minimize the number of data coherence operations. We have used the AClang compiler to measure the impact of DCA and DCO when generating code from Parboil, Polybench and Rodinia benchmarks for a set of discrete/integrated GPUs. The experimental results showed speed-ups of up to 5.25x (average of 1.39x) on an ARM Mali-T880 and up to 8.87x (average of 1.66x) on an NVIDIA GPU Pascal Titan X. © 2019 Elsevier Inc.","Compilers; Data coherence; Heterogeneous architectures"
"Exploiting multi-core and many-core architectures for efficient simulation of biologically realistic models of Golgi cells","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059472429&doi=10.1016%2fj.jpdc.2018.12.004&partnerID=40&md5=8c15c2c1dddcc58b2a0c53798896b0f9","Realistic neuronal activity simulation is of central importance for neuroscientists. These simulations allow to test new drugs, to study cerebral pathologies and to discover innovative therapies undertaking in silico experiments instead of in vivo ones. However, the processing times needed to simulate these models are very long. Therefore, high performance computing technologies should be explored in order to provide faster simulations. In this work, authors described high performant and realistic simulations of Golgi cells activity, based on the multi-core and the many-core approaches. Thus, simulations are performed on multi-core Intel processors and on NVIDIA Graphics Processing Units. Moreover, authors addresses the issue of portability among heterogeneous devices by proposing a solution based on OpenCL paradigm. The obtained results show that the considered parallel technologies, in particular the GPUs, are suitable for that kind of simulations and significantly reduce processing times. © 2018 Elsevier Inc.","Biological system modeling; Brain modeling; Cerebellar neuron simulation; Graphics processing units; Multi-core processors; Parallel simulations"
"Gathering of mobile robots with weak multiplicity detection in presence of crash-faults","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054909910&doi=10.1016%2fj.jpdc.2018.09.015&partnerID=40&md5=190ce0a8eb758885c9bc9ae84a7adf78","We study the Gathering problem for mobile robots in presence of faults. In classical gathering, robots gather at a point not known a priori in finite time. In this paper, we focus on gathering of all non-faulty robots at a single point in the presence of faulty robots. We propose a wait-free algorithm (i.e., no robot waits for other robot and the algorithm instructs each robot to move in every step, unless it is already at the gathering location), that gathers all non-faulty robots in the semi-synchronous model without any agreement in the coordinate system and with weak multiplicity detection (i.e., a robot can only detect that either there is one or more robots at a location) in the presence of at most [Formula presented] faulty robots for [Formula presented]. We show that the required capability for gathering robots is minimal in the above model since relaxing it further makes gathering impossible to solve. Also, we introduce a scheduling model as the asynchronous model with instantaneous computation (ASYNC[Formula presented]), which lies in between the asynchronous and the semi-synchronous model. Then we propose another algorithm in the ASYNC[Formula presented] model for gathering all non-faulty robots with weak multiplicity detection and without any agreement on the coordinate system in the presence of at most [Formula presented] faulty robots for [Formula presented]. © 2018 Elsevier Inc.","Distributed algorithms; Fault-tolerance; Gathering; Oblivious mobile robots"
"Edge server placement in mobile edge computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049827230&doi=10.1016%2fj.jpdc.2018.06.008&partnerID=40&md5=671c9ad77247216af11ecec0ba9c7c01","With the rapid increase in the development of the Internet of Things and 5G networks in the smart city context, a large amount of data (i.e., big data) is expected to be generated, resulting in increased latency for the traditional cloud computing paradigm. To reduce the latency, mobile edge computing has been considered for offloading a part of the workload from mobile devices to nearby edge servers that have sufficient computation resources. Although there has been significant research in the field of mobile edge computing, little attention has been given to understanding the placement of edge servers in smart cities to optimize the mobile edge computing network performance. In this paper, we study the edge server placement problem in mobile edge computing environments for smart cities. First, we formulate the problem as a multi-objective constraint optimization problem that places edge servers in some strategic locations with the objective to make balance the workloads of edge servers and minimize the access delay between the mobile user and edge server. Then, we adopt mixed integer programming to find the optimal solution. Experimental results based on Shanghai Telecom's base station dataset show that our approach outperforms several representative approaches in terms of access delay and workload balancing. © 2018 Elsevier Inc.","Access delay; Mobile edge computing; Smart city edge server placement; Workload balancing"
"Privacy-preserving anomaly detection in the cloud for quality assured decision-making in smart cities","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044715545&doi=10.1016%2fj.jpdc.2017.12.011&partnerID=40&md5=94db698855fcbcd782c74833e7b3d788","Rapid urbanisation places extensive demands on city services and infrastructure that mandate innovative and sustainable solutions which increasingly involve streamlined monitoring, collection, storage and analysis of massive, heterogeneous data. Analytics services, such as anomaly detection, work to both extract knowledge and support decision-making mechanisms that enable smart functionality over such contexts. However, data privacy and data quality remain significant challenges to assuring the quality of decision-making. This paper introduces a scalable, cloud-based model to provide a privacy preserving anomaly detection service for quality assured decision-making in smart cities. Homomorphic encryption is employed to preserve data privacy during the analysis and MapReduce based distribution of tasks and parallelisation is used to overcome computational overheads associated with homomorphic encryption. Experiments demonstrate that a high level of accuracy is maintained for anomaly detection performed on encrypted data with the adopted distributed data processing approach significantly reducing associated computational overheads. © 2018 Elsevier Inc.","Anomaly detection; Cloud computing; Fully homomorphic encryption; Secure data analysis; Smart cities"
"The unified chart of mobility services: Towards a systemic approach to analyze service quality in smart mobility ecosystem","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060353787&doi=10.1016%2fj.jpdc.2018.12.009&partnerID=40&md5=6e7e0f7eebf936e708552b3e3187cde4","Local transportation services are an essential component of urban infrastructures. The constantly growing requirements and expectations from several stakeholders, ranging from public authorities, operators and passengers, demand a deep rethinking of the entire system. Such services must be oriented towards sustainability in everyday life, requiring systemic approaches capable of fostering Quality of Service (QoS) improvements. This paper aims at highlighting the main facets of QoS from different perspectives (service providers’ stakeholders’ passengers’ and commuters’) and proposes a systemic modeling approach that supports the design, implementation and monitoring of a QoS system in smart mobility. The whole architectural framework is described and UCoMS, the platform for analyzing QoS, is proposed. The approach has been validated in the Apulia region in the South of Italy, using taxonomy of quality indicators and service levels on the basis of the European and Italian regulatory backgrounds. © 2019 Elsevier Inc.","Microservices architecture; Quality of experience; Quality of service; Service lifecycle model; Smart mobility"
"Portable and efficient FFT and DCT algorithms with the Heterogeneous Butterfly Processing Library","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058530657&doi=10.1016%2fj.jpdc.2018.11.011&partnerID=40&md5=03824c96548efcef06aad27069a88210","The existence of a wide variety of computing devices with very different properties makes essential the development of software that is not only portable among them, but which also adapts to the properties of each platform. In this paper, we present the Heterogeneous Butterfly Processing Library (HBPL), which provides optimized portable kernels for problems of small sizes that allow using orthogonal transform algorithms such as the FFT and DCT on different accelerators and regular CPUs. Our library is implemented on the OpenCL standard, which provides portability on a large number of platforms. Furthermore, high performance is achieved on a wide range of devices by exploiting run-time code generation and metaprogramming guided by a parametrization strategy. An exhaustive evaluation on different platforms shows that our proposal obtains competitive or better performance than related libraries. © 2018 Elsevier Inc.","GPUs; Heterogeneous platform; Open computing language (OpenCL); Signal processing; Tuned library"
"Fog computing enabled cost-effective distributed summarization of surveillance videos for smart cities","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060872619&doi=10.1016%2fj.jpdc.2018.11.004&partnerID=40&md5=9ef8c3d05fd28ca934be5e2e55c78433","Fog computing is emerging an attractive paradigm for both academics and industry alike. Fog computing holds potential for new breeds of services and user experience. However, Fog computing is still nascent and requires strong groundwork to adopt as practically feasible, cost-effective, efficient and easily deployable alternate to currently ubiquitous cloud. Fog computing promises to introduce cloud-like services on local network while reducing the cost. In this paper, we present a novel resource efficient framework for distributed video summarization over a multi-region fog computing paradigm. The nodes of the Fog network is based on resource constrained device Raspberry Pi. Surveillance videos are distributed on different nodes and a summary is generated over the Fog network, which is periodically pushed to the cloud to reduce bandwidth consumption. Different realistic workload in the form of a surveillance videos are used to evaluate the proposed system. Experimental results suggest that even by using an extremely limited resource, single board computer, the proposed framework has very little overhead with good scalability over off-the-shelf costly cloud solutions, validating its effectiveness for IoT-assisted smart cities. © 2018 Elsevier Inc.","And computational efficiency; Energy-efficient cloud computing; Fog computing; Internet of things (IoT); Surveillance videos; Video summarization"
"QoS prediction for service recommendations in mobile edge computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033605403&doi=10.1016%2fj.jpdc.2017.09.014&partnerID=40&md5=425633e88a16124925c5c7ab77429ee6","Mobile edge computing is an emerging technology that provides services within the close proximity of mobile subscribers by edge servers that are deployed in each edge server. Mobile edge computing platform enables application developers and content providers to serve context-aware services (such as service recommendation) by using real time radio access network information. In service recommendation system, quality of service (QoS) prediction plays an important role when mobile devices or users want to invoke services that can satisfy user QoS requirements. However, user mobility (e.g., from one edge server to another) often makes service QoS prediction values deviate from actual values in traditional mobile networks. Unfortunately, many existing service recommendation approaches fail to consider user mobility. In this paper, we propose a service recommendation approach based on collaborative filtering and make QoS prediction based on user mobility. This approach initially calculates user or edge server similarity and selects the Top-K most-similar neighbors, predicts service QoS, and then makes service recommendation. We have implemented our proposed approach with experiments based on Shanghai Telecom datasets. Experimental results show that our approach can significantly improve on the accuracy of service recommendation in mobile edge computing. © 2017 Elsevier Inc.","Edge server similarity; Mobile edge computing; QoS; Service recommendation"
"Towards augmented proactive cyberthreat intelligence","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056154937&doi=10.1016%2fj.jpdc.2018.10.006&partnerID=40&md5=16a0eb54a4c3945e2010039fd05101b7","In cyber crimes, attackers are becoming more inventive with their exploits and use more sophisticated techniques to bypass the deployed security system. These attacks are targeted and are commonly referred as Advanced Persistent Threats (APTs). The currently available techniques to tackle these attacks are mostly reactive and signature based. Security Information and Event Management (SIEM), a proactive approach is the best solution. However, the major problem with SIEM is tackling huge amount of data in real time that makes it a time consuming and tedious task for security analyst. The use of threat intelligence caters to such issue by prioritizing the level of threat. In this paper, we assign risk score and confidence value to each feed generated at our product “T-Eye platform”. On the basis of these values, we assign a severity score to each feed type. Severity score assigns a level to the threat means prioritize the threat. The results, we achieved for prioritizing the threat is more apparent and accurate. In addition, we optimize the rules of IBM-Q-Radar by using threat feeds generated at T-Eye platform. Furthermore, a huge amount of false positive alarms generated at IBM Q-Radar is reduced to a certain extent. © 2018 Elsevier Inc.","Confidence; IBM Q-Radar; Risk score; Rules; Severity; T-Eye feeds; T-Eye platform"
"Workload aware VM consolidation method in edge/cloud computing for IoT applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055092545&doi=10.1016%2fj.jpdc.2018.09.011&partnerID=40&md5=00672d42aa33986aabb840d74659d6ed","Wide-ranging edge cloud data centers are a vital part of the solution for the problems caused by enormous growth in the IT industry for high computational power by advanced service applications. Majority of IoT applications switched to the Cloud and this stimulated the emergence of Edge technology to better manage the computing applications, data, resource and services. Consequently, with the massive client size and enormous applications trying to benefit from the cloud service, it makes it a challenging task for the edge cloud data centers to work in a power saving mode. In this paper, we propose a virtual machine consolidation method to switch the idle physical servers into hibernation mode, resulting in reduced power usage. We know that edge cloud data centers offer storage as a service, in this study we address the issues pertaining to storage units in the data centers. A unique classification approach is adopted to ensure load is balanced accordingly during allocation and our main contribution is on the VM migration technique. The VM migration is aimed at consolidating the VMs based on the workload to reduced number of physical machines to mitigate the energy consumption and promoting green computing. Therefore, we name the approach as Workload Aware Virtual Machine Consolidation Method (WAVMCM). We validate the proposed method with a competitive analysis of experimental results gathered from comparing it with Artificial Intelligence based probabilistic algorithm like Simulated Annealing, Genetic Algorithm and a case of no migration. Experimental results demonstrate that the proposed WAVMCM reduces 9% active servers saving 15% of power consumption when compared to genetic algorithm based method. © 2018 Elsevier Inc.","Cloud computing; Dynamic consolidation; Edge computing; Energy efficient allocation; Green computing; Resource management; Virtual machines; Workload aware"
"Efficient threshold password-authenticated secret sharing protocols for cloud computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061959900&doi=10.1016%2fj.jpdc.2019.01.013&partnerID=40&md5=a6f6daf0b35fff74c6984813b4149b06","Threshold password-authenticated secret sharing (TPASS) protocols allow a client to distribute a secret s amongst n servers and protect it with a password pw, so that the client can later recover the secret s from any subset of t of the servers using the password pw. In this paper, we present two efficient TPASS protocols, one is built on two-phase commitment and has lower computation complexity, and another is based on zero-knowledge proof and has less communication rounds. Both protocols are in particular efficient for the client, who only needs to send a request and receive a response. In addition, we have provided rigorous proofs of security for the proposed protocols in the standard model. The experimental results have shown that the proposed two TPASS protocols are more efficient than Camenisch et al.’s protocols and save up to 85%–95% total computational time and up to 65%–75% total communication overhead. © 2019 Elsevier Inc.","Diffie–Hellman problems; ElGamal encryption scheme; Shamir secret sharing scheme; Threshold password-authenticated secret sharing protocol"
"An efficient method of computation offloading in an edge cloud platform","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060913057&doi=10.1016%2fj.jpdc.2019.01.003&partnerID=40&md5=4c72b37348214b1f235636e40c4d510d","In a data-rich digital world, our hand-held resource-constrained mobile devices are restricted to performing small-to-medium-level computation processes and are incapable of performing high-computation processes. Computation offloading is a suitable solution for overcoming this shortcoming. Until recently, we have perceived cloud computing as an appropriate computation-offloading platform for mobile devices. However, cloud data centers, being far-end networks for mobile devices, increase the latency or network delay, which in turn affects the performance of real-time mobile Internet-of-Things applications. Hence, for critical real-time applications, a near-end network approach of computation offloading is required. Furthermore, the major hurdles for geographically distributed mobile devices are mobility and heterogeneity in the process of computation offloading. To overcome these challenges, the use of a deep-learning-based response-time-prediction framework is proposed in this paper to determine whether to offload in the nearby fog/edge node or neighbor fog/edge node, or cloud node. Furthermore, a restricted Boltzmann machines learning is applied to tackle the randomness in the availability of resources. We simulate the proposed model in MATLAB while considering the mobility and fluctuating resource demands of the end users. Implementing our deep-learning-based response-time-prediction framework improves the performance of the computation offloading because it facilitates a prompt selection of the offloading location. © 2019 Elsevier Inc.","Computation offloading; Deep learning; Mobile edge/fog computing; Resource provisioning"
"TACD: A throughput allocation method based on variant of Cobb–Douglas for hybrid storage system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061893490&doi=10.1016%2fj.jpdc.2019.01.012&partnerID=40&md5=668708c3391be8128a2384e6c2ffb147","The development of the cloud and data centers alike has caused users to increasingly share storage systems, and storage systems have adopted SSDs to improve performance. The number of users is growing sharply and causing resource competition, so the throughput allocation of a hybrid storage system has an important effect on users’ performance. A hybrid storage architect faces a challenge: sharing I/O throughput resources between SSDs and HDDs fairly with high resource utilization. Drawing on economic game-theory, many methods based on Leontief preference have been proposed to determine fair throughput allocation in a hybrid storage system with having the properties of Sharing Incentive (SI), Envy Freeness (EF) and Pareto Optimality (PO). However, users’ workload characteristics vary because of the adoption of SSDs, so there is heterogeneity among the storage media. The heterogeneity of storage media and the diversity of user workloads together create more constraints on fairness, which imposes restrictions to improve the resource utilization. To improve the resource utilization, some researchers have proposed relaxing the limitation of fairness properties by adjusting users’ allocations. However, when adjusting the allocation between users, some users’ allocations are reduced but others’ are improved, with the goal of increasing the resource utilization of the whole system, this will pose a problem because of the Leontief preference restriction: some users’ performance is degraded, while other users’ performance is improved. In this paper, we first propose one variant of the Cobb–Douglas preference based on users’ workloads and allocations, and then present a throughput allocation model based on this variant named TACD that can capture the effects of diminishing returns and substitutability in throughput allocation for hybrid storage. By making use of these effects, TACD can improve some users’ performance while providing the same performance for other users and achieving higher resource utilization in the whole system, and it can provide a fair allocation while enjoying SI, EF and PO properties. The variant of the Cobb–Douglas preference proposed in this paper has taken consideration of users’ workloads, so TACD can conduct the throughput allocation based on these workloads for meeting the performance requirements of users reasonably. Extensive experiments are conducted to prove the effectiveness of TACD. The simulation results show that TACD can fit the throughput allocation of hybrid storage system very well. The Linux results show that compared with the allocation method based on Leontief preference, TACD can provide a higher resource utilization and performance for users throughout the system, it can improve some users’ performance when maintaining comparable performance for other users, in contrast to the Leontief method based on weaker fairness under the same resource utilization, and its allocation result enjoys SI, EF and PO properties. © 2019 Elsevier Inc.","Efficiency; Fairness; Hybrid storage; Throughput allocation; Variant of Cobb–Douglas"
"On the support of inter-node P2P GPU memory copies in rCUDA","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060740185&doi=10.1016%2fj.jpdc.2018.12.011&partnerID=40&md5=9eafd4a1ae11d93199387315c0d8e641","Although GPUs are being widely adopted in order to noticeably reduce the execution time of many applications, their use presents several side effects such as an increased acquisition cost of the cluster nodes or an increased overall energy consumption. To address these concerns, GPU virtualization frameworks could be used. These frameworks allow accelerated applications to transparently use GPUs located in cluster nodes other than the one executing the program. Furthermore, these frameworks aim to offer the same API as the NVIDIA CUDA Runtime API does, although different frameworks provide different degree of support. In general, and because of the complexity of implementing an efficient mechanism, none of the existing frameworks provides support for memory copies between remote GPUs located in different nodes. In this paper we introduce an efficient mechanism devised for addressing the support for this kind of memory copies among GPUs located in different cluster nodes. Several options are explored and analyzed, such as the use of the GPUDirect RDMA mechanism. We focus our discussion on the rCUDA remote GPU virtualization framework. Results show that is possible to implement this kind of memory copies in such an efficient way that performance is even improved with respect to the original performance attained by CUDA when GPUs located in the same cluster node are leveraged. © 2019 Elsevier Inc.","CUDA; GPUDirect RDMA; Virtualization"
"Traffic flow monitoring systems in smart cities: Coverage and distinguishability among vehicles","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050993758&doi=10.1016%2fj.jpdc.2018.07.008&partnerID=40&md5=4fff00608e6f4a127ce7055f11d3559e","Traffic flow monitoring systems aim to measure and monitor vehicle trajectories in smart cities. Their critical applications include vehicle theft prevention, vehicle localization, and traffic congestion solution. This paper studies an RoadSide Unit (RSU) placement problem in traffic flow monitoring systems, in order to secure vehicles through location proofs. Given some traffic flows on streets, the objective is to place a minimum number of RSUs to cover and distinguish all traffic flows. A traffic flow is covered and distinguishable, if the set of its passing RSUs is non-empty and unique among all traffic flows. The RSU placement problem is NP-hard, monotonic, and non-submodular. It is a non-trivial extension of the traditional set cover problem that is submodular. Three bounded RSU placement algorithms are proposed with respect to the number of given traffic flows. To further reduce the number of deployed RSUs, this paper extends a credential propagation mechanism via vehicle-to-vehicle communications, which essentially enlarges the coverage of an RSU. Extensive real data-driven experiments demonstrate the efficiency and effectiveness of the proposed algorithms. © 2018 Elsevier Inc.","Coverage and distinguishability; Location proof; RSU placement; Smart city; Traffic flow tracking systems"
"Node aware sparse matrix–vector multiplication","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064433826&doi=10.1016%2fj.jpdc.2019.03.016&partnerID=40&md5=fbe21d016d80b18d402cb255b63a09a8","The sparse matrix–vector multiply (SpMV) operation is a key computational kernel in many simulations and linear solvers. The large communication requirements associated with a reference implementation of a parallel SpMV result in poor parallel scalability. The cost of communication depends on the physical locations of the send and receive processes: messages injected into the network are more costly than messages sent between processes on the same node. In this paper, a node aware parallel SpMV (NAPSpMV) is introduced to exploit knowledge of the system topology, specifically the node-processor layout, to reduce costs associated with communication. The values of the input vector are redistributed to minimize both the number and the size of messages that are injected into the network during a SpMV, leading to a reduction in communication costs. A variety of computational experiments that highlight the efficiency of this approach are presented. © 2019 Elsevier Inc.","Matrix–vector multiplication; Node aware; Parallel communication; Sparse; SpMV"
"Automatic generation of benchmarks for I/O-intensive parallel applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055754292&doi=10.1016%2fj.jpdc.2018.10.004&partnerID=40&md5=419ddf71736a26b4e3b82aa091267591","The benchmarks of I/O-intensive parallel applications are important for evaluating and optimizing HPC softwares and hardwares. However, extracting high-fidelity benchmarks which can fully reflect computation, communication, and I/O behaviors of original I/O-intensive parallel applications is very difficult. This work contributes a framework which can automatically generate benchmarks for I/O-intensive parallel applications. We demonstrate our framework on Taub and TianHe-2 supercomputers with five NAS Parallel Benchmarks (NPB) and four I/O-intensive parallel applications. The results show that our trace merging algorithm and trace compressing algorithm are better than others, and the generated benchmarks can accurately mimic the computation, communication, and I/O behaviors of original I/O-intensive parallel applications. Also, these generated benchmarks can be used to predict the performance of original applications, while reducing the prediction overhead by scaling down the execution time of benchmark proportionally. © 2018 Elsevier Inc.","Benchmark generation; I/O-intensive applications; Performance prediction; Trace compressing; Trace merging"
"A randomized adaptive neighbor discovery for wireless networks with multi-packet reception capability","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059846561&doi=10.1016%2fj.jpdc.2018.11.010&partnerID=40&md5=ddf822d30b1c6614a742eabbacd5d072","Neighbor discovery is a first step in the initialization of wireless networks in large-scale ad hoc networks. In this paper, we propose a randomized neighbor discovery scheme for wireless networks with a multi-packet reception (MPR) capability. We let the nodes to have different advertisement probabilities. The node gradually adjusts its probability according to its operation phases: greedy or slow-start. In the greedy phase, the node advertises aggressively while it does moderately in the slow-start phase. Initial phase and advertisement probability are determined randomly. Then, the nodes change the probability adaptively according to advertisements the reception state from the other nodes. In order to decide the reception state precisely, the exact number of nodes in the network is necessary. To make our proposed scheme work in case of no prior knowledge of the population, we propose a population estimation method based on a maximum likelihood estimation. We evaluate our proposed scheme through numerical analysis and simulation. Through the numerical analysis, we show that the discovery completion time is lower bounded in [Formula presented] and upper bounded in [Formula presented] when there exists N nodes with MPR-k capability. The bounds are the same as those of previous studies that propose static optimal advertisement probability. Through the simulation, we evaluate that our adaptive scheme outperforms in terms of discovery completion time, advertisement efficiency, and wasted time slot ratio than a scheme with static advertisement probability when the population of the network is unknown. © 2018 Elsevier Inc.","Adaptive advertisement; Maximum likelihood estimation; Multi-packet reception; Neighbor discovery; Randomized algorithm; Wireless network"
"Optimal power allocation and load balancing for non-dedicated heterogeneous distributed embedded computing systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064061105&doi=10.1016%2fj.jpdc.2019.03.019&partnerID=40&md5=747da0d555bf3a8a7a4e0d804cf7f7ce","This paper investigates on the optimal power allocation and load balancing problem encountered by heterogeneous and distributed embedded systems with mixed tasks. Given that each node has real and different urgent tasks in the majority of practical heterogeneous embedded systems, three priority disciplines are considered: dedicated jobs without priority, prioritized dedicated jobs without preemption, and prioritized dedicated jobs with preemption. A model is established for heterogeneous embedded processors with dedicated-task-dependent dynamic power and load balancing management; each processor is considered as an M/M/1 queueing sub-model with mixed generic and dedicated tasks. The processors have different levels of power consumption, and each one can employ any of the three disciplines. The objective of this study is to find an optimal load balancing (for generic tasks) and power allocation strategy for heterogeneous processors preloaded by different amounts of dedicated tasks such that the average response time of generic tasks is minimized. Considering that this problem is a multi-constrained, multi-variable optimization problem for which a closed-form solution is unlikely to be obtained, we propose an optimal power allocation and load balancing scheme by employing Lagrange method and binary search approach, which are completed by utilizing two new rules established by observing numerical variations of parameters. Several numerical examples are presented to demonstrate the effectiveness of our solution. To the best of our knowledge, this is the first work on analytical study that combines load balancing, energy efficiency, and priority of tasks in heterogeneous and distributed embedded systems. © 2019 Elsevier Inc.","Embedded and distributed system; Load distribution; Power allocation; Queueing model; Response time"
"Secure authentication and load balancing of distributed edge datacenters","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056603868&doi=10.1016%2fj.jpdc.2018.10.007&partnerID=40&md5=0960836e59e0c66c783bd5d054345535","Edge computing is an emerging research area to incorporate cloud computing into edge network devices. An Edge datacenter, also referred to as EDC, processes data streams and user requests in real-time and is therefore used to decrease the latency and congestion in the network. EDC is usually setup as a distributed system and is accordingly placed between the cloud datacenter and the data source. These EDCs work as an intermediate layer in the fog hierarchy between IoT and Cloud datacenter. EDC's are aided by load balancers, responsible for distributing the workload amongst multiple EDC, in order to optimize resource utilization and response time. The load balancers make sure that the workload is equally divided amongst the available EDCs to avoid over loading of some EDCs while other remain idle as this directly impacts the user response and real-time event detection. Given the fact that EDCs are deployed in remote environments, the need for secure authentication is of major importance. In this paper we propose a novel load balancing technique that enables EDC authentication as well as identification of idle EDCs for better load balancing. The proposed load balancing technique is also compared with existing approaches and proves to be more efficient in locating EDC's with less workload. In addition to the improved efficiency, the proposed scheme also strengthens the security of the network by incorporating destination EDC authentication. © 2018","Authentication; Cloud; Edge computing; Edge datacenter; Fog computing; Load balancing; Security"
"A survey of architectural approaches for improving GPGPU performance, programmability and heterogeneity","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061054217&doi=10.1016%2fj.jpdc.2018.11.012&partnerID=40&md5=85337f9a01857d62e605544ec76c73fd","With the skyrocketing advances of process technology, the increased need to process huge amount of data, and the pivotal need for power efficiency, the usage of Graphics Processing Units (GPUs) for General Purpose Computing becomes a trend and natural. GPUs have high computational power and excellent performance per watt, for data parallel applications, relative to traditional multicore processors. GPUs appear as discrete or embedded with CPUs, leading to a scheme of heterogeneous computing. Heterogeneous computing brings as many challenges as it brings opportunities. To get the most of such systems, we need to guarantee high GPU utilization, deal with irregular control flow of some workloads, and struggle with far-friendly-programming models. The aim of this paper is to provide a survey about GPUs from two perspectives: architectural advances to improved performance and programmability and advances to enhance CPU–GPU integration in heterogeneous systems. This will help researchers see the opportunities and challenges of using GPUs for general purpose computing, especially in the era of big data and the continuous need of high-performance computing. © 2018 Elsevier Inc.","Control divergence; GPGPU; Heterogeneous architecture; Memory systems"
"Universal behavior of the linear threshold model on weighted networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055721667&doi=10.1016%2fj.jpdc.2018.10.003&partnerID=40&md5=3d481fa6e3ec9ba9fb32ddf839949146","The linear threshold model is widely adopted as a classic prototype for studying contagion processes on social networks, where nodes, representing individuals, are assumed to be in one of two states: inactive or active. Each inactive node can be activated via a threshold rule during evolution. Although both contagion mechanisms and network impacts have been well studied, very few studies paid attention on the effect of interacting strengths on the threshold rule. In this paper, a modified linear threshold model on weighted networks is proposed. On one hand, the weight of a link in the network is characterized by a power-law function of the product of endpoint degrees. On the other hand, peer influences on a node incorporate both the number of its active neighbors and associated link weights. The systematic dynamics is explored by the combination of the spin-glass theory and Monte-Carlo algorithm. In analogy to unweighted networks, a global cascade is not triggered in weighted networks when the average degree of nodes is either too small or too large, however, large cascades are realized within an intermediate range, which is referred to as the cascade window. Moreover, two regimes of the power exponent of the weight function are identified in which the system exhibits distinct behaviors: when networks are very sparse, there exist one extreme of the weight exponent making the system susceptible to large cascades; when the networks are relatively dense, on the contrary, there exists the other extreme of the weight exponent causing the system to maintain optimal robustness. All these results demonstrate the importance of both network connectivity and link weights, and offer a sophisticated description of social contagions. © 2018 Elsevier Inc.","Cascade; Linear threshold model; Robustness; Weighted networks"
"Spreading Aggregation: A distributed collision-free approach for data aggregation in large-scale wireless sensor networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058490547&doi=10.1016%2fj.jpdc.2018.11.007&partnerID=40&md5=bc1df10dfdf083bfd2ada17d6599308f","Recently, numerous works have shown that serial aggregation in large wireless sensor networks is scalable and very efficient, in terms of avoiding collisions and conserving energy and, more importantly, in terms of reducing response time. In this paper, a novel serial data aggregation approach, called Spreading Aggregation (SA), is proposed with the aim of shortening the traversal path and further reducing communications. First, given the fact that it is not based on a pre-established itinerary, SA is data structure maintenance-free and does not require any communications in this regard. Each time an aggregation process is launched, a new path is built, which decreases vulnerability to failure in links and nodes and allows the approach to handle topology changes. Second, SA is a localized approach that relies only on the one-hop neighbors table of each node to gradually construct the path, which makes it very scalable. A third interesting feature of SA is the merge of path construction and data processing. While the path is progressively constructed, data is simultaneously aggregated, saving a considerable amount of time and energy. In addition to all that, SA also saves energy and time due to its collision-free nature. In fact, in SA, only one packet is present in the entire network at any given time. In this paper, we formally prove the correctness of SA (i.e., free of looping and ensures the traversal of all connected nodes). Furthermore, the extensive OMNeT++ simulations, we performed, confirm that the proposed approach reduces communications, scales well in large networks, and conserves time and energy. The obtained results also show that SA outperforms state-of-the-art serial approaches. © 2018 Elsevier Inc.","Collision-free data aggregation; Distributed serial in-network processing; Large-scale wireless sensor networks; Sensor query processing; Serial data fusion"
"A mobility-aware approach for distributed data update on unstructured mobile P2P networks","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054920422&doi=10.1016%2fj.jpdc.2018.09.013&partnerID=40&md5=5b4d4a0a7d9cd0f4d2f728617d44d8a8","In unstructured mobile peer-to-peer systems (MP2P), the frequent link breakages lead frequent topology mismatching problems and data transmission failures due to high mobility of nodes. The overhead of data transmission and synchronization cannot be neglected. In order to keep the data consistent, flooding is a fundamental and straightforward data synchronization mechanism since a mobile node does not know which else has the same shared data item. However, data flooding causes the broadcast storm problem. In this paper, we propose a mobility-aware data update approach (MADU) to improve the data dissemination and to reduce the overhead of maintaining the consistency of shared data items in an MP2P network. We use safe-time which is derived from the neighbor's location and speed to determine the time for a node to do the checking and updating between the neighbor nodes and itself. We also consider the network connectivity of a mobile node and access frequency of a data item as the factors to trigger the update process. By combining the mobility information of nodes, network connectivity, and access frequency of data items, we set a reasonable data update mechanism, which can significantly decrease the number of retransmissions and redundant messages so as to reduce the overhead of maintaining the data consistency. © 2018 Elsevier Inc.","Data consistency; Data synchronization; Mobile peer-to-peer systems; Mobility-awareness; Safe-time"
"Online learning offloading framework for heterogeneous mobile edge computing system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063211650&doi=10.1016%2fj.jpdc.2019.02.003&partnerID=40&md5=0c642a956a5e486b8c2d07f0099a7325","Cloud of Things (CoT) is a significant paradigm for bridging cloud resource and mobile terminals. Mobile edge computing (MEC) is a supporting architecture for CoT. The objectives of this paper are to describe and evaluate a method to handle the computation offloading problem during user mobility which minimizes the offloading failure rate in heterogeneous network. Furthermore, users’ mobility and their choices for offloading lead to the everchanging condition of wireless network and opportunistic resource available. By modeling such dynamic mobile edge environment, quantizing the user cost, failure penalty and diversified QoS requirements, computation offloading problem is converted into an online decision-making problem in a stochastic process. We divide the decision-making into two phases: offloading planning phase and offloading running phase. In both phases the learning agent can continuously improve the control policy. We also conduct a failure recovery policy to tackle different types of failure and is included in the decision-making process. The numerical results show that the proposed online learning offloading method for mobile users can derive the optimal offloading scheme compared with the baseline algorithms. © 2019","Cloudlet; Computation offloading; Mobile edge computing; Offloading failure; Reinforcement learning"
"Protected pointers to specify access privileges in distributed systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058656178&doi=10.1016%2fj.jpdc.2018.12.001&partnerID=40&md5=3e5433aaa36efcf3b041b76f755dc65e","With reference to a distributed environment consisting of nodes connected in an arbitrary network topology, we propose the organization of a protection system in which a set of subjects, e.g. processes, generates access attempts to memory segments. One or more primary passwords are associated with each node. An access to a given segment can be successfully accomplished only if the subject attempting the access holds an access privilege, certified by possession of a valid protected pointer (p-pointer) referencing that segment. Each p-pointer includes a local password; the p-pointer is valid if the local password descends from a primary password by application of a universally known, parametric one-way generation function. A set of protection primitives makes it possible to manage the primary passwords, to reduce p-pointers to include less access rights, to allocate new segments, to delete existing segments, to read the segment contents and to overwrite these contents. The resulting protection environment is evaluated from a number of viewpoints, which include p-pointer forging and revocation, the network traffic generated by the execution of the protection primitives, the memory requirements for p-pointer storage, security, and the relation of our work to previous work. An indication of the flexibility of the p-pointer concept is given by applying p-pointers to the solution of a variety of protection problems. © 2018 The Author(s)","Access privilege; Distributed system; Parametric one-way function; Password; Protection; Segment"
"INRFlow: An interconnection networks research flow-level simulation framework","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064450433&doi=10.1016%2fj.jpdc.2019.03.013&partnerID=40&md5=bc633abddb963e15c655419b0ed68e0b","This paper presents INRFlow, a mature, frugal, flow-level simulation framework for modelling large-scale networks and computing systems. INRFlow is designed to carry out performance-related studies of interconnection networks for both high performance computing systems and datacentres. It features a completely modular design in which adding new topologies, routings or traffic models requires minimum effort. Moreover, INRFlow includes two different simulation engines: a static engine that is able to scale to tens of millions of nodes and a dynamic one that captures temporal and causal relationships to provide more realistic simulations. We will describe the main aspects of the simulator, including system models, traffic models and the large variety of topologies and routings implemented so far. We conclude the paper with a case study that analyses the scalability of several typical topologies. INRFlow has been used to conduct a variety of studies including evaluation of novel topologies and routings (both in the context of graph theory and optimization), analysis of storage and bandwidth allocation strategies and understanding of interferences between application and storage traffic. © 2019 The Authors","Datacentres; Interconnection networks; Large-scale systems; Network topologies and routing; Simulation and modelling; Supercomputers"
"Privacy-aware smart city: A case study in collaborative filtering recommender systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042144386&doi=10.1016%2fj.jpdc.2017.12.015&partnerID=40&md5=484f40de3809158d808818d7b7e16728","Ensuring privacy in recommender systems for smart cities remains a research challenge, and in this paper we study collaborative filtering recommender systems for privacy-aware smart cities. Specifically, we use the rating matrix to establish connections between a privacy-aware smart city and k-coRating, a novel privacy-preserving rating data publishing model. First, we model privacy concerns in a smart city as the problem of privacy-preserving collaborative filtering recommendation. Then, we introduce k-coRating to address privacy concerns in published rating matrices, by filling the null ratings with predicted scores. This allows us to mask the original ratings to preserve k-anonymity-like data privacy, and enhance data utility (quantified using prediction accuracy in this paper). We show that the optimal k-coRated mapping is an NP-hard problem and design an efficient greedy algorithm to achieve k-coRating. We then demonstrate the utility of our approach empirically. © 2018 Elsevier Inc.","Data privacy; Parallel computing; Privacy-preserving collaborative filtering; Recommendation systems; Smart cities"
"Performance evaluation of live virtual machine migration in SDN-enabled cloud data centers","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065441528&doi=10.1016%2fj.jpdc.2019.04.014&partnerID=40&md5=208e07a462c28b29bd541dc3be0da159","In Software-Defined Networking (SDN) enabled cloud data centers, live VM migration is a key technology to facilitate the resource management and fault tolerance. Despite many research focus on the network-aware live migration of VMs in cloud computing, some parameters that affect live migration performance are neglected to a large extent. Furthermore, while SDN provides more traffic routing flexibility, the latencies within the SDN directly affect the live migration performance. In this paper, we pinpoint the parameters from both system and network aspects affecting the performance of live migration in the environment with OpenStack platform, such as the static adjustment algorithm of live migration, the performance comparison between the parallel and the sequential migration, and the impact of SDN dynamic flow scheduling update rate on TCP/IP protocol. From the QoS view, we evaluate the pattern of client and server response time during the pre-copy, hybrid post-copy, and auto-convergence based migration. © 2019 Elsevier Inc.","Cloud computing; Live VM migration; OpenDaylight; OpenStack; Performance measures; Software-Defined Networking; Virtual machine"
"DATALET: An approach to manage big volume of data in cyber foraged environment","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065139701&doi=10.1016%2fj.jpdc.2019.04.005&partnerID=40&md5=11d4ae86c5945132f60da43ddae07120","In the new era of cloud computing, the users get various services from the cloud. In recent years, the increasing volume of data specially from pervasive devices has become a great matter of concern. In mobile computing, cloudlets act as a shadow image of the data centers and provide low latency cloud environment. In this work, the authors have proposed an approach called DATALET that deals with distribution of data by utilizing the processing and storage resources of big intermittent networks. In DATALET, the cloudlets act as central managers for data management. DATALET provides a robust architecture which is fault-tolerant and also has a cloudlet job scheduler. The cloudlets maintain the information based on the availability of the user at a particular location and also utilize their computational resources. The model is simulated using NS-3 and will also provide the service in an environment where the network does not exist. The results indicate that DATALET approach has higher performance in terms of latency, Internet outgoing bandwidth for the users and resource utilization of the user devices. © 2019 Elsevier Inc.","Big data; Cloudlets; Cyber-foraging; Disruption-tolerant network"
"Large-scale parallel similarity search with Product Quantization for online multimedia services","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058014593&doi=10.1016%2fj.jpdc.2018.11.009&partnerID=40&md5=846dc2488868cecdf193457560813aac","The similarity search in high-dimensional spaces is a core operation found in several online multimedia retrieval applications. With the popularity of these applications, they are required to handle very large and increasing datasets, while keeping the response time low. This problem is worsened in the context of online applications, mostly due to the fact that load on these systems vary during the execution according to the users demands. Those variations require the application to adapt during the execution in order to minimize the response times. In this paper, we address these challenges with an efficient parallelization of the Product Quantization Approximate Nearest Neighbor Search (PQANNS) indexing. This method is capable of answering queries with a reduced memory demand and, coupled with a distributed memory parallelization proposed here, can efficiently handle very large datasets. We have also proposed mechanisms to minimize the query response times in online scenarios in which the query rates vary at run-time. For this sake, our strategies tune the parallelism configurations and task granularity during the execution. The parallelism and granularity tuning approaches (ADAPT and ADAPT+G) have shown, for instance, to reduce the query response times by a factor of 6.4× in comparison with the best static configuration of parallelism and task granularity. Further, the distributed memory execution using 128 nodes/3584 CPU cores has attained a parallel efficiency of 0.97 with a dataset of 256 billion SIFT vectors. © 2018 Elsevier Inc.","Descriptor indexing; Dynamic parallelism; Multimedia similarity search; Product Quantization"
"A time-inhomogeneous Markov chain and its distributed solution for message dissemination in OUSNs","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064649331&doi=10.1016%2fj.jpdc.2019.03.012&partnerID=40&md5=d8c14877242548eafce5829069dba0e7","Opportunistic underwater sensor networks (OUSNs)are deployed for various underwater applications, such as underwater creatures tracking and tactical surveillance. The index of freshness-cost ratio is firstly introduced to describe the objectives of spatio-temporal cost minimization and message freshness preservation for the message dissemination in OUSNs. To analyze the message propagation process, a time-inhomogeneous ergodic Markov chain is constructed, and the delivery progress of each message is determined by the number of hops from the destination to the nearest message holder, which is mapped into several propagation states of the Markov chain. Then, a message dissemination method (MDM)derived from a Markov decision process is specifically designed to improve the freshness-cost ratio. In the MDM, the message holders decide how many copies should be disseminated at each time slot, according to the Markov decision process. The performance of MDM is analyzed through simulation experiments that produce preferable tradeoff results, which indicate that MDM can deliver the messages with fewer spatio-temporal cost while preserving the message freshness as much as possible. © 2019 Elsevier Inc.","Markov decision process; Message dissemination; Opportunistic underwater sensor networks; Time-inhomogeneous Markov chain"
"Fast parallel multidimensional FFT using advanced MPI","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063063901&doi=10.1016%2fj.jpdc.2019.02.006&partnerID=40&md5=d78e53e7896c690846d2f99eb7d20542","We present a new method for performing global redistributions of multidimensional arrays essential to parallel fast Fourier (or similar) transforms. Traditional methods use standard all-to-all collective communication of contiguous memory buffers, thus necessarily requiring local data realignment steps intermixed in-between redistribution and transform steps. Instead, our method takes advantage of subarray datatypes and generalized all-to-all scatter/gather from the MPI-2 standard to communicate discontiguous memory buffers, effectively eliminating the need for local data realignments. Despite generalized all-to-all communication of discontiguous data being generally slower, our proposal economizes in local work. For a range of strong and weak scaling tests, we found the overall performance of our method to be on par and often better than well-established libraries like MPI-FFTW, P3DFFT, and 2DECOMP&FFT. We provide compact routines implemented at the highest possible level using the MPI bindings for the C programming language. These routines apply to any global redistribution, over any two directions of a multidimensional array, decomposed on arbitrary Cartesian processor grids (1D slabs, 2D pencils, or even higher-dimensional decompositions). The high level implementation makes the code easy to read, maintain, and eventually extend. Our approach enables for future speedups from optimizations in the internal datatype handling engines within MPI implementations. © 2019 Elsevier Inc.","ALLTOALLW; FFT; MPI; Pencil; Slab"
"QDaS: Quality driven data summarisation for effective storage management in Internet of Things","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046371424&doi=10.1016%2fj.jpdc.2018.03.013&partnerID=40&md5=e86ef01ac206c884b10472e3db28fc67","The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets. © 2018 Elsevier Inc.","Cloud computing; Data summarisation; Internet of Things (IoT); Quality of data; Quality of service; Storage management"
"VCube-PS: A causal broadcast topic-based publish/subscribe system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057113212&doi=10.1016%2fj.jpdc.2018.10.011&partnerID=40&md5=f1169b70c545db0e7f93164a8c3c6fc1","In this work we present VCube-PS, a topic-based Publish/Subscribe system built on the top of a virtual hypercube-like topology. Membership information and published messages are broadcast to subscribers (members) of a topic group over dynamically built spanning trees rooted at the publisher. For a given topic, the delivery of published messages respects the causal order. VCube-PS was implemented on the PeerSim simulator, and experiments are reported including a comparison with the traditional Publish/Subscribe approach that employs a single rooted static spanning-tree for message distribution. Results confirm the efficiency of VCube-PS in terms of scalability, latency, number and size of messages. © 2018 Elsevier Inc.","Causal broadcast; Distributed spanning trees; Hypercube-like topologies; Publish/subscribe; Topic-based pub/sub"
"On the maturity of parallel applications for asymmetric multi-core processors","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061033076&doi=10.1016%2fj.jpdc.2019.01.007&partnerID=40&md5=181674325b5e8ef0657080b82fe9292f","Asymmetric multi-cores (AMCs) are a successful architectural solution for both mobile devices and supercomputers. By maintaining two types of cores (fast and slow) AMCs are able to provide high performance under the facility power budget. This paper performs the first extensive evaluation of how portable are the current HPC applications for such supercomputing systems. Specifically we evaluate several execution models on an ARM big.LITTLE AMC using the PARSEC benchmark suite that includes representative highly parallel applications. We compare schedulers at the user, OS and runtime levels, using both static and dynamic options and multiple configurations, and assess the impact of these options on the well-known problem of balancing the load across AMCs. Our results demonstrate that scheduling is more effective when it takes place in the runtime system level as it improves the baseline by 23%, while the heterogeneous-aware OS scheduling solution improves the baseline by 10%. © 2019","Asymmetric multi-cores; HPC; Parallel programming; Runtime systems; Scheduling"
"Simplified Biased Contribution Index (SBCI): A mechanism to make P2P network fair and efficient for resource sharing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055638532&doi=10.1016%2fj.jpdc.2018.10.002&partnerID=40&md5=2ba0988f31b4163919c72b041ed7646d","To balance the load and to discourage the free-riding in peer-to-peer (P2P) networks, many incentive mechanisms and policies have been proposed in the recent years. Global peer ranking is one such mechanism. In this mechanism, peers are ranked based on a metric called contribution index. Contribution index is defined in such a manner that peers are motivated to share the resources in the network. Fairness in the terms of upload to download ratio in each peer can be achieved by this method. However, calculation of contribution index is not trivial. It is computed distributively and iteratively in the entire network and requires strict clock synchronization among the peers. A very small error in clock synchronization may lead to wrong results. Furthermore, iterative calculation requires a lot of message overhead and storage capacity, which makes its implementation more complex. In this paper, we propose a simple incentive mechanism based on the contributions of peers, which can balance the upload and download amount of resources in each peer. It does not require iterative calculation, therefore, can be implemented with lesser message overhead and storage capacity without requiring strict clock synchronization. This approach is efficient as there are very less rejections among the cooperative peers. It can be implemented in a truly distributed fashion with O(N) time complexity per peer. © 2018 Elsevier Inc.","DHT; Free-rider; P2P network"
"Modeling the asynchronous Jacobi method without communication delays","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062456355&doi=10.1016%2fj.jpdc.2019.02.002&partnerID=40&md5=9d7e214733b2a6a8e39bf0e08192a0c0","Asynchronous iterative methods for solving linear systems are gaining renewed interest due to the high cost of synchronization points in massively parallel codes. Historically, theory on asynchronous iterative methods has focused on asymptotic behavior, while the transient behavior remains poorly understood. In this paper, we study a model of the asynchronous Jacobi method without communication delays, which we call simplified asynchronous Jacobi. Simplified asynchronous Jacobi can be used to model asynchronous Jacobi implemented in shared memory or distributed memory with fast communication networks. Our analysis uses the idea of a propagation matrix, which is similar in concept to an iteration matrix. We show that simplified asynchronous Jacobi can continue to reduce the residual when some processes are slower than other processes. We also show that simplified asynchronous Jacobi can converge when synchronous Jacobi does not. We verify our analysis of simplified asynchronous Jacobi using results from asynchronous Jacobi implemented in shared and distributed memory. © 2019 Elsevier Inc.","Asynchronous; Gauss–Seidel; Iterative solvers; Jacobi; Remote memory access; Sparse linear systems"
"Controlled secure social cloud data sharing based on a novel identity based proxy re-encryption plus scheme","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064438538&doi=10.1016%2fj.jpdc.2019.03.018&partnerID=40&md5=d93d1476c6678e2982b5256e1c879521","Currently we are witnessing a rapid integration of social networks and cloud computing, especially on storing social media contents on cloud storage due to its cheap management and easy accessing at any time and from any place. However, how to securely store and share social media contents such as pictures/videos among social groups is still a very challenging problem. In this paper, we try to tackle this problem by using a new cryptographic primitive: identity based proxy re-encryption plus (IBPRE +), which is a variant of proxy re-encryption (PRE). In PRE, by using re-encryption keys, a ciphertext computed for Alice can be transferred to a new one for Bob. Recently, the concept of PRE plus (PRE+) was introduced by Wang et al. In PRE+, all the algorithms are almost the same as traditional PRE, except the re-encryption keys are generated by the encrypter instead of the delegator. The message-level based fine-grained delegation property and the weak non-transferable property can be easily achieved by PRE +, while traditional PRE cannot achieve them. Based on the 3-linear map, we first propose a new IBE scheme and a new IBPRE+ scheme, we prove the security of these schemes and give the properties and performance analysis of the new IBPRE+ scheme. Finally, we propose a new framework based on this new primitive for secure cloud social data sharing. © 2019 Elsevier Inc.","3-linear map; Identity based encryption; Identity based proxy re-encryption plus; Proxy re-encryption plus; Secure social cloud data sharing"
"BMMI-tree: A Peer-to-Peer m-ary tree using 1-m node splitting for an efficient multidimensional complex query search","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056867763&doi=10.1016%2fj.jpdc.2018.09.018&partnerID=40&md5=0a60de0764089f92fb0c9bce448c5543","Peer-to-Peer (P2P) applications such as content distribution and sharing (like file, audio, video), multiuser communication (games, desktop sharing, e-learning) have emerged as a new paradigm over a last decade. However, scalability requirements remain a major concern and hence, the distribution and effective search of multidimensional data have become major challenges for P2P computing. Most of the existing P2P overlays either do not give support to Multidimensional Indexing (MI) or the frameworks are less efficient for complex query search or they are limited up to binary trees only, with the search complexity O(log2N). However, traditional MI based on m-ary tree is strengthened for the complex query search (bound to logmN) using higher fanout, m > 2. Based on these observations, we propose BMMI-tree (Balanced Multiway Multidimensional Indexing-tree) that uses an m-ary P2P tree overlay network and also provides the support of MI tree indexing methods such as R-tree or SS-tree in this paper. The paper also analyzes the complex query search algorithms performed in O(log[Formula presented]N) steps with the experimental results. In addition, the construction of the P2P tree network requires to split some existing node and its data objects into m new child nodes (during node join) and vice versa (during node leave). To the best of our knowledge, none of the existing node splitting algorithms for multiway multidimensional trees offer 1-m node splitting. Hence, in this paper, we also propose two different approaches to split the MI tree node into m number of nodes (m-ary split) to be used effectively to create a dynamic tree overlay. Lastly, we present how the BMMI-tree can be applied for service provisioning in cloud computing in a decentralized and distributed manner. © 2018 Elsevier Inc.","Complex query search; Distributed computing; Multidimensional indexing; Multiway tree structures; Node splitting; Peer-to-peer networks"
"Fully parallelized Lattice Boltzmann scheme for fast extraction of biomedical geometry","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062885406&doi=10.1016%2fj.jpdc.2019.02.004&partnerID=40&md5=39c954c3e2bec999e0fb4a0650e70477","We develop a fully parallel numerical method which quickly performs 2D and 3D segmentation on GPU to extract anatomical structures from medical images. The algorithm solves the level set equations completely within a Lattice Boltzmann model (LBM). Compared with existing LBM-based segmentation approaches, a parallel distance field regularization is added to the LBM computing scheme to keep computation stable with large time step iteration. This approach also avoids external regularization which has been a major impediment to direct parallelization of level set evolution with LBM. It allows the whole computing process to be efficiently executed on GPU. Moreover, the method can be incorporated with different image features to adopt in various image segmentation tasks. Therefore, our method enables fully GPU accelerated geometric extraction from medical images, leading to high computing performance which is demanded in many practical applications. This method is used to exactly accurate 2D and 3D anatomical structures from many real world CT and MRI images. The achieved results can also directly feed required boundary information to LBM-based hemodynamics simulation. © 2019 Elsevier Inc.","Biomedical geometry extraction; GPU computing; Lattice Boltzmann method; Parallel image segmentation"
"Secure hardware-entangled field programmable gate arrays","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065532497&doi=10.1016%2fj.jpdc.2019.04.002&partnerID=40&md5=90272e5893001c6eb88724a08e63dc38","The configuration bitstream is a persistent source of vulnerability in FPGA designs, and thus FPGA vendors have implemented bitstream encryption. A number of attacks on these countermeasures have been demonstrated including direct probing of the configuration storage cells, side-channel attacks on the decryption blocks, and attacks on the scan chain. Thus, we present an FPGA design that never stores the configuration data in the clear, even at the lowest level of the hardware. We deeply hardware entangle the reconfigurable logic and interconnect by one time pad encrypting the bitstream using a PUF response. By leveraging recent work in high performance, high density, high reliability, and low power PUF design, we tightly integrate a PUF bit with every configuration bit. This has significant security benefits including high resistance to probing attacks and unique per-die configuration bitstreams, while only requiring minor modification of the FPGA design. Based on overheads from a PUF implementation in an industrial 65 nm bulk CMOS process, we simulate such an FPGA design and achieve modest overheads in power, area, and performance across multiple security-focused benchmark applications, as well as various MCNC benchmark circuits from a variety of real applications. © 2019","Bitstream encryption; FPGA security; PUFs; Side-channel attacks"
"A privacy preserving location service for cloud-of-things system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055094956&doi=10.1016%2fj.jpdc.2018.09.005&partnerID=40&md5=37de514def7c0a93fe443ae658f30ead","The natural characteristics of Location Based Services (LBS) cause potential threats to location privacy. Users need to send their current locations to get the service, which may lead to the leakage of their location privacy. An effective way to protect user's location privacy is to use an imprecise location and send this region to the Cloud-of-things system to replace his real location. When user moves and sends continuous queries, the user needs to transmit the region to the server continuously and an attacker is able to infer the real location from the overlapping regions. In this paper, we propose a novel location privacy pre-protection method for cloud-of things system to preserve user's trajectory privacy. User's moving behaviors are analyzed through Mobility Markov chain. The proposed location cloaking algorithm enlarges the small area to satisfy the user's privacy requirements, so that the location trajectory privacy in the environment of cloud-of-things system is addressed efficiently. Experimental results show the performance of our method in terms of the number of moving steps, the cloaking threshold value, in addition to the user chosen anonymity value. © 2018 Elsevier Inc.","Cloud of Things (CoT); Location based service; Privacy preservation; Trajectory privacy"
"Auto-tuned OpenCL kernel co-execution in OmpSs for heterogeneous systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057329099&doi=10.1016%2fj.jpdc.2018.11.001&partnerID=40&md5=c1c6be57303c755f8b45f9016dc5ccd9","The emergence of heterogeneous systems has been very notable recently. The nodes of the most powerful computers integrate several compute accelerators, like GPUs. Profiting from such node configurations is not a trivial endeavour. OmpSs is a framework for task based parallel applications, that allows the execution of OpenCl kernels on different compute devices. However, it does not support the co-execution of a single kernel on several devices. This paper presents an extension of OmpSs that rises to this challenge, and presents Auto-Tune, a load balancing algorithm that automatically adjusts its internal parameters to suit the hardware capabilities and application behavior. The extension allows programmers to take full advantage of the computing devices with negligible impact on the code. It takes care of two main issues. First, the automatic distribution of datasets and the management of device memory address spaces. Second, the implementation of a set of load balancing algorithms to adapt to the particularities of applications and systems. Experimental results reveal that the co-execution of single kernels on all the devices in the node is beneficial in terms of performance and energy consumption, and that Auto-Tune gives the best overall results. © 2018 Elsevier Inc.","Co-execution; Heterogeneous systems; OmpSs programming model; OpenCL"
"Using hashing and lexicographic order for Frequent Itemsets Mining on data streams","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057490263&doi=10.1016%2fj.jpdc.2018.11.002&partnerID=40&md5=2d89c1630dede84413b35e56916350f5","Frequent Itemsets Mining is a Data Mining technique that has been employed to extract useful knowledge from datasets and, more recently, also from data streams. Data streams are unbounded and infinite flows of data arriving at high rates which cannot be stored for off-line processing; therefore, proposed algorithms for Frequent Itemsets Mining approaches from datasets cannot be used straightforwardly for Frequent Itemsets Mining from data streams. Frequent Itemsets Mining is a compute intensive task, hence developing custom hardware-based architectures to speed up this process is an active research topic. This paper introduces an algorithm for a hardware-based Frequent Itemsets Mining on data streams that uses the top-k frequent 1-itemsets detection as preprocessing. The received transactions are handled using hash functions, and the lexicographic order of items is used for obtaining frequent itemsets. The proposed algorithm is focused on discovering frequent itemsets in data streams composed of short transactions in large alphabets. Experimental results demonstrate that the proposed algorithm outperforms the processing time of the state-of-the-art algorithms used as the baseline. © 2018 Elsevier Inc.","Data streams; Frequent Itemsets Mining; Hashing; Lexicographic order; Reconfigurable hardware"
"Publish/subscribe based multi-tier edge computational model in Internet of Things for latency reduction","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060641744&doi=10.1016%2fj.jpdc.2019.01.004&partnerID=40&md5=4525cba9e772bb9988e034817d19e1a4","Most of the Internet of Things (IoT) applications are time sensitive and require low latency, as a millisecond delay can affect a huge production in the machine-dependent environment. Due to the rapid growth of the internet connected devices, it is very difficult to handle this huge processing load in the Cloud and traffic in the network. Data communication protocols in vogue are based on IoT devices connected to Cloud. But with the introduction of edge/Fog computing in IoT where all processing happens at the edge, there is a need to revisit the existing IoT protocols. The existing protocols were not designed with edge/Fog computing in mind. The most predominantly used IoT protocol is Message Queue Telemetry Transport (MQTT) protocol which is based on publish/subscribe model. This paper proposes software-defined multi-tier edge computing model by modifying the existing MQTT protocol for edge computing with a remote broker in Fog node and main broker in Cloud. A mathematical model is proposed towards computing the performance metrics of the proposed system. The performance of the proposed system is been compared with the traditional MQTT based IoT model. Python-Virtual Fog (PVFOG) simulator has been used to measure the performance in terms of service latency, transmission latency, processing latency, and packet send to Cloud. The experimental results show that the proposed system outperformed the traditional MQTT based IoT Model. © 2019 Elsevier Inc.","Fog computing; Internet of Things; Latency; MQTT; Multi-tier Fog computational model; PVFOG simulator; Remote broker; Service latency; Transmission latency"
"Minimizing energy consumption with reliability goal on heterogeneous embedded systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060943290&doi=10.1016%2fj.jpdc.2019.01.006&partnerID=40&md5=4138ff51684c8c323512805d8c6decba","The embedded systems generally require to be low-powered and highly reliable. In order to achieve the low-power design goal, dynamic voltage frequency scaling (DVFS) technique has been widely employed in various embedded application scenarios. However, DVFS reduces execution frequency, which increases transient faults of the processor dramatically. As a result, the reliability of the application will be severely reduced. In this paper, we aim at minimizing energy consumption with reliability goal for parallel application on heterogeneous embedded systems. Since the reliability of the application is the product of the reliability of all the tasks that belong to the application, the reliability goal of the application is transformed into the reliability goal of each task. Considering that some systems may not support DVFS techniques, two methods are proposed to transform the reliability goal of the application into each task for non-DVFS and DVFS, respectively. Based on the reliability goal transformation methods, two energy-efficient scheduling algorithms with the reliability goal are designed. Experiments with real parallel applications demonstrate that the proposed algorithms have significant improvements in energy efficiency compared with the state-of-the-art algorithms. © 2019 Elsevier Inc.","DAG-based parallel application; DVFS technique; Energy-efficient; Heterogeneous embedded system; Reliability goal"
"Analyzing and optimizing the performance and energy efficiency of transactional scientific applications on large-scale NUMA systems with HTM support","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060338221&doi=10.1016%2fj.jpdc.2018.12.007&partnerID=40&md5=4d157228ae1b1c07387b9db44925735f","Hardware transactional memory (HTM) is widely supported by commodity processors. While the effectiveness of HTM has been evaluated based on small-scale multi-core systems, it still remains unexplored to quantify the performance and energy efficiency of HTM for scientific workloads on large-scale NUMA systems, which have been increasingly adopted to high-performance computing. To bridge this gap, this work investigates the performance and energy-efficiency impact of HTM on scientific applications on large-scale NUMA systems. Specifically, we quantify the performance and energy efficiency of HTM for scientific workloads based on the widely-used CLOMP-TM benchmark. We then discuss a set of generic software optimizations, which effectively improve the performance and energy efficiency of transactional scientific workloads on large-scale NUMA systems. Further, we present case studies in which we apply a set of the performance and energy-efficiency optimizations to representative transactional scientific applications and investigate the potential for high-performance and energy-efficient runtime support. © 2019 Elsevier Inc.","Energy efficiency; Hardware transactional memory; High performance; Non-uniform memory access; Scientific applications"
"CFPA: Congestion aware, fault tolerant and process variation aware adaptive routing algorithm for asynchronous Networks-on-Chip","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063051463&doi=10.1016%2fj.jpdc.2019.03.001&partnerID=40&md5=2fe336dd47d837ad263dc12c977339d5","Delays caused by congestion, faults and process variation (PV) degrade networks-on-chip (NoC) performance. A congestion aware, fault tolerant and process variation aware adaptive routing algorithm (CFPA) is introduced for congested and faulty asynchronous NoCs. The proposed routing algorithm maintains two routing tables to determine the packet path: one for routing directions based on propagation delay (including PV delay), and the other to keep track of the queuing delays at each router port. The queuing delay is used as an indication for congestion. The proposed routing tables store multiple paths to every destination via all polar directions, which makes CFPA a fault tolerant algorithm in case of path failures. The proposed algorithm is verified against other popular routing algorithms for NoCs with different topologies and network dimensions. On average, CFPA enhances the NoC throughput by 60% compared to the recently proposed routing algorithms. With CFPA, the impact of faults on NoC throughput is alleviated by 48%. In addition, the average delay of messages routed using CFPA is shorter than that of other algorithms by (26∼75)% under process variation conditions. Furthermore, the proposed algorithm minimizes the impact of PV on NoC throughput to less than 5% of the nominal throughput for mesh topology. © 2019 Elsevier Inc.","Congestion; Fault tolerant; Network-on-Chip; NoC routing algorithm; Process variation; Throughput"
"Energy trading with dynamic pricing for electric vehicles in a smart city environment","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049727106&doi=10.1016%2fj.jpdc.2018.06.010&partnerID=40&md5=a3dfb297238d51e5a544c545eac90718","Smart cities are equipped with latest technologies to provide sustainable and economical services to their citizens. With an increase in carbon emissions, the popularity of electric vehicles (EVs) is a major step towards environment friendly smart cities. However, energy trading with dynamic pricing is one of the major challenges for EVs in a smart city environment. Most of the existing solutions reported in the literature do not consider energy trading with an aim to maximize benefits to EVs in terms of their demand satisfaction. EVs have to pay higher price as they have limited knowledge about the location and pricing policy of the charging stations (CSs). Moreover, they have to wait for long time till the required amount of energy is met from the CSs. To address these issues, a multi-leader multi-follower Stackelberg game for energy trading is proposed by assuming EVs as the consumers and CSs as energy providers. Using this concept, a dynamic pricing scheme known as multi parameter pricing scheme is designed by taking parameters such as — electricity usage, time-of-use, location, and type of EVs. Two cases of Stackelberg Game are considered in the proposal- (i) EVs as leaders and CSs as followers, and (ii) CSs as leaders and EVs as followers. The proposed scheme is evaluated using three types of vehicles with respect to performance metrics such as (a) price of energy (b) utility function and (c) satisfaction factor. The results obtained clearly depict the superior performance of the proposed scheme in comparison to the existing schemes. © 2018 Elsevier Inc.","Charging stations; Dynamic pricing; Electric vehicles; Smart city; Smart grid; Stackelberg game"
"GPU accelerated t-distributed stochastic neighbor embedding","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065059323&doi=10.1016%2fj.jpdc.2019.04.008&partnerID=40&md5=81d659d3cf6fa0df8982f85b82553252","Modern datasets and models are notoriously difficult to explore and analyze due to their inherent high dimensionality and massive numbers of samples. Existing visualization methods which employ dimensionality reduction to two or three dimensions are often inefficient and/or ineffective for these datasets. This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of t-Distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and models. t-SNE-CUDA significantly outperforms current implementations with 15-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for the first time, large scale visualizations of modern computer vision datasets such as ImageNet, as well as larger NLP datasets such as GloVe. From these new visualizations, we can draw a number of interesting conclusions. In addition, the performance on machine learning datasets allows us to compute t-SNE embeddings in close to real time, and we explore the applications of such fast embeddings in the domain of importance sampling for neural network training. © 2019 Elsevier Inc.","Applications; CUDA; Embedding; GPU computing; Parallel computing; T-SNE"
"ACAS: An anomaly-based cause aware auto-scaling framework for clouds","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059839019&doi=10.1016%2fj.jpdc.2018.12.002&partnerID=40&md5=506d81e03d5f00f18935269cdd0a92e9","Cloud computing as a model to deliver distributed resource and services on the pay-as-you-go policy has become increasingly popular for all organizations including industry. However, the inherent dynamicity in this environment makes it prone to various types of performance problems which introduce many challenges in the area of distributed resource management. Advances in the big data learning approaches can bring the opportunity for a data aware dynamic management of resources in the cloud. The collected data from the performance indicators of the system can be a valuable source of information to identify unusual behaviors in the resource consumptions or application performance. Different types of problems can cause the performance degradations at VM or system level. System administrators are overwhelmed with the huge amount of data to be analyzed to find the problems and overall health of the system. In this paper, we argue that a better selection of dynamic resource scaling policies can be employed for better performance by predicting the anomalies in the system and narrowing down the possible cause of the anomaly to one of the attributes of the system. Therefore, we propose a 2-level cause aware auto-scaling framework which leverages two types of resource management solutions, horizontal and vertical, as the corrective actions when the performance is degraded. We show the effectiveness of vertical scaling strategy as a quick solution for cases that a VM is exposed to some type of the local anomaly, while the horizontal scaling solutions can be used for system wide anomaly to add new VMs in the system. Moreover, our data analysis module can predict anomalies to give sufficient time to the scaling system to make an effective scaling decision. The proposed unsupervised anomaly detection module leverages a new updating strategy for renewing the models which considers the changes in the state of the system to reduce the overhead of recurrent model trainings. We have performed a comparison of the proposed framework with an approach which is used by several popular cloud providers to show the advantage of mixing the multi-level auto-scaling with the knowledge of anomaly detection analysis in resolving performance problems in the cloud. © 2018 Elsevier Inc.","Anomaly detection; Cloud computing; Isolation-trees; Performance management; Vertical scaling"
"Haste makes waste: The On–Off algorithm for replica selection in key–value stores","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064329289&doi=10.1016%2fj.jpdc.2019.03.017&partnerID=40&md5=a6ee042d198cb94faac8ae00ff063768","                             In current large-scale distributed key–value stores, the tail latency of the key–value accesses generated by end-user requests is crucial to the response time of these requests. To cut the tail latency, the replica selection algorithm, which selects a replica server for each key by a client and thus determines the latency of each key–value access, is crucial. Naturally, all of current replica selection algorithms send keys out immediately to reduce the tail latencies of key–value accesses. In this paper, we find that sending out keys in haste makes waste of the chance to select a better replica server certain time later, and suggest to await for a better replica server available when all current replica servers are bad. To realize this idea, we develop the On–Off algorithm, which recognizes bad replica servers according to the feedback information and put them into the OFF state. Special attention is paid on the time interval replica servers stay at the OFF state. Obviously, the On–Off algorithm adds awaiting time at clients, but can greatly reduce the dominated queuing delays at replica servers. In total, the On–Off algorithm improves the 99                             th                              latency by about 29% under the default simulation configuration and outperforms the C3 algorithm proposed recently under kinds of scenarios.                          © 2019 Elsevier Inc.","Key–value stores; On–Off; Replica selection; Tail latency"
"Towards evaluation of cloud ontologies","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059515352&doi=10.1016%2fj.jpdc.2018.12.005&partnerID=40&md5=6d6f698faa7b0209e509c8d33e0a2996","Many enterprises consider that cloud computing will continue to play a key role due to its ability to deliver various types of on-demand IT services, and according to customer needs. Although cloud technology has characteristics that distinguish it from other technologies, unfortunately, services in this technology suffer from the heterogeneity issue. As a result, cloud services have many challenges, such as service description, interoperability, and service discovery. Many studies have contributed dealing with such challenges using ontology, as it can participate as a mapping layer to present such services in a unified description. Cloud ontologies included in these studies can be classified into three types of ontologies; ontologies that focus on representing functional features of cloud services, ontologies that focus on non-functional features, and ontologies that focus on both function and non-function features. These ontologies are not unified in representing such features, where they agree in some features and differ in others. By using techniques developed to evaluate ontologies related to the other technologies, it is difficult to decide which cloud ontology captures cloud services in the right way. We have noticed that there is not a framework or a benchmark to evaluate the cloud ontologies. In this survey, the current cloud ontologies are analyzed to identify a set of observations that represent their strengths and weaknesses. According to these observations, a set of suggested rules are introduced and verified to be considered during the construction or the evaluation of cloud ontologies. The work in this paper can be considered as the fundamental step for developing a formal framework to automatically evaluate cloud ontologies. © 2018 Elsevier Inc.","Cloud computing; Cloud ontology; Evaluation; Heterogeneity"
"Towards predictable performance via two-layer bandwidth allocation in cloud datacenter","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059321212&doi=10.1016%2fj.jpdc.2018.11.013&partnerID=40&md5=31d30202750dbdf09945abb01d2e2c0f","In today's production-grade cloud datacenters, cloud service providers do not offer any bandwidth guarantee between VMs, which results in unpredictable performance of tenants’ applications. The research community has recognized this problem; however, existing solutions to bandwidth allocation fail to take into consideration tenants’ request for bandwidth and the actual bandwidth usage of applications simultaneously, which leads to a waste of bandwidth resources or unpredictable performance. To address these issues, we present SpongeNet, a bandwidth allocation solution that consists of three components through two layers—static bandwidth guarantees at the tenant layer and a dynamic rate allocation at the application layer to realize predictable performance. The first component, named FGVC model, is a network abstraction model that provides a simple, accurate and flexible way for tenants to specify network requirements and achieve high utilization through bandwidth saving. The second component is a two-phase VM placement algorithm that provides optimal combinations of ordering policies and dispatching policies to meet multiple goals. The third component, named E–F runtime mechanism, can achieve the fairness between guaranteed and unguaranteed tenants in utilizing the unused bandwidth resources. Extensive simulations based on real application traces and 3-level tree topology show that SpongeNet enhances bandwidth saving when compared to the state-of-the-art solutions (e.g., the Oktopus system), and significantly improves the throughput rate by 18% and response time by 92%. With a small prototype implementation on a 7-server testbed, we demonstrate that SpongeNet provides fair work-conserving bandwidth guarantee among all tenants, even in extreme cases. © 2018 Elsevier Inc.","Bandwidth guarantee; Cloud computing; Datacenter networks; Rate allocation; VM placement"
"Energy efficient torus networks with on/off links","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064267424&doi=10.1016%2fj.jpdc.2019.03.015&partnerID=40&md5=5745f82f0d8158dbf6c5c034c783a0d3","Future exascale computing systems will require energy and performance efficient interconnection networks to respond to the high data movement demands of new applications, such as those coming from big-data and artificial intelligence areas. The network structure plays a major role in the overall interconnect performance, for this reason torus is a common topology used in the current largest supercomputers. There are several proposals to improve energy efficiency of interconnection networks. However, few works combine both energy and performance, and sometimes they are treated as opposed issues. In this paper, we try to determine which torus network configuration offers the best performance/energy ratio when high-radix switches are used to build the interconnect system. The performance/energy evaluation has been performed by trace-driven simulation under realistic scenarios, where several mixes of scientific applications share a supercomputer system and are scheduled to be executed with the available resources at each moment. © 2019 Elsevier Inc.","Energy and performance evaluation; Energy consumption; Interconnection networks; n-dimensional torus; Port aggregation; Power consumption"
"Fast block distributed CUDA implementation of the Hungarian algorithm","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064282306&doi=10.1016%2fj.jpdc.2019.03.014&partnerID=40&md5=787d02ad83372b38328cfc023845d837","The Hungarian algorithm solves the linear assignment problem in polynomial time. A GPU/CUDA implementation of this algorithm is proposed. GPUs are massive parallel machines. In this implementation, the alternating path search phase of the algorithm is distributed by several blocks in a way to minimize global device synchronization. This phase is very important and has a big contribution to the execution time. Other advanced features also implemented are: parallel graph traversal; the parallel detection of multiple alternating paths in a single iteration; a simplified and fast matrix compression that stores the zeros of the slack matrix, resulting in very fast graph traversal; highly optimized reductions for the initial slack matrix calculation and update. This results in a fast implementation for moderate size problems. © 2019 Elsevier Inc.","GPU; Hungarian algorithm; Linear assignment problem"
"Thermal-aware processing-in-memory instruction offloading","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064859772&doi=10.1016%2fj.jpdc.2019.03.005&partnerID=40&md5=62e190928880db8336baf7d716d2aa92","With the advent of die stacking technology and big data applications, Processing-in-memory (PIM)is regaining attention as a promising technology for improving performance and energy efficiency. Although various PIM techniques have been proposed in recent studies for effectively offloading computation from the host, the thermal impacts of PIM offloading have not been fully explored. This paper investigates the thermal constraints of PIM and proposes techniques to enable thermal awareness for efficient PIM offloading. To understand the thermal effects of 3D-stacked designs, we measure the temperature of a real Hybrid Memory Cube (HMC)prototype and observe that compared to conventional DRAM, HMC reaches a significantly higher operating temperature, which causes thermal shutdowns with a passive cooling solution. Even with a commodity-server cooling solution, when in-memory processing is highly utilized, HMC fails to maintain the temperature of the memory dies within the normal operating range. In this paper, we propose a collection of software- and hardware-based techniques to enable thermal-aware PIM offloading by controlling the intensity of PIM offloading at runtime. Our evaluation results show that the proposed techniques achieve up to 1.4 × and 1.37× speedups compared to non-offloading and naïve offloading scenarios. © 2019 Elsevier Inc.","Hybrid Memory Cube; Processing-in-memory; Thermal"
"An efficient scheme for applying software updates in pervasive computing applications","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061192662&doi=10.1016%2fj.jpdc.2019.01.010&partnerID=40&md5=fd7e96919a0fa69e67fad2825c7e3420","The Internet of Things (IoT) offers a vast infrastructure of numerous interconnected devices capable of communicating and exchanging data. Pervasive computing applications can be formulated on top of the IoT involving nodes that can interact with their environment and perform various processing tasks. Any task is part of intelligent services executed in nodes or the back end infrastructure for supporting end users’ applications. In this setting, one can identify the need for applying updates in the software/firmware of the autonomous nodes. Updates are extensions or patches significant for the efficient functioning of nodes. Legacy methodologies deal with centralized approaches where complex protocols are adopted to support the distribution of the updates in the entire network. In this paper, we depart from the relevant literature and propose a distributed model where each node is responsible to, independently, initiate and conclude the update process. Nodes monitor a set of metrics related to their load and the performance of the network and through a time-optimized scheme identify the appropriate time to conclude the update process. We report on an infinite horizon optimal stopping model on top of the collected performance data. The aim is to make nodes capable of identifying when their performance and the performance of the network are of high quality to efficiently conclude the update process. We provide specific formulations and the analysis of the problem while extensive simulations and a comparison assessment reveal the advantages of the proposed solution. © 2019","Internet of Things; Optimal stopping theory; Pervasive computing; Updates management"
"The security of machine learning in an adversarial setting: A survey","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064004174&doi=10.1016%2fj.jpdc.2019.03.003&partnerID=40&md5=3d36b46fc7ecc909ad53198e65da5b30","Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models. © 2019 Elsevier Inc.","Adversarial attack; Adversarial example; Adversarial setting; Machine learning; Security model"
"Malicious code detection based on CNNs and multi-objective algorithm","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063761381&doi=10.1016%2fj.jpdc.2019.03.010&partnerID=40&md5=fab0d4699796a3a708d5cf5d46af7249","An increasing amount of malicious code causes harm on the internet by threatening user privacy as one of the primary sources of network security vulnerabilities. The detection of malicious code is becoming increasingly crucial, and current methods of detection require much improvement. This paper proposes a method to advance the detection of malicious code using convolutional neural networks (CNNs) and intelligence algorithm. The CNNs are used to identify and classify grayscale images converted from executable files of malicious code. Non-dominated Sorting Genetic Algorithm II (NSGA-II) is then employed to deal with the data imbalance of malware families. A series of experiments are designed for malware image data from Vision Research Lab. The experimental results demonstrate that the proposed method is effective, maintaining higher accuracy and less loss. © 2019 Elsevier Inc.","CNN; Deep learning; Imbalance data; Malicious code; NSGA-II"
"Bayesian inference of private social network links using prior information and propagated data","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057730304&doi=10.1016%2fj.jpdc.2018.11.003&partnerID=40&md5=a67a1ad1580c64428693d610537069a5","Inferring network structure has many applications ranging from viral marketing to privacy forensics, infection prevention and information feed ranking. In complex or social networks some agents or users tend to keep their connections hidden. The main focus of this research is inferring hidden and invisible network connections through the data collected from propagations or cascades with the help of some other rich information e.g. comments, profile information, joint photos or interactions of users. We analyze the information propagation mechanism based on a two-phase algorithm. Traversing the first phase relies on using the social network data set in order to estimate the friendship probability among its users. Performing this procedure would result in generating a primitive friendship graph which includes the probability of every two users being effectively connected. The propagation times of one or more cascades are fed into a Maximum A Posteriori (MAP) estimator to find the active hidden links using a Bayesian inference method. The algorithm was evaluated on a real network graph extracted from part of Facebook. Mutual friendships were used as the prior information for test purposes. The results showed that it is possible to achieve very high accuracy with limited cascades if the ratio of the nodes with hidden friends is not high. © 2018 Elsevier Inc.","Cascading; Facebook; Information propagation; Maximum A posteriori; Network inference; Privacy; Social networks"
"Cataloging the visible universe through Bayesian inference in Julia at petascale","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061068145&doi=10.1016%2fj.jpdc.2018.12.008&partnerID=40&md5=17ab7970b9fcf1947369203f0bafbde0","A key task in astronomy is to locate astronomical objects in images and to characterize them according to physical parameters such as brightness, color, and morphology. This task, known as cataloging, is challenging for several reasons: many astronomical objects are much dimmer than the sky background, labeled data is generally unavailable, overlapping astronomical objects must be resolved collectively, and the datasets are enormous – terabytes now, petabytes soon. In this work, we infer an astronomical catalog from 55 TB of imaging data using Celeste, a Bayesian variational inference code written entirely in the high-productivity programming language Julia. Using over 1.3 million threads on 650,000 Intel Xeon Phi cores of the Cori Phase II supercomputer, Celeste achieves a peak rate of 1.54 DP PFLOP/s. Celeste is able to jointly optimize parameters for 188 M stars and galaxies, loading and processing 178 TB across 8192 nodes in 14.6 min. To achieve this, Celeste exploits parallelism at multiple levels (cluster, node, and thread) and accelerates I/O through Cori's burst buffer. Julia's native performance enables Celeste to employ high-level constructs without resorting to hand-written or generated low-level code (C/C++/Fortran) while still achieving petascale performance. © 2019 Elsevier Inc.","Astronomy; Bayesian; Distributed optimization; High-performance computing; Julia; Variational inference"
"Approximation algorithm for the energy-aware profit maximizing problem in heterogeneous computing systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056617792&doi=10.1016%2fj.jpdc.2018.10.013&partnerID=40&md5=aa865f00de982bacf282944777f1f194","Trade-offs between energy and performance are important for energy-aware scheduling. Recently, a novel model, called energy-aware profit maximizing scheduling problem (EAPM), which combines energy and makespan into the objective of maximizing the profit per unit of time has been proposed. The user pay a given price to have a bag-of-tasks processed and the objective is to maximize the profit per unit time. In this study, we design a polynomial-time algorithm for the EAPM problem. The execution time of our algorithm is polynomial in the number of task types which is an improvement over the previous algorithm, whose execution time is polynomial in the number of tasks. Moreover, we demonstrate that the approximation ratio of our algorithm is close to 2 for a special case, which may be the best result we can obtain. Experimental results show that our algorithm can produce a feasible solution with better objective value than the previous algorithm. © 2018","Approximation algorithm; Bag-of-tasks; High performance computing; Load balancing; Task scheduling"
"Parallel multi-swarm PSO strategies for solving many objective optimization problems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058717102&doi=10.1016%2fj.jpdc.2018.11.008&partnerID=40&md5=1a756c9761750c1bb1a72d85acc89c70","In this work we present two parallel PSO strategies based on multiple swarms to solve MaOPs (Many-Objective Optimization Problems). The first strategy is based on Pareto dominance and the other is based on decomposition. Multiple swarms execute on independent processors and communicate using broadcast on a fully connected network. We investigate the impact of using both synchronous and asynchronous communication strategies for the decomposition-based approach. Experimental results were obtained for several benchmark problems. It is possible to conclude that the parallelization has a positive effect on the convergence and diversity of the optimization process for problems with many objectives. However, there is no single strategy that is the best results for all classes of problems. In terms of scalability, for higher numbers of objectives the parallel algorithms based on decomposition are always either the best or present comparable results with the Pareto approach. There are exceptions, but only when the problem itself has discontinuities on the Pareto Front. © 2018 Elsevier Inc.","Evolutionary computing; Many-objective optimization problems; Multi-swarm PSO"
"Collaborative Compaction Optimization System using Near-Data Processing for LSM-tree-based Key-Value Stores","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065403684&doi=10.1016%2fj.jpdc.2019.04.011&partnerID=40&md5=23018fca866bbd36291f05428e0a308a","Log-structured merge tree (LSM-tree) based key–value stores are widely employed in large-scale storage systems. In compaction, high-level sorted string table files (i.e., SSTables) are merged with low-level overlapping key ranges and sorted for data queries. However, the compaction process incurs write amplification, which degrades system performance particularly under update-intensive workloads. Current optimizations mostly focus on reducing the overload of compaction in the host but rarely exploit parallelisms between the host and the device to fully utilize computing resources in the entire system for optimizing compaction performance. In this study, we propose Co-KV, a Collaborative Key-Value store, to improve compaction performance in LSM-tree-based key–value stores. Co-KV is based on a near-data processing (i.e., NDP) model-enabled storage device. Co-KV exhibits the following advantages: (1) it reduces write amplification and host-side CPU costs using a compaction offloading scheduling between a host computer and an NDP-enabled storage device; (2) it relieves the overload associated with data transfer between the host and the storage device; and (3) it improves the compaction of LSM-tree based key–value stores under update-intensive workloads. We employed an Open Ethernet Driver (OED), which is a real-world NDP platform as the testbed for our experiments. Extensive db_bench evaluations demonstrate that Co-KV achieves overall throughput improvements of approximately 1.75x, CPU cost reductions of approximately 68.1%, and write amplification reductions by up to 50.0% over the state-of-the-art LevelDB. Under YCSB workloads, Co-KV increases throughput by 1.7x ∼ 1.9x while decreasing the write amplification and average latency by up to 50.0% and 46.3%, respectively. © 2019 Elsevier Inc.","Compaction; Key-value store; LSM-tree; Near-data processing; Offloading; Partitioning"
"Comparative assessment of GPGPU technologies to accelerate objective functions: A case study on parsimony","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059535225&doi=10.1016%2fj.jpdc.2018.12.006&partnerID=40&md5=d3055e68afb25a3766c724f56b159a23","Objective functions provide measurements of solution quality that represent the core calculations required to tackle NP-hard optimization problems. Since their complexity keeps growing with the introduction of more realistic data, research efforts have turned their interest into the proposal of efficient objective function implementations that take advantage of potential parallelism. This work explores GPGPU technologies to accelerate objective functions, considering as a case study the parallelization of phylogenetic parsimony calculations from DNA data. We undertake the comparative evaluation of different GPU programming models and architectures, highlighting the benefits and drawbacks of each approach through experimentation on six real-world biological datasets. Experimental results shed light on the strong relationship between the characteristics of the input data and the effective utilization of GPU resources. Furthermore, comparisons with other parallel architectures and methods point out how current and future optimization scenarios can benefit from the use of accurate, efficient GPU approaches. © 2018 Elsevier Inc.","Bioinformatics; GPGPU; Objective functions; Parallel algorithms"
"LDA classifier monitoring in distributed streaming systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055042693&doi=10.1016%2fj.jpdc.2018.09.017&partnerID=40&md5=cfef82251a1822f146cccab801b5fbb0","An important problem in real systems for mining data streams is to detect changes in the dynamic model describing the temporal data. Such changes indicate that the underlying data has undergone a transition which may well require attention. A distributed setting poses one of the main challenges in this type of change detection. In a distributed setting, model training requires centralizing the data from all nodes (hereafter, synchronization), which is very costly in terms of communication. In order to minimize communication, a monitoring algorithm should be executed locally at each node, while preserving the validity of the global model (that is, the model that will be computed if a synchronization takes place). To achieve this goal, we propose the first communication-efficient algorithm for monitoring a classification model over distributed, dynamic data streams. While the approach is general, here we concentrate on Linear Discriminant Analysis (LDA), a popular method for classification and dimensionality reduction in many fields. We mainly apply tools from the realms of linear algebra and multi-variate analysis in order to solve the problem at hand. The resulting implementation is quite straightforward. The emphasis of this work is not on solving the distributed optimization problem that corresponds to finding a classifier over the distributed data; instead, we continuously monitor the current classifier to check that it still fits the data. In addition to the theoretical guarantee of the proposed monitoring algorithm, we demonstrate how it reduces communication volume by up to two orders of magnitude (compared to synchronization in every round) on synthetic data as well as three real datasets from different worlds of content. Our approach monitors the classification model itself as opposed to its misclassifications, which makes it possible to detect the change before misclassifications occur. © 2018 Elsevier Inc.","Distributed monitoring; Linear discriminant analysis"
"Optimizing sparse tensor times matrix on GPUs","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053395367&doi=10.1016%2fj.jpdc.2018.07.018&partnerID=40&md5=f56b48d47df2b05c694907eb14b364c2","This work optimizes tensor-times-dense matrix multiply (Ttm) for general sparse and semi-sparse tensors on CPU and NVIDIA GPU platforms. Ttm is a computational kernel in tensor methods-based data analytics and data mining applications, such as the popular Tucker decomposition. We first design an in-place sequential SpTtm to avoid explicit data reorganizing between a tensor and a matrix in its conventional approach. We further optimize SpTtm on NVIDIA GPU platforms. Five approaches including employing fine thread granularity, arranging coalesced memory access, rank blocking, and using fast GPU shared memory are developed for GPU-SpTtm. We also optimize semi-sparse tensor-times-dense matrix multiply (SspTtm) to take advantage of the inside dense sub-structures. The optimized SpTtm and SspTtm are applied to Tucker decomposition to improve its overall performance. Our sequential SpTtm is 3–120× faster than the SpTtm from Tensor Toolbox library. GPU-SpTtm obtains 6–19× speedup on NVIDIA K40c and 23–67× speedup on NVIDIA P100 over CPU-SpTtm respectively. Our GPU-SpTtm is 3.9× faster than the state-of-the-art GPU implementation. Our SspTtm implementations outperform SpTtms by up to 4.5×, which handles the input semi-sparse tensor in a general way. Tucker decomposition achieves up to 3.2× speedup after applying the optimized Ttms. The code will be publicly released in ParTI! library: https://github.com/hpcgarage/ParTI. © 2018 Elsevier Inc.","GPU; Irregular algorithms; Sparse tensors; Tensor decomposition"
"Topology and computational-power aware tile mapping of perfectly nested loops with dependencies on distributed systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063519211&doi=10.1016%2fj.jpdc.2019.03.002&partnerID=40&md5=c6fefa33d273e4e1a1b432470b1d2fe1","Nested loops are main source of the parallelism in many scientific applications. Partitioning the iteration space of nested loops with data dependencies into tiles and assigning them to processing nodes for parallel execution is essential for achieving high performance. Although most of the previous work focused on tiling on fully connected homogeneous distributed systems, some studies have been devoted to tiling on partially connected distributed systems. In this paper, we address the parallelization of perfectly nested loops with dependencies on partially connected heterogeneous distributed systems and present a topology and computational-power aware tile mapping. This work aims to take into account not only the node's computational power when tiling iteration space of nested loops but also the exploitation of the network topology when mapping tiles to processing nodes. This approach allows minimizing the parallel execution time by improving the load balancing and minimizing the communication costs. We demonstrate the performance of proposed method by comparing it with the computational-power aware tile mapping and the topology aware tile mapping. The experimental results show that the proposed method improves the parallel execution time by up to 62% and 28% compared with the computational-power aware tile mapping and the topology aware tile mapping, respectively. © 2019 Elsevier Inc.","Computational-power aware tile mapping; Distributed systems; Nested loop; Parallelization; Topology aware tile mapping"
"Scheduling opportunities for asymmetrically reliable caches","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2019.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060194747&doi=10.1016%2fj.jpdc.2019.01.005&partnerID=40&md5=7126d6b783f0f3c098530e7460f6b97c","Modern systems become more vulnerable to soft errors with technology scaling. Providing fault tolerance strategies on all structures in a system may lead to high energy consumption. Our framework with asymmetrically reliable caches with at least one protected core and several unprotected cores dynamically assigns the software threads executing critical code fragments to the protected core(s) with the FCFS-based algorithm. The framework can provide good reliability, performance, and power consumption trade-offs compared with the fully protected and unprotected systems. However, FCFS-based scheduling algorithm may degrade the system performance and unfairly slow down applications for some workloads. In this paper, a set of scheduling algorithms is proposed to improve both the system performance and fairness perspectives. Various static priority techniques that require preliminary information about the applications (such as their execution order, cache usage, number of requests sent to the protected core(s), and total burst time spent on the protected core(s)) are implemented and evaluated. On the other hand, dynamic priority techniques that target to equalize the total time spent of applications on the protected core(s) or the progress of the applications’ requests are presented. Extensive evaluations using multi-application workloads validate significant improvements of our static and dynamic priority scheduling techniques on system performance and fairness over the FCFS algorithm. © 2019 Elsevier Inc.","Asymmetric cores; Fairness; Performance evaluation; Reliability; Scheduling algorithms; Selective protection"
"Modeling and analysis of epidemic spreading on community networks with heterogeneity","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047095464&doi=10.1016%2fj.jpdc.2018.04.009&partnerID=40&md5=62430eee88061788e2f36b1bbb3043df","A large number of real world networks exhibit community structure, and different communities may often possess heterogeneity. In this paper, considering the heterogeneity among communities, we construct a new community network model in which the communities show significant differences in average degree. Based on this heterogeneous community network, we propose a novel mathematical epidemic model for each community and study the epidemic dynamics in this network model. We find that the location of the initial infection node only affects the spreading velocity and barely influences the epidemic prevalence. And the epidemic threshold of entire network decreases with the increase of heterogeneity among communities. Moreover, the epidemic prevalence increases with the increase of heterogeneity around the epidemic threshold, while the converse situation holds when the infection rate is much greater than the epidemic threshold. © 2018 Elsevier Inc.","Community structure; Epidemic dynamics; Heterogeneity; Mean-field theory"
"A wearable sensor-based activity prediction system to facilitate edge computing in smart healthcare system","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054007010&doi=10.1016%2fj.jpdc.2018.08.010&partnerID=40&md5=3314bbacb13c4bd3f07b76004924d090","An increase in world population along with elderly people is causing fast rises in healthcare costs. Technologies (e.g., Internet-of-Things, Edge-of-Things, and Cloud-of-Things) in healthcare systems are going through a transformation where health monitoring of people is possible without hospitalization. The advancement of sensing technologies helps to make it possible to develop smart systems to monitor human behaviors continuously. In this work, a wearable sensor-based system is proposed for activity prediction using Recurrent Neural Network (RNN) on an edge device (i.e., personal computer or laptop). The input data of the system are obtained from multiple wearable healthcare sensors such as electrocardiography (ECG), magnetometer, accelerometer and gyroscope sensors. Then, an RNN is trained based on the features. The trained RNN is used for predicting the activities. The system has been compared against the conventional approaches on a publicly available standard dataset. The experimental results show that the proposed approach outperforms other traditional methods. Graphics Processing Unit (GPU) in the edge device is utilized to take the advantage of fast computation of experimental data. © 2018 Elsevier Inc.","Edge device; Prediction; RNN; Sensors"
"Aspen-based performance and energy modeling frameworks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038370671&doi=10.1016%2fj.jpdc.2017.11.005&partnerID=40&md5=bdafd7098616888cc215fd168ee1ce54","With the anticipation of exascale architectures, energy consumption is becoming one of the critical design parameters, especially in light of the energy budget of 20–30 Megawatts set by the U.S. Department of Energy. Understanding an application's execution pattern and its energy footprint is critical to improving application operation on a diverse heterogeneous architecture. Applying application-specific performance optimization can consequently improve energy consumption. However, this approach is only applicable to current systems. As we enter a new era of exascale architectures that is projected to contain more complex memory hierarchies, increased levels of parallelism, heterogeneity in hardware, and complex programming models and techniques, energy and performance management is getting more cumbersome. We therefore propose techniques that predict the energy consumption beforehand or at runtime to enable proactive tuning. Such energy prediction approaches must be generic and adapt themselves at runtime to changing application and hardware configurations. Most existing energy estimation and prediction approaches are empirical in nature and thus tied to current systems. To overcome this limitation, we propose two energy estimation techniques: ACEE (Algorithmic and Categorical Energy Estimation), which uses a combination of analytical and empirical modeling techniques; and AEEM (Aspen's Embedded Energy Estimation), a system-level analytical energy estimation technique. Both of these models incorporate the Aspen domain specific language for performance modeling. We present the methodologies of these two models and test their accuracy using five proxy applications. We also describe three use cases. © 2017 Elsevier Inc.","Analytical modeling; Aspen; Domain specific language; Energy modeling; Exascale computing; Performance; Performance modeling; Performance portability"
"Scalable data analytics using crowdsourced repositories and streams","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049473123&doi=10.1016%2fj.jpdc.2018.06.013&partnerID=40&md5=12a515e9a6efcfb3b05b4bf75050805d","The scalable analysis of crowdsourced data repositories and streams has quickly become a critical experimental asset in multiple fields. It enables the systematic aggregation of otherwise disperse data sources and their efficient processing using significant amounts of computational resources. However, the considerable amount of crowdsourced social data and the numerous criteria to observe can limit analytical off-line and on-line processing due to the intrinsic computational complexity. This paper demonstrates the efficient parallelisation of profiling and recommendation algorithms using tourism crowdsourced data repositories and streams. Using the Yelp data set for restaurants, we have explored two different profiling approaches: entity-based and feature-based using ratings, comments, and location. Concerning recommendation, we use a collaborative recommendation filter employing singular value decomposition with stochastic gradient descent (SVD-SGD). To accurately compute the final recommendations, we have applied post-recommendation filters based on venue suitability, value for money, and sentiment. Additionally, we have built a social graph for enrichment. Our master–worker implementation shows super-linear scalability for 10, 20, 30, 40, 50, and 60 concurrent instances. © 2018 Elsevier Inc.","Big data; Crowdsourcing; Data analytics; Distributed computing; High performance computing; Parallel processing; Recommender systems; Smart tourism"
"Enhancing in-memory efficiency for MapReduce-based data processing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046167866&doi=10.1016%2fj.jpdc.2018.04.001&partnerID=40&md5=61de495efdf7506f9276fe3c1ab2fdd5","As the memory capacity of computational systems increases, the in-memory data management of Big Data processing frameworks becomes more crucial for performance. This paper analyzes and improves the memory efficiency of Flame-MR, a framework that accelerates Hadoop applications, providing valuable insight into the impact of memory management on performance. By optimizing memory allocation, the garbage collection overheads and execution times have been reduced by up to 85% and 44%, respectively, on a multi-core cluster. Moreover, different data buffer implementations are evaluated, showing that off-heap buffers achieve better results overall. Memory resources are also leveraged by caching intermediate results, improving iterative applications by up to 26%. The memory-enhanced version of Flame-MR has been compared with Hadoop and Spark on the Amazon EC2 cloud platform. The experimental results have shown significant performance benefits reducing Hadoop execution times by up to 65%, while providing very competitive results compared to Spark. © 2018 Elsevier Inc.","Big data; Garbage collector (GC); In-memory computing; MapReduce; Performance evaluation"
"Making sense of performance in in-memory computing frameworks for scientific data analysis: A case study of the spark system","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036670276&doi=10.1016%2fj.jpdc.2017.10.016&partnerID=40&md5=1ba8dacac303f0dcd7cdfeda0badaf99","Over the last five years, Apache Spark has become a major software platform for in-memory data analysis. Acknowledging its widespread use, we present a comprehensive study of system characteristics of Spark targeting scientific data analytics performing large-scale matrix operations. We compare its performance to SciDB, a disk-based platform for array data analysis. A benchmark, ArrayBench, is developed to evaluate the performance of four analytics processing gene expression matrices using basic data operators of Spark and SciDB. It is applied to data from a real biological workflow whose data inputs are in matrix form. Herein, we report the findings, which shed light on the improvement of Spark and SciDB and the future development of data-intensive scientific data analytics using the in-memory computing frameworks. © 2017 Elsevier Inc.","In-memory computing; SciDB; Scientific data analytics; Spark"
"Rethink data dissemination in opportunistic mobile networks with mutually exclusive requirement","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046167500&doi=10.1016%2fj.jpdc.2018.03.012&partnerID=40&md5=17ac71c82d906630c5b33365eeede0ef","With the increase of mobile devices, opportunistic mobile networks become a promising technique for disseminating data in a local area. However, existing works focus on the single data dissemination and fail to consider the practical applications where there are multiple data under different topics. Multiple data dissemination shows the potential applications in many scenarios, e.g., product coupon distribution. In this paper, we focus on budget-constrained multiple data dissemination services. A mobile user may be interested in data under different topics, but receiving data for any topic is enough due to user experiences and participation constraints. This is the mutually exclusive delivery requirement in many scenarios. In light of the different amounts of data and the different popularity levels of data in each topic, deciding which data should be forwarded to mobile users becomes an important problem. This paper aims to design an efficient data dissemination scheme that minimizes the maximum dissemination delay while incurring a small communication overhead for the aforementioned scenario. In this paper, we discuss three different scenarios according to different knowledge. We start with the data dissemination with network topology, and a corresponding optimal solution is proposed. Later, we consider the probability estimation with k-hop information, and lastly propose a distributed data forwarding algorithm, which considers the amount of data in different topics, the mobile users’ interest, and their data forwarding abilities, respectively. The real trace-driven experiments show that the proposed scheme achieves a good performance. © 2018 Elsevier Inc.","Delay tolerant network; Mobile data dissemination; Opportunistic mobile network"
"Distributed shielded execution for transmissible cyber threats analysis","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051392028&doi=10.1016%2fj.jpdc.2018.07.014&partnerID=40&md5=546b6cd1dbc9f1fd72ecd7a742023765","Transmissible cyber threats have become one of the most serious security issues in cyberspace. Many techniques have been proposed to model, simulate and identify threats’ sources and their propagation in large-scale distributed networks. Most techniques are based on the analysis of real networks dataset that contains sensitive information. Traditional in-memory analysis of these dataset often causes data leakage due to system vulnerabilities. If the dataset itself is compromised by adversaries, this threat cost would be even higher than the threat being analysed. In this paper, we propose a new distributed shielded execution framework (Disef) for cyber threats analysis. The Disef framework enables efficient distributed analysis of network dataset while achieves security guarantees of data confidentiality and integrity. In-memory dataset is protected by using a new encrypted key–value format and could be efficiently transferred into Intel SGX enabled enclaves for shielded execution. Our experimental results showed that the proposed framework supports secure in-memory analysis of large network dataset and has comparable performance with systems that have no confidentiality and integrity guarantees. © 2018 Elsevier Inc.","Data security; Distributed shielded execution; Secure enclaves"
"Designing lab sessions focusing on real processors for computer architecture courses: A practical perspective","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044159414&doi=10.1016%2fj.jpdc.2018.02.026&partnerID=40&md5=8035570d6e7487cbef12f6526d3c82a4","Computer architecture courses typically include lab sessions to reinforce, from a practical perspective, concepts and architectural mechanisms studied in lectures. Lab sessions are mainly based on simulation frameworks because they benefit learning. Reading the source code that models certain processor mechanisms allows students to acquire a sound knowledge of how hardware works. Unfortunately, simulators that model current multicore processors are getting more and more complex, which lengthens the learning phase and complicates their use in time-bounded lab sessions. In this paper, we propose a new approach that complements the use of simulation frameworks in lab sessions of computer architecture courses. This approach is based on performing experiments on current commercial processors, where multiple hardware events related to the performance of the computer components under study are monitored. Then, students analyze the measured events and how they impact the overall performance. Such analysis motivates students and, not only helps reinforcing the theoretical concepts, but also increases their analysis skills. In this paper we present the methodology and scheduling framework that support the proposed approach and discuss five lab sessions, which can be applied in different courses, covering multiple computer architecture topics. © 2018 Elsevier Inc.","Computer architecture; Lab sessions; Processor complexity; Real processors; Scheduling framework"
"MGR: Multi-parameter Green Reliable communication for Internet of Things in 5G network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045127911&doi=10.1016%2fj.jpdc.2017.12.012&partnerID=40&md5=f7f9e103b260bfdabd01413eddba8d0a","Internet of Things (IoT) plays a major role in connecting the physical world with the cyber world through new services and seamless interconnection between heterogeneous devices. However, exploiting green schemes for IoT is still a challenge because as IoT attains a large scale and becomes more multifaceted, the current trends are not directly applicable to it. Similarly, achieving green communication through the use of 5G also poses new challenges when it comes to transferring huge volume of data efficiently. To address the challenges above, this paper presents a scheme for green IoT in 5G network. Grouping mobile nodes achieve green IoT in a cluster. Also, a mobility management model is designed that helps in triggering efficient handover and selecting optimal networks based on multicriteria decision modeling. Afterwards, we develop a system architecture which integrates green IoT with 5G network. It also helps them in communicating with other heterogeneous networks efficiently with minimum energy requirements. Moreover, the 5G network architecture is supported by the proposed protocol stack, which maps Internet Protocol (IP), Medium Access Protocol (MAC), and Location Identifiers (LOC). The proposed scheme is implemented using C programming language, and extensive mathematical and statistical analysis is carried out regarding cost, energy, and Quality of Service. © 2018 Elsevier Inc.","5G; Clustering mechanism; Green Internet of Things; Network architecture; Protocol stack"
"Hamiltonian cycle and path embeddings in 3-ary n-cubes based on K1,3-structure faults","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049353347&doi=10.1016%2fj.jpdc.2018.06.007&partnerID=40&md5=975ec41f45e17db8243b7fbae512e9bc","The k-ary n-cube is one of the most attractive interconnection networks for parallel and distributed computing system. In this paper, we investigate hamiltonian cycle and path embeddings in 3-ary n-cubes Qn 3 based on K1,3-structure faults, which means each faulty element is isomorphic to any connected subgraph of a connected graph K1,3. We show that for two arbitrary distinct healthy nodes of a faulty Qn 3, there exists a fault-free hamiltonian path connecting these two nodes if the number of faulty element is at most n−2 and each faulty element is isomorphic to any connected subgraph of K1,3. We also show that there exists a fault-free hamiltonian cycle if the number of faulty element is at most n−1 and each faulty element is isomorphic to any connected subgraph of K1,3. Then, we provide the simulation experiment to find a hamiltonian cycle and a hamiltonian path in structure faulty 3-ary n-cubes and verify the theoretical results. These results mean that the 3-ary n-cube Qn 3 can tolerate up to 4(n−2) faulty nodes such that Qn 3−V(F) is still hamiltonian and hamiltonian-connected, where F denotes the faulty set of Qn 3. © 2018 Elsevier Inc.","3-ary n-cube; Hamiltonian cycle; Hamiltonian path; Interconnection network; Structure fault"
"GPU inclusion test for triangular meshes","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049482070&doi=10.1016%2fj.jpdc.2018.06.003&partnerID=40&md5=9d4e20fe73cd96c5617bd9cbdcce0eba","Querying the relative position of a point regarding a solid defined by a triangular mesh is a fundamental algorithm in geometric modelling. This algorithm has many applications in fields like Computer Graphics or Computer Aided Design and is the basis of many other basic algorithms in these areas. In this paper we present an efficient implementation of one of the classic algorithms for solving this problem, the point-in-solid test of Feito and Torres based on simplicial coverings. This algorithm is simple, robust and valid for non-manifold solids. Our implementation resolves the test, including all the special cases, needing no conditional branches. This fact allows us to obtain a parallel and very efficient GPU implementation of the algorithm. We have coded the algorithm in CUDA and the results showed that this GPU implementation achieved a speedup of up to 142× with respect to a CPU single-thread implementation of the same optimized algorithm. Against a multi-thread implementation in CPU, our CUDA algorithm obtains a speedup of up to 38×. We have also compared our algorithm to a previous GPU implementation in CUDA of the inclusion test of Feito and Torres. Against this GPU implementation, our algorithm achieved a speedup of up to 11.8×. © 2018 Elsevier Inc.","CUDA; GPU; Point inclusion test; Triangular mesh"
"Hot-N-Cold model for energy aware cloud databases","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054877741&doi=10.1016%2fj.jpdc.2018.09.012&partnerID=40&md5=7378baabf091439d47babfedf754d234","A lot of cloud computing and cloud database techniques are adopted in industry and academia to face the explosion of the arrival of the big data era. Meanwhile, energy efficiency and energy saving become a major concern in data centers, which are in charge of large distributed systems and cloud databases. However, the phenomenon of energy wasting is related to resource provisioning. Hot-N-Cold model is introduced in this paper, which uses workload predictions and DVFS(Dynamic Voltage and Frequency Scaling) to cope with the resource provisioning problem within energy aware cloud database systems. In this model, the resource provisioning problem is considered as two bounded problems. A nonlinear programming algorithm and a multi-phase algorithm are proposed to solve them. The experimental results show that one of the proposed algorithms has great scalability which can be applied to a cloud database system deployed on 70 nodes. Using Hot-N-Cold model can save up to 21.5% of the energy of the running time. © 2018 Elsevier Inc.","Bounded problem; Cloud database; DVFS; Energy efficiency; Hot-N-Cold"
"Gradual stabilization","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054009426&doi=10.1016%2fj.jpdc.2018.09.002&partnerID=40&md5=cb0662f7c3752e25cba70a5389da076a","We consider dynamic distributed systems, i.e., distributed systems that can suffer from topological changes over the time. Following the superstabilizing approach, we assume here that topological changes are transient events. In this context, we introduce the notion of gradual stabilization under(τ,ρ)-dynamics (gradual stabilization, for short). A gradually stabilizing algorithm is a self-stabilizing algorithm with the following additional feature: after up to τ dynamic steps of a given type ρ occur starting from a legitimate configuration, it first quickly recovers to a configuration from which a specification offering a minimum quality of service is satisfied. It then gradually converges to specifications offering stronger and stronger safety guarantees until reaching a configuration (1) from which its initial (strong) specification is satisfied again, and (2) where it is ready to achieve gradual convergence again in case of up to τ new dynamic steps of type ρ. A gradually stabilizing algorithm being also self-stabilizing, it still recovers within finite time (yet more slowly) after any other finite number of transient faults, including for example more than τ arbitrary dynamic steps or other failure patterns such as memory corruptions. We illustrate this new property by considering three variants of a synchronization problem respectively called strong, weak, and partial unison. We propose a self-stabilizing unison algorithm which achieves gradual stabilization in the sense that after one dynamic step of a certain type BULCC (such a step may include several topological changes) occurs starting from a configuration which is legitimate for the strong unison, it maintains clocks almost synchronized during the convergence to strong unison: it satisfies partial unison immediately after the dynamic step, then converges in at most one round to weak unison, and finally re-stabilizes to strong unison. © 2018 Elsevier Inc.","Gradual stabilization; Safe-convergence; Self-stabilization; Superstabilization; Synchronization problems; Unison"
"MR-Advisor: A comprehensive tuning, profiling, and prediction tool for MapReduce execution frameworks on HPC clusters","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037723937&doi=10.1016%2fj.jpdc.2017.11.004&partnerID=40&md5=134ce13dfab1ba9e5054f731f468c05f","MapReduce is the most popular parallel computing framework for big data processing which allows massive scalability across distributed computing environment. Advanced RDMA-based design of Hadoop MapReduce has been proposed that alleviates the performance bottlenecks in default Hadoop MapReduce by leveraging the benefits from RDMA. On the other hand, data processing engine, Spark, provides fast execution of MapReduce applications through in-memory processing. Performance optimization for these contemporary big data processing frameworks on modern High-Performance Computing (HPC) systems is a formidable task because of the numerous configuration possibilities in each of them. In this paper, we propose MR-Advisor, a comprehensive tuning, profiling, and prediction tool for MapReduce. MR-Advisor is generalized to provide performance optimizations for Hadoop, Spark, and RDMA-enhanced Hadoop MapReduce designs over different file systems such as HDFS, Lustre, and Tachyon. Performance evaluations reveal that, with MR-Advisor's suggested values, the job execution performance can be enhanced by a maximum of 58% over the current best-practice values for user-level configuration parameters. To the best of our knowledge, this is the first tool that supports tuning and prediction for both Apache Hadoop and Spark, as well as the RDMA and Lustre-based advanced designs. © 2017 Elsevier Inc.","HPC; MapReduce; Prediction; Tuning"
"HMVFS: A Versioning File System on DRAM/NVM Hybrid Memory","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036665514&doi=10.1016%2fj.jpdc.2017.10.022&partnerID=40&md5=93100340f2dd6f43753f662520c8eb30","The byte-addressable Non-Volatile Memory (NVM) offers fast, fine-grained access to persistent storage, and a large volume of recent researches are conducted on developing NVM-based in-memory file systems. However, existing approaches focus on low-overhead access to the memory and only guarantee the consistency between data and metadata. In this paper, we address the problem of maintaining consistency among continuous snapshots for NVM-based in-memory file systems. We propose an efficient versioning mechanism and implement it in Hybrid Memory Versioning File System (HMVFS), which achieves fault tolerance efficiently and has low impact on I/O performance. Our results show that HMVFS provides better performance on snapshotting and recovering compared with the traditional versioning file systems for many workloads. Specifically, HMVFS has lower snapshotting overhead than BTRFS and NILFS2, improving by a factor of 9.7 and 6.6, respectively. Furthermore, HMVFS imposes minor performance overhead compared with the state-of-the-art in-memory file systems like PMFS. © 2017 Elsevier Inc.","Checkpoint; File system; Memory; Snapshot; Versioning"
"The Open Computing Abstraction Layer for Parallel Complex Systems Modeling on Many-Core Systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048765701&doi=10.1016%2fj.jpdc.2018.07.005&partnerID=40&md5=9f6c998c8d6482a159aa56df360a91d7","This article introduces OpenCAL, a new open source computing abstraction layer for multi- and many-core computing based on the Extended Cellular Automata general formalism. OpenCAL greatly simplifies the implementation of structured grid applications, contextually making parallelism transparent to the user. Different OpenMP- and OpenCL-based implementations have been developed, together with a preliminary MPI-based distributed memory version, which is currently under development. The system software architecture is presented and underlying data structures and algorithms described. Numerical correctness and efficiency have been assessed by considering the SciddicaT Computational Fluid Dynamics landslide simulation model as reference example. Eventually, a comprehensive study has been performed to devise the best platform for execution as a function of numerical complexity and computational domain extent. Results obtained have highlighted the OpenCAL's potential for numerical models development and their execution on the most suitable high-performance parallel computational device. © 2018 Elsevier Inc.","Complex systems modeling; Extended cellular automata formalism; GPGPU computing; MPI; OpenCL; OpenMP"
"IoTDeM: An IoT Big Data-oriented MapReduce performance prediction extended model in multiple edge clouds","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044747009&doi=10.1016%2fj.jpdc.2017.11.001&partnerID=40&md5=96fbdcaf3bb56de2b8470d5d5640f5ae","Uploading all IoT Big Data to a centralized cloud for data analytics is infeasible because of the excessive latency and bandwidth limitation of the Internet. A promising approach to addressing the challenges for data analytics in IoT is “edge cloud” that pushes various computing and data analysis capabilities to multiple edge clouds. MapReduce provides an efficient way to deal with a large amount of data. When performing data analysis, a challenge is to predict the performance of MapReduce jobs. In this paper, we propose and evaluate IoTDeM, which is an extended IoT Big Data-oriented model for predicting MapReduce performance in multiple edge clouds. IoTDeM is able to predict MapReduce jobs’ total execution time in a general implementation scenario with varying reduce amounts and cluster scales in Hadoop 2, rather than Hadoop 1. The extended model is based on historical job execution records and Locally Weighted Linear Regression (LWLR) techniques to predict the execution time of each job. Through extracting more representative features to represent a job, the IoTDeM model selects a cluster scale as a crucial parameter to further extend LWLR model. In the environment of Hadoop 2 with Ceph as the storage system, the experiments verify IoTDeM can effectively predict the total execution time of MapReduce applications, with the average relative error of less than 10%. © 2017 Elsevier Inc.","Big Data; Edge cloud; Hadoop; IoT; Job estimation; MapReduce; Performance modeling; Performance prediction"
"Beyond scalability: Swarm intelligence affected by magnetic fields in distributed tuple spaces","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054463135&doi=10.1016%2fj.jpdc.2018.09.004&partnerID=40&md5=1a3e16a12c7178b0a08d610629a8afc2","Tuple Spaces have long been recognized as a simple and elegant model for parallel and distributed computing. This is mainly because of spatial and temporal uncoupling of system components, which simplifies inter-process communication as well as component inclusion and replacement. However, Tuple Spaces have shown scalability limitations when employed in large scale and highly demanding contexts. In order to deal with this problem, “bioinspired” techniques based on swarm intelligence including SwarmLinda and Anti-Over-Clustering have been proposed. This work shows that although these approaches do improve the system scalability, they end up producing an important degradation on tuple search performance, due to poor tuple placement. By applying the concept of “virtual magnetic fields” to swarms, a novel solution called Magnetic SwarmLinda is proposed. Magnetic SwarmLinda arranges tuples in expandable clusters of clusters (called “magnetic clusters”) naturally providing load balancing among the supporting computing nodes. Simulation results show that the proposed strategy outperforms previous approaches in most scenarios. © 2018 Elsevier Inc.","Swarm intelligence; Tuple space; Virtual magnetic fields"
"Building web-based services for practical exercises in parallel and distributed computing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044088667&doi=10.1016%2fj.jpdc.2018.02.024&partnerID=40&md5=315f7191ee9eadf3630649e876666831","The paper presents an approach to the design and implementation of web-based environments for practical exercises in parallel and distributed computing (PDC). The presented approach introduces minimal development and operational costs by relying on Everest, a general-purpose platform for building computational web services. The flexibility of proposed service-oriented architecture enables the development of different types of services targeting various use cases and PDC topics. The generic execution services support the execution of different types of parallel and distributed programs on corresponding computing systems, while the assignment evaluation services implement the execution and evaluation of solutions to programming assignments. As was demonstrated by teaching two introductory PDC courses, the presented approach helps to enhance students’ practical experience while avoiding low-level interfaces, reducing the grading time and providing a level of automation necessary for scaling the course to a large number of students. In contrast to other efforts, the exploited Platform as a Service model provides the ability to quickly reuse this approach by other PDC educators without installation of the Everest platform. © 2018 Elsevier Inc.","Parallel and distributed computing education; Platform as a service; Program testing; Programming assignments; Web services; Web-based environment"
"An Optimized trust model integrated with linear features for cyber-enabled recommendation services","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044507719&doi=10.1016%2fj.jpdc.2017.10.003&partnerID=40&md5=648b2b52bb3c3968ae146913297709c4","The growth of cyberspace brings more information to service recommendation. The scores of item are used in most of the recommendation algorithms, but the attributes of users and items are rarely involved in trust recommendation in cyberspace. Both the rating features and attribute information are important for trust recommendation results. In this paper, we combine the heterogeneous information in cyberspace and propose a novel trust recommendation model based on the latent factor model and trusty neighborhood fitting model. We utilize the feature based Latent Factor Model and study the linear features integrated model To solve the failure problem of the latent factor model in the integrated model under the cold-start situations, we propose two optimized methods, which contain the filling method based on feature similarity and the filling method based on feature regression through mapping attributes to features. Experimental results show that the improved method outperforms traditional collaborative recommendation in terms of recommendation accuracy. Meanwhile, our proposed method has been verified to free from the impact of the cold-start problem. © 2017 Elsevier Inc.","Cold start; Cyberspace; Feature filling; Latent factor model; Trusty neighborhood"
"An intelligent regressive ensemble approach for predicting resource usage in cloud computing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054013023&doi=10.1016%2fj.jpdc.2018.08.008&partnerID=40&md5=8d433a7a94228cc1e6943bca392ae48b","Cloud Computing has become prime infrastructure for scientists to deploy scientific applications as it offers parallel and distributed environment for large-scale computations. During deployment, the significant prediction of resource usage is essential to achieve optimal scheduling for scientific applications. The existing resource prediction models fall short in providing reasonable accuracy because of high variances of cloud metrics. Therefore, to handle the varying cloud resource demands, it is necessary to accurately predict the future resource requirements for automatically provisioning the resources. In this paper, an Intelligent Regressive Ensemble Approach for Prediction (REAP) has been proposed which integrates feature selection and resource usage prediction techniques to achieve high performance. The effectiveness of proposed approach is evaluated in a real cloud environment by conducting a series of experiments. The experimental results show that the proposed approach outperforms the existing models by significantly improving the accuracy rate and reducing the execution time. The results are further validated by comparing the existing Learning Automata (LA) based ensemble approach with the proposed approach on the basis of error rate. © 2018 Elsevier Inc.","Cloud computing; Ensembling; Feature selection; Regression techniques; Resource prediction; Scientific applications"
"Preparing the software engineer for a modern multi-core world","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044101321&doi=10.1016%2fj.jpdc.2018.02.028&partnerID=40&md5=a76372f51c13d283059051adc0eac9f5","Parallel and Distributed Computing (PDC) was traditionally viewed as an advanced subject reserved for elective graduate courses. The last decade has seen two areas with rapid growth, whose synergy is demanding new skills for software engineers in a modern multi-core world. The first has been society's increasing demand for software engineering solutions, evident in the integral role that software plays in daily life. Unlike traditional PDC applications in the scientific and engineering domains, modern software applications are interacting directly with millions of users on mainstream laptops, smartphones and tablets. The second trend is that of multi-core processors powering such devices, which is further fueling the potential of software applications. This paper proposes a Modern Parallel Programming Framework that recognizes that successful software engineering in this domain involves a combination of hard skills and soft skills. A course dedicated to this goal is presented and evaluated, incorporating a research-infused, problem-based and active learning approach. © 2018 Elsevier Inc.","Active learning; Concurrency; Graphical user interfaces; Object-oriented programming; Parallel programming; Problem-based learning; Research-infused learning; Soft skills; Software engineering"
"Janus: Diagnostics and reconfiguration of data parallel programs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044324067&doi=10.1016%2fj.jpdc.2018.02.030&partnerID=40&md5=0648acaec2e069f170180835b2f49c76","The increasing amount of data being stored and the variety of algorithms proposed to meet processing demands of the data scientists have led to a new generation of computational environments and paradigms. These environments simplify the task of programmers, but achieving the ideal performance continues to be a challenge. In this work we investigate important factors concerning the performance of common big-data applications and consider the Spark framework as the target for our contributions. Based on that, we present the design and implementation of Janus, a tool that automates the reconfiguration of Spark applications. It leverages logs from previous executions as input, enforces configurable adjustment policies over the collected statistics and makes its decisions taking into account communication behaviors specific of the application evaluated. In order to accomplish that, Janus identifies global parameters that should be updated, or points in the user program where the data partitioning can be adjusted based on those policies. Our results show gains of up to 1.9× in the scenarios considered. © 2018 Elsevier Inc.","Dynamic reconfiguration; Performance diagnosis; Spark"
"Evaluating model checking for cyber threats code obfuscation identification","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047166991&doi=10.1016%2fj.jpdc.2018.04.008&partnerID=40&md5=8940760de05b7e481fb9786acde2cd45","Code obfuscation is a set of transformations that make code programs harder to understand. The goal of code obfuscation is to make reverse engineering of programs infeasible, while maintaining the logic on the program. Originally, it has been used to protect intellectual property. However, recently code obfuscation has been also used by malware writers in order to make cyber threats easily able to evade antimalware scanners. As a matter of fact, metamorphic and polymorphic viruses exhibit the ability to obfuscate their code as they propagate. In this paper we propose a model checking-based approach which is able to identify the most widespread obfuscating techniques, without making any assumptions about the nature of the obfuscations used. We evaluate the proposed method on a real-world dataset obtaining an accuracy equal to 0.9 in the identification of obfuscation techniques. © 2018 Elsevier Inc.","Android; Formal methods; Malware; Model checking; Obfuscation"
"Energy and communication aware task mapping for MPSoCs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052075172&doi=10.1016%2fj.jpdc.2018.03.010&partnerID=40&md5=3eb4deb71d7c2bbfb46aad3adbc4a842","Minimizing energy consumption and network load is a major challenge for network-on-chip (NoC) based multi-processor systems-on-chip (MPSoCs). Efficient task and core mapping can greatly reduce the overall energy consumption and communication overhead among the interdependent tasks. In this paper, we propose a novel Knapsack based bin packing algorithm for workload consolidation that places tasks in such a manner that utilization of available processing elements is maximized, while network overhead, regarding the communication among the tasks, is minimized. We also propose a task swapping algorithm that attempts to further optimize the task placement produced by the bin packing algorithms. Moreover, several core mapping techniques are implemented and the performance of each technique is evaluated under varying configurations. In addition, we also apply a Pareto-efficient algorithm, on top of the bin packing algorithms, attempting to explore the solution in two dimensions, i.e., energy consumption and network load. The experimental results show that the proposed Knapsack based bin packing algorithm coupled with the Pareto-efficient algorithm achieves significant energy savings and reduction in network load as compared to state-of-the-art algorithms, as well as the greedy algorithm. Particularly, the Pareto-efficient algorithm when applied on top of the Knapsack algorithm shows on average 50% and 55% reduction in energy consumption and network load as compared to the greedy algorithm, respectively. While the proposed Pareto-efficient algorithm applied with Knapsack algorithm also demonstrate superior performance compared to three other state-of-the-art heuristics. © 2018 Elsevier Inc.","Core mapping; Energy and communication aware mapping; Multi-processor system-on-chip (MPSoC); Network-on-chip (NoC); Task mapping"
"Cost effective routing techniques in 2D mesh NoC using on-chip transmission lines","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054397675&doi=10.1016%2fj.jpdc.2018.09.009&partnerID=40&md5=bd19ceec9b36a57f2a3b8cccb0249162","Advancements in CMOS technology led to the increase in number of processing cores on a single chip. Communication between different cores in such multicore systems is facilitated by an underlying interconnect. Due to the limitations of traditional bus-based system Network on Chip (NoC) based interconnect is the most acceptable cost effective framework for inter-core communication. A packet in an NoC travels through a sequence of intermediate routers before arriving at its destination. As the size of NoC scales high, the average number of intermediate routers that a packet traverse also increases. This results in higher packet latency which degrades application performance. In this work, we introduce cost effective adaptive routing techniques that can forward long distance packets through specialized channels made of Transmission Line (TL). These extra TLs introduced in the chip reduce the diameter of the network thereby reducing average packet latency. We propose two novel router architectures; SBTR and e-SBTR that reduce packet latency by reducing the number of intermediate hops. We use PARSEC benchmark and SPEC CPU 2006 benchmark mixes to evaluate the performance of our proposed techniques. SBTR and e-SBTR reduce average packet latency by 7.9% and 25% respectively. Both the techniques also reduce average hop count by 8.13% and 27.6% respectively. We also observe that our proposed technique e-SBTR performs better than the state-of-the-art Express Virtual Channel technique in terms of packet latency and hop count respectively. © 2018 Elsevier Inc.","Adaptive routing; Chip multiprocessor; Express Virtual Channel; Hop count reduction; Hybrid NoC; Packet latency"
"Zynq-based acceleration of robust high density myoelectric signal processing","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054375988&doi=10.1016%2fj.jpdc.2018.07.004&partnerID=40&md5=8735e4b4d14204f29a737b11aeb0c2ce","Advances in electromyographic (EMG) sensor technology and machine learning algorithms have led to an increased research effort into high density EMG-based pattern recognition methods for prosthesis control. With the goal set on an autonomous multi-movement prosthesis capable of performing training and classification of an amputee's EMG signals, the focus of this paper lies in the acceleration of the embedded signal processing chain. We present two Xilinx Zynq-based architectures for accelerating two inherently different high density EMG-based control algorithms. The first hardware accelerated design achieves speed-ups of up to 4.8 over the software-only solution, allowing for a processing delay lower than the sample period of 1 ms. The second system achieved a speed-up of 5.5 over the software-only version and operates at a still satisfactory low processing delay of up to 15 ms while providing a higher reliability and robustness against electrode shift and noisy channels. © 2018","FPGA acceleration; High density electromyography; Medical signal processing; Pattern recognition; Prosthetics"
"A data-driven approach of performance evaluation for cache server groups in content delivery network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046851894&doi=10.1016%2fj.jpdc.2018.04.010&partnerID=40&md5=c72ed611f345866fb3455f6d7ed12096","In industry, Content Delivery Network (CDN) service providers are increasingly using data-driven mechanisms to build the performance models of the service-providing systems. Building a model to accurately describe the performance of the existing infrastructure is very crucial to make resource management decisions. Conventional approaches that use hand-tuned parameters or linear models have their drawbacks. Recently, data-driven paradigm has been shown to greatly outperform traditional methods in modeling complex systems. We design a data-driven approach to building a reasonable and feasible performance model for CDN cache server groups. We use deep LSTM auto-encoder to capture the temporal structures from the high-dimensional monitoring data, and use a deep neural network to predict the reach rate which is a client QoS measurement from the CDN service providers’ perspective. The experimental results have shown that our model is able to outperform state-of-the-art models. © 2018 Elsevier Inc.","Content delivery network; Deep learning; Edge computing; High dimensional data; Predictive analysis; Sequence learning"
"Unifying computing resources and access interface to support parallel and distributed computing education","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044303975&doi=10.1016%2fj.jpdc.2018.02.020&partnerID=40&md5=ba8b81ef99939c46db58c704eefdde65","This article presents how various on-site and remote computing resources are combined into a framework to support teaching parallel and distributed computing (PDC) at the undergraduate level. The combination of these resources enables the delivery of PDC programming, system, and architectural concepts via a browser-based common interface (JupyterHub) and a single programming environment (Python and its supported libraries). This also allows lecturers and students to focus more on the principles of PDC and less on the technicalities of native languages for different platforms. We describe how this framework can support a comprehensive set of PDC course modules, including lectures, assignments, and projects, for a full semester junior-level class. Adoption of this framework in various teaching environments at Clemson University has received positive feedback from both instructors and participants. © 2018","Computing resources; Distributed systems; MapReduce; MPI; Parallel computing education; Python; Spark"
"Visual attention feature (VAF) : A novel strategy for visual tracking based on cloud platform in intelligent surveillance systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049557041&doi=10.1016%2fj.jpdc.2018.06.012&partnerID=40&md5=c8b0e2fe893b7176aee28a0c10e57e48","Visual object tracking has been challenged in computer vision because objects often undergo significant representation changes caused by occlusion, illumination variation, scale variation and rotation. Unfortunately, recent trackers do not focus on dynamic characteristics of tracking object. In this paper, we present a strategy based on visual attention feature (VAF), which uses features related to visual attention mechanism and object movement distribution. Firstly, Visual attention feature has been constructed and extracted. Then, concepts of visual attention feature have been illustrated. Next, it is used to improve tracking performance based on classic discriminative method and parallel computing platform. Finally, statistical results demonstrate effectiveness and accuracy of our object tracking strategy based on VAF. In addition, experimental results on OTB-2013 and OTB-2015 benchmark datasets show that our strategy with parallel candidate region generating algorithm improves robustness of recent object tracking methods against many challenging attributes and suffices to reach state-of-the-art performance. © 2018 Elsevier Inc.","Computer vision; Human visual system; Parallel computing; Tracking strategy; Visual attention feature; Visual object tracking"
"Robust orchestration of concurrent application workflows in mobile device clouds","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048430206&doi=10.1016%2fj.jpdc.2018.05.004&partnerID=40&md5=c7961077f1d428cc374526c5b82ec4a1","A hybrid mobile/fixed device cloud that harnesses sensing, computing, communication, and storage capabilities of mobile and fixed devices in the field as well as those of computing and storage servers in remote datacenters is envisioned. Mobile device clouds can be harnessed to enable innovative pervasive applications that rely on real-time, in-situ processing of sensor data collected in the field. To support concurrent mobile applications on the device cloud, a robust distributed computing framework, called Maestro, is proposed. The key components of Maestro are (i) a task scheduling mechanism that employs controlled task replication in addition to task reallocation for robustness and (ii) Dedup for task deduplication among concurrent pervasive workflows. An architecture-based solution that relies on task categorization and authorized access to the categories of tasks is proposed for different levels of trust. Experimental evaluation through prototype testbed of Android- and Linux-based mobile devices as well as simulations is performed to demonstrate Maestro's capabilities. © 2018 Elsevier Inc.","Application workflows; Controlled replication; Distributed computing; Mobile device clouds; Testbed"
"On the adaptive data forwarding in opportunistic underwater sensor networks using GPS-free mobile nodes","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052479887&doi=10.1016%2fj.jpdc.2018.08.004&partnerID=40&md5=b11909d9f128e661d416e741b6fb38d5","Opportunistic underwater sensor networks (OUSNs) are developed for a set of underwater applications, including the tracking of underwater creatures and tactical surveillance. The data forwarding objectives of OUSNs differ significantly from those in wireless sensor networks or delay-tolerant networks due to multifarious factors of underwater environments, such as the large energy consumption and the long transmission delay. As a result of the elusive moving intentions of nodes and the sophisticated external forces, the underwater movements of nodes are usually much more irregular, which accordingly makes the contacts among nodes become more stochastic. In addition, GPS signal is not available because of the limitations of satellite coverage and the underwater obstructions, and thus the real-time positions of nodes cannot be obtained and served for the future trajectory prediction. In this paper, we study the OUSNs data forwarding problem where the nodes are not equipped with GPS devices. Especially, the forwarding probabilities of data holders are exploited, and three vital metrics (movement speed, time slot sequence and distribution uniformity) are investigated for setting the probabilities to minimize the energy consumption. Then, we propose an adaptive GPS-free data forwarding approach (GDFA), in which the data holders probabilistically decide whether to forward the carried data packets or not, according to their movement speeds and the current time slot sequences rather than the node positions. Furthermore, the number of forwarded copies is determined by the number of neighboring data holders. Finally, the GDFA performance is evaluated through simulation experiments under an underwater mobility model. Simulation results indicate the satisfactory energy saving, delivery ratio and transmission delay obtained from GDFA. It is also shown that GDFA has a good scalability, and it is especially suitable for densely deployed OUSNs. © 2018 Elsevier Inc.","Adaptive data forwarding; Delivery ratio; Energy consumption; Opportunistic underwater sensor networks; Transmission delay"
"BMB synthesis of binary functions using symbolic functional decomposition for LUT-based FPGAs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047245636&doi=10.1016%2fj.jpdc.2018.05.001&partnerID=40&md5=2ec4cc8724d4c31b77a9e1019cf498d8","The paper presents the BMB methodology for synthesis of binary functions, which can be applied to synthesis of functions implemented in LUT-based FPGAs. The BMB synthesis uses the functional decomposition based on symbolic decomposition and symbolic encoding. The methodology uses two optimization methods. First, the symbolic decomposition is used to build the multiple-valued network of functions. Second, the symbolic encoding is used to find an encoding that simplifies the decomposition. The symbolic optimization overpasses limits of the existing decomposition methods, where optimal encoding for LUT-based implementations is not possible. Simultaneously, the method can be used in the synthesis of high-level architecture where dependencies can be described using symbolic values. Experimental results showed that the presented methodology allows significant reduction of the cost of the FPGA implementation. © 2018 Elsevier Inc.","CAD; FPGA; Logic synthesis; LUT; Multiple-valued network; Multiple-valued variable; Symbolic decomposition"
"Optimal task execution speed setting and lower bound for delay and energy minimization","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054005057&doi=10.1016%2fj.jpdc.2018.09.003&partnerID=40&md5=01b5054be37f44a88ef0cec501f41d1a","The current technology trend reveals that static power consumption is growing at a faster rate than dynamic power consumption. In this paper, energy-efficient task scheduling is studied when static power consumption is a significant part of energy consumption which cannot be ignored. The problems of scheduling a set of independent sequential tasks on identical processors so that the schedule length is minimized for a given energy consumption constraint or the energy consumption is minimized for a given schedule length constraint are investigated. For a given schedule, the optimal task execution speed setting for delay and energy minimization is found analytically. Lower bounds for the minimum schedule length of a set of tasks with a given energy consumption constraint and the minimum energy consumption of a set of tasks with a given schedule length constraint are established. Our lower bounds are applicable to sequential or parallel, and independent or precedence constrained tasks, on processors with discrete or continuous speed levels, and bounded or unbounded speed ranges. The significance of these lower bounds is that they can be used to evaluate the performance of any heuristic algorithms when compared with optimal algorithms. Experimental study on the performance of list scheduling algorithms is performed and it is shown that their performance is very close to the optimal. To the best of the author's knowledge, this is the first paper that provides such analytical results for energy-efficient task scheduling with both dynamic and static power consumptions. © 2018 Elsevier Inc.","Delay minimization; Energy minimization; Energy-efficient task scheduling; List scheduling; Lower bound; Optimal speed setting; Static power consumption"
"A novel hierarchical architecture for Wireless Network-on-Chip","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044103832&doi=10.1016%2fj.jpdc.2018.02.032&partnerID=40&md5=01d94727e880b32632b3964674a6ca96","In the architecture of Networks-on-Chip (NoCs), wired structure and multi-hop communications can lead to high power consumption and latency. Wireless NoC (WiNoC) architecture is a new alternative to solve these challenges. In this architecture, long-range wireless links are used instead of multi-hop wired paths. In this paper, a combination of several topologies are investigated to develop an efficient hierarchical structure for the architecture of WiNoC. The performances of considered hierarchical structures are compared under different traffic patterns. Finally, by using the Analytic Hierarchy Process (AHP) technique, a new hierarchical wireless NoC is proposed. In the proposed architecture, hierarchical structure and wireless links with high bandwidth are regarded as two significant factors for reducing the number of hops between distant nodes. Based on the results of simulations, the proposed hierarchical structure has better efficiency than other WiNoC architectures. © 2018 Elsevier Inc.","AHP; Hierarchical structure; Hub location; Topology; Wireless Network-on-Chip"
"DITVA: Dynamic Inter-Thread Vectorization Architecture","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039036659&doi=10.1016%2fj.jpdc.2017.11.006&partnerID=40&md5=68f2cebe3987e1b92b5c6e62798b01f6","In the Single-Program Multiple-Data (SPMD) programming model, threads of an application exhibit very similar control flows and often execute the same instructions, but on different data. In this paper, we propose the Dynamic Inter-thread Vectorization Architecture (DITVA) to leverage the implicit Data Level Parallelism that exists across threads on SPMD applications. By assembling dynamic vector instructions at runtime, DITVA extends an in-order SMT processor with a dynamic inter-thread vector execution mode akin to the Single-Instruction, Multiple-Thread model of Graphics Processing Units. In this mode, multiple scalar threads running in lockstep share a single instruction stream and their respective instruction instances are aggregated into SIMD instructions. DITVA can leverage existing SIMD units and maintains binary compatibility with existing CPU architectures. To balance thread- and data-level parallelism, threads are statically grouped into fixed-size independently scheduled warps. Additionally, to maximize dynamic vectorization opportunities, we adapt the fetch steering policy to favor thread synchronization within warps and thus improve lockstep execution. Our experimental evaluation of the DITVA architecture on the SPMD applications from the PARSEC and Rodinia OpenMP benchmarks show that a 4-warp × 4-lane 4-issue DITVA architecture with a realistic bank-interleaved cache achieves 1.55× higher performance compared to a 4-thread 4-issue SMT architecture with AVX instructions, while fetching and issuing 51% fewer instructions, and achieving an overall 24% energy reduction. DITVA also enables applications limited by memory to scale with higher bandwidth architectures. For instance, when the bandwidth is increased from 2GB/s to 16GB/s, we find that memory bound applications show an improvement in performance by 3× in comparison with the baseline SMT. Therefore, DITVA appears as a cost-effective design for achieving very high single-core performance on SPMD parallel sections. © 2017 Elsevier Inc.","Simultaneous Multi-threading; Single instruction multiple data; Single program multiple data; Vectorization"
"On an exact solution of the rate matrix of G∕M∕1-type Markov process with small number of phases","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046873497&doi=10.1016%2fj.jpdc.2018.04.013&partnerID=40&md5=e709f2076814e356e8086f9118ef70eb","In this research paper we consider the matrix polynomial equation arising naturally in the equilibrium analysis of a structured G∕M∕1-type Markov process. We obtain an explicit expression for the unknown rate matrix R being 2 × 2 matrix. The method is based on symbolic solution of the determinantal polynomial equation. Using Cayley–Hamilton theorem, the matrix polynomial equation for the matrix R is reduced to the system of linear equations. Motivated by applications in Edge Computing by means of Internet of Things devices having tight constraints in energy consumption, we demonstrate the applicability of the method by a novel approach to energy efficiency of a single-server computing system. A new randomized regime switching scheme is proposed, which, as it is shown by means of numerical experiment, provides significant decrease of energy consumption of the system under study. © 2018 Elsevier Inc.","Energy efficiency; Explicit solution; G∕M∕1-type Markov process; Matrix polynomial equation; Matrix-analytic method"
"Let's HPC: A web-based platform to aid parallel, distributed and high performance computing education","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044964529&doi=10.1016%2fj.jpdc.2018.03.001&partnerID=40&md5=eb215943f73db33f2109f8ddc9f18d9a","Let's HPC (www.letshpc.org) is an evolving open-access web-based platform to supplement conventional classroom oriented High Performance Computing (HPC) and Parallel & Distributed Computing (PDC) education. This platform has been developed to allow users to learn, evaluate, teach and see the performance of parallel algorithms from a system's viewpoint. The Let's HPC platform's motivation comes from the experiences of teaching HPC/PDC courses and it is designed to help streamline the process of analyzing parallel programs. At the heart of this platform is a database archiving the performance and execution environment related data of standard parallel algorithm implementations run on different computing architectures using different programming environments. The online plotting and analysis tools of our platform can be combined seamlessly with the database to aid self-learning, teaching, evaluation and discussion of different HPC related topics, with a particular focus on a holistic system's perspective. The user can quantitatively compare and understand the importance of numerous deterministic as well as non-deterministic factors of both the software and the hardware that impact the performance of parallel programs. Instructors of HPC/PDC related courses can use the platform's tools to illustrate the importance of proper data collection and analysis in understanding factors impacting performance as well as to encourage peer learning among students. Scripts are provided for automatically collecting performance related data, which can then be analyzed using the platform's tools. The platform also allows students to prepare a standard lab/project report aiding the instructor in uniform evaluation. The platform's modular design enables easy inclusion of performance related data from contributors as well as addition of new features in the future. This paper summarizes the background and motivation behind the Let's HPC project, the design philosophy of the platform, the present capabilities of the platform, as well as the plans for future developments. © 2018 Elsevier Inc.","HPC database; HPC education; Multicore architecture; Parallel & distributed programming; Performance analyzer"
"A collaborative CPU–GPU approach for principal component analysis on mobile heterogeneous platforms","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048465745&doi=10.1016%2fj.jpdc.2018.05.006&partnerID=40&md5=05780c1d4642fb3829e94ee089b74d92","The advent of the modern GPU architecture has enabled computers to use General Purpose GPU capabilities (GPGPU) to tackle large scale problem at a low computational cost. This technological innovation is also available on mobile devices, addressing one of the primary problems with recent devices: the power envelope. Unfortunately, recent mobile GPUs suffer from a lack of accuracy that can prevent them from running any large scale data analysis tasks, such as principal component analysis (Shlens, 0000) (PCA). The goal of our work is to address this limitation by combining the high precision available on a CPU with the power efficiency of a mobile GPU. In this paper, we exploit the shared memory architecture of mobile devices in order to enhance the CPU–GPU collaboration and speed up PCA computation without sacrificing precision. Experimental results suggest that such an approach drastically reduces the power consumption of the mobile device while accelerating the overall workload. More generally, we claim that this approach can be extended to accelerate other vectorized computations on mobile devices while still maintaining numerical accuracy. © 2018 Elsevier Inc.","Acceleration; Data analysis; Energy efficient; GPGPU; Heterogeneous system; Machine learning; Mobile computing; OpenCL; PCA"
"HAS: Hybrid auto-scaler for resource scaling in cloud environment","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047380880&doi=10.1016%2fj.jpdc.2018.04.016&partnerID=40&md5=a6b466a10249b92375c9f96d30b51e95","Auto-scaling is a crucial mechanism that supports autonomic provisioning and de-provisioning of computing resources in accordance with fluctuating demands in a cloud environment. The success factor of autonomic provisioning depends on efficient resource utilization and response time performance metrics. Existing literature focuses on reactive or predictive auto-scaling mechanism where the computing system is unable to scale proportionally with the Slashdot effect or abrupt traffic bursts while these mechanisms are employed in a discrete fashion. Predictive methods strive to predict the future computational needs and subsequently obtain or release the resources in advance; however it could be directed to under-utilization. Hence, a Hybrid Auto-Scaler (HAS) is proposed to adjust the required resources automatically to the application in demand. HAS forecasts the future behaviour of the system using a time series method and deploys the anticipated resources by computing the required capacity through a queuing model. Further, it uses a reactive approach to scale out the resources in accordance as the provisioned resources are insufficient to deal with the current needs. HAS also balances the load efficiently by employing Continuous Time Markov Model (CTMM). The proposed HAS is validated with several benchmark workloads to achieve significant improvement in CPU utilization and response time. © 2018 Elsevier Inc.","Auto-scaling; Continuous time Markov model; Queuing model; Virtualization"
"Online task scheduling for edge computing based on repeated stackelberg game","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052873006&doi=10.1016%2fj.jpdc.2018.07.019&partnerID=40&md5=c0d71feec0cc32303253961f56f106b0","A key function of an edge service provider (ESP) is to dynamically allocate resources to tasks existing at the edges upon request. This is, however, a challenging task due to a number of several factors: real-time decision-making without any prior knowledge of future arrivals, tasks’ satisfactions provided by requests, and utilization of resources. To address these challenges, we propose an online scheduling that maps various tasks to the given relevant resources based on a repeated Stackelberg game. First, we model this problem as a long-term vs. short-term repeated Stackelberg game. In particular, for each round of the game, acting as a short-term leader, a user with a request first decides the unit prices for processing tasks within the relevant budget to maximize current total satisfaction of tasks. Then, based on the prices offered by different users in different rounds, to maximize the long-term profits earned from users, the ESP acts as the follower whose strategy is matching resources with tasks, and splitting those tasks among different edge centers owning various types of resources (edge mobile devices). The Stackelberg equilibrium between the ESP and the users is obtained using our proposed algorithms. Finally, we evaluate the effectiveness of our proposal, in terms of task distributions. © 2018 Elsevier Inc.","Allocation scheduling; Edge computing resourcing; Equilibrium; Repeated Stackelberg game"
"NVHT: An efficient key–value storage library for non-volatile memory","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043398249&doi=10.1016%2fj.jpdc.2018.02.013&partnerID=40&md5=5fd0f31aa26062e074edddcaf1c9f2c2","Non-Volatile Memory (NVM) promises persistence, byte-addressability and DRAM-like read/write latency. These properties indicate that NVM has the potential to be incorporated with key–value stores to achieve high performance and durability simultaneously. Specifically, data can be stored in NVM inherently without DRAM buffering, which eliminates expensive disk I/Os and data format transformation cost. However, several challenges such as data inconsistency and write endurance arise along with the benefits. We propose a library named NVHT to provide APIs for NVM-based key–value store operations. In NVHT, we introduce non-volatile pointer to solve the dynamic address mapping problem and design a wear-out-aware memory allocator for NVM. The core of NVHT is a novel NVM-friendly hash table structure. NVHT guarantees consistency using a log-based mechanism. The experimental results show that compared with LevelDB and BerkeleyDB running on a DRAM-based file system, NVHT achieves more than 2x and 4x speedup for insert and search operation respectively. Compared with in-memory key–value store system Redis, NVHT still achieves higher transaction performance in terms of random update throughput (up to 1.5x and 2.5x for RDB scheme and AOF scheme, respectively). © 2018 Elsevier Inc.","In-memory system; Key–value store; Non-volatile memory"
"Application Characteristics-Aware Sporadic Cache Bypassing for high performance GPGPUs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053795012&doi=10.1016%2fj.jpdc.2018.09.001&partnerID=40&md5=42548c689a4d62e2a8984a9c85e80d53","Modern graphics processing units (GPUs) with massive parallel architecture can boost the performance of both graphics and general-purpose applications. With the support of new programming tools, GPUs have become one of the most attractive platforms in the exploitation of the high thread-level parallelism. In the recent GPUs, hierarchical cache memories have been employed to support irregular memory-access patterns. However, the L1 data cache exhibits a poor efficiency in GPUs, and this is mainly due to the cache contention and the resource congestion. This paper shows that the L1 data cache does not always positively impact applications in terms of the performance; in fact, many applications are even slowed down due to the use of the L1 data cache. In this paper, a novel cache bypassing mechanism (CARB) is proposed to increase the efficiency of the GPU cache management and to improve the GPU performance. The CARB mechanism exploits the characteristics of the currently executed applications to estimate the performance impact of the L1 data cache on the GPU, and it then allows memory requests to bypass the cache in discrete phases during the execution time. The bypassing decision is determined adaptively at runtime. Experiment results show that the CARB mechanism achieves an average speedup of 22% for a wide range of GPGPU applications. © 2018 Elsevier Inc.","Bypass; Cache; GPGPU; Miss rate; Performance"
"Using Jacobi iterations and blocking for solving sparse triangular systems in incomplete factorization preconditioning","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047148777&doi=10.1016%2fj.jpdc.2018.04.017&partnerID=40&md5=ab6036dc253a99060a0511a71dc7cda7","When using incomplete factorization preconditioners with an iterative method to solve large sparse linear systems, each application of the preconditioner involves solving two sparse triangular systems. These triangular systems are challenging to solve efficiently on computers with high levels of concurrency. On such computers, it has recently been proposed to use Jacobi iterations, which are highly parallel, to approximately solve the triangular systems from incomplete factorizations. The effectiveness of this approach, however, is problem-dependent: the Jacobi iterations may not always converge quickly enough for all problems. Thus, as a necessary and important step to evaluate this approach, we experimentally test the approach on a large number of realistic symmetric positive definite problems. We also show that by using block Jacobi iterations, we can extend the range of problems for which such an approach can be effective. For block Jacobi iterations, it is essential for the blocking to be cognizant of the matrix structure. © 2018 Elsevier Inc.","Iterative solvers; Preconditioning; Sparse linear systems; Triangular solves"
"Optimizing nonzero-based sparse matrix partitioning models via reducing latency","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052645214&doi=10.1016%2fj.jpdc.2018.08.005&partnerID=40&md5=0622bac99c2dd4a2ffee467a8f1a770c","For the parallelization of sparse matrix–vector multiplication (SpMV) on distributed memory systems, nonzero-based fine-grain and medium-grain partitioning models attain the lowest communication volume and computational imbalance among all partitioning models. This usually comes, however, at the expense of high message count, i.e., high latency overhead. This work addresses this shortcoming by proposing new fine-grain and medium-grain models that are able to minimize communication volume and message count in a single partitioning phase. The new models utilize message nets in order to encapsulate the minimization of total message count. We further fine-tune these models by proposing delayed addition and thresholding for message nets in order to establish a trade-off between the conflicting objectives of minimizing communication volume and message count. The experiments on an extensive dataset of nearly one thousand matrices show that the proposed models improve the total message count of the original nonzero-based models by up to 27% on the average, which is reflected on the parallel runtime of SpMV as an average reduction of 15% on 512 processors. © 2018 Elsevier Inc.","Communication overhead; Fine-grain partitioning; Hypergraph; Load balancing; Medium-grain partitioning; Recursive bipartitioning; Row–column-parallel SpMV; Sparse matrix; Sparse matrix–vector multiplication"
"Enabling cognitive contributory societies using SIoT: QoS aware real-time virtual object management","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054187069&doi=10.1016%2fj.jpdc.2018.09.007&partnerID=40&md5=5e76decb121bf1f22367f71b20b4010a","Internet of things (IoT) has initiated a few interesting research directions such as Social Internet of Things(SIoT), Cloud of Things(CoT), and Edge of Things(EoT) etc. Nowadays, a large number of IoT nodes are available in our surroundings and there is a need to ensure automated access and communication among these IoT nodes. An emerging area of study is to make a social platform for IoT nodes so that devices can communicate with each other and create automated and on-demand services. As these IoT nodes and services are going to co-exist with us (human), we foresee establishing of cognitive contributory skills where IoT nodes, services and even human skills can collectively form a cognitive society to share resources, information and skills. However, building such intelligent societies automatically using SIoT is a big challenge, mainly due to the complexity of the systems and availability of a large number of nodes. In such scenarios, it is not trivial to find a suitable SIoT service node correctly to avail a service in real-time. We propose a common platform to virtualize the physical objects and make them available in the cyber-world and at the same time to ensure resource sharing. In this paper, we focus on virtual object management and selection process for SIoT platform. We propose a QoS aware object selection using Integer Programming solutions to find a right service at the right time. © 2018 Elsevier Inc.","Cognitive societies; Ontology; Optimization; QoS; Social IoT (SIoT); Virtual object selection"
"Accelerating breadth-first graph search on a single server by dynamic edge trimming","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032175992&doi=10.1016%2fj.jpdc.2017.09.007&partnerID=40&md5=49b9b3cea0c6f8a8f1b5cab9d2e1f3a6","Breadth-first graph search (a.k.a., BFS) is one of the typical in-memory computing models with complicated and frequent memory accesses. Existing single-server graph computing systems fail to take advantage of access pattern of BFS for performance optimization, hence suffering from a lot of extra memory latencies due to accessing no longer useful data elements of a big graph as well as wasting plenty of computing resources for processing them. In this article, we propose FastBFS, a new approach that accelerates breadth-first graph search on a single server by leverage of the access pattern during iterating over a big graph. First, FastBFS uses an edge-centric graph processing model to obtain the high bandwidth of sequential memory and/or disk access without expensive data preprocessing. Second, with a dynamic and asynchronous trimming mechanism, FastBFS can efficiently reduce the size of a big graph by eliminating useless edges in parallel with the computation. Third, FastBFS schedules I/O streams efficiently and can attain greater parallelism if an additional disk is available. We implement FastBFS by modifying the X-Stream system developed by EPFL. Our experimental results show that FastBFS can attain speedups of up to 7.9× and 10.4× in the computing speed compared with X-stream and GraphChi respectively. With an additional disk, the performance can be further improved. © 2017 Elsevier Inc.","BFS; Graph computing; I/O optimization; In-memory computing"
"Design and performance evaluation of Mesh-of-Tree-based hierarchical wireless network-on-chip for multicore systems","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054465858&doi=10.1016%2fj.jpdc.2018.09.008&partnerID=40&md5=5dc21eebdefab87ff19c45c9b2ecce87","Hybrid Wireless Network on Chip (WNoC) architecture has been proposed as a promising solution for addressing on-chip communication problems in many multicore systems. The choice of infrastructure topology directly affects the performance gain of the architecture. For exploiting the benefits offered by both mesh and tree topologies, we employ Mesh-of-Tree (MoT) topology as a new communication infrastructure for hybrid WNoC architectures. Moreover, long-distance links are effectively added to the MoT topology to form wireless MoT architecture and an appropriate communication routing scheme is presented to employ this paradigm. The performance and the system cost of the proposed WNoC architecture have been evaluated and compared with other notable WNoC architectures through comprehensive network-level simulations. Experimental results demonstrate the effectiveness of this wireless MoT architecture under both synthetic and realistic traffic patterns in terms of network throughput, latency, and power consumption. The results demonstrate that the proposed architecture can approximately achieve 35% improvement in saturation throughput, 58% reduction in transmission latency, and 43% improvement in power consumption over a wireline MoT-NoC, on average. © 2018 Elsevier Inc.","Communication protocol; Mesh-of-Tree (MoT); Multicore systems; Network-on-Chip (NoC); Wireless interconnections"
"Opportunistic computing offloading in edge clouds","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054463716&doi=10.1016%2fj.jpdc.2018.09.006&partnerID=40&md5=1eb3afed19bc71b3b64ad8171552afc6","Nowadays, the advanced mobile devices provide considerable computation capacity. However, due to the instinct limitation of resources, mobile devices have to offload computation by Mobile Cloud Computing (MCC) and Ad-Hoc Cloudlet for improving the performance and prolonging the battery life. However, efficient model for ad-hoc cloudlet-assisted computation offloading is remaining open issues. In this article, we provide an overview of existing computation offloading modes, e.g. remote cloud service mode and ad-hoc cloudlet-assisted service mode, and propose an opportunistic computation offloading (OPPOCO) to enable a more energy-efficient and intelligent strategy for computation offloading. Moreover, the simulation by OPNET verifies that our proposal is available and practical to improve mobile users’ Quality of Service (QoS) and Quality of Experience (QoE). © 2018","Computing offloading; Edge clouds; IoT"
"Approximate and incremental network function placement","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049331701&doi=10.1016%2fj.jpdc.2018.06.006&partnerID=40&md5=ef57c129161c3d5b0aea9f0d55a2440d","The virtualization of modern computer networks introduces interesting new opportunities for a more flexible placement of network functions and middleboxes (firewalls, proxies, traffic optimizers, virtual switches, etc.). This paper studies approximation algorithms for the incremental deployment of a minimum number of middleboxes, such that capacity constraints at the middleboxes and length constraints on the communication routes are respected. Based on a new, purely combinatorial and rigorous proof of submodularity, we obtain our main result: a deterministic greedy approximation algorithm which only employs augmenting paths to serve future communication pairs. Hence, our algorithm does not require any changes to the locations of existing middleboxes or the preemption of previously served communication pairs when additional middleboxes are deployed. It is hence particularly attractive for incremental deployments. We prove that the achieved polynomial-time approximation bound is optimal, unless P=NP holds. This paper also initiates the study of a weighted problem variant, in which entire groups of nodes need to communicate via a middlebox, possibly at different rates. We present an LP relaxation and randomized rounding algorithm for this problem, leveraging an interesting connection to scheduling. We complement our formal results with a simulation study of a large set of synthetically generated instances. Our results indicate that the presented algorithms yield near-optimal solutions in practice. © 2018 Elsevier Inc.","Algorithms; Network function virtualization; Networked systems; Scheduling; Software-defined networking"
"Dark-Silicon Aware Design Space Exploration","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037740223&doi=10.1016%2fj.jpdc.2017.11.002&partnerID=40&md5=3a35e45cb978c1257397b345fdce1ddb","The design of processor platforms comprised of multiple cores has been subject of dramatic changes in the last years. Mainly due to physical constraints imposed by increases in leakage current stemming from shrinking dimensions in transistor manufacturing processes. Such constraints have brought forth what we know as “dark silicon” the area of a chip which should be turned off or work on a minimum clock frequency to meet the power dissipation constraints. The technical challenge has been how to choose the hardware blocks (type and number) to meet all chip design constraints and goals. This work introduces a less conservative dark silicon estimate based on chip components power density and technological process and a technique that performs the design space exploration aware of the dark silicon constraints. Our design space exploration technique is built on the top of a multiobjective optimization model and it adopts the NSGA-II genetic algorithm to provide solutions (platforms) aware of the dark silicon. The technique has been validated and evaluated along with a brute force algorithm. Our experimental results show dark silicon chip percentages up to 13.61% leading to a chip area of 134mm2 which is tantamount to three cores area on the chip. The experiments comparing our DS-DSE performance to the brute force algorithm have shown that our strategy presents more performance scalability as the design space exploration (IP cores database) is increased. © 2017 Elsevier Inc.","Dark silicon; Design space exploration (DSE); Multicore design; Performance simulation; Physical design estimates"
"TSGL: A tool for visualizing multithreaded behavior","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044942771&doi=10.1016%2fj.jpdc.2018.02.025&partnerID=40&md5=f7aa412e5d3c16de7cb0eb1a8ccd5f5b","Since multicore processors are now the architectural standard and parallel computing is in the core CS curriculum, CS educators must create pedagogical materials and tools to help their students master parallel abstractions and concepts. This paper describes the thread safe graphics library (TSGL), a tool by which an educator can add graphics calls to a working multithreaded program in order to make visible the underlying parallel behavior. Using TSGL, an instructor (or student) can create parallel visualizations that clearly show the parallel patterns or techniques a given program is using, allowing students to see the parallel behavior in near real-time as the program is running. TSGL includes many examples that illustrate its use; this paper presents a representative sample, that can be used either in a lecture or a self-paced lab format. We also present evidence that such visualizations improve student understanding of abstract parallel concepts. © 2018 Elsevier Inc.","Graphics; Library; Multicore; Parallel; Threads; Visualization"
"An approach to task-based parallel programming for undergraduate students","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044095811&doi=10.1016%2fj.jpdc.2018.02.022&partnerID=40&md5=5679ec40cbb2704a82882fb3f23eebd5","This paper presents the description of a compulsory parallel programming course in the bachelor degree in Informatics Engineering at the Barcelona School of Informatics, Universitat Politècnica de Catalunya UPC-BarcelonaTech. The main focus of the course is on the shared-memory programming paradigm, which facilitates the presentation of fundamental aspects and notions of parallel computing. Unlike the “traditional” loop-based approach, which is the focus of parallel programming courses in other universities, this course presents the parallel programming concepts using a task-based approach. Tasking allows students to explore a broader set of parallel decomposition strategies, including linear, iterative and recursive strategies, and their implementation using the current version of OpenMP (OpenMP 4.5), which offers mechanisms (pragmas and intrinsic functions) to easily map these strategies into parallel programs. Simple models to understand the benefits of a task decomposition and the trade-offs introduced by different kinds of overheads are included in the course, together with the use of tools that allow an easy exploration of different task decomposition strategies and their potential parallelism (Tareador) and instrumentation and analysis of task parallel executions on real machines (Extrae and Paraver). © 2018 Elsevier Inc.","OpenMP tasking model; Performance models and tools; Task decomposition strategies and programming"
"Coping with silent and fail-stop errors at scale by combining replication and checkpointing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052881011&doi=10.1016%2fj.jpdc.2018.08.002&partnerID=40&md5=dd311736320907d1a36d4d2905654779","This paper provides a model and an analytical study of replication as a technique to cope with silent errors, as well as a mixture of both silent and fail-stop errors on large-scale platforms. Compared with fail-stop errors that are immediately detected when they occur, silent errors require a detection mechanism. To detect silent errors, many application-specific techniques are available, either based on algorithms (e.g., ABFT), invariant preservation or data analytics, but replication remains the most transparent and least intrusive technique. We explore the right level (duplication, triplication or more) of replication for two frameworks: (i) when the platform is subject to only silent errors, and (ii) when the platform is subject to both silent and fail-stop errors. A higher level of replication is more expensive in terms of resource usage but enables to tolerate more errors and to even correct some errors, hence there is a trade-off to be found. Replication is combined with checkpointing and comes with two flavors: process replication and group replication. Process replication applies to message-passing applications with communicating processes. Each process is replicated, and the platform is composed of process pairs, or triplets. Group replication applies to black-box applications, whose parallel execution is replicated several times. The platform is partitioned into two halves (or three thirds). In both scenarios, results are compared before each checkpoint, which is taken only when both results (duplication) or two out of three results (triplication) coincide. Otherwise, one or more silent errors have been detected, and the application rolls back to the last checkpoint, as well as when fail-stop errors have struck. We provide a detailed analytical study for all of these scenarios, with formulas to decide, for each scenario, the optimal parameters as a function of the error rate, checkpoint cost, and platform size. We also report a set of extensive simulation results that nicely corroborates the analytical model. © 2018 Elsevier Inc.","Checkpointing; Fail-stop errors; Fault tolerance; High-performance computing; Replication; Silent errors"
"Many-core needs fine-grained scheduling: A case study of query processing on Intel Xeon Phi processors","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039726997&doi=10.1016%2fj.jpdc.2017.09.005&partnerID=40&md5=f2ac340368f44c5acbb865914b9b54a6","Emerging many-core processors feature very high memory bandwidth and computational power. For example, Intel Xeon Phi many-core processors of the Knights Corner (KNC) and Knights Landing (KNL) architectures embrace 60 to 64 x86-based CPU cores with 512-bit SIMD capabilities and high-bandwidth memories like the GDDR5 on KNC and on-package DRAMs on KNL. In this paper, we study the performance main-memory database operators and online analytical processing (OLAP) on such many-core architectures. We find that even the state-of-the-art database operators suffer severely from memory stalls and resource underutilization on those many-core processors. We argue that a software approach decomposing a coarse-grained operator into fine-grained phases and executing two independent phases with complementary resource requirements concurrently can address this problem. This approach allows more fine-grained control of resource utilization. Our experiments demonstrate significant performance gain and high resource utilization achieved by our proposed approaches on both KNC and KNL. © 2017 Elsevier Inc.","Fine-grained scheduling; In-memory query processing; Many-core processor"
"DADTA: A novel adaptive strategy for energy and performance efficient virtual machine consolidation","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049893081&doi=10.1016%2fj.jpdc.2018.06.011&partnerID=40&md5=ab0176881fb44a497f473eeddfaa9b72","Large-scale virtualized data centers are increasingly becoming the norm in our data-intensive society. One pressing challenge is to reduce energy consumption in such data centers for edge computing deployment, which would have flow-on effects on reducing the operating costs and carbon dioxide emissions. Dynamic virtual machine consolidation is an effective way to improve resource utilization and energy efficiency. In this paper, a comprehensive strategy is proposed, which is based on time-series forecasting approach. In this strategy, specific adjustment of threshold is applied to adapt the dynamic workload. We then use a real-world dataset (i.e. workload trace in Google) for evaluation, whose findings demonstrate that our strategy outperforms other benchmarks. © 2018 Elsevier Inc.","Cloud data center; Energy conservation; IaaS Cloud; SLAV; Trade-off"
"Edge computing framework for enabling situation awareness in IoT based smart city","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053423759&doi=10.1016%2fj.jpdc.2018.08.009&partnerID=40&md5=eb15863fe79781437daa7bf93fe94345","The Internet of Things (IoT) offers a lot of benefits for building smart cities. Such cities will be able to utilize a huge number of heterogeneous IoT devices that can generate a sheer volume of data. So, considering this heterogeneity, one of the major challenges in smart cities is how to process this data and identify different situations for decision-makers on the basis of this data. The traditional cloud computing approach can provide enormous computing and storage facilities to support data processing. However, it requires all the data to be moved to the cloud from the edge devices of the user endpoint, thus introducing a high latency. In this paper, we used the edge computing approach to minimize such latency. Besides, as major portion of data is generated from the user endpoint, processing this data in the edge can significantly improve the performance. Our experiment shows that processing raw IoT data at the edge devices is effective in terms of latency and provides situational awareness for the decision makers of smart city in a seamless fashion. © 2018 Elsevier Inc.","Edge computing; Internet of Things; Situation awareness; Smart city"
"FluteDB: An efficient and scalable in-memory time series database for sensor-cloud","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051828251&doi=10.1016%2fj.jpdc.2018.07.021&partnerID=40&md5=0cd42e3cffd7a75a75be40a93a64c1e1","Recently, with the widespread use of large-scale sensor network, time series data is vastly generated and requires to be processed. However, those traditional databases show their limitations on storage when handling such a large stream data in cloud, and even their actual reliability and availability are also difficult to be guaranteed. To deal with the problem, this paper proposes FluteDB, an efficient and scalable in-memory time series database for sensor-cloud. We adequately analyze the unique characteristics of time series data and its relevant operations to strike the right balance among efficiency, scalability, resources consumption, reliability and availability. Specifically, on basis of the aggregate analysis of root cause for ongoing time series problems, FluteDB targeted optimizes the strategies for key operations in memory and physical storage, at the expense of partial acceptable data precision and consistency. FluteDB's enhanced strategies are primarily comprised of Triggered Time Series Merge Tree (TTSM Tree), time series enhanced cache management and corresponding compression algorithms for different data types. The validations of all sub-modules have demonstrated that our improved strategies outperform existing methods in real time series environment significantly. Global experimental results also show that the integrated FluteDB reduces query latency by 17x, improves write rate by 98x and saves about 47% storage resources. The average available service time and recovery rate and degree of FluteDB are competitive with the state-of-the-art reliability and availability strategy in real and simulated faults, which demonstrates FluteDB can provide highly stable large-scale data cloud services. © 2018 Elsevier Inc.","Disaster tolerant; In-memory database; Scalability; Sensor-cloud; Time series"
"Response time optimization for cloudlets in Mobile Edge Computing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046168877&doi=10.1016%2fj.jpdc.2018.04.004&partnerID=40&md5=8652a5601f7fda6ac85ec26dbdc63ca2","Mobile Edge computing (MEC) and caching are new forms of computing architecture which brings network functions to the network edge or physical proximity of the users. In MEC, the mobile cloudlets are smart phones which offer computational services to other smart phones available in the physical proximity. Selection of the device for offloading computationally intensive tasks is a very important criterion for end-users Quality of Service. This work focuses on optimal selection of devices which receives offloaded computational tasks. This paper presents a non-cooperative extensive game model where players maximize their pay-offs which leads to minimization of response time. Further, we make our proposed model to implicitly depend upon the battery life of the computational task receivers. The game model achieves Nash Equilibrium by using backward induction technique. This work also takes care of the device availability by clustering the previous availability data. Finally, we evaluate the performance of the proposed model upon response time, end-users utility and memory utilizations. We also see that the proposed work out performs the different schemes which we used for comparisons. © 2018 Elsevier Inc.","Extensive game model; Load balancing; Mobile cloudlets; Mobile Edge Computing"
"An efficient tile size selection model based on machine learning","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049643247&doi=10.1016%2fj.jpdc.2018.06.005&partnerID=40&md5=1d339676c3ba21330944f5472c63537b","Tiling is a classic loop optimization to improve data locality and achieve coarse-grained parallelism. Tile size selection (TSS) plays an important role in tiling to determine the performance of tiled codes. Most of the previous TSS approaches involve much highly skilled manpower, but it is still difficult to find the optimal tile sizes. In this article, we propose an efficient TSS model using machine learning technique to predict optimal rectangular tile sizes for a given program on multi-core processors. A set of loop features is extracted on tiled codes to capture the locality of data references and the effect of vectorization in tiled loop dimensions. Using the features and corresponding best tile sizes, the generalized regression neural network is employed to build the TSS model, hiding the complicated interactions between tile sizes and underlying factors. Although the impact of multithreading is not directly considered in training the model, the predicted tile sizes can be well adapted to different numbers of threads. Experimental results show that the predicted tile sizes achieve 90% and 81% of the optimal performance on average for 20 selected benchmarks on an Intel Xeon and an IBM Power6 multi-core platforms, respectively. The optimal performance is delivered by the tile sizes that are obtained through a heuristically exhaustive search. Our TSS model outperforms an artificial neural network (ANN)-based TSS prediction model which depends on the prefetched features by over 9% in average performance for 9 benchmarks. It also outperforms a state-of-the-art analytical TSS model which uses the cache set associativity and interaction with the single instruction multiple data (SIMD) units to estimate the optimal tile sizes by over 7% in average performance for 7 benchmarks. © 2018 Elsevier Inc.","Locality of data references; Loop features; Machine learning; Parallel load balance; Tile size selection"
"Uniform deployment of mobile agents in asynchronous rings","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046170436&doi=10.1016%2fj.jpdc.2018.03.008&partnerID=40&md5=7c0f35611a0314cfd4fe75dac82c41f7","In this paper, we consider the uniform deployment problem of mobile agents in asynchronous unidirectional rings, which requires the agents to uniformly spread in the ring. The uniform deployment problem is in striking contrast to the rendezvous problem which requires the agents to meet at the same node. While rendezvous aims to break the symmetry, uniform deployment aims to attain the symmetry. It is well known that the symmetry breaking is difficult in distributed systems and the rendezvous problem cannot be solved from some initial configurations. Hence, we are interested in clarifying what difference the uniform deployment problem has on the solvability and the number of agent moves compared to the rendezvous problem. We consider two problem settings, with knowledge of k (or n) and without knowledge of k or n where k is the number of agents and n is the number of nodes. First, we consider agents with knowledge of k (or n since k andn can be easily obtained if one of them is given). In this case, we propose two algorithms. The first algorithm solves the uniform deployment problem with termination detection. This algorithm requires O(klogn) memory space per agent, O(n) time, and O(kn) total moves. The second algorithm also solves the uniform deployment problem with termination detection. This algorithm reduces the memory space per agent to O(logn), but uses O(nlogk) time, and requires O(kn) total moves. Both algorithms are asymptotically optimal in terms of total moves since there are some initial configurations such that agents require Ω(kn) total moves to solve the problem. Next, we consider agents with no knowledge of k or n. In this case, we show that, when termination detection is required, there exists no algorithm to solve the uniform deployment problem. For this reason, we consider the relaxed uniform deployment problem that does not require termination detection, and we propose an algorithm to solve the relaxed uniform deployment problem. This algorithm requires O((k∕l)log(n∕l)) memory space per agent, O(n∕l) time, and O(kn∕l) total moves when the initial configuration has symmetry degree l. This means that the algorithm can solve the problem more efficiently when the initial configuration has higher symmetric degree (i.e., is closer to uniform deployment). Note that all the proposed algorithms achieve uniform deployment from any initial configuration, which is a striking difference from the rendezvous problem because the rendezvous problem is not solvable from some initial configurations. © 2018 Elsevier Inc.","Distributed system; Mobile agent; Ring networks; Symmetry degree; Token; Uniform deployment"
"Parallel exploration of partial solutions in Boolean matrix factorization","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054908648&doi=10.1016%2fj.jpdc.2018.09.014&partnerID=40&md5=191bcfd8e849e6deb2559c2eb8463790","Boolean matrix factorization (BMF) is a well established method for preprocessing and analysis of data. There is a number of algorithms for BMF, but none of them uses benefits of parallelization. This is mainly due to the fact that many of the algorithms utilize greedy heuristics that are inherently sequential. In this work, we propose a general parallelization scheme for BMF in which several locally optimal partial matrix decompositions are constructed simultaneously in parallel, instead of just one in a sequential algorithm. As a result of the computation, either the single best final decomposition or several top-k of them may be returned. The scheme can be applied to any sequential heuristic BMF algorithm and we show the application on two representative algorithms, namely GRECOND and ASSO. Improvements in decompositions are presented via results from experiments with the new algorithms on synthetic and real datasets. © 2018 Elsevier Inc.","Boolean matrix factorization; Data preprocessing; Parallel algorithm"
"Distributed computing by leveraging and rewarding idling user resources from P2P networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051678618&doi=10.1016%2fj.jpdc.2018.07.017&partnerID=40&md5=fc88b06331cde67cd8d501c150aa22f4","Currently, many emerging computer science applications call for collaborative solutions to complex projects that require huge amounts of computing resources to be completed, e.g., physical science simulation, big data analysis. Many approaches have been proposed for high performance computing designing a task partitioning strategy able to assign pieces of execution to the appropriate workers in order to parallelize task execution. In this paper, we describe the CoremunitiTM system, our peer to peer solution for solving complex works by using the idling computational resources of users connected to our network. More in detail, we designed a framework that allows users to share their CPU and memory in a secure and efficient way. By doing this, users help each other by asking the network computational resources when they face high computing demanding tasks. In this respect, as users provide their computational power without providing specific human skill, our approach can be considered as a hybrid crowdsourcing. Differently from many proposals available for volunteer computing, users providing their resources are rewarded with tangible credits, i.e., they can redeem their credits by asking for computational power to solve their own task and/or by exchanging them for money. We conducted a comprehensive experimental assessment in an interesting scenario as 3D rendering, which allowed us to validate the scalability and effectiveness of our solution and its profitability for end-users. As we do not require to power additional resources for solving tasks (we better exploit unused resources already powered instead), we hypothesize a remarkable side effect at steady state: energy consumption reduction compared with traditional server farms or cloud based executions. © 2018 Elsevier Inc.",""
"A multi-hop pointer forwarding scheme for efficient location update in low-rate wireless mesh networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051988202&doi=10.1016%2fj.jpdc.2018.07.012&partnerID=40&md5=ad4835bb2c63b44d165e27fd8b77dd51","Recently, a pointer forwarding scheme (PFS) was proposed to reduce location update overhead in wireless mesh networks. Using PFS, a location update is replaced with a simple forwarding pointer setup between two neighboring mesh routers (MRs). However, in PFS, if the two MRs are not one hop neighbors, PFS fails to set up a forwarding pointer, thus increasing location update overhead. To improve PFS, we present a multi-hop pointer forwarding scheme (MPFS). MPFS allows forwarding pointers to be constructed over multi-hop at once even if MRs are not one hop neighbor by using logical tree distance constructed during network formation. The tree distance is used to relay forwarding pointer packets over multi-hop links without additional control overhead during forwarding pointer setup and to estimate hop distance between two MRs. By doing so, MPFS improves the probability of success in forwarding pointer setup while ensuring k≤km, resulting in lowering the location update overhead. Also, we analyze pointer forwarding success probability and average chain length and discuss why MPFS is suitable for resource-constrained LRWMNs. Using ns-2, we show that MPFS significantly reduces the number of location update events, location update delay and signaling overhead, and packet losses during location updates. With real-world implementation, we also confirm feasibility of MPFS. © 2018 Elsevier Inc.","6LoWPAN; Handoff; IEEE 802.15.4; Location management; Mobility management; Pointer forwarding; Wireless mesh networks"
"Fast classification of MPI applications using Lamport's logical clocks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048493885&doi=10.1016%2fj.jpdc.2018.05.005&partnerID=40&md5=003d31242a252ca521d4d9de55e4063b","We present a novel trace-based analysis tool that rapidly classifies an MPI application as bandwidth-bound, latency-bound, load-imbalance-bound, or computation-bound for different interconnection networks. The tool uses an extension of Lamport's logical clock to track application progress in the trace replay. It has two unique features. First, it can predict application performance for many latency and bandwidth parameters from a single replay of the trace. Second, it infers the performance characteristics of an application and classifies the application using the predicted performance trend for a range of network configurations instead of using the predicted performance for a particular network configuration. We describe the techniques used in the tool and its design and implementation, and report our performance study of the tool and our experience with classifying twelve applications and mini-apps from the DOE DesignForward project as well as the NAS Parallel Benchmarks. © 2018 Elsevier Inc.","Parallel application; Performance analysis; Tool"
"Security supportive energy-aware scheduling and energy policies for cloud environments","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047114393&doi=10.1016%2fj.jpdc.2018.04.015&partnerID=40&md5=2f555414a01c8ac58f2d9df0e70c0dfd","Cloud computing (CC) systems are the most popular computational environments for providing elastic and scalable services on a massive scale. The nature of such systems often results in energy-related problems that have to be solved for sustainability, cost reduction, and environment protection. In this paper we defined and developed a set of performance and energy-aware strategies for resource allocation, task scheduling, and for the hibernation of virtual machines. The idea behind this model is to combine energy and performance-aware scheduling policies in order to hibernate those virtual machines that operate in idle state. The efficiency achieved by applying the proposed models has been tested using a realistic large-scale CC system simulator. Obtained results show that a balance between low energy consumption and short makespan can be achieved. Several security constraints may be considered in this model. Each security constraint is characterized by: (a) Security Demands (SD) of tasks; and (b) Trust Levels (TL) provided by virtual machines. SD and TL are computed during the scheduling process in order to provide proper security services. Experimental results show that the proposed solution reduces up to 45% of the energy consumption of the CC system. Such significant improvement was achieved by the combination of an energy-aware scheduler with energy-efficiency policies focused on the hibernation of VMs. © 2018 Elsevier Inc.","Cloud computing; Cloud security; Energy efficiency; Genetic algorithms; Independent task scheduling; VM hibernating"
"Curve fitting based efficient parameter selection for robust provable data possession","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047874657&doi=10.1016%2fj.jpdc.2018.05.007&partnerID=40&md5=db97da0e34c3c81d5034ebc44842ad6d","Cyberspace faces a series of threats that can spread rapidly across distributed systems. Many such transmissible cyber threats aim to damage users’ data. In recent years, the popularity of cloud computing has driven a lot of users to save their data in the cloud. The centralization of users’ data in the cloud has created new opportunities and incentives for transmissible cyber threats targeting users’ data. In this context, in addition to cloud vendor's security mechanisms, it is important to allow users to efficiently verify the integrity of their data saved in the cloud. The seminal sampling based PDP (Provable Data Possession) scheme can attain effective probabilistic verification with high computational efficiency for the integrity of users’ data saved in the cloud by use of a set of randomly sampled data blocks. By integrating with the forward error correcting (FEC) technique, a recently-proposed robust PDP scheme can protect against arbitrarily small amounts of data corruption and has therefore been widely adopted in practice. It is a core task to determine the number of sample blocks in this scheme because this parameter plays a fundamental role in balancing the security of the scheme and the computational cost. A smaller value can comprise the security while a larger one can incur extra high computational cost. Existing work mainly leverages the Monte Carlo methods to estimate this parameter. However, these methods suffer from heavy computational cost. In this paper, we propose a method to determine this parameter based on the curve fitting technique. Specifically, we formally analyze the parameter selection process of the robust PDP scheme, and leverage the curve fitting technique to improve the efficiency of parameter selection while ensuring the optimality of the number of samples for the robustness of the scheme. Extensive experimental results demonstrate the effectiveness and efficiency of our approach. Specifically, it can be 25 times faster than the existing solution for 1, 000, 000 times simulated attacks. © 2018 Elsevier Inc.","Cloud computing; Cloud storage; Integrity verification; Parameter selection; Provable data possession"
"Memory efficient parallel algorithm for optimal DAG structure search using direct communication","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045440206&doi=10.1016%2fj.jpdc.2018.03.011&partnerID=40&md5=af71d0405383481d64763d63ab52ed39","We present a novel parallel algorithm that solves the optimal directed acyclic graph search problem using less memory as compared to existing algorithms. Using a dynamic programming approach, the time and space complexity of the problem is found to be O(n⋅2n), where n represents the number of vertices. The previous algorithm uses adjacent communication to efficiently exchange data between computation nodes. However, it consumes too much memory, and thus is capable of solving up to n=36. Our novel algorithm, ParaOS-DC, employs direct communication to reduce the memory consumption; this is the main cause of the inefficiency of the previous algorithm. Through computational experiments, we confirmed that our proposed algorithm is much faster than the previous algorithm when memory is insufficient. We also succeeded in solving a problem for n=37 without any constraints, which is the largest problem size solved in literature till date. © 2018 Elsevier Inc.","Bayesian network learning; Optimal DAG search algorithm"
"FPGA-based simultaneous multichannel audio processor for musical genre indexing applications in broadcast band","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047075002&doi=10.1016%2fj.jpdc.2018.02.011&partnerID=40&md5=b2841379be50df24979aac2fa7ba0988","This paper presents an architecture for real-time music genres classification on broadcast data from the standard Frequency Modulation (FM) radio band. The architecture is composed of an FPGA-based MFCC (Mel-Frequency Cepstral Coefficient) feature extraction stage, followed by a classification procedure. The proposed system enables automatic audio indexing of broadcast data from the standard FM radio band. Using a system-level design approach that reduces overall design time, the system was successfully implemented on Virtex 6 FPGA clocked at more than 150 MHz. The experiments have shown on one hand, a high accuracy between the FPGA-based MFCC calculation and its Matlab-based reference model, and on the other hand, nearly similar result when used in music genre classification with sequential patterns mining techniques. Furthermore, the proposed system satisfies real-time requirements, multichannel scalability and is suitable for indexing applications in embedded systems. Results are also discussed regarding operating frequencies and resources utilization. © 2018 Elsevier Inc.","Audio indexing; FPGA prototyping; Music genre classification; Software radio; System-level design"
"Remote visual analysis of large turbulence databases at multiple scales","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048804257&doi=10.1016%2fj.jpdc.2018.05.011&partnerID=40&md5=ae8ad9bdd3fb0cb00133587ef2d77ad2","The remote analysis and visualization of raw large turbulence datasets is challenging. Current accurate direct numerical simulations (DNS) of turbulent flows generate datasets with billions of points per time-step and several thousand time-steps per simulation. Until recently, the analysis and visualization of such datasets was restricted to scientists with access to large supercomputers. The public Johns Hopkins Turbulence database simplifies access to multi-terabyte turbulence datasets and facilitates the computation of statistics and extraction of features through the use of commodity hardware. We present a framework designed around wavelet-based compression for high-speed visualization of large datasets and methods supporting multi-resolution analysis of turbulence. By integrating common technologies, this framework enables remote access to tools available on supercomputers and over 230 terabytes of DNS data over the Web. The database toolset is expanded by providing access to exploratory data analysis tools, such as wavelet decomposition capabilities and coherent feature extraction. © 2018 Elsevier Inc.","Data reduction; Databases; Distributed systems; Remote visualization; Turbulence; Wavelets"
"Exploiting social network graph characteristics for efficient BFS on heterogeneous chips","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044537905&doi=10.1016%2fj.jpdc.2017.11.003&partnerID=40&md5=ccdfcec077842b797cd2cf4084814bb4","Several approaches implement efficient BFS algorithms for multicores and for GPUs. However, when targeting heterogeneous architectures, it is still an open problem how to distribute the work among the CPU cores and the accelerators. In this paper, we assess several approaches to perform BFS on different heterogeneous chips (a multicore CPU and an integrated GPU). In particular, we propose three heterogeneous approaches that exploit the collaboration between both devices: Selective, Concurrent and Asynchronous. We identify how to take advantage of the features of social network graphs, that are a particular example of highly connected graphs-with fewer iterations and more unbalanced-, as well as the drawbacks of each algorithmic implementation. One key feature of our approaches is that they switch between different versions of the algorithm, depending on the device that collaborates in the computation. Through exhaustive evaluation we find that our heterogeneous implementations can be up to 1.56× faster and 1.32× more energy efficient with respect to the best baseline where only one device is used, being the overhead w.r.t. an oracle scheduler below 10%. We also compare with other related heterogeneous approach finding that ours can be up to 3.6× faster. © 2017 Elsevier Inc.","BFS; Heterogeneous chips; Performance–energy efficiency; Social network graphs"
"Teaching distributed memory programming from mental models","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044300671&doi=10.1016%2fj.jpdc.2018.02.029&partnerID=40&md5=c713cccc7616a0497b8c9b9a1c088d77","Distributed memory programming, typically through the MPI library, is the de facto standard for programming large scale parallelism, with up to millions of individual processes. Its dominant paradigm of Single Program Multiple Data (SPMD) programming is different from threaded and multicore parallelism, to an extent that students have a hard time switching models. In contrast to threaded programming, which allows for a view of the execution with central control and a central repository of data, SPMD programming has a symmetric model where all processes are active all the time, with none privileged, and where data is distributed. This model is counterintuitive to the novice parallel programmer, so care needs to be taken how to instill the proper ‘mental model’. Adoption of an incorrect mental model leads to broken or inefficient code. We identify problems with the currently common way of teaching MPI, and propose a structuring of MPI courses that is geared to explicit reinforcing the symmetric model. Additionally, we advocate starting from realistic scenarios, rather than writing artificial code just to exercise newly-learned routines. © 2018","Distributed memory; Mental models; Parallel programming; Teaching parallel programming"
"A scalable distributed machine learning approach for attack detection in edge computing environments","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045682233&doi=10.1016%2fj.jpdc.2018.03.006&partnerID=40&md5=5071e6ed2406720f471e3533d8792b67","The ever-increasing number of IoT applications and cyber–physical services is introducing significant challenges associated to their cyber-security. Due to the constrained nature of the involved devices, some heavier computational tasks, such as deep traffic inspection and classification, essential for implementing automatic attack detection systems, are moved on specialized “edge” devices, in order to distribute the processing intelligence near to the data sources. These edge devices are mainly capable of effectively running pre-built classification models but have not enough storage and processing capabilities to build and upgrade such models from huge volumes of field training data, imposing a serious barrier to the deployment of such solutions. This work leverages the flexibility of cloud-based architectures, together with the recent advancements in the area of large-scale machine learning for shifting the more computationally-expensive and storage-demanding operations to the cloud in order to benefit of edge computing capabilities only for effectively performing traffic classification based on sophisticated Extreme Learning Machines models that are pre-built over the cloud. © 2018 Elsevier Inc.","Attack detection; Distributed machine learning; Edge computing; Extreme learning machines; IoT"
"Towards high performance data analytic on heterogeneous many-core systems: A study on Bayesian Sequential Partitioning","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050959763&doi=10.1016%2fj.jpdc.2018.07.011&partnerID=40&md5=f2ca747fe0a95d92d36e8be610364235","Bayesian Sequential Partitioning (BSP) is a statistically effective density estimation method to comprehend the characteristics of a high dimensional data space. The intensive computation of the statistical model and the counting of enormous data have caused serious design challenges for BSP to handle the growing volume of the data. This paper proposes a high performance design of BSP by leveraging a heterogeneous CPU/GPGPU system that consists of a host CPU and a K80 GPGPU. A series of techniques, on both data structures and execution management policies, is implemented to extensively exploit the computation capability of the heterogeneous many-core system and alleviate system bottlenecks. When compared with a parallel design on a high-end CPU, the proposed techniques achieve 48x average runtime enhancement while the maximum speedup can reach 78.76x. © 2018 Elsevier Inc.","Data processing; Design and optimization; Heterogeneous system; Many-core system; Performance analysis"
"A framework for joint resource allocation of MapReduce and web service applications in a shared cloud cluster","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048843187&doi=10.1016%2fj.jpdc.2018.05.010&partnerID=40&md5=98dedc6e0064359cb507ae62156e41dd","The ongoing uptake of cloud-based solutions by different business domains and the rise of cross-border e-commerce in the EU require for additional public and private cloud solutions. Private clouds are an alternative for e-commerce sites to host not only Web Service (WS) applications but also Business Intelligence ones that consist of batch and/or interactive queries and resort to the MapReduce (MR) programming model. In this study, we take the perspective of an e-commerce site hosting its WS and MR applications on a fixed-size private cloud cluster. We assume Quality of Service (QoS) guarantees must be provided to end-users, represented by upper-bounds on the average response times of WS requests and on the MR jobs execution times, as MR applications can be interactive nowadays. We consider multiple MR and WS user classes with heterogeneous workload intensities and QoS requirements. Being the cluster capacity fixed, some requests may be rejected at heavy load, for which penalty costs are incurred. We propose a framework to jointly optimize resource allocation for WS and MR applications hosted in a private cloud with the aim to increase cluster utilization and reduce its operational and penalty costs. The optimization problem is formulated as a non linear mathematical programming model. Applying the KKT conditions, we derive an equivalent problem that can be solved efficiently by a greedy procedure. The proposed framework increases cluster utilization by up to 18% while cost savings go up to 50% compared to a priori partitioning the cluster resources between the two workload types. © 2018 Elsevier Inc.","MapReduce; Nonlinear programming; Resource management; Shared clusters; Web service"
"Energy-efficient acceleration of MapReduce applications using FPGAs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045264459&doi=10.1016%2fj.jpdc.2018.02.004&partnerID=40&md5=dd94d7b13108dfa0c372fafb3b1006b6","In this paper, we present a full end-to-end implementation of big data analytics applications in a heterogeneous CPU+FPGA architecture. Selecting the optimal architecture that results in the highest acceleration for big data applications requires an in-depth of each application. Thus, we develop the MapReduce implementation of K-means, K nearest neighbor, support vector machine and naive Bayes in a Hadoop Streaming environment that allows developing mapper functions in a non-Java based language suited for interfacing with FPGA-based hardware accelerating environment. We further profile various components of Hadoop MapReduce to identify candidates for hardware acceleration. We accelerate the mapper functions through hardware+software (HW+SW) co-design. Moreover, we study how various parameters at the application (size of input data), system (number of mappers running simultaneously per node and data split size), and architecture (choice of CPU core such as big vs little, e.g., Xeon vs Atom) levels affect the performance and power-efficiency benefits of Hadoop streaming hardware acceleration and the overall performance and energy-efficiency of the system. A promising speedup as well as energy-efficiency gains of up to 8.3× and 15× is achieved, respectively, in an end-to-end Hadoop implementation. Our results show that HW+SW acceleration yields significantly higher speedup on Atom server, reducing the performance gap between little and big cores after the acceleration. On the other hand, HW+SW acceleration reduces the power consumption of Xeon server more significantly, reducing the power gap between little and big cores. Our cost Analysis shows that the FPGA-accelerated Atom server yields execution times that are close to or even lower than stand-alone Xeon server for the studied applications, while reducing the server cost by more than 3×. We confirm the scalability of FPGA acceleration of MapReduce by increasing the data size on 12-node Xeon cluster and show that FPGA acceleration maintains its benefit for larger data sizes on a cluster. © 2018 Elsevier Inc.","FPGA; Hadoop; Hardware+software co-design; Machine learning; MapReduce; Zynq boards"
"A course on big data analytics","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043980445&doi=10.1016%2fj.jpdc.2018.02.019&partnerID=40&md5=7146667045fe43e8a9dcd19b021cd936","This report details a course on big data analytics designed for undergraduate junior and senior computer science students. The course is heavily focused on projects and writing code for big data processing. It is designed to help students learn parallel and distributed computing frameworks and techniques commonly used in industry. The curriculum includes a progression of projects requiring increasingly sophisticated big data processing ranging from data preprocessing with Linux tools, distributed processing with Hadoop MapReduce and Spark, and database queries with Hive and Google's BigQuery. We discuss hardware infrastructure and experimentally evaluate the cost/benefit of an on-premise server versus Amazon's Elastic MapReduce. Finally, we showcase outcomes of our course in terms of student engagement and anonymous student feedback. © 2018 Elsevier Inc.","Big data; Cloud computing; Curriculum; Undergraduate education"
"A decentralized gossip based approach for data clustering in peer-to-peer networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046125738&doi=10.1016%2fj.jpdc.2018.03.009&partnerID=40&md5=ea0c503ca8b5cbca51a485149ba9c3e7","In this paper, a novel distributed approach, named GDSOM-P2P, for clustering distributed data resources is proposed by combining, an improved version of Silhouette algorithm, the dynamic Self-Organizing Map (SOM) neural network, and VICINITY protocol as a generic overlay management framework based on self-organization. The proposed GDSOM-P2P is adapted to the dynamic conditions of these networks. In the proposed GDSOM-P2P algorithm, at first, each node extracts a number of important data through the SOM and Silhouette algorithms. Then each of the nodes chooses one of its neighbors with the help of the VICINITY algorithm, and exchanges their important data with their neighbors. By doing this, over a period, the nodes’ data will be distributed over the entire network and the nodes in the network access the summary data model of the whole data. Finally, each node aggregates its internal data with a summary model and then performs the final clustering to cluster its internal data correctly. Evaluation results over a real P2P environment verify the efficiency of proposed GDSOM-P2P. Furthermore, the proposed GDSOM-P2P is also compared with the existing well-established distributed data clustering techniques. The results show a significant accuracy improvement of the proposed method. © 2018 Elsevier Inc.","Clustering algorithms; Distributed computing; Peer to peer computing"
"Accelerating subset sum and lattice based public-key cryptosystems with multi-core CPUs and GPUs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046860164&doi=10.1016%2fj.jpdc.2018.04.014&partnerID=40&md5=d492d1bba5bada64ed332b3660190d51","Post-quantum cryptosystems based on subset sum and lattice problems have gained much attention from researchers due to their simple construction, their resistance to quantum attacks, the new potential applications they provide, and above all, the mathematical security proofs that rigorously relate them to computational hard problems. However, the computational complexity of these cryptosystems is still high compared to classic number-theoretical ones, which may impede their adoption on a large scale. We studied the performance of three public-key cryptosystems based on subset sum, learning with errors and ring learning with errors problems. We provide a systematic study for choosing their parameters to guarantee sufficient security levels and detail an asymptotic comparison between them in terms of storage and running time complexities. We accelerate the running time of these cryptosystems by exploiting the inherent parallelism in computations through a GPGPU-based parallel implementation. The cryptosystems are implemented using C++ on Intel(R) Xeon(R) multi-core 64-bit processors machine with CUDA-enabled Tesla K80 GPUs. The parallel implementation is based on OpenCL framework and can run on arbitrary hardware platform accelerators with minor changes. Several optimizations and efficient algorithms were used to compute the core operations in each cryptosystem to achieve optimum performance. The ring learning with errors based cryptosystem showed the best performance while the Subset Sum cryptosystem showed the highest speedup gain for the encryption primitive. © 2018 Elsevier Inc.","GPGPU programming; Lattice cryptography; Learning with errors; Parallel polynomial multiplier; Post-quantum cryptography; Public-key cryptosystem; Subset sum"
"A visual programming environment for introducing distributed computing to secondary education","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044310220&doi=10.1016%2fj.jpdc.2018.02.021&partnerID=40&md5=bf6689346e1012c62fb36df0f6e8e1fa","The paper introduces a visual programming language and corresponding web and cloud-based development environment called NetsBlox. NetsBlox is an extension of Snap! and builds upon its visual formalism as well as its open source code base. NetsBlox adds distributed programming capabilities by introducing two well-known abstractions to block-based programming: message passing and Remote Procedure Calls (RPC). Messages containing data can be exchanged by two or more NetsBlox programs running on different computers connected to the Internet. RPCs are called on a client program and are executed on the NetsBlox server. These two abstractions make it possible to create distributed programs such as multi-player games or client–server applications. We believe that NetsBlox not only teaches basic distributed programming concepts but also provides increased motivation for high-school students to become creators and not just consumers of technology. © 2018 Elsevier Inc.","Computer science education; Distributed programming; Visual programming"
"LXCloudFT: Towards high availability, fault tolerant Cloud system based Linux Containers","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051396487&doi=10.1016%2fj.jpdc.2018.07.015&partnerID=40&md5=aa04a061381494898a2fb3f669c5f7ff","Infrastructure-as-a-Service container-based virtualization is gaining interest as a platform for running distributed applications. With increasing scale of Cloud architectures, faults are becoming a frequent occurrence, which makes availability a challenge. LXCloudFT is a fault tolerant Cloud system, which is composed of LXCloud-CR, a Checkpoint–Restart model and GC-CR, a garbage collector component that eliminates old snapshots of containers. LXCloudFT is designed, originally, for scientific applications and all its components are decentralized. We want to adapt it to serve stateless loosely coupled applications such as web applications. Replication is a method to survive failures for such applications. This paper addresses the issue of replication and contributes with a novel replication model, LXCloud-Rep, in LXCloudFT. LXCloud-Rep is a replication model with versioning and garbage collection, which is able to replicate Linux Container instances on several nodes in a decentralized manner. Following a node failure, LXCloud-Rep restarts failed containers on a new node from distributed images of containers not from snapshots. It optimizes the use of storage space. Large-scale experiments on Grid’5000 improve the performance of applications. © 2018 Elsevier Inc.","Cloud computing; Containers; Fault tolerance; Grid’5000; Replication; Versioning; Virtualization"
"Nontrivial and universal helping for wait-free queues and stacks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049877611&doi=10.1016%2fj.jpdc.2018.06.004&partnerID=40&md5=f3466690181b6eeba8301784a4b8be11","This paper studies two approaches to formalize helping in wait-free implementations of shared objects. The first approach is based on operation valency, and it allows us to make an important distinction between trivial and nontrivial helping. We show that any wait-free implementation of a queue from Test&Set requires nontrivial helping. We also define a weaker type of nontrivial helping and show that any wait-free queue implementation from a set of arbitrary base objects requires it. In contrast, there is a wait-free implementation of a stack from Test&Set with only trivial helping. These results shed light on the well-known open question of whether there exists a wait-free implementation of a queue in Common2, and indicate why it seems to be more difficult than implementing a stack. The other approach formalizes the helping mechanism employed by Herlihy's universal wait-free construction and is based on having an operation by one process restrict the possible linearizations of operations by other processes. We show that queue and stack implementations possessing such universal helping can be used to solve consensus. This result can be used to show that a strongly linearizable (Golab et al., 2011) implementation of a queue or a stack for n processes must use objects that allow to solve consensus among n or more processes. © 2018 Elsevier Inc.","Common2; Consensus number; Helping; Non-blocking; Queues; Shared objects; Stacks; Strong linearizability; Wait-freedom"
"Efficient selective multicore prefetching under limited memory bandwidth","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047754269&doi=10.1016%2fj.jpdc.2018.05.002&partnerID=40&md5=833254b77aced6e8b9ee95f6632373dd","Current multicore systems implement multiple hardware prefetchers to tolerate long main memory latencies. However, memory bandwidth is a scarce shared resource which becomes critical with the increasing core count. To deal with this fact, recent works have focused on adaptive prefetchers, which control the prefetcher aggressiveness to regulate the main memory bandwidth consumption. Nevertheless, in limited bandwidth machines or under memory-hungry workloads, keeping active the prefetcher can damage the system performance and increase energy consumption. This paper introduces selective prefetching, where individual prefetchers are activated or deactivated to improve both main memory energy and performance, and proposes ADP, a prefetcher that deactivates local prefetchers in some cores when they present low performance and co-runners need additional bandwidth. Based on heuristics, an individual prefetcher is reactivated when performance enhancements are foreseen. Compared to a state-of-the-art adaptive prefetcher, ADP provides both performance and energy enhancements in limited memory bandwidth. © 2018 Elsevier Inc.","Adaptive prefetching; Deactivation policies; Global feedback; Multicore prefetching"
"Detection of transmissible service failure in distributed service-based systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045700538&doi=10.1016%2fj.jpdc.2018.03.005&partnerID=40&md5=a8dcfa2e111e998cd7d04328b7b5514c","Detection of service failure, also known as service monitoring, is an important research problem in distributed service-based systems (SBSs). Failure of services is a transmissible threat in distributed SBSs, because services in distributed SBSs may have dependent relationships among them and thus the failure of one service may cause the failure of other services. Therefore, such transmissible service failure has to be detected in a timely manner whereas the corresponding resource consumption should be as little as possible. Most of the existing service monitoring approaches are centralised which suffer the potential of single point of failure and are not suitable in large scale distributed SBSs. Moreover, these centralised approaches are designed only in single-tenant SBSs. Nowadays, the scale of distributed SBSs is extremely large, i.e., including a large number of services and clients. Thus, it is essential for monitoring approaches to work well in large scale distributed SBSs and support multi-tenancy. Towards this end, in this paper, a novel agent-based decentralised service monitoring approach is developed in distributed SBSs. Compared to the centralised approaches, the proposed decentralised approach can avoid the single point of failure and can balance the computation over the monitoring agents. Also, unlike existing approaches which consider only single tenancy, the proposed approach takes multi-tenancy into account in distributed SBSs. Experimental results demonstrate that the proposed approach can respond as quickly as centralised approaches with much less computation overhead. © 2018 Elsevier Inc.","Distributed service-based systems; Multi-agent systems; Multi-tenancy; Service monitoring; Transmissible service failure"
"A fog-based privacy-preserving approach for distributed signature-based intrusion detection","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050884878&doi=10.1016%2fj.jpdc.2018.07.013&partnerID=40&md5=a4bc1dde4aae01d6f4da4db67fb0777b","Intrusion detection systems (IDSs) are the frontier of defense against transmissible cyber threats that spread across distributed systems. Modern IDSs overcome the limitation of hardware processing power by offloading computation extensive operations such as signature matching to the cloud. Moreover, in order to prevent the rapid spread of transmissible cyber threats, collaborative intrusion detection schemes are widely deployed to allow distributed IDS nodes to exchange information with each other. However, no party wants to disclose their own data during the detection process, especially sensitive user data to others, even the cloud providers for privacy concerns. In this background, privacy-preserving technology has been researched in the field of intrusion detection, whereas a collaborative intrusion detection network (CIDN) environment still lacks of appropriate solutions due to its geographical distribution. With the advent of fog computing, in this paper, we propose a privacy-preserving framework for signature-based intrusion detection in a distributed network based on fog devices. The results in both simulated and real environments demonstrate that our proposed framework can help reserve the privacy of shared data, reduce the workload on the cloud side, and offer less detection delay as compared to similar approaches. © 2018 Elsevier Inc.","Cloud environment; Collaborate network; Fog computing; Intrusion detection; Privacy preserving"
"SLA based healthcare big data analysis and computing in cloud network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046692676&doi=10.1016%2fj.jpdc.2018.04.006&partnerID=40&md5=9822b6088198824bee5e68c24c48255c","Large volume of multi-structured and low-latency patient data are generated in healthcare services, which is achallenging task to process and analyze within the Service Level Agreement (SLA). In this paper, a Parallel Semi-Naive Bayes (PSNB) based probabilistic method is used to process the healthcare big data in cloud for future health condition prediction. In order to improve the accuracy of PSNB method, a Modified Conjunctive Attribute (MCA) algorithm is proposed for reducing the dimension. Emergency condition of the patient is considered by setting a global priority among the patients and an Optimal Data Distribution (ODD) algorithm is proposed to position both batch and streaming patient data into the Spark nodes. Further, a Dynamic Job Scheduling (DJS) algorithm is designed to schedule the jobs efficiently to the most suitable nodes for processing the data taking SLA into account. Our proposed PSNB algorithm provides better accuracy of 87.8% for both batch and streaming data, which is 12.8% higher than the original Naive–Bayes (NB) algorithm and can conveniently be employed in various patient monitoring applications. © 2018 Elsevier Inc.","Big Data; Cloud computing; Healthcare; Spark"
"An experimental evaluation of a parallel simulated annealing approach for the 0–1 multidimensional knapsack problem","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044049509&doi=10.1016%2fj.jpdc.2018.02.031&partnerID=40&md5=21a420da3fce28b1d7ce77d0f5659f44","The recent progresses in the development and improvement of multicore/manycore architectures instigate the design of algorithms capable of exploring the functionalities provided by these architectures to solve more efficiently hard problems. The NP-hard class of problems contains several problems which demand for efficient alternative solutions, since there is a wide range or real-world problems which can be modeled as one of them and it is unknown if they can be exactly solved in feasible time. The 0–1 multidimensional knapsack problem (0–1 MKP) is one of the most emblematic NP-hard problems and this work focuses on the proposal of a parallel simulated annealing algorithm using GPGPU. The results achieved by the parallel SA were compared to other reference works and showed that GPGPU is effective on the task of obtaining better quality solutions in reduced execution time when compared to sequential programs. Our proposed approach can be adapted to other parallel platforms. © 2018 Elsevier Inc.","0–1 multidimensional knapsack problem; GPGPU; Simulated annealing"
"A malicious threat detection model for cloud assisted internet of things (CoT) based industrial control system (ICS) networks using deep belief network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047404138&doi=10.1016%2fj.jpdc.2018.04.005&partnerID=40&md5=4abafc5ea8a187649eb7da395f6ad422","Internet of Things (IoT) devices are extensively used in modern industries combined with the conventional industrial control system (ICS) network through the industrial cloud to make the production data easily available to the corporate business management and easier control for highly profitable production systems. The different devices within the conventional ICS network originally manufactured to run on an isolated network and was not considered for the privacy and security of the control and production/architecture data being trafficked over the manufacturing plant to the corporate. Due to their extensive integration with the industrial cloud network over the internet, these ICS networks are exposed to a significant threat of malicious activities created by malicious software. Protecting ICS from such attacks requires continuous update of their database of anti-malware tools which requires efforts from manual experts on a regular basis. This limits real time protection of ICS. Earlier work by Huda et al. (2017) based on a semi-supervised approach performed well. However training process of the semi-supervised-approach (Huda et al., 2017) is complex procedure which requires a hybridization of feature selection, unsupervised clustering and supervised training techniques. Therefore, it could be time consuming for ICS network for real time protection. In this paper, we propose an adaptive threat detection model for industrial cloud of things (CoT) based on deep learning. Deep learning has been used in many domain of pattern recognition and a popular approach for its simple training procedure. Most importantly, deep learning can learn the hidden patterns of the domain in an unsupervised manner which can avoid the requirements of huge expensive labeled data. We used this particular characteristic of deep learning to design our detection model. Two different types of deep learning based detection models are proposed in this work. The first model uses a disjoint training and testing data for a deep belief network (DBN) and corresponding artificial neural network (ANN). In the second proposed detection model, DBN is trained using new unlabeled data to provide DBN with additional knowledge about the changes in the malicious attack patterns. Novelty of the proposed detection models is that the models are adaptive where training procedures is simpler than earlier work (Huda et al, 2017) and can adapt new malware behaviors from already available and cheap unlabeled data at the same time. This will avoid expensive manual labeling of new attacks and corresponding time complexity making it feasible for ICS networks. Performances of standard DBNs are sensitive to its configurations and values for the hyper-parameters including number of hidden nodes, learning rate and number epochs. Therefore proposed detection models find an optimal configuration by varying the structure of DBNs and other parameters. The proposed detection models are extensively tested on a real malware test bed. Experimental results show that the proposed approaches achieve higher accuracies than standard detection algorithms and obtain similar performances with earlier semi-supervised work (Huda et al., 2017) but provide a comparatively simplified training model. © 2018 Elsevier Inc.","Deep belief network; Dynamic analyses; Industrial control system; Malware behavior selection; Semi-supervised model"
"An efficient theta-join query processing in distributed environment","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050114377&doi=10.1016%2fj.jpdc.2018.07.007&partnerID=40&md5=616e64c53963b57fc3ecbf807fb0a7b2","Theta-join query is very useful in many data analysis tasks, but it is not efficiently processed in distributed environment, especially in large scale data. Although there is much progress in dealing theta-join with MapReduce paradigm, the methods are either complex which require fundamental changes to MapReduce framework or only consider the overheads of load balance in the network, when data scale is large, they will make much computation cost and induce OOM (Out of Memory) errors. In this work, we propose a filter method for theta-join on the purpose of reducing the computation cost and achieving the minimum execution time in distributed environment. We consider not only the load balance in the cluster, but also the memory cost in parallel framework. We also propose a keys-based join solution for multi-way theta-join to reduce the data amount for cross product, then improve the performance of join efficiency. We implement our methods in a popular general-purpose data processing framework, Spark. The experimental results demonstrate that our methods can significantly improve the performance of theta-joins comparing with the state-of-art solutions. © 2018 Elsevier Inc.","Large scale data processing; Parallel distributed framework; Query optimization; Theta-join algorithm"
"Edge-of-things computing framework for cost-effective provisioning of healthcare data","2019","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054163117&doi=10.1016%2fj.jpdc.2018.08.011&partnerID=40&md5=3ab2cde6dd3e0ca11fe719a08b448c84","Edge-of-Things (EoT)-based healthcare services are forthcoming patient-care amenities related to autonomic and persuasive healthcare, where an EoT broker usually works as a middleman between the Healthcare Service Consumers (HSC) and Computing Service Providers (CSP). The computing service providers are the edge computing service providers (ECSP) and cloud computing service provider (CCSP). Sensor observations from a patient's body area networks (BAN) and patients’ medical and genetic historical data are very sensitive and have a high degree of interdependency. It follows that EoT based patient monitoring systems or applications are tightly coupled and require obstinate synchronization. Therefore, this paper proposes a portfolio optimization solution for the selection of virtual machines (VMs) of edge and/or cloud computing service providers. The dynamic pricing for an EoT computation service is considered by the EoT broker for optimal VM provisioning in an EoT environment. The proposed portfolio optimization solution is compared with the traditional certainty equivalent approach. As the portfolio optimization is a centralized solution approach, this paper also proposes an alternating direction method of multipliers (ADMM) based distributed provisioning method for the healthcare data in the EoT computing environment. A comparative study shows the cost-effective provisioning for the healthcare data through portfolio optimization and ADMM methods over the traditional certainty equivalent and greedy approach, respectively. © 2018 The Author(s)","ADMM; Body area networks; Dynamic pricing; Edge-of-things; Mobile edge computing; Portfolio optimization; Virtual machine"
"Parallel time–space reduction by unbiased filtering for solving the 0/1-Knapsack problem","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052865813&doi=10.1016%2fj.jpdc.2018.08.003&partnerID=40&md5=9e751d5cd518b6bfcb1b2bb3e4de0ab8","The 0/1-Knapsack problem (0/1-KP) is one of popular NP-hard problems since its optimal solutions are meaningful to data-science computing in real-world applications (i.e., maximum profit of product planning-and-loading, minimum encryption of reliable information-transmission, etc.). Theoretically, the optimal solutions of the 0/1-KP can be solved by the dynamic programming (DP) but in non-polynomial time, which is not reasonable for big data n and massive limited-capacity C. Practically, the hybrid swarm-optimization could improve the 0/1-KP solutions in polynomial time but best fitness-solutions may not be optimal. Therefore, this paper proposes the parallel time–space reduction for solving the 0/1-KP in polynomial time. Our innovation and contribution focus on achieving 1. the fast processing as the FS (feature sorting) and the optimal solutions close to the DP by our unbiased-filtering and 2. the optimized time-complexity O(n/p log2p + C’/p) by our parallel time–space reduction. In this new approach, we integrate the FS method (for the efficient initial solutions) and the optimal DP (on the remaining critical-objects n’ and knapsack-capacity C’), incorporated with the proposed unbiased-filtering. Finally, performance is evaluated by simulation study on a variety of problem-sizes. In experiment, most of our solutions are optimal as comparable to the optimal solutions of the DP. © 2018 Elsevier Inc.","0/1-KP feature sorting; Dynamic programming; NP-hard problems and optimization; Parallel time–space reduction for solving 0/1-KP; Unbiased filtering for polynomial-time reduction"
"A PUF-based hardware mutual authentication protocol","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046687141&doi=10.1016%2fj.jpdc.2018.04.007&partnerID=40&md5=c7d84cae77c6be3ca6833b877a9a1819","Physically Unclonable Functions (PUFs) represent a promising security primitive due to their unclonability, uniqueness and tamper-evident properties, and have been recently exploited for device identification and authentication, and for secret key generation and storage purposes. In this paper, we present PHEMAP (Physical Hardware-Enabled Mutual Authentication Protocol), that allows to achieve mutual authentication in a one-to-many communication scenario, where multiple devices are connected to a sink node. The protocol exploits the recursive invocation of the PUF embedded on the devices to generate sequences (chains) of values that are used to achieve synchronization among communicating parties. We demonstrate that, under reasonable assumptions, PHEMAP is secure and robust against man-in-the-middle attacks and other common physical attacks. We discuss PHEMAP performance in several operation conditions, by measuring the efficiency of the protocol when varying some of the underlying parameters. Finally, we present an implementation of PHEMAP on devices hosting an FPGA belonging to the Xilinx Zynq-7000 family and embedding an Anderson PUF architecture, and show that the computation and hardware overhead introduced by the protocol makes it feasible for commercial mid-range devices. © 2018 Elsevier Inc.","FPGA; Hardware security; Mutual authentication; PUF"
"Big data transfer optimization through adaptive parameter tuning","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048029041&doi=10.1016%2fj.jpdc.2018.05.003&partnerID=40&md5=2a86009a13e120350efed11a49cc00c0","Obtaining optimal data transfer performance is of utmost importance to today's data-intensive distributed applications and wide-area data replication services. Tuning application-layer protocol parameters such as pipelining, parallelism, and concurrency can significantly increase efficient utilization of the available network bandwidth as well as the end-to-end data transfer performance. However, determining the best settings for these parameters is a challenging problem, as network conditions can vary greatly between sites and over time. Poor protocol tuning can cause either under- or over-utilization of network resources and thus degrade transfer performance. In this paper, we present three novel algorithms for application-layer parameter tuning and transfer scheduling to maximize transfer throughput in wide-area networks. Our algorithms use heuristics to tune the level of control channel pipelining (for small file optimization), the number of parallel data streams per file (for large file optimization), and the number of concurrent file transfers to increase I/O throughput (for all types of files). The proposed algorithms improve the transfer throughput up to 10x compared to the baseline and 7x compared to the state-of-the-art solutions. We also propose adaptive tuning to adjust the values of parameters based on real-time observations. The results show that adaptive tuning can further improve transfer throughput by up to 24% compared to the heuristic approach. © 2018 Elsevier Inc.","Application-level protocol tuning; Throughput optimization; Wide-area data transfers"
"Transferring activity recognition models in FOG computing architecture","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052469008&doi=10.1016%2fj.jpdc.2018.07.020&partnerID=40&md5=1bd77d0ac888f27dd6464a41c40a486b","A major focus of research in the field of in-home activity recognition (AR) and home automation (HA) is the ability to transfer data models to other homes for the purpose of applying new services, annotating classified data, and generating datasets due to lack of training ones. The wide spread of fog computing as an architecture for organizing edge devices in Internet-of-Things (IoT) systems lends support to the sharing of different environmental characteristics between different fogs (smart homes). In this paper, we propose a framework that serves the transfer of data models between different smart homes in a bid to overcome the lack of training data, which prevents the development of high-performance models that utilize fog computing characteristics. Our technique incorporates the sharing of environmental characteristics (by Fogs) in order to analyze the data features at the source and target smart homes. The features, then, are mapped onto each other using a fusion method that guarantees to keep the variations between different homes by reducing the divergence between them. The hidden Markov model has also been applied in order to model activities at target homes. Three experiments have been conducted to measure the performance of the proposed framework: first, against the accuracy of feature-mapping techniques; second, measuring the performance of classifying data at target homes; and, third, the ability of the proposed framework to function well due to noise data. The results show promising indicators and highlight the limitations of the proposed methodology. © 2018 Elsevier Inc.","Activity recognition; Cloud architecture; Data mining; Edge computing; Fog computing; Internet of things; Transfer learning; Wireless networks"
"Event-based sensor data exchange and fusion in the Internet of Things environments","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046662495&doi=10.1016%2fj.jpdc.2017.12.010&partnerID=40&md5=ec98caf33f1bd002eb93ce98ce96324b","Internet of Things (IoT) is a promising technology for improving our lives and society by integrating smart devices in our environment and paving the way for novel ICT application, spanning from smart cities to energy efficiency and home automation. However, such a vision encompasses the availability of thousands of smart devices, or even more, that continuously exchange a huge volume of data among each other and with cloud-based services, raising a big data problem. Such a problem can be approached by properly applying data fusion practices within an IoT infrastructure. Due to the characteristics and peculiarities of the communications among smart devices within the IoT, an event-based data fusion is needed, where devices exchange notifications of events among each others. Such data fusion should be focused on special devices where notification heterogeneity, and data source trust issues have to be faced with. Accordingly, the contribution of this work is proposing (i) a novel broker-less event-based communication protocol specifically tailored to sensors with constrained resources, (ii) a solution for flexible event-based communications among heterogeneous data sources, and (iii) an approach based on the theory of evidence for data fusion processes that depend on the matching and trust degree of the data to be fused. © 2017 Elsevier Inc.","Automatic schema matching; Beaconing; Publish/subscribe service; Sensor data fusion; Theory of evidence; Trust management"
"An agent-based service adaptation approach in distributed multi-tenant service-based systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050874272&doi=10.1016%2fj.jpdc.2018.07.006&partnerID=40&md5=0bc861e9ec7b5b21f0c912df0bec916f","Service adaptation aims to alleviate the runtime quality management problem of distributed service-based systems (SBSs). Most of the existing service adaptation approaches are designed for single-tenant SBSs. However, modern distributed SBSs must achieve multi-tenancy due to simultaneous existence of multiple tenants. Thus, it is vital to study service adaptation in distributed multi-tenant SBSs. Currently, service adaptation has not been properly addressed in distributed multi-tenant SBSs. Existing approaches for service adaptation in multi-tenant SBSs are centralised which are not very efficient if the SBSs are large and distributed. Some decentralised service adaptation approaches, which are developed in single-tenant SBSs, may be extended to accommodate multi-tenant SBSs. These approaches, however, either incur significant communication overhead to obtain required information, or simply assume that some specific global information is already known, which is not realistic in large and distributed SBSs. To overcome the limitations of existing approaches, in this paper, a novel agent-based hybrid service adaptation approach for distributed multi-tenant SBSs is proposed, which is based on the multi-agent coalition formation technique. Our hybrid approach combines the advantages of both centralised and decentralised approaches while avoiding their disadvantages. The experimental results demonstrate the effectiveness of the proposed approach. © 2018","Multi-agent coalition formation; Multi-tenant service-based systems; Service adaptation"
"Computational science and HPC education for graduate students: Paving the way to exascale","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044002869&doi=10.1016%2fj.jpdc.2018.02.023&partnerID=40&md5=c6c002ebc28199ca316627dea346f6d5","The article discusses the experience of teaching supercomputer disciplines to students specializing in Computational Mathematics. Graduates specializing in this field become future developers and users of complex supercomputing applications and systems. The article presents the structure of a training program that has been implemented at the Faculty of Computational Mathematics and Cybernetics of Lomonosov Moscow State University. It focuses on the content of disciplines related to parallel computing with a detailed description of the structure and content of the course “Supercomputing Simulation and Technologies” which is offered as part of the Master's degree training program at the Faculty. The content of practical assignments supporting this discipline is discussed in detail, along with the results produced by the students who performed these practical assignments on Lomonosov and IBM Blue Gene/P supercomputers. The main contribution of the paper is twofold: we draw attention to the importance of study of a wide set of parallel algorithms properties and provide a practical methodology to reach this goal. © 2018 Elsevier Inc.","Computational mathematics; HPC education; Parallel computing; Supercomputer; Supercomputing simulation"
"Constructing spanning trees in augmented cubes","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052904850&doi=10.1016%2fj.jpdc.2018.08.006&partnerID=40&md5=323fc25954524740652e01348fbf9000","The spanning trees T1,T2,…,Tk of G are edge-disjoint spanning trees (EDSTs) if they are pairwise edge-disjoint. In addition to it if they are pairwise internally vertex disjoint then they are called completely independent spanning trees (CISTs) in G. In networks, EDSTs and CISTs are useful to increase fault-tolerance, bandwidth, and security. The possible geometric configurations in which hundreds or even thousands of processors may be linked together are examined to find the geometry that best supports computations. A much-studied topology is the hypercube and its variants. The n-dimensional augmented cube, denoted as AQn, a variation of the hypercube possesses several embeddable properties that the hypercube and its other variations do not possess. Wang et al. (2017) asked to derive an algorithm that constructs edge-disjoint spanning trees in an augmented cube. In this paper, construction of n−1 edge-disjoint spanning trees of the augmented cube AQn (n≥3) is given. The result is optimal with respect to the number of edge-disjoint spanning trees. Pai and Chang (2016) provided an approach for constructing two CISTs in several hypercube-variant networks with diameter 2n−1. They asked to design algorithms to construct more than two CISTs in high dimensional hypercube-variant networks with a smaller diameter. For AQn (n≥6), we construct four completely independent spanning trees of which two trees are with diameters 2n−5 and two trees are with diameters 2n−3. © 2018 Elsevier Inc.","Augmented cubes; Completely independent spanning trees; Edge-disjoint spanning trees"
"A scalable algorithm for simulating the structural plasticity of the brain","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039933863&doi=10.1016%2fj.jpdc.2017.11.019&partnerID=40&md5=15c661b0d2a2b2585f30d8758f12f461","The neural network in the brain is not hard-wired. Even in the mature brain, new connections between neurons are formed and existing ones are deleted, which is called structural plasticity. The dynamics of the connectome is key to understanding how learning, memory, and healing after lesions such as stroke work. However, with current experimental techniques even the creation of an exact static connectivity map, which is required for various brain simulations, is very difficult. One alternative is to use network models to simulate the evolution of synapses between neurons based on their specified activity targets. This is particularly useful as experimental measurements of the spiking frequency of neurons are more easily accessible and reliable than biological connectivity data. The Model of Structural Plasticity (MSP) by Butz and van Ooyen is an example of this approach. However, to predict which neurons connect to each other, the current MSP model computes probabilities for all pairs of neurons, resulting in a complexity O(n2). To enable large-scale simulations with millions of neurons and beyond, this quadratic term is prohibitive. Inspired by hierarchical methods for solving n-body problems in particle physics, we propose a scalable approximation algorithm for MSP that reduces the complexity to O(nlog2n) without any notable impact on the quality of the results. We show that an MPI-based parallel implementation of our scalable algorithm can simulate the structural plasticity of up to 109 neurons—four orders of magnitude more than the naïve O(n2) version. © 2017 Elsevier Inc.","Barnes–Hut; Brain; Connectome; Dynamics; Large-scale; Simulation"
"EvoDeep: A new evolutionary approach for automatic Deep Neural Networks parametrisation","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046683629&doi=10.1016%2fj.jpdc.2017.09.006&partnerID=40&md5=f1547d79bd36238d7780efb621c61765","Deep Neural Networks (DNN) have become a powerful, and extremely popular mechanism, which has been widely used to solve problems of varied complexity, due to their ability to make models fitted to non-linear complex problems. Despite its well-known benefits, DNNs are complex learning models whose parametrisationand architecture are made usually by hand. This paper proposes a new Evolutionary Algorithm, named EvoDeep, devoted to evolve the parameters and the architecture of a DNN in order to maximise its classification accuracy, as well as maintaining a valid sequence of layers. This model is tested against a widely used dataset of handwritten digits images. The experiments performed using this dataset show that the Evolutionary Algorithm is able to select the parameters and the DNN architecture appropriately, achieving a 98.93% accuracy in the best run. © 2017 Elsevier Inc.","Automated parametrisation; Deep Learning; Evolutionary Algorithms; Finite-State Machines"
"Confident information coverage hole detection in sensor networks for uranium tailing monitoring","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017559042&doi=10.1016%2fj.jpdc.2017.03.005&partnerID=40&md5=6d189d299aa47ccb650d4881b98dc4a2","The wireless sensor networks recently abstract great attention and are used for a wide range of cyber-enabled applications each of which with rigid accuracy requirements. The emergence and existence of coverage holes in WSNs will dramatically degrade the network coverage performance and quality of service. To diminish the negative effects of coverage holes, this paper addresses and studies the confident information coverage hole detection problem (CICHD) based on the proposed novel confident information coverage model (CIC). For solving the CICHD problem, we design two effective heuristic CIC holes detection algorithms including the CHD without considering the nodes residual energy and the other CHDRE taking the nodes’ residual energy into account. In the both proposed algorithms, the sensing field is firstly partitioned into a series of reconstruction grids based on the spatial correlation and correlation range. Then each reconstruction grid will be scanned and detected based on the CIC model to be judged whether it is a hole. Once obtaining the coverage status of every reconstruction grid, the boundary of the coverage hole will be exacted by image processing method. The results of the simulations show that both the proposed schemes can efficiently detect the emerged coverage holes including the locations and the number, and the CHDRE algorithm is more practical and efficient compared to the CHD without considering the energy problem. © 2017 Elsevier Inc.","Confident information coverage; Coverage hole detection; Cyber-enabled applications; Sensor networks"
"Implications of deep learning for the automation of design patterns organization","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026774084&doi=10.1016%2fj.jpdc.2017.06.022&partnerID=40&md5=c924598dfce3f8b647da8b1b9b481e5d","Though like other domains such as email filtering, web page classification, sentiment analysis, and author identification, the researchers have employed the text categorization approach to automate organization and selection of design patterns. However, there is a need to bridge the gap between the semantic relationship between design patterns (i.e. Documents) and the features which are used for the organization of design patterns. In this study, we propose an approach by leveraging a powerful deep learning algorithm named Deep Belief Network (DBN) which learns on the semantic representation of documents formulated in the form of feature vectors. We performed a case study in the context of a text categorization based automated system used for the classification and selection of software design patterns. In the case study, we focused on two main research objectives: 1) to empirically investigate the effect of feature sets constructed through the global filter-based feature selection methods besides the proposed approach, and 2) to evaluate the significant improvement in the classification decision (i.e. Pattern organization) of classifiers using the proposed approach. The adjustment of DBN parameters such as a number of hidden layers, nodes and iteration can aid a developer to construct a more illustrative feature set. The experimental promising results suggest the significance of the proposed approach to construct a more representative feature set and improve the classifier's performance in terms of organization of design patterns. © 2017 Elsevier Inc.","Classifiers; Deep learning; Design patterns; Feature set; Performance"
"Model-driven scheduling for distributed stream processing systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044845984&doi=10.1016%2fj.jpdc.2018.02.003&partnerID=40&md5=d27f11d7522df19d658c50d36a7929e8","Distributed Stream Processing Systems (DSPS) are “Fast Data” platforms that allow streaming applications to be composed and executed with low latency on commodity clusters and Clouds. Such applications are composed as a Directed Acyclic Graph (DAG) of tasks, with data parallel execution using concurrent task threads on distributed resource slots. Scheduling such DAGs for DSPS has two parts—allocation of threads and resources for a DAG, and mapping threads to resources. Existing schedulers often address just one of these, make the assumption that performance linearly scales, or use ad hoc empirical tuning at runtime. Instead, we propose model-driven techniques for both mapping and allocation that rely on low-overhead a priori performance modeling of tasks. Our scheduling algorithms are able to offer predictable and low resource needs that is suitable for elastic pay-as-you-go Cloud resources, support a high input rate through high VM utilization, and can be combined with other mapping approaches as well. These are validated for micro and application benchmarks, and compared with contemporary schedulers, for the Apache Storm DSPS. © 2018 Elsevier Inc.","Big data; Cloud computing; Distributed systems; Performance models; Scheduling algorithms; Stream processing"
"Photonic-based express coherence notifications for many-core CMPs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038827163&doi=10.1016%2fj.jpdc.2017.11.015&partnerID=40&md5=7db70fb9cdc43434f2fbaf0fabe21e30","Directory-based coherence protocols (Directory) are considered the design of choice to provide maximum performance in coherence maintenance for shared-memory many-core CMPs, despite their large memory overhead. New solutions are emerging to achieve acceptable levels of on-chip area overhead and energy consumption such as optimized encoding of block sharers in Directory (e.g., SCD) or broadcast-based coherence (e.g., Hammer). In this work, we propose a novel and efficient solution for the cache coherence problem in many-core systems based on the co-design of the coherence protocol and the interconnection network. Particularly, we propose ECONO, a cache coherence protocol tailored to future many-core systems that resorts on PhotoBNoC, a special lightweight dedicated silicon-photonic subnetwork for efficient delivery of the atomic broadcast coherence messages used by the protocol. Considering a simulated 256-core system, as compared with Hammer, we demonstrate that ECONO+PhotoBNoC reduces performance and energy consumption by an average of 34% and 32%, respectively. Additionally, our proposal lowers the area overhead entailed by SCD by 2×. © 2017 Elsevier Inc.","Cache coherency; Many-core CMP; Silicon-photonic technology"
"Co-processing heterogeneous parallel index for multi-dimensional datasets","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038024474&doi=10.1016%2fj.jpdc.2017.10.015&partnerID=40&md5=4102e36c0b1e0b23a5d7a6d71e225f77","We present a novel multi-dimensional range query co-processing scheme for the CPU and GPU. It has been reported that traversing hierarchical tree structures in parallel is inherently not efficient because of large branching factors. Besides, it is known that the recursive tree traversal algorithm required for multi-dimensional range queries is not well suited for the GPU architecture owing to its small shared memory. In this paper, we propose co-processing range queries using both the CPU and GPU to make the most use of each architecture. In Hybrid tree that we present in this paper, we let CPU navigate the internal nodes of hierarchical tree structures and make GPU scan leaf nodes in a linear fashion using a massively large number of processing units. With the co-processing scheme, we can asynchronously leverage the strengths of each architecture. We also propose a novel dynamic GPU block scheduling algorithm for multiple range queries. In our scheduling algorithm, we consider the selection ratio of each query to determine the number of GPU blocks to launch. By assigning the right number of GPU blocks, we can significantly improve the query processing throughput for multiple concurrent queries. Our extensive experimental study shows that the proposed co-processing scheme shows up to 12× faster query response time than the state-of-the-art GPU tree traversal algorithm. We also show that our dynamic GPU block assignment algorithm improves the query processing throughput by up to 4×. © 2017 Elsevier Inc.","GPU; Multi-dimensional index; Query co-processing"
"Energy optimization of security-sensitive mixed-criticality applications for distributed real-time systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043979952&doi=10.1016%2fj.jpdc.2018.02.014&partnerID=40&md5=b281257e61dd205b9b219e52fc87ebdf","Existing studies on mixed-criticality systems are usually safety-oriented, which seriously ignore the security and energy related requirements. In this paper we are interested in the design of security-sensitive mixed-criticality real-time systems. We first establish the system model to capture security-critical applications in mixed-criticality systems. Higher security-criticality protection always results in significant time and energy overhead in mixed-criticality systems. Thus, this paper proposes a system-level design framework for energy optimization of security-sensitive mixed-criticality system with hard real-time constraints. Since the time complexity of finding optimal solutions grows exponentially as problem size grows, a GA (Genetic Algorithm) based on efficient heuristic algorithm is devised to address the system-level optimization problem. Extensive experiments and a real-life case study have been conducted to show the efficiency of the proposed technique, which can obtain balanced minimal energy consumption while satisfying strict security and timing constraints. The proposed approach can save up to 28.9% energy consumption compared with other three candidates. © 2018 Elsevier Inc.","Distributed real-time system; Energy; Mixed-criticality; Scheduling; Security; System design"
"New scheduling approach using reinforcement learning for heterogeneous distributed systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021778383&doi=10.1016%2fj.jpdc.2017.05.001&partnerID=40&md5=ff7f37b422fa3a42431bd8051ec4ebaf","Computer clusters, cloud computing and the exploitation of parallel architectures and algorithms have become the norm when dealing with scientific applications that work with large quantities of data and perform complex and time-consuming calculations. With the rise of social media applications and smart devices, the amount of digital data and the velocity at which it is produced have increased exponentially, determining the development of distributed system frameworks and platforms that increase productivity, consistency, fault-tolerance and security of parallel applications. The performance of such systems is mainly influenced by the architectural disposition and composition of the physical machines, the resource allocation and the scheduling of jobs and tasks. This paper proposes a reinforcement learning algorithm to solve the scheduling problem in distributed systems. The machine learning technique takes into consideration the heterogeneity of the nodes and their disposition within the grid, and the arrangement of tasks in a directed acyclic graph of dependencies, ultimately determining a scheduling policy for a better execution time. This paper also proposes a platform, in which the algorithm is implemented, that offers scheduling as a service to distributed systems. © 2017 Elsevier Inc.","Distributed systems; Machine learning; SARSA; Scheduling"
"High-level synthesis of on-chip multiprocessor architectures based on answer set programming","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044475089&doi=10.1016%2fj.jpdc.2018.02.010&partnerID=40&md5=303d42458635b3cb92d9b1858efaabf5","We present a system-level synthesis approach for heterogeneous multi-processor on chip, based on Answer Set Programming(ASP). Starting with a high-level description of an application, its timing constraints and the physical constraints of the target device, our goal is to produce the optimal computing infrastructure made of heterogeneous processors, peripherals, memories and communication components. Optimization aims at maximizing speed, while minimizing chip area. Also, a scheduler must be produced that fulfills the real-time requirements of the application. Even though our approach will work for application specific integrated circuits, we have chosen FPGA as target device in this work because of their reconfiguration capabilities which makes it possible to explore several design alternatives. This paper addresses the bottleneck of problem representation size by providing a direct and compact ASP encoding for automatic synthesis that is semantically equivalent to previously established ILP and ASP models. We describe a use-case in which designers specify their applications in C/C++ from which optimum systems can be derived. We demonstrate the superiority of our approach toward existing heuristics and exact methods with synthesis results on a set of realistic case studies. © 2018 Elsevier Inc.","Answer set programming; Architecture synthesis; Multi-objective optimization; Reconfigurable architecture; System design; Technology mapping"
"Teaching high-performance service in a cluster computing course","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044123106&doi=10.1016%2fj.jpdc.2018.02.027&partnerID=40&md5=c740a710236f34d4803b46468760b60a","Most courses on cluster computing in graduate and postgraduate studies are focused on parallel programming and high-performance/high-throughput computing. This is the typical usage of clusters in academia and research centres. However, nowadays, many companies are providing web, mail and, in general, Internet services using computer clusters. These services require a different “cluster flavour”: high-performance service and high availability. Despite the fact that computer clusters for each environment demand a different configuration, most university cluster computing courses keep focusing only on high-performance computing, ignoring other possibilities. In this paper, we propose several teaching strategies for a course on cluster computing that could fill this gap. The content developed here would be taught as a part of the course. The subject shows several strategies about how to configure, test and evaluate a high-availability/load-balanced Internet server. A virtualization-based platform is used to build a cluster prototype, using Linux as its operating system. Evaluation of the course shows that students knowledge and skills on the subject are improved at the end of the course. On the other hand, regarding the teaching methodology, the results obtained in the yearly survey of the University confirm student satisfaction. © 2018 Elsevier Inc.","Computer engineering education; High-availability; High-performance service; Linux clusters; Load balancing"
"A deep stochastic weight assignment network and its application to chess playing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029769676&doi=10.1016%2fj.jpdc.2017.08.013&partnerID=40&md5=d88400b99efb225d0c8cad0e2e067bb5","Chinese chess is an ancient game in which the chess situation is the information ensemble of chess pieces’ spatial locations and interrelations. The situation evaluation plays an extremely important role in the policy decisions of Chinese chess game. However, the situation evaluation is too complex for human to cover every detail with naked eyes. The deep stochastic weight assignment network (DSWAN) proposed in this paper to classify the situations in advantages and disadvantages can solve the above problem. DSWAN is a type of multi-layer perception (MLP) which is divided into two main components: unsupervised feature extractor formed by multi-layer auto encoder and supervised classifier trained by stochastic weight assignment network (SWAN). We summarize a series of chess situation features by gathering specialized knowledges of Chinese chess, and these features are proved valid to estimate the situation. Another highlight of this paper is that the auto encoder is constrained by L1∕2 regularization. By doing so, it can bring more sparsity to data set, make situation features more representative and ease the overfitting trouble of SWAN. © 2017 Elsevier Inc.","Auto encoder; Chess situation analysis; Chinese chess game; Deep stochastic weight assignment network (DSWAN); L<sub>1∕2</sub> regularization; Stochastic weight assignment network (SWAN)"
"Interplay between SIR-based disease spreading and awareness diffusion on multiplex networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041471445&doi=10.1016%2fj.jpdc.2018.01.001&partnerID=40&md5=576673abbc8895a6baceff271cd94e01","In this paper, we propose a coupled multiplex network framework to model the epidemic spreading and its corresponding information diffusion among a population. In the model, as far as the information perception on the epidemics is concerned, the individuals can be divided into two classes, namely aware or unaware ones; Meanwhile, the awareness diffusion is depicted by utilizing the traditional contact process. From the perspective of infectious disease spreading, the contagion dynamics among nodes can be characterized with the classic SIR (susceptible–infective–recovered) model. Based on the microscopic Markov chain approach, we build the probability tree to describe the switching process between different states, and then intensively perform the theoretical analysis for the state transition. In particular, we analytically derive the epidemic threshold regarding the disease propagation, which is correlated with the multiplex network topology and the coupling relationship between two transmission dynamics. After being compared with extensive numerical Monte Carlo (MC) simulations, it is clearly found that the achieved analytical results concur with the MC simulations. Current results will be beneficial to substantially enhance the predictability of the epidemic outbreaks within many realistic dissemination cases. © 2018 Elsevier Inc.","Epidemic spreading; Information diffusion; Multiplex network; Transmission dynamics"
"A hardware accelerated system for high throughput cellular image analysis","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037525833&doi=10.1016%2fj.jpdc.2017.11.013&partnerID=40&md5=b4facb57ffcf26e741c84f9eec5f9a5c","Imaging flow cytometry and high speed microscopy have shown immense promise for clinical diagnostics, biological research, and drug discovery. They enable high throughput screening and sorting using biological, chemical, or mechanical properties of cells. These techniques can separate mature cells from immature ones, determine the presence of cancerous cells, classify stem cells during differentiation, and screen drugs based upon how they affect cellular architecture. The process works by imaging cells at a high rate, extracting features of the cell (e.g., size, location, circularity, deformation), and using those features to classify the cell. Modern systems have a target throughput of thousands of cells per second, which requires imaging at rates of more than 60,000 frames per second. The cellular features must be calculated in less than a millisecond to enable real-time sorting. This creates challenging computing performance constraints in terms of both throughput and latency. In this paper, we present a hardware accelerated system for high throughput cellular image analysis. We carefully developed algorithms and their corresponding hardware implementations to meet the strict computational demands. Our algorithm analyzes and extracts cellular morphological features from low resolution microscopic images. Our hardware accelerated system operates at over 60,000 frames per second with 0.068 ms latency. This is almost 1400× faster in throughput than similar software based analysis and 335× better in terms of latency. © 2017 Elsevier Inc.","Biomedical image analysis; Cytometry; FPGA; Hardware acceleration; High throughput; Reconfigurable hardware"
"HMFS: A hybrid in-memory file system with version consistency","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043354187&doi=10.1016%2fj.jpdc.2018.02.002&partnerID=40&md5=1f6748f6531b387cb44af69dea67c9c9","Emerging non-volatile memory (NVM) such as PCM and STT-RAM has memory-like byte-addressability as well as disk-like persistent storage capability. It offers an opportunity to bring NVM into the existing computer architecture for constructing an efficient and high-performance in-memory file system. Several NVM-optimized file systems have been designed. However, most of them fail to exploit all important features of NVM, and can only guarantee the file system consistency to the data consistency level. In this paper, we present HMFS, a hybrid in-memory full-versioning file system. HMFS manages DRAM and NVM in a unified address space and adopts different updating mechanisms to them. Besides, HMFS achieves version consistency with a simple and efficient multi-version approach. Experimental results show that HMFS achieves significant throughput improvement comparing with the state-of-the-art NVM-optimized file systems, such as PMFS and NOVA, and 3.1× to 13.5× higher versioning efficiency compared to some other multi-versioned file system such as BTRFS and NILFS2. © 2018","Hybrid memory architecture; In-memory file system; Multi-versioning; Non-volatile memory"
"From P2P to NoSQL: A continuous metric for classifying large-scale storage systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039758134&doi=10.1016%2fj.jpdc.2017.11.017&partnerID=40&md5=ae69aec4d5f46d056f66406bb02c58d3","Big Science and Big Data are concepts emerged from the rather recent trend comprising the use of extensive computational resources in the industry and academy, respectively. In this context, the storage and processing of ever increasing data volumes have catalyzed the popularization of innovative platforms that, differently from traditional DBMSs, present distinct trade-offs between throughput, latency, capacity and consistency to reach higher orders of scalability. However, in this myriad of platforms, how can one judiciously choose the most appropriate for a given application, or choose to endeavor in a new system development? In this paper, we present a continuous metric based on the concept of normalized entropy to classify large-scale storage systems according to their architectural and data-distribution characteristics, as opposed to the traditional approach of static taxonomies. As a result, we expect to contribute to a better understanding of how suitable each system is for any given application requirement, and guide future developments towards the right direction. © 2017 Elsevier Inc.","Big-data; Distributed storage; Key–value storage systems; Peer-to-peer"
"Energy-efficient and delay-aware distributed routing with cooperative transmission for Internet of Things","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030861504&doi=10.1016%2fj.jpdc.2017.08.002&partnerID=40&md5=31efc64d0f4bd4d33a3da93ccce43e36","As an important part of IoT, Flying ad hoc networks (FANETs) can provide communication services for different devices in IoT and cyber-enabled applications. However, mobility of unmanned aerial vehicle (UAV) in FANETs results in wireless link unpredictability and increases the complexity of routing algorithms in these applications, especially for real-time routing. In this work, we propose an adaptive distributed routing method with cooperative transmission to effectively solve the problem presented above, in which the transmitters only use local information to transmit packets with help from their cooperative nodes. We formulate a corresponding mathematical optimization problem, in which the objective is to maximize network utility, while keeping end-to-end delay below a prescribed threshold. To support variations in network characteristics and channel states, we estimate the single-hop delay at relay node for each transmission, and use the dual decomposition method to transform the centralized problem into a distributed problem which allows the relay nodes to use only the local channel information and estimated delays to route the packets. Simulation results show that the proposed routing method can improve network performance in terms of energy efficiency, throughput and end-to-end delay. © 2017 Elsevier Inc.","Cooperative transmission; Delay constraint; Distributed routing; IoT and cyber-enabled applications"
"Design and evaluation of ZMesh topology for on-chip interconnection networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034647341&doi=10.1016%2fj.jpdc.2017.10.011&partnerID=40&md5=65707d92ca2772b0ae787b6c89f03491","This article presents the design and evaluation of a scalable and energy efficient Network-on-Chip topology with diagonal links, called ZMesh. A heuristic technique for mapping applications onto the ZMesh topology has been proposed. Further, a mapping technique based on particle swarm optimization combined with simulated annealing has also been considered for a fair comparison of the performance of ZMesh with that of other topologies. Experimental evaluation shows that ZMesh when compared with baseline Mesh, shows an average improvement of up to 1.25× and 1.20× in energy consumption and execution time, respectively while running real application traces. Compared to the state-of-the-art topologies such as DMesh, ZMesh shows an average improvement of up to 2.39× in terms of energy consumption, while suffering a degradation of up to 9.37% in execution time, for running real application traces. For the synthetic as well as real applications with more uniform traffic pattern, such as constant geometry algorithms, ZMesh achieves better energy efficiency than the other considered topologies. Evaluation of ZMesh for multicast kind of traffic using a proposed multicast routing algorithm shows improvement in link energy consumption by up to 55%, for the considered scenarios, as compared to unicast-based multicast routing. © 2017 Elsevier Inc.","Constant geometry; Diagonal links; Energy efficiency; Network-on-Chip; X-architecture"
"A data replication algorithm for groups of files in data grids","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035751957&doi=10.1016%2fj.jpdc.2017.10.008&partnerID=40&md5=f872468a06e990819e7255e982649e8e","Data grid is emerging as the main part of the infrastructure for large-scale data intensive applications such as high energy physics and bioinformatics. The deployment of such infrastructures has allowed users of a grid site to gain access to a large amount of distributed data. Data replication is a key issue in a data grid and could be applied intelligently because it reduces data access time and bandwidth consumption for each grid site. In this paper, we introduce a new dynamic data replication algorithm named Popular Groups of Files Replication (PGFR). Our proposed algorithm is based on an assumption: users in a Virtual Organization have similar interests in groups of files. Based on this assumption, and file access history, PGFR builds a connectivity graph to recognize a group of dependent files in each grid site and replicates the most Popular Groups of Files to each grid site, thus increasing the local availability. We used OptorSim simulator to evaluate the efficiency of PGFR algorithm. The simulation results show that PGFR achieves better performance compared to the existing algorithm; PGFR minimized the mean job execution time, bandwidth consumption, and avoiding unnecessary replication. © 2017 Elsevier Inc.","Connectivity graph; Data grids; Dynamic data replication; Group replication"
"Combined pre-detection and sleeping for energy-efficient spectrum sensing in cognitive radio networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041174483&doi=10.1016%2fj.jpdc.2017.12.013&partnerID=40&md5=b8c295de00f827fc6e4c494e85447328","In this paper, we propose a cooperative spectrum sensing scheme including a combined pre-detection and sleeping policy. In the scheme, the designed pre-detection sub-phase is applied at the beginning of the detection of the presence of a primary user, where all sensing nodes are involved to improve the detection performance. The sleeping policy is applied for each sensing node respectively at the end of the pre-detection sub-phase and the beginning of the transmission of the local detection results, in order to decrease sensing energy consumption. We formulate the problem of minimizing the maximum average energy consumption per sensing node in Rayleigh fading channels, considering the constraints of the required global detection and false alarm probabilities and the tolerable interference caused to the primary user. Numerical results show that the scheme achieves significant energy saving as compared to a scheme that includes only a sleeping policy and does not consider pre-detection. © 2017 Elsevier Inc.","Cognitive radio; Energy efficiency; High performance communication; Optimization; Spectrum sensing"
"Scalable Distributed Semantic Network for knowledge management in cyber physical system","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037720810&doi=10.1016%2fj.jpdc.2017.11.014&partnerID=40&md5=f82902d1ae1d1e3f40dea38ef393e59f","The remarkable growth of emerging technologies and computing paradigms in cyberspace and the cyber physical systems generate a huge mass of data sources. These different autonomous and heterogeneous data sources can contain complementary or semantically equivalent information stored under different formats that vary from structured, semi structured, to unstructured. These heterogeneities influence on data semantics and meaning. Therefore, knowledge management became more and more difficult and sometimes fruitless. In this paper, we propose a new scalable model, named Distributed Semantic Network (DSN), for heterogeneous data representation and can extract more semantic information from different data sources. We use the prior knowledge of WordNet and Wikipedia to scale out DSN horizontally and vertically. Furthermore, we proposed a MapReduce based framework to construct the knowledge base more effectively in Parallel and Distributed Computing (PDC). The experimental results show that DSN can better model the semantic information in the text. It can extract a larger amount of information from the text with a higher precision, achieving 34% increase in quantity and 15% promotion on precision than the best-performing alternative method on same datasets. On the three datasets, our proposed PDC framework shorten the process time by 5.8–11.5 times. © 2017 Elsevier Inc.","Cyber physical system; Distributed semantic network; Knowledge management; MapReduce framework; Parallel and distributed computing"
"A self-stabilizing memory efficient algorithm for the minimum diameter spanning tree under an omnipotent daemon","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043388001&doi=10.1016%2fj.jpdc.2018.02.007&partnerID=40&md5=8e9fd4d807a92f5ade8bd54ada62f1f9","Routing protocols are at the core of distributed systems performances, especially in the presence of faults. A classical approach to this problem is to build a spanning tree of the distributed system. Numerous spanning tree construction algorithms depending on the optimized metric exist (total weight, height, distance with respect to a particular process, …) both in fault-free and faulty environments. In this paper, we aim at optimizing the diameter of the spanning tree by constructing a minimum diameter spanning tree. We target environments subject to transient faults (i.e. faults of finite duration). Hence, we present a self-stabilizing algorithm for the minimum diameter spanning tree construction problem in the state model. Our protocol has the following attractive features. It is the first algorithm for this problem that operates under the unfair and distributed adversary (or daemon). In other words, no restriction is made on the asynchronous behavior of the system. Second, our algorithm needs only O(logn) bits of memory per process (where n is the number of processes), that improves the previous result by a factor n. These features are not achieved to the detriment of the convergence time, which stays polynomial. © 2018 Elsevier Inc.","Center; Diameter; MDST; Self-stabilization; Spanning tree; Unfair daemon"
"A multi-model estimation of distribution algorithm for energy efficient scheduling under cloud computing system","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043401144&doi=10.1016%2fj.jpdc.2018.02.009&partnerID=40&md5=0c32be3598b923faf0dd4433da111e5e","How to manage the applications under computing systems such as a cloud computing system in a more efficient way is a focus problem. The primary performance goal is to reduce the execution time (makespan) of the application. As the need to cloud computing grows, the environmental influence of data centers attracts much attention. This paper aims at the scheduling of the precedence-constrained parallel application to minimize time and energy consumption efficiently. A multi-model estimation of distribution (mEDA) algorithm is adopted to determine both task processing permutation and voltage supply levels (VSLs). Specific operators to decrease execution time and energy consumption are designed. An improvement operator is also designed to enhance the diversity of the non-dominated solutions. The proposed algorithm is compared with the standard heuristic methods and a parallel bi-objective genetic algorithm (bGA). The comparative results show the Pareto solution set by the proposed algorithm is able to dominate a large proportion of those solutions by both the heuristic methods and the bGA. © 2018 Elsevier Inc.","Cloud computing; Energy efficient scheduling; Estimation of distribution algorithm; Precedence-constrained parallel application; Task graph scheduling"
"Resource discovery for distributed computing systems: A comprehensive survey","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037707037&doi=10.1016%2fj.jpdc.2017.11.010&partnerID=40&md5=26f85f8114249faa597785d0d48835d3","Large-scale distributed computing environments provide a vast amount of heterogeneous computing resources from different sources for resource sharing and distributed computing. Discovering appropriate resources in such environments is a challenge which involves several different subjects. In this paper, we provide an investigation on the current state of resource discovery protocols, mechanisms, and platforms for large-scale distributed environments, focusing on the design aspects. We classify all related aspects, general steps, and requirements to construct a novel resource discovery solution in three categories consisting of structures, methods, and issues. Accordingly, we review the literature, analyzing various aspects for each category. © 2017 Elsevier Inc.","Distributed systems; Grid computing; HPC; P2P; Resource description; Resource sharing"
"Fast auto-clean CNN model for online prediction of food materials","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044986836&doi=10.1016%2fj.jpdc.2017.07.004&partnerID=40&md5=ad512e4138f027265ea49068e56350ec","Online food image detection is a key issue for intelligent food materials receiving and food supply chain applications, how to efficiently, accurately and quickly detect the image of food materials is a challenging research topic. A fast auto-clean convolutional neural network (CNN) model for online prediction of food materials is proposed, which is aiming at problems of the complex characteristics of the food images such as the complexity of the food materials, the focus of the dislocation and the uniformity of illumination. Firstly, a new approach of the auto-clean CNN models is proposed for automatic image cleaning and classification, which starting from original images and ending with multi-class prediction of clean images. Given a vocabulary of K classes, and a Yes/No clean label, two CNN models will learn a class label and a clean label respectively. Secondly, after the forward pass of two CNN models, the joint features generated from the last convolutional layers will be fed into our two loss layers. Combined with multi-class classification method, it classifies and optimizes the image dataset intelligently. Finally, an online prediction algorithm is proposed to improve the image recognition efficiency. Experimental results show that the proposed model and algorithm have good efficiency and accuracy, and the results of this study have significance to optimize the efficiency of the food supply chain industry and food quality evaluation. © 2017 Elsevier Inc.","CNN model; Fast auto-clean; Food image detection; Multi-class prediction"
"Cloud-centric IoT based disease diagnosis healthcare framework","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039726533&doi=10.1016%2fj.jpdc.2017.11.018&partnerID=40&md5=3c676c228e17dff84bbe7eb4ff602eb6","In the last few years, the m-healthcare applications based on Internet of Things (IoT) have provided multi-dimensional features and real-time services. These applications provide a platform to millions of people to get health updates regularly for a healthier lifestyle. Induction of IoT devices in the healthcare environment have revitalized multiple features of these applications. The big data generated by IoT devices in healthcare domain is analyzed on the cloud instead of solely relying on limited storage and computation resources of handheld devices. Relative to this context, a cloud-centric IoT basedm-healthcare monitoring disease diagnosing framework is proposed which predicts the potential disease with its level of severity. Key terminologies are defined to generate user-oriented health measurements by exploring the concept of computational sciences. The architectural prototype for smart student healthcare is designed for application scenario. The results are computed after processing the health measurements in a specific context. In our case study, systematic student perspective health data is generated using UCI dataset and medical sensors to predict the student with different disease severity. Diagnosis schemes are applied using various state-of-the-art classification algorithms and the results are computed based on accuracy, sensitivity, specificity, and F-measure. Experimental results show that the proposed methodology outperforms the baseline methods for disease prediction. © 2017 Elsevier Inc.","Cloud computing; Internet of Things (IoT); m-health; Smart Student Interactive System (SSIS); User Diagnosis Result (UDR)"
"PAHON: Power-Aware Hybrid Optical Network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042174699&doi=10.1016%2fj.jpdc.2018.01.007&partnerID=40&md5=84fa75625f03bba46b04288cffdbff7e","Power consumption of a network, as one of the important concerns in network design, increases when the size and traffic load of the network raises. In this article, a green hybrid optical network namely Power Aware Hybrid Optical Network (PAHON) is proposed. PAHON considers some implementation criteria including proper level of QoS, the optimal use of network resources, domain of network performance, and power consumption of the network. The proposed method considers four switching mechanisms (OCS, long-OBS, short-OBS and OPS) for providing desirable QoS and it could reach to high domain of performance by providing the ability for edge and core nodes to select the proper switching mechanism. PAHON can also decrease the power consumption of the network in comparison with electronic-optical networks and all electronic networks by using optical switches in core nodes. The results of simulations justify the optimal resource utilization and efficient power consumption of our proposed method. © 2018 Elsevier Inc.","Green optical networks; Hybrid optical networks; OBS; OCS; OPS; PAHON"
"A fast and vectorizable alternative to binary search in O(1) with wide applicability to arrays of floating point numbers","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034623037&doi=10.1016%2fj.jpdc.2017.10.007&partnerID=40&md5=97158f52547c7ffaa40d7dc9513e4f3a","Given an array X of N+1 strictly ordered floating point numbers2 and a floating point number z belonging to the interval [X0,XN), a common problem in numerical methods is to find the index i of the interval [Xi,Xi+1) containing z, i.e. the index of the largest number in the array X which is smaller or equal than z. This problem arises for instance in the context of spline interpolation or the computation of empirical probability distribution from empirical data. Often it needs to be solved for a large number of different values z and the same array X, which makes it worth investing resources upfront in pre-processing the array X with the goal of speeding up subsequent search operations. In some cases the values z to be processed are known simultaneously in blocks of size M, which offers the opportunity to solve the problem vectorially, exploiting the parallel capabilities of modern CPUs. The common solution is to sequentially invoke M times the well known binary search algorithm, which has complexity O(log2N) per individual search and, in its classic formulation, is not vectorizable, i.e. it is not SIMD friendly. This paper describes technical improvements to the binary search algorithm, which make it faster and vectorizable. Next it proposes a new vectorizable algorithm, based on an indexing technique, applicable to a wide family of X partitions, which solves the problem with complexity O(1) per individual search at the cost of introducing an initial overhead to compute the index and requiring extra memory for its storage. Test results using streaming SIMD extensions compare the performance of the algorithm versus various benchmarks and demonstrate its effectiveness. Depending on the test case, the algorithm can produce a throughput up to two orders of magnitude larger than the classic binary search. Applicability limitations and cache-friendliness related aspects are also discussed. © 2017 Elsevier Inc.","AVX; Data binning; FMA; Interpolation; Lower_bound; Quantization; SIMD; Spline; SSE; Vectorization"
"Enhancing debug observability for HLS-based FPGA circuits through source-to-source compilation","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044157656&doi=10.1016%2fj.jpdc.2018.02.012&partnerID=40&md5=3a057f5528bcb1a9c93a4feab1790d59","C-based High Level Synthesis (HLS)-compatible FPGA circuit descriptions from the CHStone benchmark suite are instrumented for debugging purposes using a source-to-source compiler. The debug instrumentation connects C expressions to top-level ports that can be observed during the debugging process. Approximately 50,000 different experiments are conducted to determine the impact on the final circuit caused by the debug instrumentation. Experimental data indicate initial feasibility of the instrumentation approach; all assignment expressions in a program can be instrumented for an average increase in LUT count of about 24%. Increases in FF count and clock period were in the range of 5% to 10%. The source-to-source compiler approach is compatible with many HLS synthesis systems and is flexible and extensible. © 2018 Elsevier Inc.","Debug; FPGA; High level synthesis; Instrumentation; Observability; Productivity"
"Approaches for optimizing virtual machine placement and migration in cloud environments: A survey","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030853223&doi=10.1016%2fj.jpdc.2017.08.010&partnerID=40&md5=f9523cc8952f517e65ed4e15ad853743","Cloud computing is a model for providing computing resources as a utility which faces several challenges on management of virtualized resources. Accordingly, virtual machine placement and migration are crucial to achieve multiple and conflicting goals. Regarding the complexity of these tasks and plethora of existing proposals, this work surveys the state-of-the-art in the area. It presents a cloud computing background, a review of several proposals, a discussion of problem formulations, advantages and shortcomings of reviewed works. Furthermore, it highlights the challenges for new solutions and provides several open issues, showing the relevancy of the topic in an increasing and demanding market. © 2017 Elsevier Inc.","Cloud computing; Resource allocation; Server consolidation; Service Level Agreement (SLA); VM migration; VM placement"
"Solving combinatorial problems using a parallel framework","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020905819&doi=10.1016%2fj.jpdc.2017.05.019&partnerID=40&md5=0a3606bbde342bee897c311f5736021b","This paper presents a new IBobpp framework which is an improvement of a high level parallel programming framework called Bobpp to optimize the performance of solving combinatorial problems. The Bobpp parallel computation model is as the majority of parallel models proposed in the context of tree search algorithms with node oriented parallelization, meaning that at every step of the algorithm, each thread gets one node from a unique global pool of non-explored nodes, generates the child nodes, and reinserts them into the pool to be explored later. This classical model has two drawbacks. First, the use of many threads creates a bottleneck problem. Second, grabbing a node causes memory contention problem when many nodes are generated and inserted into the same pool. To solve these problems, IBobpp framework proposes three solutions. The first consists of using multiple pools of nodes shared between all threads. The second solution consists of using a new computation model. The third solution consists of hybridization of the two previous solutions. Preliminary result shows that IBobpp gives a good result using the third solution. © 2017 Elsevier Inc.","Cluster; Combinatorial problems; Parallelism; Search algorithms"
"Dynamic scheduling strategy with efficient node availability prediction for handling divisible loads in multi-cloud systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034734816&doi=10.1016%2fj.jpdc.2017.10.006&partnerID=40&md5=72075941cd8e2358bb06fb46bcbf31a6","With large resource capacity, clouds have become a primary infrastructure for users to store big amount of data and perform large-scale computations. With the increasing demands and diversity of applications’ requirements, users are facing a fundamental problem of management of resources reserved from clouds. Particularly, the problem of load scheduling is the most important since it directly affects the performance of the system. Designing an efficient scheduling strategy for minimizing the total processing time of loads is challenging since it has to consider many intrinsic characteristics of the system such as the availability and heterogeneity of computing nodes, network topology and capacity. In this paper, we propose a novel architecture of a multi-cloud system that can satisfy complex requirements of users’ applications. Based on this architecture, we propose a dynamic scheduling strategy (DSS) that integrates the Divisible Load Theory and node availability prediction techniques to achieve high performance. We conduct intensive simulations to evaluate the performance of the proposed scheduling strategy. The results show that the proposed scheduling strategy outperforms the baseline schemes by reducing the total processing time of loads up to 44.60%. The results also provide useful insights on the applicability of the proposed approach in realistic scenarios. © 2017 Elsevier Inc.","Cloud computing; Divisible load theory; Multi-cloud system; Prediction techniques; Scheduling strategy"
"A cost minimization data allocation algorithm for dynamic datacenter resizing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018879853&doi=10.1016%2fj.jpdc.2017.03.010&partnerID=40&md5=4231b8f75a7035a820137a50ecd98a70","Modern datacenters dynamically adjust the number of active servers in different geographic regions to adapt to the dynamic workloads from user requests and electricity price heterogeneity. One of the main challenges for datacenter resizing is that the heavy network traffic among datacenters causes significant deterioration of the overall performance and considerably increases the operational expenditure of datacenters. In this paper, we propose an efficient data allocation technique that considers both the static and dynamic characteristics of datacenters to enable more efficient datacenter resizing. We first formulate the optimal data allocation problem, propose a generic model for minimizing the communicating cost in datacenter resizing, and show that the data allocation problem is NP-hard. To produce feasible solution in polynomial time, we propose a heuristic algorithm considering the traffic flow in the network topology of datacenters by first transforming the data allocation problem into a chunk distribution tree (CDT) construction problem, and then reducing the CDT construction to a graph partitioning problem. The experimental results show that our efficient data allocation approach can improve the performance of MapReduce operations effectively with lower communicating and computing costs for datacenter resizing. © 2017 Elsevier Inc.","Big data processing; Cost minimization; Data allocation; Datacenter resizing; MapReduce operation"
"An efficient deep model for day-ahead electricity load forecasting with stacked denoising auto-encoders","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021827104&doi=10.1016%2fj.jpdc.2017.06.007&partnerID=40&md5=7c43a929dee0d7219cd4ef133f0242da","In real word it is quite meaningful to forecast the day-ahead electricity load for an area, which is beneficial to reduction of electricity waste and rational arrangement of electric generator units. The deployment of various sensors strongly pushes this forecasting research into a “big data” era for a huge amount of information has been accumulated. Meanwhile the prosperous development of deep learning (DL) theory provides powerful tools to handle massive data and often outperforms conventional machine learning methods in many traditional fields. Inspired by these, we propose a deep learning based model which firstly refines features by stacked denoising auto-encoders (SDAs) from history electricity load data and related temperature parameters, subsequently trains a support vector regression (SVR) model to forecast the day-ahead total electricity load. The most significant contribution of this heterogeneous deep model is that the abstract features extracted by SADs from original electricity load data are proven to describe and forecast the load tendency more accurately with lower errors. We evaluate this proposed model by comparing with plain SVR and artificial neural networks (ANNs) models, and the experimental results validate its performance improvements. © 2017 Elsevier Inc.","Deep learning; Feature extraction; Multi-modal; Stacked denoising auto-encoders; Support vector regression"
"Reproducing dynamics related to an Internet of Things framework: A numerical and statistical approach","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028081818&doi=10.1016%2fj.jpdc.2017.06.020&partnerID=40&md5=dd6d8f5355bd424ca7f55c06ee4b114f","In the Cultural Heritage domain, novel fruition and enjoyment approaches, based on Internet of Things (IoT) paradigm, have the effect to change the way people experiencing cultural spaces, such as museums and exhibitions. Within these environments, a main challenge is to classify and forecast the behaviours of visitors by collecting their movements, choices and needs. In this paper, starting from the design of an IoT system for collecting behavioural data, we propose and discuss a model able to reproduce, and then “predict” the dynamics related to the interactions of the visitors with the exposed artworks and, in particular, with the available technologies. Generally, collected data are affected by many kinds of errors; to address this issue a powerful statistical method to reproduce the dynamics related to the visiting styles of the spectators is proposed. Numerical experiments on real data have been reported in order to assess the proposed methodology. © 2017 Elsevier Inc.","Internet of things; Particle filter; Visiting style; Visiting style prediction"
"Prediction of human protein subcellular localization using deep learning","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029536657&doi=10.1016%2fj.jpdc.2017.08.009&partnerID=40&md5=4ed6c19cead5d3f0ac80af3e27ccb81c","Protein subcellular localization (PSL), as one of the most critical characteristics of human cells, plays an important role for understanding specific functions and biological processes in cells. Accurate prediction of protein subcellular localization is a fundamental and challenging problem, for which machine learning algorithms have been widely used. Traditionally, the performance of PSL prediction highly depends on handcrafted feature descriptors to represent proteins. In recent years, deep learning has emerged as a hot research topic in the field of machine learning, achieving outstanding success in learning high-level latent features within data samples. In this paper, to accurately predict protein subcellular locations, we propose a deep learning based predictor called DeepPSL by using Stacked Auto-Encoder (SAE) networks. In this predictor, we automatically learn high-level and abstract feature representations of proteins by exploring non-linear relations among diverse subcellular locations, addressing the problem of the need of handcrafted feature representations. Experimental results evaluated with three-fold cross validation show that the proposed DeepPSL outperforms traditional machine learning based methods. It is expected that DeepPSL, as the first predictor in the field of PSL prediction, has great potential to be a powerful computational method complementary to existing tools. © 2017 Elsevier Inc.","Deep learning; Feature representation; Protein subcellular localization"
"A rear-end collision prediction scheme based on deep learning in the Internet of Vehicles","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029210049&doi=10.1016%2fj.jpdc.2017.08.014&partnerID=40&md5=cad4b832dc50e32df3c628f7830a3a53","Recently, the deep learning schemes have been well investigated for improving the driving safety and efficiency in the transportation systems. In this paper, a probabilistic model named as CPGN (Collision Prediction model based on GA-optimized Neural Network) for decision-making in the rear-end collision avoidance system is proposed, targeting modeling the impact of important influential factors of collisions on the occurring probability of possible accidents in the Internet of Vehicles (IoV). The decision on how to serve the chauffeur is determined by a typical deep learning model, i.e., the BP neural network through evaluating the possible collision risk with V2I (Vehicle-to-Infrastructure) communication, V2V (Vehicle-to-Vehicle) communication and GPS infrastructure supporting. The proper structure of our BP neural network model is deeply learned with training data generated from VISSIM with multiple influential factors considered. In addition, since the selection of the connection coefficient array and thresholds of the neural network has great randomness, a local optimization issue is readily occurring during the modeling procedure. To overcome this problem and consider the ability to efficiently find out a global optimization, this paper chooses the genetic algorithm to optimize the coefficient array and thresholds of proposed neural network. For the purpose of enhancing the convergence speed of the proposed model, we further adjust the studying rate according to the relationship between the actual and predicated values of two adjacent iterations. Simulation results demonstrate that the proposed collision risk evaluation framework could offer rationale estimations to the possible collision risk in car-following scenarios for the next discrete monitoring interval. © 2017 Elsevier Inc.","Genetic algorithm; Internet of Vehicles; Neural network; Rear-end collision"
"Parallel algorithms for computing the smallest binary tree size in unit simplex refinement","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021116501&doi=10.1016%2fj.jpdc.2017.05.016&partnerID=40&md5=67477611dc242c9271d2cd0317dc8b43","Refinement of the unit simplex by iterative longest edge bisection (LEB) up to sub-simplices have a size smaller or equal to a given accuracy, generates a binary tree. For a dimension higher than three, the size of the generated tree depends on the bisected LE. There may exist more than one selection sequence of LE that solves the Smallest Binary Tree Size Problem (SBTSP). Solving SBTSP by full enumeration requires considering every possible LE bisection in each sub-simplex. This is an irregular Combinatorial Optimization problem with an increasing computational burden in the dimension and the stopping criterion. Therefore, parallel computing is appealing to find the minimum size for hard instances in a reasonable time. The aim of this study is to develop and compare threaded algorithms running on multicore systems to solve the SBTS problem. Versions running on multicore systems with a static number of threads using TBB, and a dynamic number of threads using Pthread are compared. Interestingly, TBB scales better than the Pthread implementations for lower dimensional problems. However, when the problem dimension is higher than six, the Pthread approach with a dynamic number of threads finds a solution, where the TBB version fails. This is caused by the smaller memory footprint of the Pthread version, as it traverses deeper branches of the tree than the TBB work-stealing approach. © 2017 Elsevier Inc.","Binary tree; Dynamic number of threads; Longest edge bisection; Pthreads; Regular simplex; Shared memory; TBB"
"Harnessing sliding-window execution semantics for parallel stream processing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035330994&doi=10.1016%2fj.jpdc.2017.10.021&partnerID=40&md5=11250148ea77f881500b4c06a68eb35b","According to the recent trend in data acquisition and processing technology, big data are increasingly available in the form of unbounded streams of elementary data items to be processed in real-time. In this paper we study in detail the paradigm of sliding windows, a well-known technique for approximated queries that update their results continuously as new fresh data arrive from the stream. In this work we focus on the relationship between the various existing sliding window semantics and the way the query processing is performed from the parallelism perspective. From this study two alternative parallel models are identified, each covering semantics with very precise properties. Each model is described in terms of its pros and cons, and parallel implementations in the FastFlow framework are analyzed by discussing the layout of the concurrent data structures used for the efficient windows representation in each model. © 2017 Elsevier Inc.","Continuous queries; Data stream processing; Internet of Things; Parallel computing; Sliding windows"
"Replicable parallel branch and bound search","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035034159&doi=10.1016%2fj.jpdc.2017.10.010&partnerID=40&md5=0330b6276c8b8d02c7071abccf4a58ff","Combinatorial branch and bound searches are a common technique for solving global optimisation and decision problems. Their performance often depends on good search order heuristics, refined over decades of algorithms research. Parallel search necessarily deviates from the sequential search order, sometimes dramatically and unpredictably, e.g. by distributing work at random. This can disrupt effective search order heuristics and lead to unexpected and highly variable parallel performance. The variability makes it hard to reason about the parallel performance of combinatorial searches. This paper presents a generic parallel branch and bound skeleton, implemented in Haskell, with replicable parallel performance. The skeleton aims to preserve the search order heuristic by distributing work in an ordered fashion, closely following the sequential search order. We demonstrate the generality of the approach by applying the skeleton to 40 instances of three combinatorial problems: Maximum Clique, 0/1 Knapsack and Travelling Salesperson. The overheads of our Haskell skeleton are reasonable: giving slowdown factors of between 1.9 and 6.2 compared with a class-leading, dedicated, and highly optimised C++ Maximum Clique solver. We demonstrate scaling up to 200 cores of a Beowulf cluster, achieving speedups of 100x for several Maximum Clique instances. We demonstrate low variance of parallel performance across all instances of the three combinatorial problems and at all scales up to 200 cores, with median Relative Standard Deviation (RSD) below 2%. Parallel solvers that do not follow the sequential search order exhibit far higher variance, with median RSD exceeding 85% for Knapsack. © 2017 The Author(s)","Algorithmic skeletons; Branch-and-bound; Combinatorial optimisation; Distributed computing; Parallel algorithms; Repeatability"
"PA-Star: A disk-assisted parallel A-Star strategy with locality-sensitive hash for multiple sequence alignment","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020453359&doi=10.1016%2fj.jpdc.2017.04.014&partnerID=40&md5=fcf4169ec01601dacab7ebea236df3da","Multiple Sequence Alignment (MSA) is a basic operation in Bioinformatics, and is used to highlight the similarities among a set of sequences. The MSA problem was proven NP-Hard, thus requiring a high amount of memory and computing power. This problem can be modeled as a search for the path with minimum cost in a graph, and the A-Star algorithm has been adapted to solve it sequentially and in parallel. The design of a parallel version for MSA with A-Star is subject to challenges such as irregular dependency pattern and substantial memory requirements. In this paper, we propose PA-Star, a locality-sensitive multithreaded strategy based on A-Star, which computes optimal MSAs using both RAM and disk to store nodes. The experimental results obtained in 3 different machines show that the optimizations used in PA-Star can achieve an acceleration of 1.88× in the serial execution, and the parallel execution can attain an acceleration of 5.52× with 8 cores. We also show that PA-Star outperforms a state-of-the-art MSA tool based on A-Star, executing up to 4.77× faster. Finally, we show that our disk-assisted strategy is able to retrieve the optimal alignment when other tools fail. © 2017 Elsevier Inc.","A-Star; Locality-sensitive hash; Multiple sequence alignment; Parallel algorithms"
"A light-weight log-based hybrid storage system","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040468763&doi=10.1016%2fj.jpdc.2017.12.009&partnerID=40&md5=bb765fd58447bdcc5cd05ab15ff08f93","The Internet of Things (IoT) and cloud computing are two important technologies in our life. They are integrated together to support each other. However, with the rapid development of IoT, the disk-based storage systems in clouds fail to process and analyze data timely from IoT devices. In order to improve the storage capabilities of traditional disk-based storage systems, we present a light-weight log-based hybrid storage system, which comprises of the phase change memory (PCM), flash memory-based SSD and disks. These devices are merged as a single block device with a continuous linear-address space. We also design and implement a log-based hybrid file system called HFS for the block device. By combining the semantic characteristics of data with distinct performance characteristics of devices, HFS optimizes the file system layout and overcomes the shortcomings of traditional log-structured systems. We implement HFS in the Linux kernel and compare it with modern file systems such as Ext4 and F2FS. Evaluation results show the efficiency of the proposed light-weight log-based hybrid storage system. © 2017 Elsevier Inc.","Cloud computing; File system; Hybrid storage system; Internet of Things"
"A learning automata-based algorithm for energy and SLA efficient consolidation of virtual machines in cloud data centers","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034414434&doi=10.1016%2fj.jpdc.2017.10.009&partnerID=40&md5=b3b9968d2fc2b6aac966ece7c32cb846","Resource management in cloud computing consists of allocating processing resources, storage, and network to a set of software applications. Resource providers focus on performance and utilization of resources considering the constraints of service level agreement. Resource performance is achieved by virtualization techniques, which share infrastructure of the resource provider between different virtual machines. This study proposes a novel algorithm based on learning automata, which improves resource utilization and reduces energy consumption. The proposed algorithm considers changes in the user demanded resources to predict the PM, which may suffer from overload. Due to preventing server overload, the proposed algorithm improves PMs’ utilization, reduces the number of migrations, and shuts down idle servers to reduce the energy consumption of the data center. The proposed algorithm is simulated in CloudSim simulator; the 10-day processor information of a real PlanetLab cloud infrastructure system are used for workload data. Performance of the proposed algorithm is compared with existing algorithms such as DVFS, NPA, and the threshold algorithm in terms of energy consumption and the number of shut down PMs. Simulation results indicate that the proposed algorithm outperforms other algorithms with 175.48 Kwh, 0.00326 in energy consumption, SLA violation respectively. © 2017 Elsevier Inc.","Cloud computing; Energy consumption; Resource management; Service level agreement; Virtual machine migration"
"On solving separable block tridiagonal linear systems using a GPU implementation of radix-4 PSCR method","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042195254&doi=10.1016%2fj.jpdc.2018.01.004&partnerID=40&md5=8bc5ad3b5bf9197d9110ef4611ce5236","Partial solution variant of the cyclic reduction (PSCR) method is a direct solver that can be applied to certain types of separable block tridiagonal linear systems. Such linear systems arise, e.g., from the Poisson and the Helmholtz equations discretized with bilinear finite-elements. Furthermore, the separability of the linear system entails that the discretization domain has to be rectangular and the discretization mesh orthogonal. A generalized graphics processing unit (GPU) implementation of the PSCR method is presented. The numerical results indicate up to 24-fold speedups when compared to an equivalent CPU implementation that utilizes a single CPU core. Attained floating point performance is analyzed using roofline performance analysis model and the resulting models show that the attained floating point performance is mainly limited by the off-chip memory bandwidth and the effectiveness of a tridiagonal solver used to solve arising tridiagonal subproblems. The performance is accelerated using off-line autotuning techniques. © 2018 Elsevier Inc.","Fast direct solver; GPU computing; Partial solution technique; PSCR method; Roofline model; Separable block tridiagonal linear system"
"Floating-point accelerator for biometric recognition on FPGA embedded systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031996404&doi=10.1016%2fj.jpdc.2017.09.010&partnerID=40&md5=a652affc1827cbc36c60385942bf38fa","This paper aims at presenting a Floating-Point Biometric Accelerator (FPBA) specially designed to speed-up processing kernels used in biometric algorithms. The FPBA was developed in order to facilitate its inclusion as part of an embedded system that was implemented on a low-cost FPGA family from Xilinx. The advantage of this approach is that such algorithms can be programmed on the same hardware architecture following a software design flow. The internal design includes several blocks that compute basic operations and transcendental functions useful in biometrics. For comparison purposes, the execution time of four typical biometric kernels resolved by the FPBA were compared against the floating-point unit (FPU) provided by Xilinx. In all cases, the experimental results show that the FPBA reduces the execution time by a factor ranging from x7 to x22. The results also show the execution of two biometric recognition algorithms that are accelerated by x7 and x16. © 2017 Elsevier Inc.","Accelerator; Biometrics; DTW; Embedded system; FFT; Floating-point; FPGA; GMM; SVM"
"A keyword-aware recommender system using implicit feedback on Hadoop","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041139619&doi=10.1016%2fj.jpdc.2017.12.008&partnerID=40&md5=56c3bc889d785b863554c60e3f61f13d","Recommendation mechanisms offer an effective way to utilize user observations and ratings. The cold-start problem is most prevalent to affect the accuracy and reasoning of recommendation tasks, since new users or items have no side information in recommendation domains. A common architecture of keyword-aware recommender systems is proposed to improve the problem of cold-start users using extra information from external user or item domains. Keywords in a textual description of the extra information are significant factors in estimating user rating of items. Gathering the extra information through WEB scraping methods advances the dimensionality of keyword spaces. To implement the proposed architecture, textual descriptions for the users, who have installed APPs, and the movie-related items are expressed as extra information of the user and item domains. The keyword datasets of users and items drive the estimation of user initial ratings, and make further movie's recommendations to cold-start users. A Big Data environment is essential to perform rating estimation with extensive datasets to establish a high-quality recommendation model. Experimental results obtained from the analysis of keyword similarity and rating accuracy represent that the proposed architecture of keyword-aware recommenders is effective and promising, as well offer profitable recommendation services on improving the recommendation accuracy. © 2017 Elsevier Inc.","Cold-start problem; Collaborative filtering; Keyword-aware; Recommendation"
"Marine depth mapping algorithm based on the edge computing in Internet of things","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041189836&doi=10.1016%2fj.jpdc.2017.12.016&partnerID=40&md5=5eed43cddbae44a41da1656de76beb26","In recent years, the research of marine environmental monitoring system has been especially popular. The construction of Internet of things between devices occupies the main position in marine environment detection system. In order to protect and utilize marine resources, it is urgent to realize the collection and treatment of marine information. We have established the Internet of things system for large ocean data collected by sensors. We calculate the ocean data in terminal devices such as sensors. In the process of calculation and arrangement of data, this paper presents a new method for the calculation of data contours. The aim of this method is to quickly describe the contour of data. The distribution of contour lines can be calculated accurately in a short time. This paper introduces the principle and calculation process of the method. The simulation of big data calculation is carried out in practice. The simulation results are also analyzed. Finally, the advantages of this method are illustrated by comparison simulation. In the same set of data and conditions, the method achieves the improvement of computational efficiency and calculation accuracy. It has certain significance in practical application. © 2018 Elsevier Inc.","Contour line; Marine information; Marine Internet of things; Triangle grid"
"A scalable and manageable IoT architecture based on transparent computing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027730552&doi=10.1016%2fj.jpdc.2017.07.003&partnerID=40&md5=adfdfc5dca01fb2563cbebfdf7223ab7","With the explosion of connected devices, the Internet-of-Things (IoT) is expected to be the fundamental infrastructure of the information society and receives a wide variety of applications in different scenarios. However, the fast-growing IoT technology is still facing many challenges posed by large-scale heterogeneous IoT devices. To address these challenges we propose a transparent computing based IoT architecture to build scalable and manageable IoT applications. The proposed architecture consists of five layers, i.e., end-user layer, edge network layer, core network layer, service&storage layer, and management layer. It can provide centralized management of various resources like operating systems, services and data for IoT applications, and enable on-demand services to be executed on heterogeneous IoT devices. We also build a prototype system to evaluate the performance of the proposed architecture in terms of the delay and energy consumption in remote service updating. The experimental results demonstrate that it can provide efficient management of various resources and achieve on-demand service provisioning for IoT devices. © 2017 Elsevier Inc.","Centralized resource management; Internet of Things; Scalability; Streaming execution; Transparent computing"
"A dynamic tradeoff data processing framework for delay-sensitive applications in Cloud of Things systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032446267&doi=10.1016%2fj.jpdc.2017.09.009&partnerID=40&md5=b02ccb86db7df439e74ad5f2ec8fd1dc","The steep rise of Internet of Things (IoT) applications along with the limitations of Cloud Computing to address all IoT requirements leveraged a new distributed computing paradigm called Fog Computing, which aims to process data at the edge of the network. With the help of Fog Computing, the transmission latency, monetary spending and application loss caused by Cloud Computing can be effectively reduced. However, as the processing capacity of fog nodes is more limited than that of cloud platforms, running all applications indiscriminately on these nodes can cause some QoS requirement to be violated. Therefore, there is important decision-making as to where executing each application in order to produce a cost effective solution and fully meet application requirements. In particular, we are interested in the tradeoff in terms of average response time, average cost and average number of application loss. In this paper, we present an online algorithm, called unit-slot optimization, based on the technique of Lyapunov optimization. The unit-slot optimization is a quantified near-optimal online solution to balance the three-way tradeoff among average response time, average cost and average number of application loss. We evaluate the performance of the unit-slot optimization algorithm by a number of experiments. The experimental results not only match up the theoretical analyses properly, but also demonstrate that our proposed algorithm can provide cost-effective processing, while guaranteeing average response time and average number of application loss in a three-tier Cloud of Things system. © 2017 Elsevier Inc.","Average response time; Fog computing; Internet of Things; Lyapunov optimization"
"Threshold load balancing with weighted tasks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038867773&doi=10.1016%2fj.jpdc.2017.10.012&partnerID=40&md5=c055a67dc936d62e90c7cb5cf6b50069","We study threshold-based load balancing protocols for weighted tasks. We are given an arbitrary graph G with n nodes (resources, bins) and m≥n tasks (balls). Initially the tasks are distributed arbitrarily over the n nodes. The resources have a threshold and we are interested in the balancing time, i.e., the time it takes until the load of all resources is below or at the threshold. We distinguish between resource-based and user-based protocols. In the case of resource-based protocols, resources with a load larger than the threshold are allowed to send tasks to neighboring resources. In the case of user-based protocols, the tasks make the migration decisions and we restrict ourselves to the complete graph in the model. Any task allocated to a resource with a load above the threshold decides whether to migrate to a neighboring resource independently of the other tasks. For resource-controlled protocols, we present results for arbitrary graphs. For the user-controlled migration, we consider complete graphs and derive bounds for both above-average and tight thresholds. © 2017 Elsevier Inc.","Mixing time of random walks; Random walks; Threshold load balancing; Weighted tasks"
"Fair-share scheduling in single-ISA asymmetric multicore architecture via scaled virtual runtime and load redistribution","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030086350&doi=10.1016%2fj.jpdc.2017.08.012&partnerID=40&md5=0adc2ccc3e31dec38dbee6c3981cf9f5","Performance-asymmetric multicore processors have been increasingly adopted in embedded systems due to their architectural benefits in improved performance and power savings. While fair-share scheduling is a crucial kernel service for such applications, it is still at an early stage with respect to performance-asymmetric multicore architecture. In this article, we first propose a new fair-share scheduler by adopting the notion of scaled CPU time that reflects the performance asymmetry between different types of cores. Using the scaled CPU time, we revise the virtual runtime of the completely fair scheduler (CFS) of the Linux kernel, and extend it into the scaled virtual runtime (SVR). In addition, we propose an SVR balancing algorithm that bounds the maximum SVR difference of tasks running on the same core types. The SVR balancing algorithm periodically partitions the tasks in the system into task groups and allocates them to the cores in such a way that tasks with smaller SVR receive larger SVR increments and thus proceed more quickly. We formally show the fairness property of the proposed algorithm. To demonstrate the effectiveness of the proposed approach, we implemented our approach into Linaro's scheduling framework on ARM's Versatile Express TC2 board and performed a series of experiments using the PARSEC benchmarks. The experiments show that the maximum SVR difference is only 4.09 ms in our approach, whereas it diverges indefinitely with time in the original Linaro's scheduling framework. In addition, our approach incurs a run-time overhead of only 0.4% with an increased energy consumption of only 0.69%. © 2017 Elsevier Inc.","Fair-share scheduling; Load balancing; Multicore; Performance-asymmetry"
"Delta state replicated data types","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029529969&doi=10.1016%2fj.jpdc.2017.08.003&partnerID=40&md5=6303cf062046288e3844954928dcafed","Conflict-free Replicated Data Types (CRDTs) are distributed data types that make eventual consistency of a distributed object possible and non ad-hoc. Specifically, state-based CRDTs ensure convergence through disseminating the entire state, that may be large, and merging it to other replicas. We introduce Delta State Conflict-Free Replicated Data Types (δ-CRDT) that can achieve the best of both operation-based and state-based CRDTs: small messages with an incremental nature, as in operation-based CRDTs, disseminated over unreliable communication channels, as in traditional state-based CRDTs. This is achieved by defining δ-mutators to return a delta-state, typically with a much smaller size than the full state, that to be joined with both local and remote states. We introduce the δ-CRDT framework, and we explain it through establishing a correspondence to current state-based CRDTs. In addition, we present an anti-entropy algorithm for eventual convergence, and another one that ensures causal consistency. Finally, we introduce several δ-CRDT specifications of both well-known replicated datatypes and novel datatypes, including a generic map composition. © 2017 Elsevier Inc.","Conflict-free replicated data types; CRDT; Delta; Distributed systems; Eventual consistency; State-based"
"IoT-based intelligent fitness system","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027446310&doi=10.1016%2fj.jpdc.2017.05.006&partnerID=40&md5=dcf194128b4a6ff9dbb4d63fe83c573e","With the global economic growth, fitness club is developing rapidly in the world. Meanwhile, the fitness industry is booming especially for urban white-collar population. In the circumstances, people need more scientific and practical guidance to build their body. In this paper, we design an Internet of things (IoT) based fitness system to monitor the health statuses of exercisers. The system provides guidance for exercisers. When exercising, the exercise data is collected by sensors and fitness band. Subsequently, these data are sent to the system to be analyzed. With the help of artificial intelligence technology, the system can extract useful guidance information for users’ body building. In this paper, we will describe the details of the system and further reach out to the implementation technologies. The design of this kind of system is a trend for the future fitness application. © 2017 Elsevier Inc.","Artificial intelligence; Body building; Fitness; Fitness band; IoT; Sensor"
"GCHAR: An efficient Group-based Context—aware human activity recognition on smartphone","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021154566&doi=10.1016%2fj.jpdc.2017.05.007&partnerID=40&md5=814ab55d969d23f43bfb9bfad7e64617","With smartphones increasingly becoming ubiquitous and being equipped with various sensors, nowadays, there is a trend towards implementing HAR (Human Activity Recognition) algorithms and applications on smartphones, including health monitoring, self-managing system and fitness tracking. However, one of the main issues of the existing HAR schemes is that the classification accuracy is relatively low, and in order to improve the accuracy, high computation overhead is needed. In this paper, an efficient Group-based Context-aware classification method for human activity recognition on smartphones, GCHAR is proposed, which exploits hierarchical group-based scheme to improve the classification efficiency, and reduces the classification error through context awareness rather than the intensive computation. Specifically, GCHAR designs the two-level hierarchical classification structure, i.e., inter-group and inner-group, and utilizes the previous state and transition logic (so-called context awareness) to detect the transitions among activity groups. In comparison with other popular classifiers such as RandomTree, Bagging, J48, BayesNet, KNN and Decision Table, thorough experiments on the realistic dataset (UCI HAR repository) demonstrate that GCHAR achieves the best classification accuracy, reaching 94.1636%, and time consumption in training stage of GCHAR is four times shorter than the simple Decision Table and is decreased by 72.21% in classification stage in comparison with BayesNet. © 2017 Elsevier Inc.","Context awareness; Hierarchical classifier; Human Activity Recognition (HAR); Machine learning"
"A distributed k-mutual exclusion algorithm based on autonomic spanning trees","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042181927&doi=10.1016%2fj.jpdc.2018.01.008&partnerID=40&md5=7b48926418c3d25c32a84e3db928cdc1","Distributed k-mutual exclusion ensures that at most a single process has permission to access each of the k copies of a critical resource. In this work we present an autonomic solution for distributed k-mutual exclusion that adapts itself after system changes. Our solution employs a hierarchical best-effort broadcast algorithm to propagate messages reliably and efficiently. The broadcast is based on another autonomic building block: a distributed algorithm for creating and maintaining spanning trees constructed in a fully distributed and adaptive way on top of a virtual hypercube-like topology, called VCube. The proposed solutions are autonomic in the sense that they reconfigure themselves automatically after the detection of faults given the set of correct processes in the system. All proposed algorithms are described, specified, and proofs of correctness are given. Results from simulation show that the proposed approach is more efficient and scalable compared to other solutions. © 2018 Elsevier Inc.","Distributed applications; Fault-tolerance; Hypercube-like topology; Mutual exclusion; Reliable broadcast"
"Hybrid online protocols for source location privacy in wireless sensor networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042086240&doi=10.1016%2fj.jpdc.2018.01.006&partnerID=40&md5=801e182c7649f15fc626616cf4fec17b","Wireless sensor networks (WSNs) will form the building blocks of many novel applications such as asset monitoring. These applications will have to guarantee that the location of the occurrence of specific events is kept private from attackers, in what is called the source location privacy (SLP) problem. Fake sources have been used in numerous techniques, however, the solution's efficiency is typically achieved by fine-tuning parameters at compile time. This is undesirable as WSN conditions may change. In this paper, we first present an SLP algorithm – Dynamic – that estimates the relevant parameters at runtime and show that it provides a high level of SLP, albeit at the expense of a high number of messages. To address this, we provide a hybrid online algorithm – DynamicSPR – that uses directed random walks for the fake sources allocation strategy to reduce energy usage. We perform simulations of the various protocols we present and our results show that DynamicSPR provides a similar level of SLP as when parameters are optimised at compile-time, with a lower number of messages sent. © 2018 Elsevier Inc.","Fake sources; Online algorithm; Random walks; Source location privacy; Wireless sensor networks"
"On the performance of greedy forwarding on Yao and Theta graphs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043514372&doi=10.1016%2fj.jpdc.2018.02.006&partnerID=40&md5=12666a4ede678328a92f00f77bd5c3d8","Greedy Forwarding algorithm is an important geometric routing algorithm for wireless networks. However, it can fail if the network topologies contain voids, in which a packet cannot be moved closer to destination. Since Yao and Theta graphs are two types of geometric graphs exploited to construct wireless network topologies, this paper first gives theoretical results on whether these two types of graphs can contain voids with respect to their cone numbers. Then, this paper examines the performance of Greedy Forwarding on Yao and Theta graphs in terms of stretch (the ratio between the path length found by Greedy Forwarding and the shortest path length from a source to a destination). Both worst-case and average-case stretches are studied. For the worst case, this paper shows that the stretches of Greedy Forwarding on both Yao and Theta graphs do not have a constant upper bound (i.e., not competitive) in hop metric. For the average case, this paper studies the stretch experimentally by running Greedy Forwarding on a large number of Yao and Theta graphs with randomly generated node sets. The average-case stretches in both hop and Euclidean metrics are measured, and several interesting findings are revealed. © 2018 Elsevier Inc.","Geometric routing; Greedy forwarding; Theta graph; Voids; Wireless networks; Yao graph"
"Asynchronous and multithreaded communications on irregular applications using vectorized divide and conquer approach","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040112611&doi=10.1016%2fj.jpdc.2017.12.004&partnerID=40&md5=a12c209559f0dfbe97cef66447e11b8e","The evolution of hardware architectures driven by the increasing requirement for performance and energy efficiency has led to complex HPC systems. In the context of Finite Element Methods, exposing massive parallelism on unstructured mesh computations with efficient load balancing and minimal synchronizations is challenging. Several parallelization strategies have to be combined together to exploit the multiple levels of parallelism. We propose several contributions aimed at addressing irregular codes and data structures in an efficient way. We have developed a hybrid parallelization approach based on the Divide & Conquer (D&C) principle which combines the distributed, shared, and vectorial forms of parallelism in a fine grain task-based parallelism approach applied to irregular structures. We experiment our approach using a matrix assembly step of an industrial application from Dassault Aviation on standard Xeon multicores and Xeon Phi KNC manycores. On 512 Intel Xeon E5-2670 Sandy Bridge cores, we surpass the pure MPI approach by up to 3.47× and reach 77% of parallel efficiency using only 2000 vertices per core. On 4 Xeon Phi 5110p KNC, D&C has similar performance to 96 Intel Xeon E5-2670 Sandy Bridge cores; it achieves an excellent parallel efficiency of 96%, and up to 6.56× speedup compared to pure MPI. © 2017 Elsevier Inc.","Cilk; D&C; FEM; GASPI; Multithreading; PGAS; Unstructured mesh; Vectorization"
"Topology control for minimizing interference with delay constraints in an ad hoc network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034663407&doi=10.1016%2fj.jpdc.2017.10.005&partnerID=40&md5=49c4168494411fa53256b0a13617347c","Most of the existing work on topology control is to minimize the interference or delay separately. However, minimizing the interference and reducing the delay are two conflicting goals; therefore, considering a trade-off between them is necessary. In this paper, our goal is to minimize the interference while guaranteeing the end-to-end delay under a constraint. We take into account three optimization objectives, including the maximum interference, average interference and average path interference. We propose a centralized algorithm with a greedy strategy that can minimize the maximum interference while satisfying the delay constraint. A distributed algorithm (LDMST) is proposed to minimize the average interference. In LDMST, each node builds a delay-constrained minimum spanning tree (DMST). To minimize the path interference, a localized delay-constrained Bellman–Ford (LDBF) algorithm is proposed. LDBF employs an improved Bellman–Ford algorithm to find the optimal path, which has the minimum interference and satisfies the delay constraint. The final topology is composed of these optimal paths. The simulation results illustrate that the proposed topology control algorithms exhibit good performance in terms of the interference objectives and can also guarantee the end-to-end delay under the constraint. © 2017 Elsevier Inc.","Ad hoc networks; Delay; Interference; Topology control algorithm"
"Using convolution control block for Chinese sentiment analysis","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035765981&doi=10.1016%2fj.jpdc.2017.10.018&partnerID=40&md5=26885c7f143491902d665a75b1d81d8f","Convolutional neural network (CNN) has lately received great attention because of its good performance in the field of computer vision and speech recognition. It has also been widely used in natural language processing. But those methods for English cannot be transplanted due to phrase segmentation. Those for Chinese are not good enough for poorly semantic retrieving. We propose a Chinese sentiment classification model on the concept of convolution control block (CCB). It aims at classifying Chinese sentences into the positive or the negative. CCB based model considers short and long context dependencies. Parallel convolution of different kernel sizes is designed for phrase segmentation, gate convolution for merging and filtering abstract features, and tiering 5 layers of CCBs for word connection in sentence. Our model is evaluated on Million Chinese Hotel Review dataset. Its positive emotion accuracy reaches 92.58%, which outperforms LR_all and DCN by 2.89% and 4.03%, respectively. Model depth and sentence length are positively related to the accuracy. Gate convolution indeed improves model accuracy. © 2017 Elsevier Inc.","Convolutional neural network; Deep learning; Natural language processing; Sentiment analysis"
"Deadline-aware rate allocation for IoT services in data center network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032206642&doi=10.1016%2fj.jpdc.2017.09.012&partnerID=40&md5=70b8a98d84b7b0072544ab816fea03f7","Data center is the key infrastructure for a plenty of applications involving a high volume of data in Internet of Things (IoT). The Data Center Network (DCN) connecting multiple servers plays a vital role. Its mechanism for managing the traffic has a significant impact on the performance of IoT services. To guarantee the real-time performance of IoT services is one of the major challenges. In the paper Deadline-aware Rate Allocation (DRA) algorithm for scheduling the heterogeneous flows in DCNs is proposed. A non-cooperative game-theoretic framework is introduced to model the interactions in the scenario. The core idea of DRA is to assign the traffic with deadline constraints a higher priority. The worker with a lower served rate in the past period is assigned a higher priority. Meanwhile, DRA is a kind of preemptive algorithm. Simulation results have shown that under the mechanism flows wait shorter time and the real-time performance is guaranteed. DRA also achieves good fairness among different IoT services. © 2017 Elsevier Inc.","Big data; Data center network; Incast congestion; Internet of things; Online service"
"Verifying temporal properties of programs: A parallel approach","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030778248&doi=10.1016%2fj.jpdc.2017.09.003&partnerID=40&md5=8304ac8e62b5ad83e94ad8fd1bc7fd45","Due to the nature in dealing only with observed executions of a real system, runtime verification is being pursued as a lightweight verification technique. However, the overhead for analyzing the desired temporal properties usually degrades performance greatly. The reasons are: (1) the low efficiency of the interpretive execution in the analyzing process, and (2) nondeterminism of the automata generated from temporal logic properties. To overcome them, this paper presents a parallel approach to verify full regular temporal properties of a program. With this approach, the program is written in Modeling, Simulation and Verification Language (MSVL), and the property is specified by a Propositional Projection Temporal Logic (PPTL) formula. Execution and verification of a program are carried out at the same time. Specifically, each trace generated from executing a program is divided into several segments which are verified in parallel. Also, in the process of verifying each segment, nondeterministic choices in the relative automaton of a temporal property are also handled in parallel. Finally, verification results of all the segments are merged to form the eventual result as well as the counterexample. Our parallel mechanism takes full advantage of the hardware resources and makes temporal property verification efficient in a multi-core system. Experiments show that our approach is practical in verifying temporal properties of large-scale programs in the real world. © 2017 Elsevier Inc.","Full regular properties; Model checking; Parallel; Program verification; Runtime verification"
"Racetrack Memory based hybrid Look-Up Table (LUT) for low power reconfigurable computing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044001086&doi=10.1016%2fj.jpdc.2018.02.018&partnerID=40&md5=7925957a2f7d6c851c557cf47f422bf2","The large area and high power consumption are the two main bottlenecks in the conventional SRAM-based Field Programmable Gate Arrays (FPGAs). In recent works, resistive Non-Volatile Memories (NVMs) have been widely proposed to tackle the above issues in the reconfigurable computing systems, due to their non-volatility, fast read/write speed and high-density. The magnetic Domain-Wall (DW) Racetrack Memory (RM) is the emerging NVM with the great prospect of the development of the low-power and high-density circuits and systems. This paper presents RM based single-context and multi-context hybrid Look-Up Tables (LUTs). The hybrid structure allows the LUT to support both volatile input (low-power and high-speed input) and non-volatile input. The non-volatile input is used to reduce the leakage power and also to provide additional reusable resources to increase the hardware utilization. Compared to the SRAM-based 6-input LUT, the proposed non-volatile LUT reduces the number of transistors and leakage power by 80.2% and 84.2%, respectively. The proposed design also reduces the leakage power of the conventional 6-input non-volatile LUT by 17.4% with 27.3% fewer transistors and 36% faster operation speed. The Verilog-to-Routing (VTR) simulation results show that the proposed 6-input LUT consumes 27.1% less power than the SRAM-based counterpart. It may also provide 15.2% additional reusable resource. The context of the proposed multi-context LUT can be switched in 4 ns with the context switching energy of 397.24 fJ/LUT. © 2018 Elsevier Inc.","Hybrid look-up table; Multi-context; Non-volatile memory; Racetrack memory; Reconfiguration; Reusable resource"
"Efficiency analysis methodology of FPGAs based on lost frequencies, area and cycles","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038097261&doi=10.1016%2fj.jpdc.2017.11.012&partnerID=40&md5=a57cde1d8eca4c4132a70b88bc362620","We propose a methodology to study and to quantify efficiency and the impact of overheads on runtime performance. Most work on High-Performance Computing (HPC) for FPGAs only studies runtime performance or cost, while we are interested in how far we are from peak performance and, more importantly, why. The efficiency of runtime performance is defined with respect to the ideal computational runtime in absence of inefficiencies. The analysis of the difference between actual and ideal runtime reveals the overheads and bottlenecks. A formal approach is proposed to decompose the efficiency into three components: frequency, area and cycles. After quantification of the efficiencies, a detailed analysis has to reveal the reasons for the lost frequencies, lost area and lost cycles. We propose a taxonomy of possible causes and practical methods to identify and quantify the overheads. The proposed methodology is applied on a number of use cases to illustrate the methodology. We show the interaction between the three components of efficiency and show how bottlenecks are revealed. © 2017 Elsevier Inc.","FPGA; High-Level Synthesis; High-Performance Computing; Lost cycle analysis; Performance efficiency; Vivado HLS"
"A modeling front-end for seamless design and generation of context-aware Dynamically Reconfigurable Systems-on-Chip","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032029227&doi=10.1016%2fj.jpdc.2017.09.011&partnerID=40&md5=8a05813d7f937b01bf709fb41732a761","In this paper, we present a Model Driven Engineering (MDE) methodology for facilitating the modeling of the partial reconfiguration process, and for implementing Dynamic Reconfigurable System-on-Chip (DRSoC). The rationale for this approach is to provide a modeling front-end that enables to visually compose a hardware platform, containing heterogeneous components, using both static and context-management hardware wrappers. A model transformations engine (MTE) processes the high-level models to obtain the inputs for the Xilinx dynamic partial reconfiguration (DPR) design flow, with the benefit of better exploiting and reusing the designer intentions regarding the allocation of tasks into reconfigurable areas. Furthermore, the automatic synthesis of a reconfiguration controller (RecOS) with context-management and task relocation capabilities is supported in this version of our tool. The latter feature is possible due to the integration of relocation tool OORBIT into the design chain, but we point out at other avenues of research. We present a case study in which we assess the benefits of the methodology and present a thorough analysis of the reconfiguration costs associated with the context and relocation management, showing speedups of 1.5x over other solutions. © 2017 Elsevier Inc.","Bistream relocation; Context-aware management; IP-XACT; Reconfigurable systems; System-on-Chip; UML MARTE"
"A proactive approach based on online reliability prediction for adaptation of service-oriented systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041209425&doi=10.1016%2fj.jpdc.2017.12.006&partnerID=40&md5=99852aa71a8061f5a76e1b22fc6dac0e","Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards System as a Service (i.e. SaaS), and aims to construct a robust and value-added complex system by outsourcing external component systems through service composition technology. A service-oriented SoS runs under a dynamic and uncertain environment. To successfully deploy SoS's run-time quality assurance, online reliability time series prediction, which aims to predict the reliability in near future for a service-oriented SoS,arises as a grand challenge in SoS research. In this paper, we tackle the prediction challenge by exploiting two novel prediction models. We adopt motifs-based Dynamic Bayesian Networks (or m_DBNs) model to perform one-step-ahead time series prediction, and propose a multi-steps trajectories DBNs (or multi_DBNs) model to further revise the future reliability prediction. Finally, a proactive adaption strategy is achieved based on the reliability prediction results. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently. © 2017 Elsevier Inc.","Online reliability prediction; Proactive adaption; Service-oriented system; System of Systems; Time series"
"A scalable parallel cooperative coevolutionary PSO algorithm for multi-objective optimization","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021167079&doi=10.1016%2fj.jpdc.2017.05.018&partnerID=40&md5=40527c71dff266d1b2d1ab065d1598b4","We present a parallel multi-objective cooperative coevolutionary variant of the Speed-constrained Multi-objective Particle Swarm Optimization (SMPSO) algorithm. The algorithm, called CCSMPSO, is the first multi-objective cooperative coevolutionary algorithm based on PSO in the literature. SMPSO adopts a strategy for limiting the velocity of the particles that prevents them from having erratic movements. This characteristic provides the algorithm with a high degree of reliability. In order to demonstrate the effectiveness of CCSMPSO, we compare our work with the original SMPSO and three different state-of-the-art multi-objective CC metaheuristics, namely CCNSGA-II, CCSPEA2 and CCMOCell, along with their original sequential counterparts. Our experiments indicate that our proposed solution, CCSMPSO, offers significant computational speedups, a higher convergence speed and better or comparable results in terms of solution quality, when evaluated against three other CC algorithms and four state-of-the-art optimizers (namely SMPSO, NSGA-II, SPEA2, and MOCell), respectively. We then provide a scalability analysis, which consists of two studies. First, we analyze how the algorithms scale when varying the problem size, i.e., the number of variables. Second, we analyze their scalability in terms of parallelization, i.e., the impact of using more computational cores on the quality of solutions and on the execution time of the algorithms. Three different criteria are used for making the comparisons, namely the quality of the resulting approximation sets, average computational time and the convergence speed to the Pareto front. © 2017 Elsevier Inc.","Coevolutionary; Cooperative; Metaheuristics; Parallelism; Particle swarm optimization"
"A survey of real-time approximate nearest neighbor query over streaming data for fog computing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042633871&doi=10.1016%2fj.jpdc.2018.01.005&partnerID=40&md5=2ba02166e2398312b65b3df2ab2ea918","Real-time approximate nearest neighbor (ANN) query over streaming data in fog computing environment is the fundamental problem of real-time analysis of big data. As the fog computing paradigm needs to provide real-time and low latency services, and traditional streaming data ANN query technology cannot be directly applied. Exploring the basic theory, querying framework and technology of real-time ANN query over streaming data for fog computing becomes one of the current research hotspots. This paper summarizes the related ANN query technology based on random hash, learning-to-hash and synopses, analyzes the problems and challenges of real-time ANN query in resource-limited fog computing environment, and finally discusses in detail the basic theory and method of the query, the dimension reduction and encoding method based on learning-to-hash, the generating synopses method for ANN query over streaming data from Internet of Thing, and the future related research directions of ANN query framework and others. Additionally, we propose a Dynamic Adaptive Quantization (DAQ) method for learning-to-hash. Experiments show that DAQ outperformed other quantization methods. © 2018 Elsevier Inc.","Approximate nearest neighbor query; Fog computing; Hashing learning; Quantization method; Real-time analysis; Streaming data"
"QoE enhancement in cloud virtual machine allocation using Eagle strategy of hybrid krill herd optimization","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030773226&doi=10.1016%2fj.jpdc.2017.08.015&partnerID=40&md5=8ed2dc9bfb4e422e699bb9f2be1eb368","This paper presents a novel VM allocation policy using the Eagle Strategy of Hybrid Krill Herd (KH) Optimization technique to enhance the cloud service experience to internet users. The Cloud Service provides data centre connectivity to share the resources through Virtual Machines (VM). The performance issue in Virtual Machine (VM) allocation during eccentrics degrades data centres load and clogging degrades the Quality of Experience (QoE) with Cloud Data Centres. Proposed optimized virtual machine allocation policy eradicates eccentrics effect and clogging effect, as it provides uninterrupted access throughput for better QoE. An optimization flag is additionally added in the Service Level Agreement (SLA). During burst load time, VM expansion is done automatically using Hybrid Krill Herd (KH) re-optimization if the optimization flag is set as true. Change and Response Protocol and Agreement Protocol have been proposed to predict the optimal resources for the Virtual Machine to avoid eccentrics and clogging. The prime parameters like delay, latency, packet rate and throughput are monitored continuously to activate the re-optimization and for achieving QoE with minimized load. Experimental analysis proves the capability of Hybrid KH algorithm over Particle Swarm Optimization, Ant Colony Optimization, Genetic Algorithm and Simulated Annealing algorithms. © 2017 Elsevier Inc.","Cloud computing; Load balancing; Optimization; Quality of Experience; Virtual machine allocation policy"
"SDN-based energy management scheme for sustainability of data centers: An analysis on renewable energy sources and electric vehicles participation","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026837644&doi=10.1016%2fj.jpdc.2017.07.002&partnerID=40&md5=eb3099827784a477f3a2cd2ac485b583","Energy management is becoming one of the major issues from last many years due to an exponential increase in the smart device users for accessing various services from the geo-distributed cloud data centers (DCs) using Internet. During this time, there is an emergence of new technology called as cloud computing which provides on-demand pay-per-use services such as storage, computation, and network to the end users. These services are provided to the end users by various geographically located DCs which are hosted by different service providers such as—Microsoft, Amazon, IBM etc. With an increase in dependence of end users, DCs have expanded both in size and number. With such an expansion, these DCs consume huge amount of energy to accomplish their routine tasks. Such an increase in energy consumption generates a lot of load on the power grid. Moreover, the carbon emissions from these DCs has a global impact on the environment. To mitigate these issues, the integration of DCs with renewable energy sources (RES) can be helpful to reduce the carbon emissions and to ease the load on the power grid. For this purpose, a SDN-based energy management scheme for sustainability of DCs using RES is proposed in this paper.1 To achieve the aforementioned objectives, an energy-efficient flow scheduling algorithm is proposed using SDN. Moreover, a charging-discharging scheme for penetration of electric vehicles (EVs) is also presented to manage the intermittency of renewable energy. An energy trading and reward point scheme is designed to attract the EVs to participate in the proposed energy management scheme. The efficacy of the proposed scheme is proved using realistic weather traces. The results obtained clearly show the effectiveness of the proposed scheme for sustainability of DCs. © 2017 Elsevier Inc.","Data centers; Electric vehicles; Energy management; Renewable energy; Sustainability"
"Heterogeneous domain adaptation network based on autoencoder","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021996209&doi=10.1016%2fj.jpdc.2017.06.003&partnerID=40&md5=1bf1f5fa98234ca2a1b900cf453a4664","Heterogeneous domain adaptation is a more challenging problem than homogeneous domain adaptation. The transfer effect is not ideally caused by shallow structure which cannot adequately describe the probability distribution and obtain more effective features. In this paper, we propose a heterogeneous domain adaptation network based on autoencoder, in which two sets of autoencoder networks are used to project the source-domain and target-domain data to a shared feature space to obtain more abstractive feature representations. In the last feature and classification layer, the marginal and conditional distributions can be matched by empirical maximum mean discrepancy metric to reduce distribution difference. To preserve the consistency of geometric structure and label information, a manifold alignment term based on labels is introduced. The classification performance can be improved further by making full use of label information of both domains. The experimental results of 16 cross-domain transfer tasks verify that HDANA outperforms several state-of-the-art methods. © 2017 Elsevier Inc.","Autoencoder; Heterogeneous domain adaptation; Manifold alignment; Maximum mean discrepancy"
"Efficient and secure searchable encryption protocol for cloud-based Internet of Things","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029392185&doi=10.1016%2fj.jpdc.2017.08.007&partnerID=40&md5=53eba029b5a4efa1dd3c09a1ab7682eb","Internet of things (IoT) applications comprising thousands or millions of intelligent devices or things is fast becoming a norm in our inter-connected world, and the significant amount of data generated from IoT applications is often stored in the cloud. However, searching encrypted data (i.e. Searchable Encryption—SE) in the cloud remains an ongoing challenge. Existing SE protocols include searchable symmetric encryption (SSE) and public-key encryption with keyword search (PEKS). Limitations of SSE include complex and expensive key management and distribution, while PEKS suffer from inefficiency and are vulnerable to insider keyword guessing attacks (KGA). Besides, most protocols are insecure against file-injection attacks carried out by a malicious server. Thus, in this paper, we propose an efficient and secure searchable encryption protocol using the trapdoor permutation function (TPF). The protocol is designed for cloud-based IoT (also referred to as Cloud of Things – CoT) deployment, such as Cloud of Battlefield Things and Cloud of Military Things. Compared with other existing SE protocols, our proposed SE protocol incurs lower computation cost at the expense of a slightly higher storage cost (which is less of an issue, considering the decreasing costs of storage). We also prove that our protocol achieves inside KGA resilience, forward privacy, and file-injection attack resilience. © 2017 Elsevier Inc.","Cloud-of-Things; File-injection attack resilience; Forward privacy; Insider keyword guessing attack resilience; Internet of Things; Searchable encryption"
"A power-aware 2-covered path routing for wireless body area networks with variable transmission ranges","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031806213&doi=10.1016%2fj.jpdc.2017.08.006&partnerID=40&md5=b7c7d0606e7d2b07df8419af7894b23a","Wireless body area networks (WBAN) are an emerging form of technology which provides a base for various implantable and wearable sensors. This paper presents a 2-covered path routing that provides each WBAN along a path sheltered from an intersectional coverage of at least two WBAN hubs to provide high availability and reliable multi-hop communication in WBANs. In radio frequency (RF) systems, the power consumption required to transmit data grows at least quadratic times as its transmission range. Therefore, this paper studies the 2-covered path problem in which antennas have discrete transmission power levels. Given a set of n antennas with m available radii, a source and a sink on the plane, we propose three power-aware methods to construct 2-covered paths between source and sink. The proposed methods are graph transformation planning (GTP), 2-covered area stretching planning (TASP) and radii shrinking planning (RSP) which apply different strategies to reduce overall power consumption of the network. Experiments show that GTP obtains the maximum power saving while RSP and TASP are polynomial-time algorithms and save power up to 96% of those achieved by GTP. © 2017 Elsevier Inc.","2-covered path problem; Fault tolerance; Power-aware routing; Variable radii; Wireless body area networks"
"PAME: Evolutionary membrane computing for virtual network embedding","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029371141&doi=10.1016%2fj.jpdc.2017.08.005&partnerID=40&md5=92303f04e1df52cb04abb153af2497e3","Virtual network embedding is an NP-hard online problem, partially due to diversity effects during mapping. Mapping diversity is critical not only for single virtual network but also for a set of online virtual networks. Two common ways, using heuristic information or using extra constraints, are employed to reduce the problem of hardness. Both restrict diversity during mapping. A preferable technique is executing parallel scatter mapping in the solution space, which can promote mapping quality while maintaining diversity. This technique is challenging due to a nested paralleling. Using membrane computing, we designed a P system with active non-elementary membranes (PAME). With a specific membrane structure, PAME achieves nested paralleling via a dual-parallel mapping stage. The stage couples an inter-increment parallel with an intra-increment one, relying on non-elementary membrane self-division and elementary membrane bootstrap. Simulation experiments showed that PAME outperformed existing algorithms in long-term average revenue, acceptance ratio, and long-term revenue-to-cost ratio. © 2017 Elsevier Inc.","Dual parallel; Mapping diversity; Membrane computing; P system; Virtual network embedding"
"Socially-conforming cooperative computation in cloud networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021716414&doi=10.1016%2fj.jpdc.2017.06.006&partnerID=40&md5=efd1af64f000f81988903adbc1464263","In the context of two-party computation, such as file exchange or contract signing, the security property of fairness is of great importance. After the impossibility result of Cleve for achieving complete fairness in general, recent research proposes to circumvent such impossibility by assuming that parties behave as rational players of a game. However, such works involve rational parties as independent individuals without considering the impact of the environment. In our work, we apply the notion of social conformity to show that under certain assumptions rational parties belonging to a cloud will choose to cooperate. Then, we simulate a real setting to show that party in fact cooperates and achieves fairness. Finally we discuss a general model to describe the connection between mutual cooperation in social conformity and fairness in secure two-party computation. © 2017 Elsevier Inc.","Cloud cooperation; Fairness; Game theory; Social conformity"
"Prediction based opportunistic routing for maritime search and rescue wireless sensor network","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032349025&doi=10.1016%2fj.jpdc.2017.06.021&partnerID=40&md5=fecc4cf7e5330207115ca1484582c998","In recent years, maritime and air crashes occur frequently. The existing rescue measures rely only on board satellite communications equipment, which makes it difficult to confirm the accurate positioning information and vital signs of drowning people. Recently, wireless sensor networks (WSN) are introduced to Maritime Search and Rescue (MSR). WSNs feature quick expansion, self-organization, and self-adaptation to the marine environment. However, the constant changing nodes location and link reliability in marine search and rescue WSN makes the routing metrics between nodes highly dynamic. Traditional routing protocols such as AODV that establish a fixed single route based on static nodes information will provide poor packet delivery rate and take no consideration of the limited energy on the irreplaceable WSN nodes. We propose to employ opportunistic routing which can make best use of the broadcast property of radio propagation. The forwarding decisions in opportunistic routing are only based on its neighbor's information. No network-wide flooding is required to establish routes. In order to maintain the latest neighbor information and minimize the energy cost of collecting these information, we propose a light-weight time series based routing metric prediction method to deal with the high communication cost incurred by collecting the latest routing metrics between nodes. Results: Our implementation of opportunistic routing protocol achieved 30% more Packet Delivery Ratio compared to the traditional AODV protocol. Also opportunistic routing protocol with prediction performed slightly better than opportunistic routing protocol without prediction. Our approach generated 90% efficiency where as 60% efficiency was achieved using AODV protocol. In achieving this an additional 3% energy is consumed by the nodes. We feel additional 3% energy consumption to improve delivery greatly by 30% is a good tradeoff. © 2017 Elsevier Inc.","Complex network theory; Maritime search and rescue; Opportunistic routing protocol; Topology control; Wireless sensor network"
"LXCloud-CR: Towards LinuX Containers Distributed Hash Table based Checkpoint-Restart","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030485147&doi=10.1016%2fj.jpdc.2017.08.011&partnerID=40&md5=7907ceffdf1c9478260abff600314c0d","Infrastructure-as-a-Service (IaaS) container-based virtualization technology is gaining, over these years, significant interest in industry as an alternative platform for running distributed applications. They present an interesting alternative to virtual machines in the Cloud due to newer advances in container-based virtualization as a new technology that simplify the deployment of applications. As the cloud architectures continue to grow in scale and complexity, faults become very recurrent which make reliability a true challenge. Given the dynamic nature of IaaS clouds and the pay-as-you-go cloud model where the costs are directly proportional to the resource usage, a Checkpoint Restart mechanism is essential in this context. We propose LXCloud-CR our new decentralized Checkpoint-Restart model based on a distributed checkpoints repository using key–value store on a Distributed Hash Table (DHT). It is able to take snapshots of the whole Linux Container (LXC) instances. LXCloud-CR aims to reduce the runtime and storage overheads of checkpointing. Large scale experiments on the Grid’5000 testbed demonstrate the benefits of our proposal. Obtained results validate our model and improve the performance of applications. © 2017 Elsevier Inc.","Checkpoint-Restart; Cloud computing; Container; DHT; Fault tolerance; Grid’5000; IaaS; Migration; Virtualization"
"A contention adapting approach to concurrent ordered sets","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041473172&doi=10.1016%2fj.jpdc.2017.11.007&partnerID=40&md5=f3d996c8df24de6df7746dadbd4f9553","With multicores being ubiquitous, concurrent data structures are increasingly important. This article proposes a novel approach to concurrent data structure design where the data structure dynamically adapts its synchronization granularity based on the detected contention and the amount of data that operations are accessing. This approach not only has the potential to reduce overheads associated with synchronization in uncontended scenarios, but can also be beneficial when the amount of data that operations are accessing atomically is unknown. Using this adaptive approach we create a contention adapting search tree (CA tree) that can be used to implement concurrent ordered sets and maps with support for range queries and bulk operations. We provide detailed proof sketches for the linearizability as well as deadlock and livelock freedom of CA tree operations. We experimentally compare CA trees to state-of-the-art concurrent data structures and show that CA trees beat the best of the data structures that we compare against by over 50% in scenarios that contain basic set operations and range queries, outperform them by more than 1200% in scenarios that also contain range updates, and offer performance and scalability that is better than many of them on workloads that only contain basic set operations. © 2017 Elsevier Inc.","Concurrent data structures; Linearizability; Ordered sets; Range queries"
"Password-based protection of clustered segments in distributed memory systems","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042180413&doi=10.1016%2fj.jpdc.2018.01.003&partnerID=40&md5=f67f89f7a257586fed0e21f445670c72","With reference to a distributed system consisting of nodes connected by a local area network, we consider the problems related to the distribution, verification, review and revocation of access permissions. We propose the organization of a protection system that takes advantage of a form of protected pointer, the handle, to reference clusters of segments allocated in the same node. A handle is expressed in terms of a selector and a password. The selector specifies the segments, the password specifies an access right, read or write. Two primary passwords are associated with each cluster, corresponding to an access permission for all the segments in that cluster. A handle weakening algorithm takes advantage of a parametric one-way function to generate secondary passwords corresponding to less segments. A small set of protection primitives makes it possible to allocate and delete segments in active clusters, and to use handles to access remote segments both to read and to write. The resulting protection environment is evaluated from a number of viewpoints, which include handle forging, review and revocation, the memory costs for handle storage, the execution times for handle validation and the network traffic generated by the execution of the protection primitives. An indication of the flexibility of the handle concept is given by applying handles to the solution of a variety of protection problems. © 2018 Elsevier Inc.","Access right; Distributed system; Parametric one-way function; Protection; Revocation; Segment"
"Feasible enhancements to congestion control in InfiniBand-based networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032261386&doi=10.1016%2fj.jpdc.2017.09.008&partnerID=40&md5=4fe2f497d251b19c833cd13beabf5fff","The interconnection network architecture is crucial for High-Performance Computing (HPC) clusters, since it must meet the increasing computing demands of applications. Current trends in the design of these networks are based on increasing link speed, while reducing latency and number of components in order to lower the cost. The InfiniBand Architecture (IBA) is an example of a powerful interconnect technology, delivering huge amounts of information in few microseconds. The IBA-based hardware is able to deliver EDR and HDR speed (i.e. 100 and 200 Gb/s, respectively). Unfortunately, congestion situations and their derived problems (i.e. Head-of-Line blocking and buffer hogging), are a serious threat for the performance of both the interconnection network and the entire HPC cluster. In this paper, we propose a new approach to provide IBA-based networks with techniques for reducing the congestion problems. We propose Flow2SL-ITh, a technique that combines a static queuing scheme (SQS) with the closed-loop congestion control mechanism included in IBA-based hardware (a.k.a. injection throttling, ITh). Flow2SL-ITh separates traffic flows storing them in different virtual lanes (VLs), in order to reduce HoL blocking, while the injection rate of congested flows is throttled. Meanwhile congested traffic vanishes, there is no buffer sharing among traffic flows stored in different VLs, which reduces congestion negative effects. We have implemented Flow2SL-ITh in OpenSM, the open-source implementation of the IBA subnet manager (SM). Experimental results obtained by running simulations and real workloads in a small IBA cluster show that Flow2SL-ITh outperforms existing techniques by up to 44%, under some traffic scenarios. © 2017 Elsevier Inc.","Congestion management; High-Performance Computing; InfiniBand; Interconnection networks; OpenFabrics Software (OFS); Virtual lanes"
"Concurrent use of write-once memory","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039724552&doi=10.1016%2fj.jpdc.2017.12.001&partnerID=40&md5=c819f2e948de22a9809f98aedd925c90","We consider the problem of implementing general shared-memory objects on top of write-once bits, which can be changed from 0 to 1 but not back again. In a sequential setting, write-once memory (WOM) codes have been developed that allow simulating memory that support multiple writes, even of large values, setting an average of 1+o(1) write-once bits per write. We show that similar space efficiencies can be obtained in a concurrent setting, though at the cost of high time complexity and fixed bound on the number of write operations. As an alternative, we give an implementation that permits unboundedly many writes and has much better amortized time complexity, but at the cost of unbounded space complexity. Whether one can obtain both low time complexity and low space complexity in the same implementation remains open. © 2017 Elsevier Inc.","Concurrent algorithms; Space complexity; Write-once memory"
"Tuning synthesis flags to optimize implementation goals: Performance and robustness of the LEON3 processor as a case study","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032806695&doi=10.1016%2fj.jpdc.2017.10.002&partnerID=40&md5=599d79fb604661e3474f21bf3eec18ff","The steady growth in complexity of FPGAs has led designers to rely more and more on manufacturers’ and third parties’ design tools to meet their implementation goals. However, as modern synthesis tools provide a myriad of different optimization flags, whose contribution towards each implementation goal is not clearly accounted for, designers just make use of a handful of those flags. This paper addresses the challenging problem of determining the best configuration of available synthesis flags to optimize the designer's implementation goals. First, fractional factorial design is used to reduce the whole design space. Resulting configurations are implemented to estimate the actual impact, and statistical significance, of each considered synthesis flag. After that, multiple regression analysis techniques predict the expected outcome for each possible combination of these flags. Finally, multiple-criteria decision making techniques enable the selection of the best set of synthesis flags according to explicitly defined implementation goals. © 2017 Elsevier Inc.","Design space exploration; Implementation goals optimization; Multiple regression analysis; Multiple-criteria decision making; Synthesis options"
"GPUDirect Async: Exploring GPU synchronous communication techniques for InfiniBand clusters","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044868049&doi=10.1016%2fj.jpdc.2017.12.007&partnerID=40&md5=28c3f4acbe83ee0897f90e80b4bc45f0","NVIDIA GPUDirect is a family of technologies aimed at optimizing data movement among GPUs (P2P) or among GPUs and third-party devices (RDMA). GPUDirect Async, introduced in CUDA 8.0, is a new addition which allows direct synchronization between GPU and third party devices. For example, Async allows an NVIDIA GPU to directly trigger and poll for completion of communication operations queued to an InfiniBand Connect-IB network adapter, with no involvement of CPU in the critical communication path of GPU applications. In this paper we describe the motivations and the building blocks of GPUDirect Async. After an initial analysis with a micro-benchmark, by means of a performance model, we show the potential benefits of using two different asynchronous communication models supported by this new technology in two MPI multi-GPU applications: HPGMG-FV, a proxy for real-world geometric multi-grid applications and CoMD-CUDA, a proxy for Classical Molecular Dynamics codes. We also report a test case in which the use of GPUDirect Async does not provide any advantage, that is an implementation of the Breadth First Search algorithm for large scale graphs. © 2017 Elsevier Inc.","Asynchronous communication models; CUDA 8.0; GPUDirect Async; InfiniBand"
"Rate-based thermal, power, and co-location aware resource management for heterogeneous data centers","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020408666&doi=10.1016%2fj.jpdc.2017.04.015&partnerID=40&md5=c633f1839e8da5e33362bf4e919cd7a5","Today's data centers contain large numbers of compute nodes that require substantial power, and therefore require a large amount of cooling resources to operate at a reliable temperature. The high power consumption of the computing and cooling systems produces extraordinary electricity costs, requiring some data center operators to be constrained by a specified electricity budget. In addition, the processors within these systems contain a large number of cores with shared resources (e.g., last-level cache), heavily affecting the performance of tasks that are co-located on cores and contend for these resources. This problem is only exacerbated as processors move to the many-core realm. These issues lead to interesting performance-power tradeoffs; by considering resource management in a holistic fashion, the performance of the computing system can be maximized while satisfying power and temperature constraints. In this work, the performance of the system is quantified as the total reward earned from completing tasks by their individual deadlines. By designing three resource allocation techniques, we perform a rigorous analysis on thermal, power, and co-location aware resource management using two different facility configurations, three different workload environments, and a sensitivity analysis of the power and thermal constraints. © 2017 Elsevier Inc.","DVFS; Heterogeneous computing; Memory interference; Power-aware computing; Resource management; Thermal-aware computing"
"Hybrid multi-core CPU and GPU-based B&B approaches for the blocking job shop scheduling problem","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043354270&doi=10.1016%2fj.jpdc.2018.02.005&partnerID=40&md5=03a5b3bb8d93c8f141fa8b7485fd076c","The Branch and Bound algorithm (B&B) is a well known method for solving optimally Combinatorial Optimization Problems. This method is based on intelligent enumeration of all feasible solutions which reduce considerably the search space. Nevertheless, it remains inefficient when using the sequential approach to deal with large problem instances due to its huge resolutions time. However, the execution time can be reduced considerably by using parallel computing architectures. With the huge evolution of the multi-cores CPUs and GPUs, it is quite hard to design schemes that efficiently exploit the different hardware architectures simultaneously. As a result, most of the existing works focus on exploiting one hardware architecture at a time. In this paper, we propose five parallel approaches to accelerate the B&B execution time using Multi and Many-core systems. Our goal is to solve optimally the Blocking Job Shop Scheduling problem (BJSS) which is one of the hardest scheduling problem. The first proposed approach is a multi-search parallelization based on master/worker paradigm, exploiting the multi-Core CPU-processors. The second and the third approaches represent a GPU-based parallelization schemes having different level of parallelism and GPU occupancy. The fourth and fifth approaches represent a hybridization between the Multi-core approach and the GPU-based parallelization approaches. The goal of this hybridization is to benefit from the power of both the CPU-cores and the GPU at the same time. This hybridization is based on concurrent kernels execution provided by Nvidia Multi process Service (MPS) that allows multiple host processes (Master and workers) to use simultaneously the GPU to launch their kernels in order to accelerate the bounding of one or several nodes at a time. Experiments using the well known Taillard instances confirm the efficiency of our proposals and show a relative speedup of 160x as compared to an optimized sequential B&B algorithm. © 2018 Elsevier Inc.","Hybrid multi-core CPU and GPU B&B; Job shop scheduling; Nvidia MPS; Parallel B&B algorithm"
"Enhanced dynamic programming approach for subunit modelling to handle segmentation and recognition ambiguities in sign language","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026765786&doi=10.1016%2fj.jpdc.2017.07.001&partnerID=40&md5=6e0cccc24739cb1ce24c857a8c785e0a","Sign language serves as a primary means of communication among the deaf impaired community. The major challenges faced by the Sign Language Recognition (SLR) system are recognizing signs from large vocabularies in continuous video sequences. In this research paper, a novel subunit sign modelling framework is proposed for vision-based SLR which aims in solving the major issues in SLR systems. The problem of hand segmentation ambiguities and segregating epentheses movements between two adjacent signs in continuous video sequences are addressed. A novel subunit sign modelling framework is presented and illustrated to embark upon these problems while considering large-vocabularies. This framework is developed using a novel methodology of Enhanced Dynamic Programming (EDP) approach in subunit sign modelling. This EDP framework works with a combination of dynamic time warping and spatiotemporal clustering techniques. Since, sign language consists of both spatial and temporal feature vectors, dynamic time warping is used as a distance measure to compute the distance between two adjacent signs in sign trajectories. This distance is used as a temporal feature vector during the clustering of spatial feature vectors using Minimum Entropy Clustering (MEC). This process is done recursively to cluster all the epentheses movements dynamically without using any explicit or implicit modelling. Experimental results have confirmed that the computation cost of the proposed system is less because the epenthesis movements are eliminated before classification and the gesture base space utilized by the sign gestures is very low because the proposed system does not require any modelling to handle epenthesis movements. The results obtained from the proposed subunit sign modelling framework is compared with other existing models in order to prove that the proposed system is best among the existing systems. © 2017 Elsevier Inc.","Epentheses movements; Sign language recognition; Spatiotemporal clustering; Subunit sign modelling"
"Scaling of a Fast Fourier Transform and a pseudo-spectral fluid solver up to 196608 cores","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034626510&doi=10.1016%2fj.jpdc.2017.10.014&partnerID=40&md5=e930e63df8d043f46668883da683d818","In this paper we present scaling results of a FFT library, FFTK, and a pseudospectral code, Tarang, on grid resolutions up to 81923 grid using 65536 cores of Blue Gene/P and 196608 cores of Cray XC40 supercomputers. We observe that communication dominates computation, more so on the Cray XC40. The computation time scales as Tcomp∼p−1, and the communication time as Tcomm∼n−γ with γ2 ranging from 0.7 to 0.9 for Blue Gene/P, and from 0.43 to 0.73 for Cray XC40. FFTK, and the fluid and convection solvers of Tarang exhibit weak as well as strong scaling nearly up to 196608 cores of Cray XC40. We perform a comparative study of the performance on the Blue Gene/P and Cray XC40 clusters. © 2017 Elsevier Inc.","Extreme-resolution turbulence simulation; Fast Fourier transform; Pseudospectral method; Turbulence simulation"
"A location Prediction-based routing scheme for opportunistic networks in an IoT scenario","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029688459&doi=10.1016%2fj.jpdc.2017.08.008&partnerID=40&md5=555fcc207184380203d08bb6badff52f","Opportunistic Internet of Things (OppIoT) is a paradigm, technology, and system that promotes the opportunistic exploitation of interactions between IoT devices to achieve increased connectivity, reliability, network capacity, and overall network lifetime. The increased demand for identifying such opportunistic exploitation is illustrated by IoT scenarios, where the goal is to recognize when an opportunity for communication is possible, thereby allowing for data forwarding and routing. In an OppIoT system, devising a routing scheme is a challenging task due to the difficulty in guaranteeing the existence of connectivity between devices (nodes) and in identifying an intermediate node as a packet forwarder towards its destination. Considering that opportunistic networks (oppNets) are a subclass of OppIoT and considering IoT scenarios where the opportunistic exploitation of IoT devices is possible even in case the device's presence is uncertain or may change over time, this paper proposes a novel routing scheme for OppNets (called Location Prediction-based Forwarding for Routing using Markov Chain (LPFR-MC)) that can also be used in IoT scenarios. The proposed LPFR-MC scheme considers the node's present location and the angle formed by it and the corresponding source (resp. destination) to predict the node's next location or region using a Markov chain and to determine the probability of a node moving towards the destination. Simulation results are provided, showing that the proposed LPFR-MC outperforms the existing traditional protocols in terms of message delivery probability, hop count, number of messages dropped, message overhead ratio, and average buffer time. © 2017 Elsevier Inc.","Delay-tolerant networks (DTN); Epidemic; HBPR; Markov chain; Opportunistic IoT systems; Opportunistic networks (OppNets); Prophet; ProWait"
"Parallel adaptive mesh refinement method based on bubble-type local mesh generation","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2018.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043356203&doi=10.1016%2fj.jpdc.2018.02.008&partnerID=40&md5=911e1477959500830f51981c5a485d9f","An efficient parallel adaptive mesh refinement method based on bubble-type local mesh generation (BLMG), employing ParMETIS-based dynamic domain decomposition method, is developed. The BLMG method is applied to generate the mesh with high quality, and the local mesh for each processor can be generated simultaneously without communication. The node-based distributed mesh structure is designed to reduce the communication amount spent in mesh generation and finite element calculation. To gain the load balance, a new load balancing algorithm for the new mesh structure is devised to make sure the whole algorithm is efficient. Several numerical examples are carried out to verify the high efficiency of the algorithm. © 2018 Elsevier Inc.","Adaptive mesh refinement; Distributed mesh structure; Local mesh generation; Mesh migration; Node-based finite element method"
"A parallel metaheuristic data clustering framework for cloud","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036648237&doi=10.1016%2fj.jpdc.2017.10.020&partnerID=40&md5=ae23b13de247d2e6ec31b36c5b276a5f","A high performance data analytics for internet of things (IoT) has been a promising research subject in recent years because traditional data mining algorithms may not be applicable to big data of IoT. One of the main reasons is that the data that need to be analyzed may exceed the storage size of a single machine. The computation cost of data analysis tasks that is too high for a single computer system is another critical problem we have to confront when analyzing data from an IoT system. That is why an efficient data clustering framework for metaheuristic algorithm on a cloud computing environment is presented in this paper for data analytics, which explains how to divide mining tasks of a mining algorithm into different nodes (i.e., the Map process) and then aggregate the mining results from these nodes (i.e., Reduce process). We further attempted to use the proposed framework to implement data clustering algorithms (e.g., k-means, genetic k-means, and particle swarm optimization) on a standalone system and Spark. The experimental results show that the performance of the proposed framework makes it useful to develop data clustering algorithms on a cloud computing environment. © 2017 Elsevier Inc.","Data clustering problem; Internet of things; Metaheuristic algorithm"
"Achieving verifiable, dynamic and efficient auditing for outsourced database in cloud","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034036164&doi=10.1016%2fj.jpdc.2017.10.004&partnerID=40&md5=03b66dec8b85c3eebf94ff3e98c5502b","With the help of cloud computing, users can outsource their data to a cloud service provider (CSP) and enjoy the high-quality on-demand services from the cloud. On the other hand, once owners no longer have physical possession of the outsourced data, the protection of data integrity in cloud computing becomes a critical and challenging task. In this paper, we propose a novel and verifiable auditing scheme for outsourced database without a third party auditor (TPA). It can simultaneously authenticate the correctness and completeness of query results, preventing the dishonest CSP from returning fake or incomplete results to the users. Our scheme also supports partial attribute retrieval and flexible data dynamics. The security proof and the performance analysis show that our proposed scheme is secure and efficient for practical deployment. © 2017 Elsevier Inc.","Cloud computing; Database outsourcing; Integrity auditing; Outsourcing security"
"Towards understanding HPC users and systems: A NERSC case study","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030328478&doi=10.1016%2fj.jpdc.2017.09.002&partnerID=40&md5=d3d78af6517248fd398b072c2dc29faf","High performance computing (HPC) scheduling landscape currently faces new challenges due to the changes in the workload. Previously, HPC centers were dominated by tightly coupled MPI jobs. HPC workloads increasingly include high-throughput, data-intensive, and stream-processing applications. As a consequence, workloads are becoming more diverse at both application and job levels, posing new challenges to classical HPC schedulers. There is a need to understand the current HPC workloads and their evolution to facilitate informed future scheduling research and enable efficient scheduling in future HPC systems. In this paper, we present a methodology to characterize workloads and assess their heterogeneity, at a particular time period and its evolution over time. We apply this methodology to the workloads of three systems (Hopper, Edison, and Carver) at the National Energy Research Scientific Computing Center (NERSC). We present the resulting characterization of jobs, queues, heterogeneity, and performance that includes detailed information of a year of workload (2014) and evolution through the systems’ lifetime (2010–2014). © 2017 Elsevier Inc.","Heterogeneity; HPC; k-means; NERSC; Scheduling; Supercomputer; Workload analysis"
"Advanced services for efficient management of smart farms","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035311659&doi=10.1016%2fj.jpdc.2017.10.017&partnerID=40&md5=9020c446bd80ea902523ca812ddbbd00","Lately, the technology has been developed quickly causing the appearance of smart software products in many domains such as agriculture which are designed to ease people's work. Due to bad weather phenomena and growing demand for agricultural products, have started to appear a lot of greenhouses, where there is a strict control around each parameter, by sensors, contributing to the increase of the production. This paper presents a smart platform which helps farmers to efficiently manage their greenhouses and to interact with other farmers. We present the software architecture and each implemented service. We describe the general implementation details and then we present the data gathering algorithm. This algorithm is one of the most important features of the platform because it deals with sending notifications to users in case of a problem in greenhouses. It also offers data for processing to other services like the statistics service. Performance tests show the necessary time for gathering a large amount of data and for their processing. In addition, we will analyze the time needed to send notifications to users. © 2017 Elsevier Inc.","Distributed systems; Internet of data; Internet of things; Smart farming"
"A system for hardware aided decision tree ensemble evolution","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032742391&doi=10.1016%2fj.jpdc.2017.10.001&partnerID=40&md5=df51bd5f5954b8339ff48111c5ae013c","In this paper a system for hardware-aided induction of decision tree ensembles using the evolutionary approach (Decision Tree Ensemble Evolution co-Processor—DTEEP) is proposed. DTEEP is used for hardware acceleration of the fitness evaluation, since it is shown that most of the ensemble inference time is spent on this task. The DTEEP co-processor can significantly improve execution time of an algorithm for a full tree evolutionary induction of the decision tree ensembles (An Evolutionary Ensemble of Full Trees Induction—EEFTI) by accelerating the fitness evaluation task. Comparing the HW/SW EEFTI implementation with the pure software implementation suggests that the proposed HW/SW architecture offers substantial speedups for all the tests performed on the selected UCI (University of California, Irvine) machine learning repository datasets. © 2017 Elsevier Inc.","Co-processor; Data mining; Decision trees; Ensemble classifiers; Evolutionary algorithms; FPGA; Hardware acceleration; Hardware–software co-design; Machine learning"
"Incorporating both qualitative and quantitative preferences for service recommendation","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040310324&doi=10.1016%2fj.jpdc.2017.12.005&partnerID=40&md5=6429c660c03ef7b1791eae88c8c79444","The aim of service recommendation is to help a service user find an optimal service. Preference-based service recommendation has gained significant popularity in recent years. Existing studies primarily focus on using similarity measures for quantitative preference. None of them has considered both the quantitative and qualitative preference simultaneously. In this paper, we propose to integrate both the quantitative and qualitative preference to calculate the user similarity. We then seek similar users by using user similarity. Finally, service recommendation is performed using the preference information of those similar users based on the idea of collaborative filtering. A series of experiments have been conducted to validate the correctness of our method and comparison with existing approaches demonstrates the effectiveness of our method. © 2017 Elsevier Inc.","Collaborative filtering; Qualitative preference; Quantitative preference; Service recommendation; Similar user"
"An efficient data exchange mechanism for chained network functions","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040028371&doi=10.1016%2fj.jpdc.2017.12.003&partnerID=40&md5=85ea39b75b3404809914d75f2db3020b","Thanks to the increasing success of virtualization technologies and processing capabilities of computing devices, the deployment of virtual network functions is evolving towards a unified approach aiming at concentrating a huge amount of such functions within a limited number of commodity servers. To keep pace with this trend, a key issue to address is the definition of a secure and efficient way to move data between the different virtualized environments hosting the functions and a centralized component that builds the function chains within a single server. This paper proposes an efficient algorithm that realizes this vision and that, by exploiting the peculiarities of this application domain, is more efficient than classical solutions. The algorithm that manages the data exchanges is validated by performing a formal verification of its main safety and security properties, and an extensive functional and performance evaluation is presented. © 2017 Elsevier Inc.","Data exchange mechanism; High speed packet processing; Network function virtualization; Parallel algorithms"
"TT-XSS: A novel taint tracking based dynamic detection framework for DOM Cross-Site Scripting","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028664204&doi=10.1016%2fj.jpdc.2017.07.006&partnerID=40&md5=993906b2b6afc9986df45f74219256db","Most work on DOM Cross-Site Scripting (DOM-XSS) detection methods can be divided into three kinds: black-box fuzzing, static analysis, and dynamic analysis. However, black-box fuzzing and static analysis suffer much from high false negative rates and high false positive rates respectively. Current dynamic analysis is complex and expensive, though it can obtain more efficient results. In this paper, we propose a dynamic detection framework (TT-XSS) for DOM-XSS by means of taint tracking at client side. We rewrite all JavaScript features and DOM APIs to taint the rendering process of browsers. To this end, new data types and methods are presented to extend the semantic description ability of the original data structure, based on which we can analyze the taint traces through tainting all sources, sinks and transfer processes during pages parsing. In this way, attack vectors are derived to verify the vulnerabilities automatically. Compared to AWVS 10.0, our framework detects more 1.8% vulnerabilities, and it can generate the corresponding attack vectors to verify 9.1% vulnerabilities automatically. © 2017 Elsevier Inc.","DOM Cross-Site Scripting; Dynamic analysis; Static analysis; Vulnerabilities"
"High-throughput Ant Colony Optimization on graphics processing units","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039856989&doi=10.1016%2fj.jpdc.2017.12.002&partnerID=40&md5=2f9e0c5cd4d38920b9b97f4804b49f5e","Nowadays, computer researchers can face ever-complex scientific problems by using a hardware and software co-design. One successful approach is exploring novel massively-parallel Natural-inspired algorithms, such as the Ant Colony Optimization (ACO) algorithm, through the exploitation of high-throughput accelerators such as GPUs, which are designed to provide high levels of parallelism and low Energy per instruction (EP) cost through heavy vectorization. In this paper, we demonstrate how to take advantage of contemporary hardware-based CUDA vectorization to optimize the ACO algorithm when applied to the Traveling Salesman Problem (TSP). Several parallel designs are proposed and analyzed on two different CUDA architectures. Our results reveal that our vectorization approaches can obtain good performance on these architectures. Moreover, atomic operations are under study showing good benefits on latest generations of CUDA architectures. This work lays the groundwork for future developments of ACO algorithm on high-performance platforms. © 2017 Elsevier Inc.","ACO; Agnostic vectorization; Atomic operations; GPUs; TSP"
"Internet of agents framework for connected vehicles: A case study on distributed traffic control system","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.10.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034582272&doi=10.1016%2fj.jpdc.2017.10.019&partnerID=40&md5=3ddd7f3a663b882e90110aa8de17f6a7","This study focuses on the distributed traffic control system by inspiration of advanced connected vehicle technology. In this regard, we introduce an Internet of Agents (IoA) framework for connected vehicles where agents make their own decisions to improve the effectiveness of the system by connectivity and automatic negotiation with other agents. Specifically, each of the connected vehicles can be regarded as an agent which is able to communicate and collaborate with others based on Vehicle-to-Vehicle (V2V) communication technologies. A case study on distributed traffic control system without traffic signal is presented in this paper. In particular, we consider traffic control at intersection problem as a group mutual exclusion problem where only connected vehicles in non-conflict relationship are able to enter the core of intersection simultaneously. Therefore, we extend the Ricart–Agrawala based-logical clock algorithm to deal with this problem. Various parameters (e.g., number of message exchange, average waiting time, total number of vehicle passing) have been measured to evaluate our approach. The simulated results show that our approach outperforms compared with existing traffic systems and previous works. © 2017 Elsevier Inc.","Connected vehicle; Distributed traffic control; Group mutual exclusion; Internet of Agents; V2V communication"
"An intelligent decision computing paradigm for crowd monitoring in the smart city","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016477738&doi=10.1016%2fj.jpdc.2017.03.002&partnerID=40&md5=b79ae1181912f1cf92af437e6449ca56","The ever-expanding urbanization and the advent of smart cities need better crowd management and security surveillance systems. Advanced systems are required to improve and automate the crowd management system. The aim of the closed circuit television and visual monitoring systems using multiple cameras faces many challenges like illumination variance, occlusion and small spatial–temporal resolution, person in sleep, shadows, dynamic backgrounds, and noises. Therefore, the crowd monitoring, prevention of stampedes and crowd-related emergencies in the smart cities are major challenging problems. In this paper, we propose an intelligent decision computing based paradigm for crowd monitoring in the smart city. In the intelligent computing based framework, the optimization algorithm is applied to compute the feature of crowd motion and measure the correlation between agents based motion model and the crowd data using extended Kalman filtering approach and KL-divergence technique. The proposed framework measures the correlation measure based on extracted novel distinctive feature, and holistic feature of crowd data represent and to classify the crowd motion of individual. Our experimental results demonstrate that the proposed approach yields 96.20% average precision in classifying real-world highly dense crowd scenes. © 2017 Elsevier Inc.","Agent motion model; Computer vision; Crowd monitoring; Crowd motion; K-NN; KL-divergence; SIFT; Smart city"
"AccessAuth: Capacity-aware security access authentication in federated-IoT-enabled V2G networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031690586&doi=10.1016%2fj.jpdc.2017.09.004&partnerID=40&md5=7bcfb807cf033c6f66d04ec223b2f5bf","Vehicle-to-Grid (V2G) systems promoted by the federated Internet of Things (IoT) technology will be ubiquitous in the future; therefore, it is crucial to provide trusted, flexible and efficient operations for V2G services using high-quality measures for security and privacy. These can be achieved by access and authority authentication. This paper presents a lightweight protocol for capacity-based security access authentication named AccessAuth. Considering the overload probability and system capacity constraints of the V2G network domain, as well as the mobility of electric vehicles, the ideal number of admissible access requests is first calculated adaptively for each V2G network domain to actively achieve capacity-based access admission control. Subsequently, to provide mutual authentication and maintain the data privacy of admitted sessions, by considering whether there is prior knowledge of the trust relationship between the relevant V2G network domains, a high-level authentication model with specific authentication procedures is presented to enforce strict access authentication such that the sessions are conducted only by authorized requesters. Additionally, efficient session revocation with forward security and session recovery with no extra authentication delay are also discussed. Finally, analytical and evaluation results are presented to demonstrate the performance of AccessAuth. © 2017 Elsevier Inc.","Authentication; Capacity; Security; V2G"
"REU Site: Bio-Grid Initiatives for interdisciplinary research and education","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010955743&doi=10.1016%2fj.jpdc.2017.01.012&partnerID=40&md5=1bd50e7f843e3f95ff06ad6f7829bfa5","Recently more and more universities have been incorporating HPC (High Performance Computing) in their computing curriculum. The Bio-Grid REU (Research Experience for Undergraduates) Site offers undergraduate students interested or experienced in HPC a summer research opportunity to participate in projects that apply HPC in various life-science disciplines. The projects are associated with the Bio-Grid Initiatives conducted at the University of Connecticut. Training seminars are designed to equip students with background knowledge such as basic parallel programming, large-scale data analytics, and middleware support, etc., as well as some ongoing projects using these computing methods. Students participate in several collaborative projects supported by a campus-wide computational and data grid. The REU project introduces such interdisciplinary research work to students in the early stage of their academic career to spark their interest. The project aims at preparing future software engineers to formalize and solve emerging life-science problems, as well as life-science researchers with a strong background in high-performance computing. The Bio-Grid REU Site was supported by the National Science Foundation from 08–10 and 12–14, with a website located at http://biogrid.engr.uconn.edu/REU. © 2017 Elsevier Inc.","Computational biology; Grid and cloud computing; HPC"
"Non-cooperative power and latency aware load balancing in distributed data centers","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019110726&doi=10.1016%2fj.jpdc.2017.04.006&partnerID=40&md5=07f2651273060b0cdd17a7e5a0127c30","In this paper we propose an algorithm for load balancing in distributed data centers based on game theory. We model the load balancing problem as a non-cooperative game among the front-end proxy servers. We model the operating cost associated with a data center as a weighted linear combination of the energy cost and the latency cost. We propose a non-cooperative load balancing game with the objective of minimizing the operating cost and obtain the structure of Nash equilibrium. Based on this structure, a distributed load balancing algorithm is designed. We compare the performance of the proposed algorithm with the existing approaches. Numerical results demonstrate that the solution achieved by the proposed algorithm approximates the global optimal solution in terms of the cost and it also ensures fairness among the users. © 2017 Elsevier Inc.","Distributed data centers; Front-end proxy servers; Game theory; Optimization of combined energy and latency cost"
"Matching circuits can be small: Partial evaluation and reconfiguration for FPGA-based packet processing","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020294580&doi=10.1016%2fj.jpdc.2017.05.004&partnerID=40&md5=066b4217a73251ae5cf3dba9f4b5abd4","Network functions like routing or firewalling require specialized hardware such as FPGAs to process packets at high rates. Such hardware must be fast enough to process packets at line rate, and it must be programmable to update the installed packet processing policy. However, these goals are conflicting because a generic programmable circuit must provide sufficient resources to support a wide range of policies, which can lead to unused circuitry and low clock rates. Also, it misses logic optimization opportunities with regard to the structure of the installed policy. In this work, we investigate the optimization potential of policy-specific generated network processing circuits. Using the example of router forwarding information bases (FIBs), we demonstrate that FIB-specialized circuits need significantly fewer logic resources than equivalent generic forwarding circuits. In combination with the partial reconfiguration capability of FPGAs, we obtain efficient low-latency forwarding engines whose matching circuitry can be replaced on demand. © 2017 Elsevier Inc.","Circuit generation; Packet forwarding; Partial evaluation; Partial reconfiguration"
"Scalable training of 3D convolutional networks on multi- and many-cores","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016059955&doi=10.1016%2fj.jpdc.2017.02.006&partnerID=40&md5=f07eb4eba5626170c239691d60bfddbb","Convolutional networks (ConvNets) have become a popular approach to computer vision. Here we consider the parallelization of ConvNet training, which is computationally costly. Our novel parallel algorithm is based on decomposition into a set of tasks, most of which are convolutions or FFTs. Theoretical analysis suggests that linear speedup with the number of processors is attainable. To attain such performance on real shared-memory machines, our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses, and sums the convergent convolution outputs via an almost wait-free concurrent method to reduce time spent in critical sections. Benchmarking with multi-core CPUs shows speedup roughly equal to the number of physical cores. We also demonstrate 90x speedup on a many-core CPU (Xeon Phi Knights Corner). Our algorithm can be either faster or slower than certain GPU implementations depending on specifics of the network architecture, kernel sizes, and density and size of the output patch. © 2017 Elsevier Inc.","Convolutional neural networks; Deep learning; Dynamic scheduling; FFT convolution; Multi-core algorithms; Wait-free summation"
"A new reliability model in replication-based big data storage systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014316828&doi=10.1016%2fj.jpdc.2017.02.001&partnerID=40&md5=43f83f9a650f8b9f37c6fe7ee6439765","Reliability is a critical metric in the design and development of replication-based big data storage systems such as Hadoop File System (HDFS). In the system with thousands of machines and storage devices, even in-frequent failures become likely. In Google File System, the annual disk failure rate is 2.88%, which means that you were expected to see 8760 disk failures in a year. Unfortunately, given an increasing number of node failures, how often a cluster starts losing data when being scaled out is not well investigated. Moreover, there is no systemic method that can be used to quantify the reliability for multi-way replication based data placement methods, which has been widely used in enterprise large-scale storage systems to improve the I/O parallelism. In this paper, we develop a new reliability model by incorporating the probability of replica loss to investigate the system reliability of multi-way declustering data layouts and analyze their potential parallel recovery possibilities. Our comprehensive simulation results on Matlab and SHARPE show that the shifted declustering data layout outperforms the random declustering layout in a multi-way replication scale-out architecture, in terms of data loss probability and system reliability by up to 63% and 85%, respectively. Our study on both 5-year and 10-year system reliability equipped with various recovery bandwidth settings shows that the shifted declustering layout surpasses the two baseline approaches in both cases by consuming up to 79% and 87% less recovery bandwidth for copyset, as well as 4.8% and 10.2% less recovery bandwidth for random layout. © 2017 Elsevier Inc.","Continuous Time Markov Chains (CTMC); Copyset; Multi-way replication; Random declustering; Reliability"
"PTNet: An efficient and green data center network","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018347537&doi=10.1016%2fj.jpdc.2017.03.007&partnerID=40&md5=77630fe1d2e871b8ff9e8473c52caea8","In recent years, data centers have witnessed an exponential growth for hosting hundreds of thousands of servers as well as to accommodating a very large demand for resources. To fulfill the required level of demand, some approaches tackled network aspects so to host a huge number of servers while others focused on delivering rapid services to the clients by minimizing the path length between any two servers. In general, network devices are often designed to achieve 1:1 oversubscription. Alternatively, in a realistic data center environment, the average utilization of a network could vary between 5% and 25%, and thus the energy consumed by idle devices is wasted. This paper proposes a new parameterizable data center topology, called PTNet. PTNet offers a gradual scalability that interconnects small to large networks covering different ranges of sizes. This new interconnection network provides also a small path length between any two servers even in large sized data centers. PTNet does not only reduce path length and latency, it also uses a power-aware routing algorithm which saves up to 40% of energy with an acceptable computation time. In comparison to existing solutions (e.g. Flatnet, BCube, DCell and Fat-tree), PTNet shows substantial improvements in terms of capacity, robustness, cost-effectiveness and power efficiency: this improvement reaches up to 50% in some cases. © 2017 Elsevier Inc.","Average path length; Data center network; Energy saving; Network topology; Scalability"
"Parallel programming with pictures is a Snap!","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013935727&doi=10.1016%2fj.jpdc.2017.01.018&partnerID=40&md5=a8e9487625f9872eae01ad983e6da7e7","For decades, computing speeds seemingly doubled every 24 months by increasing the processor clock speed, thus giving software a “free ride” to better performance. This free ride, however, effectively ended by the mid-2000s. With clock speeds having plateaued and computational horsepower instead increasing due to increasing the number of cores per processor, the vision for parallel computing, which started more than 40 years ago, is a revolution that has now (ubiquitously) arrived. In addition to traditional supercomputing clusters, parallel computing with multiple cores can be found in desktops, laptops, and even mobile smartphones. This ubiquitous parallelism in hardware presents a major challenge: the difficulty in easily extracting parallel performance via current software abstractions. Consequently, this paper presents an approach that reduces the learning curve to parallel programming by introducing such concepts into a visual (but currently sequential) programming language called Snap!, which was inspired by MIT's Scratch project. Furthermore, our proposed visual abstractions can automatically generate parallel code for the end user to run in parallel on a variety of platforms from personal computing devices to supercomputers. Ultimately, this work seeks to increase parallel programming literacy so that users, whether novice or experienced, may leverage a world of ubiquitous parallelism to enhance productivity in all walks of life, including the sciences, engineering, commerce, and liberal arts. © 2017 Elsevier Inc.","Block-based programming; Computer science education; Explicit parallel computing; Languages for PDC and HPC; Parallel computational patterns; Pedagogical tools; Programming environments; Visual programming"
"Scalable system scheduling for HPC and big data","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027978393&doi=10.1016%2fj.jpdc.2017.06.009&partnerID=40&md5=bdfa4c2b3771734d2adfb46a6081b7ad","In the rapidly expanding field of parallel processing, job schedulers are the “operating systems” of modern big data architectures and supercomputing systems. Job schedulers allocate computing resources and control the execution of processes on those resources. Historically, job schedulers were the domain of supercomputers, and job schedulers were designed to run massive, long-running computations over days and weeks. More recently, big data workloads have created a need for a new class of computations consisting of many short computations taking seconds or minutes that process enormous quantities of data. For both supercomputers and big data systems, the efficiency of the job scheduler represents a fundamental limit on the efficiency of the system. Detailed measurement and modeling of the performance of schedulers are critical for maximizing the performance of a large-scale computing system. This paper presents a detailed feature analysis of 15 supercomputing and big data schedulers. For big data workloads, the scheduler latency is the most important performance characteristic of the scheduler. A theoretical model of the latency of these schedulers is developed and used to design experiments targeted at measuring scheduler latency. Detailed benchmarking of four of the most popular schedulers (Slurm, Son of Grid Engine, Mesos, and Hadoop YARN) is conducted. The theoretical model is compared with data and demonstrates that scheduler performance can be characterized by two key parameters: the marginal latency of the scheduler ts and a nonlinear exponent αs. For all four schedulers, the utilization of the computing system decreases to <10% for computations lasting only a few seconds. Multi-level schedulers (such as LLMapReduce) that transparently aggregate short computations can improve utilization for these short computations to >90% for all four of the schedulers that were tested. © 2017 Elsevier Inc.","Data analytics; High performance computing; Job scheduler; Resource manager; Scheduler"
"An OpenCL framework for high performance extraction of image features","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020885429&doi=10.1016%2fj.jpdc.2017.05.011&partnerID=40&md5=ff1983ddea98d39b17e1ebdaf9e5ad2f","Image features are widely used for object identification in many situations, including interpretation of data containing natural scenes captured by unmanned aerial vehicles. This paper presents a parallel framework to extract additive features (such as color features and histogram of oriented gradients) using the processing power of GPUs and multicore CPUs to accelerate the algorithms with the OpenCL language. The resulting features are available in device memory and then can be fed into classifiers such as SVM, logistic regression and boosting methods for object recognition. It is possible to extract multiple features with better performance. The GPU accelerated image integral algorithm speeds up computations up to 35x when compared to the single-thread CPU implementation in a test bed hardware. The proposed framework allows real-time extraction of a very large number of image features from full-HD images (better than 30 fps) and makes them available for access in coalesced order by GPU classification algorithms. © 2017 Elsevier Inc.","Additive features; Haar features; Heterogeneous programming; Histogram of oriented gradients; Image descriptors; OpenCL; Parallel processing"
"Optimum Benefit Protocol: A fast converging, bandwidth-efficient decentralized similarity overlay","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021259787&doi=10.1016%2fj.jpdc.2017.05.013&partnerID=40&md5=fd95992648549faa0f3a14ee2923e5a5","Due to large volumes of data available online, techniques such as document classification and clustering are required for organization, analysis and management of data. Similarity-based Clustering (SBC) is used by many existing systems for filtering information. Decentralized gossip-based overlays offer a simple, robust and scalable solution to SBC clustering. Convergence and communication complexity are the two key areas of concern when SBC is implemented using these overlays. Convergence guarantees accurate clustering but costs bandwidth because these systems rely on message passing to achieve convergence. In this work, we address the long tail problem, experienced by low in-degree nodes in a long tailed similarity distribution, such as power-law distribution and propose a new SBC approach, Optimum Benefit Protocol (OBP), that converges more rapidly than existing approaches and reduces the long tail. The proposed protocol only sends messages that could benefit the receiver, reducing bandwidth to one fifth of the default. Our protocol obtains at least 90% convergence for a 900 node network, starting in a random configuration, in less than 10 cycles, for all observed experiments. We used real world distributions from Yahoo, Movielens, and Epinion datasets, for experimentation. © 2017 Elsevier Inc.","Bandwidth; Convergence; Decentralized networks; Gossip protocols; Similarity overlays; Similarity-based clustering; Sparsity"
"Evaluating and optimizing stabilizing dining philosophers","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020471512&doi=10.1016%2fj.jpdc.2017.05.003&partnerID=40&md5=01fa3475f94793e9ddf62b92dd33770c","We study theoretical and practical aspects of five of the most well-known self-stabilizing dining philosophers algorithms. We theoretically prove that three of them are incorrect. For practical evaluation, we simulate these five algorithms as well as two classic non-self-stabilizing algorithms and evaluate their fault-tolerance, latency and throughput of critical section access. We present a new combined algorithm that achieves the best throughput of the two remaining correct self-stabilizing algorithms by determining the system load and switching between these basic algorithms. We prove the combined algorithm correct, simulate it and study its performance characteristics. © 2017 Elsevier Inc.","Dining philosophers; Distributed algorithms; Fault tolerance; Self-stabilization; Simulation"
"Automatic translation of MPI source into a latency-tolerant, data-driven form","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015722274&doi=10.1016%2fj.jpdc.2017.02.009&partnerID=40&md5=bd799638fbb461ec317f227bb53c6fe8","Hiding communication behind useful computation is an important performance programming technique but remains an inscrutable programming exercise even for the expert. We present Bamboo, a code transformation framework that can realize communication overlap in applications written in MPI without the need to intrusively modify the source code. We reformulate MPI source into a task dependency graph representation, which partially orders the tasks, enabling the program to execute in a data-driven fashion under the control of an external runtime system. Experimental results demonstrate that Bamboo significantly reduces communication delays while requiring only modest amounts of programmer annotation for a variety of applications and platforms, including those employing co-processors and accelerators. Moreover, Bamboo's performance meets or exceeds that of labor-intensive hand coding. The translator is more than a means of hiding communication costs automatically; it demonstrates the utility of semantic level optimization against a well-known library. © 2017 Elsevier Inc.","Automatic communication hiding; Data-driven execution; Source-to-source translator; Task dependency graph"
"Model transformations of MapReduce Design Patterns for automatic development and verification","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008485922&doi=10.1016%2fj.jpdc.2016.12.017&partnerID=40&md5=99a62a9496c65c537f35fe8be5672b0b","Mapping MapReduce frameworks to Cloud Architecture became a must in last years because of the need of managing large data sets and Big Data in fast, reliable (and as cheap as possible) way. Scientific Literature proposes many works about new architectures, frameworks and algorithms improving and optimizing performances while performing MapReduce operations. Anyway, MapReduce framework is only the starting point for building a plethora of services based on different analyses. This is the reason for recent application of Design Patterns methodologies to develop MapReduce applications. Here we propose a Model Driven Engineering methodology to design, verify and develop MapReduce applications on Cloud Systems. The methodology is driven by MapReduce Design Patterns and is used to analyse soundness and reliability of services based on MapReduce from early design stage to runtime. © 2016 Elsevier Inc.","Cloud computing; Cloud patterns; Formal verification; MapReduce; Model driven engineering"
"A hardware compilation framework for text analytics queries","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021130573&doi=10.1016%2fj.jpdc.2017.05.015&partnerID=40&md5=ad693120e2734fad6831a9eb511e9e33","Unstructured text data is being generated at an unprecedented rate in the form of Twitter feeds, machine logs or medical records. The analysis of this data is an important step to gaining significant insight regarding innovation, security and decision-making. The performance of traditional compute systems struggles to keep up with the rapid data growth and the expected high quality of information extraction. To cope with this situation, a compilation framework is presented that can transform text analytics queries into a hardware description. Deployed on an FPGA, the queries can be executed 60 times faster on average compared to a multi-threaded software implementation. The performance has been evaluated on two generations of high-end server systems including two generations of FPGAs, demonstrating the performance gains from advanced technology. © 2017 Elsevier Inc.","Accelerator; FPGA; Query compilation; Text analytics"
"Decidability classes for mobile agents computing","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021267723&doi=10.1016%2fj.jpdc.2017.04.003&partnerID=40&md5=5800f0303ce9ee07089b8b56ddf6cecf","We establish a classification of decision problems that are to be solved by mobile agents operating in unlabeled graphs, using a deterministic protocol. The classification is with respect to the ability of a team of agents to solve decision problems, possibly with the aid of additional information. In particular, our focus is on studying differences between the decidability of a decision problem by agents and its verifiability when a certificate for a positive answer is provided to the agents (the latter is to the former what NP is to P in the framework of sequential computing). We show that the class MAV of mobile agents verifiable problems is much wider than the class MAD of mobile agents decidable problems. Our main result shows that there exist natural MAV-complete problems: the most difficult problems in this class, to which all problems in MAV are reducible via a natural mobile computing reduction. Beyond the class MAV we show that, for a single agent, three natural oracles yield a strictly increasing chain of relative decidability classes. © 2017","Distributed decision; Mobile computing; Rendezvous; Theory of distributed computing"
"Exploring big graph computing — An empirical study from architectural perspective","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994416040&doi=10.1016%2fj.jpdc.2016.07.006&partnerID=40&md5=b8a4266825ed81b3575b3ffa93b250c9","Graph computing is widely applied in a large number of big data applications. Despite its importance, high performance graph computing remains a challenge, especially for large-scale graphs. In this paper, by analyzing from the architectural perspective, we study computational behaviors of graph computing in real-world use cases. We benchmark a set of representative graph algorithms implemented on a unified framework and conduct experiments to analyze comprehensive performance characteristics. In the characterization, we observed multiple insights, including irregular memory patterns, significant diverse behavior across different computations, highly data dependent behaviors, etc., using large-scale synthetic and real-world graphs. To the best of our knowledge, this is the first comprehensive architectural study on the full-scope of graph computing. It can improve our understanding on graph computing and help high performance computing research for graph-based big data applications. © 2016 Elsevier Inc.","Architectural characterization; Big data; Graph computing"
"Cross-scale efficient tensor contractions for coupled cluster computations through multiple programming model backends","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017567231&doi=10.1016%2fj.jpdc.2017.02.010&partnerID=40&md5=4cfce1b60c7f3c31dcc3ab1b893275ef","Coupled-cluster methods provide highly accurate models of molecular structure through explicit numerical calculation of tensors representing the correlation between electrons. These calculations are dominated by a sequence of tensor contractions, motivating the development of numerical libraries for such operations. While based on matrix–matrix multiplication, these libraries are specialized to exploit symmetries in the molecular structure and in electronic interactions, and thus reduce the size of the tensor representation and the complexity of contractions. The resulting algorithms are irregular and their parallelization has been previously achieved via the use of dynamic scheduling or specialized data decompositions. We introduce our efforts to extend the Libtensor framework to work in the distributed memory environment in a scalable and energy-efficient manner. We achieve up to 240× speedup compared with the optimized shared memory implementation of Libtensor. We attain scalability to hundreds of thousands of compute cores on three distributed-memory architectures (Cray XC30 and XC40, and IBM Blue Gene/Q), and on a heterogeneous GPU-CPU system (Cray XK7). As the bottlenecks shift from being compute-bound DGEMM's to communication-bound collectives as the size of the molecular system scales, we adopt two radically different parallelization approaches for handling load-imbalance, tasking and bulk synchronous models. Nevertheless, we preserve a unified interface to both programming models to maintain the productivity of computational quantum chemists. © 2017 Elsevier Inc.","Cyclops. High performance computing; Distributed memory programming models; Energy efficiency; Libtensor; Quantum chemistry; Tensor contraction engines"
"ECG encryption and identification based security solution on the Zynq SoC for connected health systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008490311&doi=10.1016%2fj.jpdc.2016.12.016&partnerID=40&md5=a89004a3c2d74e02edfb1de61dafe3c9","Connected health is a technology that associates medical devices, security devices and communication technologies. It enables patients to be monitored and treated remotely from their home. Patients’ data and medical records within a connected health system should be securely stored and transmitted for further analysis and diagnosis. This paper presents a set of security solutions that can be deployed in a connected health environment, which includes the advanced encryption standard (AES) algorithm and electrocardiogram (ECG) identification system. Efficient System-on-Chip (SoC) implementations for the proposed algorithms have been carried out on the Xilinx ZC702 prototyping board. The Achieved hardware implementation results have shown that the proposed AES and ECG identification based system met the real-time requirements and outperformed existing field programmable gate array (FPGA)-based systems in different key performance metrics such as processing time, hardware resources and power consumption. The proposed systems can process an ECG sample in 10.71ms and uses only 30% of the available hardware resources with a power consumption of 107mW. © 2016 Elsevier Inc.","Advanced encryption standard (AES); Electrocardiogram (ECG) encryption and identification; Field programmable gate array (FPGA); Zynq7 system on chip (SoC)"
"Interdisciplinary teamwork in HPC education: Challenges, concepts, and outcomes","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009771990&doi=10.1016%2fj.jpdc.2016.12.025&partnerID=40&md5=6592be641c95d500e7418a7946be6e36","We present our concept “Teamwork Across Disciplines” which enables interdisciplinary teamwork and soft skill training at course level. The concept is realized in the scope of the course “Turbulent Flow Simulation on HPC-Systems”. We describe the course curriculum and detail various additional aspects of the course with regard to student feedback, continuous course development techniques, and the student team projects. © 2017 Elsevier Inc.","Computational fluid dynamics; High-performance computing; Interdisciplinary; Teaching; Teamwork"
"Goal-based composition of scalable hybrid analytics for heterogeneous architectures","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009460393&doi=10.1016%2fj.jpdc.2016.11.009&partnerID=40&md5=87a8523ac5df49faa9f70af9ecd6f474","Crafting scalable analytics in order to extract actionable business intelligence is a challenging endeavour, requiring multiple layers of expertise and experience. Often, this expertise is irreconcilably split between an organisation's engineers and subject matter domain experts. Previous approaches to this problem have relied on technically adept users with tool-specific training. Such an approach has a number of challenges: Expertise — There are few data-analytic subject domain experts with in-depth technical knowledge of compute architectures; Performance — Analysts do not generally make full use of the performance and scalability capabilities of the underlying architectures; Heterogeneity — calculating the most performant and scalable mix of real-time (on-line) and batch (off-line) analytics in a problem domain is difficult; Tools — Supporting frameworks will often direct several tasks, including, composition, planning, code generation, validation, performance tuning and analysis, but do not typically provide end-to-end solutions embedding all of these activities. In this paper, we present a novel semi-automated approach to the composition, planning, code generation and performance tuning of scalable hybrid analytics, using a semantically rich type system which requires little programming expertise from the user. This approach is the first of its kind to permit domain experts with little or no technical expertise to assemble complex and scalable analytics, for hybrid on- and off-line analytic environments, with no additional requirement for low-level engineering support. This paper describes (i) an abstract model of analytic assembly and execution, (ii) goal-based planning and (iii) code generation for hybrid on- and off-line analytics. An implementation, through a system which we call MENDELEEV, is used to (iv) demonstrate the applicability of this technique through a series of case studies, where a single interface is used to create analytics that can be run simultaneously over on- and off-line environments. Finally, we (v) analyse the performance of the planner, and (vi) show that the performance of MENDELEEV's generated code is comparable with that of hand-written analytics. © 2016 The Author(s)","Analytic planning; Data intensive computing; Data science; Hadoop; Heterogeneous compute; Hybrid analytics; Streaming analysis"
"FPGA optimized cellular automaton random number generator","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021063583&doi=10.1016%2fj.jpdc.2017.05.022&partnerID=40&md5=f637f0df0f9ea8d420ae167e63fc4598","Pseudo-random number generators (PRNGs) are important to applications ranging from cryptography to Monte-Carlo methods. Consequently, many PRNG architectures have been proposed, including some optimized for FPGA, e.g the LUT-SR family of PRNGs which utilize embedded FPGA shift registers, and self-programmable cellular automaton (SPCA) PRNGs. However, LUT-SR and other PRNGs do not utilize key features of modern Xilinx FPGAs: embedded carry chains and splittable Look-Up Tables (LUTs), i.e., 6-input LUTs which can operate as two 5-input LUTs which share inputs. In this paper we explore the SPCA structure and derive a set of parameter constraints which allow a SPCA PRNG to produce 2 random bits per LUT in every clock cycle on modern Xilinx FPGAs. We determine this to be the maximum logic density achievable for SPCA, and propose an architectural improvement of SPCA to enable further density increase by making use of FPGA embedded carry chains as a method to compute an additional random bit per LUT in each clock cycle. The resulting Split-LUT-Carry SPCA (SLC-SPCA) PRNG achieves 6x improvement in logic density compared to LUT-SR, and a 1.5x density increase compared to SPCA. We evaluate the randomness of SLC-SPCA utilizing the NIST Statistical Test Suite, and we provide a power and energy comparison of LUT-SR and SLC-SPCA on a Xilinx Zynq 7020 FPGA device. Our results indicate that SLC-SPCA generates 3x more bits per clock at approximately the same power dissipation as LUT-SR, and consequently 3x less energy to generate 1 gigabit of random data. SLC-SPCA is also 1.5x more energy-efficient than a SPCA PRNG. © 2017 Elsevier Inc.","Cellular automaton; FPGA; Random number generator"
"Mitigation of Hardware Trojan based Denial-of-Service attack for secure NoCs","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026774220&doi=10.1016%2fj.jpdc.2017.06.014&partnerID=40&md5=ff414dc3d3222ab6c14372da7a050b12","As Multiprocessor System-on-Chips (MPSoCs) continue to scale, security for Network-on-Chips (NoCs) is a growing concern as rogue agents threaten to infringe on the hardware's trust and maliciously implant Hardware Trojans (HTs) to undermine their reliability. The trustworthiness of MPSoCs will rely on our ability to detect Denial-of-Service (DoS) threats posed by the HTs and mitigate HTs in a compromised NoC to permit graceful network degradation. In this paper, we propose a new light-weight target-activated sequential payload (TASP) HT model that performs packet inspection and injects faults to create a new type of DoS attack. Faults injected are used to trigger a response from error correction code (ECC) schemes and cause repeated retransmission to starve network resources and create deadlocks capable of rendering single-application to full chip failures. To circumvent the threat of HTs, we propose a heuristic threat detection model to classify faults and discover HTs within compromised links. To prevent further disruption, we propose several switch-to-switch link obfuscation methods to avoid triggering of HTs in an effort to continue using links instead of rerouting packets with minimal overhead (1–3 cycles). To sustain data integrity over a compromised link, we propose an optimized implementation of algebraic manipulation detection (AMD) codes to detect any fault injection in targeted flits. Our proposed modifications complement existing fault detection and obfuscation methods and only add 2% in area overhead and 6% in excess power consumption in the NoC micro-architecture. © 2017 Elsevier Inc.","Denial-of-Service; Fault injection; Hardware trojan; Network-on-chip"
"Fixed length lightweight compression for GPU revised","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018767332&doi=10.1016%2fj.jpdc.2017.03.011&partnerID=40&md5=35e25cee557ae6d8c6fd1d1cdc2d700d","Limitations of I/O bandwidth and latency are a serious burden for many data intensive algorithms. This is even more noticeable in distributed heterogeneous clusters that heavily utilize GPU processors, where communication costs and limitation of memory play crucial role. Lightweight lossless compression methods are often applied to cope with those limitations. In this paper we investigate new approaches to parallel lossless lightweight compression methods based on fixed-length minimum bit encoding for GPU processors. By developing new memory organization and by utilizing various inter thread and inter warp communication abilities, we have proposed algorithms which suit GPU architecture better. As a result we have significantly improved compression ratio and bandwidths. This allows for many new applications in computational clusters as well as in computational algorithms. Our claims are supported by tests conducted using simulated data and TPC-H database benchmarking tools. © 2017 Elsevier Inc.","CUDA; GPU; Lightweight data compression; Lossless data compression; Multi-GPU; Parallel processing"
"Distributed online Temporal Fuzzy Concept Analysis for stream processing in smart cities","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015679534&doi=10.1016%2fj.jpdc.2017.02.002&partnerID=40&md5=98f2baa54a4c822298a1d68a6e471bdd","Nowadays one of the main challenges in the smart cities is mining high-level semantics from low-level activities. In this context real-time data streams are continuously produced and analysed by efficient and effective algorithms, which are able to handle complexities related to big data, in order to enable the core functions of Decision Support Systems in the smart city. These algorithms should receive input data coming from different city domains (or pillars) and process, aggregate and reason over them in a way that it is possible to find hidden correlations among different and heterogeneous elements (e.g., traffic, weather, cultural events) along space and time dimensions. This paper proposes the online implementation and deployment of Temporal Fuzzy Concept Analysis on a distributed real-time computation system, based on Apache Storm, to face with big data stream analysis in the smart city context. Such online distributed algorithm is able to incrementally generate the timed fuzzy lattice that organizes the knowledge on several and cross-domain aspects of the city. Temporal patterns, of how situations evolve in the city, can be elicited by both exploring the lattice and observing its growth in order to obtain actionable knowledge to support smart city decision-making processes. © 2017 Elsevier Inc.","Distributed architecture; Online data streams; Parallel computation; Smart city; Temporal Fuzzy Concept Analysis"
"Distributed stream clustering using micro-clusters on Apache Storm","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003946086&doi=10.1016%2fj.jpdc.2016.06.004&partnerID=40&md5=54c5de5d15f8d2a38a8ac24813f9e29c","The recent need to extract real-time insights from data has driven the need for machine learning algorithms that can operate on data streams. Given the current extreme rates of data generation (around 5000 messages per second), these algorithms need to be able to handle data streams of very high velocity. Many current algorithms do not reach this requirement, in some cases processing only tens of messages per second. In this work we address the problem of limited achievable throughput of stream clustering by developing scalable distributed algorithms based on the micro-clustering paradigm that run on cloud platforms. We present two distributed architectures to execute the algorithms in parallel and implement these architectures on the Apache Storm stream processing platform. We demonstrate that we are able to gain close to an order of magnitude of improvement of performance in our experiments. © 2016 Elsevier Inc.","Distributed data mining; Stream clustering; Stream data mining"
"OnRamp: A web-portal for teaching parallel and distributed computing","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013057632&doi=10.1016%2fj.jpdc.2017.01.014&partnerID=40&md5=ffedf4a7cf0cabd652fe4cdeb7f8cd5e","Computer Science students must understand parallel and distributed computing (PDC) concepts to be effective computer scientists in the workforce, as reflected in the 2013 ACM Curriculum guidelines. Significant work has been done by CS educators to develop curriculum materials and increase access to parallel compute environments (PCEs) by leveraging and clustering a plethora of small multicore systems. Even with these resources there is a barrier to entry for students to use PCEs, namely the unfamiliar and complex system software ecosystem of modern PCEs. The OnRamp project lowers that barrier to entry for exploring PDC concepts on a variety of PCEs by abstracting away the details of interacting with the system and focuses the students’ attention on the PDC concepts. This top-down approach is in contrast to existing approaches involving all aspects of PDC necessarily being taught before the key concepts can be explored. In this paper we discuss the motivation, design and implementation of OnRamp, a general purpose web portal for supporting the exploration of PDC concepts that harnesses the existing educational resources created by the CS education community. It coaches students through interactive modules that teach them about PDC concepts and PCEs while allowing them to launch parallel applications from day one. © 2017 Elsevier Inc.","Interactive learning environment; Parallel and distributed computing education; Web portal"
"A rapid detection algorithm of corrupted data in cloud storage","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028894452&doi=10.1016%2fj.jpdc.2017.08.004&partnerID=40&md5=03f2040aac74832ecd7fbaf02cd55999","The cloud computing provides dynamically scalable and virtualized resource service for users to access the storage data. Although having been bringing enormous convenience, it also incurs the threat of users’ data loss or corruption, such as data intentionally deleted or corrupted, the service providers’ hardware error and careless operation. Most of the data verification schemes based POR or PDP are proposed to verify the integrity of a data block or even a batch of data blocks. However, once the batch verification fails, it results in that all the blocks in the batch of data cannot be judged to be intact or corrupted since the corrupted blocks are not accurately identified. To improve the efficiency of corrupted data identified, we propose a rapid detection algorithm of corrupted data based on three-dimensional data locating, which is called cube-based detection. Furthermore, the consistent hash is utilized to balance the computation cost, and the cube splitting is applied to locate the corrupted blocks by narrowing the range of suspicious blocks step by step. Finally, both data hierarchically concatenated and data blind technology are utilized to improve the verification efficiency and preserve users’ data privacy in the detection process. Theoretic analysis and simulation results demonstrate that our algorithm has strong detection capability and identification capability for all the corrupted blocks, and greatly decreases the cost of verification data transmission and computation. © 2017 Elsevier Inc.","Cloud storage; Corrupted data detecting; Cube-based hierarchical verification; Data integrity verification"
"Deister: A light-weight autonomous block management in data-intensive file systems using deterministic declustering distribution","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975517334&doi=10.1016%2fj.jpdc.2016.03.005&partnerID=40&md5=ce4dbc8cde272ffd37359888895fb187","During the last few decades, Data-intensive File Systems (DiFS), such as Google File System (GFS) and Hadoop Distributed File System (HDFS) have become the key storage architectures for big data processing. These storage systems usually divide files into fixed-sized blocks (or chunks). Each block is replicated (usually three-way) and distributed pseudo-randomly across the cluster. The master node (namenode) uses a huge table to record the locations of each block and its replicas. However, with the increasing size of the data, the block location table and its corresponding maintenance could occupy more than half of the memory space and 30% of processing capacity in master node, which severely limit the scalability and performance of master node. We argue that the physical data distribution and maintenance should be separated out from the metadata management and performed by each storage node autonomously. In this paper, we propose Deister, a novel block management scheme that is built on an invertible deterministic declustering distribution method called Intersected Shifted Declustering (ISD). Deister is amendable to current research on scaling the namespace management in master node. In Deister, the huge table for maintaining the block locations in the master node is eliminated and the maintenance of the block-node mapping is performed autonomously on each data node. Results show that as compared with the HDFS default configuration, Deister is able to achieve identical performance with a saving of about half of the RAM space and 30% of processing capacity in master node and is expected to scale to double the size of current single namenode HDFS cluster, pushing the scalability bottleneck of master node back to namespace management. © 2016 Elsevier Inc.","Data block distribution; Data intensive file system; Metadata server scalability"
"SideIO: A Side I/O system framework for hybrid scientific workflow","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994360868&doi=10.1016%2fj.jpdc.2016.07.001&partnerID=40&md5=26f8220045a6697eae9c5e36253725cf","Recent years have seen an increasing number of Hybrid Scientific Applications. They often consist of one HPC simulation program along with its corresponding data analytics programs. Unfortunately, current computing platform settings do not accommodate this emerging workflow very well, especially write-once-read-many workflows. This is mainly because HPC simulation programs store output data into a dedicated storage cluster equipped with Parallel File System(PFS). To perform analytics on data generated by simulation, data has to be migrated from storage cluster to compute cluster. This data migration could introduce severe delay which is especially true given an ever-increasing data size. To solve the data migration problem in small-medium sized HPC clusters, we propose to construct a sided I/O path, named as SideIO, to explicitly direct analysis data to data-intensive file systems (DIFS in brief) that co-locates computation with data. In contrast, checkpoint data may not be read back later, it is written to the dedicated PFS to maximize I/O throughput. There are three components in SideIO. An I/O splitter separates simulation outputs to different storage systems (PFS or DIFS); an I/O middle-ware component allows original HPC simulation programs to execute direct I/O operations over DIFS without any porting effort and an I/O scheduler dynamically smooths out both disk write and read traffic for both simulation and analysis programs. By experimenting with two real-world scientific workflows over a 46-node SideIO prototype, we found that SideIO is able to achieve comparable read/write I/O performance in small-medium sized HPC clusters equipped with PFS. More importantly, since SideIO completely avoids the most expensive data movement overhead, it achieves up to 3x speedups for hybrid scientific workflow applications compared with current solutions. © 2016 Elsevier Inc.","Data migration; Data-intensive; HDFS; HPC; MPI; Scientific workflow"
"Node-independent spanning trees in Gaussian networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025114377&doi=10.1016%2fj.jpdc.2017.06.018&partnerID=40&md5=b0bd8b0ec6ca606320b9493ee6792c20","Message broadcasting in networks can be efficiently carried over spanning trees. A set of spanning trees in the same network is node independent if two conditions are satisfied. First, all trees are rooted at the same node r. Second, for every node u in the network, all trees’ paths from r to u are node-disjoint, excluding the end nodes r and u. Independent spanning trees have applications in fault-tolerant communications and secure message distributions. Gaussian networks and two-dimensional toroidal networks share similar topological characteristics. They are regular of degree four, symmetric, and node-transitive. Gaussian networks, however, have relatively lesser network diameter that could result in a better performance. This promotes Gaussian networks to be a potential alternative for two-dimensional toroidal networks. In this paper, we present constructions for node independent spanning trees in dense Gaussian networks. Based on these constructions, we design routing algorithms that can be used in fault-tolerant routing and secure message distribution. We also design fault-tolerant algorithms to construct these trees in parallel. © 2017 Elsevier Inc.","Circulant graphs; Fault-tolerant routing; Gaussian networks; Independent spanning trees; Spanning trees"
"Parallelizing maximal clique and k-plex enumeration over graph data","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017435475&doi=10.1016%2fj.jpdc.2017.03.003&partnerID=40&md5=fcd4911a0f23784edf98cbffe6da2c19","In a wide variety of emerging data-intensive applications, such as social network analysis, Web document clustering, entity resolution, and detection of consistently co-expressed genes in systems biology, the detection of dense subgraphs cliques and k-plex is an essential component. Unfortunately, these problems are NP-Complete and thus computationally intensive at scale — hence there is a need to come up with techniques for distributing the computation across multiple machines such that the computation, which is too time-consuming on a single machine, can be efficiently performed on a machine cluster given that it is large enough. In this paper, we first propose a new approach for maximal clique and k-plex enumeration, which identifies dense subgraphs by binary graph partitioning. Given a connected graph G=(V,E), it has a space complexity of O(|E|) and a time complexity of O(|E|μ(G)), where μ(G) represents the number of different cliques (k-plexes) existing in G. It recursively divides a graph until each task is sufficiently small to be processed in parallel. We then develop parallel solutions and demonstrate how graph partitioning can enable effective load balancing. Finally, we evaluate the performance of the proposed approach on real and synthetic graph data and show that it performs considerably better than existing approaches in both centralized and parallel settings. In the parallel setting, it can achieve the speedups of up to 10x over existing approaches on large graphs. Our parallel algorithms are primarily implemented and evaluated on MapReduce, a popular shared-nothing parallel framework, but can easily generalize to other shared-nothing or shared-memory parallel frameworks. The work presented in this paper is an extension of our preliminary work on the approach of binary graph partitioning for maximal clique enumeration. In this work, we extend the proposed approach to handle maximal k-plex detection as well. © 2017 Elsevier Inc.","MapReduce; Maximal clique enumeration; Maximal k-plex enumeration; Parallel graph processing"
"The design and integration of a software configurable and parallelized coprocessor architecture for LQR control","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013066184&doi=10.1016%2fj.jpdc.2017.01.028&partnerID=40&md5=b41c87776bcfacb27a7c35249d7cab84","The increasing integration of computing into the physical systems we rely on everyday motivates the need to more easily marry advanced control theory, which is used to control these systems, with the computing platforms used to implement the controllers. This article explores one path of easing this integration using reconfigurable hardware technology, and discusses practical system-level details that must be addressed for integrating our idea into real-world systems. We present a software configurable and parallelized coprocessor architecture for LQR control that can control physical processes representable by a linear state-space model. Our proposed architecture has distinct advantages over purely software or purely hardware approaches. It differs from other hardware controllers in that it is not hardwired to control one or a small range of plant types (e.g. only electric motors). Via software, an embedded systems engineer can easily reconfigure the controller to suit a wide range of control applications that can be represented as a state-space model. One goal of our approach is to support a design methodology to help bridge the gap between controls and embedded system software engineering. Control of the well-understood inverted pendulum on a cart is used as an illustrative example of how the proposed hardware accelerator architecture supports our envisioned design methodology for helping bridge this gap. Additionally, we explore the design space of our co-processor's parallel architecture in terms of computing speed and resource utilization. Our performance results show a 3.4 to 100 factor speedup over a 666 MHz embedded ARM processor, for plants that can be represented by 4 to 128 states, respectively. This article concludes with a discussion of the practical integration details required for interfacing the controller with a real inverted pendulum–cart system. © 2017 Elsevier Inc.","Co-processor; FPGA; Inverted pendulum; LQR; Parallel processing; State space control"
"FPGA-based tsunami simulation: Performance comparison with GPUs, and roofline model for scalability analysis","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009284694&doi=10.1016%2fj.jpdc.2016.12.015&partnerID=40&md5=ff13309d023d4f9f4e3362f9f247c643","MOST (Method Of Splitting Tsunami) is widely used to solve shallow water equations (SWEs) for simulation of tsunami. This paper presents high-performance and power-efficient computation of MOST for practical tsunami simulation with FPGA. The custom hardware for simulation is based on a stream computing architecture for deeply pipelining to increase performance with a limited bandwidth. We design a stream processing element (SPE) of computing kernels combined with stencil buffers. We also introduce an SPE array architecture with spatial and temporal parallelism to further exploit available hardware resources by implementing multiple SPEs with parallel internal pipelines. Our prototype implementation with Arria 10 FPGA demonstrates that the FPGA-based design performs numerically stable tsunami simulation with real ocean-depth data in single precision by introducing non-dimensionalization. We explore the design space of SPE arrays, and find that the design of six cascaded SPEs with a single pipeline achieves the sustained performance of 383 GFlops and the performance per power of 8.41 GFlops/W with a stream bandwidth of only 7.2 GB/s. These numbers are 8.6 and 17.2 times higher than those of NVidia Tesla K20c GPU, and 1.7 and 7.1 times higher than those of AMD Radeon R9 280X GPU, respectively, for the same tsunami simulation in single precision. Moreover, we proposed a roofline model for stream computing with the SPE array in order to investigate factors of performance degradation and possible performance improvement for given FPGAs. With the model, we estimate that an upcoming Stratix 10 GX2800 FPGA can achieve the sustained performance of 8.7 TFlops at most with our SPE array architecture for tsunami simulation. © 2016 Elsevier Inc.","Custom hardware; FPGA; GPU; Roofline model; Stream computing; Tsunami simulation"
"How to apply Amdahl's law to multithreaded multicore processors","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018999828&doi=10.1016%2fj.jpdc.2017.03.006&partnerID=40&md5=386d71a6e949933f5775e53db7e82277","In this paper we comment on a recent article on Amdahl's law for multithreaded multicore processors. We propose that models for predicting speedup in such systems should explicitly separate the memory and computational parts of a workload. On the one hand, not doing so can lead to speedup calculations with more than one possible result. We show this for a simple example which exploits cache affinity between two sequential tasks. On the other hand, we show that explicit separation of the workload in this case leads to a unique result. We contend that decomposition of workloads into computational and memory parts can resolve similar ambiguity in general and is fundamental since it is apparent in the Turing Machine model of computation. Further research into this separation in the formulation of Amdahl's law for modern architectures is advocated. © 2017 Elsevier Inc.","Amdahl's law; Multicore platforms; Speedup models"
"On the injection of hardware faults in virtualized multicore systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017134381&doi=10.1016%2fj.jpdc.2017.03.004&partnerID=40&md5=5b665ab3d0a5e0de9981a9e4f41a00af","Virtualized multicore systems represent an emerging computing paradigm in the critical systems industry. Virtualization-based solutions leverage the different cores of the processor to run operating systems and applications within separate partitions, to support the development of parallel mixed-criticality systems, and to improve fault-tolerance by protecting and isolating the operating environments. The critical systems industry is subjected to international standards, which recommend fault injection as a mean to contribute with evidence to safety cases. This paper proposes a framework to inject hardware faults in virtualized multicore systems. Our proposal capitalizes on the error reporting architecture of modern processors and allows injecting faults both at hypervisor- and guest-OS-level. We implement the framework in the context of the widely used Intel Core i7 processor and Xen hypervisor. We demonstrate the use of the framework by means of about 60,000 injection experiments in a Linux-based virtualized multicore system installation. © 2017 Elsevier Inc.","Dependability; Fault injection; Machine check exception; Multicore; Virtualization"
"DINO: Divergent node cloning for sustained redundancy in HPC","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026196979&doi=10.1016%2fj.jpdc.2017.06.010&partnerID=40&md5=f0f0a6484d0d1978d709a500a503a241","Complexity and scale of next generation HPC systems pose significant challenges in fault resilience methods such that contemporary checkpoint/restart (C/R) methods that address fail-stop behavior may be insufficient. Redundant computing has been proposed as an alternative at extreme scale. Triple redundancy has an advantage over C/R in that it can also detect silent data corruption (SDC) and then correct results via voting. However, current redundant computing approaches do not repair failed or corrupted replicas. Consequently, SDCs can no longer be detected after a replica failure since the system has been degraded to dual redundancy without voting capability. Hence, a job may have to be aborted if voting uncovers mismatching results between the remaining two replicas. And while replicas are logically equivalent, they may have divergent runtime states during job execution, which presents a challenge to simply creating new replicas dynamically. This problem is addressed by, DIvergent NOde cloning (DINO), a redundant execution environment that quickly recovers from hard failures. DINO consists of a novel node cloning service integrated into the MPI runtime system that solves the problem of consolidating divergent states among replicas on-the-fly. With DINO, after degradation to dual redundancy, a good replica can be quickly cloned so that triple redundancy is restored. We present experimental results over 9 NAS Parallel Benchmarks (NPB), Sweep3D and LULESH. Results confirm the applicability of the approach and the correctness of the recovery process and indicate that DINO can recover from failures nearly instantly. The cloning overhead depends on the process image size that needs to be transferred between source and destination of the clone operation and varies between 5.60 to 90.48 s. Simulation results with our model show that dual redundancy with DINO recovery always outperforms 2x and surpasses 3x redundancy on up to 1 million nodes. To the best of our knowledge, the design and implementation for repairing failed replicas in redundant MPI computing is unprecedented. © 2017 Elsevier Inc.","Fault tolerance; High performance computing; Node cloning; Redundant computing"
"Parallel cut tree algorithms","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020053651&doi=10.1016%2fj.jpdc.2017.04.007&partnerID=40&md5=4a629813c39074ee1f9e84af365fe48e","A cut tree is a combinatorial structure that represents the edge-connectivity between all pairs of vertices of an undirected graph. Cut trees solve the all pairs minimum s–t-cut problem efficiently. Cut trees have a large number of applications including the solution of important combinatorial problems in fields such as graph clustering and graph connectivity. They have also been applied to scheduling problems, social network analysis, biological data analysis, among others. Two sequential algorithms to compute a cut tree of a capacitated undirected graph are well known: the Gomory–Hu algorithm and the Gusfield algorithm. In this work three parallel cut tree algorithms are presented, including parallel versions of Gusfield and Gomory–Hu algorithms. A hybrid algorithm that combines techniques from both algorithms is proposed which provides a more robust performance for arbitrary instances. Experimental results show that the three algorithms achieve significant speedups on real and synthetic graphs. We discuss the trade-offs between the alternatives, each of which presents better results given the characteristics of the input graph. On several instances the hybrid algorithm outperformed both other algorithms, being faster than the parallel Gomory–Hu algorithm on most instances. © 2017 Elsevier Inc.","Cut tree algorithms; Graph edge-connectivity; Parallel algorithms"
"Low latency and division free Gauss–Jordan solver in floating point arithmetic","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009243702&doi=10.1016%2fj.jpdc.2016.12.013&partnerID=40&md5=9094d702ac4faf39fcd3c1625fa9cb43","In many applications, the solution of a linear system is computed with Gaussian elimination followed by back-substitution, or Gauss–Jordan elimination. The latter is intrinsically more parallel, enabling smaller computing latencies at the price of more complex hardware. However both methods require the division operator, which leads to a time-consuming resource in the critical path of the algorithms and impacts the global processing's latency. Jordan was already aware of a division free algorithm. However, its implementation involves multiplications at each step and the size of the numbers rapidly becomes too big for an efficient implementation of large systems. In this work, we present a small modification to the division free algorithm in order to keep the size of the numbers in a reasonable range for standard floating point numbers. This is possible thanks to the special format of floating point numbers, which enables error free and hardware efficient divisions by powers of two. We also propose a parallel and pipelined architecture that best exploits the proposed algorithm, including partial pivoting. We specially focus on the global latency of the system as a function of its size, the latency of the floating point operators, and the number of operators that are available. Results demonstrate that current FPGAs can solve linear systems larger than hundred equations within ten microseconds. This represents a two order of magnitude improvement over previous implementations for relatively small systems. © 2016 Elsevier Inc.","Gaussian elimination; Gauss–Jordan; Linear solver; Low latency solver; Real time simulation"
"Machine learning-based thread-parallelism regulation in software transactional memory","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022022359&doi=10.1016%2fj.jpdc.2017.06.001&partnerID=40&md5=a5bc76946a5645df123c1cb5546cf133","Transactional Memory (TM) stands as a powerful paradigm for manipulating shared data in concurrent applications. It avoids the drawbacks of coarse grain locking schemes, namely the potentially excessive limitation of concurrency, while jointly providing support for synchronization transparency to the programmers, which is achieved by embedding code-blocks accessing shared data within transactions. On the downside, excessive transaction aborts may arise in scenarios with non-negligible volumes of conflicting data accesses, which might significantly impair performance. TM needs therefore to resort to methods enabling applications to run with the maximum degree of transaction concurrency that still avoids thrashing. In this article, we focus on Software TM (STM) implementations and present a machine learning-based approach that enables the dynamic selection of the best suited number of threads to be kept alive along specific phases of the execution of STM applications, depending on (variations of) the shared data access pattern. Two key contributions are provided with our approach: (i) the identification of the well suited set of features allowing the instantiation of a reliable neural network-based performance model and (ii) the introduction of mechanisms enabling the reduction of the run-time overhead for sampling these features. We integrated a real implementation of our machine learning-based thread-parallelism regulation approach within the TinySTM open source package and present experimental data, based on the STAMP benchmark suite, which show the effectiveness of the presented thread-parallelism regulation policy in optimizing transaction throughput. © 2017 Elsevier Inc.","Concurrency; Performance optimization; Performance prediction; Transactional memory"
"Bimodal packet aware scheduling for an OFDMA based on-chip RF interconnect","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020012624&doi=10.1016%2fj.jpdc.2017.05.002&partnerID=40&md5=f0c365e743ddf3507432055cafb43924","As massive microprocessors with thousands of cores are on the horizon, using Radio Frequency (RF) or state-of-the-art nanophotonic on-chip interconnects appears as a solution to cope with current latency constraints. Due to their reliance on numerous static circuitry to generate communication channels, proposed architectures cannot rearbitrate the available bandwidth to on-chip nodes according to instantaneous traffic demands. In this paper, we present an Orthogonal Frequency Division Multiple Access (OFDMA) based wired on-chip RF interconnect as an effective reconfigurable and broadcast capable modulation. A hierarchical 2048-core CMP architecture is explained along with its hybrid cache coherency mechanism. Based on this novel architecture, we introduce an innovative bandwidth arbitration mechanism which allocates more bandwidth to cache-line carrying long packets without requiring extra signaling overhead. Exploiting broadcast capability and effective reconfigurability, we show that this bimodal packet aware communication infrastructure can provide up to 10× less average latency compared to a static counterpart under certain circumstances. © 2017 Elsevier Inc.","Chip multiprocessors (CMP); Dynamic bandwidth allocation; On-chip RF interconnect; Orthogonal Frequency Division Multiplexing (OFDM)"
"Exploring parallel multi-GPU local search strategies in a metaheuristic framework","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029145308&doi=10.1016%2fj.jpdc.2017.06.011&partnerID=40&md5=61c4d2b293550f63eac6024cea721f45","Optimization tasks are often complex, CPU-time consuming and usually deal with finding the best (or good enough) solution among alternatives for a given problem. Parallel metaheuristics have been used in many real-world and scientific applications to efficiently solve these kind of problems. Local Search (LS) is an essential component for some metaheuristics and, very often, represents the dominant computational effort accomplished by an algorithm. Several metaheuristic approaches try to adapt traditional LS models to parallel platforms without considering the intrinsic features of the available architectures. In this work, we present a novel local search strategy, so-called Distributed Variable Neighborhood Descent (DVND), specially designed for CPU and multi-GPU environment. Furthermore, a new neighborhood search strategy, so-called Multi Improvement, is introduced, taking advantage of GPU massive parallelism in order to boost up LS procedures. A hard combinatorial problem is considered as case of study, the Minimum Latency Problem (MLP). For tackling this problem, a hybrid metaheuristic algorithm is considered, which combines good quality initial solutions, generated by a Greedy Randomized Adaptive Search Procedures, with a flexible and powerful refinement procedure, inside the scope of an Iterated Local Search. The DVND was compared to the classic local search procedures, producing results that outperformed the best known sequential algorithm found in the literature. The speedups ranged from 7.3 to 13.7, for the larger MLP instances with 500 to 1000 clients. Results demonstrate the effectiveness of the proposed techniques in terms of solution quality, performance and scalability. © 2017 Elsevier Inc.","GRASP; ILS; Local search; Minimum latency problem; Multi-GPU; Parallel metaheuristic; VND"
"Efficient disjoint boundary detection algorithm for surveillance capable WSNs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021869051&doi=10.1016%2fj.jpdc.2017.06.002&partnerID=40&md5=54fd8548643adad06d6ad83ed547592d","Wireless Sensor Networks (WSN) have given a new approach for applications such as surveillance, tracking, and monitoring. In such monitoring systems, the sensor nodes at the network boundary require remaining vigilant to detect events like objects entering and leaving the area under surveillance. Nodes performing surveillance of target area can be classified as boundary node and others as an interior node. Due to continuous vigilance, boundary node suffers from quicker energy exhaustion (compare to internal nodes) which results to a shorter life span of the node. None of the reported works on boundary detection algorithms have considered the problem of maximizing the lifetime of boundary nodes. The problem of maximizing the lifetime of boundary nodes collectively addresses the problem of an optimal number of boundary nodes detection and maximization of boundary node's lifetime. In this work, we formulate the problem of the maximum lifetime of the boundary as the problem of partitioning the boundary region of the network into disjoint dominating sets of a boundary region with the aim of maximizing the number of disjoint dominating sets of a boundary region in the network. This is the problem of the domatic partition of boundary region in graph theory, where each partition is dominating the set of boundary region (i.e., not the dominating set of entire network). Rotating the disjoint dominating sets of boundary region in the network resulting into maximizing the lifetimes of boundary nodes. We found that our disjoint dominating sets of boundary region algorithm have an approximation ratio ((4.8+ln5)opt+1.2), where opt is the minimum size dominating set which has the same approximation ratio as the minimum dominating set problem. The time complexity of boundary detection algorithm is O(mn2) where m=|Euk∖1|, n=|Nuk∖1| and k is the radius of graph. Simulation results reveal that using our proposed domatic partition of boundary region algorithm, the boundary node saves 35.71% energy lesser as compared to without domatic partition based boundary detection algorithm. To support our claim, we extended our work one step further and performed the experimental analysis of our proposed algorithm on telosb motes. © 2017 Elsevier Inc.","Distributed sensor networks; Pervasive computing; Surveillance"
"A survey on information security threats and solutions for Machine to Machine (M2M) communications","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021447987&doi=10.1016%2fj.jpdc.2017.05.021&partnerID=40&md5=b2721a05d720644bf66910cf1e77fd64","Although Machine to Machine (M2M) networks allow the development of new promising applications, the restricted resources of machines and devices in the M2M networks bring several constraints including energy, bandwidth, storage, and computation. Such constraints pose several challenges in the design of M2M networks. Furthermore, some elements that contributed to the rise of M2M applications have caused several new security threats and risks, typically due to the advancements in technology, increasing computing power, declining hardware costs, and freely available software tools. Due to the restricted capabilities of M2M devices, most of the recent research efforts on M2M have focused on computing, resource management, sensing, congestion control and controlling technologies. However, there are few studies on security aspects and there is a need to introduce the threats existing in M2M systems and corresponding solutions. Accordingly, in this paper, after presenting an overview of potential M2M applications, we present a survey of security threats against M2M networks and solutions to prevent or reduce their impact. Then, we investigate security-related challenges and open research issues in M2M networks to provide an insight for future research opportunities. Moreover, we discuss the oneM2M standard, one of the prominent standard initiatives for more secure and smoother M2M networks and the Internet of Things. © 2017 Elsevier Inc.","Countermeasures; Machine to Machine (M2M) communications; OneM2M standard; Security threats"
"Pervasive parallel and distributed computing in a liberal arts college curriculum","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010993624&doi=10.1016%2fj.jpdc.2017.01.005&partnerID=40&md5=b98d000f9b75587535fdde8f68288257","We present a model for incorporating parallel and distributed computing (PDC) throughout an undergraduate CS curriculum. Our curriculum is designed to introduce students early to parallel and distributed computing topics and to expose students to these topics repeatedly in the context of a wide variety of CS courses. The key to our approach is the development of a required intermediate-level course that serves as an introduction to computer systems and parallel computing. It serves as a requirement for every CS major and minor and is a prerequisite to upper-level courses that expand on parallel and distributed computing topics in different contexts. With the addition of this new course, we are able to easily make room in upper-level courses to add and expand parallel and distributed computing topics. The goal of our curricular design is to ensure that every graduating CS major has exposure to parallel and distributed computing, with both a breadth and depth of coverage. Our curriculum is particularly designed for the constraints of a small liberal arts college, however, much of its ideas and its design are applicable to any undergraduate CS curriculum. © 2017 The Authors","CS curriculum; Parallel and distributed computing"
"HW/SW Co-Design of the HOG algorithm on a Xilinx Zynq SoC","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020481249&doi=10.1016%2fj.jpdc.2017.05.005&partnerID=40&md5=b2004cf2243218136bcec1e1ce64c431","An accurate and fast human detection is a crucial task for a wide variety of applications such as automotive and person identification. The histogram of oriented gradients (HOG) algorithm is one of the most reliable and applied algorithms for this task. However the HOG algorithm is also a compute intensive task. This paper presents three different implementations using the Zynq SoC that consists of an ARM processor and an FPGA. The first uses OpenCV functions and runs on the ARM processor. A speedup of 249×is achieved due to several optimizations that are implemented in this OpenCV-based HOG approach. The second is a HW/SW Co-Design implemented on the ARM processor and the FPGA. The third is completely implemented on the FPGA and optimized for an FPGA implementation to achieve the highest performance for high resolution images (1920×1080). This implementation achieves 39.6 fps which is a speedup of 503.9× compared to the OpenCV-based approach and 2× compared to this implementation with optimizations. The HW/SW Co-Design achieves a speedup of approximately 9× compared to an original HOG implementation running on the ARM processor. © 2017","FPGA; HOG algorithm; Human detection; HW/SW Co-Design; Image processing; Real-time; SDSoC; Xilinx Zynq"
"VAYU: Accelerating stream processing applications through dynamic network-aware topology re-optimization","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026884454&doi=10.1016%2fj.jpdc.2017.06.023&partnerID=40&md5=e7e4502acd8ac1ca52054106d075e1cf","Stream processing applications for online analytics are commonly used in domains ranging from sensor data processing to social networking. To achieve high-throughput, stream processing engines support pipelined execution, low-overhead fault-tolerance, and efficient group communication overlays. The throughput of pipelined application workflows is significantly impacted by dynamic system state. In particular, we show that a single bottleneck in the pipeline (congested link or an overloaded operator) can drastically impact the system throughput. In this paper, we present a number of techniques for addressing bottlenecks in stream engines. Our techniques fall into two major classes–network-aware routing for fine grained control of streams; and dynamic overlay generation for optimizing performance of group communication operations. To enable fast workflow re-optimization, we present a light-weight protocol for consistent modification of pipelines. We present detailed algorithms, their implementation in a real system, and address issues of fault tolerance and performance. We evaluate performance of the proposed techniques in the context of three real applications. We show that our techniques improve performance by 20% to 200%, under various overheads, relative to a baseline representative of current implementations. We demonstrate that our techniques are robust to highly dynamic state, as well as complex congestion patterns. Given the widespread use of streaming systems and the need for dealing with dynamic system state, our techniques represent a significant and practical improvement. © 2017 Elsevier Inc.","Dynamic Reoptimization; Storm; Stream-processing"
"Hybrid Message Pessimistic Logging. Improving current pessimistic message logging protocols","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014447131&doi=10.1016%2fj.jpdc.2017.02.003&partnerID=40&md5=ce0a95846dd9b4e837d7a1f849989126","With the growing scale of HPC applications, there has been an increase in the number of interruptions as a consequence of hardware failures. The remarkable decrease of Mean Time Between Failures (MTBF) in current systems encourages the research of suitable fault tolerance solutions. Message logging combined with uncoordinated checkpoint compose a scalable rollback-recovery solution. However, message logging techniques are usually responsible for most of the overhead during failure-free executions. Taking this into consideration, this paper proposes the Hybrid Message Pessimistic Logging (HMPL) which focuses on combining the fast recovery feature of pessimistic receiver-based message logging with the low failure-free overhead introduced by pessimistic sender-based message logging. The HMPL manages messages using a distributed controller and storage to avoid harming system's scalability. Experiments show that the HMPL is able to reduce overhead by 34% during failure-free executions and 20% in faulty executions when compared with a pessimistic receiver-based message logging. © 2017 Elsevier Inc.","Availability; Fault tolerance; Message logging; MPI; Performance; Scalability"
"Precise shape matching of large shape datasets using hybrid approach","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019831673&doi=10.1016%2fj.jpdc.2017.04.004&partnerID=40&md5=aef2fcd8493fbdc2a3edba11a47bd04b","Precise and fast shape matching and retrieval from very large datasets is a challenging task because of the existence of many distortions such as noise, occlusion and affine distortions. In this paper, we aim to propose a time-saving and effective shape matching and retrieval framework, that employs pruning which will enable online shape retrieval from extremely large datasets. First, using a hierarchical tree-based structure supporting parallel processing and efficient feature descriptors, irrelevant shapes are pruned and a subset of shapes relevant to the query is selected, then using more sophisticated feature descriptors, accurate retrieval is performed. Contour representation of an object is considered as most significant visual shape similarity measure by the humans. Using boundary information, we generate two simplified and efficient feature descriptors for fast pruning, and a sophisticated feature descriptor for effective and accurate retrieval. Tests performed on standard datasets unveil that the proposed technique is computationally more efficient than the state-of-the-art techniques while maintaining comparable matching and retrieval performance. Its performance is scalable for huge datasets and is robust against affine transformations, articulations and occlusion. © 2017 Elsevier Inc.","Contour-based shape matching; Hybrid shape matching framework; Online retrieval; Pruning; Shape distance; Very large datasets"
"ReduxSTM: Optimizing STM designs for Irregular Applications","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019617117&doi=10.1016%2fj.jpdc.2017.04.009&partnerID=40&md5=f611468cd910818aba3a94355a17ea82","The exploitation of optimistic concurrency in modern multicore architectures via Transactional Memory (TM) is becoming a mainstream programming paradigm. TM features can be leveraged to provide support for speculative parallel execution of irregular applications, characterized by a lack of knowledge about data dependences at compile-time. This work is focused on software TM (STM) solutions and how they can be adapted and optimized to deal efficiently with irregular memory access patterns, mainly those caused by reduction operations. With this aim, ReduxSTM is introduced as a specific STM system designed by combining techniques for speculative execution with TM algorithms. ReduxSTM is based on three main design aspects: a transactional commit order mechanism which is available to guarantee sequential semantics when needed; a specific transactional memory primitive defined for expressing commutative and associative operations (reductions) that leverages the underlying TM privatization mechanism to avoid unnecessary transaction aborts caused by reduction memory patterns; and an enhanced conflict resolution mechanism that takes advantage of the two previous features. © 2017 Elsevier Inc.","Irregular applications; Reduction operations; Software transactional memory; Thread level speculation"
"One step at a time: Parallelism in an introductory programming course","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009826121&doi=10.1016%2fj.jpdc.2016.12.024&partnerID=40&md5=71f4a847e203c7b652806886627b530b","By introducing parallelism in introductory programming courses, all computer science students can receive a basic understanding of this crucial topic. Such an early introduction, however, has many challenges. This paper first presents a fall 2013 comparison of two Computer Science I (CS1) sections, leading to a conclusion emphasizing the importance of devoting sufficient time to a sufficiently small set of parallelism topics. Six additional CS1 sections are then considered, offered from spring 2014 through spring 2016 by three different instructors. Five of these removed coverage of Java thread programming due to challenges found in fall 2013, only to show measurably reduced effectiveness of the parallelism module. Thus a new thread programming integration strategy is presented, as done in spring 2016. This strategy includes active out-of-class activities that split the disparate challenges of Java thread programming into distinct exercises. Results demonstrate improved student interest and learning. © 2017 Elsevier Inc.","CS1; Education; Introductory programming; Java; Multi-threading; Parallelism"
"Accelerating distributed Expectation–Maximization algorithms with frequent updates","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027418885&doi=10.1016%2fj.jpdc.2017.07.005&partnerID=40&md5=45328eea307f8889785ff60f6b0e4b14","Expectation–Maximization (EM) is a popular approach for parameter estimation in many applications, such as image understanding, document classification, and genome data analysis. Despite the popularity of EM algorithms, it is challenging to efficiently implement these algorithms in a distributed environment for handling massive data sets. In particular, many EM algorithms that frequently update the parameters have been shown to be much more efficient than their concurrent counterparts. Accordingly, we propose two approaches to parallelize such EM algorithms in a distributed environment so as to scale to massive data sets. We prove that both approaches maintain the convergence properties of the EM algorithms. Based on the approaches, we design and implement a distributed framework, FreEM, to support the implementation of frequent updates for the EM algorithms. We show its efficiency through two categories of EM applications, clustering and topic modeling. These applications include k-means clustering, fuzzy c-means clustering, parameter estimation for the Gaussian Mixture Model, and variational inference for Latent Dirichlet Allocation. We extensively evaluate our framework on both a cluster of local machines and the Amazon EC2 cloud. Our evaluation shows that the EM algorithms with frequent updates implemented on FreEM can converge much faster than those implementations with traditional concurrent updates. © 2017 Elsevier Inc.","Clustering; Concurrent updates; Distributed framework; Expectation–Maximization; Frequent updates; Topic modeling"
"High Performance Computing education in an Indian engineering institute","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011260737&doi=10.1016%2fj.jpdc.2017.01.019&partnerID=40&md5=d7275e2aaa0b01d5f0dcf1a1687a3015","Quality of education can be enriched with better outcome in undergraduate (UG) computer science and engineering program with the introduction of High Performance Computing (HPC) courses into the UG curriculum. The paper gives the background of the author's institution, processes of curricula enhancement, infrastructure development to enable students to carry out the work and records the students’ achievements academic year wise. The paper also discusses the evolution and continuous upgrading of the HPC courses along with the issues and challenges experienced. The paper gives the details about the courses along with the process of teaching and evaluation. It lists the students’ research achievements with a focus on HPC, for each academic year. Although the paper has focused on UG HPC education for the past seven academic years, the author also provides additional information about HPC subjects introduced to postgraduate (PG) students of computer science and engineering at various levels in the academic year 2015–16. The paper aims to serve as reference for those faculty and students who wish to introduce similar HPC courses at their schools by making use of the best practices that helped us in providing better inputs to our students to attain laudable results. © 2017 Elsevier Inc.","High Performance Computing (HPC) courses; Infrastructure development; Student results; Teaching and evaluating students; Undergraduate and postgraduate engineering curricula; Upgrading curricula"
"A novel hardware support for heterogeneous multi-core memory system","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016627436&doi=10.1016%2fj.jpdc.2017.02.008&partnerID=40&md5=1684d7d3865069b546190c54172d25db","Memory technology is one of the cornerstones of heterogeneous multi-core system efficiency. Many memory techniques are developed to give good performance within the lowest possible energy budget. These technologies open new opportunities for the memory system architecture that serves as the primary means for data storage and data sharing between multiple heterogeneous cores. In this paper, we study existing state of the art memories, discuss a conventional memory system and propose a novel hardware mechanism for heterogeneous multi-core memory system called Pattern Aware Memory System (PAMS). The PAMS supports static and dynamic data structures using descriptors and specialized scratchpad memory. In order to prove the proposed memory system, we implemented and tested it on real-prototype and simulator environments. The benchmarking results on real-prototype hardware show that PAMS achieves a maximum speedup of 12.83x and 1.23x for static and dynamic data structures respectively. When compared to the Baseline System, the PAMS consumes up to 4.8 times less program memory for static and dynamic data structures respectively. The PAMS consumes 4.6% and 1.6 times less dynamic power and energy respectively. The results of simulator environment show that the PAMS transfers data-structures up to 5.12x faster than the baseline system. © 2017 Elsevier Inc.","Controller; Heterogeneous; HPC"
"A low-area unified hardware architecture for the AES and the cryptographic hash function Grøstl","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016069832&doi=10.1016%2fj.jpdc.2017.01.029&partnerID=40&md5=c703178260e276bc27d5598efb82a585","This article describes the design of a compact 8-bit coprocessor for the Advanced Encryption standard (AES) (encryption, decryption, and key expansion) and the cryptographic hash function Grøstl. Our Arithmetic and Logic Unit has only one instruction that allows for implementing AES encryption, AES decryption, AES key expansion, and Grøstl at all levels of security (i.e. 128-, 192-, and 256-bit encryption keys; 256- and 512-bit message digests). A fully autonomous implementation of Grøstl and AES on a Virtex-6 FPGA requires 169 slices and a single 36k memory block, and achieves a competitive throughput (up to 217 Mbits/s and 92 Mbits/s for encryption and hashing, respectively). The proposed coprocessor is well-suited for resource-constrained embedded systems, where several security protocols rely only on block ciphers and hash functions. One can exploit the design philosophy presented in this paper in order to design a unified architecture for other algorithms. © 2017 Elsevier Inc.","Ciphers; Coprocessors; Cryptographic hash functions; Cryptography; Embedded systems; Field programmable gate arrays"
"Patternlets — A teaching tool for introducing students to parallel design patterns","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011589697&doi=10.1016%2fj.jpdc.2017.01.008&partnerID=40&md5=821f023c9bc0013c48c57e512f98422b","Thanks to the ubiquity of multicore processors, today's CS students must be introduced to parallel computing or they will be ill prepared as modern software developers. Professional developers of parallel software think in terms of parallel design patterns, which are markedly different from traditional (sequential) design patterns. It follows that the more we can teach students to think in terms of parallel patterns, the more their thinking will resemble that of parallel software professionals. In this paper, we present patternlets—minimalist, scalable, syntactically correct programs, each designed to introduce students to a particular parallel design pattern. The collection currently includes 44 patternlets (16 MPI, 17 OpenMP, 9 Pthreads, and 2 heterogeneous), of which we present a representative sample. We also present data that indicate the use of patternlets to introduce parallelism in CS2 produced a modest improvement in student understanding of parallel concepts. © 2017 Elsevier Inc.","Design patterns; Education; MPI; Multiprocessing; Multithreading; OpenMP; Parallel; Patternlets; Teaching; Threads"
"Scheduling parallel and distributed processing for automotive data stream management system","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024391523&doi=10.1016%2fj.jpdc.2017.06.012&partnerID=40&md5=6a256728295cd898ced0900503923cda","In this paper, to analyze end-to-end timing behavior in heterogeneous processor and network environments accurately, we adopt and modify a heterogeneous selection value on communication contention (HSV_CC) algorithm, which can synchronize tasks and messages simultaneously, for stream processing distribution. In order to adapt the concepts of a static algorithm like HSV_CC to automotive data stream management system (DSMSs), one must first address three issues: (i) previous task and message schedules might lead to less efficient resource usages in this scenario; (ii) the conventional method to determine the task scheduling order may not be best suited to deal with stream processing graphs, and; (iii) there is a need to be able to schedule tasks with time-varying computational requirements efficiently. To address (i), we propose the heterogeneous value with load balancing and communication contention (HVLB_CC) (A) algorithm, which considers load balancing in addition to the parameters considered by the HSV_CC algorithm. We propose HVLB_CC (B) to address issue (ii). HVLB_CC (B) can deal with stream processing task graphs and more various directed acyclic graphs to prevent assigning a higher priority to successor tasks. In addition, to address issue (iii), we propose HVLB_CC_IC. To schedule tasks more efficiently with various computation times, HVLB_CC_IC utilizes schedule holes left in processors. These idle time slots can be used for the execution of an optional part to generate more precise data results by applying imprecise computation models. Experimental results demonstrate that the proposed algorithms improve minimum schedule length, accuracy, and load balancing significantly compared to the HSV_CC algorithm. In addition, the proposed HVLB_CC (B) algorithm can schedule more varied task graphs without reducing performance, and, using imprecise computation models, HVLB_CC_IC yields higher precision data than HVLB_CC without imprecise computation models. © 2017 Elsevier Inc.","Automotive data stream management system; Heterogeneous processor and network; Imprecise computation; List scheduling; Load balancing"
"Pedagogy and tools for teaching parallel computing at the sophomore undergraduate level","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009726483&doi=10.1016%2fj.jpdc.2016.12.026&partnerID=40&md5=73486f8a9da78f8f5a85ab9640f8f5c3","As the need for multicore-aware programmers rises in both science and industry, Computer Science departments in universities around the USA are having to rethink their parallel computing curriculum. At Rice University, this rethinking took the shape of COMP 322, an introductory parallel programming course that is required for all Bachelors students. COMP 322 teaches students to reason about the behavior of parallel programs, educating them in both the high level abstractions of task-parallel programming as well as the nitty gritty details of working with threads in Java. In this paper, we detail the structure, principles, and experiences of COMP 322, gained from 6 years of teaching parallel programming to second-year undergraduates. We describe in detail two particularly useful tools that have been integrated into the curriculum: the HJlibparallel programming library and the Habanero Autograder for parallel programs. We present this work with the hope that it will help augment improvements to parallel computing education at other universities. © 2017 Elsevier Inc.","Autograding; Education; Java; JVM; MPI; Multi-threading; Parallel; Pedagogy; Tools"
"Design and evaluation of small–large outer joins in cloud computing environments","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015637447&doi=10.1016%2fj.jpdc.2017.02.007&partnerID=40&md5=57eab8918dab7c1f6edce0641bdeb5ef","Large-scale analytics is a key application area for data processing and parallel computing research. One of the most common (and challenging) operations in this domain is the join. Though inner join approaches have been extensively evaluated in parallel and distributed systems, there is little published work providing analysis of outer joins, especially in the extremely popular cloud computing environments. A common type of outer join is the small–large outer join, where one relation is relatively small and the other is large. Conventional implementations on this condition, such as one based on hash redistribution, often incur significant network communication, while the duplication-based approaches are complex and inefficient. In this work, we present a new method called DDR (duplication and direct redistribution), which aims to enable efficient small–large outer joins in cloud computing environments while being easy to implement using existing predicates in data processing frameworks. We present the detailed implementation of our approach and evaluate its performance through extensive experiments over the widely used MapReduce and Spark platforms. We show that the proposed method is scalable and can achieve significant performance improvements over the conventional approaches. Compared to the state-of-art method, the DDR algorithm is shown to be easier to implement and can achieve very similar or better performance under different outer join workloads, and thus, can be considered as a new option for current data analysis applications. Moreover, our detailed experimental results also have provided insights of current small–large outer join implementations, thereby allowing system developers to make a more informed choice for their data analysis applications. © 2017 Elsevier Inc.","Cloud computing; Outer joins; Parallel joins; Performance evaluation; Small–large joins"
"Multi-tenant virtual GPUs for optimising performance of a financial risk application","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003758624&doi=10.1016%2fj.jpdc.2016.06.002&partnerID=40&md5=93078879ac7d13e0a7c9442696f943ad","Graphics Processing Units (GPUs) are becoming popular accelerators in modern High-Performance Computing (HPC) clusters. Installing GPUs on each node of the cluster is not efficient resulting in high costs and power consumption as well as underutilisation of the accelerator. The research reported in this paper is motivated towards the use of few physical GPUs by providing cluster nodes access to remote GPUs on-demand for a financial risk application. We hypothesise that sharing GPUs between several nodes, referred to as multi-tenancy, reduces the execution time and energy consumed by an application. Two data transfer modes between the CPU and the GPUs, namely concurrent and sequential, are explored. The key result from the experiments is that multi-tenancy with few physical GPUs using sequential data transfers lowers the execution time and the energy consumed, thereby improving the overall performance of the application. © 2016 Elsevier Inc.","Acceleration-as-a-Service; Energy efficiency; GPU virtualisation; Multi-tenancy; rCUDA"
"QoS provisioning of a task-scheduling algorithm for lightweight devices","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019110512&doi=10.1016%2fj.jpdc.2017.04.010&partnerID=40&md5=07e5f86f5d821eb33050de6c5ac42ceb","Task scheduling is a main objective in operating system design. Many operating systems have been designed and applied to various systems. With the upcoming Internet of things, the requirements for the operating system will vary according to the application. We propose a round-robin-based scheduling algorithm to satisfy the quality of service (QoS) of real-time tasks. Many conventional scheduling algorithms have dealt with the real-time issue, but they still require improvement. Moreover, most assume the burst time is known beforehand, an unrealistic assumption in practical implementations. Thus, we present a heuristic method to predict the burst time of each task and a round-robin-based weighted scheduling algorithm providing QoS to real-time tasks for lightweight devices. The performance of the proposed scheduling algorithm is verified on an embedded system. Experimental results proved the QoS of this algorithm was satisfied. © 2017 Elsevier Inc.","Internet of things; Quantum time allocation; Real-time operating system; Scheduling; Task scheduler"
"Introducing computational thinking, parallel programming and performance engineering in interdisciplinary studies","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009815234&doi=10.1016%2fj.jpdc.2016.12.027&partnerID=40&md5=96053bbccb42155aa3164da7f11c12c3","Nowadays, many fields of science and engineering are evolving through the joint contribution of complementary fields. Computer science, and especially High Performance Computing, has become a key factor in the development of many research fields, establishing a new paradigm called computational science. Researchers and professionals from many different fields require knowledge of High Performance Computing, including parallel programming, to develop fruitful and efficient work in their particular field. Therefore, at Universitat Autònoma of Barcelona (Spain), an interdisciplinary Master on “Modeling for Science and Engineering” was started 5 years ago to provide a thorough knowledge of the application of modeling and simulation to graduate students in different fields (Mathematics, Physics, Chemistry, Engineering, Geology, etc.). In this Master's degree, “Parallel Programming” appears as a compulsory subject because it is a key topic for them. The concepts learned in this subject must be applied to real applications. Therefore, a complementary subject on “Applied Modeling and Simulation” has also been included. It is very important to show the students how to analyze their particular problems, think about them from a computational perspective and consider the related performance issues. So, in this paper, the methodology and the experience in introducing computational thinking, parallel programming and performance engineering in this interdisciplinary Master's degree are shown. This overall approach has been refined through the Master's life, leading to excellent academic results and improving the industry and students appraisal of this programme. © 2017 Elsevier Inc.","Agent-based models; CUDA; GPUs; Message passing; Model simulation; MPI; OpenMP; Parallel programming; Shared memory"
"MixHeter: A global scheduler for mixed workloads in heterogeneous environments","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028730773&doi=10.1016%2fj.jpdc.2017.07.007&partnerID=40&md5=4f39115985890809011c4b795675119f","As data centers and applications grow more heterogeneous, allocating the proper resources to various applications increasingly depends on understanding the tradeoffs between different allocations, because mixed workloads may benefit from different resources e.g. GPU, Solid State Drives(as SSD). However, traditional distributed programming models are designed and improved for homogeneous environments and have poor performances in current heterogeneous environments. Thus we reconsider the problem in this paper and make three contributions: (1) Through analysis of experimental results, we summarize the main reasons of poor performances are unreasonable allocation of tasks between heterogeneous nodes and improper allocation of resources to mixed workloads; (2) To resolve them, we propose a global scheduler MixHeter based on or-constraints. Or-constraints imbibes advantages of no-constraints and hard-constraints, which satisfy applications’ resource preferences when all the resources are available and do not waste the non-preferred resources when the preferred resources are occupied. The model of or-constraints is based on utility function, which can associate utility with different resource requests to represent resource preferences and maximize overall utility to improve system efficiency. (3) Finally, we prove MixHeter can greatly decrease execution time than capacity scheduler of Hadoop 2.7.3 and capacity scheduler with label-based scheduling up to 15%–60% in heterogeneous environments, especially in the condition of mixed workloads with different resource preferences. © 2017 Elsevier Inc.","Global optimization; Heterogeneous environments; Mixed workloads; Scheduling; Utility function"
"Energy-aware task assignment for mobile cyber-enabled applications in heterogeneous cloud computing","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028741784&doi=10.1016%2fj.jpdc.2017.08.001&partnerID=40&md5=a2823efa0fbd16efa745da8e5c83f491","Recent remarkable growth of mobile computing has led to an exceptional hardware upgrade, including the adoption of the multiple core processors. Along with this trend, energy consumptions are becoming greater when the computation capacity or workload grows. As one of the solutions, using cloud computing can mitigate energy costs due to the centralized computation. However, simply offloading the workloads to the remote side cannot efficiently reduce the energy consumptions when the energy costs caused by wireless communications are greater than that of on mobile devices. In this paper, we focus on the energy-saving problem and consider the energy wastes when tasks are assigned to remote cloud servers or heterogeneous core processors. Our solution aims to reduce the total energy cost of the mobile heterogeneous embedded systems by a novel task assignment to heterogeneous cores and mobile clouds. The proposed model is called Energy-Aware Heterogeneous Cloud Management (EA-HCM) model and the main algorithm is Heterogeneous Task Assignment Algorithm (HTA2). Our experimental evaluations have proved that our approach is effective to save energy when deploying heterogeneous embedded systems in mobile cloud systems. © 2017 Elsevier Inc.","Cloud computing; Cyber-enabled applications; Energy-aware; Mobile embedded systems; NP-hard; Task assignment"
"Improving the robustness and performance of parallel joins over distributed systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024925710&doi=10.1016%2fj.jpdc.2017.06.016&partnerID=40&md5=28c6f3853d4dcfc13b8fe857f1bf18ff","High-performance data processing systems typically utilize numerous servers with large amounts of memory. An essential operation in such environment is the parallel join, the performance of which is critical for data intensive operations. In many real-world workloads, data skew is omnipresent. Techniques that do not cater for the possibility of data skew often suffer from performance failures and memory problems. State-of-the-art methods designed to handle data skew propose new ways to distribute computation that avoid hotspots. However, this comes at the expense of global collection of statistics, redundant computation, duplication of data or increased network communication. In this light, performance could be further improved by removing the dependency on global skew knowledge and broadcasting. In this paper, we propose a new method called PRPQ (partial redistribution & partial query), with targets for efficient and robust joins with large datasets over high performance clusters. We present the detailed implementation of our approach and compare its performance with current implementations. The experimental results demonstrate that the proposed algorithm is scalable and robust and can also outperform the state-of-the-art approach with less network communication, figures that confirm our theoretical analysis. © 2017 Elsevier Inc.","Data skew; High performance computing; Parallel joins; Robust"
"Research-oriented teaching of PDC topics in integration with other undergraduate courses at multiple levels: A multi-year report","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010961161&doi=10.1016%2fj.jpdc.2017.01.004&partnerID=40&md5=99b0ebf3003c1242766ef6cdcf9c1fc1","Parallel and distributed computing (PDC) is finding its usage from system, algorithms, and architecture perspectives in research and industries of many domains. Due to its ever-increasing applications and benefits, the need of skilled manpower in the area of PDC is also increasing. It is felt that if the basic knowledge taught through the core course of PDC is supplemented with the discussion of PDC topics in integration with other computer science courses, then it will not only provide the students with more opportunities to ‘think in parallel’, but will also motivate them to harness the best of PDC. In this paper, we present our experiences of performing research-oriented teaching of PDC topics in integration with other undergraduate courses since 2014 to 2016 spread over multiple semesters. The courses mainly include Software Engineering, Computer Networks, Computer Architecture, Network Programming, and Network based Laboratory taught to undergraduate level students of Computer Science and Engineering. Our integration plan is encouraged from the objectives of NSF/TCPP–IEEE core curriculum initiative on PDC. Most of these courses are compulsory courses for undergraduate students of our department. In addition to the goals of the respective courses, we tried to fulfill many pedagogical goals corresponding to the PDC course. The teaching-contents of these courses have been adapted to also cover the aspects of PDC. A well-defined methodology for selection of PDC topics for integration, selection of topics for laboratory projects and home assignments, conduct of examinations (mid-term/end-term), pre-post feedback and evaluation, and other related activities has been planned. The methodology is intended to fulfill the priorly set goals and to achieve the intended learning outcomes (ILOs). This paper presents the methodology used, detailed topics and integration plan of PDC topics along with corresponding bloom levels, ILOs, evaluation strategies, and performance evaluation based on the students’ feedback and statistical analysis. Success of integration has been validated by performing statistical analysis of students’ pre-post feedback and performance in examinations. © 2017 Elsevier Inc.","Computer architecture; Computer Networks; Network based laboratory; Network programming; NSF early adopter award; PDC; Software engineering"
"Teaching concurrent and parallel programming by patterns: An interactive ICT approach","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010952937&doi=10.1016%2fj.jpdc.2017.01.010&partnerID=40&md5=5e1ac5590fb868d9abe74fc46118f77a","The use of programming patterns is considered to be a conceptual aid for programmers for developing understandable and testable concurrent and parallel code which is not only well built but also safe. By using programming patterns and their implementations as computer programs, difficult new concepts can be smoothly taught in lectures to students who before trying this teaching approach would have been reluctant to enroll on Parallel and Concurrent Programming courses. The approach presented in this paper consists in changing the traditional programming teaching and learning model to one where students are first introduced to syntactical constructs through selected introductory program code-patterns. In the theory lessons that follow, through the use of laptops with multi-core processors and access to the Virtual Campus services of our university, the students are easily able to implement and master the new concepts as they are taught. This teaching experiment was implemented to teach a concurrent and real-time programming course which is part of the computer engineering (CE) degree and taught during the third semester of the CE curriculum. Evaluation of the students’ academic performance when they had been taught with this approach revealed a 20.6% improvement in the students’ end-of-course grades. © 2017 Elsevier Inc.","Code; Concurrent programming; ICT integration; Interactive theoretical teaching; Lecturing model; Parallel programming; Patterns; Performance improvement; Students; Teaching improvement; Teaching innovation; Virtual campus"
"Coordinated cooperative task computing using crash-prone processors with unreliable multicast","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024405331&doi=10.1016%2fj.jpdc.2017.06.013&partnerID=40&md5=4e17a1626f01e773b00f05c8ee7f1216","This paper presents a new message-passing algorithm, called Do-UM, for distributed cooperative task computing in synchronous settings where processors may crash, and where any multicasts (or broadcasts) performed by crashing processors are unreliable. We specify the algorithm, prove its correctness and analyse its complexity. We show that its worst case available processor steps is S=Θt+n [Formula presented] +f(n−f) and that the number of messages sent is less than n2t+ [Formula presented], where n is the number of processors, t is the number of tasks to be executed and f is the number of failures. To assess the performance of the algorithm in practical scenarios, we perform an experimental evaluation on a planetary-scale distributed platform. This also allows us to compare our algorithm with the currently best algorithm that is, however, explicitly designed to use reliable multicast; the results suggest that our algorithm does not lose much efficiency in order to cope with unreliable multicast. © 2017 Elsevier Inc.","Crash faults; Fault-tolerant distributed algorithms; Task computing; Unreliable multicast"
"DSPONE48: A methodology for automatically synthesize HDL focus on the reuse of DSP slices","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011999976&doi=10.1016%2fj.jpdc.2017.01.021&partnerID=40&md5=6def7ed5ac66675b4d7a9114ed65cfaa","This work proposes a methodology to synthesize arithmetic operations maximizing the reuse of the DSP48E1 blocks presented in the new reconfigurable architectures. The input for DSPONE48 is a VHDL code without any reference to the FPGA hardware resources. This input code is modified, so the synthesis tool is able to implement it with DSP slices. In order to achieve this objective we use DSP block instantiation templates and we encourage the use of SIMD mode within the DSP block. This methodology replaces automatically the most common arithmetic operations by their equivalents on DSP slices. The methodology guarantees that the new code preserves the functionality and the number of execution cycles of the original design. Experimental results, on a Virtex 7 FPGA, show that the designs obtained by DSPONE48 use less DSPs than those obtained automatically by Xilinx ISE or Vivado. Moreover, these designs have lower area and higher frequency. © 2017 Elsevier Inc.","DSP; FPGA; System-level Design tools; VHDL"
"A quantitative roofline model for GPU kernel performance estimation using micro-benchmarks and hardware metric profiling","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019045521&doi=10.1016%2fj.jpdc.2017.04.002&partnerID=40&md5=5d3cb23f27850cd3839b59b511e5da05","Typically, the execution time of a kernel on a GPU is a difficult to predict measure as it depends on a wide range of factors. Performance can be limited by either memory transfer, compute throughput or other latencies. In this paper, we improve on the roofline model following a quantitative approach and present a completely automated GPU performance prediction technique. In this respect this model utilizes micro-benchmarking and profiling in a “black box” fashion as no inspection of source/binary code is required. The proposed model combines parameters in order to characterize the performance limiting factor and to estimate execution time. In addition, we propose the quadrant-split visual representation, which captures the characteristics of multiple processors in relation to a particular kernel. We performed experiments on stencil computation (red/black SOR), SGEMM and a total of 28 kernels of the Rodinia benchmark suite, using six CUDA GPUs and we showed an absolute error in predictions of 27.66% in the average case. Furthermore, the performance model was also examined on an AMD GPU through the HIP programming environment. Prediction errors were comparable despite the significant architectural differences between different vendor GPUs. © 2017 Elsevier Inc.","CUDA; GPU computing; HIP; Micro-benchmarks; Performance estimation; Performance model"
"An efficient and secure recoverable data aggregation scheme for heterogeneous wireless sensor networks","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026754252&doi=10.1016%2fj.jpdc.2017.06.019&partnerID=40&md5=04e8b1f1df7fe70f9cebfc020953a757","In wireless sensor networks, data aggregation plays an important role in reducing energy consumption. Recently, research has focused on secure data aggregation due to the open and hostile environment deployed. The Homomorphic Encryption (HE) scheme is extensively used to protect data confidentiality. However, HE-based data aggregation schemes have the following drawbacks: malleability, unauthorized aggregation, and limited aggregation functions. To solve these problems, we propose a secure data aggregation scheme by combining homomorphic encryption technology with a signature scheme. The proposed scheme makes three contributions. First, it achieves in-network false data filtering to avoid the consumption of unnecessary transmission energy. Second, it ensures authorized aggregation and the base station can identify the origin and validity of the messages received. Finally, the base station can recover the original sensing data, and thus can perform arbitrary aggregation operations. Comprehensive analyses and comparisons were conducted to prove the feasibility and efficiency of our scheme. The results show that our scheme performs effectively in terms of communication overhead, computational overhead, energy consumption, and delay. Additionally, even though the size of the aggregated results is large, our scheme enables the base station to quickly decrypt ciphertexts and obtain the original data unlike its counterparts. © 2017","Homomorphic encryption; Recoverable; Secure data aggregation; Wireless sensor networks"
"A massively parallel Grammatical Evolution technique with OpenCL","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025648230&doi=10.1016%2fj.jpdc.2017.06.017&partnerID=40&md5=0aaa52d79ecba94acc1f23ff57af735b","Grammatical Evolution (GE) is a bio-inspired metaheuristic capable of evolving programs in an arbitrary language using a formal grammar. Among the major applications of the technique, the automatic inference of models from data can be highlighted. As with other genetic programming techniques, GE has a high computational cost. However, the algorithm has steps that can be computed independently, enabling the use of parallel computing to reduce the execution time and, consequently, making it possible its application to larger and more complex problems. Here, models of massively parallel computation for GE are studied and proposed using OpenCL, a framework for the creation of parallel algorithms in heterogeneous computing environments. Computational experiments were conducted to analyze the performance of an implementation using GPUs (Graphics Processing Units), when compared to a sequential implementation in CPUs (Central Processing Units). Finally, speedups of up to 528× were achieved, when all steps are performed in parallel in a GPU. © 2017 Elsevier Inc.","Genetic programming; Grammatical evolution; OpenCL; Parallel computing"
"A local average broadcast gossip algorithm for fast global consensus over graphs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024827293&doi=10.1016%2fj.jpdc.2017.05.008&partnerID=40&md5=ca4df6cb497c2b235421e79f4763eb1b","Motivated by applications to wireless sensor, peer-to-peer, and social networks, the canonical average consensus problem is considered in random and regular graphs in this paper. A local average information exchange (LAIE) algorithm is developed to compute the global consensus of the initial measurements of the nodes at every node in the network. In the proposed algorithm, each node interacts with all of its neighboring nodes in each round of the diffusion process to compute and exchange the local average value, such that all nodes can asymptotically reach a global consensus in a distributed manner very quickly. This is in contrast to the conventional random gossip scheme, where each node only interacts with one of its neighboring nodes, leading to very long convergence time. Results show that in a random graph with n nodes, the convergence time of the LAIE algorithm is bounded below by Ω [Formula-presented],1 where the parameter Δ denotes the largest degree of the graphs. When a network has n nodes represented by d-regular topology graphs (d>2, where each node has the same number of neighbors d, the convergence time of the LAIE algorithm is bounded below by Θ [Formula-presented]. This shows that the proposed algorithms can achieve quicker convergence to the global consensus than other schemes based on the classic random gossip algorithm. Finally, we assess and compare the communication cost of the local average algorithm to achieve consensus through numerical results. © 2017 Elsevier Inc.","Broadcasting; Consensus; d-regular graph; Local average; Random graph"
"Parallelization of a bound-consistency enforcing procedure and its application in solving nonlinear systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019100226&doi=10.1016%2fj.jpdc.2017.03.009&partnerID=40&md5=7ccd716d67ec1e14165dab1dad63bacd","This paper considers incorporating a bound-consistency enforcing procedure to an interval branch-and-prune method. A heuristic to decide, when to use the developed operator, is proposed. As enforcing the bound-consistency is much more time consuming than performing other narrowing tools, we parallelize the procedure, using Intel TBB. A few parallelization versions are considered. Also, this is a good opportunity to make a case-study of performance of various lock instances, implemented in the TBB package. Numerical results for typical benchmark problems are presented and analyzed. A specific lock version, proper for the application, is proposed. Performance on two architectures is considered: Intel Xeon and Intel Xeon Phi (MIC). © 2017 Elsevier Inc.","Big reader lock; Bound-consistency; Interval computations; MIC; Multithreaded programming; Nonlinear equations systems; Readers-writer lock; TBB"
"A tasks reordering model to reduce transfers overhead on GPUs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024362468&doi=10.1016%2fj.jpdc.2017.06.015&partnerID=40&md5=fc571e16484f58fbb7a7e872a8c84d33","The compute capabilities of current GPUs allow exploiting concurrency when several independent tasks are simultaneously launched. These tasks are typically composed by data transfer commands and kernel computation commands. In this paper we develop a run-time approach to optimize the concurrency between data transfers and kernel computation operations in a multithreaded scenario where each CPU thread is sending tasks to the GPU. Our solution is based on a temporal execution model for concurrent tasks that is able to establish the tasks execution order that minimizes the total execution time, including data transfers. Moreover, a heuristic to select the best order has been developed, which is able to improve the execution time achieved by the hardware scheduler of current NVIDIA cards. Our approach obtains performance improvements, under real workloads, of up to 19% with respect to the execution using multiple hardware queues managed by Hyper-Q. © 2017 Elsevier Inc.","Concurrency; GPU; Hyper-Q; Streams; Tasks scheduling"
"Poly-logarithmic adaptive algorithms require revealing primitives","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021122681&doi=10.1016%2fj.jpdc.2017.05.010&partnerID=40&md5=447129eb681e6774194dfb11429f334a","This paper studies the step complexity of adaptive algorithms, depending on the revealing properties of the primitives used, namely, how many processes are revealed when concurrently applying the primitive (in specific situations). When only 0- or 1-revealing primitives are used, e.g., reads, writes, test&set, compare&swap and LL/SC, then for any collect algorithm there is an execution in which k processes collectively perform Ω(k2) steps, provided k∈O(loglogn). This implies that any adaptive collect algorithm has an Ω(k) amortized (and hence, worst-case) step complexity in an execution with total contention k∈O(loglogn). The lower bound applies for snapshot and renaming, both one-shot and long-lived. While there are snapshot algorithms whose step complexity is polylogarithmic in n using only reads and writes, there is no adaptive algorithm whose step complexity is polylogarithmic in the contention, even when compare&swap and LL/SC are used. Primitives like fetch&inc are more revealing, and using them admits snapshot algorithms with O(logk) step complexity, where k is the total or the point contention. These algorithms combine a renaming algorithm with a mechanism for propagating values so they can be quickly collected. The main implication of these results is that the step complexity of adaptive algorithms depends on the revealing power of the primitives used. Even conditional primitives that allow to solve consensus for any number of processes, like compare&swap and LL/SC, do not improve the step complexity of adaptive algorithms. © 2017 Elsevier Inc.","Atomic snapshot; Collect; Compare&swap; Fetch&inc; Renaming"
"Dynamic and discrete cache insertion policies for managing shared last level caches in large multicores","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016451916&doi=10.1016%2fj.jpdc.2017.02.004&partnerID=40&md5=b0248bffd53e72fe4d8184dc5e6baa79","Multi-core processors employ shared Last Level Caches (LLC). This trend will continue in the future with large multi-core processors (16 cores and beyond) as well. At the same time, the associativity of LLC tends to remain in the order of sixteen. Consequently, with large multicore processors, the number of applications or threads that share the LLC becomes larger than the associativity of the cache itself. LLC management policies have been extensively studied for small scale multi-cores (4–8 cores) and associativity degree in the 16 range. However, the impact of LLC management on large multi-cores is essentially unknown, in particular when the associativity degree is smaller than the number of applications. In this study, we introduce Adaptive Discrete and deprioritized Application PrioriTization (ADAPT), an LLC management policy addressing the large multi-cores where the LLC associativity degree is smaller than the number of applications. ADAPT builds on the use of the Footprint-number metric. We propose a monitoring mechanism that dynamically samples cache sets to estimate the Footprint-number of applications and classify them into discrete (distinct and more than two) priority buckets. The cache replacement policy leverages this classification and assigns priorities to cache lines of applications during cache replacement operations. We further find that de-prioritizing certain applications during cache replacement is beneficial to the overall performance. We evaluate our proposal on 16, 20 and 24-core multi-programmed workloads and discuss other aspects in detail. © 2017 Elsevier Inc.","Bypassing; Cache management; Discrete priorities; Footprint-number; More cores than associativity; Shared resource"
"Segment access-aware dynamic semantic cache in cloud computing environment","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019604063&doi=10.1016%2fj.jpdc.2017.04.011&partnerID=40&md5=d4c5fa2cdebff374c958c55e3d81a745","In recent years, researches focus on addressing the query bottleneck issue using semantic cache. However, the challenges of this method are how to increase cache hit ratio, decrease the query processing time, and address cache consistency issue. In this paper, we construct segment access-aware dynamic semantic cache for relational databases. Some definitions of semantic segment, probe query, and remainder query are proposed to describe the semantic cache. Then, estimation of the query result is proposed. Next, cache access algorithm of our proposed segment access-aware dynamic semantic cache is presented in case of cache exact hit, cache extended hit, cache partial hit and cache miss. Cache item with effective lifecycle tag is proposed to address cache consistency issue. Finally, experimental results show that this approach performs better than regular semantic cache and decisional semantic cache. © 2017 Elsevier Inc.","Cache hit; Cache miss; Data consistency; Semantic cache"
"Analysis of the efficiency characteristics of the first High-Temperature Direct Liquid Cooled Petascale supercomputer and its cooling infrastructure","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019237081&doi=10.1016%2fj.jpdc.2017.04.005&partnerID=40&md5=af14e9856c69f3e0d51a8e41406610b8","SuperMUC, deployed at the Leibniz Supercomputing Centre, is the first High-Temperature (ASHRAE W4 chiller-less) DirectLiquid Cooled (HT-DLC) Petascale supercomputer installed worldwide. Chiller-less direct liquid cooling can save data centers a substantial amount of energy by reducing data center cooling overheads. An essential question remains unanswered — how to determine an optimal operational environment for balancing scientific discovery with the energy consumption of both the supercomputer and the cooling infrastructure? This paper shows, for the first time, how the new technologies (HT-DLC and chiller-less cooling) influence the performance and energy/power efficiency of large-scale HPC applications and how different inlet temperatures affect the overall system power consumption and the HT-DLC efficiency. © 2017 Elsevier Inc.","Cooling efficiency; Energy-efficiency; High-Temperature Direct Liquid Cooling (HT-DLC); HPC; System characteristic"
"Scalable communication event tracing via clustering","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021912379&doi=10.1016%2fj.jpdc.2017.06.008&partnerID=40&md5=0e60b746a4eb3a03a8692829a1d25347","Communication traces help developers of high-performance computing (HPC) applications understand and improve their codes. When run on large-scale HPC facilities, the scalability of tracing tools becomes a challenge. To address this problem, traces can be clustered into groups of processes that exhibit similar behavior. Instead of collecting trace information of each individual node, it then suffices to collect a trace of a small set of representative nodes, namely one per cluster. However, clustering algorithms themselves need to have low overhead, be scalable, and adapt to application characteristics. We devised an adaptive clustering algorithm for large-scale applications called ACURDION that traces the MPI communication of code with O(log P) time complexity. First, ACURDION identifies the parameters that differ across processes by using a logarithmic algorithm called Adaptive Signature Building. Second, it clusters the processes based on those parameters. Experiments show that collecting traces of just nine nodes/clusters suffices to capture the communication behavior of all nodes for a wide set of HPC benchmarks codes while retaining sufficient accuracy of trace events and parameters. In summary, ACURDION improves trace scalability and automation over prior approaches. © 2017 Elsevier Inc.","Clustering algorithms; Concurrent programming; Performance measurement; Programming techniques"
"Efficient optimization approach for fast GPU computation of Zernike moments","2018","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028702546&doi=10.1016%2fj.jpdc.2017.07.008&partnerID=40&md5=d830d97e03c773dc1d8ac0aea3f9f63b","Our study focuses on accelerating the computation of Zernike moments on graphics processing units (GPUs). There are two ideas to achieve the goal. First is to implement a novel re-layout that involves reordering the image pixels and addressing the diagonal pixels in advance, so that computations of all pixels are allocated to an octant effectively. Second is to the leverage the constant memory to store precomputed values used across GPU threads. An in-depth study has been carried out to evaluate the performance in each case and to compare against GPU implementation of other algorithms and to discuss the bottleneck. The result shows that our approach is effective and achieves significant performance improvement compared to other GPU state-of-the-art implementations. Furthermore, our approach is suited for allocating the data flow into multiple GPUs. © 2017 Elsevier Inc.","Addressing diagonal in advance; GPU; Reordering image pixels; Zernike moments"
"Modeling, analysis, and experimental comparison of streaming graph-partitioning policies","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962163332&doi=10.1016%2fj.jpdc.2016.02.003&partnerID=40&md5=8c5db0b11eb66de6417686f17d250660","In recent years, many distributed graph-processing systems have been designed and developed to analyze large-scale graphs. For all distributed graph-processing systems, partitioning graphs is a key part of processing and an important aspect to achieve good processing performance. To keep low the overhead of partitioning graphs, even when processing the ever-increasing modern graphs, many previous studies use lightweight streaming graph-partitioning policies. Although many such policies exist, currently there is no comprehensive study of their impact on load balancing and communication overheads, and on the overall performance of graph-processing systems. This relative lack of understanding hampers the development and tuning of new streaming policies, and could limit the entire research community to the existing classes of policies. We address these issues in this work. We begin by modeling the execution time of distributed graph-processing systems. By analyzing this model under the load of realistic graph-data characteristics, we propose a method to identify important performance issues and then design new streaming graph-partitioning policies to address them. By using three typical large-scale graphs and three popular graph-processing algorithms, we conduct comprehensive experiments to study the performance of our and of many alternative streaming policies on a real distributed graph-processing system. We also explore the impact on performance of using different real-world networks and of other real-world technical details. We further discuss how to use our results, the coverage of our model and method, and the design of future partitioning policies. © 2016 Elsevier Inc.","Graph-processing systems; Large-scale graphs; Modeling analysis; Performance evaluation; Streaming graph-partitioning policies"
"The nine node Extrapolated Diffusion method for weighted torus graphs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017250460&doi=10.1016%2fj.jpdc.2017.02.012&partnerID=40&md5=f2e02cfda73175986538fe6d9e5d2409","The convergence analysis of the Extrapolated Diffusion (EDF) method was developed in Karagiorgos and Missirlis (2008) and Markomanolis and Missirlis (2010) for 2D weighted torus and mesh graphs, respectively using the set N1(i) of the nearest neighbors of a source node i in the graph. In the present work we propose a Diffusion scheme which employs the set N1(i)∪N2(i), where N2(i) denotes the set of the nearest neighbors of a source node i with path length two in an attempt to improve the performance of the new method. We develop the convergence analysis of the new EDF method by considering two subsets of N2(i) for 2D weighted torus graphs. In particular, we study five different communication routes for computing the load of each node i in the torus graph and for each route we find closed form formulae for the optimum values of the edge weights, the extrapolation parameters and the convergence factor of the new EDF scheme. A comparison of the convergence factors of all these EDF schemes reveals a 60% improvement in the performance using the cross communication route compared to the conventional EDF method, a fact which is shown theoretically and experimentally. © 2017 Elsevier Inc.","Fourier analysis; Iterative diffusion; Laplacian matrix; Load balancing; Weighted torus"
"Massively parallel first-principles simulation of electron dynamics in materials","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015690286&doi=10.1016%2fj.jpdc.2017.02.005&partnerID=40&md5=2f49bce286b33da65cefff00035689b9","We present a highly scalable, parallel implementation of first-principles electron dynamics coupled with molecular dynamics (MD). By using optimized kernels, network topology aware communication, and by fully distributing all terms in the time-dependent Kohn–Sham equation, we demonstrate unprecedented time to solution for disordered aluminum systems of 2000 atoms (22,000 electrons) and 5400 atoms (59,400 electrons), with wall clock time as low as 7.5 s per MD time step. Despite a significant amount of non-local communication required in every iteration, we achieved excellent strong scaling and sustained performance on the Sequoia Blue Gene/Q supercomputer at LLNL. We obtained up to 59% of the theoretical sustained peak performance on 16,384 nodes and performance of 8.75 Petaflop/s (43% of theoretical peak) on the full 98,304 node machine (1,572,864 cores). Scalable explicit electron dynamics allows for the study of phenomena beyond the reach of standard first-principles MD, in particular, materials subject to strong or rapid perturbations, such as pulsed electromagnetic radiation, particle irradiation, or strong electric currents. © 2017","Blue Gene/Q; Electron dynamics; Explicit time integration; TDDFT"
"Designing and implementing a heuristic cross-architecture combination for graph traversal","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973547924&doi=10.1016%2fj.jpdc.2016.05.007&partnerID=40&md5=da7152edfa7b39659fbb0c0776803309","Breadth-First Search (BFS) is widely used in real-world applications including computational biology, social networks, and electronic design automation. The most effective BFS approach has been shown to be a combination of top-down and bottom-up approaches. Such hybrid techniques need to identify a switching point which is conventionally found through expensive trial-and-error and exhaustive search routines. We present an adaptive method based on regression analysis that enables dynamic switching at runtime with little overhead. We improve the performance of our method by exploiting popular heterogeneous platforms and efficiently design the approach for a given architecture. A 155× speedup is achieved over the standard top-down approach on GPUs. Our approach is the first to combine top-down and bottom-up across different architectures. Unlike combination on a single architecture, a mistuned switching point may significantly decrease the performance of cross-architecture combination. Our adaptive method can predict the switching point with high accuracy, leading to 7× speedup compared to the switching point in average case (1000 switching points). © 2016 Elsevier Inc.","Combination; Cross-architecture optimization; Data-intensive; Graph algorithm; Kepler K20x GPU; Knights corner MIC; Regression analysis"
"Domain decomposition approach for parallel improvement of tetrahedral meshes","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019372287&doi=10.1016%2fj.jpdc.2017.04.008&partnerID=40&md5=9b241de4ff2b0977cc170f7007fd635c","Presently, a tetrahedral mesher based on the Delaunay triangulation approach may outperform a tetrahedral improver based on local smoothing and flip operations by nearly one order in terms of computing time. Parallelization is a feasible way to speed up the improver and enable it to handle large-scale meshes. In this study, a novel domain decomposition approach is proposed for parallel mesh improvement. It analyses the dual graph of the input mesh to build an inter-domain boundary that avoids small dihedral angles and poorly shaped faces. Consequently, the parallel improver can fit this boundary without compromising the mesh quality. Meanwhile, the new method does not involve any inter-processor communications and therefore runs very efficiently. A parallel pre-processing pipeline that combines the proposed improver and existing parallel surface and volume meshers can prepare a quality mesh containing hundreds of millions of elements in minutes. Experiments are presented to show that the developed system is robust and applicable to models of a complication level experienced in industry. © 2017 Elsevier Inc.","Domain decomposition; Dual graph; Mesh generation; Parallel algorithms; Quality improvement"
"On a course on computer cluster configuration and administration","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010999495&doi=10.1016%2fj.jpdc.2017.01.009&partnerID=40&md5=3591749ce27132ccc82a83e064d4e8fa","Computer clusters are today a cost-effective way of providing either high-performance and/or high-availability. The flexibility of their configuration aims to fit the needs of multiple environments, from small servers to SME and large Internet servers. For these reasons, their usage has expanded not only in academia but also in many companies. However, each environment needs a different “cluster flavour”. High-performance and high-throughput computing are required in universities and research centres while high-performance service and high-availability are usually reserved to use in companies. Despite this fact, most university cluster computing courses continue to cover only high-performance computing, usually ignoring other possibilities. In this paper, a master-level course which attempts to fill this gap is discussed. It explores the different types of cluster computing as well as their functional basis, from a very practical point of view. As part of the teaching methodology, each student builds from scratch a computer cluster based on a virtualization tool. The entire process is designed to be scalable. The goal is to be able to apply it to an actual computer cluster with a larger number of nodes, such as those the students may subsequently encounter in their professional life. © 2017 Elsevier Inc.","Computer cluster configuration and administration; Computer engineering education; Lab project"
"Prior node selection for scheduling workflows in a heterogeneous system","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021653967&doi=10.1016%2fj.jpdc.2017.06.005&partnerID=40&md5=83cd44f0d8977f17ef41adf6ebc66745","Many workflow scheduling algorithms for heterogeneous systems have been developed to satisfy multiple requirements such as minimizing schedule length while maximizing throughput. In particular, in list-based scheduling approaches, the schedule length depends on the given nodes as well as the task allocation and ordering policies. This is because the scheduling priority is derived by averaging the execution time and communication time of the given nodes. If the set of nodes can be adjusted before the scheduling tasks, a small schedule length can be achieved. In this paper, we propose a prior node selection algorithm, called lower bound based candidate node selection (LBCNS) to select a subset of given nodes to minimize the schedule length while fairly scheduling each job. Our proposal has two approaches: (i) LBCNS_DEFAULT, which considers the job characteristics and each node's performance, and (ii) priority-based LBCNS, which additionally takes each scheduling priority into account for a dedicated task scheduling algorithm. The experimental results of extensive simulations show that LBCNS_DEFAULT has the best fairness for scheduling multiple workflow jobs, while priority-based LBCNS achieves the minimum schedule length with the highest efficiency for a single workflow job and multiple workflow jobs. © 2017 Elsevier Inc.","DAG; Heterogeneous system; Node grouping; Processor grouping; Task scheduling; Workflow scheduling"
"ANMR: Aging-aware adaptive N-modular redundancy for homogeneous multicore embedded processors","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020311617&doi=10.1016%2fj.jpdc.2017.04.013&partnerID=40&md5=e8b25d8863a9b55b5c99baeaa6eef8ec","Advances in semiconductor technology have made integration of multiple processing cores into one single die a promising trend towards increasing processing performance, lowering power consumption, and increasing reliability for embedded systems. Multicore processors, due to their intrinsic redundancies, are good choices for critical embedded systems for which the reliability is a crucial component. In this paper, an aging-aware adaptive fault tolerance method for DVFS-enabled multicore processors is presented. The analytical results show 3 to 6 order of magnitude increase in reliability of the system without addition of cores or redundant software. By using an aging-aware approach, the proposed method contributes to less than 0.05% negative shift of the maximum frequency of every core in the processor. Experimental results also show that the method has less than 7% energy overhead as compared to the original mode of operation. © 2017 Elsevier Inc.","DVFS; Hard real-time systems; Multicore processors; NBTI-related aging; Reliability modeling"
"SeeMore: A kinetic parallel computer sculpture for educating broad audiences on parallel computation","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011361607&doi=10.1016%2fj.jpdc.2017.01.017&partnerID=40&md5=247d40a5069d98cc1cdc2c25400d4921","We discuss the design, implementation, and evaluation of a 256-node Raspberry-Pi cluster with kinetic properties. Each compute node is attached to a servo mechanism such that movement results from local computation. The result is SeeMore, a kinetic parallel computer sculpture designed to enable visualization of parallel algorithms in an effort to educate broad audiences as to the beauty, complexity, and importance of parallel computation. The algorithms and interfaces were implemented by students from various related courses at VA Tech. We describe these designs in sufficient detail to enable others to build their own kinetic computing sculptures to augment their experiential learning programs. Our evaluations at exhibitions indicate 63% and 84% of visitors enjoyed interacting with SeeMore while 69% and 87% believed SeeMore has educational value. © 2017","Computer science education; Kinetic art; Parallel and distributed computing"
"Hardware accelerated SAT solvers—A survey","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009410515&doi=10.1016%2fj.jpdc.2016.12.014&partnerID=40&md5=a8a182f53e13292d6d51f4062ad6aed3","Boolean Satisfiability (SAT) is a problem that holds great importance both theoretically and in practical applications. Although the general SAT problem is NP-complete, advancements in solver algorithms and heuristics have meant that many industrial problems can be quickly and efficiently solved. Much of this progress has been made in the field of sequential SAT solvers; however, there have been significant recent contributions to the field of hardware SAT accelerators. This paper offers a short overview of the current state of SAT solvers in general and surveys recent contributions to hardware accelerated SAT solvers. This paper also aims to analyze the trends, challenges, and open questions facing reconfigurable SAT solvers in an extremely competitive application area. © 2016 Elsevier Inc.","Boolean; FPGA; Logic; NP complete; SAT; Satisfiability"
"Optimization of Low-Density Parity Check decoder performance for OpenCL designs synthesized to FPGAs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018300371&doi=10.1016%2fj.jpdc.2017.04.001&partnerID=40&md5=535db76ed5989a7082941f6804630b54","Open Computing Language (OpenCL) is a high-level language that allows developers to produce portable software for heterogeneous parallel computing platforms. OpenCL is available for a variety of hardware platforms, with compiler support being recently expanded to include Field-Programmable Gate Arrays (FPGAs). This article investigates flexible OpenCL designs for the iterative min-sum decoding algorithm for (3,6)-regular Low-Density Parity Check (LDPC) codes over a range of codeword lengths. The target FPGA hardware is the Altera Stratix V GX A7 based Nallatech 385n board. The computationally demanding LDPC decoding algorithm offers several forms of parallelism that could be exploited by the Altera Offline Compiler (AOC version 15.1) for OpenCL. Our best decoder design produced a corrected codeword throughput of 68.22 Mbps at the compiler-selected FPGA clock frequency of 163.88 MHz for a length-2048 (3,6)-regular LDPC code. For a length-1024 (3,6)-regular LDPC code, our best design produced a throughput of 54.8 Mbps (32 decoding iterations) which significantly improves on the throughput of around 7 Mbps (30 decoding iterations) produced by an OpenCL based decoder design reported by Falcao et al. for the same size of LDPC code. © 2017 Elsevier Inc.","Altera-Offline Compiler; FPGA; High-level synthesis; LDPC; Low-Density Parity Check; OpenCL; Parallel algorithms"
"A research-oriented course on Advanced Multicore Architecture: Contents and active learning methodologies","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011016561&doi=10.1016%2fj.jpdc.2017.01.011&partnerID=40&md5=1be80750fb4929725a15574711cf36a6","The fast evolution of multicore processors makes it difficult for professors to offer computer architecture courses with updated contents. To deal with this shortcoming that could discourage students, the most appropriate solution is a research-oriented course based on current microprocessor industry trends. Additionally, we also seek to improve the students’ skills by applying active learning methodologies, where teachers act as guiders and resource providers while students take the responsibility for their learning. In this paper, we present the Advanced Multicore Architecture (AMA) course, which follows a research-oriented approach to introduce students in architectural breakthroughs and uses active learning methodologies to enable students to develop practical research skills such as critical analysis of research papers or communication abilities. To this end five main activities are used: (i) lectures dealing with key theoretical concepts, (ii) paper review & discussion, (iii) research-oriented practical exercises, (iv) lab sessions with a state-of-the-art multicore simulator, and (v) paper presentation. An important part of all these activities is driven by active learning methodologies. Special emphasis is put on the practical side by allocating 40% of the time to labs and exercises. This work also includes an assessment study that analyzes both the course contents and the used methodology (both of them compared to other courses). © 2017 Elsevier Inc.","Advanced computer architecture courses; Lab sessions; Research-oriented method; Teaching methods"
"A parallel approximate SS-ELM algorithm based on MapReduce for large-scale datasets","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011277363&doi=10.1016%2fj.jpdc.2017.01.007&partnerID=40&md5=e6233f91a2c56966f17117543e53892d","Extreme Learning Machine (ELM) algorithm not only has gained much attention of many scholars and researchers, but also has been widely applied in recent years especially when dealing with big data because of its better generalization performance and learning speed. The proposal of SS-ELM (semi-supervised Extreme Learning Machine) extends ELM algorithm to the area of semi-supervised learning which is an important issue of machine learning on big data. However, the original SS-ELM algorithm needs to store the data in the memory before processing it, so that it could not handle large and web-scale data sets which are of frequent appearance in the era of big data. To solve this problem, this paper firstly proposes an efficient parallel SS-ELM (PSS-ELM) algorithm on MapReduce model, adopting a series of optimizations to improve its performance. Then, a parallel approximate SS-ELM Algorithm based on MapReduce (PASS-ELM) is proposed. PASS-ELM is based on the approximate adjacent similarity matrix (AASM) algorithm, which leverages the Locality-Sensitive Hashing (LSH) scheme to calculate the approximate adjacent similarity matrix, thus greatly reducing the complexity and occupied memory. The proposed AASM algorithm is general, because the calculation of the adjacent similarity matrix is the key operation in many other machine learning algorithms. The experimental results have demonstrated that the proposed PASS-ELM algorithm can efficiently process very large-scale data sets with a good performance, without significantly impacting the accuracy of the results. © 2017 Elsevier Inc.","Approximate algorithm; Big data; LSH; MapReduce; Parallel; PASS-ELM"
"Power-performance assessment of different DVFS control policies in NoCs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021773812&doi=10.1016%2fj.jpdc.2017.06.004&partnerID=40&md5=a01019820e932906ba359a0353a09901","We analyze the power-delay trade-off in a Network-on-Chip (NoC) under three Dynamic Voltage and Frequency Scaling (DVFS) policies. The first rate-based policy sets frequency and voltage of the NoC to the minimum value that allows to sustain the injection rate without reaching saturation. The second queue-based policy uses a feedback-loop approach to throttle the NoC frequency and voltage such that the average backlog of the injection queues tracks a target value. The third delay-based policy uses a closed-loop strategy that targets a given NoC end-to-end average delay. We first show that, despite the different mechanism and implementation, both rate-based and queue-based policies obtain very similar results in terms of power and delay, and we propose a theoretical interpretation of this similarity. Then, we show that delay-based policy generally offers a better power-delay trade-off. We obtained our results with an extensive set of experiments on synthetic traffic, as well as multimedia, communications and PARSEC benchmarks. For all the experiments, we report both cycle-accurate simulation results for the analysis of NoC delay and accurate power results obtained targeting a standard-cell library in an advanced 28-nm FDSOI CMOS technology. © 2017 Elsevier Inc.","Dynamic voltage and frequency scaling; Network-on-chip"
"A parallel Quantized State System Solver for ODEs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016401186&doi=10.1016%2fj.jpdc.2017.02.011&partnerID=40&md5=1bde1fb3392a135c96fadf9c802bb4d4","This work introduces novel parallelization techniques for Quantized State System (QSS) simulation of continuous time and hybrid systems and their implementation on a multi-core architecture. Exploiting the asynchronous nature of QSS algorithms, the novel methodologies are based on the use of non-strict synchronization between logical processes. The fact that the synchronization is not strict allows to achieve large speedups at the cost of introducing additional numerical errors that, under certain assumptions, are bounded depending on some given parameters. Besides introducing the parallelization techniques, the article describes their implementation on a software tool and it presents a theoretical analysis of the aforementioned additional numerical error. Finally, the performance of the novel methodology and its implementation is deeply evaluated on four large scale models. © 2017 Elsevier Inc.","Discrete event systems; Hybrid systems; Parallel ODE simulation; QSS"
"A scalable method for link prediction in large real world networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020766621&doi=10.1016%2fj.jpdc.2017.05.009&partnerID=40&md5=973c36cc4e49aefe5bc2925c58754baa","Link prediction has become an important task, especially with the rise of large-scale, complex and dynamic networks. The emerging research area of network dynamics and evolution is directly related to predicting new interactions between objects, a possibility in the near future. Recent studies show that the precision of link prediction can be improved to a great extent by including community information in the prediction methods. As traditional community-based link prediction algorithms can run only on stand-alone computers, they are not well suited for most of the large networks. Graph parallelization can be one solution to such problems. Bulk Synchronous Parallel (BSP) programming model is a recently emerged framework for parallelizing graph algorithms. In this paper, we propose a hybrid similarity measure for link prediction in real world networks. We also propose a scalable method for community structure-based link prediction on large networks. This method uses a parallel label propagation algorithm for community detection and a parallel community information-based Adamic–Adar measure for link prediction. We have developed these algorithms using Bulk Synchronous Parallel programming model and tested them with large networks of various domains. © 2017 Elsevier Inc.","Bulk synchronous parallel; Community structure; Link prediction; Parallel computing; Social networks"
"FPGA-based multi-robot tracking","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018377853&doi=10.1016%2fj.jpdc.2017.03.008&partnerID=40&md5=4ef0eb5afc8b0b3f86660bb7ed5cc9a3","Vision-based robot tracking is commonly used for monitoring and debugging in single- and multi-robot environments. Currently, most of the existing vision-based multi-robot tracking systems are based on implementations on general purpose computers. These solutions are not feasible for embedded applications requiring high resource efficiency or high robustness as well as for use-cases with large frame sizes, multiple cameras and a large number of robots to be tracked. Field Programmable Gate Array (FPGAs)-based hardware accelerators can be used to efficiently handle compute-intensive applications like vision processing due to their high inherent parallelism. In this paper, we present an FPGA-based architecture for multi-robot tracking using multiple GigE Vision cameras. A complete system is implemented, comprising a multi-camera frame grabber and IP cores for image preprocessing, edge filtering, and circle detection. The robot localization is based on shape-based object detection. The proposed design is scalable in terms of the number of cameras and robots. It detects the locations of multiple robots simultaneously using single or multiple cameras without sacrificing the performance. Our implementation can process video frames from multiple cameras for multi-robot localization with precision and recall rates of 98%. It supports a maximum total video resolution of 2048x2048 with 152 frames per second. A speed-up of more than 30 is achieved as compared to a multi-threaded OpenCV implementation on a 3.2 GHz desktop quad-core CPU. © 2017 Elsevier Inc.","Circular Hough transform; FPGA; Image processing; Multi-camera; Multi-robot systems; Object detection and recognition; Reconfigurable systems; Vision processing"
"Learning by doing, High Performance Computing education in the MOOC era","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011115161&doi=10.1016%2fj.jpdc.2017.01.015&partnerID=40&md5=ed3016db3be3c3baabfd8fa063dc8e0f","The High Performance Computing (HPC) community has spent decades developing tools that teach practitioners to harness the power of parallel and distributed computing. To create scalable and flexible educational experiences for practitioners in all phases of a career, we turn to Massively Open Online Courses (MOOCs). We detail the design of a unique self-paced online course that incorporates a focus on parallel solutions, personalization, and hands-on practice to familiarize student–users with their target system. Course material is presented through the lens of common HPC use cases and the strategies for parallelizing them. Using personalized paths, we teach researchers how to recognize the alignment between scientific applications and traditional HPC use cases, so they can focus on learning the parallelization strategies key to their workplace success. At the conclusion of their learning path, students should be capable of achieving performance gains on their HPC system. © 2017","Hands-on learning; HPC education; Interactive supercomputing; MOOC; Open edX; Personalized digital learning; Professional education"
"SAUCE: A web application for interactive teaching and learning of parallel programming","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011101723&doi=10.1016%2fj.jpdc.2016.12.028&partnerID=40&md5=8409725a5c66a493ebf948558c3971e3","Prevalent hardware trends towards parallel architectures and algorithms create a growing demand for graduate students familiar with the programming of concurrent software. However, learning parallel programming is challenging due to complex communication and memory access patterns as well as the avoidance of common pitfalls such as dead-locks and race conditions. Hence, the learning process has to be supported by adequate software solutions in order to enable future computer scientists and engineers to write robust and efficient code. This paper discusses a selection of well-known parallel algorithms based on C++11 threads, OpenMP, MPI, and CUDA that can be interactively embedded in an HPC or parallel computing lecture using a unified framework for the automated evaluation of source code—namely the “System for AUtomated Code Evaluation” (SAUCE). SAUCE is free software licensed under AGPL-3.0 and can be downloaded at https://github.com/moschlar/SAUCE free of charge. © 2017 Elsevier Inc.","Black box testing; Parallel programming; Teaching and learning; Web application"
"Self-reconfigurable architectures for HEVC Forward and Inverse Transform","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.05.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021739934&doi=10.1016%2fj.jpdc.2017.05.017&partnerID=40&md5=a80f08103eee018c044c326643072366","This work introduces a run-time reconfigurable system for HEVC Forward and Inverse Transforms that can adapt to time-varying requirements on resources, throughput, and video coding efficiency. Three scalable designs are presented: fully parallel, semi parallel, and iterative. Performance scalability is achieved by combining folded/unfolded 1D Transform architectures and one/two transposition buffers. Resource usage is optimized by utilizing both the recursive even–odd decomposition and distributed arithmetic techniques. The architecture design supports video sequences in the 8K Ultra High Definition format (7680 × 4320) with up to 70 frames per second when using 64 × 64 Coding Tree Blocks with variable transform sizes. The self-reconfigurable embedded system is implemented and tested on a Xilinx® Zynq-7000 All-Programmable System-on-Chip (SoC). Results are presented in terms of performance (frames per second), resource utilization, and run-time hardware adaptation for a variety of hardware design parameters, video resolutions, and self-reconfigurability scenarios. The presented system illustrates the advantages of run-time reconfiguration technology on PSoCs or FPGAs for video compression. © 2017 Elsevier Inc.","Embedded systems; HEVC transform; Reconfigurable hardware; Run-time partial reconfiguration"
"Architecture level analysis for process variation in synchronous and asynchronous Networks-on-Chip","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008946246&doi=10.1016%2fj.jpdc.2016.12.019&partnerID=40&md5=ef6d93a89c601dd597e8abcb9358b10f","Synchronous NoCs suffer from performance degradation due to clock skew. Clock skew is more pronounced with process variation (PV). Although asynchronous NoCs suffer from handshaking overhead, their immunity to PV is better than synchronous networks which would favor them in terms of throughput. Architecture-Level analysis aims to determine the ability of different NoC communication schemes to mitigate the impact of PV. The proposed analysis depends on redeveloped simulator which is unique PV-aware simulator for both synchronous and asynchronous NoCs. Architecture-Level simulation shows that clock skew causes significant performance degradation in synchronous networks. Clock skew represents 27% and 32% of the delay variation for 45 nm and 32 nm technologies, respectively. Using real traffic, Architecture-Level analysis shows considerable throughput reduction for synchronous NoC under PV conditions. Throughput degradation of synchronous NoC increases rapidly with technology scaling down. 64-Cores synchronous NoC loses 30% of the nominal throughput for 45 nm technology and 41% of throughput for 32 nm with PV. On the other hand, 64-Cores asynchronous network throughput degradation is 12% and 13.6% for 45 nm and 32 nm technologies, respectively. For different NoC dimensions and using different workloads, throughput reduction for synchronous design is more than double the reduction of asynchronous design. Asynchronous scheme is preferable as technology scales. © 2016 Elsevier Inc.","Architecture level simulator; Asynchronous router; Clock skew; NoC; Process variation; Synchronous router; Throughput"
"Mapping of synchronous dataflow graphs on MPSoCs based on parallelism enhancement","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005980964&doi=10.1016%2fj.jpdc.2016.11.012&partnerID=40&md5=83b8af28e90ec7f03806974545688a4b","Multi-processor systems-on-chips are widely adopted in implementing modern streaming applications to satisfy the ever increasing computation requirements. To take advantage of this kind of platform, it is necessary to map tasks of the application properly to different processors, so as to fully exploit the inherent task-level parallelism and satisfy the stringent timing requirements. We propose the Parallelism Graph to capture the task-level parallelism of the application and transform the mapping problem to a graph partitioning problem. The graph partitioning problem is formulated as an Integer Linear Programming problem, which is solved optimally using the ILP solver. To reduce the complexity, a two-step local search algorithm, i.e., the greedy partition and refinement algorithm, is proposed. Since one-shot heuristics cannot guarantee the solution quality, evolutionary algorithms are widely used to search the solution space such that better results can be found. We also integrate the idea of parallelism enhancement into the genetic algorithm and propose a hybrid genetic algorithm to improve the performance. Sets of synthesized Synchronous Data Flow Graphs and some practical applications are used to evaluate the performance of the proposed algorithms. Experiment results demonstrate that the proposed algorithms outperform available algorithms. © 2016 Elsevier Inc.","Genetic algorithm; Graph partition; Mapping; Multiprocessor; Synchronous dataflow graph"
"Distributed host-based collaborative detection for false data injection attacks in smart grid cyber-physical system","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008640878&doi=10.1016%2fj.jpdc.2016.12.012&partnerID=40&md5=2063a98de333ea752fe68aca1ec07407","False data injection (FDI) attacks are crucial security threats to smart grid cyber-physical system (CPS), and could result in cataclysmic consequences to the entire power system. However, due to the high dependence on open information networking, countering FDI attacks is challenging in smart grid CPS. Most existing solutions are based on state estimation (SE) at the highly centralized control center; thus, computationally expensive. In addition, these solutions generally do not provide a high level of security assurance, as evidenced by recent work that smart FDI attackers with knowledge of system configurations can easily circumvent conventional SE-based false data detection mechanisms. In this paper, in order to address these challenges, a novel distributed host-based collaborative detection method is proposed. Specifically, in our approach, we use a conjunctive rule based majority voting algorithm to collaboratively detect false measurement data inserted by compromised phasor measurement units (PMUs). In addition, an innovative reputation system with an adaptive reputation updating algorithm is also designed to evaluate the overall running status of PMUs, by which FDI attacks can be distinctly observed. Extensive simulation experiments are conducted with real-time measurement data obtained from the PowerWorld simulator, and the numerical results fully demonstrate the effectiveness of our proposal. © 2016 Elsevier Inc.","Adaptive reputation system; Distributed host-based collaborative detection; False data injection attack; Smart grid cyber-physical system (CPS)"
"Towards completely fair scheduling on asymmetric single-ISA multicore processors","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007424320&doi=10.1016%2fj.jpdc.2016.12.011&partnerID=40&md5=ce66d88ab4ec6eb93844da683e1114ce","Single-ISA asymmetric multicore processors (AMPs), which combine high-performance big cores with low-power small cores, were shown to deliver higher performance per watt than symmetric CMPs (Chip Multi-Processors). Previous work has highlighted that this potential of AMP systems can be realizable via OS scheduling. To date, most existing scheduling schemes for AMPs have been designed to optimize the system throughput, but they are inherently unfair. Although fairness-aware schedulers have been also proposed, they fail to effectively deal with user priorities and do not always ensure that equal-priority applications experience a similar slowdown. To overcome these limitations, we propose ACFS, an asymmetry-aware completely fair scheduler that seeks to optimize fairness while ensuring acceptable throughput. Our evaluation on real AMP hardware and using scheduler implementations in the Linux kernel demonstrates that ACFS achieves an average 23% fairness improvement over two state-of-the-art schemes, while providing higher system throughput. © 2016 Elsevier Inc.","Asymmetric multicore; CFS; Fairness; Linux kernel; Operating systems; Scheduling"
"High productivity multi-device exploitation with the Heterogeneous Programming Library","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002609055&doi=10.1016%2fj.jpdc.2016.11.001&partnerID=40&md5=4ac2796bcf3dccceaf777131033734da","Heterogeneous devices require much more work from programmers than traditional CPUs, particularly when there are several of them, as each one has its own memory space. Multi-device applications require to distribute kernel executions and, even worse, arrays portions that must be kept coherent among the different device memories and the host memory. In addition, when devices with different characteristics participate in a computation, optimally distributing the work among them is not trivial. In this paper we extend an existing framework for the programming of accelerators called Heterogeneous Programming Library (HPL) with three kinds of improvements that facilitate these tasks. The first two ones are the ability to define subarrays and subkernels, which distribute kernels on different devices. The last one is a convenient extension of the subkernel mechanism to distribute computations among heterogeneous devices seeking the best work balance among them. This last contribution includes two analytical models that have proved to automatically provide very good work distributions. Our experiments also show the large programmability advantages of our approach and the negligible overhead incurred. © 2016 Elsevier Inc.","Heterogeneity; Libraries; Load balancing; OpenCL; Parallelism; Portability; Programmability"
"A novel cooperative accelerated parallel two-list algorithm for solving the subset-sum problem on a hybrid CPU–GPU cluster","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979663916&doi=10.1016%2fj.jpdc.2016.07.003&partnerID=40&md5=643a88c6ab27984eded4d7b6664d7bd0","Many parallel algorithms have recently been developed to accelerate solving the subset-sum problem on a heterogeneous CPU–GPU system. However, within each compute node, only one CPU core is used to control one GPU and all the remaining CPU cores are in idle state, which leads to a large number of CPU cores being wasted. In this paper, based on a cost-optimal parallel two-list algorithm, we propose a novel heterogeneous cooperative computing approach to solve the subset-sum problem on a hybrid CPU–GPU cluster, which can make full use of all available computational resources of a cluster. The unbalanced workload distribution and the huge communication overhead are two main obstacles for the heterogeneous cooperative computing. In order to assign the most suitable workload to each compute node and reasonably partition it between CPU and GPU within each node, and minimize the inter-node and intra-node communication costs, we design a communication-avoiding workload distribution scheme suitable for the parallel two-list algorithm. According to this scheme, we provide an efficient heterogeneous cooperative implementation of the algorithm. A series of experiments are conducted on a hybrid CPU–GPU cluster, where each node has two 6-core CPUs and one GPU. The results show that the heterogeneous cooperative computing significantly outperforms the CPU-only or GPU-only computing. © 2016 Elsevier Inc.","Heterogeneous cooperative computing; Hybrid CPU–GPU cluster; Hybrid programming model; Subset-sum problem; Two-list algorithm; Workload distribution"
"Combining quantitative constraints with qualitative preferences for effective non-functional properties-aware service composition","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994817726&doi=10.1016%2fj.jpdc.2016.10.013&partnerID=40&md5=6005cb5d4db46f147e3c51fb56ef35dd","With the increasing popularity of the service-oriented architecture and web service technologies, service composition has become widely adopted to create value-added services from existing ones. As more web services have been deployed on the Internet, it results in a large number of services providing identical functionalities while differing in their non-functional properties (NFPs). However, most of the existing techniques for NFP-aware service composition only consider quantitative NFPs. In this paper, we present a model that deals with both quantitative and qualitative NFPs. We develop two algorithms, where the first one combines global optimization with local selection and the second one leverages a genetic algorithm. We have conducted extensive experiments to evaluate the effectiveness of our model and algorithms. © 2016 Elsevier Inc.","Non-functional properties; Quantitative and qualitative; Service composition; TCP-nets"
"New distributed algorithms for fast sign detection in residue number systems (RNS)","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979524773&doi=10.1016%2fj.jpdc.2016.06.005&partnerID=40&md5=8cfc19463d3816692f74e81ecc5a51d1","We identify a canonical parameter in the Chinese Remainder Theorem (CRT) and call it the “Reconstruction Coefficient”, (denoted by “RC”); and introduce the notions of “Partial” and “Full” Reconstruction. If the RC can be determined efficiently, then arithmetic operations that are (relatively) harder to realize in RNS; including Sign Detection, Base change/extension and Scaling or division by a constant can also be implemented efficiently. This paper therefore focuses on and presents two distinct methods to efficiently evaluate the RC at long wordlengths. A straightforward application of these methods leads to ultra-fast sign-detection. An independent contribution of this paper is to illustrate non-trivial trade-offs between run-time computation vs. pre-computation and look-up. We show a simple method to select the moduli which leads to both the (i) number of RNS channels  n;  as well as  (ii) the largest channel modulus  mn satisfying {O(n),O(mn)}⪅N≡ the full-precision bit-length. The net result is that for many canonical operations; exhaustive look-up covering all possible input values is feasible even at long cryptographic bit-lengths N. Under fairly general and realistic assumptions about the capabilities of current hardware, the memory needed for exhaustive look-up tables is shown to be bounded by a low degree polynomial of  n. Moreover, both methods to compute RC can achieve a delay of O(lgn) in a RN system with n channels. To the best of our knowledge, no other method published to date has shown a path to achieve that lower bound on the execution delay. Further, small values of channel moduli make it ideal to implement each individual RNS channel on a simple core in a many-core processor or as a distributed node, and our algorithms require a limited number of inter-channel communications, averaging O(n). Results from a multi-core GPU implementation corroborate the theory. © 2016 Elsevier Inc.","Fast sign detection; Partial reconstruction; Reconstruction coefficient; Reduced precision; Residue number systems; RNS"
"Distributed Newest Vertex Bisection","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011844262&doi=10.1016%2fj.jpdc.2016.12.003&partnerID=40&md5=1528fc5f1e13a1c12a25711bab4a779f","Distributed adaptive conforming refinement requires multiple iterations of the serial refinement algorithm and global communication as the refinement can be propagated over several processor boundaries. We show bounds on the maximum number of iterations. The algorithm is implemented within the open-source software package DUNE-ALUGRID. © 2016 Elsevier Inc.","Adaptive method; DUNE; Mesh refinement; Parallel"
"Coordination for dynamic weighted task allocation in disaster environments with time, space and communication constraints","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978803486&doi=10.1016%2fj.jpdc.2016.06.010&partnerID=40&md5=f2dc7d09fc0c86127abbbe0ad28a27ed","Coordination for dynamic task allocation based on available resources is a very challenging research issue in disaster environments with time, space and communication constraints. In addition, the space and communication constraints and the dynamic features of disaster environments make an extra difficulty to achieve efficient coordination through centralised coordination approaches, which require the coordinators to have global knowledge of the environments. To this end, a coordination approach for dynamic weighted task allocation is proposed in this paper. The proposed approach considers time, space and communication constraints in disaster environments and urgent degrees of workloads of tasks without requiring the global knowledge of the environment. In particular, a dynamic group formation mechanism is developed to help agents to form groups and share information for task allocation under space and communication constraints in a decentralised manner, which can reflect real-life situations in disaster environments. The efficient coordination for task allocation is achieved through the utility calculation within each group. The experimental results show that the proposed approach outperforms most of other coordination approaches, such as the group formation approach proposed by Glinton et al. and the heuristics task allocation approach proposed by Ramchurn et al. in terms of group formation and weighted task allocation in disaster environments with time, space and communication constraints. © 2016 Elsevier Inc.","Agent coordination; Disaster environments; Intelligent agents; Resource allocation"
"ReLog: A systematic approach for supporting efficient reprogramming in wireless sensor networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008222734&doi=10.1016%2fj.jpdc.2016.12.010&partnerID=40&md5=27e3baba4e5b83eacb67e3c9af0f8d63","Wireless sensor networks are shifting to application platforms that poses several challenges on reprogramming efficiency. To better support the efficient reprogramming, this paper proposes a systematic approach named ReLog which consists of a programming language, a compiler, and a virtual machine. To make application programs concise and easy to modify, the ReLog language extends from a traditional logical programming language and makes the extension part have the similar coding style. To reduce the size of data for reprogramming, the compiler first produces extremely compact executable code by compiling application programs into high-level representations. It also implements efficient incremental reprogramming to diminish differences between the current and new executable code. To mitigate the energy consumption incurred by interpretive execution, the virtual machine optimizes the executable code as well as the execution process to improve the runtime efficiency. We have implemented ReLog and evaluated it with respect to real reprogramming cases. Our experimental results show that it is easy to modify ReLog programs to satisfy new application requirements. Meanwhile, the compiler reduces the size of executable code by 61.4%–83.2% compared to the existing work. In addition, the lifetime of sensors running the ReLog virtual machine is close (97.04%–98.31%) to that running the native code. © 2016 Elsevier Inc.","Compiler; Logical programming language; Reprogramming efficiency; Virtual machine; Wireless sensor network"
"Parallel pattern classification utilizing GPU-based kernelized Slackmin algorithm","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991585511&doi=10.1016%2fj.jpdc.2016.09.001&partnerID=40&md5=abea67863a72b37596973448a688e0d9","This paper introduces a parallel implementation of the kernelized Slackmin algorithm able to tackle medium scale data in pattern classification applications. Initially, the main principles of the serial Slackmin algorithm are described, with emphasis to its parallel nature making its parallelization a straightforward task. The parallelization is achieved by utilizing the parallel processing capabilities of the CUDA architecture of a low cost NVIDIA GPU card. The resulted GPU-based Slackmin algorithm named cuKSlackmin is able to classify medium scale data in a reasonable time without sacrificing its classification performance. A detailed comparison with some established GPU-based classification algorithms, widely used in machine learning, has proved the high performance of the proposed scheme as an alternative tool for medium scale data classification. © 2016 Elsevier Inc.","Big data; GPU programming; Machine learning; Parallel algorithms; Pattern classification"
"Matrix transpose on meshes with buses","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975229858&doi=10.1016%2fj.jpdc.2016.05.015&partnerID=40&md5=e86f33aca1b3c1bfcd8bb4ff8a647a0f","In this paper we analyze the matrix transpose problem for 2- and 3-dimensional mesh architectures with row and column buses. First we consider the 2-dimensional problem, and we give a lower bound of approximately 0.45n for the number of steps required by any matrix transpose algorithm on an n×n mesh with buses. Next we present an algorithm which solves this problem in less than 0.5n+9 steps. Finally, we prove that the given lower bound remains valid for the 3-dimensional case as well. © 2016 Elsevier Inc. All rights reserved.","Algorithm analysis; Matrix transpose; Mesh architecture"
"Topology exploration of a thermally resilient wavelength-based ONoC","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994381376&doi=10.1016%2fj.jpdc.2016.07.004&partnerID=40&md5=07cdf4fba5da35100f7e3ffa8229956f","With the growing number of cores, high-performance systems face power challenges due to dominating communication power. Thus, attaining energy efficient high-bandwidth inter-core communication nominates photonic network-on chip as the most promising interconnection paradigm. Although photonic networks pave the way for extremely higher performance communications, their intrinsic susceptibility to thermal fluctuations intimidates reliability of system. This necessitates the development of methodologies to analyze and model thermal effects on network behavior. In this paper, we model temperature fluctuations of optical chips and analyze photonic networks in a holistic approach. We present a novel wavelength-routed all-optical mesh network-on-chip, which significantly reduces optical contention scenarios throughout the network. While leveraging slightly larger number of wavelengths, it attains tolerance against thermal-induced faults. Our proposed architecture is compared against a popular wavelength-routed network, i.e. λ-router, in terms of throughput, in the presence of temperature drifts. Moreover, reliability comparison addressing fault rate in terms of temperature variation reveals that our proposed architecture significantly outperforms λ-router, as its competitor. © 2016 Elsevier Inc.","Heat conduction modeling; Optical network-on-chip; Power pattern of applications; Thermal resilient topology; Thermal-induced faults"
"A novel multi-objective evolutionary algorithm for recommendation systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006747029&doi=10.1016%2fj.jpdc.2016.10.014&partnerID=40&md5=e14adbeba163336e9a47d34b02e7071c","Nowadays, the recommendation algorithm has been used in lots of information systems and Internet applications. The recommendation algorithm can pick out the information that users are interested in. However, most traditional recommendation algorithms only consider the precision as the evaluation metric of the performance. Actually, the metrics of diversity and novelty are also very important for recommendation. Unfortunately, there is a conflict between precision and diversity in most cases. To balance these two metrics, some multi-objective evolutionary algorithms are applied to the recommendation algorithm. In this paper, we firstly put forward a kind of topic diversity metric. Then, we propose a novel multi-objective evolutionary algorithm for recommendation systems, called PMOEA. In PMOEA, we present a new probabilistic genetic operator. Through the extensive experiments, the results demonstrate that the combination of PMOEA and the recommendation algorithm can achieve a good balance between precision and diversity. © 2016 Elsevier Inc.","Genetic operator; Multi-objective optimization; Recommendation algorithm; Topic diversity"
"Theorem proving based Formal Verification of Distributed Dynamic Thermal Management schemes","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995475223&doi=10.1016%2fj.jpdc.2016.06.011&partnerID=40&md5=f7cf855b3ecfb22b40e7e334093a9ceb","Distributed Dynamic Thermal Management (DDTM) schemes are widely being used nowadays to cater for the elevated chip temperatures for many-core systems. Traditionally, DDTM schemes are analyzed using simulation or emulation but the non-exhaustive and incomplete nature of these analysis techniques may compromise on the reliability of the chip. Recently, model checking has been proposed for formally verifying simple DDTM schemes but, despite several abstractions, the analysis is limited to less than 100 cores due to the state-space explosion problem. As a more scalable approach for next-generation many-core systems, we propose a methodology based on theorem proving to perform formal verification of DDTM schemes. The proposed approach allows specification and verification of both functional and timing properties for any number of cores and for all times. For this purpose, the paper provides a higher-order-logic formalization of a generic DDTM scheme. The proposed generic model can be specialized to formally specify most of the existing DDTM schemes and thus formally verify their thermal properties, like temperature bounds and balancing and time to reach thermal stability, as higher-order-logic theorems. As an illustrative example, the paper presents a formal model and analysis of a Distributed Task Migration based DDTM scheme for many-core systems. © 2016 Elsevier Inc.","Dynamic Thermal Management; Higher-order logic; Many-core systems; Task migration; Theorem proving"
"Toward high-performance key-value stores through GPU encoding and locality-aware encoding","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977071145&doi=10.1016%2fj.jpdc.2016.04.015&partnerID=40&md5=8bdad157e09e4d93a7fbc611eb42dfa6","Although distributed key–value store is becoming increasingly popular in compensating the conventional distributed file systems, it is often criticized due to its costly full-size replication for high availability that causes high I/O overhead. This paper presents two techniques to mitigate such I/O overhead and improve key–value store performance: GPU encoding and locality-aware encoding. Instead of migrating full-size replicas over the network, we split the original file into smaller chunks and encode them with a few additional parity codes using GPUs before dispersing them onto remote nodes. The parity code is usually much smaller than the original file, which saves the extra space required for high availability and reduces the I/O overhead. Meanwhile, the compute-intensive encoding process is largely accelerated by the massive number of GPU cores. Yet, splitting the original file into smaller chunks stored on multiple nodes breaks data locality from application's perspective. To this end, we present a locality-aware encoding mechanism that allows a job to be dispatched as finer-grained tasks right on the node where the required chunk resides. Therefore, the data locality is preserved at the finer granularity of sub-job (i.e., task) level. We conduct an in-depth analysis of the proposed approach and implement a system prototype named Gest. Gest has been deployed and evaluated on a variety of testbeds demonstrating that high data availability, high space efficiency, and high I/O performance could be collectively achieved at the same time. © 2016 Elsevier Inc.","Erasure coding; GPGPU; Key–value store"
"Coping with recall and precision of soft error detectors","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982852275&doi=10.1016%2fj.jpdc.2016.07.007&partnerID=40&md5=94121565b07fb1ff17ecf96e5e7e1fe1","Many methods are available to detect silent errors in high-performance computing (HPC) applications. Each method comes with a cost, a recall (fraction of all errors that are actually detected, i.e., false negatives), and a precision (fraction of true errors amongst all detected errors, i.e., false positives). The main contribution of this paper is to characterize the optimal computing pattern for an application: which detector(s) to use, how many detectors of each type to use, together with the length of the work segment that precedes each of them. We first prove that detectors with imperfect precisions offer limited usefulness. Then we focus on detectors with perfect precision, and we conduct a comprehensive complexity analysis of this optimization problem, showing NP-completeness and designing an FPTAS (Fully Polynomial-Time Approximation Scheme). On the practical side, we provide a greedy algorithm, whose performance is shown to be close to the optimal for a realistic set of evaluation scenarios. Extensive simulations illustrate the usefulness of detectors with false negatives, which are available at a lower cost than the guaranteed detectors. © 2016 Elsevier Inc.","Exascale; Fault tolerance; High-performance computing; Partial verification; Recall and precision; Silent data corruption"
"Fault-tolerant embedding of complete binary trees in locally twisted cubes","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002773588&doi=10.1016%2fj.jpdc.2016.11.005&partnerID=40&md5=d9992763d9c8a9ef59f2ca100b2bc9ac","The complete binary tree is an important network structure for parallel and distributed computing, which has many nice properties and is often used to be embedded into other interconnection architectures. The locally twisted cube LTQn is an important variant of the hypercube Qn. It has many better properties than Qn with the same number of edges and vertices. The main results obtained in this paper are: (1) The complete binary tree CBTn rooted at an arbitrary vertex of LTQn can be embedded with dilation 2 and congestion 1 into LTQn. (2) When there exists only one faulty node in LTQn, both the dilation and congestion will become 2 after reconfiguring CBTn. (3) When there exist two faulty nodes in LTQn, then both the dilation and congestion will become 3 after reconfiguring CBTn. (4) For any faulty set F of LTQn with 2<|F|≤2n−1, both the dilation and congestion will become 3 after reconfiguring CBTn under certain constraints. © 2016 Elsevier Inc.","Complete binary tree; Congestion; Dilation; Embedding; Fault-tolerance; Interconnection architecture; Locally twisted cube"
"Asynchronous and synchronous models of executions on Intel® Xeon Phi™ coprocessor systems for high performance of long wave radiation calculations in atmosphere models","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009804117&doi=10.1016%2fj.jpdc.2016.12.018&partnerID=40&md5=b368b15c2d554a2a38557d94760607de","Long Wave Radiation Calculations are one of the most time-consuming calculations in atmosphere modeling. In this work, we explore two models for executions of these calculations on Intel® Xeon Phi™ Coprocessor Systems. In the asynchronous model, we offload the radiation calculations to the coprocessors and simultaneously execute calculations on the coprocessors along with the other atmosphere model calculations in the CPU cores. In the synchronous model, the CPU cores after offloading, wait for the results, and use the results in the same time step. We developed various techniques to complete these synchronous executions in minimal time, including loop rearrangement and low-cost interpolations. Using our experiments on an Intel Xeon Phi cluster, we show that our asynchronous execution model results in savings of many months in wall-clock execution time for multi-century climate simulations. Our synchronous execution model results in performance improvements of up to 70% in long-wave radiation calculations. © 2017 Elsevier Inc.","CAM; Intel Xeon Phi co-processors; Long-wave radiations; Offloading"
"Design optimization of secure message communication for energy-constrained distributed real-time systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994013709&doi=10.1016%2fj.jpdc.2016.10.004&partnerID=40&md5=21e112b5f4e2044b8e1c75b910f21b53","Modern Distributed Embedded Systems (DESs) tend to be more connected to other peers or external networks. This opens the gate for potential security attacks, although it is beneficial for embedded applications running on DESs. Security protections in DESs require significant time and energy overhead. Longer execution time rises the possibility of deadline violation of real-time applications, which may cause serious consequences in hard real-time systems. Energy efficiency is required since energy budget is usually very scarce under the context of DESs. Therefore, it is of critical importance to study the trade-off between security protection and corresponding timing and energy overheads so that the overall system performance can be maximally improved. In this paper, we approach the design of energy- and security-critical distributed real-time embedded systems from the early mapping and scheduling phases. We focus on providing the best confidentiality protection of internal communication in DESs under timing and energy constraints. The complexity of finding the optimal solution grows exponentially as the problem size increases. Therefore, we propose an efficient, genetic algorithm based heuristic to solve the problem. In the presented algorithm, solutions are evolved gradually so that good solutions can be obtained. Extensive experiments, including a real-life case study, demonstrate the efficiency of the proposed technique. © 2016 Elsevier Inc.","Distributed real-time systems; Energy; Message protection; Security; System design"
"4DGIN-3: A new design layout of 4-disjoint gamma interconnection network","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984994820&doi=10.1016%2fj.jpdc.2016.08.002&partnerID=40&md5=634b70cfb961e63e7316d28f443570d3","Various multistage interconnection networks have been proposed in the literature and utilized in commercial projects. But there exists a further scope of improvement and the need for new fault tolerant multistage interconnection networks, which can withstand multiple switch or link failures. Recently, design layouts of four disjoint paths gamma interconnection networks have been proposed and named as 4DGIN-1 and 4DGIN-2, respectively. These proposed design layouts of 4DGINs produce different number of alternative paths ranging from five to seven for different tag values. In this paper, a new variant, viz., 4DGIN-3 is introduced, which generates four disjoint paths similar to 4DGIN-1 and 4DGIN-2 but with a total of six number of alternative paths for every tag value. This feature of the proposed design, in turn, results in same reliability for each tag value at various assumed reliability of switching elements despite having the similar topology and hardware cost as that of the earlier 4DGINs. A comparison of the proposed design with the existing 4DGINs has also been provided in this paper. © 2016 Elsevier Inc.","Disjoint paths; Fault tolerant MINs; Gamma interconnection networks (GIN); Multistage interconnection networks (MINs); Terminal reliability"
"Optimal construction of node-disjoint shortest paths in folded hypercubes","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006982214&doi=10.1016%2fj.jpdc.2016.12.002&partnerID=40&md5=f8cbb82c0560add69a41a431f0def083","Node-disjoint paths have played an important role in the study of routing, reliability, and fault tolerance of an interconnection network. In this paper, we give a necessary and sufficient condition, which can be verified in O(mn1.5) time, for the existence of m node-disjoint shortest paths from one source node to other m (not necessarily distinct) target nodes, respectively, in an n-dimensional folded hypercube, where m≤n+1. Moreover, when the condition holds, the m node-disjoint shortest paths can be constructed in optimal O(mn) time. In the situation that all of the source node and target nodes are mutually distinct, brute-force computations show that the probability of existence of the m node-disjoint shortest paths in an n-dimensional folded hypercube is not less than 100%, 86%, 86%, 92%, and 94% for (n,m)=(3,4),(4,4),(5,6),(6,6), and (7, 8), respectively. © 2016 Elsevier Inc.","Folded hypercube; Maximum matching; Node-disjoint paths; Optimization problem"
"An efficient and secure information retrieval framework for content centric networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014439491&doi=10.1016%2fj.jpdc.2017.01.024&partnerID=40&md5=9c17d763216eb485c668ca65be1a6ab8","Content centric networking is a new networking architecture designed to work with existing network architecture and protocols. In content centric networks emphasis is on data rather than its location. Content centric networks, support data caching in intermediate nodes that allows them to serve future request. This results in more efficient content delivery. As data is cached at various intermediate nodes, the owner of the data has no control over the data. Data stored in intermediate nodes can be vulnerable and be exploited by the intermediate nodes, malicious users and intruders, who do not have legitimate access to these data. In addition to this entire data is being broadcasted by the intermediate nodes whenever they receive a request for that corresponding data. This utilizes a substantial portion of the network bandwidth. To address these issues we propose a framework in this paper that (1) efficiently organizes data that is cached in intermediate nodes using a NOSQL (Not Only SQL) graph database, (2) reduces the network utilization, (3) secures all the data transferred across content centric networks, such that only owners of the data can provide access to the consumers of their data and (4) provides role-based access to the consumers, and reduces complications in key distribution and management. Our experimental results indicate that our framework is able to reduce network utilization by over 79% in the best-case scenario and by over 65% in the worst-case scenario. © 2017 Elsevier Inc.","Content centric networks; Network utilization; NOSQL; Role-based access; Security"
"Grasping the gap between blocking and non-blocking transactional memories","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994895289&doi=10.1016%2fj.jpdc.2016.10.008&partnerID=40&md5=04d5e950be377ecac240d0c9314eb6a5","Transactional memory (TM) is an inherently optimistic abstraction: it allows concurrent processes to execute sequences of shared-data accesses (transactions) speculatively, with an option of aborting them in the future. Early TM designs avoided using locks and relied on non-blocking synchronization to ensure obstruction-freedom: a transaction that encounters no step contention is not allowed to abort. However, it was later observed that obstruction-free TMs perform poorly and, as a result, state-of-the-art TM implementations are nowadays blocking, allowing aborts because of data conflicts rather than step contention. In this paper, we explain this shift in the TM practice theoretically, via complexity bounds. We prove a few important lower bounds on obstruction-free TMs. Then we present a lock-based TM implementation that beats all of these lower bounds. In sum, our results exhibit a considerable complexity gap between non-blocking and blocking TM implementations. © 2016 Elsevier Inc.","Blocking; Disjoint-access parallelism; Expensive synchronization; Invisible reads; Lower bounds; Memory stalls; Non-blocking; Obstruction-freedom; Perturbability; Transactional memory"
"Efficient clustering for ultra-scale application tracing","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984851142&doi=10.1016%2fj.jpdc.2016.08.001&partnerID=40&md5=6ac5f4a90834d0fd8fde671f0bb5be62","Extreme-scale computing poses a number of challenges to application performance. Developers need to study application behavior by collecting detailed information with the help of tracing toolsets to determine shortcomings. But not only applications are “scalability challenged”, current tracing toolsets also fall short of exascale requirements for low background overhead since trace collection for each execution entity is becoming infeasible. One effective solution is to cluster processes with the same behavior into groups. Instead of collecting performance information from each individual node, this information can be collected from just a set of representative nodes. This work contributes a fast, scalable, signature-based clustering algorithm that clusters processes exhibiting similar execution behavior. Instead of prior work based on statistical clustering, our approach produces precise results nearly without loss of events or accuracy. The proposed algorithm combines low overhead at the clustering level with log(P) time complexity, and it splits the merge process to make tracing suitable for extreme-scale computing. Overall, this multi-level precise clustering based on signatures further generalizes to a novel multi-metric clustering technique with unprecedented low overhead. © 2016 Elsevier Inc.","Clustering algorithms; Concurrent programming; Performance measurement; Programming techniques"
"VA-DE: Valuable ATAPE with dynamic embedding and super-pipeline scheduling on partitionable multistage interconnection networks+","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007170387&doi=10.1016%2fj.jpdc.2016.11.010&partnerID=40&md5=3b86c7dab10d1fc3370f9e262ed0ccf6","The optimal ATAPE (all-to-all personalized exchange) is a key solution to achieve the optimal communication in parallel matrix-transposition, parallel fast-Fourier-transformation, etc. The ATAPE has been extensively studied on static networks and dynamic binary-switch MINs (multistage interconnection networks). Recently, the ultimate ATAPE was successfully embedded on MINs+ and the fast crossbar of MINs+. However, the MINs+ cannot be partitioned for multiple tasks and the crossbar of MINs+ is more expensive and less subsystem-utilization. Therefore, this paper proposes the prototype of the x×x crossbar of partitionable MINs+ (PMINs+) using I/O cross-control switches and the generalized d-nary-switch MINs+. In addition, the optimal VA-DE (valuable ATAPE with dynamic embedding) with triple-right scheduling (right-task, right-section, right-time) is presented for multiple tasks of flexible sizes N″=N/2t (where t≤log2x) and the efficient subsystem-utilization on the PMINs+. New partitionable ATAPE embedding and automatic mapping for the partitionable section y(=0 to x−1) are Dy=Sy⊕Cly (global) and lDy=(Dy%N″)⊕cz (local) using double-jumping controls Cly=(yN/x+l)modN (to right section) and cz=zN″/x (to right subtask), incorporated with super-pipelining for outperforming optimal time O(N/x) over O(N), the best of existing ATAPEs. In experiments, remarkable speedup and throughput were strongly confirmed on the simulated PMIN+ systems. © 2016 Elsevier Inc.","Double jumping of controls and super-pipelining for the triple-right scheduling (right task, right section, right time); Embedding partitionable ATAPE (all-to-all personalized exchange) and automatic mapping on PMINs<sup>+</sup>; Optimal VA-DE (valuable ATAPE with dynamic embedding); Partitionable MINs<sup>+</sup> (multistage interconnection networks<sup>+</sup>) using I/O cross-control switches"
"Efficient routing through discretization of overlapped road segments in VANETs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007240065&doi=10.1016%2fj.jpdc.2016.09.005&partnerID=40&md5=6a4996efb937c8c588dacaebdeb8b579","Routing in vehicular ad-hoc networks (VANETs) is based on the contacts through vehicle-to-vehicle communications. Due to the high probability and long duration, contacts on the overlapped trajectories among vehicles play an important role during routing. However, both the stochastic vehicular mobilities and non-ignorable road distances lead to high spatio-temporal fluctuations for the probabilistic contacts on the overlapped trajectories, which can diversify routing decisions. In this paper, we propose a novel Sampling-based Estimation Scheme (SES), which discretizes the probabilistic contacts on the overlapped roads into a small number of segments, and abstracts each segment as a sample. The contact duration between two vehicles moving in opposite directions on their overlapped road is lower, but their contact probability is higher. By contrast, the duration of the contact between two vehicles moving in the same direction on their overlapped road is higher, but their contact probability is lower. The proposed SES can achieve efficient routing by considering the above properties of stochastic contacts. Furthermore, we investigate the content transmission among the probabilistic contacts, through the flow model with probabilistic capacities. Extensive experiments validate the competitive performance of the proposed SES with the probabilistic contacts in VANETs. © 2016 Elsevier Inc.","Discretization; Overlapped road; Probabilistic contact; Routing; Vehicular ad-hoc networks"
"Accelerating an algorithm for perishable inventory control on heterogeneous platforms","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009144950&doi=10.1016%2fj.jpdc.2016.12.021&partnerID=40&md5=69e08be290005411b886fa747619b1b0","This paper analyses and evaluates parallel implementations of an optimization algorithm for perishable inventory control problems. This iterative algorithm has high computational requirements when solving large problems. Therefore, the use of parallel and distributed computing reduces the execution time and improves the quality of the solutions. This work investigates two implementations on heterogeneous platforms: (1) a MPI-PTHREADS version; and (2) a multi-GPU version. A comparison of these implementations has been carried out. Experimental results show the benefits of using parallel and distributed codes to solve this kind of problems. Furthermore, the distribution of the workload among the available processing elements is a challenging problem. This distribution of tasks can be modelled as a Bin-Packing problem. This implies that the selection of the set of tasks assigned to every processing element requires the design of a heuristic capable of efficiently balancing the workload statically with no significant overhead. This heuristic has been used for the parallel implementations of the optimization for perishable inventory control problem. © 2016 Elsevier Inc.","Bin-Packing problem; GPU computing; Heterogeneous computing; Monte-Carlo simulation; Optimization; Perishable inventory control"
"EPLS: A novel feature extraction method for migration data clustering","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010210689&doi=10.1016%2fj.jpdc.2016.11.008&partnerID=40&md5=8e50bfc3f2ac339f21169b6bf8e48cb5","Nowadays human activity data such as migration data can be easily accumulated by personal devices thanks for GPS. Analysis on migration data is very useful for society decision. Migration data as non-line time series have the properties of higher noise and outliers. Traditional feature extraction methods cannot address this issue very well because of inherent characteristics. Aiming at this problem, a novel numerical feature extraction approach EPLS is proposed. It is an integration of the Ensemble Empirical Mode (EEMD), Principal Component Analysis (PCA) and Least Square (LS) method. The EPLS model includes (1) Mode Decomposition in which EEMD algorithm is applied to the aggregation dataset; (2) Dimension Reduction is carried out for a more significant set of vectors; (3) Least Squares Projection in which all testing data are projected to the obtained vectors. Experimental results show that EPLS can overcome the higher noise and outliers based on migration data clustering. Meanwhile, EPLS feature extraction method can achieve high performance compared with several different clustering methods and distance measures. © 2016","Clustering; Distance measures; EPLS; Feature extraction; Migration data"
"The GPU-based parallel Ant Colony System","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969645507&doi=10.1016%2fj.jpdc.2016.04.014&partnerID=40&md5=f2022b9fae0e6df351bff922d94ca0fc","The Ant Colony System (ACS) is, next to Ant Colony Optimization (ACO) and the MAX–MIN Ant System (MMAS), one of the most efficient metaheuristic algorithms inspired by the behavior of ants. In this article we present three novel parallel versions of the ACS for the graphics processing units (GPUs). To the best of our knowledge, this is the first such work on the ACS which shares many key elements of the ACO and the MMAS, but differences in the process of building solutions and updating the pheromone trails make obtaining an efficient parallel version for the GPUs a difficult task. The proposed parallel versions of the ACS differ mainly in their implementations of the pheromone memory. The first two use the standard pheromone matrix, and the third uses a novel selective pheromone memory. Computational experiments conducted on several Traveling Salesman Problem (TSP) instances of sizes ranging from 198 to 2392 cities showed that the parallel ACS on Nvidia Kepler GK104 GPU (1536 CUDA cores) is able to obtain a speedup up to 24.29x vs the sequential ACS running on a single core of Intel Xeon E5-2670 CPU. The parallel ACS with the selective pheromone memory achieved speedups up to 16.85x, but in most cases the obtained solutions were of significantly better quality than for the sequential ACS. © 2016 Elsevier Inc.","CUDA; GPU; Parallel Ant Colony System; Parallel metaheuristic; Selective pheromone memory"
"Exploring performance and energy tradeoffs for irregular applications: A case study on the Tilera many-core architecture","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996835595&doi=10.1016%2fj.jpdc.2016.06.006&partnerID=40&md5=780cd7d684423d48c02e61f5b4cc0953","High performance, parallel applications with irregular data accesses are becoming a critical workload class for modern systems. In particular, the execution of such workloads on emerging many-core systems is expected to be a significant component of applications in data mining, machine learning, scientific computing and graph analytics. However, power and energy constraints limit the capabilities of individual cores, memory hierarchy and on-chip interconnect of such systems, thus leading to architectural and software trade-offs that must be understood in the context of the intended application's behavior. Irregular applications are notoriously hard to optimize given their data-dependent access patterns, lack of structured locality and complex data structures and code patterns. We have ported two irregular applications, graph community detection using the Louvain method (Grappolo) and high-performance conjugate gradient (HPCCG), to the Tilera many-core system and have conducted a detailed study of platform-independent and platform-specific optimizations that improve their performance as well as reduce their overall energy consumption. To conduct this study, we employ an auto-tuning based approach that explores the optimization design space along three dimensions—memory layout schemes, GCC compiler flag choices and OpenMP loop scheduling options. We leverage MIT's OpenTuner auto-tuning framework to explore and recommend energy optimal choices for different combinations of parameters. We then conduct an in-depth architectural characterization to understand the memory behavior of the selected workloads. Finally, we perform a correlation study to demonstrate the interplay between the hardware behavior and application characteristics. Using auto-tuning, we demonstrate whole-node energy savings and performance improvements of up to 49.6% and 60% relative to a baseline instantiation, and up to 31% and 45.4% relative to manually optimized variants. © 2016 Elsevier Inc.","Auto tuning; Community detection; Data layouts; Energy optimization; Irregular applications; Many-core processors; Sparse conjugate gradient"
"Delay analysis and optimization for inter-core interference in real-time embedded multicore systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009265660&doi=10.1016%2fj.jpdc.2016.12.004&partnerID=40&md5=b11b7a5458c9bce3d72b249d34fa9ba5","The Worst Case Execution Time (WCET) is one of the most important performance metrics in real-time systems. With multi-core architectures becoming a trend in real-time systems, the WCET analysis is of great challenge, since multiple cores accessing shared hardware resources, such as cache and bus, may result in significant interference on them. In this paper, we propose a finer grained approach to analyze the inter-core interference(bank conflict and bus access interference) on multi-core platforms with the interference-aware bus arbiter(IABA) and bank-column cache partitioning, and our approach can reasonably estimate interference delays based on request timing. Moreover, we optimize bank-to-core mapping to reduce the interference delays, and develop an algorithm for finding the best bank-to-core mapping. The experimental results show that our interference analysis approach can improve the tightness of interference delays by 18.36% on average compared to Upper Bound Delay(UBD) approach, and the optimized bank-to-core mapping can achieve the WCET improvement by 8.93% on average. © 2016 Elsevier Inc.","Bank conflict; Inter-core interference; Multi-core; WCET"
"Fattened backfilling: An improved strategy for job scheduling in parallel systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978975444&doi=10.1016%2fj.jpdc.2016.06.013&partnerID=40&md5=75102f60094a29b4eb8d7aaf7d1d1ee9","Job scheduling is a very important topic in parallel systems. Although there exist algorithms that theoretically provide optimal performance, in general, they are too complex for implementation in real production systems. The simplest and most feasible approach that provides efficient and fair scheduling is backfilling. In fact, backfilling is used in many real production systems. Scheduling policies have been widely studied and discussed, but there is still a big field of research to be explored, since most scheduling algorithms are motivated by performance, but also by the psychology of queuing and the fairness concept. In this article, we propose an algorithm (fattened backfilling) that provides more backfilling opportunities, and is consequently more efficient. In particular, our algorithm allows short jobs to move forward if they do not delay the first job of the queue more than the average waiting time of the already finished jobs. The results of our simulations show a great improvement in response time and waiting time in most of the cases, with improved or similar slowdown values. Hence, we can conclude that fattened backfilling outperforms conservative and EASY backfilling, and, due to its simplicity, it is recommended for implementation in modern scheduling systems. © 2016 Elsevier Inc.","Backfilling; Job scheduling; Parallel computing; Performance evaluation; Simulator"
"BlackOut: Enabling fine-grained power gating of buffers in Network-on-Chip routers","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011843595&doi=10.1016%2fj.jpdc.2017.01.016&partnerID=40&md5=b8ce15caea4f48cb2344a96cd2535811","The Network-on-Chip (NoC) router buffers play an instrumental role in the performance of both the interconnection fabric and the entire multi-/many-core system. Nevertheless, the buffers also constitute the major leakage power consumers in NoC implementations. Traditionally, they are designed to accommodate worst-case traffic scenarios, so they tend to remain idle, or under-utilized, for extended periods of time. The under-utilization of these valuable resources is exemplified when one profiles real application workloads; the generated traffic is bursty in nature, whereby high traffic periods are sporadic and infrequent, in general. The mitigation of the leakage power consumption of NoC buffers via power gating has been explored in the literature, both at coarse (router-level) and fine (buffer-level) granularities. However, power gating at the router granularity is suitable only for low and medium traffic conditions, where the routers have enough opportunities to be powered down. Under high traffic, the sleeping potential rapidly diminishes. Moreover, disabling an entire router greatly affects the NoC functionality and the network connectivity. This article presents BlackOut, a fine-grained power-gating methodology targeting individual router buffers. The goal is to minimize leakage power consumption, without adversely impacting the system performance. The proposed framework is agnostic of the routing algorithm and the network topology, and it is applicable to any router micro-architecture. Evaluation results obtained using both synthetic traffic patterns and real applications in 64-core systems indicate energy savings of up to 70%, as compared to a baseline NoC, with a near-negligible performance overhead of around 2%. BlackOut is also shown to significantly outperform–by 35%, on average–two current state-of-the-art power-gating solutions, in terms of energy savings. © 2017 Elsevier Inc.","Low power; Multi-cores; Networks-on-Chip; Power gating; Static power"
"An efficient causal group communication protocol for P2P hierarchical overlay networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008608290&doi=10.1016%2fj.jpdc.2016.12.007&partnerID=40&md5=e280ee147839ceb4d698978a2ba376f6","Peer-to-peer applications such as multiplayer online games are characterized by considering group communication among geographically distributed peers. In such environments, causal ordering is an essential property for consistent exchange of information among peers. Although several works are oriented to ensure message causal order, most of them are not suitable for hierarchical overlay networks. In this paper, we propose an efficient causal protocol oriented to be used in a hierarchical overlay network. In our protocol the overhead timestamp per message is based on the number of messages with immediate dependency relation. By using the information about network architecture and representing message dependencies on a bit level, proposed protocol ensures causal message ordering without enforcing super peers order to all of the peers in a group. The protocol has been simulated and the results show that it presents lower overhead than currently existing causal protocols. © 2016 Elsevier Inc.","Causal ordering; Hierarchical topology; Peer-to-peer"
"Resource provisioning and work flow scheduling in clouds using augmented Shuffled Frog Leaping Algorithm","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999635191&doi=10.1016%2fj.jpdc.2016.11.003&partnerID=40&md5=85e6223af75b4cf94afe3d3296efb18d","The on-demand provisioning and resource availability in cloud computing make it ideal for executing scientific workflow applications. An application can start execution with a minimum number of resources and allocate further resources when required. However, workflow scheduling is an NP hard problem and therefore meta-heuristics based solutions have been widely explored for the same. This paper presents an augmented Shuffled Frog Leaping Algorithm (ASFLA) based technique for resource provisioning and workflow scheduling in the Infrastructure as a service (IaaS) cloud environment. The performance of the ASFLA has been compared with the state of art PSO and SFLA algorithms. The efficacy of ASFLA has been assessed over some well-known scientific workflows of varied sizes using a custom Java based simulator. The simulation results show a marked improvement in the performance criteria of achieving minimum execution cost and meeting the schedule deadlines. © 2016 Elsevier Inc.","Cloud computing; Resource provisioning; Scheduling; Scientific workflow; Shuffled frog leaping algorithm"
"Can everybody be happy in the cloud? Delay, profit and energy-efficient scheduling for cloud services","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976406246&doi=10.1016%2fj.jpdc.2016.05.013&partnerID=40&md5=c111dcded363bbeacdb87f907d20410d","The rapid development of Cloud Computing provides consumers and service providers with a wide range of opportunities and challenges. Considering the substantial infrastructure investments being made by cloud providers, the reduction of operating expenses (OPEX) while maximizing the profit of the provided services is of great importance. One way to achieve this is by maximizing the efficiency of resource utilization. However, profit maximization does not necessarily coincide with the improvement of a user's Quality of Service (QoS); users generating higher profit for the provider may be scheduled first, causing high delays to low-paying users. Further, the contradictory nature of users’ and providers’ needs also extends to the energy consumption problem, as the minimization of service delays could cause cloud resources to be constantly “on”, leading to high energy consumption, high costs for providers and undue environmental impact. The objective of our work is to analyze this multi-dimensional trade-off. We first investigate the problem of efficient resource allocation strategies for time-varying traffic, and propose a new algorithm, MinDelay, which aims at achieving the minimum service delay while taking into account provider's profit. Then, we propose E-MinDelay, an energy-efficient approach for CPU-intensive tasks in cloud systems. Furthermore, we propose an improved version of the Energy Conscious Task Consolidation (ECTC) algorithm, which combines task consolidation and migration techniques with E-MinDelay. Our results demonstrate that energy consumption and service delays corresponding to profit loss can be simultaneously decreased using an efficient scheduling algorithm. © 2016 Elsevier Inc.","Cloud computing; Cloud services; Delay; Energy efficiency; Profit; Scheduling algorithms"
"Silent self-stabilizing BFS tree algorithms revisited","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978219604&doi=10.1016%2fj.jpdc.2016.06.003&partnerID=40&md5=f7dfbfd0d2c77614d92d7b7983661952","In this paper, we revisit two fundamental results of the self-stabilizing literature about silent BFS spanning tree constructions: the Dolev et al. algorithm and the Huang and Chen's algorithm. More precisely, we propose in the composite atomicity model three straightforward adaptations inspired from those algorithms. We then present a deep study of these three algorithms. Our results are related to both correctness (convergence and closure, assuming a distributed unfair daemon) and complexity (analysis of the stabilization time in terms of rounds and steps). © 2016 Elsevier Inc.","BFS spanning tree; Composite atomicity model; Distributed unfair daemon; Round and step complexity; Self-stabilization; Stabilization time"
"Path planning algorithms for mobile anchors towards range-free localization","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978399744&doi=10.1016%2fj.jpdc.2016.06.001&partnerID=40&md5=85dd460f8c31cb5b4a5a21a2c0098c08","The objective of path planning for a mobile anchor is to find the path of minimum length that the anchor traverses to localize all sensors. The challenge is to design a movement strategy which reduces path length while meeting the requirements of a good range-free localization technique. A novel deterministic movement strategy is proposed in this paper that reduces path length and uses an existing range-free localization scheme which yields good positional accuracy. The mobile anchor moves in a hexagonal pattern to localize all the sensors which form a connected network. We compare performance of our algorithm with an existing path planning algorithm in terms of both path length and localization accuracy. Simulation results show that even in presence of irregular radio propagation, our algorithm achieves full localization. We have proposed another movement strategy for a mobile anchor using same hexagonal pattern to localize all the sensors lying in a rectangular region. Improvement in path length is shown theoretically compared to existing path planning schemes. © 2016 Elsevier Inc.","Connected networks; Mobile anchor; Path planning; Range-free localization; Rectangular region; Wireless sensor networks"
"Research on semantic of updatable distributed logic and its application in access control","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011982208&doi=10.1016%2fj.jpdc.2016.12.006&partnerID=40&md5=b723ed225e02ffbf21254b9bbb344eea","The paper presents a distributed logic UD-Datalog whose advantage lies that it extends U-Datalog to distributed environment but still keeps the logic semantic and evaluation method of U-Datalog. The logic presented a new approach to define update in distributed environment based on non-immediate update semantics which distinguishes the language from other distributed datalog. The language is pure declarative and allows us to use top-down and equivalent bottom-up computational evaluation so the already developed techniques for Datalog evaluation can be reused. Firstly, the paper elaborates the syntax and semantic of the logic. Secondly, the evaluation method of the logic is explained. Finally, an application example of the logic in access control of network is discussed which shows the application and expressiveness of the logic. © 2016 Elsevier Inc.","Access control; Distributed logic; UD-Datalog; Updatable distributed datalog"
"The Tag Filter Architecture: An energy-efficient cache and directory design","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969567657&doi=10.1016%2fj.jpdc.2016.04.016&partnerID=40&md5=761411c7957e010361771f2cd463b4e3","Power consumption in current high-performance chip multiprocessors (CMPs) has become a major design concern that aggravates with the current trend of increasing the core count. A significant fraction of the total power budget is consumed by on-chip caches which are usually deployed with a high associativity degree (even L1 caches are being implemented with eight ways) to enhance the system performance. On a cache access, each way in the corresponding set is accessed in parallel, which is costly in terms of energy. On the other hand, coherence protocols also must implement efficient directory caches that scale in terms of power consumption. Most of the state-of-the-art techniques that reduce the energy consumption of directories are at the cost of performance, which may become unacceptable for high-performance CMPs. In this paper, we propose an energy-efficient architectural design that can be effectively applied to any kind of cache memory. The proposed approach, called the Tag Filter (TF) Architecture, filters the ways accessed in the target cache set, and just a few ways are searched in the tag and data arrays. This allows the approach to reduce the dynamic energy consumption of caches without hurting their access time. For this purpose, the proposed architecture holds the X least significant bits of each tag in a small auxiliary X-bit-wide array. These bits are used to filter the ways where the least significant bits of the tag do not match with the bits in the X-bit array. Experimental results show that, on average, the TF Architecture reduces the dynamic power consumption across the studied applications up to 74.9%, 85.9%, and 84.5% when applied to L1 caches, L2 caches, and directory caches, respectively. © 2016 Elsevier Inc.","Cache; Directory; Dynamic consumption; Multicore processors"
"Concurrency in snap-stabilizing local resource allocation","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006994948&doi=10.1016%2fj.jpdc.2016.11.004&partnerID=40&md5=52e6363a9a25d48ee6662865f18b0fbf","In distributed systems, resource allocation consists in managing fair access of a large number of processes to a typically small number of reusable resources. As soon as the number of available resources is greater than one, the efficiency in concurrent accesses becomes an important issue, as a crucial goal is to maximize the utilization rate of resources. In this paper, we tackle the concurrency issue in resource allocation problems. We first characterize the maximal level of concurrency we can obtain in such problems by proposing the notion of maximal concurrency. Then, we focus on Local Resource Allocation problems (LRA). Our results are both negative and positive. On the negative side, we show that there is a wide class of instances of LRA for which it is impossible to obtain maximal concurrency without compromising the fairness. On the positive side, we propose a snap-stabilizing LRA algorithm which achieves a high (but not maximal) level of concurrency, called here strong concurrency. © 2016 Elsevier Inc.","Distributed algorithms; Local resource allocation; Self-stabilization; Snap-stabilization"
"Sufficient conditions for Hamiltonicity in multiswapped networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996562099&doi=10.1016%2fj.jpdc.2016.10.015&partnerID=40&md5=88cca420c99082bf7fb886c5b059a22d","OTIS networks are interconnection networks amenable to deployment as hybrid networks containing both electronic and optical links. Deficiencies as regards symmetry led to the subsequent formulation of biswapped networks which were later generalized to multiswapped networks so as to still enable optoelectronic implementation (as it happens, multiswapped networks also generalize previously studied hierarchical crossed cubes). Multiswapped networks of the form Msw(H;G) are known to possess good (graph-theoretic) properties as regards their use as (optoelectronic) interconnection networks (in distributed-memory multiprocessors) and in relation to those of the component networks G and H. Combinatorially, they provide a hierarchical mechanism to define new networks from existing networks (so that the properties of the new network can be controlled in terms of the constituent networks). In this paper, we prove that if G and H are Hamiltonian networks then the multiswapped network Msw(H;G) is also Hamiltonian. At the core of our proof is finding specially designed Hamiltonian cycles in 2-dimensional and heavily pruned 3-dimensional tori, irrespective of the actual networks G and H we happen to be working with. This lends credence to the role of tori as fundamental networks within the study of interconnection networks. © 2016 The Author(s)","Hamiltonian cycles; Interconnection networks; Multiswapped networks; Network topology; Optoelectronic networks"
"Highly intensive data dissemination in complex networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988516632&doi=10.1016%2fj.jpdc.2016.08.004&partnerID=40&md5=adfb2205b7da40156fca9a22d2cfff6f","This paper presents a study on data dissemination in unstructured Peer-to-Peer (P2P) network overlays. The absence of a structure in unstructured overlays eases the network management, at the cost of non-optimal mechanisms to spread messages in the network. Thus, dissemination schemes must be employed that allow covering a large portion of the network with a high probability (e.g. gossip based approaches). We identify principal metrics, provide a theoretical model and perform the assessment evaluation using a high performance simulator that is based on a parallel and distributed architecture. A main point of this study is that our simulation model considers implementation technical details, such as the use of caching and Time To Live (TTL) in message dissemination, that are usually neglected in simulations, due to the additional overhead they cause. Outcomes confirm that these technical details have an important influence on the performance of dissemination schemes and that the studied schemes are quite effective to spread information in P2P overlay networks, whatever their topology. Moreover, the practical usage of such dissemination mechanisms requires a fine tuning of many parameters, the choice between different network topologies and the assessment of behaviors such as free riding. All this can be done only using efficient simulation tools to support both the network design phase and, in some cases, at runtime. © 2016 Elsevier Inc.","Complex networks; Data dissemination; Performance evaluation; Simulation"
"Distributed computing on core–periphery networks: Axiom-based design","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992108816&doi=10.1016%2fj.jpdc.2016.08.003&partnerID=40&md5=ac69b8b964c9bb4e2b717c06f36bd72a","Inspired by social networks and complex systems, we propose a core–periphery network architecture that supports fast computation for many distributed algorithms, is robust and uses a linear number of links. Rather than providing a concrete network model, we take an axiom-based design approach. We provide three intuitive and independent algorithmic axioms and prove that any network that satisfies all axioms enjoys an efficient algorithm for a range of tasks (such as MST, sparse matrix multiplication, and more). We also show the necessity of our axiom set: for networks that satisfy any subset of the axioms, the same efficiency cannot be guaranteed for any deterministic algorithm. © 2016 Elsevier Inc.","Axiom-base design; Core–periphery networks; Distributed computing; Minimum spanning tree"
"A distributed and parallel control mechanism for self-reconfiguration of modular robots using L-systems and cellular automata","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007236644&doi=10.1016%2fj.jpdc.2016.11.016&partnerID=40&md5=8132c7cbb4de4c264cc18b7b06f28831","For distributed self-reconfiguration of Modular Self-Reconfigurable (MSR) robots, one of the main difficulties is the contradiction between limited information of decentralized modules and well-organized global structure. This paper presents a distributed and parallel mechanism for decentralized self-reconfiguration of MSR robots. This mechanism is hybrid by combining Lindenmayer systems (L-systems) describing the topological structure as configuration target and Cellular Automata (CA) for local motion planning of individual modules. Turtle interpretations are extended to modular robotics for generating module-level predictions from global description. According to local information, independent modules make motion planning by Cellular Automata in parallel. This distributed mechanism is robust to failure of modules, scalable to varying module numbers, and convergent to predefined reconfiguration targets. Simulations and statistical results are provided for validating the proposed algorithm. © 2016 Elsevier Inc.","Cellular automata; Distributed algorithm; L-systems; Modular robots; Parallel algorithm; Self-reconfiguration; Self-repair"
"Multi-level energy/power-aware design methodology for MPSoC","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975129584&doi=10.1016%2fj.jpdc.2016.03.013&partnerID=40&md5=a18529dd94cf9ba0e47733ce6c89a37e","Multiprocessor Systems-on-Chip (MPSoC) are becoming one of the most used solutions in order to meet the growing computation requirements of modern embedded applications. In such systems, power/energy consumption is a critical metric that should be taken into account in the design flow. System designers need an efficient power-aware design methodology and tools to cope with the complexity of MPSoC design. To address this challenge, we present in this paper a power-aware design methodology that relies on multi-level design space exploration. We propose a two-phase exploration process making profit first from functional-level simulations to reduce rapidly and significantly the solution space, and second from transactional-level simulations for better accuracy to select the most appropriate solution. Our methodology uses the same power modeling approach for the MPSoC at both the functional and transactional levels in order to guarantee the coherence of the estimation strategy. Furthermore, our methodology integrates runtime optimization techniques to reduce the energy/power consumption of the system. The efficiency of the proposed power-aware design methodology was demonstrated through a H.264 video decoder case study. © 2016 Elsevier Inc.","Design space exploration; DVFS; Functional level; MPSoC; Power modeling; Transactional level"
"Count on me: Reliable broadcast and efficient routing in DTNs through social skeletons","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975511088&doi=10.1016%2fj.jpdc.2016.05.011&partnerID=40&md5=f3ebabe1dbc83342db498a2b9e88cef0","This paper challenges the belief that reliable broadcasting and efficient routing primitives are not possible when DTNs are involved. Firstly, we present COM, a reliable broadcasting mechanism for hybrid networks where nodes can rarely use long-range and costly communication links (e.g. 3G) to complete missing opportunistic links. COM is based on the Social Skeleton, a compact and connected subgraph, computed in an efficient and distributed way, that best represents the strongest social links among nodes. COM exploits the Social Skeleton to guarantee reachability of 100% of nodes with the lowest number of long communications. Then we empirically prove that the Social Skeleton can be used to build routing mechanisms upon it. We deliver SR (Skeleton Routing), which involves at most 3 copies per message, and yields delivery rates up to 5.5 times higher than state-of-the-art forwarding protocols. © 2016 Elsevier Inc. All rights reserved.","Efficient multi-hop routing; Reliable Broadcast; Social mobile networks"
"Pipelined fission for stream programs with dynamic selectivity and partitioned state","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975458327&doi=10.1016%2fj.jpdc.2016.05.003&partnerID=40&md5=8eab6a921a2313deedb8b6828bebd833","There is an ever increasing rate of digital information available in the form of online data streams. In many application domains, high throughput processing of such data is a critical requirement for keeping up with the soaring input rates. Data stream processing is a computational paradigm that aims at addressing this challenge by processing data streams in an on-the-fly manner, in contrast to the more traditional and less efficient store-and-then process approach. In this paper, we study the problem of automatically parallelizing data stream processing applications in order to improve throughput. The parallelization is automatic in the sense that stream programs are written sequentially by the application developers and are parallelized by the system. We adopt the asynchronous data flow model for our work, which is typical in Data Stream Processing Systems (DSPS), where operators often have dynamic selectivity and are stateful. We solve the problem of pipelined fission, in which the original sequential program is parallelized by taking advantage of both pipeline parallelism and data parallelism at the same time. Our pipelined fission solution supports partitioned stateful data parallelism with dynamic selectivity and is designed for shared-memory multi-core machines. We first develop a cost-based formulation that enables us to express pipelined fission as an optimization problem. The bruteforce solution of this problem takes a long time for moderately sized stream programs. Accordingly, we develop a heuristic algorithm that can quickly, but approximately, solve the pipelined fission problem. We provide an extensive evaluation studying the performance of our pipelined fission solution, including simulations as well as experiments with an industrial-strength DSPS. Our results show good scalability for applications that contain sufficient parallelism, as well as close to optimal performance for the heuristic pipelined fission algorithm. © 2016 Elsevier Inc. All rights reserved.","Auto-parallelization; Data stream processing; Fission; Pipelining"
"A hybrid computing method of SpMV on CPU–GPU heterogeneous computing systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009917819&doi=10.1016%2fj.jpdc.2016.12.023&partnerID=40&md5=73320ae8bb4968a80e793735cf3e07d3","Sparse matrix–vector multiplication (SpMV) is an important issue in scientific computing and engineering applications. The performance of SpMV can be improved using parallel computing. The implementation and optimization of SpMV on GPU are research hotspots. Due to some irregularities of sparse matrices, the use of a single compression format is not satisfactory. The hybrid storage format can expand the range of adaptation of the compression algorithms. However, because of the imbalance of non-zero elements, the parallel computing capability of a GPU cannot be fully utilized. The parallel computing capability of a CPU is also rising due to increased number of cores in CPU. However, when a GPU is computing, the CPU controls the process instead of contributing to the computational work. It leads to under-utilization of the computing power of CPU. Due to the characteristics of the sparse matrices, the data can be split into two parts using the hybrid storage format to be allocated to CPU and GPU for simultaneous computing. In order to take full advantage of computing resources of CPU and GPU, the CPU–GPU heterogeneous computing model is adopted in this paper to improve the performance of SpMV. With analysis of the characteristics of CPU and GPU, an optimization strategy of sparse matrix partitioning using a distribution function is proposed to improve the computing performance of SpMV on the heterogeneous computing platform. The experimental results on two test machines demonstrate noticeable performance improvement. © 2017 Elsevier Inc.","Heterogeneous computing; Hybrid storage format; Partition; Sparse matrix–vector multiplication"
"A chaos-oriented prediction and suppression model to enhance the security for cyber physical power systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008498802&doi=10.1016%2fj.jpdc.2016.11.015&partnerID=40&md5=cec0304bec82c746c1732ea70f7d6127","Smart grid has become a fully automated system with common use of electronic equipments to control, compute and communicate. This can be modeled as cyber physical power systems (CPPS) to analyze connections and interactions, and security is one of the key factors to find the inner unmanaged vulnerabilities. In some cases, voltage source inverters (VSI) with pulse width modulation (PWM) are sensitive to initial conditions, and this is a typical chaotic state. In this paper, we present a CPPS model with the improved chaos prediction and suppression methods to enhance the security with the abilities to avoid and eliminate chaos. The main process is “prediction–quantization–control”. First, an improved maximum velocity criterion method is used to predict the possibility of chaos in the parameter space of CPPS. Second, when a possibility of chaos is predicted, a sinusoidal wave frequency modulation is used to suppress chaos. To analyze the connections of the inner entities’ transmission, a security coefficient is introduced to obtain hazard perception and to express measurements quantitatively. Third, it has proposed a chaotic control algorithm based on a fuzzy model with closed-loop dynamic mechanism to solve the security coefficient effectively. The experiments prove the predicted result has a margin of error of plus or minus 10%, and the sinusoidal wave frequency modulation can avoid chaos on CPPS. Also, the results show that security coefficient increases by 13.89%, which means it is useful with sensitive parameters and other related disturbances. Overall, this CPPS model can eliminate potential chaos phenomena caused by inversion processes, and it can be used into smart grids to improve the security effectively. © 2016 Elsevier Inc.","Chaos prediction; Chaos suppression; Cyber-physical power system; Pulse width modulation; Voltage source inverters"
"Heterogeneous packet processing in shared memory buffers","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984674732&doi=10.1016%2fj.jpdc.2016.07.002&partnerID=40&md5=455e26d4a1b8060d9b3b8f0a3477e6d8","Packet processing increasingly involves heterogeneous requirements. We consider the well-known model of a shared memory switch with bounded-size buffer and generalize it in two directions. First, we consider unit-sized packets labeled with an output port and a processing requirement (i.e., packets with heterogeneous processing), maximizing the number of transmitted packets. We analyze the performance of buffer management policies under various characteristics via competitive analysis that provides uniform guarantees across traffic patterns (Borodin and ElYaniv 1998). We propose the Longest-Work-Drop policy and show that it is at most 2-competitive and at least 2-competitive. Second, we consider another generalization, posed as an open problem in Goldwasser (2010), where each unit-sized packet is labeled with an output port and intrinsic value, and the goal is to maximize the total value of transmitted packets. We show first results in this direction and define a scheduling policy that, as we conjecture, may achieve constant competitive ratio. We also present a comprehensive simulation study that validates our results. © 2016 Elsevier Inc.","Admission control; Buffer management; Competitive analysis"
"Towards the efficient parallelization of multi-pass adaptive blocking for entity matching","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997171536&doi=10.1016%2fj.jpdc.2016.11.002&partnerID=40&md5=1cbbf119875bbf549ca3a33237643c4b","Modern parallel computing programming models, such as MapReduce (MR), have proven to be powerful tools for efficient parallel execution of data-intensive tasks such as Entity Matching (EM) in the era of Big Data. For this reason, studies about challenges and possible solutions of how EM can benefit from this well-known cloud computing programming model have become an important demand nowadays. Furthermore, the effectiveness and scalability of MR-based implementations for EM depend on how well the workload distribution is balanced among all reduce tasks. In this article, we investigate how MapReduce can be used to perform efficient (load balanced) parallel EM using a variation of the multi-pass Sorted Neighborhood Method (SNM) that uses a varying size (adaptive) window. We propose Multi-pass MapReduce Duplicate Count Strategy (MultiMR-DCS++), a MR-based approach for multi-pass adaptive SNM, aiming to increase even more the performance of the SNM. The evaluation results based on real-world datasets and cluster infrastructure show that our approach increases the performance of MapReduce-based SNM regarding the EM execution time and detection quality. © 2016 Elsevier Inc.","Adaptive windowing; Blocking; Entity matching; Indexing"
"Solving Poisson's equation using FFT in a GPU cluster","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006998906&doi=10.1016%2fj.jpdc.2016.09.004&partnerID=40&md5=afca3b19cfe02148505b3e25fbe40634","Poisson's equation is present in many scientific computations and its efficient solution is achieved by means of several methods. One of the most efficient methods is the Fast Fourier Transform (FFT), which is very widely used in lots of computational problems. In this work we implement a Poisson solver that uses FFT as base method and runs in a cluster of Graphics Processing Units (GPU). We analyze the execution of our implementation to find the main bottlenecks and we compare the results to a CPU based solver. The results show a good scalability up to 16 GPUs, the number of GPUs we have available. Moreover, although the main time consuming part of the process is the network communication, the GPU implementation is about 2.5 times faster than the CPU implementation. © 2016 Elsevier Inc.","CUDA; FFT; GPU; MPI; Poisson's equation"
"Distributed construction of connected dominating set in unit disk graphs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012146189&doi=10.1016%2fj.jpdc.2017.01.023&partnerID=40&md5=3fc953c5ca81e092dbe6ad2e5b37fab3","Let G=(V,E) be a unit disk graph corresponding to a given set P of n points in R2. We propose a distributed approximation algorithm to find a minimum connected dominating set of G. The maintenance of the connected dominating set constructed is fully localized. Our algorithm produces a connected dominating set of size (104opt+52), where opt is the size of a minimum connected dominating set. The time and message complexities of our algorithm are O(Δ) and O(n) respectively, where Δ and n are the maximum node degree and number of nodes in G respectively. Our distributed approximation algorithm outperforms existing algorithms in terms of its time and message complexities. We also propose a scheduling scheme that obtains O(Δ) conflict-free time slots to deal with interference. © 2017 Elsevier Inc.","Approximation algorithm; Connected dominating set; Unit disk graph"
"A novel oriented cuckoo search algorithm to improve DV-Hop performance for cyber–physical systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009748988&doi=10.1016%2fj.jpdc.2016.10.011&partnerID=40&md5=ce1b4e24936cdb914579de8edf3e3d32","Wireless sensor network (WSN) is an important component of a cyber–physical system. Locating node information is a crucial problem for WSN. Currently, distance vector-hop method (DV-Hop), one of popular range-free algorithms, is widely deployed to estimate the location. However, the estimation precision is challenging. In this paper, a new evolutionary algorithm named oriented cuckoo search algorithm (OCS) is designed. In OCS, the global search capability is dominated by the combination of two different random distributions. To provide a deep investigation, ten different random distributions are employed and compared with CEC2013 test suits. Numerical results show the hybrid distribution combined with Lévy distribution and Cauchy distribution achieves the best performance. Furthermore, OCS with this hybrid distribution is also incorporated into the methodology of DV-Hop algorithm to improve the precision performance. Simulation results demonstrate that our modification achieves better precision performance when compared with three other DV-Hop algorithms. © 2016","Cyber–physical systems; DV-Hop algorithm; Lévy flight; Oriented cuckoo search algorithm; Probability distribution"
"Optimizing checkpoint data placement with guaranteed burst buffer endurance in large-scale hierarchical storage systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994491726&doi=10.1016%2fj.jpdc.2016.10.002&partnerID=40&md5=2ce47aab80062f43e399f57a30916389","Non-volatile devices, such as SSDs, will be an integral part of the deepening storage hierarchy on large-scale HPC systems. These devices can be on the compute nodes as part of a distributed burst buffer service or they can be external. Wherever they are located in the hierarchy, one critical design issue is the SSD endurance under the write-heavy workloads, such as the checkpoint I/O for scientific applications. For these environments, it is widely assumed that checkpoint operations can occur once every 60 min and for each checkpoint step as much as half of the system memory can be written out. Unfortunately, for large-scale HPC applications, the burst buffer SSDs can be worn out much more quickly given the extensive amount of data written at every checkpoint step. One possible solution is to control the amount of data written by reducing the checkpoint frequency. However, a direct effect caused by reduced checkpoint frequency is the increased vulnerability window of system failures and therefore potentially wasted computation time, especially for large-scale compute jobs. In this paper, we propose a new checkpoint placement optimization model which collaboratively utilizes both the burst buffer and the parallel file system to store the checkpoints, with design goals of maximizing computation efficiency while guaranteeing the SSD endurance requirements. Moreover, we present an adaptive algorithm which can dynamically adjust the checkpoint placement based on the system's dynamic runtime characteristics and continuously optimize the burst buffer utilization. The evaluation results show that by using our adaptive checkpoint placement algorithm we can guarantee the burst buffer endurance with at most 5% performance degradation per application and less than 3% for the entire system. © 2016 Elsevier Inc.","Burst buffer; Checkpoint; Fault tolerance; Hierarchical storage systems; Solid-state drive"
"NEAT: Network link emulation with adaptive time dilation","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010638148&doi=10.1016%2fj.jpdc.2017.01.013&partnerID=40&md5=15611f11b2f6880a283ab9196bf8a0d4","In evaluating the performance of highly complex networked systems, emulation is often used as it maintains much of the realism of testbeds, while offering increased flexibility and scalability. In large emulation systems, multiple and heterogeneous virtual machines can be deployed in relatively few general purpose physical hosts. Time dilation is a technique that allows virtual time to pass at a different (and potentially variable) rate with respect to real time, allowing for increased scalability of the emulated system. In this paper we present networking links in a large emulated system employing adaptive time dilation. The link emulation focuses on accurate delay and throughput emulation while allowing varying time dilation factors. To evaluate our system, we measure the delay and throughput of the virtual links under variable system loads. © 2017 Elsevier Inc.","Distributed system; KVM; Network emulation; QEMU; Time dilation; Virtual machines; Virtual time"
"Communication and cooling aware job allocation in data centers for communication-intensive workloads","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975488991&doi=10.1016%2fj.jpdc.2016.05.016&partnerID=40&md5=de0600cba663000f79b9bb0d958080e8","Energy consumption is an increasingly important concern in data centers. Today, nearly half of the energy in data centers is consumed by the cooling infrastructure. Existing policies on thermally-aware workload allocation do not consider applications that include many tasks (or threads) running on a large set of nodes with significant communication among the tasks. Such jobs, however, constitute most of the cycles in high performance computing (HPC) domain, and have started to appear in other data centers as well. Job allocation strongly affects the performance of such communication-intensive applications. Communication-aware job allocation methods exist, but they focus solely on performance and do not consider cooling energy. This paper proposes a novel job allocation methodology to jointly minimize communication cost and cooling energy consumption in data centers. We formulate and solve the joint optimization problem using binary quadratic programming. Our joint optimization algorithm reduces cooling energy by 16.4%on average with only a 2.66%average increase in application running time compared to solely performance-aware allocations. To further optimize the communication cost, we develop a Charm++ based framework that extracts the communication behavior of applications. We then integrate our job allocation policy with recursive coordinate bisection (RCB) based task mapping method to place highly-communicating tasks in close proximity. Experimental results show that task mapping further decreases the communication cost by up to 20.9%compared to assuming all-to-all communication, a popular assumption in much of the prior work. © 2016 Elsevier Inc. All rights reserved.","Communication Pattern; Cooling management; High-performance computing; Job allocation; Optimization; Task mapping"
"A modeling language to describe massive data storage management in cyber-physical systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011030476&doi=10.1016%2fj.jpdc.2016.12.008&partnerID=40&md5=1ac15369be66453dd84cea73604af05d","Massive data storage systems (MDSS, for short) are elementary parts of data gathering and analysis in cyber-physical systems. MDSSs have more features than traditional storage systems. An important one is that data files are split and stored into blocks in MDSSs. And each block is considered as a storage unit. Hence, MDSSs usually have two kinds of storage units: ordinary memory locations and block storage locations. Then it comes the question that how we formally model and describe MDSSs. In this paper we propose a modeling language to describe the management programs in MDSSs. New expressions and commands are introduced which mainly focus on block operations. Their denotational semantics are defined using the concepts of heap and store. According fixed-point theorem is proved. Using this method, management programs of MDSSs can be expressed more clearly and intuitively, and it allows us to analyze MDSSs more easily. © 2016","Denotational semantics; Fixed-point theorem; Massive data storage; Modeling language"
"Spread and shrink: Point of interest discovery and coverage with mobile wireless sensors","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007280212&doi=10.1016%2fj.jpdc.2016.09.003&partnerID=40&md5=6d870a99f4507c3c361d1a5ef651d312","In this paper we tackle the problem of deploying mobile wireless sensors while maintaining connectivity with a sink throughout the deployment process. These mobile sensors should discover some points of interest (PoI) in an autonomous way and continuously report information from the observed events to the sink. Unlike previous works, we design an algorithm that uses only local information and local interactions with surrounding sensors. Moreover, unlike other approaches, our algorithm implements both the discovery and the coverage phase. In the discovery phase, the mobile sensors spread to discover new events all over the field and in the second phase, they shrink to concentrate only on the discovered events, named points of interest. We prove that connectivity is preserved during both phases and the spreading phase is terminated in a reasonable amount of time. Real experiments are conducted for small-scale scenarios that are used as a “proof of concept”, while extensive simulations are performed for more complex scenarios to evaluate the algorithm performance. A comparison with an existing work which uses virtual forces has been made as well. The results show the capability of our algorithm to scale fast in both discovery, coverage and shrinking phases. © 2016 Elsevier Inc.","Connectivity; Coverage; Deployment; Discovery; Mobile wireless sensor networks"
"Location distribution of a mobile terminal and its application to paging cost reduction and minimization","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991609102&doi=10.1016%2fj.jpdc.2016.09.002&partnerID=40&md5=4c7f0fd7480f34ed1ca503e699cc5d02","Reducing the cost of dynamic mobility management in wireless communication networks has been an interesting and important issue. It is well known that by using a selective paging method, both costs for location update and terminal paging can be reduced significantly. However, an efficient selective paging method needs the information of the location distribution of a mobile terminal. Based on our previous results on random walks among rings of cell structures, we analyze the location distribution of a mobile terminal in a paging area when a phone call arrives, where the inter-call time and the cell residence time can have arbitrary probability distributions. Our analysis is conducted for both distance-based and movement-based location management schemes, and for two different call handling models, i.e., the call plus location update model and the call without location update model. Together with our earlier results on location distribution in time-based location management schemes, for several selective paging methods, including progressive paging methods, ring paging methods, and cell paging methods, we are able to obtain their expected costs of paging for distance-based, movement-based, and time-based location management schemes. We find that a progressive paging method with very small time delay can reduce the terminal paging cost dramatically, while further increasing the time delay does not result in noticeable reduction of terminal paging cost. Our work reported in this paper significantly extends our understanding of cost reduction and minimization of dynamic location management in wireless communication networks. © 2016 Elsevier Inc.","Cost reduction; Dynamic mobility management; Location distribution; Progressive paging; Selective paging; Time delay; Wireless communication network"
"Collision-tolerant broadcast scheduling in duty-cycled wireless sensor networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994882467&doi=10.1016%2fj.jpdc.2016.10.006&partnerID=40&md5=4644a41c1326d4b0d29419af4456f98c","The minimum-latency broadcast problem in duty-cycled wireless sensor networks has received significant attention over the last few years. A common approach for the problem is to assign collision-free transmitting times to forwarding nodes for disseminating a message from one source node to all other nodes according to their given duty-cycle schedules and transmission ranges. However, preventing collision for all transmissions may increase latency in the broadcast schedules. This paper proposes a novel strategy of Collision-Tolerant Scheduling (CTS) that offers an opportunity to reduce broadcast latency by allowing collisions at non-critical nodes to speed up the broadcast process for critical ones. The completion of broadcast scheduling, i.e. all nodes receive a broadcast message, is ensured by additionally transmitting the message to non-critical nodes experiencing collision. We employ the scheduling strategy in two proposed broadcast schemes: Degree-based CTS (DCTS) and MIS-based CTS (MCTS), which select forwarding nodes based on the node degree and maximal independent set information, respectively. The results of both theoretical analysis and simulation reveal the remarkable advantages of CTS in minimizing broadcast latency in duty-cycled WSNs. DCTS and MCTS guarantee approximation ratios of (Δ−1)T and 12T in terms of broadcast latency, where Δ and T denote the maximum node degree and the number of time slots in a working period, respectively. The two schemes reduce to at least 94 percent of the broadcast latency compared with existing schemes, while slightly increasing the number of transmissions due to the additional transmissions. Thanks to the latency reduction, the proposed schemes require 93 percent less energy than existing ones. © 2016 Elsevier Inc.","Broadcast scheduling; Collision tolerant; Duty cycle; Latency efficient; Wireless sensor networks"
"Relaxing real-time order in opacity and linearizability","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995467249&doi=10.1016%2fj.jpdc.2016.10.007&partnerID=40&md5=58384411da09c9c3137181dc5acb27ce","In this paper we introduce two families of safety properties: ♢-opacity and ♢-linearizability. The new properties relax (to a various degree) the real-time order requirement on transaction execution in opacity and, analogically, the real-time order requirement on operation execution in linearizability. This way we can formalize the guarantees provided by a wide class of strongly consistent replicated systems for which opacity and linearizability are too strong. We show the formal relationship between ♢-opacity and ♢-linearizability which allows us to directly compare semantics of transactional and non-transactional systems and, in particular, opacity and linearizability in their original definitions. We also illustrate how the new properties can be used by proving correctness of Deferred Update Replication, a well known optimistic concurrency control scheme. We show that it satisfies update-real-time opacity, a member of the ♢-opacity family, which allows read-only and aborted transactions to operate on stale (but still consistent) data. © 2016 Elsevier Inc.","Correctness; Deferred update replication; Linearizability; Opacity"
"Distance distribution between nodes in a 3D wireless network","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007137254&doi=10.1016%2fj.jpdc.2016.09.006&partnerID=40&md5=aed190e166aaf89d4fe79d80ece75a97","In assessing the performance characteristics of a mobile, ad-hoc wireless network, the distance between nodes is of critical importance. Previous analysis of distance statistics for such networks have focused on the two-dimensional case, where nodes are distributed in the plane. Here we extend this analysis to three dimensions, deriving the probability distribution for distance between nearest neighboring nodes in a cubic volume. This distribution is further used to derive several measures that have proven useful in quantifying network reliability. The results are applicable to mobile networks of airborne or undersea vehicles, or even in multi-story buildings where nodes are distributed in all three spatial dimensions. © 2016","Euclidean distance; Geometric probability; MANET"
"A portable and adaptable fault tolerance solution for heterogeneous applications","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011867563&doi=10.1016%2fj.jpdc.2017.01.020&partnerID=40&md5=e11853a53d1f778fbd6d7e4c9c85aeb3","Heterogeneous systems have increased their popularity in recent years due to the high performance and reduced energy consumption capabilities provided by using devices such as GPUs or Xeon Phi accelerators. This paper proposes a checkpoint-based fault tolerance solution for heterogeneous applications, allowing them to survive fail-stop failures in the host CPU or in any of the accelerators used. Besides, applications can be restarted changing the host CPU and/or the accelerator device architecture, and adapting the computation to the number of devices available during recovery. The proposed solution is built combining CPPC (ComPiler for Portable Checkpointing), an application-level checkpointing tool, and HPL (Heterogeneous Programming Library), a library that facilitates the development of OpenCL-based applications. Experimental results show the low overhead introduced by the proposal and prove its portability and adaptability benefits. © 2017 Elsevier Inc.","Checkpointing; Fault tolerance; Heterogeneous systems; OpenCL; Portability"
"A hybrid approach of mobile malware detection in Android","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006166560&doi=10.1016%2fj.jpdc.2016.10.012&partnerID=40&md5=b77ac6012de8c8b25715678c902fc707","Android security incidents occurred frequently in recent years. This motivates us to study mobile app security, especially in Android open mobile operating system. In this paper, we propose a novel hybrid approach for mobile malware detection by adopting both dynamic analysis and static analysis. We collect execution data of sample malware and benign apps using a net_link technology to generate patterns of system calls related to file and network access. Furthermore, we build up a malicious pattern set and a normal pattern set by comparing the patterns of malware and benign apps with each other. For detecting an unknown app, we use a dynamic method to collect its system calling data. We then compare them with both the malicious and normal pattern sets offline in order to judge the unknown app. Based on the test on a set of mobile malware and benign apps, we found that our approach achieves better detection success rate than some methods using either static analysis or dynamic analysis. What is more, the proposed approach is generic, which can detect different types of malware effectively. Its detection accuracy can be further improved since the pattern sets can be automatically optimized through self-learning. © 2016 Elsevier Inc.","Android; Malware detection; Pattern match; System call"
"Vertex-disjoint paths in DCell networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977090963&doi=10.1016%2fj.jpdc.2016.05.001&partnerID=40&md5=be8de0597fb5024af6f78ac34adc4f02","The DCell network is suitable for massively scalable data centers with high network capacity by only using commodity switches. In this paper, we construct n+k−1 vertex-disjoint paths between every two distinct vertices of the DCell network. Their longest length is not greater than 2k+1+3, where it was proved that the diameter of a k-dimensional DCell, DCellk, has an upper bound 2k+1−1. Furthermore, we propose an O(k2k) algorithm for finding vertex-disjoint paths between every two distinct vertices in DCell networks. Finally, we give the simulation result of the maximal length of disjoint paths gotten by our algorithm. © 2016 Elsevier Inc.","Algorithm; Data center networks; DCell networks; Disjoint paths"
"Stochastic-based robust dynamic resource allocation for independent tasks in a heterogeneous computing system","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979658366&doi=10.1016%2fj.jpdc.2016.06.008&partnerID=40&md5=e34c25c89656602fb06ac7a1a8c9b728","Heterogeneous parallel and distributed computing systems frequently must operate in environments where there is uncertainty in system parameters. Robustness can be defined as the degree to which a system can function correctly in the presence of parameter values different from those assumed. In such an environment, the execution time of any given task may fluctuate substantially due to factors such as the content of data to be processed. Determining a resource allocation that is robust against this uncertainty is an important area of research. In this study, we define a stochastic robustness measure to facilitate resource allocation decisions in a dynamic environment where tasks are subject to individual hard deadlines and each task requires some input data to start execution. In this environment, the tasks that cannot meet their deadlines are dropped (i.e., discarded). We define methods to determine the stochastic completion times of tasks in the presence of the task dropping. The stochastic task completion time is used in the definition of the stochastic robustness measure. Based on this stochastic robustness measure, we design novel resource allocation techniques that work in immediate and batch modes, with the goal of maximizing the number of tasks that meet their individual deadlines. We compare the performance of our technique against several well-known approaches taken from the literature and adapted to our environment. Simulation results of this study demonstrate the suitability of our new technique in a dynamic heterogeneous computing system. © 2016 Elsevier Inc.","Dynamic resource allocation; Heterogeneous computing; Robustness; Scheduling; Stochastic models"
"Fault tolerant communication-optimal 2.5D matrix multiplication","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013212182&doi=10.1016%2fj.jpdc.2017.01.022&partnerID=40&md5=84dc7f7ed6c6ce65ae4077fcc01d6005","In future computing systems, handling faults efficiently at the algorithmic level is expected to become more and more important. In this paper, we illustrate that in practice classical algorithm-based fault tolerance (ABFT) cannot protect all exponent bits of a floating-point number. Consequently, we extend the method to recover from bit-flips in all positions without additional overhead. We also derive fault detection conditions suitable for multiple checksum encoding vectors. Moreover, we show how to efficiently employ ABFT to protect communication-optimal parallel 2.5D matrix multiplication against bit-flips occurring silently during the computation. Furthermore, we show that for very low fault rates the overhead of fault tolerance in the context of the 2.5D matrix multiplication algorithms can be reduced even further. Numerical experiments on a high-performance cluster illustrate the high scalability and low overhead of our algorithms. We demonstrate the fault tolerance of our approach with randomly and asynchronously injected bit-flips and illustrate that our method can also handle bit-flips occurring at high frequencies. Like in classical ABFT, the overhead per correctable bit-flip of our approach decreases with increasing error rate. © 2017 Elsevier Inc.","2.5D matrix multiplication; ABFT; Bit-flip; Fault tolerance; Silent data corruption"
"Cyber–physical cloud-oriented multi-sensory smart home framework for elderly people: An energy efficiency perspective","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013749624&doi=10.1016%2fj.jpdc.2016.10.005&partnerID=40&md5=102e3f9c9b6f4d2843962169d2a5a6f0","The emerging Cyber–Physical Systems (CPSs) provide a number of services ranging from smart homes to elderly care. With a CPS, a user's (elderly person's) interactions with smart home appliances are sensed, collected, and shared in physical spaces. This interaction is then delivered to cyberspaces for processing in order to monitor energy efficiency. In this paper, we propose an energy-efficient cyber–physical smart home system for monitoring the elderly that uses the potential of cloud computing and big data technologies. Given that an elderly person may need assistance with the complex activities of maintaining energy efficiency, a smart multimedia-enabled middleware assistant is proposed that enables him/her to observe different energy-efficient processes, control smart home appliances through gestures, receive notifications regarding appliance statuses, and share multimedia messages with a community of interest. For data processing, robust and low-dimensional discriminative features are extracted in a server from the data available via sensors and multimedia. Events are also automatically classified in the server to assist caregiver decision-making. To save energy, an automatic switch on/off control system using speech or gesture is developed. Keywords are detected from the speech and used as control commands. Experimental results show the encouraging potential of deploying this framework at a large scale. © 2016 Elsevier Inc.","Cloud analytics for energy data; Energy optimization; Gesture-based appliance control; Multi-sensory data recognition"
"An analytical model based on performance demand of workload for energy-efficient heterogeneous multicore systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997530261&doi=10.1016%2fj.jpdc.2016.05.012&partnerID=40&md5=ff30c6a538a8bdcb6ccc0fe3e73c0d37","Compute platforms are increasingly adopting heterogeneous multicore processing. This paper derives an analytical model to study the benefits and preferred configurations of the single instruction set architecture (ISA) heterogeneous multicore system. We analyze the performance, energy consumption, and energy efficiency of the heterogeneous multicore system under realistic workload based on proposed analytical model. We start by deriving a model for a simple machine that has a single big core (that is high performance) and a single small core (that is energy efficient). Then, we extend our model to deal with machines that have multiple big and small cores. The results obtained using our models show that we can achieve higher performance and lower energy consumption with a heterogeneous multicore system than with a similarly provisioned homogeneous multicore system. Additionally we derive models that assume different practical constraints from ideal heterogeneous multicore model to offer further insights into how to configure given processing cores for optimal efficiency in performance and energy consumption. © 2016 Elsevier Inc.","Analytical model; Energy consumption; Heterogeneous multicore system; Performance demand of workload"
"Fair synchronization","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978198646&doi=10.1016%2fj.jpdc.2016.06.007&partnerID=40&md5=863b8fa29fdda3c775de852027a71fc7","Most published concurrent data structures which avoid locking do not provide any fairness guarantees. That is, they allow processes to access a data structure and complete their operations arbitrarily many times before some other trying process can complete a single operation. Such a behavior can be prevented by enforcing fairness. However, fairness requires waiting or helping. Helping techniques are often complex and memory consuming. Furthermore, it is known that it is not possible to automatically transform every data structure, which has a non-blocking implementation, into the corresponding data structure which in addition satisfies a very weak fairness requirement. Does it mean that for enforcing fairness it is best to use locks? The answer is negative. We show that it is possible to automatically transfer any non-blocking or wait-free data structure into a similar data structure which satisfies a strong fairness requirement, without using locks and with limited waiting. The fairness we require is that no process can initiate and complete two operations on a given resource while some other process is kept waiting on the same resource. Our approach allows as many processes as possible to access a shared resource at the same time as long as fairness is preserved. To achieve this goal, we introduce and solve a new synchronization problem, called fair synchronization. Solving the new problem enables us to add fairness to existing implementations of concurrent data structures, and to transform any solution to the mutual exclusion problem into a fair solution. © 2016 Elsevier Inc.","Concurrent data structures; Fairness; Locks; Mutual exclusion; Non-blocking; Synchronization; Wait-freedom"
"A new power efficient high performance interconnection network for many-core processors","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006710298&doi=10.1016%2fj.jpdc.2016.11.007&partnerID=40&md5=3bff26c4f697333a6205eb97aafd87cc","Next generation high performance computing will most likely depend on the massively parallel computers. The overall performance of a massively parallel computer system is heavily affected by the interconnection network and its processing nodes. Continuing advances in VLSI technologies promise to deliver more power to individual nodes. However, the on-chip interconnection networks consume up to 50% of the total chip power and off-chip bandwidth is limited to the maximum number of possible out going physical links. In addition, the long wiring and low performance of communication network overwhelm the benefit of parallel computer system whereas it increases total cost. In this paper, we propose a new interconnection network that reduces the problems of high power consumption, long wiring length and low bandwidth issues. We have measured the static network performance and required power consumption of our proposed ‘3D-TESH’ interconnection network and compared the performance with other networks at different levels of hierarchy such as inter-chips, inter-nodes and inter-cabinets. 3D-TESH network has achieved about 52.08% better diameter and about 45.71% better average distance than the 3D-Torus network with 12.61% less router power usage at on-chip level. Furthermore, 3D-TESH requires about 41% less router power usage than 5D-Torus at the on-chip level. © 2016 Elsevier Inc.","3D-TESH network; Interconnection network; Power estimation; Routing algorithm; Static network performance"
"Analysis of flip ambiguity for robust three-dimensional node localization in wireless sensor networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979690310&doi=10.1016%2fj.jpdc.2016.06.012&partnerID=40&md5=30b27c1f776ab292e23bb9846688cdb3","To detect flip ambiguity for range-based three-dimensional nodes localization in wireless sensor networks, we have proposed and proved that flip ambiguity detection for three-dimensional node localization is equivalent to finding the intersecting plane that intersects with all range error spheres of the reference nodes of unknown node in the ideal radio model, which is called the existence of intersecting plane (EIP) problem. To solve EIP problem, we further have proposed two algorithms: common tangent plane algorithm (CTP) and orthogonal projection algorithm (OP). The simulation results demonstrate that the computation complexity of OP is lower than CTP, while the detection results of OP is comparable with CTP. © 2016 Elsevier Inc.","Common tangent plane; Flip ambiguity; Orthogonal projection; Three-dimensional node localization; Wireless sensor networks"
"The multi-budgeted and weighted bounded degree metric Steiner network problem","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009289950&doi=10.1016%2fj.jpdc.2016.12.001&partnerID=40&md5=fc344f4eb4381651bb178bbef11975aa","We study the multi-budgeted version of the metric Survivable Network Design Problem (SND) (Jain, 2001), also known as Steiner Network problem, where besides the usual connectivity requirements (i.e., lower bound on the number edge-disjoint paths) between pre-specified pairs of points, we are also asked to satisfy a set of extra linear constraints (the budgets). Based on combinatorial properties of extreme point solutions of the natural LP relaxation of the problem and using the iterative method, we design new approximation algorithms with the following performance guarantee: if any edge participates in at most f∈N budgets, then we show how we can device an (f+2,f+2) bi-criteria approximation algorithm in polynomial time i.e., return a solution with cost at most (2+f) times the optimal one with a potential budget overflow bounded from above by the same factor. In other words, the approximation and budget violation guarantee we achieve is a linear function of the maximum frequency among all the edges in the budgets, as opposed to a function of the number of budgets themselves and this constitutes a strict improvement over the previous approaches. Our approach is flexible enough and we demonstrate how it can be used to provide a (4,4) bi-criteria algorithm for the Minimum Weighted Bounded Degree version of the SND problem in which we have the standard SND connectivity constraints and, additionally, for every vertex v in the graph we have an upper bound on the sum of the weights of the edges incident to that edge in any feasible solution. These problems, besides their obvious theoretical interest, have numerous real life applications from Cloud Computing to Optical Networking systems. © 2016 Elsevier Inc.","Cloud Computing; Communication networks; Design and analysis of algorithms; Fault tolerant computing; Survivable Network Design"
"An adaptive cache coherence protocol: Trading storage for traffic","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008392087&doi=10.1016%2fj.jpdc.2016.12.020&partnerID=40&md5=e8c0a26380fedad08ca444fed5efa864","This paper introduces a new adaptive cache coherence protocol which minimizes energy requirements and guarantees scalability. It includes two complementary parts: a non-inclusive sparse-directory to track only actively shared blocks and a structure to determine the presence of a block in the private caches based on an improved counting bloom filter. It uses token counting to preserve the system correctness, to improve performance and to reduce the implementation complexity. Combining all these characteristics, the proposal has a low storage overhead and is able to suppress most of the traffic inherent to snoop-based protocols and reduce the size of directory-based structures. Using a capacity to track only 40% of all the blocks allocated in the private caches, this coherence protocol is able to achieve better performance than an over-provisioned sparse-directory with a capacity to track 160% of the blocks kept in private caches. The complementarity of both structures enables the coherence controller to change dynamically the way the storage available is dedicated according to the data-sharing properties of the application. Thus, applications with high-sharing degree will need more directory space while low-sharing degree patterns will need more private block-presence space to include more information. With only 5% of the private cache entries tracked, the average performance degradation is less than 8% compared to a 160% over-provisioned sparse-directory. © 2016 Elsevier Inc.","CMPs; Coherence protocol; Multicore"
"CLAP-NET: Bandwidth adaptive optical crossbar architecture","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973560783&doi=10.1016%2fj.jpdc.2016.05.004&partnerID=40&md5=3884da21927c0eaf495c26e7a0820d45","While the number of processing cores placed on individual silicon dies climbs towards hundreds, and even thousands of cores, there is growing demand for efficient and scalable on-chip interconnects. Offering many advantages over metallic interconnects, nanophotonic interconnects enable new design possibilities, however, nanophotonic interconnects also require an off-chip laser source, which is often wasted to insertion losses and periods of low network activity. We present a new optical crossbar architecture that leverages capabilities of nanophotonics to provide high-performance inter-core communication while maximizing utilization of the laser power by means of dynamic bandwidth and laser power reconfiguration schemes. We compare our architecture with other proposed optical crossbar designs according to power consumption, throughput, and latency. We evaluate the network using synthetic patterns to show approximately a 13% improvement in throughput can be achieved compared to other optical crossbar designs, and a 92% improvement compared to a conventional electrical flattened butterfly architecture. © 2016 Elsevier Inc.","Bandwidth reallocation; Multicore; Network-on-chip; Optical crossbar; Optical interconnects; Power reconfiguration"
"Intersecting two families of sets on the GPU","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013218729&doi=10.1016%2fj.jpdc.2017.01.026&partnerID=40&md5=d78d42945bed6622beda01488f636b0d","The computation of the intersection family of two large families of unsorted sets is an interesting problem from the mathematical point of view which also appears as a subproblem in decision making applications related to market research or temporal evolution analysis problems. The problem of intersecting two families of sets F and F′ is to find the family I of all the sets which are the intersection of some set of F and some other set of F′. In this paper, we present an efficient parallel GPU-based approach, designed under CUDA architecture, to solve the problem. We also provide an efficient parallel GPU strategy to summarize the output by removing the empty and duplicated sets of the obtained intersection family, maintaining, if necessary, the sets frequency. The complexity analysis of the presented algorithm together with experimental results obtained with its implementation is also presented. © 2017 Elsevier Inc.","Algorithm; CUDA; Graphics processing unit (GPU); Intersection of families of sets"
"Hardware-accelerated generation of 3D diffusion-limited aggregation structures","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978208638&doi=10.1016%2fj.jpdc.2016.06.009&partnerID=40&md5=19dda1f980f4b21bd238cc061a59f54a","The diffusion and aggregation of particles in a medium can result in complex geometric forms with an artistic interpretation, yet these aggregates can represent many natural processes as well. Although the method is quite simple, it takes many particles to form an aggregation. If the process is simulated using a computer, it directly translates into lengthy computation times. In this paper, the acceleration of the diffusion-limited aggregation was investigated. The algorithm of aggregation was implemented on a serial single-core CPU, and that served as the base-case. With the aim of reducing run times, the algorithm was implemented on three accelerator architectures using OpenCL as the connecting software framework. Performance testing of the OpenCL implementation was done on a multi-core CPU, a GPU and an FPGA. Metrics such as run time, relative speedup and speedup-per-watt were used to compare the hardware-accelerated implementations. Even though using a GPU is not the most economical alternative energy-wise, its performance resulted in the highest speedup, while an FPGA or a multi-core CPU offered other viable options in accelerating the creation of diffusion-limited aggregation structures. © 2016 Elsevier Inc.","Diffusion-limited aggregation; FPGA; GPU; Hardware acceleration; Multi-core CPU; OpenCL; Simulation"
"Reliable and efficient hierarchical organization model for computational grid","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013436297&doi=10.1016%2fj.jpdc.2017.01.027&partnerID=40&md5=7e674427ff23f5d8cb8cd4e8cc46a719","Although the hierarchical model appears to be an effective solution to organize the resource managements in grid systems which have more stringent demand for both scalability and efficiency, it has some limitations which need to be addressed. For example, the master/manager resources in different levels represent single points of failure and they may be sources of bottleneck and communication overhead especially if they are not efficiently selected. Moreover, the dynamic and fault-prone nature of grids cannot be treated by static structures while the manual construction and repairing are also prohibitive due to the highly caused overhead which often represents a significant obstruction to an efficient resource utilization (especially for those with intermittent availabilities). The main objective of this paper is to first introduce a self-repairing n-try dynamic hierarchical grid model for scheduling and load balancing in which each master resource will be replicated on one of its children resources. Second, an efficient methodology to elect masters–replicas resources is proposed. In this methodology, the masters–replicas are selected based on both resource reliability (in terms of MTBF) and resource proximity from the other nodes in specified groups (in terms of communication latency). Validation of the proposed methodology based on the proposed model is done via simulation. Experimental results show that the proposed model has a great impact on the overall performance. Compared to other approaches, the simulations show that our approach decrements the average completion time (ACT) by 18.9%–25%, increases the tree stability ratio (TSR) up to 26.2%–27.1%, and minimizes the total communication overhead (TCO) by 4.4%–18.7% in the range of system parameter values examined. © 2017 Elsevier Inc.","Computational grid; Distributed system; Fault tolerance; Grid scheduling; Load balancing"
"Energy-aware scheduling on heterogeneous multi-core systems with guaranteed probability","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009469636&doi=10.1016%2fj.jpdc.2016.11.014&partnerID=40&md5=e7b213ff58d0a849fdc65bdcca7c1d76","The main challenge for embedded real-time systems, especially for mobile devices, is the trade-off between system performance and energy efficiency. Previous works mainly focused on finding an optimal tasks assignment with the minimum energy under the constraints of time or architecture. In this paper, we propose an Accelerated Search (AS) algorithm based on Dynamic Programming (DP) to obtain a combination of various task schemes which can be completed in a given time with the minimum possible energy by introducing the guaranteed probability and data migration energy. We adopt a DAG (Directed Acyclic Graph) to represent the dependent relation between tasks and develop a Minimum-Energy Model to find the optimal tasks assignment. The heterogeneous multi-core architectures can execute tasks under different voltage levels with DVFS (Dynamic Voltage and Frequency Scaling) which leads to different execution times and different consumption energies. We first design a Minimum Energy Under Probability Constraints (MEUPC) algorithm to assign a proper core and proper voltage level to each task to satisfy the probability constraints with the minimum energy and then a Leaf-Partition (LP) algorithm is used to determine the execution sequence on each core according to the position of the task in DAG. Finally, a Trading Energy For Time (TEFT) algorithm is proposed to explore the opportunity the parallelism of the tasks to reduce the execution time. The experimental results demonstrate that our approach outperforms state-of-the-art algorithms in this field (maximum improvement of 30.7%). © 2016 Elsevier Inc.","Accelerated search algorithm; Heterogeneous multi-core embedded system; Minimum energy model; Optimized energy scheduling; Probability statistics"
"A CPS framework based perturbation constrained buffer planning approach in VLSI design","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009493278&doi=10.1016%2fj.jpdc.2016.11.013&partnerID=40&md5=e2ae82a5ffbc2063aa4f5221ef8c329c","As VLSI technology advances towards nanoscale devices, interconnect delay is becoming increasingly important, and could be effectively reduced using buffer insertion. The widely-used buffer insertion technique in industry is to insert a set of buffers on the chip, which may overlap some gates, and then greedily move the buffers to the nearest available buffer holes. The moving distance of inserted buffers largely affects the wirelength which may result in the increase of the interconnect delay. This necessitates efficient algorithms to minimize the moving distance of buffers for effective buffer insertion to obtain high-performance VLSI designs. This paper proposes an efficient, perturbation constrained buffer planning algorithm to maximize the candidate buffer holes with regarding to the feature of CPS based buffering design framework. Instead of directly moving buffers to the existing available buffer holes, the proposed algorithm changes the original placement by moving some gates tinily to provide more flexibility for buffer insertion. The integer linear programming based technique is designed for the physical design flow which allows small moving range of gates. Parallel technique is utilized to solve the ILP problems efficiently when the scale of chip is increasing. Experimental results have shown that the proposed algorithm achieves at most 41.49% increase in the available buffer holes when compared to the algorithm with no gate movement. © 2016 Elsevier Inc.","Buffer insertion; Buffer planning; Integer linear programming; Parallel computing; Perturbation"
"Shared resource aware scheduling on power-constrained tiled many-core processors","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994753671&doi=10.1016%2fj.jpdc.2016.10.001&partnerID=40&md5=4845d017eaea179848e866488f457320","Power management through dynamic core, cache and frequency adaptation is becoming a necessity in today's power-constrained many-core environments. Unfortunately, as core count grows, the complexity of both the adaptation hardware and the power management algorithms increases exponentially. This calls for hierarchical solutions, such as on-chip voltage regulators per-tile rather than per-core, along with multi-level power management. As power-driven adaptation of shared resources affects multiple threads at once, the efficiency in a tile-organized many-core processor architecture hinges on the ability to co-schedule compatible threads to tiles in tandem with hardware adaptations per tile and per core. In this paper, we propose a two-tier hierarchical power management methodology to exploit per-tile voltage regulators and clustered last-level caches. In addition, we include a novel thread migration layer that (i) analyzes threads running on the tiled many-core processor for shared resource sensitivity in tandem with core, cache and frequency adaptation, and (ii) co-schedules threads per tile with compatible behavior. On a 256-core setup with 4 cores per tile, we show that adding sensitivity-based thread migration to a two-tier power manager improves system performance by 10% on average (and up to 20%) while using 4× less on-chip voltage regulators. It also achieves a performance advantage of 4.2% on average (and up to 12%) over existing solutions that do not take DVFS sensitivity into account. © 2016 Elsevier Inc.","Adaptive microarchitecture; Many-core tiled architecture; Power budget; Thread migration"
"Minimizing deep sea data collection delay with autonomous underwater vehicles","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010705254&doi=10.1016%2fj.jpdc.2017.01.006&partnerID=40&md5=36f1c920e125da485a7d14f0606e7b6c","As a special application of delay tolerant networks (DTNs), efficient data collection in the deep sea poses some unique challenges, due to the need for timely data reporting and the delay of acoustic transmission in the ocean. Autonomous underwater vehicles (AUVs) are deployed in the deep sea to surface frequently to transmit collected data from sensors (in a 2-dimensional or 3-dimensional search space) to the surface stations. However, additional delay occurs at each resurfacing. In this paper, we want to minimize the average data reporting delay, through optimizing the number and locations of AUV resurfacing events. We also study the AUV trajectory planning using an extended Euler circuit, where the search space is a set of segments (e.g., oil pipes) in the deep sea. To further reduce the data reporting delay, several schemes, which schedules multiple AUVs cooperatively, are also explored. Finally, experiments in both the synthetic and real traces validate the efficiency and effectiveness of the proposed algorithms. © 2017 Elsevier Inc.","Autonomous underwater vehicles; Deep sea data collection; Delay tolerant networks; Euler circuit; Trajectory scheduling"
"EnDebug: A hardware-software framework for automated energy debugging","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975472143&doi=10.1016%2fj.jpdc.2016.05.005&partnerID=40&md5=e3cf81ef68484f04674bb02f6ca13240","Energy consumption by software applications is one of the critical issues that determine the future of multicore software development. Inefficient software has been often cited as a major reason for wasteful energy consumption in computing systems. Without adequate tools, programmers and compilers are often left to guess the regions of code to optimize, that results in frustrating and unfruitful attempts at improving application energy. In this paper, we propose enDebug, an energy debugging framework that aims to automate the process of energy debugging. It first profiles the application code for high energy consumption using a hardware-software cooperative approach. Based on the observed application energy profile, an automated recommendation system that utilizes artificial selection genetic programming is used to generate the energy optimizing program mutants while preserving functional accuracy. We demonstrate the usefulness of our framework using several Splash-2, PARSEC-1.0 and SPEC CPU2006 benchmarks, where we were able to achieve up to 7% energy savings beyond the highest compiler optimization (including profile guided optimization) settings on real-world Intel Core i7 processors. © 2016 Elsevier Inc. All rights reserved.","Energy optimization; Energy profiling; Genetic programming"
"An efficient hypercube labeling schema for dynamic Peer-to-Peer networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009115807&doi=10.1016%2fj.jpdc.2016.12.022&partnerID=40&md5=259117777534a829073a97a9809d6ee0","This paper addresses the general problem of reducing unnecessary message transmission thereby lowering overall bandwidth utilization in a Peer-to-Peer (P2P) network. In particular, we exploit the characteristics of a P2P network engineered to resemble a hypercube. The reason for doing this is to achieve constant computation time for inter-node distances that are needed in the process of query optimization. To realize such a hypercube-like engineered structure, we develop a new labeling scheme which assigns identifiers (labels) to each node and then uses these labels to determine inter-node distances as is done in a hypercube, thus eliminating the need to send out queries to find the distance from one node to another. The labels allow for creating a virtual overlay which resembles a hypercube. We prove that the labeling scheme does in fact allow for reducing bandwidth utilization in the network. To confirm our theoretical findings we conduct various experiments with randomly selected P2P networks of various sizes. Detailed statistics on the outcome of these experiments are provided which show clearly the practical utility of the labeling approach. © 2016 Elsevier Inc.","Distributed algorithms; Hypercube networks; P2P networks; Structured overlays"
"The spanning connectivity of the arrangement graphs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982690188&doi=10.1016%2fj.jpdc.2016.07.005&partnerID=40&md5=c49cfa9ce21479c7124ed215287b8bd1","A w-container Cw(u,v) of a graph G between two distinct vertices u and v is a set of w disjoint paths between u and v. A w-container Cw(u,v) is a w∗-container if every vertex of G is on some path in Cw(u,v). A graph G is said to be w∗-connected if there exists a w∗-container between any two distinct vertices u and v. The connectivity of the arrangement graph An,k is k(n−k). In this paper, we prove that An,k is k(n−k)∗-connected for n−k≥2. © 2016 Elsevier Inc.","Arrangement graph; Interconnection network; Spanning connectivity"
"A novel approach to accelerate calibration process of a k-nearest neighbours classifier using GPU","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012288996&doi=10.1016%2fj.jpdc.2017.01.003&partnerID=40&md5=4c7a9fef88e0738fd5d08ca19661a25e","General purpose data parallel computing with graphical processing unit (GPU) is much structured today with NVIDIA®  CUDA and other parallel programming frameworks. Exploiting the CUDA programming framework, the present work proposes a novel methodology formulated around the GPU hardware architecture and memory hierarchy to accelerate the calibration process of a classification model named eNN10. Primarily developed for avalanche forecasting, eNN10 is based on brute force k-nearest neighbours (k-NN) approach and employs snow-meteorological variables to search for past days with similar conditions. The events associated with past similar days are then analysed to generate forecast. The model is required to be calibrated regularly to ensure higher degree of forecast accuracy in terms of Heidke skill score (HSS). The calibration of eNN10 is carried out by Artificial Bee Colony (ABC) algorithm, a swarm intelligence driven population based metaheuristic algorithm, and it requires thousands of HSS evaluations during the complete calibration process. A MATLAB sequential code for calibration runs for over 400 minutes and the proposed methodology delivered about 10× acceleration in calibration process. The methodology combines primitives of parallel implementations of brute force k-NN algorithm with that of population based metaheuristic algorithms and is scalable to deal with other similar real-world problems. The major objective of this paper is to highlight the methodology and associated future research areas. © 2017 Elsevier Inc.","ABC algorithm; Avalanche forecasting; GPU; k-nearest neighbours; NVIDIA CUDA; SIMD"
"Higher dimensional Eisenstein–Jacobi networks","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007246632&doi=10.1016%2fj.jpdc.2016.11.006&partnerID=40&md5=ed3b8fb25553f4b00b0e240344e6596d","An efficient interconnection topology called Eisenstein–Jacobi (EJ) network has been proposed in Martínez et al. (2008). In this paper this concept is generalized to higher dimensions. Important properties such as distance distribution and the decomposition of higher dimensional EJ networks into edge-disjoint Hamiltonian cycles are explored in this paper. In addition, an optimal shortest path routing algorithm and a one-to-all broadcast algorithm for higher dimensional EJ networks are given. Further, we give comparisons between higher EJ networks and Generalized Hypercube (GHC) networks and we show that higher EJ networks cost less and have more nodes than GHC networks. © 2016 Elsevier Inc.","Broadcasting; Eisenstein–Jacobi integers; Hamiltonian decomposition; Interconnection network; Routing"
"Energy-efficient multigrid smoothers and grid transfer operators on multi-core and GPU clusters","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996867980&doi=10.1016%2fj.jpdc.2016.05.006&partnerID=40&md5=d7a687d521cd336677f713882e429555","We investigate time and energy to solution for the CPU- and GPU-based execution of the compute intensive smoother and grid transfer operators in a geometric multigrid linear solver. We use a hybrid parallel implementation for both shared and distributed memory multi-core host systems comprising CUDA-capable devices. Our numerical experiments are designed to assess the effect of combining an MPI-parallel multigrid framework with OpenMP host threads or CUDA accelerators instead of MPI-only CPU computations for various parallel setups. We present runtime and energy measurements from a quad-CPU test system equipped with two GPUs. We find that using an accelerated asynchronous smoother can yield substantial savings of time and energy to solution over using a host-only Jacobi smoother in small and medium sized host systems with one or two multi-core CPUs. The acceleration of the grid transfer operators also yields a benefit, yet smaller than the benefit from the smoother. For large host systems a hybrid MPI-OpenMP parallelization turns out to be most beneficial with respect to energy consumption, although it is not the fastest option. © 2016","Energy-aware numerics; Geometric multigrid; Heterogeneous platforms; Hybrid parallelization; Performance and energy assessment"
"Parallel algorithms for switching edges in heterogeneous graphs","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009143814&doi=10.1016%2fj.jpdc.2016.12.005&partnerID=40&md5=6238967571fd2ea8133d6a526ff8ddb0","An edge switch is an operation on a graph (or network) where two edges are selected randomly and one of their end vertices is swapped with each other. Edge switch operations have important applications in graph theory and network analysis, such as in generating random networks with a given degree sequence, modeling and analyzing dynamic networks, and in studying various dynamic phenomena over a network. The recent growth of real-world networks motivates the need for efficient parallel algorithms. The dependencies among successive edge switch operations and the requirement to keep the graph simple (i.e., no self-loops or parallel edges) as the edges are switched lead to significant challenges in designing a parallel algorithm. Addressing these challenges requires complex synchronization and communication among the processors leading to difficulties in achieving a good speedup by parallelization. In this paper, we present distributed memory parallel algorithms for switching edges in massive networks. These algorithms provide good speedup and scale well to a large number of processors. A harmonic mean speedup of 73.25 is achieved on eight different networks with 1024 processors. One of the steps in our edge switch algorithms requires the computation of multinomial random variables in parallel. This paper presents the first non-trivial parallel algorithm for the problem, achieving a speedup of 925 using 1024 processors. © 2016","Edge switch; Multinomial distribution; Network dynamics; Parallel algorithms; Random network generation"
"Market-based autonomous resource and application management in private clouds","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994728946&doi=10.1016%2fj.jpdc.2016.10.003&partnerID=40&md5=ab6fe560b4a51440fb49922afc60342e","High Performance Computing (HPC) clouds need to be efficiently shared between selfish tenants having applications with different resource requirements and Service Level Objectives (SLOs). The main difficulty relies on providing concurrent resource access to such tenants while maximizing the resource utilization. To overcome this challenge, we propose Merkat, a market-based SLO-driven cloud platform. Merkat relies on a market-based model specifically designed for on-demand fine-grain resource allocation to maximize resource utilization and it uses a combination of currency distribution and dynamic resource pricing to ensure proper resource distribution among tenants. To meet the tenant's SLO, Merkat uses autonomous controllers, which apply adaptation policies that: (i) dynamically tune the application's provisioned CPU and memory per virtual machine in contention periods, or (ii) dynamically change the number of virtual machines. Our evaluation with simulation and on the Grid’5000 testbed shows that Merkat provides flexible support for different application types and SLOs and good tenant satisfaction compared to existing centralized systems, while the infrastructure resource utilization is improved. © 2016 Elsevier Inc.","Cloud computing; Elastic scaling; HPC; Market mechanisms; Resource management; Service Level Objective"
"A privacy-preserved full-text retrieval algorithm over encrypted data for cloud storage applications","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988527726&doi=10.1016%2fj.jpdc.2016.05.017&partnerID=40&md5=4184eb022fc3d78059a4b85aa75c3bda","As Cloud Computing becomes prevalent, more and more sensitive information has been outsourced into cloud. A straightforward methodology that can protect data privacy is to encrypt the data before outsourcing. Recently, many searchable encryption schemes have been proposed to allow users to execute keyword-based search over encrypted data. However, it is different for users to exactly find all the interested files from the huge amounts of data by relying solely on keyword-based search. In information retrieval domain, full-text retrieval is an efficient information retrieval technology that allows efficient searches over massive amount of web data. Unfortunately, when applied in the cloud paradigm, full-text retrieval over encrypted cloud data have not been well studied. The full-text retrieval service requires extracting all the words in the contents of documents. The huge scale of index words cannot be efficiently supported by the existing searchable encryption schemes. Moreover, to protect user's privacy, a privacy-preserved full-text retrieval index is required. These problems make efficient full-text retrieval over a large amount of encrypted cloud data a very challenging task. In this paper, we first establish a set of strict privacy requirements for full-text retrieval in cloud storage systems. To address the challenging problem, we design a Bloom filter based tree index. Our scheme fine-tunes the similarity between the query and encrypted documents by proposing the membership entropies of index words. Our scheme is provably secure through our security analysis. We demonstrate the effectiveness and efficiency of the proposed scheme through extensive experimental evaluation. The experimental results manifest the search operation can be done in 60 milliseconds using an off-the-shelf moderate PC. © 2016","Cloud computing; Hierarchical Bloom filter tree index; Membership entropy; Privacy-preserved full-text retrieval"
"Elastic transactions","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995704466&doi=10.1016%2fj.jpdc.2016.10.010&partnerID=40&md5=5129e4f297af3044b85ed6699131857c","This paper presents elastic transactions, an appealing alternative to traditional transactions, in particular to implement search structures in shared memory multicore architectures. Upon conflict detection, an elastic transaction might drop what it did so far within a separate transaction that immediately commits, and resume its computation within a new transaction which might itself be elastic. We present the elastic transaction model and an implementation of it, then we illustrate its simplicity and performance on various concurrent data structures, namely double-ended queue, hash table, linked list, and skip list. Elastic transactions outperform classical ones on various workloads, with an improvement of 35% on average. They also exhibit competitive performance compared to lock-based techniques and are much simpler to program with than lock-free alternatives. © 2016 Elsevier Inc.","Concurrent data structures; Transactional memory"
"Energy efficiency for cloud computing system based on predictive optimization","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.11.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007573032&doi=10.1016%2fj.jpdc.2016.11.011&partnerID=40&md5=4f5288d868698affebc96d84922bfa80","In recent years, power consumption has become one of the hottest research trends in computer science and industry. Most of the reasons are related to the operational budget and the environmental issues. In this paper, we would like to propose an energy-efficient solution for orchestrating the resource in cloud computing. In nature, the proposed approach firstly predicts the resource utilization of the upcoming period based on the Gaussian process regression method. Subsequently, the convex optimization technique is engaged to compute an appropriate quantity of physical servers for each monitoring window. This quantity of interest is calculated to ensure that a minimum number of servers can still provide an acceptable quality of service. Finally, a corresponding migrating instruction is issued to stack the virtual machines and turn off the idle physical servers to achieve the objective of energy savings. In order to evaluate the proposed method, we conduct the experiments using synthetic data from 29-day period of Google traces and real workload from the Montage open-source toolkit. Through the evaluation, we show that the proposed approach can achieve a significant result in reducing the energy consumption as well as maintaining the system performance. © 2016 Elsevier Inc.","Convex optimization; Energy efficiency; Gaussian process; IaaS cloud computing; Predictive analysis"
"Enhancing scalability in best-effort hardware transactional memory systems","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010674489&doi=10.1016%2fj.jpdc.2017.01.002&partnerID=40&md5=0a5364272c5066f3f110bd4e4c3560dd","Current industry proposals for hardware transactional memory focus on best-effort solutions where hardware limits are imposed on transactions. These designs can efficiently execute transactions but they may abort due to different hardware and operating system limitations, with a significant impact on performance. For instance, transactions cannot survive capacity overflows, exceptions, interrupts, operating system events like page faults, migrations, context switches, and so on. To deal with these events, best-effort hardware transactional memory systems usually provide a software fallback path to execute a non-transactional version of the code. In this paper we propose hardware implementation solutions to make transactions survive some of such limitations, in order to improve the performance and scalability of transactional applications in best-effort systems. First, we propose a hardware irrevocability mechanism that works either when hardware capacity overflows occur or in high contention scenarios. It anticipates capacity overflows and reduces the abort count. This mechanism avoids writing a fallback code, simplifying the programming of the transactional application. Second, we propose a two-phase abort mechanism to support both the execution of privileged mode code inside transactions and the interaction of this code with the irrevocability mechanism. Third, we propose a privileged-aware cache replacement policy to reduce capacity overflows in the presence of privileged code. We evaluate our proposals with all the benchmarks of the STAMP transactional suite and carry out a performance comparison with a fallback-based hardware transactional memory system, after considering different fallback codes, showing significant performance benefits for requester-wins and requester-stalls conflict resolution policies. © 2017 Elsevier Inc.","Best-effort; Cache replacement policy; Hardware transactional memory; Irrevocability; Privileged mode code; Requester-stalls; Requester-wins"
"Two-level main memory co-design: Multi-threaded algorithmic primitives, analysis, and simulation","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009410595&doi=10.1016%2fj.jpdc.2016.12.009&partnerID=40&md5=dd021bb956fa2199d990944e34ae0c3d","A challenge in computer architecture is that processors often cannot be fed data from DRAM as fast as CPUs can consume it. Therefore, many applications are memory-bandwidth bound. With this motivation and the realization that traditional architectures (with all DRAM reachable only via bus) are insufficient to feed groups of modern processing units, vendors have introduced a variety of non-DDR 3D memory technologies (Hybrid Memory Cube (HMC),Wide I/O 2, High Bandwidth Memory (HBM)). These offer higher bandwidth and lower power by stacking DRAM chips on the processor or nearby on a silicon interposer. We will call these solutions “near-memory,” and if user-addressable, “scratchpad.” High-performance systems on the market now offer two levels of main memory: near-memory on package and traditional DRAM further away. In the near term we expect the latencies near-memory and DRAM to be similar. Thus, it is natural to think of near-memory as another module on the DRAM level of the memory hierarchy. Vendors are expected to offer modes in which the near memory is used as cache, but we believe that this will be inefficient. In this paper, we explore the design space for a user-controlled multi-level main memory. Our work identifies situations in which rewriting application kernels can provide significant performance gains when using near-memory. We present algorithms designed for two-level main memory, using divide-and-conquer to partition computations and streaming to exploit data locality. We consider algorithms for the fundamental application of sorting and for the data analysis kernel k-means. Our algorithms asymptotically reduce memory-block transfers under certain architectural parameter settings. We use and extend Sandia National Laboratories’ SST simulation capability to demonstrate the relationship between increased bandwidth and improved algorithmic performance. Memory access counts from simulations corroborate predicted performance improvements for our sorting algorithm. In contrast, the k-means algorithm is generally CPU bound and does not improve when using near-memory except under extreme conditions. These conditions require large instances that rule out SST simulation, but we demonstrate improvements by running on a customized machine with high and low bandwidth memory. These case studies in co-design serve as positive and cautionary templates, respectively, for the major task of optimizing the computational kernels of many fundamental applications for two-level main memory systems. © 2017 Elsevier Inc.","High-bandwidth memory; k-means clustering; Sorting; Two-level memory"
"Caching architecture for flexible FPGA ray tracing platform","2017","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2017.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009992406&doi=10.1016%2fj.jpdc.2017.01.001&partnerID=40&md5=96ddbc781c97f66079d1bd40f4548603","Ray tracing is a computationally intensive task required by movie-makers to create the highly realistic images they require for motion pictures. GPUs currently dominate as hardware accelerators in the multi-billion dollar movie industry. However, FPGAs with their flexibility have the potential to be more power efficient accelerators. This paper investigates and proposes an FPGA acceleration platform to easily and efficiently explore parallelism in ray tracing on FPGAs. It is integrated with LuxRender, a modern and open rendering engine. A major focus is on the proposal of a cache architecture, given that memory bandwidth is identified as a bottleneck. For the design of the domain-specific cache, we study the typical memory access patterns to ray tracing data structures, i.e. acceleration hierarchy and scene primitives. The results lead to the proposal of a data structure specific cache (separating nodes from primitives) and a novel node cache replacement policy. We demonstrate that the proposed cache architecture successfully alleviates the memory bandwidth bottleneck and that the novel replacement policy can provide a good performance increase for small cache sizes and may work well as a complimentary cache along side a direct mapped cache. The evaluation further shows that scaling the number of traversal units with a cache provides an increase in performance over all scenes. We also demonstrate the energy and bandwidth efficiency of the proposed platform in comparison to a CPU and a GPU platform. © 2017 Elsevier Inc.","Application specific cache; Co-processor; Graph traversal; Image processing; Ray tracing; Reconfigurable architectures"
"A high-throughput DPI engine on GPU via algorithm/implementation co-optimization","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948408110&doi=10.1016%2fj.jpdc.2015.11.001&partnerID=40&md5=3c33568ad6ffd6d1aaf56b45182a9a26","The Graphics Processing Unit (GPU) is a promising platform to implement Deep Packet Inspection (DPI) due to the GPU's rich parallelism and programmability for high performance and frequent pattern update requirements. However, it is a great challenge to achieve a high performance implementation due to the GPU's performance sensitivity to algorithm and implementation issues such as memory overhead, thread divergence, and large lookup table sizes. In this paper, we propose algorithm and implementation co-optimization techniques that achieve high performance by reducing required memory, removing thread divergence, optimizing memory access patterns, and optimizing for multithreading. To lower the implementation cost, a GPU performance model is developed to detect the bottlenecks and provide design direction for the GPU kernel. Based on these optimization techniques, a prototype implementation of DPI at 150 Gb/s is achieved on a single NVIDIA K20 GPU. © 2015 Elsevier Inc.","Deep packet inspection; GPU; Optimization techniques; Pattern matching"
"Reactive circuits: Dynamic construction of circuits for reactive traffic in homogeneous CMPs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994556239&doi=10.1016%2fj.jpdc.2016.04.002&partnerID=40&md5=72dfc8f9b98b0ded9aeb36fa0d43f912","Networks on Chip (NoCs) have a large impact on system performance, area, and energy. NoCs convey request and response messages among cores following the message patterns dictated by the cache banks. Such patterns do not only guarantee a coherent memory state, but also provide an opportunity for NoC optimization. Request messages can smartly reserve the resources to dynamically build a circuit for replies, thus reducing their network latency. Starting from this simple idea, which we denote Reactive Circuits, we evaluate several implementations of the mechanism: with and without sharing circuits between messages, performing timed reservations, and removing the implicit coherence messages. A careful implementation of this circuit reservation mechanism in a wormhole router achieves an average 20.8% reduction in network energy consumption, 5.8% smaller router area and a 4.8% system performance increase in a 64-core chip, compared with a conventional network. © 2016 Elsevier Inc.","Chip multiprocessor; Coherence protocol; Interconnection network"
"Parallelizing image feature extraction algorithms on multi-core platforms","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962434258&doi=10.1016%2fj.jpdc.2016.03.001&partnerID=40&md5=0aac8958b56d9da9adbe4eed4dd42039","Currently, multimedia data has become one of the most important data types processed and transferred over the Internet. To extract useful information from a huge amount of such data, SIFT and SURF, as two most popular image feature extraction algorithms, have been widely used in many applications running on multi-core platforms. However, limited parallelism in existing designs makes it hard or impossible to apply them in many applications with real-time requirements. Therefore, it has become one of the major challenges to improve the processing speed of image feature extraction algorithms. In this paper, we first analyze the parallelism constraints in the algorithms, such as imbalanced workloads and indeterminate time distributions. Based on such analyses, we present an adaptive pipeline parallel scheme (AD-PIPE) to adjust the thread number in different stages according to their workloads dynamically, which achieves a balanced partition for constant input workloads. Furthermore, we also implement a power efficient version (AE-PIPE) for AD-PIPE through scheduling threads based on variable input workloads. Experimental results show that AD-PIPE achieves a speedup of 16.88X and 20.33X respectively over SIFT and SURF on a 16-core machine. Moreover, AE-PIPE achieves up to 52.94% and 58.82% power saving with only 3% performance loss. © 2016 Elsevier Inc.","Adaptive pipeline; Image feature extraction; Multi-core; SIFT; SURF"
"Efficient scatter-based kernel superposition on GPU","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938386638&doi=10.1016%2fj.jpdc.2015.07.003&partnerID=40&md5=414df849e10f7c08b9ab79e95e50c55a","Kernel superposition, where an image is convolved with a spatially varying kernel, is commonly used in optics, astronomy, medical imaging and radiotherapy. This operation is computationally expensive and generally cannot benefit from the mathematical simplifications available for true convolutions. We systematically evaluated the performance of a number of implementations of a 2D Gaussian kernel superposition on several graphics processing units of two recent architectures. The 2D Gaussian kernel was used because of its importance in real-life applications and representativeness of expensive-to-evaluate, separable kernels. The implementations were based both on the gather approach found in the literature and on the scatter approach presented here. Our results show that, over a range of kernel sizes, the scatter approach delivers speedups of 2.1-14.5 or 1.3-4.9 times, depending on the architecture. These numbers were further improved to 4.8-28.5 and 3.7-16.8 times, respectively, when only ""exact"" implementations were compared. Speedups similar to those presented are expected for other separable kernels and, we argue, will also remain applicable for problems of higher dimensionality. © 2015 Elsevier Inc.","GPU; Kernel superposition; Point spread function; Scatter; Spatially varying; Variable kernel convolution"
"Data-aware task scheduling for all-to-all comparison problems in heterogeneous distributed systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966267256&doi=10.1016%2fj.jpdc.2016.04.008&partnerID=40&md5=031aa32a3554ad0bd598c6f0e78605e9","Solving large-scale all-to-all comparison problems using distributed computing is increasingly significant for various applications. Previous efforts to implement distributed all-to-all comparison frameworks have treated the two phases of data distribution and comparison task scheduling separately. This leads to high storage demands as well as poor data locality for the comparison tasks, thus creating a need to redistribute the data at runtime. Furthermore, most previous methods have been developed for homogeneous computing environments, so their overall performance is degraded even further when they are used in heterogeneous distributed systems. To tackle these challenges, this paper presents a data-aware task scheduling approach for solving all-to-all comparison problems in heterogeneous distributed systems. The approach formulates the requirements for data distribution and comparison task scheduling simultaneously as a constrained optimization problem. Then, metaheuristic data pre-scheduling and dynamic task scheduling strategies are developed along with an algorithmic implementation to solve the problem. The approach provides perfect data locality for all comparison tasks, avoiding rearrangement of data at runtime. It achieves load balancing among heterogeneous computing nodes, thus enhancing the overall computation time. It also reduces data storage requirements across the network. The effectiveness of the approach is demonstrated through experimental studies. © 2016 Elsevier Inc.","All-to-all comparison; Data distribution; Distributed computing; Task scheduling"
"A failure index for HPC applications","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025832635&doi=10.1016%2fj.jpdc.2016.04.009&partnerID=40&md5=16084f541f19ab6a6fc8ad00f22c6234","This paper conducts an examination of log files originating from High Performance Computing (HPC) applications with known reliability problems. The results of this study further the maturation and adoption of meaningful metrics representing HPC system and application failure characteristics. Quantifiable metrics representing the reliability of HPC applications are foundational for building an application resilience methodology critical in the realization of exascale supercomputing. In this examination, statistical inequality methods originating from the study of economics are applied to health and status information contained in HPC application log files. The main result is the derivation of a new failure index metric for HPC—a normalized representation of parallel application volatility and/or resiliency to complement existing reliability metrics such as mean time between failure (MTBF), which aims for a better presentation of HPC application resilience. This paper provides an introduction to a Failure Index (FI) for HPC reliability and takes the reader through a use-case wherein the FI is used to expose various run-time fluctuations in the failure rate of applications running on a collection of HPC platforms. © 2016 Elsevier Inc.","Adequate level of performance; Failure Index (FI); High Performance Computing; Inequality measures; Resilience; System volatility"
"Design of a Method-Level Speculation framework for boosting irregular JVM applications","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944405551&doi=10.1016%2fj.jpdc.2015.09.005&partnerID=40&md5=e8b3a2b603aff2f39914d12dfbfeed35","Despite the ubiquity of multicores, many commonly-used applications are still sequential. As a consequence, many chip designers are still investing on the creation of chips with a small number of ever-more-complex cores, showing that sequential performance is still a very important issue in some of today's computing systems. To tackle this issue, we have developed JaSPEx-MLS: a software-based automatic parallelization framework targeted at sequential irregular Java/JVM applications, that is based on Method-Level Speculation and Software Transactional Memory, and works atop the OpenJDK HotSpot JVM, a state-of-the-art managed runtime. We aim our framework as a software implementation of the boost feature in modern CPUs, allowing sequential applications to execute faster on multicores whenever parallel versions of those applications are not yet available. In this work, we describe the design of our framework, and introduce several techniques that when combined allow it to parallelize applications successfully with minimal overheads on commonly-available multicores. © 2015 Elsevier Inc. All rights reserved.","Automatic parallelization; First-class continuations; Method-Level Speculation; OpenJDK HotSpot JVM; Software Transactional Memory"
"Optimizing memory transactions for large-scale programs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951779407&doi=10.1016%2fj.jpdc.2015.12.001&partnerID=40&md5=aa5faa0ddcc65eb345cabc7c8bef9d03","Even though Software Transactional Memory (STM) is one of the most promising approaches to simplify concurrent programming, current STM implementations incur significant overheads that render them impractical for many real-sized programs. The key insight of this work is that we do not need to use the same costly barriers for all the memory managed by a real-sized application, if only a small fraction of the memory is under contention - lightweight barriers may be used in this case. In this work, we propose a new solution based on an approach of adaptive object metadata (AOM) to promote the use of a fast path to access objects that are not under contention. We show that this approach is able to make the performance of an STM competitive with the best fine-grained lock-based approaches in some of the more challenging benchmarks. © 2015 Elsevier Ltd. All rights reserved.","Concurrent programming; Runtime optimizations; Software Transactional Memory"
"MPSoCBench: A benchmark for high-level evaluation of multiprocessor system-on-chip tools and methodologies","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994560173&doi=10.1016%2fj.jpdc.2016.03.009&partnerID=40&md5=594d3f439f1bdc6d0d8167100c77b8a2","Recent design methodologies and tools aim at enhancing the design productivity by providing a software development platform before defining the final MPSoC architecture details. Motivated by the lack of MPSoC virtual platforms prototyping integrating both scalable hardware and software in order to create and evaluate new methodologies and tools, we present the MPSoCBench, a scalable set of MPSoCs including four different ISAs (PowerPC, MIPS, SPARC, and ARM) organized in platforms with up to 64 cores, cross-compilers, IPs, interconnections, 17 parallel versions of software from well-known benchmarks, and power consumption estimation for main components (processors, routers, memory, and caches), including DVFS support. © 2016 Elsevier Inc.","DVFS; Modeling; MPSoC; Simulation"
"Fault-tolerant vertex-pancyclicity of locally twisted cubes LTQn","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949554597&doi=10.1016%2fj.jpdc.2015.11.002&partnerID=40&md5=e3e645da56f060cf54dca01caeb40315","The n-dimensional locally twisted cube LTQn is a variant of the hypercube, which possesses some properties superior to the hypercube. This paper investigates the fault-tolerant vertex-pancyclicity of LTQn, and shows that if LTQn (n≥3) contains at most n-3 faulty vertices and/or edges then, for any fault-free vertex u and any integer ℓ with 4≤ℓ≤2n-fv except for 5, there is a fault-free cycle of length ℓ containing the vertex u, where fv is the number of faulty vertices. The result is optimal in some senses. © 2015 Elsevier Inc. All rights reserved.","Fault-tolerant; Locally twisted cubes; Vertex-pancyclicity"
"Embedding the optimal all-to-all personalized exchange on multistage interconnection networks+","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948392482&doi=10.1016%2fj.jpdc.2015.10.005&partnerID=40&md5=2d1017123bac095e16601ca47661a42a","All-to-all personalized exchange (ATAPE) is an inspired process to speedup the parallel and distributed computing. Recently, ATAPE algorithms were successfully applied on multistage interconnection networks (MINs), including baseline and butterfly networks. However, routing of those algorithms on MINs relies on switch-patterns for stage-control from sources (S), which is a half-routing solution since they cannot perform a full self-routing with the (S, D) protocol for all MINs. In this paper, first we propose a full-routing solution of the realizing ATAPE on a class of d-nary-switch MINs+ (i.e., baseline+, butterfly+, etc.). Our ATAPE can be embedded on-chip effectively for not only (S, D) self-routing but also stage-/switch-control routing. Two embedded ATAPE functions incorporate with multi-stage pipelining are proposed in optimal O(N+log2N): 1. a (default) static function D=S XOR (C+order) mod N and 2. an (optional) f-in-1 dynamic function D=ρ [(S+C+order) mod N] with the incrementing counter C=0 to N-1. Second, we introduce a crossbar of MINs+ with fewer delay-stages to achieve the ultimate ATAPE embedding. Finally, experimental results of applying ATAPE on such MINs+ are confirmed fruitfully, including the ATAPE-based NxN-matrix transposition in O(N+log2N), which yields the significant speedup. © 2015 Elsevier Inc.","A crossbar of MINs<sup>+</sup> and the ultimate ATAPE embedding; All-to-all personalized exchange (ATAPE); Embedding f-in-1 dynamic ATAPE function incorporate with multistage pipelining; Fully self-routable multistage interconnection networks (MINs<sup>+</sup>)"
"Reconstructing Householder vectors from Tall-Skinny QR","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941995581&doi=10.1016%2fj.jpdc.2015.06.003&partnerID=40&md5=ed1ab23a79b88132e56216bd831b89d9","The Tall-Skinny QR (TSQR) algorithm is more communication efficient than the standard Householder algorithm for QR decomposition of matrices with many more rows than columns. However, TSQR produces a different representation of the orthogonal factor and therefore requires more software development to support the new representation. Further, implicitly applying the orthogonal factor to the trailing matrix in the context of factoring a square matrix is more complicated and costly than with the Householder representation. We show how to perform TSQR and then reconstruct the Householder vector representation with the same asymptotic communication efficiency and little extra computational cost. We demonstrate the high performance and numerical stability of this algorithm both theoretically and empirically. The new Householder reconstruction algorithm allows us to design more efficient parallel QR algorithms, with significantly lower latency cost compared to Householder QR and lower bandwidth and latency costs compared with Communication-Avoiding QR (CAQR) algorithm. Experiments on supercomputers demonstrate the benefits of the communication cost improvements: in particular, our experiments show substantial improvements over tuned library implementations for tall-and-skinny matrices. We also provide algorithmic improvements to the Householder QR and CAQR algorithms, and we investigate several alternatives to the Householder reconstruction algorithm that sacrifice guarantees on numerical stability in some cases in order to obtain higher performance. © 2015 Elsevier Inc. All rights reserved.","Communication-avoiding algorithms; Dense linear algebra; QR decomposition"
"Robust neighbor discovery in multi-hop multi-channel heterogeneous wireless networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962833189&doi=10.1016%2fj.jpdc.2016.02.001&partnerID=40&md5=2f153862597c0987c674026fee5278a3","An important first step when deploying a wireless ad hoc network is neighbor discovery in which every node attempts to determine the set of nodes it can communicate within one wireless hop. In the recent years, cognitive radio (CR) technology has gained attention as an attractive approach to alleviate spectrum congestion. A cognitive radio transceiver can operate over a wide range of frequencies possibly spanning multiple frequency bands. A cognitive radio node can opportunistically utilize unused wireless spectrum without interference from other wireless devices in its vicinity. Due to spatial variations in frequency usage and hardware variations in radio transceivers, different nodes in the network may perceive different subsets of frequencies available to them for communication. This heterogeneity in the available channel sets across the network increases the complexity of solving the neighbor discovery problem in a cognitive radio network. In this work, we design and analyze several randomized algorithms for neighbor discovery in such a (heterogeneous) network under a variety of assumptions (e.g., maximum node degree known or unknown) for both synchronous and asynchronous systems under minimal knowledge. We also show that our randomized algorithms are naturally suited to tolerate unreliable channels and adversarial attacks. © 2016 Elsevier Inc. All rights reserved.","Asynchronous system; Clock drift; Cognitive radio technology; Heterogeneous channel availability; Jamming attack; Lossy channel; Multi-hop multi-channel wireless network; Neighbor discovery; Randomized algorithm"
"A framework for general sparse matrix-matrix multiplication on GPUs and heterogeneous processors","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941997483&doi=10.1016%2fj.jpdc.2015.06.010&partnerID=40&md5=cf7ecf99ab5fcf098407486a03108027","General sparse matrix-matrix multiplication (SpGEMM) is a fundamental building block for numerous applications such as algebraic multigrid method (AMG), breadth first search and shortest path problem. Compared to other sparse BLAS routines, an efficient parallel SpGEMM implementation has to handle extra irregularity from three aspects: (1) the number of nonzero entries in the resulting sparse matrix is unknown in advance, (2) very expensive parallel insert operations at random positions in the resulting sparse matrix dominate the execution time, and (3) load balancing must account for sparse data in both input matrices. In this work we propose a framework for SpGEMM on GPUs and emerging CPU-GPU heterogeneous processors. This framework particularly focuses on the above three problems. Memory pre-allocation for the resulting matrix is organized by a hybrid method that saves a large amount of global memory space and efficiently utilizes the very limited on-chip scratchpad memory. Parallel insert operations of the nonzero entries are implemented through the GPU merge path algorithm that is experimentally found to be the fastest GPU merge approach. Load balancing builds on the number of necessary arithmetic operations on the nonzero entries and is guaranteed in all stages. Compared with the state-of-the-art CPU and GPU SpGEMM methods, our approach delivers excellent absolute performance and relative speedups on various benchmarks multiplying matrices with diverse sparsity structures. Furthermore, on heterogeneous processors, our SpGEMM approach achieves higher throughput by using re-allocatable shared virtual memory. © 2015 Elsevier Inc. All rights reserved.","GPU; Heterogeneous processor; Linear algebra; Merging; Parallel algorithm; Sparse matrix; Sparse matrix-matrix multiplication"
"Joint scheduling of MapReduce jobs with servers: Performance bounds and experiments","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961784554&doi=10.1016%2fj.jpdc.2016.02.002&partnerID=40&md5=9049792d8f566db1662dc58d1d591ab5","MapReduce-like frameworks have achieved tremendous success for large-scale data processing in data centers. A key feature distinguishing MapReduce from previous parallel models is that it interleaves parallel and sequential computation. Past schemes, and especially their theoretical bounds, on general parallel models are therefore, unlikely to be applied to MapReduce directly. There are many recent studies on MapReduce job and task scheduling. These studies assume that the servers are assigned in advance. In current data centers, multiple MapReduce jobs of different importance levels run together. In this paper, we investigate a schedule problem for MapReduce taking server assignment into consideration as well. We formulate a MapReduce server-job organizer problem (MSJO) and show that it is NP-complete. We develop a 3-approximation algorithm and a fast heuristic design. Moreover, we further propose a novel fine-grained practical algorithm for general MapReduce-like task scheduling problem. Finally, we evaluate our algorithms through both simulations and experiments on Amazon EC2 with an implementation with Hadoop. The results confirm the superiority of our algorithms. © 2016 The Authors. Published by Elsevier Inc.","Fast heuristic; MapReduce; NP-complete; Scheduling; Server assignment"
"Time slot assignment for convergecast in wireless sensor networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931271286&doi=10.1016%2fj.jpdc.2015.05.004&partnerID=40&md5=e658857c6a59bd9a6a34b697f4a9103c","Abstract Convergecast, which is essentially the inverse of broadcast, can be used for data collection in a wireless sensor network. This paper addresses the problem of convergecast in a wireless sensor network that uses time division multiplexing in order to schedule its node-to-node communication in a time-bounded manner. A realistic system model and problem for convergecast with minimum delay and minimum energy consumption is formulated for wireless sensor networks. Then, based on a detailed analysis of this problem, a heuristic solution based on time slot assignments is proposed. Simulation results are used to show that the proposed algorithm performs significantly better than alternative methods for this problem. The simulation results also show that the data delivery time of the proposed algorithm is close to the theoretical bound. Furthermore, total energy consumption is significantly reduced, when compared to the alternatives, due to the time slot assignment method used in the proposed algorithm. © 2015 Elsevier Inc.","Convergecast; Data collection; Energy conservation; Time slot assignment; Wireless sensor network"
"A software scheduling solution to avoid corrupted units on GPUs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958739600&doi=10.1016%2fj.jpdc.2016.01.001&partnerID=40&md5=d2ee0df1a746f99cf81f39081831140e","Massively parallel processors provide high computing performance by increasing the number of concurrent execution units. Moreover, the transistor technology evolves to higher density, higher frequency and lower voltage. The combination of these factors increases significantly the probability of hardware failures. In this paper, we present a methodology to locate and mitigate hardware failures of Nvidia GPUs. Results show that intermittent errors can be precisely localized and have a limited impact to a well defined architecture tile. Therefore, we propose, and demonstrate on a software prototype, a rescheduling strategy to quarantine the defective hardware and ensure correct execution. Our approach significantly improves the GPU fault-tolerance capability and GPU's lifespan, at a reasonable overhead. © 2016 Elsevier Inc. All rights reserved.","Fault tolerance; GPGPU; Intermittent error; Reliability; Scheduling"
"Improving the network scalability of Erlang","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978499731&doi=10.1016%2fj.jpdc.2016.01.002&partnerID=40&md5=cbcac6d00b1655c9557c8e519866bc00","As the number of cores grows in commodity architectures so does the likelihood of failures. A distributed actor model potentially facilitates the development of reliable and scalable software on these architectures. Key components include lightweight processes which ‘share nothing’ and hence can fail independently. Erlang is not only increasingly widely used, but the underlying actor model has been a beacon for programming language design, influencing for example Scala, Clojure and Cloud Haskell. While the Erlang distributed actor model is inherently scalable, we demonstrate that it is limited by some pragmatic factors. We address two network scalability issues here: globally registered process names must be updated on every node (virtual machine) in the system, and any Erlang nodes that communicate maintain an active connection. That is, there is a fully connected O(n2) network of n nodes. We present the design, implementation, and initial evaluation of a conservative extension of Erlang — Scalable Distributed (SD) Erlang. SD Erlang partitions the global namespace and connection network using s_groups. An s_group is a set of nodes with its own process namespace and with a fully connected network within the s_group, but only individual connections outside it. As a node may belong to more than one s_group it is possible to construct arbitrary connection topologies like trees or rings. We present an operational semantics for the s_group functions, and outline the validation of conformance between the implementation and the semantics using the QuickCheck automatic testing tool. Our preliminary evaluation in comparison with distributed Erlang shows that SD Erlang dramatically improves network scalability even if the number of global operations is tiny (0.01%). Moreover, even in the absence of global operations the reduced connection maintenance overheads mean that SD Erlang scales better beyond 80 nodes (1920 cores). © 2016 Elsevier Inc.","Actor model; Conformance; Distributed system; Erlang; Operational semantics; QuickCheck; Testing; Validation"
"Simulation of NoC power-gating: Requirements, optimizations, and the Agate simulator","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994592285&doi=10.1016%2fj.jpdc.2016.03.006&partnerID=40&md5=3430dc0035fc57a02eabab694b18554d","The static power consumption of networks-on-chip (NoCs) has been increasing across each technology generation. Power-gating is a very promising approach that can dramatically reduce NoC static power but may potentially cause substantial performance penalty. Significant research is needed to explore effective ways of applying power-gating to NoC routers. To enable further research advancement, cycle-accurate NoC power-gating simulation infrastructure is much needed. In this work, we identify key requirements for NoC power-gating simulation and discuss three important optimizations that can enable such simulators to handle router pipeline draining and handshaking correctly and efficiently. We also propose Agate, an effective NoC power-gating simulator that satisfies the key requirements and optimizations. It can be integrated into Gem5 for closed-loop, full-system simulation of NoC-based many-core computing systems. We demonstrate the capability of Agate by simulating and evaluating several power-gating schemes, including the recently proposed Power Punch power-gating scheme. © 2016 Elsevier Inc.","Cycle-accurate simulator; Full-system simulation; Network-on-chip; Power-gating; Static power"
"The features, hardware, and architectures of data center networks: A survey","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973139190&doi=10.1016%2fj.jpdc.2016.05.009&partnerID=40&md5=89e542a35bc190022437d7e78c336fa8","The rapid development of cloud computing in recent years has deeply affected our lifestyles. As core infrastructures of cloud computing, data centers have gained widespread attention from both the academia and industry. In a data center, the data center network (DCN) that plays a key role in computing and communication has attracted extensive interest from researchers. In this survey, we discuss the features, hardware, and architectures of DCN's, including their logical topological connections and physical component categorizations. We first give an overview of production data centers. Next, we introduce the hardware of DCN's, including switches, servers, storage devices, racks, and cables used in industries, which are highly essential for designing DCN architectures. And then we thoroughly analyze the topology designs and architectures of DCN's from various aspects, such as connection types, wiring layouts, interconnection facilities, and network characteristics based on the latest literature. Finally, the facility settings and maintenance issues for data centers that are important in the performance and the efficiency of DCN's are also briefly discussed. Specifically and importantly, we provide both qualitative and quantitative analyses on the features of DCN's, including performance comparisons among typical topology designs, connectivity discussion on average degree, bandwidth calculation, and diameter estimation, as well as the capacity enhancement of DCN's with wireless antennae and optical devices. The discussion of our survey can be referred as an overview of the ongoing research in the related area. We also present new observations and research trends for DCN's. © 2016 Elsevier Inc.","Architecture; Data center network; Hardware; Topology"
"MicMR: An efficient MapReduce framework for CPU-MIC heterogeneous architecture","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966800464&doi=10.1016%2fj.jpdc.2016.04.007&partnerID=40&md5=2ae680ec6e1970b57354e3517e7be7de","With the high-speed development of processors, coprocessor-based MapReduce is widely studied. In this paper, we propose micMR, an efficient MapReduce framework for CPU-MIC heterogeneous architecture. micMR mainly provides the following new features. First, the two-level split and the SIMD friendly map are designed for utilizing the Vector Process Units on MIC. Second, heterogeneous pipelined reduce is developed for improving the efficiency of resource utilization. Third, a memory management scheme is designed for accessing <key, value> pairs in both the host and the MIC memory efficiently. In addition, optimization techniques, including load balancing, SIMD hash, and asynchronous task transfer, are designed for achieving more speedups. We have developed micMR not only in a single node with CPU and MIC but also in a CPU-MIC heterogeneous cluster. The experimental results show that micMR is up to 8.4x and 45.8x faster than Phoenix++, a high-performance MapReduce system for symmetric multiprocessing system, and up to 2.0x and 5.1x faster than Hadoop in a CPU-MIC cluster. © 2016 Elsevier Inc.","Hadoop; Many Integrated Core; MapReduce; Phoenix++; SIMD"
"Black hole search in computer networks: State-of-the-art, challenges and future directions","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948406328&doi=10.1016%2fj.jpdc.2015.10.006&partnerID=40&md5=77cc22792f0ce0bc1731e52aaaede425","As the size and use of networks continue to increase, network anomalies and faults are commonplace. Consequently, effective detection of such network issues is crucial for the deployment and use of network-based services. In this paper, we focus on one specific severe and pervasive network problem, namely the presence of one or more black holes. A black hole models a network node that is accidentally off-line or in which a process deletes any visiting agent or incoming data upon arrival without leaving any observable trace. Black Hole Search is the process that leverages mobile agents to locate black holes in a fully distributed way. In this paper, we review the state-of-the-art research in this area. We first distinguish between solutions for synchronous and asynchronous networks. We then consider the communication model between agents, their starting locations and the topological knowledge each may hold. We also report on the proposed algorithms with respect to their complexity and correctness. We remark that most existing work addresses locating a single black hole, multiple black hole search being significantly more complex. We not only summarize major results in this area but also briefly touch on other types of malicious hosts. Finally, we identify some open problems for future research. © 2015 Elsevier Inc.","Black hole search; Malicious host; Mobile agents; Multiple black holes; Network diagnosis"
"Architecture supported register stash for GPGPU","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952803133&doi=10.1016%2fj.jpdc.2015.12.003&partnerID=40&md5=fb79d18a6075d4fb2db7feb8c1be1649","GPGPU provides abundant hardware resources to support a large number of light-weighted threads. They are organized into blocks and run in warps. All threads of a block must be dispatched to one stream multiprocessor (SM) of GPGPU together. When the remaining resources of an SM cannot support one more block, all threads of the block are held back until former blocks retire from the SM. We found that the register file is prone to be the most limited one among all the resources, especially for SMs with less registers. Meanwhile, we revealed the dynamics of a thread's register requirement: only part of its pre-allocated registers are used for different instructions at run time. This results in considerable register underutilization. We proposed the architecture supported register stash (ASRS). It removes the limitation of registers when dispatching blocks. The hardware registers are allocated at run time according to each instruction's live registers, which can be analyzed statically by a compiler. When the hardware registers cannot meet the requirements of all running warps, some warps are suspended and their registers are reclaimed temporarily. The data in these registers are stashed to memory. On the other hand, if there are spare hardware registers, it will start a new warp or resume a suspended warp after all the warp's stashed register data are loaded from memory. The intra-block synchronization is also taken care of when some of the warps of the same block are not schedulable due to the ASRS. The ASRS alleviates the register underutilization and improves performance without modifying the current programming model or demanding extra effort from the programmers. It also enables an SM with limited registers that cannot even support a single block to execute it. Besides, it helps lower the register file energy consumption and increase the power efficiency. The ASRS achieved speedups of 1.59 and 1.14 when the registers of each SM are limited to 8K and 16K respectively with an insignificant overhead. The speedups compared with the infinite register files are 0.84 and 0.98 with 8K and 16K registers respectively. Compared with the baseline 32K register file, the ASRS decreases the 8K and 16K register file energy consumption to 66.5% and 75.8% respectively. Their power efficiencies (in ratio of performance and power) are increased to 1.29x and 1.31x respectively. © 2015 Elsevier Inc. All rights reserved.","Energy; GPGPU; Performance; Register file"
"Physical-aware predictive dynamic thermal management of multi-core processors","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994624512&doi=10.1016%2fj.jpdc.2016.03.008&partnerID=40&md5=7b718aea3244942056039e7399f8639b","The advances in silicon process technology have made it possible to have processors with larger number of cores. The increment of cores number has been hindered by increasing power consumption and heat dissipation due to high power expenditure in a small area die size. The high temperature can cause degradation in performance, reliability, transistor aging, transition speed and increase in leakage current. In this paper, we present a method which considers different thermal behavior of cores and uses both physical sensors and performance counters simultaneously to improve thermal management of both SMT multi-core processors with a physical sensor per core and Non-SMT multi-core processors with only one physical sensor for the processor. The experimental results indicate that our technique can significantly decrease the average and peak temperature in most cases compared to Linux standard scheduler, and two well-known thermal management techniques: PDTM, and TAS. © 2016 Elsevier Inc.","DVFS; Dynamic thermal management; Multi-core processors; Physical features; Task migration"
"An optimized bitonic sorting strategy with midpoint-based dynamic communication","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938405416&doi=10.1016%2fj.jpdc.2015.05.008&partnerID=40&md5=4c1e6ba5eb83dd236d57eb2aa2a2e30f","Abstract This paper proposes an optimized Bitonic sorting (OBS) strategy with midpoint-based dynamic communication. Our OBS strategy uses the midpoint-weight list ranking to improve complexity and reduce time of sorting on parallel and distributed systems. Applying a better key in the PE-list ranking can find the right place of (Pi, Pj) and improve communication time significantly (i.e., fewer iterations, better synchronization in each iteration, faster convergence to the result), while most of coarse-grain parallel sorting (P<N) approaches improve only a large amount of data exchange (N/P) in each of static (s(s+1)/2) iterations. Theoretically, the OBS method can reduce fixed (s(s+1)/2) iterations to 1,2,3,..., or s=log2P iterations, which are improved over those (≤s(s+1)/2 iterations) of the dynamic DCES method. In performance evaluation, sorting was accomplished on multicore machines. Experimental results showed that our optimized OBS outperforms those of the dynamic DCES about 35%-40% and those of the static LBM about 51%-54% (for N=10 to 100 million elements on an 8-multicore computer). © 2015 Published by Elsevier Inc.","Midpoint-based dynamic communication; Midpoint-weight list-ranking; Optimized Bitonic sorting"
"Prediction mechanisms for monitoring state of cloud resources using Markov chain model","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975104888&doi=10.1016%2fj.jpdc.2016.04.012&partnerID=40&md5=31f4eb8e73ab79d9dd361f3d05486226","Cloud computing allows for sharing computing resources, such as CPU, application platforms, and services. Monitoring these resources would benefit from an accurate prediction model that significantly reduces the network overhead caused by unnecessary push and pull messages. However, accurate prediction of the computing resources is considered hard due to the dynamic nature of cloud computing. In this paper, two monitoring mechanisms have been developed: the first is based on a Continuous Time Markov Chain (CTMC) model and the second is based on a Discrete Time Markov Chain (DTMC) model. It is found that The CTMC-based mechanism outperformed the DTMC-based mechanism. Also, the CTMC-based mechanism outperformed the Grid Resource Information Retrieval (GRIR) mechanism, which does not employ prediction, and a prediction-based mechanism, which uses Markov Chains to predict the time interval of monitoring mobile grid resources, in monitoring cloud resources. © 2016 Elsevier Inc. All rights reserved.","Cloud computing; Markov chains; Resource monitoring"
"Mining maximal cliques from a large graph using MapReduce: Tackling highly uneven subproblem sizes","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930381272&doi=10.1016%2fj.jpdc.2014.08.011&partnerID=40&md5=ec0a24d7f32b0e4fc5842a66e8492e88","We consider Maximal Clique Enumeration (MCE) from a large graph. A maximal clique is perhaps the most fundamental dense substructure in a graph, and MCE is an important tool to discover densely connected subgraphs, with numerous applications to data mining on web graphs, social networks, and biological networks. While effective sequential methods for MCE are known, scalable parallel methods for MCE are still lacking. We present a new parallel algorithm for MCE, Parallel Enumeration of Cliques using Ordering (PECO), designed for the MapReduce framework. Unlike previous works, which required a post-processing step to remove duplicate and non-maximal cliques, PECO enumerates only maximal cliques with no duplicates. The key technical ingredient is a total ordering of the vertices of the graph which is used in a novel way to achieve a load balanced distribution of work, and to eliminate redundant work among processors. We implemented PECO on Hadoop MapReduce, and our experiments on a cluster show that the algorithm can effectively process a variety of large real-world graphs with millions of vertices and tens of millions of maximal cliques, and scales well with the degree of available parallelism. © 2014 Elsevier Inc. All rights reserved.","Clique; Enumeration algorithm; Graph mining; Hadoop; Load balancing; MapReduce; Maximal clique enumeration; Parallel algorithm"
"Two approximation algorithms for bipartite matching on multicore architectures","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941998165&doi=10.1016%2fj.jpdc.2015.06.009&partnerID=40&md5=2c4b3cce79f74491eed94347002633ca","We propose two heuristics for the bipartite matching problem that are amenable to shared-memory parallelization. The first heuristic is very intriguing from a parallelization perspective. It has no significant algorithmic synchronization overhead and no conflict resolution is needed across threads. We show that this heuristic has an approximation ratio of around 0.632 under some common conditions. The second heuristic is designed to obtain a larger matching by employing the well-known Karp-Sipser heuristic on a judiciously chosen subgraph of the original graph. We show that the Karp-Sipser heuristic always finds a maximum cardinality matching in the chosen subgraph. Although the Karp-Sipser heuristic is hard to parallelize for general graphs, we exploit the structure of the selected subgraphs to propose a specialized implementation which demonstrates very good scalability. We prove that this second heuristic has an approximation guarantee of around 0.866 under the same conditions as in the first algorithm. We discuss parallel implementations of the proposed heuristics on a multicore architecture. Experimental results, for demonstrating speed-ups and verifying the theoretical results in practice, are provided. © 2015 Elsevier Inc. All rights reserved.","Approximation algorithm; Bipartite graphs; Matching; Shared memory parallelism"
"Towards scalable on-demand collective data access in IaaS clouds: An adaptive collaborative content exchange proposal","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946233657&doi=10.1016%2fj.jpdc.2015.09.006&partnerID=40&md5=d4e86547df17c69c56c9c3da9f7da70a","A critical feature of IaaS cloud computing is the ability to quickly disseminate the content of a shared dataset at large scale. In this context, a common pattern is collective read, i.e., accessing the same VM image or dataset from a large number of VM instances concurrently. Several approaches deal with this pattern either by means of pre-broadcast before access or on-demand concurrent access to the repository where the image or dataset is stored. We propose a different solution using a hybrid strategy that augments on-demand access with a collaborative scheme in which the VMs leverage similarities between their access pattern in order to anticipate future read accesses and exchange chunks between themselves in order to reduce contention to the remote repository. Large scale experiments show significant improvement over conventional approaches from multiple perspectives: completion time, sustained read throughput, fairness of I/O read operations and bandwidth utilization. © 2015 Elsevier Inc. All rights reserved.","Adaptive prefetching; Collaborative content exchange; Collective I/O; I/O access pattern awareness; IaaS; On-demand read access under concurrency; Scalable content dissemination"
"Tradeoffs between cost and information for rendezvous and treasure hunt","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933499123&doi=10.1016%2fj.jpdc.2015.06.004&partnerID=40&md5=e29c2e7b20f8b9803ca7d75b90b333ce","Abstract In rendezvous, two agents traverse network edges in synchronous rounds and have to meet at some node. In treasure hunt, a single agent has to find a stationary target situated at an unknown node of the network. We study tradeoffs between the amount of information (advice) available a priori to the agents and the cost (number of edge traversals) of rendezvous and treasure hunt. Our goal is to find the smallest size of advice which enables the agents to solve these tasks at some cost C in a network with e edges. This size turns out to depend on the initial distance D and on the ratio e/C, which is the relative cost gain due to advice. For arbitrary graphs, we give upper and lower bounds of O(Dlog(D·e/C)+logloge) and Ω(Dloge/C), respectively, on the optimal size of advice. For the class of trees, we give nearly tight upper and lower bounds of O(Dloge/C+logloge) and Ω(Dloge/C), respectively. © 2015 Elsevier Inc.","Advice; Cost; Deterministic algorithms; Mobile agents; Rendezvous; Treasure hunt"
"Energy and time constrained task scheduling on multiprocessor computers with discrete speed levels","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994577115&doi=10.1016%2fj.jpdc.2016.02.006&partnerID=40&md5=9fa73a7ff90bbef51eb563717d32ce3a","Energy and time constrained task scheduling on multiprocessor computers with discrete clock frequency and supply voltage and execution speed and power levels is addressed as combinatorial optimization problems. It is proved that the problem of minimizing schedule length with energy consumption constraint and the problem of minimizing energy consumption with schedule length constraint are NP-hard even on a uniprocessor computer with only two speed levels. A class of algorithms is developed to solve the above two problems. These algorithms include two components, namely, a list scheduling algorithm for task scheduling and a list placement algorithm for speed determination. A worst-case asymptotic performance bound and an average-case asymptotic performance bound are derived for our algorithms on uniprocessor computers, and a worst-case asymptotic performance bound is derived for our algorithms on multiprocessor computers. Extensive simulations are performed to verify our analytical results. It is found that our algorithms produce solutions very close to optimal and are practically very useful. © 2016 Elsevier Inc.","Discrete speed levels; Energy consumption; List placement; List scheduling; Performance analysis; Power-aware scheduling; Simulation; Task scheduling"
"Dellat: Delivery Latency Minimization in Wireless Sensor Networks with Mobile Sink","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932607824&doi=10.1016%2fj.jpdc.2015.05.005&partnerID=40&md5=6e21cfafd6b6e08d7d519209de6333b7","Adopting mobile data gathering in wireless sensor networks (WSNs) can reduce the energy consumption on data forwarding thus achieve more uniform energy consumption among sensor nodes. However, the data delivery latency inevitably increases in mobile data gathering due to the travel of the mobile sink. In this paper, we consider a delivery latency minimization problem (DLMP) in a randomly deployed WSN. Our goal is to minimize the travel latency of the mobile sink. We formulate the DLMP as an integer programming problem which subjects to the direct access constraint, the data transmission constraint and the route traverse constraint. We prove that the DLMP is an NP-Complete (NPC) problem, and then propose a substitution heuristic algorithm to solve it by shortening the travel route and having the mobile sink move and collect data at the same time. We compare the proposed algorithm with other two algorithms, a traveling salesman problem (TSP) heuristic algorithm and a random heuristic algorithm through simulations. Our extensive simulation results show that although all the three algorithms can shorten the data delivery latency in mobile data gathering, the proposed substitution heuristic algorithm is the most effective one. © 2015 Elsevier Inc. All rights reserved.","Delivery latency minimization; Mobile sink; Substitution heuristic algorithm; Wireless sensor networks"
"Energy-efficient contention-aware application mapping and scheduling on NoC-based MPSoCs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971300807&doi=10.1016%2fj.jpdc.2016.04.006&partnerID=40&md5=6771254eced43d6398eccc3e479791d5","We consider the problem of energy-efficient contention-aware application mapping and scheduling on Network-on-Chip (NoC) based multiprocessors. For an application represented by a directed acyclic graph, we present a model where voltage scaling techniques for processors can be combined with frequency tuning techniques for NoC links to save overall system energy consumption. We employ a two-step approach to solve the overall mapping and scheduling problem. First, the application mapping problem is formulated as a quadratic binary programming problem, which aims to minimize the communication energy; we apply a relaxation-based iterative rounding algorithm to solve it. With the mapping achieved, we further consider the application scheduling problem, which aims to find the optimal voltage level for each task of the application and optimal frequency level for each communication of the application to minimize the overall system energy consumption, given the application deadline. To attack the second problem, we first design an algorithm based on the earliest time first scheduling to determine the application's finish time if a voltage and frequency assignment is given; then, we develop a genetic algorithm to search the solution space for the voltage and frequency assignment that minimizes the overall system energy consumption and meets the application's deadline. Through these two steps, we produce a mapping and scheduling that meets the application's deadline, and significantly reduces the overall system energy consumption. Experiments are conducted for a number of randomly generated application graphs, as well as several real application graphs to verify the energy reduction and applicability of the proposed model and algorithms. © 2016 Elsevier Inc. All rights reserved.","Application mapping; Dynamic link frequency tuning; Dynamic voltage scaling; Energy-efficient scheduling; Network-on-chip (NoC)"
"Advances in patch-based adaptive mesh refinement scalability","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954219964&doi=10.1016%2fj.jpdc.2015.11.005&partnerID=40&md5=0c9a242714c251d487da189a6532d2ea","Patch-based structured adaptive mesh refinement (SAMR) is widely used for high-resolution simulations. Combined with modern supercomputers, it could provide simulations of unprecedented size and resolution. A persistent challenge for this combination has been managing dynamically adaptive meshes on more and more MPI tasks. The distributed mesh management scheme in SAMRAI has made some progress SAMR scalability, but early algorithms still had trouble scaling past the regime of 105 MPI tasks. This work provides two critical SAMR regridding algorithms, which are integrated into that scheme to ensure efficiency of the whole. The clustering algorithm is an extension of the tile-clustering approach, making it more flexible and efficient in both clustering and parallelism. The partitioner is a new algorithm designed to prevent the network congestion experienced by its predecessor. We evaluated performance using weak- and strong-scaling benchmarks designed to be difficult for dynamic adaptivity. Results show good scaling on up to 1.5M cores and 2M MPI tasks. Detailed timing diagnostics suggest scaling would continue well past that. © 2015 Elsevier Inc. All rights reserved.","Adaptive mesh refinement; Clustering algorithm; Data locality; Dynamic adaptivity; Partitioning algorithm; Scalable algorithm"
"Intrinsic fault tolerance of multilevel Monte Carlo methods","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938393594&doi=10.1016%2fj.jpdc.2015.07.005&partnerID=40&md5=549ab68f1c3f8255f5344ae923b9d13a","Abstract Monte Carlo (MC) and multilevel Monte Carlo (MLMC) methods applied to solvers for Partial Differential Equations with random input data are proved to exhibit intrinsic failure resilience. Sufficient conditions are provided for non-recoverable loss of a random fraction of MC samples not to fatally damage the asymptotic accuracy versus work of a MC simulation. Specifically, the convergence behavior of MLMC methods on massively parallel hardware with runtime faults is analyzed mathematically and investigated computationally. Our mathematical model assumes node failures which occur uncorrelated of MC sampling and with general sample failure statistics on the different levels and which also assume absence of checkpointing, i.e., we assume irrecoverable sample failures with complete loss of data. Modifications of the MLMC with enhanced resilience are proposed. The theoretical results are obtained under general statistical models of CPU failure at runtime. Particular attention is paid to node failures with so-called Weibull failure models on massively parallel stochastic finite volume computational fluid dynamics simulations are discussed. We discuss the resilience of massively parallel stochastic Finite Volume computational fluid dynamics simulations. © 2015 Elsevier Inc.","Exascale parallel computing; Failure resilience; Fault tolerance; Multilevel Monte Carlo"
"Counting-based impossibility proofs for set agreement and renaming","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944213610&doi=10.1016%2fj.jpdc.2015.09.002&partnerID=40&md5=4cd80cd96ab7c5556318698bc381bb71","Set agreement and renaming are two tasks that allow processes to coordinate, even when agreement is impossible. In k-set agreement, n processes must decide on at most k of their input values. While n-set agreement is trivially wait-free solvable by each process deciding on its input, (n-1)-set agreement is not wait-free solvable. In M-renaming, processes must decide on distinct names in a range of size M. For any number n of processes, (2n-1)-renaming is wait-free solvable, but surprisingly, (2n-2)-renaming is wait-free solvable if and only if n is not a prime power; the only previous lower bound on the number of names necessary for renaming, when n is not a prime power, is n+1. In adaptive renaming, M decreases when the number p of participants in the execution decreases. It is known that (2p-1)-adaptive renaming is wait-free solvable, while (2p-pn-1-adaptive renaming is not. This paper presents counting-based proofs for the above mentioned impossibility results: n processes can wait-free solve neither (n-1)-set agreement nor (2p-pn-1-adaptive renaming; if n is a prime power, n processes cannot wait-free solve (2n-2)-renaming. For an arbitrary number of processes, we give a lower bound for renaming, by reduction from renaming for a different number of processes, and relying on the distribution of prime numbers. Our proofs combine simple operational properties of a restricted set of executions with elementary counting arguments to show the existence of an execution violating the task's conditions. This makes the proofs easier to understand, verify, and, we hope, extend. © 2015 Elsevier Inc. All rights reserved.","Adaptive and nonadaptive renaming; Lower bounds; Wait-free algorithms; Weak and strong symmetry breaking"
"Combining performance and priority for scheduling resizable parallel applications","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946102459&doi=10.1016%2fj.jpdc.2015.09.007&partnerID=40&md5=d7682e08ff3bfe156efeda3c6b3ef03f","We illustrate and evaluate the potential impact of dynamic resizability on parallel job scheduling. Our ReSHAPE framework includes a job scheduler that supports dynamic resizing of malleable parallel applications. We propose and evaluate new scheduling policies and strategies enabled by the ReSHAPE framework. These strategies use both application performance and user-assigned priorities to inform decisions about expanding or contracting the set of processors assigned to a particular job. Experimental results show that the scheduling policies significantly improve individual job turn around time as well as overall cluster utilization. © 2015 Elsevier Inc. All rights reserved.","Dynamic resizing; Malleable parallel jobs; Parallel job scheduling; Scheduling policies"
"Incremental dataflow execution, resource efficiency and probabilistic guarantees with Fuzzy Boolean nets","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930383585&doi=10.1016%2fj.jpdc.2015.03.001&partnerID=40&md5=cdfcc5da8b769e79a36ce973458d07a0","Currently, there is a strong need for organizations to analyze and process ever-increasing volumes of data in order to answer to real-time processing demands. Such continuous and data-intensive processing is often achieved through the composition of complex data-intensive workflows (i.e., dataflows). Dataflow management systems typically enforce strict temporal synchronization across the various processing steps. Non-synchronous behavior often has to be explicitly programmed on an ad-hoc basis, which requires additional lines of code in programs and thus the possibility of errors. More so, in a large set of scenarios for continuous and incremental processing, the output of dataflow applications at each execution can suffer almost no difference when comparing to the previous execution, and therefore resources, energy and computational power are unknowingly wasted. To face such lack of efficiency, transparency, and generality, we introduce the notion of Quality-of-Data (QoD), which describes the level of changes required on a data store that cause the triggering of processing steps. This, so that the dataflow (re-)execution is reduced until its outcome would reach a significant and meaningful variation, which is inside a specified freshness limit. Based on the QoD notion, we propose a novel dataflow model, with framework (Fluxy), for orchestrating data-intensive processing steps, which communicate data via a NoSQL storage, and whose triggering semantics is driven by dynamic QoD constraints automatically defined for different datasets by means of Fuzzy Boolean Nets. These nets give probabilistic guarantees about the prediction of the cumulative error between consecutive dataflow executions. With Fluxy, we demonstrate how dataflows can be leveraged to respond to quality boundaries (that can be seen as SLAs) to deliver controlled and augmented performance, rationalization of resources, and task prioritization. © 2015 Elsevier Inc. All rights reserved.","Continuous processing; Data-intensive; Dataflow&Incremental processing; Fuzzy logic; Machine learning; NoSQL; Quality-of-service; Workflow"
"Efficient parallel simulation of spatially-explicit agent-based epidemiological models","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966292060&doi=10.1016%2fj.jpdc.2016.04.004&partnerID=40&md5=9ec206c515360a8128a3bbb9ddafe796","Agent-based approaches enable simulation driven analysis and discovery of system-level properties using descriptive models of known behaviors of the entities constituting the system. Accordingly, a spatially-explicit agent-based ecological modeling, parallel simulation, and analysis environment called SEARUMS has been developed. However, the conservatively synchronized parallel simulation infrastructure of SEARUMS did not scale effectively. Furthermore, the initial multithreaded shared-memory design prevented utilization of resources on multiple compute nodes of a distributed memory cluster. Consequently, the simulation infrastructure of SEARUMS was redesigned to operate as a Time Warp synchronized parallel and distributed discrete event simulation (PDES) on modern distributed-memory supercomputing platforms. The new PDES environment is called SEARUMS++. The spatially-explicit nature of the models posed several challenges in achieving scalable and efficient PDES, necessitating new approaches in SEARUMS++ for: 1 modeling spatial interactions and initial partitioning of agents, 2 logical migration of an agent during simulation using proxy agents to reflect migratory characteristics, and 3 ghosting of agents using multiple proxy agents to handle boundary cases that occur during logical migration of agents. This article presents our optimization efforts involving new methods to address aforementioned challenges. The design of SEARUMS++ and experimental evaluation of various alternatives that were explored to achieve scalable and efficient PDES are also discussed. Our experiments indicate that SEARUMS++ provides 200% performance improvement and maintains scalability to a larger number of processors, thus enabling efficient parallel simulation of spatially-explicit agent-based epidemiological models. © 2016 Elsevier Inc. All rights reserved.","Agent-based model; Avian influenza; Epidemiology; Ghosting; Logical process migration; Parallel Discrete Event Simulation (PDES); Spatially-explicit model; Time Warp"
"Adaptive fault-tolerant architecture and routing algorithm for reliable many-core 3D-NoC systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964771596&doi=10.1016%2fj.jpdc.2016.03.014&partnerID=40&md5=ced0c9e0945c92704cb73e13f893afc2","During the last few decades, Three-dimensional Network-on-Chips (3D-NoCs) have been showing their advantages against 2D-NoC architectures. This is thanks to the reduced average interconnect length and lower interconnect-power consumption inherited from Three-dimensional Integrated Circuits (3D-ICs). On the other hand, questions about their reliability is starting to arise. This issue is mainly caused by their complex nature where a single faulty transistor may cause intolerable performance degradation or even the entire system collapse. To ensure their correct functionality, 3D-NoC systems must be fault-tolerant to any short-term malfunction or permanent physical damage to ensure message delivery on time while minimizing the performance degradation as much as possible. In this paper, we present a fault-tolerant 3D-NoC architecture, called 3D-Fault-Tolerant-OASIS (3D-FTO).1 With the aid of a light-weight routing algorithm, 3D-FTO manages to avoid the system failure at the presence of a large number of transient, intermittent, and permanent faults. Moreover, the proposed architecture is leveraging on reconfigurable components to handle the fault occurrence in links, input-buffers, and crossbar, where the faults are more often to happen. The proposed 3D-FTO system is able to work around different kinds of faults ensuring graceful performance degradation while minimizing the additional hardware complexity and remaining power-efficient. © 2016 Elsevier Inc. All rights reserved.","3D NoC; Architecture; Deadlock-free; Dynamic reconfiguration; Fault-tolerance; Robustness"
"Simple super-matrix processor: Implementation and performance evaluation","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934989512&doi=10.1016%2fj.jpdc.2015.06.001&partnerID=40&md5=e62df41a347177bd54057dda8916059b","Data-parallel applications are growing in importance and demanding increased performance from hardware. Since the fundamental data structures for a wide variety of data parallel applications are scalar, vector, and matrix, this paper proposes a simple matrix processor (SMP) for executing scalar, vector, and matrix instructions on a unified datapath. Matrix register file and matrix control unit are added to the decode stage of the well-known 5-stage pipeline (baseline scalar processor). To further improve the performance, a simple super-matrix processor (SSMP) is proposed to execute multi-scalar/vector/matrix instructions on parallel execution datapaths. 4×32-bit instructions are fetched, decoded, and checked for dependencies. Up to four independent scalar instructions can be issued in-order to the parallel execution units. However, vector/matrix instructions iterate the issuing of four vector/matrix operations without checking. 4×32-bit contiguous vector/matrix elements can be loaded/stored per clock cycle from/to L2 cache to/from matrix register file, however, scalar data can be accessed from L1 cache in a rate of one element per clock cycle. To prove our concept, the proposed SMP/SSMP are implemented on Xilinx Virtex-6 and evaluated on vector/matrix kernels. 8679/11734 slices are required for implementing SMP/SSMP, where the complexities are 2.79/3.77 times higher than the baseline scalar processor. However, the speedups of SMP/SSMP over the baseline scalar processor are 1.57-6.33/4.32-18.23. © 2015 Elsevier Inc. All rights reserved.","Data-parallel applications; FPGA implementation; Performance evaluation; Vector/matrix processing"
"GOM-Hadoop: A distributed framework for efficient analytics on ordered datasets","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931287966&doi=10.1016%2fj.jpdc.2015.05.003&partnerID=40&md5=eddc563624c9ed447143839ccee78bf6","One of the most common datasets exploited by many corporations to conduct business intelligence analysis is event log files. Oftentimes, the records in event log files are temporally ordered, and need to be grouped by certain key with the temporal ordering preserved to facilitate further analysis. One such example is to group temporally ordered events by user ID in order to analyze user behavior. This kind of analytical workload, here referred to as RElative Order-pReserving based Grouping (Re-Org), is quite common in big data analytics, where the MapReduce programming paradigm (and its open-source implementation, Hadoop) is widely adopted for massive parallel processing. However, using MapReduce/Hadoop for executing Re-Org tasks on ordered datasets is not efficient due to its internal sort-merge mechanism when shuffling data from mappers to reducers. In this paper, we propose a distributed framework that adopts an efficient group-order-merge mechanism to speed up the execution of Re-Org tasks. We demonstrate the advantage of our framework by formally modeling its execution process and by comparing its performance with Hadoop through extensive experiments on real-world datasets. The evaluation results show that our framework can achieve up to 6.3x speedup over Hadoop in executing Re-Org tasks. © 2015 Elsevier Inc.","Distributed framework; GOM-Hadoop; Mapreduce; Ordered dataset"
"Memory limited algorithms for optimal task scheduling on parallel systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962638141&doi=10.1016%2fj.jpdc.2016.03.003&partnerID=40&md5=ee27ad52aeb19a4656b8214e33d4781c","To fully benefit from a multi-processor system, tasks need to be scheduled optimally. Given that the task scheduling problem with communication delays, P|prec,cij|Cmax, is a well known strong NP-hard problem, exhaustive approaches are necessary. The previously proposed A∗based algorithm retains its entire state space in memory and often runs out of memory before it finds an optimal solution. This paper investigates and proposes two memory limited optimal scheduling algorithms: Iterative Deepening A∗(IDA∗) and Depth-First Branch and Bound A∗(BBA∗). When finding a guaranteed near optimal schedule length is sufficient, the proposed algorithms can be combined, reporting the gap while they run. Problem specific pruning techniques, which are crucial for good performance, are studied for the two proposed algorithms. Extensive experiments are conducted to evaluate and compare the proposed algorithms with previous optimal algorithms. © 2016 Elsevier Inc. All rights reserved.","A∗; Depth-First Branch and Bound A∗; Iterative deepening; Iterative Deepening A∗; Memory limited; Optimal task scheduling; Optimisation algorithm; Parallel systems; State space pruning"
"GPU enabled XDraw viewshed analysis","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938913162&doi=10.1016%2fj.jpdc.2015.07.001&partnerID=40&md5=11da30f6315c22c0461bcc402f159fab","Viewshed analysis is an important tool in the study of digital terrain visibility. Current methods rely on the CPU performing computations to linearly calculate visibility for a given position on a portion of digital terrain. The viewshed analysis process can be sped up through the use of a GPU to parallelize the visibility algorithms. This paper presents a novel conversion of the XDraw viewshed analysis algorithm to a parallel context in an effort to increase the speed at which a viewshed can be rendered. The algorithm executed faster than current linear methods and responded well to parallelization. We conclude that XDraw is applicable for GIS applications when rendered in a parallel context. © 2015 Elsevier Inc.","C++ AMP; Digital terrain visibility; GPGPU; Viewshed; Visibility; XDraw"
"PDC: Prediction-based data-aware clustering in wireless sensor networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928988446&doi=10.1016%2fj.jpdc.2015.02.004&partnerID=40&md5=43f0cccada0da349b8c978a7cdab9ca9","Minimizing energy consumption is the most important concern in wireless sensor networks (WSNs). To achieve this, clustering and prediction methods can enjoy the inherent redundancy of raw data and reduce transmissions more effectively. In this paper, we focus on designing a prediction-based approach, named PDC, to mainly contribute in data-aware clustering. It exploits both spatial and temporal correlations to form highly stable clusters of nodes sensing similar values. Here, the sink node uses only the local prediction models of cluster heads to forecast all readings in the network without direct communication. To the best of our knowledge, PDC is a novel energy efficient approach which provides a high precision of the approximate results with bounded error. Our simple prediction model presents high accuracy as well as low computation and communication costs. Extensive simulations have been conducted to evaluate the prediction model as well as our clustering approach. The results verify the superiority of our simple prediction model. Moreover, PDC implies a significant improvement on existing alternatives. © 2015 Elsevier Inc. All rights reserved.","Clustering; Prediction; Wireless sensor networks"
"Scheduling for energy minimization on restricted parallel processors","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928998479&doi=10.1016%2fj.jpdc.2015.04.001&partnerID=40&md5=5c707d80d2a4bfec9dd520848b953d04","Scheduling for energy conservation has become a major concern in the field of information technology because of the need to reduce energy use and carbon dioxide emissions. Previous work has focused on the assumption that a task can be assigned to any processor. In contrast, we initially study the problem of task scheduling on restricted parallel processors. The restriction takes account of affinities between tasks and processors; that is, a task has its own eligible set of processors. We adopt the Speed Scaling (SS) method to save energy under an execution time constraint (on the makespan Cmax), and the processors can run at arbitrary speeds in [smin,smax]. Our objective is to minimize the overall energy consumption. The energy-efficient scheduling problem, involving task assignment and speed scaling, is inherently complex as it is proved to be NP-complete for general tasks. We formulate the problem as an Integer Programming (IP) problem. Specifically, we devise a polynomial-time optimal scheduling algorithm for the case in which tasks have a uniform size. Our algorithm runs in O(mn3logn) time, where m is the number of processors and n is the number of tasks. We then present a polynomial-time algorithm that achieves a bounded approximation factor when the tasks have arbitrary-size work. Numerical results demonstrate that our algorithm could provide an energy-efficient solution to the problem of task scheduling on restricted parallel processors. © 2015 Elsevier Inc. All rights reserved.","Approximation algorithm; Continuous speed model; Energy-efficient scheduling; Restricted parallel processors; Speed scaling"
"Toward trustworthy cloud service selection: A time-aware approach using interval neutrosophic set","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974803246&doi=10.1016%2fj.jpdc.2016.05.008&partnerID=40&md5=ab7378d6fa963e7aa99e2348353e728c","Cloud services consumers face a critical challenge in selecting trustworthy services from abundant candidates, and facilitating these choices has become a critical issue in the uncertain cloud industry. This paper employs the time series analysis to address challenges resulting from fluctuating quality of service, flexible service pricing and complicated potential risks in order to propose a time-aware trustworthy service selection approach with tradeoffs between performance-costs and potential risks. The original evaluation data about the services is preprocessed using a cloud model, and interval neutrosophic set (INS) theory is utilized to describe and measure the performance-costs and potential risks of services. In order to calculate and compare the candidate services while supporting tradeoffs between performance-costs and potential risks in different time periods, we established a cloud service interval neutrosophic set (CINS) and designed its operators and calculation rules, with theoretical proofs provided. The problem of time-aware trustworthy service selection is formulated as a multi-criterion decision-making (MCDM) problem of creating a ranked services list using CINS, and it is solved by developing a CINS ranking method. Finally, experiments based on a real-world dataset illustrate the practicality and effectiveness of the proposed approach. © 2016 Elsevier Inc. All rights reserved.","Cloud service selection; Interval neutrosophic set; Performance-costs; Potential risks; Time series analysis; Trustworthy service"
"Spline-based parallel nonlinear optimization of function sequences","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966863575&doi=10.1016%2fj.jpdc.2016.04.011&partnerID=40&md5=d5d50ab1f01b187fd9082590d14e38a9","Nonlinear dynamical system optimization problems exist in many scientific fields, ranging from computer vision to quantitative finance. In these problems, the underlying optimized parameters exhibit a certain degree of continuity, which can be formulated as a discrete sequence of nonlinear functions. Traditionally, such problems are either solved by ad-hoc algorithms or via independent optimization of the underlying functions. The former solutions are difficult to define and develop, requiring expertise in the field, while the latter approach does not take advantage of the inherent sequential properties of the functions. This paper presents a parallel spline-based algorithm for nonlinear optimization of function sequences, with emphasis on dataset sequences that represent dynamically evolving systems. The presented algorithm provides results that are more coherent with fewer evaluations than independent optimization of the sequence functions. We elaborate on the heuristic approach, the motivation behind using splines to model dynamical systems, and the various tiers of concurrency built into the algorithm. Furthermore, we present two distributed variants of the algorithm and compare their convergence with the serial version. The performance of the algorithm is demonstrated on benchmarks and real-world problems in audio signal decomposition, small angle X-ray scattering analysis, and video tracking of arbitrary objects. © 2016 Elsevier Inc.","Curve fitting; Dynamical systems; Nonlinear optimization; Parallel optimization"
"A load-balancing workload distribution scheme for three-body interaction computation on Graphics Processing Units (GPU)","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946615594&doi=10.1016%2fj.jpdc.2015.10.003&partnerID=40&md5=a4a9e78618153639b549d591c1ee44b5","Three-body effects play an important role for obtaining quantitatively high accuracy in a variety of molecular simulation applications. However, evaluation of three-body potentials is computationally costly, generally of O(N3) where N is the number of particles in a system. In this paper, we present a load-balancing workload distribution scheme for calculating three-body interactions by taking advantage of the Graphics Processing Units (GPU) architectures. Perfect load-balancing is achieved if N is not divisible by 3 and nearly perfect load-balancing is obtained if N is divisible by 3. The workload distribution scheme is particularly suitable for the GPU's Single Instruction Multiple Threads (SIMT) architecture, where particle's data accessed by threads can be coalesced into efficient memory transactions. We use two potential energy functions with three-body terms, the Axilrod-Teller potential and the Context-based Secondary Structure Potential, as examples to demonstrate the effectiveness of our workload distribution scheme. © 2015 Elsevier Inc. All rights reserved.","GPU; Load balancing; Three-body interactions"
"Energy-efficient task scheduling for multi-core platforms with per-core DVFS","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941117320&doi=10.1016%2fj.jpdc.2015.08.004&partnerID=40&md5=b6a668364ca2521487f12902a1ffb036","Energy-efficient task scheduling is a fundamental issue in many application domains, such as energy conservation for mobile devices and the operation of green computing data centers. Modern processors support dynamic voltage and frequency scaling (DVFS) on a per-core basis, i.e., the CPU can adjust the voltage or frequency of each core. As a result, the core in a processor may have different computing power and energy consumption. To conserve energy in multi-core platforms, we propose task scheduling algorithms that leverage per-core DVFS and achieve a balance between performance and energy consumption. We consider two task execution modes: the batch mode, which runs jobs in batches; and the online mode in which jobs with different time constraints, arrival times, and computation workloads co-exist in the system. For tasks executed in the batch mode, we propose an algorithm that finds the optimal scheduling policy; and for the online mode, we present a heuristic algorithm that determines the execution order and processing speed of tasks in an online fashion. The heuristic ensures that the total cost is minimal for every time interval during a task's execution. Furthermore, we analyze and derive algorithms with low time complexity for each mode. © 2015 Elsevier Inc. All rights reserved.","DVFS; Energy-efficient; Multi-core; Task characteristics; Task scheduling"
"An automaton-based index scheme supporting twig queries for on-demand XML data broadcast","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942080168&doi=10.1016%2fj.jpdc.2015.07.010&partnerID=40&md5=3ca80095a173ba37cbb319e9c1e443e9","XML data broadcast is an efficient way to deliver semi-structured information in a wireless mobile environment. In the literature, many approaches have been proposed to disseminate XML data via wireless broadcast. However, because of the existence of ""∗"" and ""//"" in queries, their performance deteriorates. In this paper, we propose a novel indexing method called Deterministic Finite Automaton-based Index (abbreviated as DFAI) on the XPath queries. Different from existing approaches which build index based on XML documents, DFAI is built based on the submitted queries. The new index treats the XPath queries as a DFA and it improves the efficiency of broadcast systems significantly. We further propose a compression strategy to reduce the index size of DFAI as well. Besides, as an extension, we extend the DFAI to support twig queries. Experiment results show that our method achieves a much better performance in terms of both access time and tuning time when compared with existing approaches. © 2015 Elsevier Inc.","Air indexing; Deterministic finite automaton; On-demand XML data broadcast; Twig query"
"Modeling the availability of Cassandra","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940655949&doi=10.1016%2fj.jpdc.2015.08.001&partnerID=40&md5=321a06ebc0bb130c6b8140028cc6d7b7","Peer-to-Peer systems have been introduced as an alternative to the traditional client-server scheme. Distributed Hash Tables, a type of structured Peer-to-Peer system, have been designed for massive storage purposes. In this work we model the behavior of a DHT based system, Cassandra, with focus on its fault tolerance capabilities, and more specifically, on its availability when facing two different situations: (1) transient failures, those in which a node goes off-line for a while and returns on-line maintaining its data, and (2) memory-less failures, those in which a node goes off-line and returns with no data. First, we introduce two analytical models (one for each scenario) that provide approximations to the behavior of Cassandra under different configurations, and secondly, in order to validate our models, we complete a set of experiments over a real Cassandra cluster. Experimental results confirm the validity of the proposed models of the availability of Cassandra. We also provide some examples of how these models can be used to optimize the availability configuration of Cassandra-based applications. © 2015 Elsevier Inc. All rights reserved.","Availability; Distributed hash table; Fault tolerance; Markov processes; Modeling techniques; Peer-to-peer"
"Edge disjoint Hamiltonian cycles in Eisenstein-Jacobi networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941089041&doi=10.1016%2fj.jpdc.2015.08.003&partnerID=40&md5=a3e819f7774f99e038e10221072717c7","Many communication algorithms in parallel systems can be efficiently solved by obtaining edge disjoint Hamiltonian cycles in the interconnection topology of the network. The Eisenstein-Jacobi (EJ) network generated by α=a+bρ, where ρ=(1+i3)/2, is a degree six symmetric interconnection network. The hexagonal network is a special case of the EJ network that can be obtained by α=a+(a+1)ρ. Generating three edge disjoint Hamiltonian cycles in the EJ network with generator α=a+bρ for gcd(a,b)=1 has been shown before. However, this problem has not been solved when gcd(a,b)=d>1. In this paper, some results to this problem are given. © 2015 Elsevier Inc.","Edge disjoint; Eisenstein-Jacobi network; Hamiltonian cycle; Interconnection networks; Parallel computing"
"Fault tolerance at system level based on RADIC architecture","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942092276&doi=10.1016%2fj.jpdc.2015.08.005&partnerID=40&md5=5ae684d1608a489c0bf8e5c316c40f50","The increasing failure rate in High Performance Computing encourages the investigation of fault tolerance mechanisms to guarantee the execution of an application in spite of node faults. This paper presents an automatic and scalable fault tolerant model designed to be transparent for applications and for message passing libraries. The model consists of detecting failures in the communication socket caused by a faulty node. In those cases, the affected processes are recovered in a healthy node and the connections are reestablished without losing data. The Redundant Array of Distributed Independent Controllers architecture proposes a decentralized model for all the tasks required in a fault tolerance system: protection, detection, recovery and masking. Decentralized algorithms allow the application to scale, which is a key property for current HPC system. Three different rollback recovery protocols are defined and discussed with the aim of offering alternatives to reduce overhead when multicore systems are used. A prototype has been implemented to carry out an exhaustive experimental evaluation through Master/Worker and Single Program Multiple Data execution models. Multiple workloads and an increasing number of processes have been taken into account to compare the above mentioned protocols. The executions take place in two multicore Linux clusters with different socket communications libraries. © 2015 Published by Elsevier Inc.","Message passing; RADIC; Resilience; Semi-coordinated checkpoint; Socket; Software fault tolerance; Uncoordinated checkpoint"
"Ephemeral networks with random availability of links: The case of fast networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946949661&doi=10.1016%2fj.jpdc.2015.10.002&partnerID=40&md5=52b530386d93c86254e1d127fc325e75","We consider here a model of temporal networks, the links of which are available only at certain moments in time, chosen randomly from a subset of the positive integers. We define the notion of the Temporal Diameter of such networks. We also define fast and slow such temporal networks with respect to the expected value of their temporal diameter. We then provide a partial characterization of fast random temporal networks. We also define the critical availability as a measure of periodic random availability of the links of a network, required to make the network fast. We finally give a lower bound as well as an upper bound on the (critical) availability. © 2015 Elsevier Inc.","Availability; Diameter; Random input; Temporal networks"
"Terminating population protocols via some minimal global knowledge assumptions","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939971194&doi=10.1016%2fj.jpdc.2015.02.005&partnerID=40&md5=9b436f3962135d29aac065a3b8e5d88e","We extend the population protocol model with a cover-time service that informs a walking state every time it covers the whole network. This represents a known upper bound on the cover time of a random walk. The cover-time service allows us to introduce termination into population protocols, a capability that is crucial for any distributed system. By reduction to an oracle-model we arrive at a very satisfactory lower bound on the computational power of the model: we prove that it is at least as strong as a Turing Machine of space logn with input commutativity, where n is the number of nodes in the network. We also give a logn-space, but nondeterministic this time, upper bound. Finally, we prove interesting similarities of this model to linear bounded automata. © 2015 Elsevier Inc.","Absence detector; Counter machine; Cover-time service; Interaction; Linear-bounded automaton; Population protocol; Rendezvous-based communication"
"A holistic approach to build real-time stream processing system with GPU","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931273833&doi=10.1016%2fj.jpdc.2015.05.002&partnerID=40&md5=1fabf4e133e21e0677c270cc73127c8e","Stream processing needs to process huge volume of data with strict deadline requirements. These applications generally consume large amount of network bandwidth and involve compute-intensive operations. Accelerating such operations with general purpose GPU has drawn a lot of attention from both academia and industry. However, GPU has not been applied to real-time stream processing due to its programming paradigm and unpredictable latency. In this paper, we study the problem of applying GPU to real-time processing and propose a holistic approach for building real-time stream processing system with GPU. Based on the proposed techniques, we build a GPU-accelerated SRTP reverse proxy that achieves more than 10Gbps overall throughput and guarantees low latency. Our work demonstrates that using GPU in high-speed real-time stream processing is both feasible and attractive. © 2015 Elsevier Inc. All rights reserved.","GPU; High-speed networking; Real-time; Stream processing"
"Big Data computing and clouds: Trends and future directions","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930936490&doi=10.1016%2fj.jpdc.2014.08.003&partnerID=40&md5=3b7489059348bf21dfac21cf1f4538f1","This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions. © 2014 Elsevier Inc. All rights reserved.","Analytics; Big Data; Cloud computing; Data management"
"On the competitiveness of scheduling dynamically injected tasks on processes prone to crashes and restarts","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938914980&doi=10.1016%2fj.jpdc.2015.07.007&partnerID=40&md5=126908c004f2b49dab694f95b5904cb6","To identify the tradeoffs between efficiency and fault-tolerance in dynamic cooperative computing, we initiate the study of a task performing problem under dynamic processes' crashes/restarts and task injections. The system consists of n message-passing processes which, subject to dynamic crashes and restarts, cooperate in performing tasks that are continuously and dynamically injected to the system. Tasks are not known a priori to the processes. This problem abstracts todays Internet-based computations, such as Grid computing and cloud services, where tasks are generated dynamically and different tasks may become known to different processes. We measure performance in terms of the number of pending tasks, and as such it can be directly compared with the optimum number obtained under the same crash-restart-injection pattern by the best off-line algorithm. Hence, we view the problem as an online problem and we pursue competitive analysis. We propose several deterministic algorithmic solutions to the considered problem under different information models and correctness criteria, and we argue that their performance is close to the best possible offline solutions. We also prove negative results that open interesting research directions. © 2015 Elsevier Inc. All rights reserved.","Competitive analysis; Distributed algorithms; Dynamic task injection; Processor crashes and restarts; Task execution"
"TEES: A novel multiple criteria optimization scheme for temperature-constrained energy efficient storage","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974794975&doi=10.1016%2fj.jpdc.2016.05.010&partnerID=40&md5=84324b891642e11da9d7cb463cc508c9","Existing energy saving schemes that have been developed for Energy Efficient Storage funnel I/O traffic on a few disks while allowing the rest idle. These schemes can cause long standing disks to overburden, resulting in a higher rate of disk failure and reliability degradation. In this paper, we develop a novel multiple criteria optimization scheme based on Fuzzy Decision Making theory, for the Temperature-constrained Energy Efficient Storage System called TEES. TEES aims to enforce a temperature constraint as well as performance requirements while also keeping energy consumption to a minimum. This is achieved by developing an online temperature prediction model and aggregating all the decision criteria, such as I/O performance, power consumption, estimated temperature and frequency of disk-status transition. The experimental results show that TEES is able to reduce disk temperature by 20-30% as compared with existing control methods, while obtaining comparable performance and power consumption. © 2016 Elsevier Inc. All rights reserved.","Energy-efficient storage; Fuzzy control; Temperature-constrained"
"Keep it cool and in time: With runtime monitoring to thermal-aware execution speeds for deadline constrained systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994667725&doi=10.1016%2fj.jpdc.2016.03.002&partnerID=40&md5=cc275348a2e56a76f38432b3b6457cfe","The Dynamic Power and Thermal Management (DPTM) system of Dynamic Voltage Frequency Scaling (DVFS) enabled processors compensates peak temperatures by slowing or even powering parts of the system down. While ensuring the integrity of computations, this comes with the drawback of losing performance. In the context of hard real-time systems, such unpredictable losses in performance are unacceptable, as they may lead to deadline misses which may yet compromise the integrity of the system. To safely execute hard real-time workloads on such systems, this article presents an online scheme for assigning speeds in such a way that (a) the system executes at low clock speed as often as possible, while (b) deadline violations are strictly ruled out. The proposed scheme is compared with an offline scheme which has complete knowledge about arrival times and execution demands of the workload. The benchmarking shows that for a workload which is always very close to the modelled maximum, our approach performs on-par with the offline scheme. In case of a workload which diverges from the modelled maximum more often, the speed assignments produced by our scheme become more pessimistic, as to ensure that all deadlines are met. © 2016 Elsevier Inc.","Dynamic power and temperature management; Dynamic Voltage Frequency Scaling; Multicore architectures; Online real-time scheduling; Real-time computing; Run-time monitoring"
"Transforming the multifluid PPM algorithm to run on GPUs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965014484&doi=10.1016%2fj.jpdc.2016.04.005&partnerID=40&md5=476b4b591c03b33d87179169a1c86bac","In the past several years, there has been much success in adapting numerical algorithms involving linear algebra and pairwise N-body force calculations to run well on GPUs. These numerical algorithms share the feature that high computational intensity can be achieved while holding only small amounts of data in on-chip storage. In previous work, we combined a briquette data structure and a heavily pipelined CFD processing of these data briquettes in sequence that results in a very small on-chip data workspace and high performance for our multifluid PPM gas dynamics algorithm on CPUs with standard sized caches. The on-chip data workspace produced in that earlier work is not small enough to meet the requirements of today's GPUs, which demand that no more than 32 kB of on-chip data be associated with a single thread of control (a warp). Here we report a variant of our earlier technique that allows a user-controllable trade-off between workspace size and redundant computation that can be a win on GPUs. We use our multifluid PPM gas dynamics algorithm to illustrate this technique. Performance results for this algorithm in 32-bit precision on a recently introduced dual-chip GPU, the Nvidia K80, are 1.7 times that on a similarly recent dual CPU node using two 16-core Intel Haswell chips. The redundant computation that allows the on-chip data context for each thread of control to be less than 32 kB is roughly 9% of the total. We have built an automatic translator from a Fortran expression to CUDA to ease the programming burden that is involved in applying our technique. © 2016 Elsevier Inc. All rights reserved.","Code transformation; Computational fluid dynamics; GPU computation; Precompilation"
"Application configuration selection for energy-efficient execution on multicore systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944676856&doi=10.1016%2fj.jpdc.2015.09.003&partnerID=40&md5=e80b60c0ab06e3c74255335cea8198bb","Modern computer systems are designed to balance performance and energy consumption. Several run-time factors, such as concurrency levels, thread mapping strategies, and dynamic voltage and frequency scaling (DVFS) should be considered in order to achieve optimal energy efficiency for a workload. Selecting appropriate run-time factors, however, is one of the most challenging tasks because the run-time factors are architecture-specific and workload-specific. While most existing works concentrate on either static analysis of the workload or run-time prediction results, in this paper, we present a hybrid two-step method that utilizes concurrency levels and DVFS settings to achieve the energy efficiency configuration for a workload. The experimental results based on a Xeon E5620 server with NPB and PARSEC benchmark suites show that the model is able to predict the energy efficient configuration accurately. On average, an additional 10% EDP (Energy Delay Product) saving is obtained by using run-time DVFS for the entire system. An off-line optimal solution is used to compare with the proposed scheme. The experimental results show that the average extra EDP saved by the optimal solution is within 5% on selective parallel benchmarks. © 2015 Elsevier Inc. All rights reserved.","Energy consumption; High performance computing; Parallel; Power model; Speedup model"
"Optimizing power consumption in multicore smartphones","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994592362&doi=10.1016%2fj.jpdc.2016.02.004&partnerID=40&md5=455beb788c0a11e0b2149cd9d0f0721b","This paper addresses the issue of managing power consumption in multicore smartphones via a middleware layer that schedules optimal number of cores for currently running applications taking into account the tradeoff between power consumption, performance and user experience. The paper first describes a simple and accurate method to measure the overall power consumption and then studies the impact of scheduling seven different popular applications over one to four cores on the overall power consumption. Based on this study, the paper proposes three new power-aware scheduling algorithms that dynamically schedule optimal number of cores as well as dynamically adjust the voltage frequency of each online core to achieve the best tradeoff between power consumption, application performance and user experience under the current context. Evaluation from a prototype implementation of the middleware on a quad-core HTC One shows that these algorithms result in significant reduction in power consumption while ensuring good performance and user experience. © 2016 Elsevier Inc.","Android smartphone; Multi-core processor; Power-aware scheduler"
"Optimization techniques for sparse matrix-vector multiplication on GPUs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968725228&doi=10.1016%2fj.jpdc.2016.03.011&partnerID=40&md5=e8936549db52da60a6a87f5210da7289","Sparse linear algebra is fundamental to numerous areas of applied mathematics, science and engineering. In this paper, we propose an efficient data structure named AdELL+ for optimizing the SpMV kernel on GPUs, focusing on performance bottlenecks of sparse computation. The foundation of our work is an ELL-based adaptive format which copes with matrix irregularity using balanced warps composed using a parametrized warp-balancing heuristic. We also address the intrinsic bandwidth-limited nature of SpMV with warp granularity, blocking, delta compression and nonzero unrolling, targeting both memory footprint and memory hierarchy efficiency. Finally, we introduce a novel online auto-tuning approach that uses a quality metric to predict efficient block factors and that hides preprocessing overhead with useful SpMV computation. Our experimental results show that AdELL+ achieves comparable or better performance over other state-of-the-art SpMV sparse formats proposed in academia (BCCOO) and industry (CSR+ and CSR-Adaptive). Moreover, our auto-tuning approach makes AdELL+ viable for real-world applications. © 2016 Elsevier Inc. All rights reserved.","Adaptive; AdELL+; Auto-tuning; Blocking; Compression; GPU; Optimization; SpMV; Unrolling"
"Snap-stabilizing committee coordination","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945205442&doi=10.1016%2fj.jpdc.2015.09.004&partnerID=40&md5=9a5402e046ff8e88834642ada890e126","In the committee coordination problem, a committee consists of a set of professors and committee meetings are synchronized, so that each professor participates in at most one committee meeting at a time. In this paper, we propose two snap-stabilizing distributed algorithms for the committee coordination. Snap-stabilization is a versatile property which requires a distributed algorithm to efficiently tolerate transient faults. Indeed, after a finite number of such faults, a snap-stabilizing algorithm immediately operates correctly, without any external intervention. We design snap-stabilizing committee coordination algorithms enriched with some desirable properties related to concurrency, (weak) fairness, and a stronger synchronization mechanism called 2-Phase Discussion. In our setting, all processes are identical and each process has a unique identifier. The existing work in the literature has shown that (1) in general, fairness cannot be achieved in committee coordination, and (2) it becomes feasible if each professor waits for meetings infinitely often. Nevertheless, we show that even under this latter assumption, it is impossible to implement a fair solution that allows maximal concurrency. Hence, we propose two orthogonal snap-stabilizing algorithms, each satisfying 2-phase discussion, and either maximal concurrency or fairness. The algorithm that implements fairness requires that every professor waits for meetings infinitely often. Moreover, for this algorithm, we introduce and evaluate a new efficiency criterion called the degree of fair concurrency. This criterion shows that even if it does not satisfy maximal concurrency, our snap-stabilizing fair algorithm still allows a high level of concurrency. © 2015 Elsevier Inc. All rights reserved.","Committee coordination; Distributed algorithms; Self-stabilization; Snap-stabilization"
"On scalable parallel recursive backtracking","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938702239&doi=10.1016%2fj.jpdc.2015.07.006&partnerID=40&md5=8bfaa851127ae1fb779d71a4a24077e2","Supercomputers are equipped with an increasingly large number of cores to use computational power as a way of solving problems that are otherwise intractable. Unfortunately, getting serial algorithms to run in parallel to take advantage of these computational resources remains a challenge for several application domains. Many parallel algorithms can scale to only hundreds of cores. The limiting factors of such algorithms are usually communication overhead and poor load balancing. Solving NP-hard graph problems to optimality using exact algorithms is an example of an area in which there has so far been limited success in obtaining large scale parallelism. Many of these algorithms use recursive backtracking as their core solution paradigm. In this paper, we propose a lightweight, easy-to-use, scalable approach for transforming almost any recursive backtracking algorithm into a parallel one. Our approach incurs minimal communication overhead and guarantees a load-balancing strategy that is implicit, i.e., does not require any problem-specific knowledge. The key idea behind our approach is the use of efficient traversal operations on an indexed search tree that is oblivious to the problem being solved. We test our approach with parallel implementations of algorithms for the well-known Vertex Cover and Dominating Set problems. On sufficiently hard instances, experimental results show nearly linear speedups for thousands of cores, reducing running times from days to just a few minutes. © 2015 Elsevier Inc. All rights reserved.","Dominating set; Load balancing; Parallel algorithms; Recursive backtracking; Vertex cover"
"ECHO: Efficient Complex Query over DHT Overlays","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948403811&doi=10.1016%2fj.jpdc.2015.10.007&partnerID=40&md5=840dbb7165a6970f6ee6ddc4646387f7","In this article we propose ECHO, a novel and lightweight solution that efficiently supports range queries over a ring-like Distributed Hash Table (DHT) structure. By implementing a tree-based index structure and an effective query routing strategy, ECHO provides low-latency and low-overhead query searches by exploiting the Tabu Search principle. Load balancing is also improved reducing the traditional bottleneck problems arising in upper level nodes of tree-based index structures such as PHT. Furthermore, ECHO copes with DHT churn problems as its index exploits logical information as opposed to static reference cache approaches or replication techniques. The performance evaluation results obtained using PeerSim simulator show that ECHO achieves efficient performance compared other solutions such as the PHT strategy and its optimized version which includes a query cache. © 2015 Elsevier Inc.","DHT; Distributed indexing; Peer-to-peer; Prefix trees; Range queries"
"Distributed formation of degree constrained minimum routing cost tree in wireless ad-hoc networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933514959&doi=10.1016%2fj.jpdc.2015.05.006&partnerID=40&md5=576e3cfbb7e6b3cf44690400500e0065","During several decades, there have been many researches on approximation algorithms for constructing minimum routing cost tree (MRCT) that minimizes the sum of routing cost of all pairs in a tree topology. Existing algorithms have been mainly studied in the field of graph theory, thus it is difficult to apply them to multi-hop wireless ad-hoc networks due to the theoretical and centralized methodology. In addition, wireless ad-hoc network protocols restrict the maximum degree, which is the maximum number of children a parent may have, in order to prevent excessive concentration of traffic. However, this limitation has not been considered by any existing algorithms. In this paper, we define the degree constrained MRCT (DC-MRCT) problem and extract the characteristics of DC-MRCT by analyzing all possible tree topologies for the given number of nodes. Based on these characteristics that DC-MRCT has the minimum sum of tree level and the maximum square sum of subtree sizes, we propose a distributed DC-MRCT Formation (DC-MRCTF) algorithm that can be applicable to any type of wireless ad-hoc network protocols working on tree topology. Performance evaluation shows that DC-MRCTF gives noticeable benefit for up to 80% of individual communication pair compared with the representative tree formation algorithm in ZigBee as well as significantly reduces the sum of routing cost of all pairs regardless of network density. © 2015 Elsevier Inc. All rights reserved.","Bounded-degree; Degree constrained; Minimum routing cost tree; MRCST; MRCT; Tree formation; Wireless"
"A hybrid genetic algorithm for optimization of scheduling workflow applications in heterogeneous computing systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946569809&doi=10.1016%2fj.jpdc.2015.10.001&partnerID=40&md5=88d9a58819515c64451c77f03e38ec2d","Workflow scheduling is a key component behind the process for an optimal workflow enactment. It is a well-known NP-hard problem and is more challenging in the heterogeneous computing environment. The increasing complexity of the workflow applications is forcing researchers to explore hybrid approaches to solve the workflow scheduling problem. The performance of genetic algorithms can be enhanced by the modification in genetic operators and involving an efficient heuristic. These features are incorporated in the proposed Hybrid Genetic Algorithm (HGA). A solution obtained from a heuristic is seeded in the initial population that provides a direction to reach an optimal (makespan)solution. The modified two fold genetic operators search rigorously and converge the algorithm at the best solution in less amount of time. This is proved to be the strength of the HGA in the optimization of fundamental objective (makespan) of scheduling. The proposed algorithm also optimizes the load balancing during the execution side to utilize resources at maximum. The performance of the proposed algorithm is analyzed by using synthesized datasets, and real-world application workflows. The HGA is evaluated by comparing the results with renowned and state of the art algorithms. The experimental results validate that the HGA outperforms these approaches and provides quality schedules with less makespans. © 2015 Elsevier Inc.","Directed Acyclic Graphs; Genetic algorithm; Heuristic; Workflow"
"An exact algorithm for sparse matrix bipartitioning","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941997217&doi=10.1016%2fj.jpdc.2015.06.005&partnerID=40&md5=5959cd6e6737393e1d4311c43e24ef7c","The sparse matrix partitioning problem arises when minimizing communication in parallel sparse matrix-vector multiplications. Since the problem is NP-hard, heuristics are usually employed to find solutions. Here, we present a purely combinatorial branch-and-bound method for computing optimal bipartitionings of sparse matrices, in the sense that they have the lowest communication volume out of all possible bipartitionings obeying a certain load balance constraint. The method is based on a way of partitioning similar to the recently proposed medium-grain heuristic, which reduces the number of solutions to be considered in the branch-and-bound method. We applied the proposed optimal bipartitioner to find the optimal communication volume of all matrices of the University of Florida sparse matrix collection with 1000 nonzeros or less. For 85% of the matrices, an optimal bipartitioning was found within a single day of computation and for 58% even within a second. We also present optimal results for selected larger matrices, up to 129,042 nonzeros. The optimal bipartitionings and corresponding communication volumes are made publicly available in a benchmark collection. © 2015 Elsevier Inc. All rights reserved.","Branch-and-bound; Exact algorithm; Hypergraph; Parallel computing; Partitioning; Sparse matrix; Sparse matrix-vector multiplication"
"FPGA, GPU, and CPU implementations of Jacobi algorithm for eigenanalysis","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975159032&doi=10.1016%2fj.jpdc.2016.05.014&partnerID=40&md5=1026a144c314e319182354783e1959ba","Parallel implementations of Jacobi algorithm for eigenanalysis of a matrix on most commonly used high performance computing (HPC) devices such as central processing unit (CPU), graphics processing unit (GPU), and field-programmable gate array (FPGA) are discussed in this paper. Their performances are investigated and compared. It is shown that CPU, even with multi-threaded implementation, is not a feasible option for large dense matrices. For the GPU implementation, performance impact of the global memory access patterns on the GPU board and the memory coalescing are emphasized. Three memory access methods are proposed. It is shown that one of them achieves 81.6% computational performance improvement over the traditional GPU methods, and it runs 68.5 times faster than a single-threaded CPU for a dense symmetric square matrix of size 1,024. Furthermore, FPGA implementation is presented and its performance running on chips from two major manufacturers are reported. A comparison of GPU and FPGA implementations is quantified and ranked. It is reported that FPGA design delivers the best performance for such a task while GPU is a strong competitor requiring less development effort with superior scalability. We predict that emerging big data applications will benefit from real-time and high performance computing implementations of eigenanalysis for information inference and signal analytics in the future. © 2016 Elsevier Inc.","Chess tournament; CORDIC; CPU; Eigenanalysis; FPGA; GPU; Jacobi algorithm; Karhunen-Loève transform; Memory coalescing; Principal component analysis"
"Single system image: A survey","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960194681&doi=10.1016%2fj.jpdc.2016.01.004&partnerID=40&md5=81db3ef81ba2a6c69111128eef70041d","Single system image is a computing paradigm where a number of distributed computing resources are aggregated and presented via an interface that maintains the illusion of interaction with a single system. This approach encompasses decades of research using a broad variety of techniques at varying levels of abstraction, from custom hardware and distributed hypervisors to specialized operating system kernels and user-level tools. Existing classification schemes for SSI technologies are reviewed, and an updated classification scheme is proposed. A survey of implementation techniques is provided along with relevant examples. Notable deployments are examined and insights gained from hands-on experience are summarized. Issues affecting the adoption of kernel-level SSI are identified and discussed in the context of technology adoption literature. © 2016 Elsevier Inc. All rights reserved.","Distributed hypervisors; Distributed operating systems; Single system image; Technology adoption"
"Failure detectors in homonymous distributed systems (with an application to consensus)","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935012005&doi=10.1016%2fj.jpdc.2015.05.007&partnerID=40&md5=dad2c9a933581c41cf3c406795d44785","This paper is on homonymous distributed systems where processes are prone to crash failures and have no initial knowledge of the system membership (""homonymous"" means that several processes may have the same identifier). New classes of failure detectors suited to these systems are first defined. Among them, the classes HΩ and HΣ are introduced that are the homonymous counterparts of the classes Ω and Σ, respectively. (Recall that the pair (Ω,Σ) defines the weakest failure detector to solve consensus.) Then, the paper shows how HΩ and HΣ can be implemented in homonymous systems without membership knowledge (under different synchrony requirements). Finally, two algorithms are presented that use these failure detectors to solve consensus in homonymous asynchronous systems where there is no initial knowledge of the membership. One algorithm solves consensus with (HΩ,HΣ), while the other uses only HΩ, but needs a majority of correct processes. Observe that the systems with unique identifiers and anonymous systems are extreme cases of homonymous systems from which follows that all these results also apply to these systems. Interestingly, the new failure detector class HΩ can be implemented with partial synchrony (i.e., all messages sent after some bounded time GST will be received after at most an unknown bounded latency δ), while the analogous class AΩ defined for anonymous systems cannot be implemented (even in synchronous systems). Hence, the paper provides the first consensus algorithm for anonymous systems with this model of partial synchrony and a majority of correct processes. © 2015 Elsevier Inc. All rights reserved.","Agreement problem; Asynchrony; Consensus; Distributed computability; Failure detector; Homonymous systems; Message-passing; Process crash"
"Adaptive, scalable and reliable monitoring of big data on clouds","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930934081&doi=10.1016%2fj.jpdc.2014.08.007&partnerID=40&md5=866a2e7ff9d18a9c37a733bc27e1c304","Real-time monitoring of cloud resources is crucial for a variety of tasks such as performance analysis, workload management, capacity planning and fault detection. Applications producing big data make the monitoring task very difficult at high sampling frequencies because of high computational and communication overheads in collecting, storing, and managing information. We present an adaptive algorithm for monitoring big data applications that adapts the intervals of sampling and frequency of updates to data characteristics and administrator needs. Adaptivity allows us to limit computational and communication costs and to guarantee high reliability in capturing relevant load changes. Experimental evaluations performed on a large testbed show the ability of the proposed adaptive algorithm to reduce resource utilization and communication overhead of big data monitoring without penalizing the quality of data, and demonstrate our improvements to the state of the art. © 2014 Elsevier Inc. All rights reserved.","Adaptivity; Big data; Cloud computing; Monitoring; Scalability"
"Reducing memory usage by the lifting-based discrete wavelet transform with a unified buffer on a GPU","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966351789&doi=10.1016%2fj.jpdc.2016.03.010&partnerID=40&md5=587c16b2588f1645ea1a68ae8a9a3637","In this study, to improve the speed of the lifting-based discrete wavelet transform (DWT) for large-scale data, we propose a parallel method that achieves low memory usage and highly efficient memory access on a graphics processing unit (GPU). The proposed method reduces the memory usage by unifying the input buffer and output buffer but at the cost of a working memory region that is smaller than the data size n. The method partitions the input data into small chunks, which are then rearranged into groups so different groups of chunks can be processed in parallel. This data rearrangement scheme classifies chunks in terms of data dependency but it also facilitates transformation via simultaneous access to contiguous memory regions, which can be handled efficiently by the GPU. In addition, this data rearrangement is interpreted as a product of circular permutations such that a sequence of seeds, which is an order of magnitude shorter than input data, allows the GPU threads to compute the complicated memory indexes needed for parallel rearrangement. Because the DWT is usually part of a processing pipeline in an application, we believe that the proposed method is useful for retaining the amount of memory for use by other pipeline stages. © 2016 Elsevier Inc. All rights reserved.","Discrete wavelet transform; GPU; In-place algorithm; Lifting scheme; Memory-saving computation"
"An integrated approach to workflow mapping and task scheduling for delay minimization in distributed environments","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938718526&doi=10.1016%2fj.jpdc.2015.07.004&partnerID=40&md5=44abf77099dfa9a84c8776587b8f9779","Many scientific applications feature large-scale workflows consisting of computing modules that must be strategically deployed and executed in distributed environments. The end-to-end performance of such scientific workflows depends on both the mapping scheme that determines module assignment, and the scheduling policy that determines resource allocation if multiple modules are mapped to the same node. These two aspects of workflow optimization are traditionally treated as two separated topics, and the interactions between them have not been fully explored by any existing efforts. As the scale of scientific workflows and the complexity of network environments rapidly increase, each individual aspect of performance optimization alone can only meet with limited success. We conduct an in-depth investigation into workflow execution dynamics in distributed environments and formulate a generic problem that considers both workflow mapping and task scheduling to minimize the end-to-end delay of workflows. We propose an integrated solution, referred to as Mapping and Scheduling Interaction (MSI), to improve the workflow performance. The efficacy of MSI is illustrated by both extensive simulations and proof-of-concept experiments using real-life scientific workflows for climate modeling on a PC cluster. © 2015 Elsevier Inc. All rights reserved.","End-to-end delay; On-node scheduling; Scientific workflows; Workflow mapping"
"A distributed selectivity-driven search strategy for semi-structured data over DHT-based networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964615071&doi=10.1016%2fj.jpdc.2016.03.015&partnerID=40&md5=59c3e343d06f559ac33baa1edf77f50c","Distributed Hash Tables (DHTs) are widely used for indexing and locating many types of resources, including semi-structured data modeled as XML documents. A common distributed strategy to process an XML query over a DHT consists in splitting it into a set of simple path queries, and resolving each of them separately. The traffic generated by this strategy grows with the number of paths in the query. To overcome this drawback, an alternative strategy consists in resolving only the sub-query associated with the most selective path, and then submitting the original query to the nodes in the result set. A first goal of this paper is to provide an analytical and experimental study of the two strategies to assess their relative merits in different scenarios. On the basis of this study, we introduce an Adaptive Path Selection (APS) search technique that resolves an XML query in a distributed way by querying either the most selective path or the whole path set, based on the selectivity of the paths in the query. The effective use of APS requires that the querying nodes know in advance the selectivity of all the paths. Addressing this problem is another goal of the paper, which is achieved through: (i) The definition of a space-efficient data structure, the Path Selectivity Table (PST), which given any path, returns an estimate of its selectivity. (ii) The definition of an efficient strategy that builds the PST in a distributed way and propagates it to all nodes in the network with logarithmic performance bounds and without redundant messages. Experimental results show that the PST accurately estimates the path selectivity values, and that the traffic generated by the APS algorithm using PST-estimated selectivity values is comparable to that produced by APS assuming to know the real path selectivity values. © 2016 Elsevier Inc. All rights reserved.","Adaptive Path Selection; Distributed Hash Tables; Path selectivity; Semi-structured data"
"Quantitative modeling of power performance tradeoffs on extreme scale systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937202329&doi=10.1016%2fj.jpdc.2015.06.006&partnerID=40&md5=49f7bef01cc893dfcba3716cc97e725e","As high performance computing (HPC) continues to grow in scale and complexity, energy becomes a critical constraint in the race to exascale computing. The days of ""performance at all cost"" are coming to an end. While performance is still a major objective, future HPC will have to deliver desired performance under the energy constraint. Among various power management methods, power capping is a widely used approach. Unfortunately, the impact of power capping on system performance, user jobs, and power-performance efficiency are not well studied due to many interfering factors imposed by system workload and configurations. To fully understand power management in extreme scale systems with a fixed power budget, we introduce a power-performance modeling tool named PuPPET (Power Performance PETri net). Unlike the traditional performance modeling approaches such as analytical methods or trace-based simulators, we explore a new approach-colored Petri nets-for the design of PuPPET. PuPPET is fast and extensible for navigating through different configurations. More importantly, it can scale to hundreds of thousands of processor cores and at the same time provide high levels of modeling accuracy. We validate PuPPET by using system traces (i.e., workload log and power data) collected from the production 48-rack IBM Blue Gene/Q supercomputer at Argonne National Laboratory. Our trace-based validation demonstrates that PuPPET is capable of modeling the dynamic execution of parallel jobs on the machine by providing an accurate approximation of energy consumption. In addition, we present two case studies of using PuPPET to study power-performance tradeoffs on petascale systems. © 2015 Elsevier Inc. All rights reserved.","Colored Petri net; Extreme scale systems; High performance computing; Power capping; Power performance analysis"
"Secure and controllable k-NN query over encrypted cloud data with key confidentiality","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951770536&doi=10.1016%2fj.jpdc.2015.11.004&partnerID=40&md5=fab217e457c6ecb39eba961922da3f4b","To enjoy the advantages of cloud service while preserving security and privacy, huge data are increasingly outsourced to cloud in encrypted form. Unfortunately, most conventional encryption schemes cannot smoothly support encrypted data analysis and processing. As a significant topic, several schemes have been recently proposed to securely compute k-nearest neighbors (k-NN) on encrypted data being outsourced to cloud server (CS). However, most existing k-NN search methods assume query users (QUs) are fully-trusted and know the key of data owner (DO) to encrypt/decrypt outsourced database. It is not realistic in many situations. In this paper, we propose a new secure k-NN query scheme on encrypted cloud data. Our approach simultaneously achieves: (1) data privacy against CS: the encrypted database can resist potential attacks of CS, (2) key confidentiality against QUs: to avoid the problems caused by key-sharing, QUs cannot learn DO's key, (3) query privacy against CS and DO: the privacy of query points is preserved as well, (4) query controllability: QUs cannot launch a feasible k-NN query for any new point without approval of DO. We provide theoretical guarantees for security and privacy properties, and show the efficiency of our scheme through extensive experiments. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; k-nearest neighbors; Privacy; Query"
"Co-optimizing application partitioning and network topology for a reconfigurable interconnect","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971320852&doi=10.1016%2fj.jpdc.2016.04.010&partnerID=40&md5=a58706871291393592f1aa8a82fa3f07","To realize the full potential of a high-performance computing system with a reconfigurable interconnect, there is a need to design algorithms for computing a topology that will allow for a high-throughput load distribution, while simultaneously partitioning the computational task graph of the application for the computed topology. In this paper, we propose a new framework that exploits such reconfigurable interconnects to achieve these interdependent goals, i.e., to iteratively co-optimize the network topology configuration, application partitioning and network flow routing to maximize throughput for a given application. We also present a novel way of computing a high-throughput initial topology based on the structural properties of the application to seed our co-optimizing framework. We show the value of our approach on synthetic graphs that emulate the key characteristics of a class of stream computing applications that require high throughput. Our experiments show that the proposed technique is fast and computes high-quality partitions of such graphs for a broad range of hardware parameters that varies the bottleneck from computation to communication. Finally, we show how using a particular topology as a seed to our framework significantly reduces the time to compute the final topology. © 2016 Elsevier Inc. All rights reserved.","Network configuration algorithm; Optical circuit switch; Reconfigurable interconnect topology; Stream-computing; Topology-aware graph partitioning"
"A unified framework for k-coverage and data collection in heterogeneous wireless sensor networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952836959&doi=10.1016%2fj.jpdc.2015.09.009&partnerID=40&md5=bdfa088076423e7f135498ad7719ddc7","One of the fundamental tasks in the development of wireless sensor networks is coverage, which measures the network effectiveness and accuracy in event detection. While most existing studies on coverage focus on homogeneous and static wireless sensor networks, where the sensors have the same features, such as sensing, communication, and initial energy reserve, this paper considers heterogeneous sensors and sink mobility, which provide a more realistic and accurate view of the network design for a variety of sensing applications. In this paper, we exploit Helly's Theorem to address the joint problem of k-coverage and data collection in heterogeneous wireless sensor networks, where each point in a field of interest is simultaneously covered by at least k active heterogeneous sensors. More precisely, we introduce a global framework that jointly considers k-coverage and data collection. Precisely, we propose a multi-tier (or hierarchical) architecture of heterogeneous sensors along with two data collection protocols. While the first protocol is based on an adaptive hybrid forwarding scheme, the second one uses a mobile sink to collect the sensed data from all the sensors in the network. To this end, we investigate the optimal mobility strategy of the sink in order to minimize the average total energy consumption due to both of data communication and sink mobility in a circular sensor field. We divide the field into concentric circular bands with the same width, and derive a closed-form solution for the optimal sink mobility. We corroborate our analysis with simulation results to assess our proposed framework. We find that our sink mobility-based data collection protocol outperforms our hybrid geographic forwarding-based data collection protocol. © 2015 Elsevier Inc. All rights reserved.","Adaptive forwarding; Data collection; Helly's theorem; Heterogeneity; Hierarchical deployment; k-coverage; Sink mobility; Wireless sensor nets"
"A highly scalable parallel algorithm for solving Toeplitz tridiagonal systems of linear equations","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946866652&doi=10.1016%2fj.jpdc.2015.10.004&partnerID=40&md5=b7b0a5363b6e94d66d82b5c6188efadf","Based on a modification of the dichotomy algorithm, we propose a novel parallel procedure for solving tridiagonal systems of equations with Toeplitz matrices. Taking the structure of the Toeplitz matrices, we may substantially reduce the number of the preliminary calculations of the dichotomy algorithm, which makes possible to efficiently solve systems of linear equations with both one and several right-hand sides. On examples of solving the 2D Poisson equation by the variable separation method and the 3D Poisson equation by a combination of the alternating direction implicit and the variable separation methods we show that the computation accuracy is comparable with the sequential version of the Thomas method, the dependence of the speedup on the number of processors being almost linear. The proposed modification is aimed at parallel implementation of a broad class of numerical methods including the Toeplitz tridiagonal matrices inversion. © 2015 Elsevier Inc.","Parallel dichotomy algorithm; Poisson equation; Thomas algorithm; Toeplitz matrices"
"Scalable linear programming based resource allocation for makespan minimization in heterogeneous computing systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938703772&doi=10.1016%2fj.jpdc.2015.07.002&partnerID=40&md5=11fe476d9c9dfe8201edd55c30ec15aa","Resource management for large-scale high performance computing systems poses difficult challenges to system administrators. The extreme scale of these modern systems require task scheduling algorithms that are capable of handling at least millions of tasks and thousands of machines. Highly scalable algorithms are necessary to efficiently schedule tasks to maintain the highest level of performance from the system. In this study, we design a novel linear programming based resource allocation algorithm for heterogeneous computing systems to efficiently compute high quality solutions for minimizing makespan. The novel algorithm tightly bounds the optimal makespan from below with an infeasible schedule and from above with a fully feasible schedule. The new algorithms are highly scalable in terms of solution quality and computation time as the problem size increases because they leverage similarity in tasks and machines. This novel algorithm is compared to existing algorithms via simulation on a few example systems. © 2015 Elsevier Inc.","Bag-of-tasks; Heterogeneous computing; High performance computing; Linear programming; Resource management; Scheduling"
"WFR-TM: Wait-free readers without sacrificing speculation of writers","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973655878&doi=10.1016%2fj.jpdc.2016.05.002&partnerID=40&md5=959d3ba30876bf250dbc3702f8ae64c8","Transactional Memory (TM) is a promising concurrent programming paradigm which employs transactions to achieve synchronization in accessing common data known as transactional variables. A transaction may either commit, making its updates to transactional variables visible, or abort, discarding its updates. We introduce WFR-TM, a TM algorithm which attempts to combine the advantages of pessimistic and optimisticTM. In a pessimistic TM, no transaction ever aborts; however, current pessimistic TM implementations, execute update transactions sequentially, decreasing the degree of achieved parallelism. In optimistic TM, transactions are executed concurrently and they commit if they have encountered no conflict during their execution. In WFR-TM, read-only transactions not only are wait-free, but also they never execute expensive synchronization operations (like CAS, LL/SC, etc.). This is achieved without sacrificing the parallelism between update transactions. Update transactions synchronize pessimistically with concurrently executed read-only transactions but they synchronize optimistically with each other. © 2016 Elsevier Inc. All rights reserved.","Concurrent programming; Optimistic TM; Pessimistic TM; Transactional memory; Wait-freedom"
"A self-adaptive reconfiguration scheme for throughput maximization in municipal WMNs","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964326623&doi=10.1016%2fj.jpdc.2016.03.004&partnerID=40&md5=fe866803599817059facd763f459f564","Multi-hop wireless mesh network (WMN) is considered to be an economical technique for last-mile broadband Internet access (Tang et al., 2006). Fueled by the rapid development of the Internet of Things, it is increasing important to provide robust and high-quality urban wireless mesh networks (WMNs) for public service. However, WMNs encounter frequent link failures during their lifetime in practical use as a consequence of large-scale wireless interference, block buildings, etc., which deeply influences the performance of the network. These failures should be recovered in time or prevented with foresight. Tremendous labor and management cost should also be avoided. In this paper, we focus on an interesting phenomenon called ""loop effect"" in urban WMNs which may cause frequent link failures. A scheme named anti-loop network reconfiguration system (ALRS) for link recovery in municipal WMNs with QoS requirement is proposed and analyzed. ALRS takes the historical information into consideration to achieve maximum throughput, interference avoidance and self-reconfiguration with no requirement on mesh node antennas. Our evaluation results show that ALRS outperforms all the existing techniques in improving network throughput and reducing the occurrence probability of continuous link failure. © 2016 Elsevier Inc. All rights reserved.","Municipal WMN; Optimal; Reconfiguration; Throughput"
"A location service for partial spatial replicas implementing an R-tree in a relational database","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958974877&doi=10.1016%2fj.jpdc.2016.01.003&partnerID=40&md5=f5bd2bf115667897d5009d3def0454ed","As parallel computing has become increasingly common, the need for scalable and efficient ways of storing and locating data has become increasingly acute. For years, both grid and cloud computing have distributed data across machines and even clusters at different geographic locations (sites). However not all sites need all of the data in a particular data set, or have the (perhaps specialized) processing capabilities required. These facts challenge the conventional wisdom that we should always move the computation to the data rather than the data to the computation. Sometimes the data actually required is small. In other cases, the site with specialized processing capabilities (such as a GPU equipped cluster) cannot handle the demands placed on it unless a way is found to let that cluster select the data that is actually needed, even if it is not stored locally.","Globus toolkit; Prefetching; R-tree; Replica; Replica location service; Spatial"
"Ariadne - Directive-based parallelism extraction from recursive functions","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940826921&doi=10.1016%2fj.jpdc.2015.07.009&partnerID=40&md5=3608d6d91df2693326b2c71d9659f6f6","In this paper we present Ariadne, a compiler that extracts parallelism from recursive function calls. Ariadne takes as input C code enhanced with directives for recursive functions and automatically produces code for multi-core architectures. It produces code for the POSIX standard, the OpenMP model and the Cilk programming language, which run on a wide variety of computing systems. Ariadne also produces code for SL, a programming language proposed for the SVP processor and model. This is of special interest, since we can map certain function calls onto SVP, which contain inherent parallelism that cannot efficiently be expressed in other programming models. Ariadne is the only compiler that extracts parallelism from various forms of recursive functions using directives. It is also the only compiler that handles all forms of reduction operations for addition, subtraction, multiplication and division. The experimental results are very promising showing significant speedups in all benchmarks. © 2015 Elsevier Inc. All rights reserved.","Cilk; Directives; OpenMP; POSIX; Recursive functions; SVP"
"Towards the modelling of secure pervasive computing systems: A paradigm of Context-Aware Secure Action System","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946962936&doi=10.1016%2fj.jpdc.2015.09.008&partnerID=40&md5=f41ed7e747b125e3ab612cfa371e4059","The design of security-critical pervasive systems is challenging due to that security constraints are often highly dependent on dynamically changing contexts. To increase the trustworthiness of pervasive systems, a dependable approach to system development must be followed, which enables seamless integration of the functional, security and context-awareness requirements. This paper proposes a paradigm which enables the specification of the functional, security and context-awareness requirements of a system in a single formalism, called Context-Aware Secure Action System (CASAS). Its syntax, formal semantics and pragmatics are presented, as well as algorithms and techniques for analysing the behaviour of a pervasive computing system. © 2015 Elsevier Inc. All rights reserved.","Access control; Compositional verification; Context-aware; Pervasive systems; Secure action system"
"Investigating different general-purpose and embedded multicores to achieve optimal trade-offs between performance and energy","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994667744&doi=10.1016%2fj.jpdc.2016.04.003&partnerID=40&md5=beb0782d77838a52d14657a1ee061f89","Thread-level parallelism (TLP) is being widely exploited in embedded and general-purpose multicore processors (GPPs) to increase performance. However, parallelizing an application involves extra executed instructions and accesses to the shared memory, to communicate and synchronize. The overhead of accessing the shared memory, which is very costly in terms of delay and energy because it is at the bottom of the hierarchy, varies depending on the communication model and level of data exchange/synchronization of the application. On top of that, multicore processors are implemented using different architectures, organizations and memory subsystems. In this complex scenario, we evaluate 14 parallel benchmarks implemented with 4 different parallel programming interfaces (PPIs), with distinct communication rates and TLP, running on five representative multicore processors targeted to general-purpose and embedded systems. We show that while the former presents the best performance and the latter will be the most energy efficient, there is no single option that offers the best result for both. We also demonstrate that in applications with low levels of communication, what matters is the communication model, not a specific PPI. On the other hand, applications with high communication demands have a huge search space that can be explored. For those, Pthreads is the most efficient PPI for Intel Processors, while OpenMP is the best for ARM ones. MPI is the worst choice in almost any scenario, and gets very inefficient as the TLP increases. We also evaluate energy delayxproduct (EDxP), weighting performance towards energy by varying the value of x. In a representative case where energy is the most important, three different processors can be the best alternative for different values of x. Finally, we explore how static power influences total energy consumption, showing that its increase brings benefits to ARM multiprocessors, with the opposite effect for Intel ones. © 2016 Elsevier Inc.","Embedded and general-purpose processors; Energy; Energy-delay product; Multicore architectures; Performance; Thread-level parallelism exploitation"
"Architectural support for efficient message passing on shared memory multi-cores","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994663261&doi=10.1016%2fj.jpdc.2016.02.005&partnerID=40&md5=b923894296a228ce799279c1d606323d","Thanks to programming approaches like actor-based models, message passing is regaining popularity outside large-scale scientific computing for building scalable distributed applications in multi-core processors. Unfortunately, the mismatch between message passing models and today's shared-memory hardware provided by commercial vendors results in suboptimal performance and a waste of energy. This paper presents a set of architectural extensions to reduce the overheads incurred by message passing workloads running on shared memory multi-core architectures. It describes the instruction set extensions and the hardware implementation. In order to facilitate programmability, the proposed extensions are used by a message passing library, allowing programs to take advantage of them transparently. As a proof-of-concept, we use modified MPI libraries and unmodified MPI programs to evaluate the proposal. Experimental results show that a best-effort design can eliminate over 60% of cache accesses caused by message data transmission and reduce the cycles spent in such task by 75%, while the addition of a simple coprocessor can completely off-load data movement from the CPU to avoid up to 92% of cache accesses, and a reduction of 12% of network traffic on average. The design achieves an improvement of 11%–12% in the energy-delay product of on-chip caches. © 2016 Elsevier Inc.","Message passing; Multicore; Shared memory"
"All-Pairs Shortest Path algorithms for planar graph for GPU-accelerated clusters","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941994607&doi=10.1016%2fj.jpdc.2015.06.008&partnerID=40&md5=447f37555ccfc2695d6a8db4759ec39f","We present a new approach for solving the All-Pairs Shortest-Path (APSP) problem for planar graphs that exploits the massive on-chip parallelism available in today's Graphics Processing Units (GPUs). We describe two new algorithms based on our approach. Both algorithms use Floyd-Warshall method, have near optimal complexity in terms of the total number of operations, while their matrix-based structure is regular enough to allow for efficient parallel implementation on the GPUs. By applying a divide-and-conquer approach, we are able to make use of multi-node GPU clusters, resulting in more than an order of magnitude speedup over fastest known Dijkstra-based GPU implementation and a two-fold speedup over a parallel Dijkstra-based CPU implementation. © 2015 Elsevier Inc. All rights reserved.","Algorithm analysis; All-pairs shortest path problem; Distributed computing; Floyd-Warshall algorithm; GPGPU; Parallel computing; Planar graphs"
"Self-stabilizing (f, g) -alliances with safe convergence","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939997919&doi=10.1016%2fj.jpdc.2015.02.001&partnerID=40&md5=4e12c946b956bba1da70c521f22b5970","Given two functions f and g mapping nodes to non-negative integers, we give a silent self-stabilizing algorithm that computes a minimal (f,g)-alliance in an asynchronous network with unique node IDs, assuming that every node p has a degree at least g(p) and satisfies f(p)≥g(p). Our algorithm is safely converging in the sense that starting from any configuration, it first converges to a (not necessarily minimal) (f,g)-alliance in at most four rounds, and then continues to converge to a minimal one in at most 5n+4 additional rounds, where n is the size of the network. Our algorithm is written in the shared memory model. It is proven assuming an unfair (distributed) daemon. Its memory requirement is Θ(logn) bits per process, and it takes O(nΔ3) steps to stabilize, where Δ is the degree of the network. © 2015 Elsevier Inc.","MSC 68W15 68M15"
"Transformer: Run-time reprogrammable heterogeneous architecture for transparent acceleration of dynamic workloads","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940924050&doi=10.1016%2fj.jpdc.2015.08.002&partnerID=40&md5=ef57d06699f3545a20fd7025f70875bd","Heterogeneous architectures face challenges regarding transparent acceleration as well as the allocation of resources to cores and accelerators. The ""Transformer"", a run-time reprogrammable, heterogeneous architecture consisting of cores and reconfigurable logic with support for coarse-grained acceleration of the dynamic, unpredictable workloads present in mobile and cloud computing environments, is proposed as a solution. The architecture allows for the run-time instantiation of one or more acceleration functions, present in an on-chip reconfigurable logic, which responds to the demands of compute-intensive software libraries. The hardware controller and software wrapper functions are designed to profile workloads, reprogram the internal logic, and invoke the appropriate acceleration functions. Novel heuristics are derived with respect to the accelerator function scheduling. In order to optimize performance and power efficiency, the appropriate system parameters are explored, including the L1 and L2 cache sizes, the accelerator local buffer sizes, as well as the allocation of resources to the cores and accelerators. The simulation results indicate that the Transformer provides significant improvements in terms of performance, up to 14× for single-type workloads and 2.3× for dynamic workloads, as well as energy efficiency, up to 6.9× for various workloads. © 2015 Elsevier Inc.","Accelerator; Coarse-grain; FPGA; Heterogeneous architecture; Transparent acceleration"
"Decentralised dispatch of distributed energy resources in smart grids via multi-agent coalition formation","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930933408&doi=10.1016%2fj.jpdc.2015.04.004&partnerID=40&md5=bc9d127a11050e2ca55749b95b9fc04d","The energy dispatch problem is a fundamental research issue in power distribution networks. With the growing complexity and dimensions of current distribution networks, there is an increasing need for intelligent and scalable mechanisms to facilitate energy dispatch in these networks. To this end, in this paper, we propose a multi-agent coalition formation-based energy dispatch mechanism. This mechanism is decentralised without requiring a central controller or any global information. As this mechanism does not need a central controller, the single point of failure can be avoided and since this mechanism does not require any global information, good scalability can be expected. In addition, this mechanism enables each node in a distribution network to make decisions autonomously about energy dispatch through a negotiation protocol. Simulation results demonstrate the effectiveness of this mechanism in comparison with three recently developed representative mechanisms. © 2015 Elsevier Inc. All rights reserved.","Coalition formation; Distributed energy dispatch; Multi-agent systems; Smart grids"
"Distributed travel-time seismic tomography in large-scale sensor networks","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953791016&doi=10.1016%2fj.jpdc.2015.12.002&partnerID=40&md5=62972b8327b4a7ac2e6174e67a6b0779","Current geophysical techniques for visualizing seismic activity employ image reconstruction methods that rely on a centralized approach for processing the raw data captured by seismic sensors. The data is either gathered manually, or relayed by expensive broadband stations, and then processed at a base station. This approach is time-consuming (weeks to months) and hazardous as the task involves manual data gathering in extreme conditions. Also, raw seismic samples are typically in the range of 16-24 bit, sampled at 50-200 Hz and transferring this high fidelity sample from large number of sensors to a centralized station results in a bottleneck due to bandwidth limitations. To avoid these issues, a new distributed method is required which processes raw seismic samples inside each node and obtains a high-resolution seismic tomography in real time. In this paper, we present a component-averaged distributed multi-resolution evolving tomography algorithm for processing data and inverting volcano tomography in the network while avoiding centralized computation and costly data collection. The algorithm is first evaluated for the correctness using a synthetic model in a CORE emulator. Later, our proposed algorithm runs using the real data obtained from Mt. St. Helens, WA, USA. The results validate that our distributed algorithm is able to obtain a satisfactory image similar to centralized computation under constraints of network resources, while distributing the computational burden to sensor nodes. © 2015 Elsevier Inc.","Distributed computing; Mt. St. Helens; Seismic tomography; Sensor network"
"Towards efficient resource provisioning in MapReduce","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994644641&doi=10.1016%2fj.jpdc.2016.04.001&partnerID=40&md5=456bf4b541915a8b0edff60e743a8b06","The paper presents a novel approach and algorithm with mathematical formula for obtaining the exact optimal number of task resources for any workload running on Hadoop MapReduce. In the era of Big Data, energy efficiency has become an important issue for the ubiquitous Hadoop MapReduce framework. However, the question of what is the optimal number of tasks required for a job to get the most efficient performance from MapReduce still has no definite answer. Our algorithm for optimal resource provisioning allows users to identify the best trade-off point between performance and energy efficiency on the runtime elbow curve fitted from sampled executions on the target cluster for subsequent behavioral replication. Our verification and comparison show that the currently well-known rules of thumb for calculating the required number of reduce tasks are inaccurate and could lead to significant waste of computing resources and energy with no further improvement in execution time. © 2016 The Authors","Energy efficiency; Hadoop MapReduce; Optimal resource provisioning; Runtime elbow curve; Spark; YARN"
"EM-KDE: A locality-aware job scheduling policy with distributed semantic caches","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934963746&doi=10.1016%2fj.jpdc.2015.06.002&partnerID=40&md5=7a00fd4702733edcc2a12961f3ec3097","In modern query processing systems, the caching facilities are distributed and scale with the number of servers. To maximize the overall system throughput, the distributed system should balance the query loads among servers and also leverage cached results. In particular, leveraging distributed cached data is becoming more important as many systems are being built by connecting many small heterogeneous machines rather than relying on a few high-performance workstations. Although many query scheduling policies exist such as round-robin and load-monitoring, they are not sophisticated enough to both balance the load and leverage cached results. In this paper, we propose distributed query scheduling policies that take into account the dynamic contents of distributed caching infrastructure and employ statistical prediction methods into query scheduling policy. We employ the kernel density estimation derived from recent queries and the well-known exponential moving average (EMA) in order to predict the query distribution in a multi-dimensional problem space that dynamically changes. Based on the estimated query distribution, the front-end scheduler assigns incoming queries so that query workloads are balanced and cached results are reused. Our experiments show that the proposed query scheduling policy outperforms existing policies in terms of both load balancing and cache hit ratio. © 2015 Elsevier Inc.","Distributed scheduling; Distributed semantic cache; Locality-aware scheduling; Parallel multi-dimensional range query"
"Read/write shared memory abstraction on top of asynchronous Byzantine message-passing systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964452130&doi=10.1016%2fj.jpdc.2016.03.012&partnerID=40&md5=8e20fb266a8293a05d93e84b84cbaf30","This paper is on the construction and use of a shared memory abstraction on top of an asynchronous message-passing system in which up to t processes may commit Byzantine failures. This abstraction consists of arrays of n single-writer/multi-reader atomic registers, where n is the number of processes. These registers enable Byzantine tolerance by recording the whole history of values written to each one of them. A distributed algorithm building such a shared memory abstraction is first presented. This algorithm assumes t<n/3, which is shown to be a necessary and sufficient condition for such a construction. Hence, the algorithm is resilient-optimal. Then the paper presents distributed objects built on top of this read/write shared memory abstraction, which cope with Byzantine processes. As illustrated by these objects, the proposed shared memory abstraction is motivated by the fact that, for a lot of problems, algorithms are simpler to design and prove correct in a shared memory system than in a message-passing system. © 2016 Elsevier Inc. All rights reserved.","Approximate agreement; Asynchronous message-passing system; Atomic read/write register; Broadcast abstraction; Byzantine process; Distributed computing; Message-passing system; Quorum; Reliable broadcast; Reliable shared memory; Single-writer/multi-reader register; t-Resilience"
"EFS: Energy-Friendly Scheduler for memory bandwidth constrained systems","2016","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2016.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994602808&doi=10.1016%2fj.jpdc.2016.03.007&partnerID=40&md5=9822f31ee02d3d0bf2306ceff543877a","Additional transistors available in each process generation are used to increase the number of cores on chip. This trend results in high execution unit performance relative to other available resources, such as memory bandwidth, I/O bandwidth, and power. Consequently, the performance bottleneck in modern systems has shifted from the execution units to other resources. In this paper we propose a dynamic scheduling scheme that avoids bottlenecks and thus saves energy. Current operating system schedulers are designed to always assign threads to available cores. We show that this approach may result in excessive loads on other resources, which can ultimately hamper performance and waste energy. Thus, perhaps paradoxically, in some cases it may be advantageous to under-utilize on-chip computing resources in order to achieve better performance and energy efficiency. More generally, we argue that operating system schedulers should consider multiple resources, such as memory bandwidth, dynamic cache conflicts, and I/O bandwidth. We develop this concept in the context of memory bandwidth, which is a critical bottleneck in many systems. To this end, we suggest a model that predicts threads’ throughput and power consumption based on contention on the memory bus. We use this model to design EFS (Energy-Friendly Scheduler), a new energy-efficient scheduler, which schedules new threads only when the benefit of the added throughput outweighs the cost of powering up additional cores. The idea is simple, and we implement it in Linux using performance monitors readily available in current microprocessors. Execution results on a real multicore system with EFS show up to 32% energy reductions in resource-constrained SPEC-CPU2006 benchmarks, as measured using an external power meter. © 2016 Elsevier Inc.","Energy efficiency; Energy Friendly Scheduler; Multicores; Performance monitors; Scheduling"
"A load balanced directory for distributed shared memory objects","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939979717&doi=10.1016%2fj.jpdc.2015.02.002&partnerID=40&md5=e5d9c1951bb4063ec2416f741e2eacdf","We present MultiBend, a novel distributed directory protocol for shared objects, suitable for large-scale distributed shared memory systems that use d-dimensional mesh-based topologies, where d≥2. Each shared object has an owner node that can modify its value. The ownership may change by moving the object from one node to another in response to move requests. The value of an object can be read by other nodes with lookup requests. MultiBend balances the load of the network edges and nodes by forwarding each move or lookup request and response along a path consisting of multiple bends in the mesh. Using an oblivious routing protocol, the multi-bend paths have a small number of overlaps which helps to reduce the maximum edge and node utilization to achieve load balancing. At the same time, MultiBend achieves small stretch for the total path length of any sequence of move requests, compared to the total optimal path length. MultiBend guarantees O(d2logn) approximation for the load, and O(dlogn) approximation for the stretch due to move requests, where n is the number of nodes in the mesh network. It also guarantees O(d2) approximation for the stretch of lookup requests. We evaluate MultiBend with simulations using various sequences of move and lookup operations in a 16×16 nodes 2-dimensional mesh network. We compare the simulation results to other protocols which are not tailored for load balancing and we find that our protocol is better by as much as the factor of 6.85 in terms of congestion in the worst-case. To the best of our knowledge, this is the first distributed shared memory directory protocol that considers the network load balancing aspect and achieves good approximation ratio for both the load and the stretch. © 2015 Elsevier Inc.","Cache-coherence; Distributed directory; Distributed systems; Load balancing; Mesh network; Oblivious routing; Shared object; Stretch"
"On the energy efficiency and performance of irregular application executions on multicore, NUMA and manycore platforms","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924246803&doi=10.1016%2fj.jpdc.2014.11.002&partnerID=40&md5=dd313d622b6d033bc1a13b628329abf2","Until the last decade, performance of HPC architectures has been almost exclusively quantified by their processing power. However, energy efficiency is being recently considered as important as raw performance and has become a critical aspect to the development of scalable systems. These strict energy constraints guided the development of a new class of so-called light-weight manycore processors. This study evaluates the computing and energy performance of two well-known irregular NP-hard problems-the Traveling-Salesman Problem (TSP) and K-Means clustering-and a numerical seismic wave propagation simulation kernel-Ondes3D-on multicore, NUMA, and manycore platforms. First, we concentrate on the nontrivial task of adapting these applications to a manycore, specifically the novel MPPA-256 manycore processor. Then, we analyze their performance and energy consumption on those different machines. Our results show that applications able to fully use the resources of a manycore can have better performance and may consume from 3.8 × to 13 × less energy when compared to low-power and general-purpose multicore processors, respectively. © 2014 Elsevier Inc. All rights reserved.","Energy efficiency; K-Means; Manycore; Multicore; NUMA; Performance; Seismic wave propagation; TSP"
"Kokkos: Enabling manycore performance portability through polymorphic memory access patterns","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910092965&doi=10.1016%2fj.jpdc.2014.07.003&partnerID=40&md5=c6965f97236efbf9c14af08d5f4ddeaa","The manycore revolution can be characterized by increasing thread counts, decreasing memory per thread, and diversity of continually evolving manycore architectures. High performance computing (HPC) applications and libraries must exploit increasingly finer levels of parallelism within their codes to sustain scalability on these devices. A major obstacle to performance portability is the diverse and conflicting set of constraints on memory access patterns across devices. Contemporary portable programming models address manycore parallelism (e.g., OpenMP, OpenACC, OpenCL) but fail to address memory access patterns. The Kokkos C++ library enables applications and domain libraries to achieve performance portability on diverse manycore architectures by unifying abstractions for both fine-grain data parallelism and memory access patterns. In this paper we describe Kokkos' abstractions, summarize its application programmer interface (API), present performance results for unit-test kernels and mini-applications, and outline an incremental strategy for migrating legacy C++ codes to Kokkos. The Kokkos library is under active research and development to incorporate capabilities from new generations of manycore architectures, and to address a growing list of applications and domain libraries. © 2014 Elsevier Inc.","GPU; Manycore; Mini-application; Multidimensional array; Parallel computing; Performance portability; Thread parallelism"
"A multi-channel cooperative MIMO MAC protocol for clustered wireless sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028219739&doi=10.1016%2fj.jpdc.2014.07.012&partnerID=40&md5=5b2598e5417db7a9f7b4ab5087c59b5f","Recently, several multi-channel MAC protocols have been proposed for wireless sensor networks (WSNs) to improve network capacity and boost energy efficiency. In addition, cooperative multiple-input multiple-output (MIMO) technique has been shown to be able to significantly enhance the energy efficiency of WSNs if properly configured. However, these two promising techniques have not been jointly considered in clustered WSNs. In this paper, we explore such a joint design by proposing a novel MAC protocol for clustered WSNs that takes advantage of both multiple channels and cooperative MIMO to further boost network performance. Specifically, sensor nodes in a WSN are organized into clusters and each cluster head selects some cooperative nodes to help forward traffic to or receive from other clusters by utilizing a cooperative MIMO technique. For intra-cluster communications, different channels are assigned to adjacent clusters to reduce collisions, while for inter-cluster communications, cooperative MIMO links are scheduled to improve energy efficiency and concurrent transmissions are enabled by assigning different channels to them. We also provide a network-wide time synchronization approach to facilitating the functionality of this protocol. We carry out extensive simulations and the results demonstrate that the proposed protocol can significantly increase throughput and improve energy efficiency compared to other schemes. For example, for a WSN with 150 sensors in a 250×250m2 field, with five available channels, the protocol can achieve three times saturated throughput while saving 14% energy per bit for inter-cluster communications. © 2015 Elsevier B.V. All rights reserved.","Channel assignment; Cooperative multiple input multiple output (MIMO); Medium access control (MAC); Wireless sensor networks (WSNs)"
"Pilot-Data: An abstraction for distributed data","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930932802&doi=10.1016%2fj.jpdc.2014.09.009&partnerID=40&md5=064c5f68c4eeda1fb93d4dfc9ca8a4b7","Scientific problems that depend on processing large amounts of data require overcoming challenges in multiple areas: managing large-scale data distribution, controlling co-placement and scheduling of data with compute resources, and storing, transferring, and managing large volumes of data. Although there exist multiple approaches to address each of these challenges and the complexity of distributed environments, an integrative approach is missing; furthermore, extending existing functionality or enabling interoperable capabilities remains difficult at best. We propose the concept of Pilot-Data to address the fundamental challenges of co-placement and scheduling of data and compute in heterogeneous and distributed environments with interoperability and extensibility as first-order concerns. Pilot-Data is an extension of the Pilot-Job abstraction for supporting the management of data in conjunction with compute tasks. Pilot-Data separates logical data units from physical storage, thereby providing the basis for efficient compute/data placement and scheduling. In this paper, we discuss the design and implementation of the Pilot-Data prototype, demonstrate its use by data-intensive applications on multiple production distributed cyberinfrastructure and illustrate the advantages arising from flexible execution modes enabled by Pilot-Data. Our experiments utilize an implementation of Pilot-Data in conjunction with a scalable Pilot-Job (BigJob) to establish the application performance that can be enabled by the use of Pilot-Data. We demonstrate how the concept of Pilot-Data also provides the basis upon which to build tools and support capabilities like affinity which in turn can be used for advanced data-compute co-placement and scheduling. © 2014 Elsevier Inc. All rights reserved.","Bigdata; Cloud; Data-intensive; Distributed computing; Grid; HPC; HTC; Pilot-jobs"
"Accelerating elliptic curve scalar multiplication over GF (2 m) on graphic hardwares","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918831559&doi=10.1016%2fj.jpdc.2014.09.001&partnerID=40&md5=962f25967c3f7a5a069768a24ea1dcd3","In this paper, we present PEG (Parallel ECC library on GPU), which is efficient implementation of Elliptic Curve Scalar Multiplication over GF(2m) on Graphic Processing Units. While existing ECC implementations over GPU focused on limited parameterizations such as (fixed scalar and different curves) or (different scalars and same base point), PEG covers all parameter options ((a) fixed scalar and variable points, (b) random scalars and fixed input point, and (c) random scalars and variable points) which are used for ECC-based protocols such as ECDH, ECDSA and ECIES. With GPU optimization concerns and through analyzing parameter types used for ECC-based protocols, we investigate promising algorithms at both finite field arithmetic and scalar multiplication level for performance optimization according to each parameterization. PEG covers ECC implementations over GF(2163), GF(2233) and GF(2283) for 80-bit, 112-bit and 128-bit security on GTX285 and GTX480. PEG can achieve remarkable performance compared with MIRACL, one of the most famous ECC library, running on Intel i7 CPU (2.67 GHz). © 2014 Elsevier Inc. All rights reserved.","CUDA; Elliptic Curve Cryptosystem (ECC); Graphic Processing Units (GPUs); Parallel cryptographic computation"
"Versatile, scalable, and accurate simulation of distributed applications and platforms","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905594033&doi=10.1016%2fj.jpdc.2014.06.008&partnerID=40&md5=3a6fad2956d8ae16c000af98faf35b97","The study of parallel and distributed applications and platforms, whether in the cluster, grid, peer-to-peer, volunteer, or cloud computing domain, often mandates empirical evaluation of proposed algorithmic and system solutions via simulation. Unlike direct experimentation via an application deployment on a real-world testbed, simulation enables fully repeatable and configurable experiments for arbitrary hypothetical scenarios. Two key concerns are accuracy (so that simulation results are scientifically sound) and scalability (so that simulation experiments can be fast and memory-efficient). While the scalability of a simulator is easily measured, the accuracy of many state-of-the-art simulators is largely unknown because they have not been sufficiently validated. In this work we describe recent accuracy and scalability advances made in the context of the SimGrid simulation framework. A design goal of SimGrid is that it should be versatile, i.e., applicable across all aforementioned domains. We present quantitative results that show that SimGrid compares favorably with state-of-the-art domain-specific simulators in terms of scalability, accuracy, or the trade-off between the two. An important implication is that, contrary to popular wisdom, striving for versatility in a simulator is not an impediment but instead is conducive to improving both accuracy and scalability. © 2014 Elsevier Inc. All rights reserved.","Scalability; SimGrid; Simulation; Validation; Versatility"
"A work stealing based approach for enabling scalable optimal sequence homology detection","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930383919&doi=10.1016%2fj.jpdc.2014.08.009&partnerID=40&md5=680c2f8cf8233834614391ecdb72d3f8","Sequence homology detection is central to a number of bioinformatics applications including genome sequencing and protein family characterization. Given millions of sequences, the goal is to identify all pairs of sequences that are highly similar (or ""homologous"") on the basis of alignment criteria. While there are optimal alignment algorithms to compute pairwise homology, their deployment for large-scale is currently not feasible; instead, heuristic methods are used at the expense of quality. Here, we present the design and evaluation of a parallel implementation for conducting optimal homology detection on distributed memory supercomputers. Our approach uses a combination of techniques from asynchronous load balancing (viz. work stealing, dynamic task counters), data replication, and exact-matching filters to achieve homology detection at scale. Results for 2.56 M sequences on up to 8K cores show parallel efficiencies of ˜75%-100%, a time-to-solution of 33 s, and a rate of ˜2.0M alignments per second. © 2014 Elsevier Inc.","Distributed task counters; Dynamic load balancing; Homology detection; Pairwise sequence alignment; Parallel suffix tree construction; Protein family identification; Work stealing"
"Neighborhood grid: A novel data structure for fluids animation with GPU computing","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918818908&doi=10.1016%2fj.jpdc.2014.10.009&partnerID=40&md5=53e19668f9d96cce5afc4f0c36b63b7d","This paper introduces a novel and efficient data structure, called neighborhood grid, capable of supporting large number of particle based elements on GPUs (graphics processing units), and is used for optimizing fluid animation with the use of GPU computing. The presented fluid simulation approach is based on SPH (smoothed particle hydrodynamics) and uses a unique algorithm for the neighborhood gathering. The brute force approach to neighborhood gathering of n particles has complexity O(n2), since it involves proximity queries of all pairs of fluid particles in order to compute the relevant mutual interactions. Usually, the algorithm is optimized by using spatial data structures which subdivide the environment in cells and then classify the particles among the cells based on their position, which is not efficient when a large number of particles are grouped in the same cell. Instead of using such approach, this work presents a novel and efficient data structure that maintains the particles into another form of proximity data structure, called neighborhood grid. In this structure, each cell contains only one particle and does not directly represent a discrete spatial subdivision. The neighborhood grid does process an approximate spatial neighborhood of the particles, yielding promising results for real time fluid animation, with results that goes up to 9 times speedup, when compared to traditional GPU approaches, and up to 100 times when compared against CPU implementations. © 2014 Elsevier Inc. All rights reserved.","Data structure; Fluid animation; Fluid simulation; GPGPU; GPU computing; Real-time simulation"
"A general framework for dynamic and automatic I/O scheduling in hard and solid-state drives","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894819055&doi=10.1016%2fj.jpdc.2014.02.002&partnerID=40&md5=40e7def137bfbcb3d33304a5429766d6","The selection of the right I/O scheduler for a given workload can significantly improve the I/O performance. However, this is not an easy task because several factors should be considered, and even the ""best"" scheduler can change over the time, specially if the workload's characteristics change too. To address this problem, we present a Dynamic and Automatic Disk Scheduling framework (DADS) that simultaneously compares two different Linux I/O schedulers, and dynamically selects that which achieves the best I/O performance for any workload at any time. The comparison is made by running two instances of a disk simulator inside the Linux kernel. Results show that, by using DADS, the performance achieved is always close to that obtained by the best scheduler. Thus, system administrators are exempted from selecting a suboptimal scheduler which can provide a good performance for some workloads, but may downgrade the system throughput when the workloads change. © 2014 Elsevier B.V. All rights reserved.","DADS; Disk modeling; I/O scheduler; Simultaneous evaluation; Virtual disk"
"Performance metrics in a hybrid MPI-OpenMP based molecular dynamics simulation with short-range interactions","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891770892&doi=10.1016%2fj.jpdc.2013.12.008&partnerID=40&md5=29a2c061dd24e58244ade11b894f0b5d","We discuss the computational bottlenecks in molecular dynamics (MD) and describe the challenges in parallelizing the computation-intensive tasks. We present a hybrid algorithm using MPI (Message Passing Interface) with OpenMP threads for parallelizing a generalized MD computation scheme for systems with short range interatomic interactions. The algorithm is discussed in the context of nano-indentation of Chromium films with carbon indenters using the Embedded Atom Method potential for Cr-Cr interaction and the Morse potential for Cr-C interactions. We study the performance of our algorithm for a range of MPI-thread combinations and find the performance to depend strongly on the computational task and load sharing in the multi-core processor. The algorithm scaled poorly with MPI and our hybrid schemes were observed to outperform the pure message passing scheme, despite utilizing the same number of processors or cores in the cluster. Speed-up achieved by our algorithm compared favorably with that achieved by standard MD packages.","Hybrid programming; Message passing; Molecular dynamics; OpenMP threading; Parallel computing"
"Flow updating: Fault-tolerant aggregation for dynamic networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940002040&doi=10.1016%2fj.jpdc.2015.02.003&partnerID=40&md5=5646d7f642c65d358fde4a6d9b1d9bc1","Data aggregation is a fundamental building block of modern distributed systems. Averaging based approaches, commonly designated gossip-based, are an important class of aggregation algorithms as they allow all nodes to produce a result, converge to any required accuracy, and work independently from the network topology. However, existing approaches exhibit many dependability issues when used in faulty and dynamic environments. This paper describes and evaluates a fault tolerant distributed aggregation technique, Flow Updating, which overcomes the problems in previous averaging approaches and is able to operate on faulty dynamic networks. Experimental results show that this novel approach outperforms previous averaging algorithms; it self-adapts to churn and input value changes without requiring any periodic restart, supporting node crashes and high levels of message loss, and works in asynchronous networks. Realistic concerns have been taken into account in evaluating Flow Updating, like the use of unreliable failure detectors and asynchrony, targeting its application to realistic environments. © 2015 Elsevier Inc.","Data aggregation; Distributed algorithms; Dynamic networks; Fault-tolerance; In-network aggregation"
"Parallel performance modeling of irregular applications in cell-centered finite volume methods over unstructured tetrahedral meshes","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027925620&doi=10.1016%2fj.jpdc.2014.10.005&partnerID=40&md5=e760fc52d10ee06ce93ffe549e4c6d39","Finite volume methods are widely used numerical strategies for solving partial differential equations. This paper aims at obtaining a quantitative understanding of the achievable performance of the cell-centered finite volume method on 3D unstructured tetrahedral meshes, using traditional multicore CPUs as well as modern GPUs. By using an optimized implementation and a synthetic connectivity matrix that exhibits a perfect structure of equal-sized blocks lying on the main diagonal, we can closely relate the achievable computing performance to the size of these diagonal blocks. Moreover, we have derived a theoretical model for identifying characteristic levels of the attainable performance as a function of hardware parameters, based on which a realistic upper limit of the performance can be predicted accurately. For real-world tetrahedral meshes, the key to high performance lies in a reordering of the tetrahedra, such that the resulting connectivity matrix resembles a block diagonal form where the optimal size of the blocks depends on the hardware. Numerical experiments confirm that the achieved performance is close to the practically attainable maximum and it reaches 75% of the theoretical upper limit, independent of the actual tetrahedral mesh considered. From this, we develop a general model capable of identifying bottleneck performance of a system's memory hierarchy in irregular applications. © 2014 Elsevier Inc. All rights reserved.","CUDA programming; Finite volume method; Multicore; Nvidia K20 GPU; OpenMP; Performance modeling; Unstructured tetrahedral mesh"
"Solutions to the st-connectivity problem using a GPU-based distributed BFS","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924264587&doi=10.1016%2fj.jpdc.2014.09.013&partnerID=40&md5=546c083b019ddf2a25431897f328ecf9","The st-connectivity problem (ST-CON) is a decision problem that asks, for vertices s and t in a graph, if t is reachable from s. Although originally defined for directed graphs, it can also be studied on undirected graphs and used as a building block for solving more complex tasks on large scale graphs. We present solutions to ST-CON based on a high performance Breadth First Search (BFS) executed on clusters of Graphics Processing Units (GPUs) using the Nvidia CUDA platform. To measure performances, we use the number of ST-CONs per second. We present the results for two different implementations that highlight the impact of atomic operations in CUDA. © 2014 Elsevier Inc. All rights reserved.","CUDA; Distributed algorithms; GPU; Graph algorithms; Large graphs"
"A bit-parallel algorithm for searching multiple patterns with various lengths","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924244756&doi=10.1016%2fj.jpdc.2014.11.003&partnerID=40&md5=9b389f775df58ec645486880ecd2ae3a","In this paper, we present an Advanced Vector Extensions (AVX) accelerated method for a bit-parallel algorithm that realizes fast string search for maximizing stable search throughput. An advantage of our method is that it accelerates string search by regularizing both control flow and data structures. This regularization facilitates the exploitation of the latest vector instruction set to achieve efficient parallel search of multiple patterns of different lengths. We use AVX instructions to increase search throughput per CPU core and employ OpenMP directives to realize data-parallel search of strings. As a result, we found that our data structure doubled search throughput as compared with a previous bit-parallel approach that used a data structure for patterns of the same length. We also found that our method achieved stable search throughput for arbitrary data if the pattern size is large, but small enough to fit into a word. Some experimental results are provided to understand the advantage and disadvantage of our method with a comparison to Aho-Corasick based methods. We believe that our method is useful for large genome texts with many partial matches. © 2014 Elsevier Inc. All rights reserved.","Acceleration; AVX; Bit-parallel algorithm; String search"
"Solving the resource constrained project scheduling problem using the parallel Tabu Search designed for the CUDA platform","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027927889&doi=10.1016%2fj.jpdc.2014.11.005&partnerID=40&md5=60262ba3f8e7100d416aee0bf32c67eb","The Resource Constrained Project Scheduling Problem, which is considered to be difficult to tackle even for small instances, is a well-known scheduling problem in the operations research domain. To solve the problem we have proposed a parallel Tabu Search algorithm to find high quality solutions in a reasonable time. We show that our parallel Tabu Search algorithm for graphics cards (GPUs) outperforms other existing Tabu Search approaches in terms of quality of solutions and the number of evaluated schedules per second. Moreover, the algorithm for graphics cards is about 10.5/42.7 times faster (J90 benchmark instances) than the optimized parallel/sequential algorithm for the Central Processing Unit (CPU). The same quality of solutions is achieved up to 5.4/22 times faster in comparison to the parallel/sequential CPU algorithm respectively. The advantages of the GPU version arise from the sophisticated data-structures and their suitable placement in the device memory, tailor-made methods, and last but not least the effective communication scheme. © 2014 Elsevier Inc. All rights reserved.","CUDA; GPU; Homogeneous model; Parallel Tabu Search; Resource Constrained Project Scheduling Problem"
"A model-driven blocking strategy for load balanced sparse matrix-vector multiplication on GPUs","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924269359&doi=10.1016%2fj.jpdc.2014.11.001&partnerID=40&md5=784016071d932a478c6f63bab2bd85db","Sparse Matrix-Vector multiplication (SpMV) is one of the key operations in linear algebra. Overcoming thread divergence, load imbalance and un-coalesced and indirect memory access due to sparsity and irregularity are challenges to optimizing SpMV on GPUs. In this paper we present a new Blocked Row-Column (BRC) storage format with a two-dimensional blocking mechanism that addresses these challenges effectively. It reduces thread divergence by reordering and blocking rows of the input matrix with nearly equal number of non-zero elements onto the same execution units (i.e., warps). BRC improves load balance by partitioning rows into blocks with a constant number of non-zeros such that different warps perform the same amount of work. We also present an approach to optimize BRC performance by judicious selection of block size based on sparsity characteristics of the matrix. A CUDA implementation of BRC outperforms NVIDIA CUSP and cuSPARSE libraries and other state-of-the-art SpMV formats on a range of unstructured sparse matrices from multiple application domains. The BRC format has been integrated with PETSc, enabling its use in PETSc's solvers. Furthermore, when partitioning the input matrix, BRC achieves near linear speedup on multiple GPUs. © 2014 Elsevier Inc. All rights reserved.","CUDA; GPU; SpMV"
"Mutual inclusion in asynchronous message-passing distributed systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027930416&doi=10.1016%2fj.jpdc.2015.01.003&partnerID=40&md5=e60f7b0a703f6f0da9235a2b7ee42ba8","In the mutual inclusion problem, at least one process is in the critical section. However, only a solution for two processes with semaphores has been reported previously. In this study, a generalized problem setting is formalized and two distributed solutions are proposed based on an asynchronous message-passing model. In the local problem setting (the local mutual inclusion problem), for each process P, at least one of P and its neighbors must be in the critical section. For the local problem setting, a solution is proposed with O(Δ) message complexity, where Δ is the maximum degree (number of neighboring processes) of a network. In a global setting (the global mutual inclusion problem), at least one of the processes must be in the critical section. For the global problem setting, a solution is proposed with O(|Q|) message complexity, where |Q| is the maximum size for the quorum of a coterie used by the algorithm, which is typically |Q| = √n, where n is the number of processes in a network. © 2015 Elsevier Inc. All rights reserved.","Distributed algorithm; Mutual exclusion mutual inclusion; Process synchronization"
"Performance and energy efficiency of big data applications in cloud environments: A Hadoop case study","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930383808&doi=10.1016%2fj.jpdc.2015.01.001&partnerID=40&md5=551dee0938fcaf0135af2f7c01fe5cb6","The exponential growth of scientific and business data has resulted in the evolution of the cloud computing environments and the MapReduce parallel programming model. The focus of cloud computing is increased utilization and power savings through consolidation while MapReduce enables large scale data analysis. Hadoop, an open source implementation of MapReduce has gained popularity in the last few years. In this paper, we evaluate Hadoop performance in both the traditional model of collocated data and compute services as well as consider the impact of separating out the services. The separation of data and compute services provides more flexibility in environments where data locality might not have a considerable impact such as virtualized environments and clusters with advanced networks. In this paper, we also conduct an energy efficiency evaluation of Hadoop on physical and virtual clusters in different configurations. Our extensive evaluation shows that: (1) coexisting virtual machines on servers decrease the disk throughput; (2) performance on physical clusters is significantly better than on virtual clusters; (3) performance degradation due to separation of the services depends on the data to compute ratio; (4) application completion progress correlates with the power consumption and power consumption is heavily application specific. Finally, we present a discussion on the implications of using cloud environments for big data analyses. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Energy efficiency; Hadoop MapReduce; Performance; Virtualization"
"Dynamic thread mapping of shared memory applications by exploiting cache coherence protocols","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892738573&doi=10.1016%2fj.jpdc.2013.11.006&partnerID=40&md5=3078179942ca8cd5f7e67bdbdca8d8ff","In current computer architectures, the communication performance between threads varies depending on the memory hierarchy. This performance difference must be considered when mapping parallel applications to processor cores. In parallel applications based on the shared memory paradigm, the communication is difficult to detect because it is implicit. Furthermore, dynamic mapping introduces several challenges, since it needs to find a suitable mapping and migrate the threads with a low overhead during the execution of the application. We propose a mechanism to detect the communication pattern of shared memory applications by monitoring cache coherence protocols. We also propose heuristics that, combined with our communication detection mechanism, allow the mapping to be performed dynamically by the operating system. Experiments with the NAS Parallel Benchmarks showed a reduction of up to 13.9% of the execution time, 30.5% of the cache misses and 39.4% of the number of invalidation messages. © 2013 Elsevier Inc. All rights reserved.","Cache coherence protocols; Communication pattern; Parallel applications; Shared memory; Thread communication; Thread mapping"
"Parallel processing of filtered queries in attributed semantic graphs","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930377850&doi=10.1016%2fj.jpdc.2014.08.010&partnerID=40&md5=93711f1a72b43ddf746d76666b70eabc","Execution of complex analytic queries on massive semantic graphs is a challenging problem in big-data analytics that requires high-performance parallel computing. In a semantic graph, vertices and edges carry attributes of various types and the analytic queries typically depend on the values of these attributes. Thus, the computation must view the graph through a filter that passes only those individual vertices and edges of interest. Previous investigations have developed Knowledge Discovery Toolbox (KDT), a sophisticated Python library for parallel graph computations. In KDT, the user can write custom graph algorithms by specifying operations between edges and vertices (semiring operations). The user can also customize existing graph algorithms by writing filters. Although the high-level language for this customization enables domain scientists to productively express their graph analytics requirements, the customized queries perform poorly due to the overhead of having to call into the Python virtual machine for each vertex and edge. In this work, we use the Selective Embedded Just-In-Time Specialization (SEJITS) approach to automatically translate semiring operations and filters defined by programmers into a lower-level efficiency language, bypassing the upcall into Python. We evaluate our approach by comparing it with the high-performance Combinatorial BLAS engine and show that our approach combines the benefits of programming in a high-level language with executing in a low-level parallel environment. We increase the system's flexibility by developing techniques that provide users with the ability to define new vertex and edge types from Python. We also present a new Roofline model for graph traversals and show that we achieve performance that is significantly closer to the bounds suggested by the Roofline. Finally, to further understand the complex interaction with the underlying architecture, we present an analysis using performance counters that quantifies the improvement in hardware behavior in the context our SEJITS methodology. Overall, we demonstrate the first known solution to the problem of obtaining high performance from a productivity language when applying graph algorithms selectively on semantic graphs with hundreds of millions of edges and scaling to thousands of processors for graphs. © 2014 Elsevier Inc. All rights reserved.","Attributed semantic graphs; Domain-specific languages; Graph analysis systems; Graph filtering; High-performance graph analysis; Knowledge discovery; Parallel computing; SEJITS"
"Work efficient parallel algorithms for large graph exploration on emerging heterogeneous architectures","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027918508&doi=10.1016%2fj.jpdc.2014.11.006&partnerID=40&md5=ab9c5381719e1ca1f340e6e077c58db0","Graph algorithms play a prominent role in several fields of sciences and engineering. Notable among them are graph traversal, finding the connected components of a graph, and computing shortest paths. There are several efficient implementations of the above problems on a variety of modern multiprocessor architectures. It can be noticed in recent times that the size of the graphs that correspond to real world datasets has been increasing. Parallelism offers only a limited succor to this situation as current parallel architectures have severe short-comings when deployed for most graph algorithms. At the same time, these graphs are also getting very sparse in nature. This calls for particular solution strategies aimed at processing large, sparse graphs on modern parallel architectures. In this paper, we introduce graph pruning as a technique that aims to reduce the size of the graph. Certain elements of the graph can be pruned depending on the nature of the computation. Once a solution is obtained on the pruned graph, the solution is extended to the entire graph. Towards, this end we investigate pruning based on two strategies that justifies their use in current real world graphs. We apply the above technique on three fundamental graph algorithms: breadth first search (BFS), Connected Components (CC), and All Pairs Shortest Paths (APSP). For experimentations, we use three different sources for real world graphs. To validate our technique, we implement our algorithms on a heterogeneous platform consisting of a multicore CPU and a GPU. On this platform, we achieve an average of 35% improvement compared to state-of-the-art solutions. Such an improvement has the potential to speed up other applications reliant on these algorithms. © 2014 Elsevier Inc. All rights reserved.","Graph algorithms; Heterogeneous computing; Input pruning; Irregular computations"
"Online vector scheduling and generalized load balancing","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894634416&doi=10.1016%2fj.jpdc.2013.12.006&partnerID=40&md5=ac30bd14d99f547452f5b5d4b0d66ff3","We give a polynomial time reduction from the vector scheduling problem (VS) to the generalized load balancing problem (GLB). This reduction gives the first non-trivial online algorithm for VS where vectors come in an online fashion. The online algorithm is very simple in that each vector only needs to minimize the Lln(md) norm of the resulting load when it comes, where m is the number of partitions and d is the dimension of vectors. It has an approximation bound of elog(md), which is in O(ln(md)), so it also improves the O(ln 2d) bound of the existing polynomial time algorithm for VS. Additionally, the reduction shows that GLB does not have constant approximation algorithms that run in polynomial time unless P=NP. © 2013 Elsevier Inc. All rights reserved.","Approximation algorithm; Load balancing; Online algorithm; Vector scheduling"
"Block pivoting implementation of a symmetric Toeplitz solver","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896833529&doi=10.1016%2fj.jpdc.2014.02.003&partnerID=40&md5=2cb0d8b3c673b78d66cd6e9ae74f5fbc","Toeplitz matrices are characterized by a special structure that can be exploited in order to obtain fast linear system solvers. These solvers are difficult to parallelize due to their low computational cost and their closely coupled data operations. We propose to transform the Toeplitz system matrix into a Cauchy-like matrix since the latter can be divided into two independent matrices of half the size of the system matrix and each one of these smaller arising matrices can be factorized efficiently in multicore computers. We use OpenMP and store data in memory by blocks in consecutive positions yielding a simple and efficient algorithm. In addition, by exploiting the fact that diagonal pivoting does not destroy the special structure of Cauchy-like matrices, we introduce a local diagonal pivoting technique which improves the accuracy of the solution and the stability of the algorithm. © 2014 Elsevier Inc. All rights reserved.","Displacement structure; Linear systems; Multicores; Pivoting; Symmetric Toeplitz matrices"
"Self adaptable multithreaded object detection on embedded multicore systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928780157&doi=10.1016%2fj.jpdc.2015.01.005&partnerID=40&md5=fc9906f41d94f6042620606eb6a569a4","Leveraging multithreading on embedded multicore platforms has been proven effective on handling the increasing resolutions of target stimuli of object detection. However, complex tradeoffs and correlated design impacts between a parallel application and the underlying multicore platform necessitate an effective and adaptable multithreaded design. This paper introduces a hybrid multithreaded object detection with high parallelism and extensive data reuse. A self adaptable flow is proposed to adjust the multithreaded object detection to fully exploit various embedded multicore architectures. The ARM-based cycle accurate simulations of multicore systems have shown the superior performance returned by the proposed design. © 2015 Elsevier Inc.","Cache memories; Face and gesture recognition; Multiprocessor systems"
"Early experiences with live migration of SR-IOV enabled InfiniBand","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939995114&doi=10.1016%2fj.jpdc.2015.01.004&partnerID=40&md5=5d32905d7ab6457d94946528160cfe1f","Virtualization is the key to efficient resource utilization and elastic resource allocation in cloud computing. It enables consolidation, the on-demand provisioning of resources, and elasticity through live migration. Live migration makes it possible to optimize resource usage by moving virtual machines (VMs) between physical servers in an application transparent manner. It does, however, require a flexible, high-performance, scalable virtualized I/O architecture to reach its full potential. This is challenging to achieve with high-speed networks such as InfiniBand and remote direct memory access enhanced Ethernet, because these devices usually maintain their connection state in the network device hardware. Fortunately, the single root IO virtualization (SR-IOV) specification addresses the performance and scalability issues. With SR-IOV, each VM has direct access to a hardware assisted virtual device without the overhead introduced by emulation or para-virtualization. However, SR-IOV does not address the migration of the network device state. In this paper we present and evaluate the first available prototype implementation of live migration over SR-IOV enabled InfiniBand devices.","Architecture; IO virtualization; SR-IOV; VM migration"
"Dynamic replica placement and selection strategies in data grids - A comprehensive survey","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891588235&doi=10.1016%2fj.jpdc.2013.10.009&partnerID=40&md5=45bba0fd5237680d69c5be86b8073962","Data replication techniques are used in data grid to reduce makespan, storage consumption, access latency and network bandwidth. Data replication enhances data availability and thereby increases the system reliability. There are two steps involved in data replication, namely, replica placement and replica selection. Replica placement involves identifying the best possible node to duplicate data based on network latency and user request. Replica selection involves selecting the best replica location to access the data for job execution in the data grid. Various replica placement and selection algorithms are available in the literature. These algorithms measure and analyze different parameters such as bandwidth consumption, access cost, scalability, execution time, storage consumption and makespan. In this paper, various replica placement and selection strategies along with their merits and demerits are discussed. This paper also analyses the performance of various strategies with respect to the parameters mentioned above. In particular, this paper focuses on the dynamic replica placement and selection strategies in the data grid environment. © 2013 Elsevier Inc. All rights reserved.","Data grid, Computational grid; Dynamic replication; Replica placement; Replica selection"
"A new augmentation based algorithm for extracting maximal chordal subgraphs","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028158631&doi=10.1016%2fj.jpdc.2014.10.006&partnerID=40&md5=932ca8fee2e0285cfed5889d70af140c","A graph is chordal if every cycle of length greater than three contains an edge between non-adjacent vertices. Chordal graphs are of interest both theoretically, since they admit polynomial time solutions to a range of NP-hard graph problems, and practically, since they arise in many applications including sparse linear algebra, computer vision, and computational biology. A maximal chordal subgraph is a chordal subgraph that is not a proper subgraph of any other chordal subgraph. Existing algorithms for computing maximal chordal subgraphs depend on dynamically ordering the vertices, which is an inherently sequential process and therefore limits the algorithms' parallelizability. In this paper we explore techniques to develop a scalable parallel algorithm for extracting a maximal chordal subgraph. We demonstrate that an earlier attempt at developing a parallel algorithm may induce a non-optimal vertex ordering and is therefore not guaranteed to terminate with a maximal chordal subgraph. We then give a new algorithm that first computes and then repeatedly augments a spanning chordal subgraph. After proving that the algorithm terminates with a maximal chordal subgraph, we then demonstrate that this algorithm is more amenable to parallelization and that the parallel version also terminates with a maximal chordal subgraph. That said, the complexity of the new algorithm is higher than that of the previous parallel algorithm, although the earlier algorithm computes a chordal subgraph which is not guaranteed to be maximal. We experimented with our augmentation-based algorithm on both synthetic and real-world graphs. We provide scalability results and also explore the effect of different choices for the initial spanning chordal subgraph on both the running time and on the number of edges in the maximal chordal subgraph. © 2014 Elsevier Inc. All rights reserved.","Maximal chordal subgraphs; Parallel graph algorithms"
"Monitoring persistent items in the union of distributed streams","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028197086&doi=10.1016%2fj.jpdc.2014.07.008&partnerID=40&md5=8a235a6ecd66ebd5abf2bbae3ecfb80e","A persistent item in a stream is one that occurs regularly in the stream without necessarily contributing significantly to the volume of the stream. Persistent items are often associated with anomalies in network streams, such as botnet traffic and click fraud. While it is important to track persistent items in an online manner, it is challenging to zero-in on such items in a massive distributed stream. We present the first communication-efficient distributed algorithms for tracking persistent items in a data stream whose elements are partitioned across many different sites. We consider both infinite window and sliding window settings, and present algorithms that can track persistent items approximately with a probabilistic guarantee on the approximation error. Our algorithms have a provably low communication cost, and a low rate of false positives and false negatives, with a high probability. We present detailed results from an experimental evaluation that show the communication cost is small, and that the false positive and false negative rates are typically much lower than theoretical guarantees. © 2015 Elsevier B.V. All rights reserved.","Distributed streams; Persistent items"
"Fast parallel algorithms for graph similarity and matching","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896867335&doi=10.1016%2fj.jpdc.2013.12.010&partnerID=40&md5=a3289ef381cb9129fd93c2c6d4bafd5a","This paper addresses the problem of global graph alignment on supercomputer-class clusters. We define the alignment of two graphs, as a mapping of each vertex in the first graph to a unique vertex in the second graph so as to optimize a given similarity-based cost function.1 Using a state of the art serial algorithm for the computation of vertex similarity scores called Network Similarity Decomposition (NSD), we derive corresponding parallel formulations. Coupling this parallel similarity algorithm with a parallel auction-based bipartite matching technique, we obtain a highly efficient and scalable graph matching pipeline. We validate the performance of our integrated approach on a large parallel platform and on diverse graph instances (including Protein Interaction, Wikipedia and Web networks). Experimental results demonstrate that our algorithms scale to large machine configurations (thousands of cores) and problem instances, enabling the alignment of networks of sizes two orders of magnitude larger than reported in the current literature. © 2014 Elsevier Inc. All rights reserved.","Auction algorithm; Graph alignment; Parallel matching; Vertex similarity"
"A survey of support for structured communication in concurrency control models","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894614205&doi=10.1016%2fj.jpdc.2013.11.005&partnerID=40&md5=64ad54d8080448aab1d0010fb6a8ea6c","The two standard models used for communication in concurrent programs, shared memory and message passing, have been the focus of much debate for a long time. Still, we believe the main issue at stake should not be the choice between these models, but rather how to ensure that communication is structured, i.e., it occurs only in syntactically restricted code regions. In this survey, we explore concurrency control models and evaluate how their characteristics contribute positively or negatively to the support for structured communication. We focus the evaluation on three properties: reasonability, which is the main property we are interested in and determines how easily programmers can reason about a concurrent program's execution; performance, which determines whether there are any distinct features which can prevent or facilitate efficient implementations; and composability, which determines whether a model offers constructs that can be used as building blocks for coarser-grained, or higher-level, concurrency abstractions. © 2013 Elsevier Inc. All rights reserved.","Communication; Concurrency; Model; Structured; Survey"
"Towards a performance-portable description of geometric multigrid algorithms using a domain-specific language","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910027923&doi=10.1016%2fj.jpdc.2014.08.008&partnerID=40&md5=f85c8d887c63223969b6eb1cd618d8a1","High Performance Computing (HPC) systems are nowadays more and more heterogeneous. Different processor types can be found on a single node including accelerators such as Graphics Processing Units (GPUs). To cope with the challenge of programming such complex systems, this work presents a domain-specific approach to automatically generate code tailored to different processor types. Low-level CUDA and OpenCL code is generated from a high-level description of an algorithm specified in a Domain-Specific Language (DSL) instead of writing hand-tuned code for GPU accelerators. The DSL is part of the Heterogeneous Image Processing Acceleration (HIPAcc) framework and was extended in this work to handle grid hierarchies in order to model different cycle types. Language constructs are introduced to process and represent data at different resolutions. This allows to describe image processing algorithms that work on image pyramids as well as multigrid methods in the stencil domain. By decoupling the algorithm from its schedule, the proposed approach allows to generate efficient stencil code implementations. Our results show that similar performance compared to hand-tuned codes can be achieved. © 2014 Elsevier Inc. All rights reserved.","Code generation; CUDA; Domain-specific language; GPU; Image pyramid; Multigrid; Multiresolution; OpenCL; Stencil codes"
"Time-optimized contextual information forwarding in mobile sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894437111&doi=10.1016%2fj.jpdc.2014.01.008&partnerID=40&md5=74d2611a60178b22b73c10703b2930e5","We study on the forwarding of quality contextual information in mobile sensor networks (MSNs). Mobile nodes form ad-hoc distributed processing networks that produce accessible and quality-stamped information about the surrounding environment. Due to the dynamic network topology of such networks the context quality indicators seen by the nodes vary over time. A node delays the context forwarding decision until context of better quality is attained. Moreover, nodes have limited resources, thus, they have to balance between energy conservation and quality of context. We propose a time-optimized, distributed decision making model for forwarding context in a MSN based on the theory of optimal stopping. We compare our findings with certain context forwarding schemes found in the literature and pinpoint the advantages of the proposed model. © 2014 Elsevier Inc. All rights reserved.","Mobile sensor networks; Optimal stopping theory; Quality context forwarding"
"Fault-tolerant scheduling on parallel systems with non-memoryless failure distributions","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896852082&doi=10.1016%2fj.jpdc.2014.01.005&partnerID=40&md5=149a6eb324094f2f3f27810eb0f2dab1","As large parallel systems increase in size and complexity, failures are inevitable and exhibit complex space and time dynamics. Most often, in real systems, failure rates are increasing or decreasing over time. Considering non-memoryless failure distributions, we study a bi-objective scheduling problem of optimizing application makespan and reliability. In particular, we determine whether one can optimize both makespan and reliability simultaneously, or whether one metric must be degraded in order to improve the other. We also devise scheduling algorithms for achieving (approximately) optimal makespan or reliability. When failure rates decrease, we prove that makespan and reliability are opposing metrics. In contrast, when failure rates increase, we prove that one can optimize both makespan and reliability simultaneously. Moreover, we show that the largest processing time (LPT) list scheduling algorithm achieves good performance when processors are of uniform speed. The implications of our findings are the accelerated completion and improved reliability of parallel jobs executed across large distributed systems. Finally, we conduct simulations to investigate the impact of failures on the performance, which is done using an actual application of biological sequence comparison. © 2014 Elsevier Inc. All rights reserved.","Fault tolerance; Multi-objective optimization; Reliability; Scheduling"
"Improving reliability in resource management through adaptive reinforcement learning for distributed systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918777333&doi=10.1016%2fj.jpdc.2014.10.001&partnerID=40&md5=e2ed2bf5ba0cf6909310bf1a60bc2de9","Demands on capacity of distributed systems (e.g., Grid and Cloud) play a crucial role in today's information era due to the growing scale of the systems. While the distributed systems provide a vast amount of computing power their reliability is often hard to be guaranteed. This paper presents effective resource management using adaptive reinforcement learning (RL) that focuses on improving successful execution with low computational complexity. The approach uses an emerging methodology of RL in conjunction with neural network to help a scheduler that effectively observes and adapts to dynamic changes in execution environments. The observation of environment at various learning stages that normalize by resource-aware availability and feedback-based scheduling significantly brings the environments closer to the optimal solutions. Our approach also solves a high computational complexity in RL system through on-demand information sharing. Results from our extensive simulations demonstrate the effectiveness of adaptive RL for improving system reliability. © 2014 Elsevier Inc. All rights reserved.","Adaptive reinforcement learning; Computational complexity; Distributed systems; Resource management; System reliability"
"Constructing all shortest node-disjoint paths in torus networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918805760&doi=10.1016%2fj.jpdc.2014.09.004&partnerID=40&md5=e9ff86707cb7edcb40b68e994f3695f9","An n-dimensional torus network, also called wrap-around mesh or toroidal network, is a Cartesian product of n cycle networks. In particular, it was named k-ary n-cube when the sizes of the n cycle networks are all equal to k. In this paper, m node-disjoint shortest paths from one source node to other m (not necessarily distinct) destination nodes are constructed in an n-dimensional torus network, provided the existence of such node-disjoint shortest paths which can be verified in O(mn1.5) time, where m is not greater than the connectivity. The worst-case time and space complexities of the construction procedure are both optimal O(mn). In the situation that all of the source node and destination nodes are mutually distinct, brute-force computations show that the probability of the existence of the m node-disjoint shortest paths (from the source node to the m destination nodes) in a k-ary n-cube is greater than 94%, 70%, 91%, 69%, and 89% for (k,n,m)=(2,7,7), (3, 4, 8), (4, 3, 6), (5, 3, 6), and (6, 3, 6), respectively. © 2014 Elsevier Inc. All rights reserved.","Hypercube; k-ary n-cube; Mesh; Node-disjoint paths; Optimization problem; Torus"
"LABS: Latency aware broadcast scheduling in uncoordinated Duty-Cycled Wireless Sensor Networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027926050&doi=10.1016%2fj.jpdc.2014.07.011&partnerID=40&md5=94d2ddc87f4d5c17ca0adb614d828a72","Broadcast is a fundamental operation in Wireless Sensor Networks (WSNs) and plays an important role in a communication protocol design. In duty-cycled scenarios, a sensor node can receive a message only in its active time slot, which makes it more difficult to design collision-free scheduling for broadcast operations. Recent studies in this area have focused on minimizing broadcast latency and guaranteeing that all nodes receive a broadcast message. This paper investigates the problem of Minimum Latency Broadcast Scheduling in Duty-Cycled (MLBSDC) WSNs. By using special geometric properties of independent sets of a broadcast tree, we reduce the number of transmissions, consequently reducing the possibility of collision. Allowing multiple transmissions in one working period, our proposed Latency Aware Broadcast Scheduling (LABS) scheme provides a latency-efficient broadcast schedule. Theoretical analysis proves that the scheme has the same approximation ratio and complexity as the previous best algorithm for the MLBSDC problem. Moreover, simulation shows that the new scheme achieves up to 34%, 37%, and 21% performance improvement over previous schemes, in terms of latency, number of transmissions, and energy consumption, respectively. © 2015 Elsevier B.V. All rights reserved.","Approximation algorithm; Broadcast scheduling; Duty-cycle; Minimum latency; Wireless sensor networks"
"CUIRRE: An open-source library for load balancing and characterizing irregular applications on GPUs","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905396063&doi=10.1016%2fj.jpdc.2014.07.004&partnerID=40&md5=010aea61a54a0403b1bb457261532bb3","While Graphics Processing Units (GPUs) show high performance for problems with regular structures, they do not perform well for irregular tasks due to the mismatches between irregular problem structures and SIMD-like GPU architectures. In this paper, we introduce a new library, CUIRRE, for improving performance of irregular applications on GPUs. CUIRRE reduces the load imbalance of GPU threads resulting from irregular loop structures. In addition, CUIRRE can characterize irregular applications for their irregularity, thread granularity and GPU utilization. We employ this library to characterize and optimize both synthetic and real-world applications. The experimental results show that a 1.63× on average and up to 2.76× performance improvement can be achieved with the centralized task pool approach in the library at a 4.57% average overhead with static loading ratios. To avoid the cost of exhaustive searches of loading ratios, an adaptive loading ratio method is proposed to derive appropriate loading ratios for different inputs automatically at runtime. Our task pool approach outperforms other load balancing schemes such as the task stealing method and the persistent threads method. The CUIRRE library can easily be applied on many other irregular problems. © 2014 Elsevier Inc. All rights reserved.","Characterizing; GPU; Irregular; Library; Load balancing"
"Space-efficient parallel algorithms for combinatorial search problems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924246146&doi=10.1016%2fj.jpdc.2014.09.007&partnerID=40&md5=21debf03093ceffef6b2716453a9eaa4","We present space-efficient parallel strategies for two fundamental combinatorial search problems, namely, backtrack search and branch-and-bound, both involving the visit of an n-node tree of height h under the assumption that a node can be accessed only through its father or its children. For both problems we propose efficient algorithms that run on a p-processor distributed-memory machine. For backtrack search, we give a deterministic algorithm running in O(n/p+hlogp) time, and a Las Vegas algorithm requiring optimal O(n/p+h) time, with high probability. Building on the backtrack search algorithm, we also derive a Las Vegas algorithm for branch-and-bound which runs in O((n/p+hlogplogn)hlog2n) time, with high probability. A remarkable feature of our algorithms is the use of only constant space per processor, which constitutes a significant improvement upon previous algorithms whose space requirements per processor depend on the (possibly huge) tree to be explored. © 2014 Elsevier Inc. All rights reserved.","Backtrack search; Branch-and-bound; Combinatorial search problems; Distributed algorithms; Space-efficient algorithms"
"Performance-constrained energy reduction in data centers for video-sharing services","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918784785&doi=10.1016%2fj.jpdc.2014.10.008&partnerID=40&md5=f9fd9c56b0be1bf2856cba773fbe2e39","Energy saving in large-scale video sharing data centers is an important yet daunting challenge due to the conflicting goal of providing real-time guarantees. Simple energy reduction techniques can result in excessive delay and severely affect the quality-of-service. This paper aims to optimize energy consumption while ensuring service delay constraints in data centers that provide large-scale video-sharing services. However, this broader goal requires three challenges that must be holistically addressed rather than in isolation. First, we propose a generic model to accurately characterize the disk behavior in a VSS by taking into account the unique characteristic of parallel video workloads. Second, the paper proposes a prediction-based algorithm that formulates and solves a constrained optimization problem for determining optimal selections of disk power modes in VSSs. Third, two novel caching algorithms are proposed that achieve additional energy saving through optimizing cache utilization. Experiments reveal that the proposed 3-component scheme achieves a significant amount of energy saving under the same delay level as compared to traditional energy management schemes. © 2014 Elsevier Inc. All rights reserved.","Energy efficiency; Parallel storage systems; Video-sharing servers"
"Pars network: A multistage interconnection network with fault-tolerance capability","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915756990&doi=10.1016%2fj.jpdc.2014.08.005&partnerID=40&md5=e9687f43d6c70ddd8bde92a4aa9e796e","Interconnection networks are used for communication between nodes in multi-processor systems as well as super-systems. These systems require effective communication between the processor and memory blocks and therefore, interconnection networks are considered as the heart of the parallel processing and multi-processor systems. Multistage interconnection networks (MINs) are the main option for use in supercomputer environments that consist of thousands of processing elements. In this paper, a regular class of fault-tolerant MINs named Pars network along with its routing algorithm are presented. Analytical results demonstrate that the Pars network outperforms known regular networks, namely ABN, ASEN, EGN, and IEGN in terms of cost, fault-tolerance, terminal reliability, mean time to failure, and permutation capability. © 2014 Elsevier Inc. All rights reserved.","Cost-effectiveness; Fault-tolerance; Multi-processor systems; Multistage interconnection network; Pars network; Reliability; Self-routing"
"Graceful deadlock-free fault-tolerant routing algorithm for 3D Network-on-Chip architectures","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893207147&doi=10.1016%2fj.jpdc.2014.01.002&partnerID=40&md5=4703dbd43c1b514cb0b0cf2911550a85","Three-Dimensional Networks-on-Chip (3D-NoC) has been presented as an auspicious solution merging the high parallelism of Network-on-Chip (NoC) interconnect paradigm with the high-performance and lower interconnect-power of 3-dimensional integration circuits. However, 3D-NoC systems are exposed to a variety of manufacturing and design factors making them vulnerable to different faults that cause corrupted message transfer or even catastrophic system failures. Therefore, a 3D-NoC system should be fault-tolerant to transient malfunctions or permanent physical damages. In this paper, we present an efficient fault-tolerant routing algorithm, called Hybrid-Look-Ahead-Fault- Tolerant (HLAFT), which takes advantage of both local and look-ahead routing to boost the performance of 3D-NoC systems while ensuring fault-tolerance. A deadlock-recovery technique associated with HLAFT, named Random-Access-Buffer (RAB), is also presented. RAB takes advantage of look-ahead routing to detect and remove deadlock with no considerably additional hardware complexity. We implemented the proposed algorithm and deadlock-recovery technique on a real 3D-NoC architecture (3D-OASIS-NoC1) and prototyped it on FPGA. Evaluation results show that the proposed algorithm performs better than XYZ, even when considering high fault-rates (i.e., ≥ 20%), and outperforms our previously designed Look-Ahead-Fault-Tolerant routing (LAFT) demonstrated in latency/flit reduction that can reach 12.5% and a throughput enhancement reaching 11.8% in addition to 7.2% dynamic-power saving thanks to the Power-management module integrated with HLAFT. © 2014 Elsevier Inc. All rights reserved.","3D-NoC; Adaptive; Architecture; Deadlock-free; Look-ahead routing; Parallel"
"An efficient construction of one-to-many node-disjoint paths in folded hypercubes","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894614298&doi=10.1016%2fj.jpdc.2013.12.005&partnerID=40&md5=7ddf68632a0a0d26afa9ac73af293657","A folded hypercube is basically a hypercube with additional links augmented, where the additional links connect all pairs of nodes with longest distance in the hypercube. In an n-dimensional folded hypercube, it has been shown that n+1 node-disjoint paths from one source node to other n+1 (mutually) distinct destination nodes, respectively, can be constructed in O(n4) time so that their maximal length is not greater than âŒ̂n/ 2âŒ‰+1, where n+1 is the connectivity and âŒ̂n/2âŒ‰ is the diameter. Besides, their maximal length is minimized in the worst case. In this paper, we further show that by minimizing the computations of minimal routing functions, these node-disjoint paths can be constructed in O(n3) time, which is more efficient, and is hard to be reduced because it must take O(n3) time to compute a minimal routing function by solving a corresponding maximum weighted bipartite matching problem with the best known algorithm. © 2013 Elsevier Inc. All rights reserved.","Folded hypercube; Hypercube; Matching problem; Node-disjoint paths; Optimization problem"
"A case study of parallel JPEG encoding on an FPGA","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923858276&doi=10.1016%2fj.jpdc.2014.09.010&partnerID=40&md5=d188270110ce9b2509bd4570dbbcdb39","In this note we focus on the empirical results on a case study of parallel JPEG encoding on real FPGA platform, which evaluates and complements Hill & Marty's findings. A hardware prototype is constructed on FPGA with MicroBlaze processors and JPEG hardware accelerators. Experimental results on this case study demonstrate that the Hill and Marty's findings reinforces the hardware/software task partitioning for hybrid MPSoC architectures and also provide creditable new insights to scalable homogeneous and heterogeneous FPGA based MPSoC domains. © 2014 Elsevier Inc.","Case study; FPGA; Hill & Marty's findings; JPEG encoding"
"Joint routing and location-based service in VANETs","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891628349&doi=10.1016%2fj.jpdc.2013.10.004&partnerID=40&md5=b602c2fcfc2e02e089500d6e4e15600c","Geographic routing protocols use location information when they need to route packets. In the meantime, location information are maintained by location-based services provided by network nodes in a distributed manner. Routing and location services are very related but are used separately. Therefore, the overhead of the location-based service is not considered when we evaluate the geographic routing overhead. Our aim is to combine routing protocols with location-based services in order to reduce communication establishment latency and routing overhead. Our main contribution is to reduce the location overhead. Thus, we propose two combinations: (1) a geographic routing protocol with GLS called Hybrid Routing and Grid Location Service (HRGLS) and (2) a geographic routing protocol with HLS denoted Hybrid Routing and Hierarchical Location Service (HRHLS), where instead of launching an exact position request, we send the packet to the old destination position and when the packet is approaching the former position, we request the exact one. The complexity of the location query cost in both proposed schemes is O(logN), while it is O(N) in the case of HLS and GLS. Simulation results also confirm the complexity analysis and show promising results in terms of latency, packet delivery ratio and control message overhead. © 2013 Elsevier Inc. All rights reserved.","AdHoc networks; Geographic routing protocols; Location services; VANET"
"Multi-objective list scheduling of workflow applications in distributed computing infrastructures","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891622266&doi=10.1016%2fj.jpdc.2013.12.004&partnerID=40&md5=a8d57dfe924b45dacffa2b924ce579f7","Executing large-scale applications in distributed computing infrastructures (DCI), for example modern Cloud environments, involves optimization of several conflicting objectives such as makespan, reliability, energy, or economic cost. Despite this trend, scheduling in heterogeneous DCIs has been traditionally approached as a single or bi-criteria optimization problem. In this paper, we propose a generic multi-objective optimization framework supported by a list scheduling heuristic for scientific workflows in heterogeneous DCIs. The algorithm approximates the optimal solution by considering user-specified constraints on objectives in a dual strategy: maximizing the distance to the user's constraints for dominant solutions and minimizing it otherwise. We instantiate the framework and algorithm for a four-objective case study comprising makespan, economic cost, energy consumption, and reliability as optimization goals. We implemented our method as part of the ASKALON environment (Fahringer et al., 2007) for Grid and Cloud computing and demonstrate through extensive real and synthetic simulation experiments that our algorithm outperforms related bi-criteria heuristics while meeting the user constraints most of the time. © 2013 Elsevier Inc. All rights reserved.","Distributed computing infrastructures; Multi-objective scheduling; Scientific workflows"
"An investigation of the efficient implementation of cellular automata on multi-core CPU and GPU hardware","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923817866&doi=10.1016%2fj.jpdc.2014.10.011&partnerID=40&md5=f22db2c163a6f48dfea1aa4135376d77","Cellular automata (CA) have proven to be excellent tools for the simulation of a wide variety of phenomena in the natural world. They are ideal candidates for acceleration with modern general purpose-graphical processing units (GPU/GPGPU) hardware that consists of large numbers of small, tightly-coupled processors. In this study the potential for speeding up CA execution using multi-core CPUs and GPUs is investigated and the scalability of doing so with respect to standard CA parameters such as lattice and neighbourhood sizes, number of states and generations is determined. Additionally the impact of 'Activity' (the number of 'alive' cells) within a given CA simulation is investigated in terms of both varying the random initial distribution levels of 'alive' cells, and via the use of novel state transition rules; where a change in the dynamics of these rules (i.e. the number of states) allows for the investigation of the variable complexity within. © 2014 Elsevier Inc. All rights reserved.","Cellular automata (CA); General purpose graphic processing unit (GPGPU); OpenCL; OpenMP; Single Instruction Multiple Data (SIMD); Single Instruction Multiple Thread (SIMT)"
"ReKonf: Dynamically reconfigurable multiCore architecture","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927804757&doi=10.1016%2fj.jpdc.2014.05.007&partnerID=40&md5=c675609f448ddd4be501d0fe60f4ac46","The increased transistor count resulting from ever-decreasing feature sizes has enabled the design of architectures containing many small but efficient processing units (cores). At the same time, many new applications have evolved with varying performance requirements. The fixed architecture of multiCore platforms often fails to accommodate the inherent diverse requirements of different applications. We present a dynamically reconfigurable multiCore architecture that detects program phase change at runtime and adapts to the changing program behavior by reconfiguring itself. We introduce simple but efficient performance counters to monitor vital parameters of reconfigurable architectures. We also present static, dynamic and adaptive reconfiguration techniques for reconfiguring the architecture. Our evaluation of the proposed reconfigurable architecture using an adaptive reconfiguration technique shows an improvement of up to 23% for multi-threaded applications and up to 27% for multiprogrammed workloads over that on statically chosen architectures, and up to 41% over the baseline SMP configuration. © 2015 Elsevier B.V. All rights reserved.","Adaptive systems; ManyCore; MultiCore; Reconfigurable architecture"
"A vertex centric parallel algorithm for linear temporal logic model checking in Pregel","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027925405&doi=10.1016%2fj.jpdc.2014.07.009&partnerID=40&md5=9e8fa61b9c9959ab04ef67dbde5dd245","Linear Temporal Logic (LTL) Model Checking is a very important and popular technique for the automatic verification of safety-critical hardware and software systems, aiming at ensuring their quality. However, it is well known that LTL model checking suffers from the state explosion problem, often leading to insurmountable scalability problems when applying it to real-world systems. While there has been work on distributed algorithms for explicit on-the-fly LTL model checking, these are not sufficiently scalable and capable of tolerating faults during computation, significantly limiting their usefulness in huge cluster environments. Moreover, implementing these algorithms is generally viewed as a very challenging, error-prone task. In this paper, we instead rely on Pregel, a simple yet powerful model for distributed computation on large graphs. Pregel has from the start been designed for efficient, scalable and fault tolerant operation on clusters of thousands of computers, including large cloud setups. To harness Pregel's power, we propose a new vertex centric distributed algorithm for explicit LTL model checking of concurrent systems. Experimental results illustrate feasibility and scalability of the proposed algorithm. Compared with other distributed algorithms, our algorithm is more scalable, reliable and efficient. © 2015 Elsevier B.V. All rights reserved.","Distributed memory algorithm; Formal method; Linear temporal logic; Model checking; Reliable model checker; Scalable algorithm"
"Sporadic decentralized resource maintenance for P2P distributed storage networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891627553&doi=10.1016%2fj.jpdc.2013.11.001&partnerID=40&md5=af5ad33a6ddfd8ad5e196f1e32ee9371","In this paper, we propose a novel decentralized resource maintenance strategy for peer-to-peer (P2P) distributed storage networks. Our strategy relies on the Wuala overlay network architecture, (The WUALA Project). While the latter is based, for the resource distribution among peers, on the use of erasure codes, e.g., Reed-Solomon codes, here we investigate the system behavior when a simple randomized network coding strategy is applied. We propose to replace the Wuala regular and centralized strategy for resource maintenance with a decentralized strategy, where users regenerate new fragments sporadically, namely every time a resource is retrieved. Both strategies are analyzed, analytically and through simulations, in the presence of either erasure and network coding. It will be shown that the novel sporadic maintenance strategy, when used with randomized network coding, leads to a fully decentralized solution with management complexity much lower than common centralized solutions. © 2013 Elsevier Inc. All rights reserved.","Decentralized maintenance; Distributed storage; Erasure coding; Peer-to-peer (P2P); Randomized network coding"
"A fair starvation-free prioritized mutual exclusion algorithm for distributed systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930634537&doi=10.1016%2fj.jpdc.2015.04.002&partnerID=40&md5=f2f08fee7849c1faf07e0f841c68dbeb","Several distributed mutual exclusion algorithms define the order in which requests are satisfied based on the priorities assigned to requests. These algorithms are very useful for real-time applications ones or those where priority is associated to a quality of service requirement. However, priority based strategies may result in starvation problems where high priority requests forever prevent low priority ones to be satisfied. To overcome this problem, many priority-based algorithms propose to gradually increase the priority of pending requests. The drawback of such an approach is that it can violate priority-based order of requests leading to priority inversion. Therefore, aiming at minimizing the number of priority violations without introducing starvation, we have added some heuristics in Kanrar-Chaki priority-based token-oriented algorithm in order to slow down the frequency with which priority of pending requests is increased. Performance evaluation results confirm the effectiveness of our approach when compared to both the original Kanrar-Chaki and Chang's priority-based algorithms. ©2015 Published by Elsevier Inc.","Algorithm; Distributed system; Mutual exclusion; Priority"
"""slow is Fast"" for wireless sensor networks in the presence of message losses","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028174499&doi=10.1016%2fj.jpdc.2014.11.004&partnerID=40&md5=f0719d3be84a4ee9612bf35ed9612617","We present a new shared memory model, SF shared memory model. In this model, the actions of each node are partitioned into slow actions and fast actions. By contrast, the traditional shared memory model only includes fast actions. Intuitively, slow actions can utilize slightly stale state information to execute successfully. However, fast actions require that the state information they use is most recent. We show that the use of slow actions can substantially benefit in improving performance of programs from the shared memory model to WAC model that has been designed for sensor networks. To illustrate this, we use three protocols concerning problems that need to be solved in sensor networks. We show that under various message loss probabilities, densities, etc., slow actions can improve the performance substantially, since slow actions reduce the performance penalty of fast actions under heavy message loss environments. Moreover, the effectiveness of the slow action increases when there is a higher probability of message loss. © 2014 Elsevier Inc. All rights reserved.","Self-stabilization; Shared memory model; Wireless sensor networks"
"An enhanced location-free Greedy Forward algorithm with hole bypass capability in wireless sensor networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027951197&doi=10.1016%2fj.jpdc.2014.10.007&partnerID=40&md5=8a897a840f6b7f4796973a80f50086dc","Greedy Forward is a technique for data routing in Wireless Sensor Networks (WSNs) in which data packets are forwarded to the node that is geographically closer to the destination node. Two main concerns can be found in routing algorithms based on this technique: first, it requires all sensor nodes to know their physical location. Second, this kind of algorithm does not work in cases when a node is located in a network 'hole', i.e., the node does not have any neighbor closer to the destination node. In this work, we propose a new Greedy Forward algorithm that can be used in routing protocols for WSNs that does not require localization of the nodes and also is able to deal with nodes located near network holes. Differently from current greedy forward algorithms, our approach uses only the Received Signal Strength Indicator (RSSI) of exchanged packets. Based on this observation, we propose the RSSR (Received Signal Strength Routing) algorithm with two variants: RSSR Election and RSSR Selection. In the RSSR Election, the next hop is dynamically elected and no packets are required for the routing task. In the RSSR Selection, neighbors exchange packets with RSSI information and the next hop of the packet is then selected from a routing table. Then, we present a novel technique for dealing with network holes even when the physical location of the nodes is unknown. This technique improves the reliability and applicability of the proposed schemes in most WSN scenarios. Our results indicate clearly that the proposed algorithms have all the benefits of a greedy forward algorithm but with better performance, better packet delivery rate and without requiring position information. © 2015, Elsevier Inc. All rights reserved.","Algorithm design and analysis; Routing protocols; Wireless sensor networks"
"Energy-aware parallel self-reconfiguration for chains microrobot networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918817863&doi=10.1016%2fj.jpdc.2014.10.003&partnerID=40&md5=7cb9debbf156ec3fd0790f55ebfba555","MEMS microrobots are miniaturized electro-mechanical elements, made using the techniques of micro-fabrication. They have limited energy capacity and low memory space. Self-reconfiguration is required for MEMS microrobots to complete their mission and/or to optimize their communication. In this paper, we present a self-reconfiguration protocol from a straight chain to square organization, which deals with MEMS microrobots characteristics. In the proposed protocol, nodes do not have the map of their target positions which makes the protocol portable, standalone, and the memory complexity is bounded by a constant. This paper improves a former solution by using parallelism in the movements of microrobots to optimize the time and the number of movements and by making the algorithm energy-aware. So each node is aware of the amount of energy that it will spend, which will improve the energy consumption. Our algorithm is implemented in Meld, a declarative language, and executed in a real environment simulator called DPRSim. © 2014 Elsevier Inc. All rights reserved.","Distributed algorithm; Energy; Logical topology; MEMS microrobot; Parallel algorithm; Self-reconfiguration"
"An energy-efficient and sink-location privacy enhanced scheme for WSNs through ring based routing","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929164299&doi=10.1016%2fj.jpdc.2015.04.003&partnerID=40&md5=8ad95d8ec71849b396d8f3b6796840c2","Sink location privacy is one of the major issues in Wireless Sensor Networks (WSNs) where adversaries may locate the sink by observing the destination of packets, directing and scaling of data flow, which leads to exposure of the sink-location privacy. In this paper, we propose a Ring Based Routing (RBR) scheme to address the issue of sink-location privacy in WSNs. The RBR scheme is composed of multiple routing rings and routing lines where the nodal data is not directly sent to the sink but to the nearest routing ring. In our scheme, data is routed through each node in the ring and then sent to other routing rings via routing lines, where the number of anonymity sink nodes is equal to the total number of nodes in the network. More specifically, under the RBR scheme the routing rings move in irregular patterns which can confuse adversaries even if the sink location is static and this greatly improves the sink-location privacy. In addition, the routing rings are constructed according to comprehensive analysis of network energy, which can fully use the remaining energy and improve energy efficiency and network lifetime. Both theoretical analysis and simulation results show that our scheme can protect location privacy of sinks effectively. © 2015 Elsevier Inc.","Energy efficiency; Preserving sink location privacy; Routing; Security"
"SABA: A security-aware and budget-aware workflow scheduling strategy in clouds","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918795447&doi=10.1016%2fj.jpdc.2014.09.002&partnerID=40&md5=858c1e42193c9a73f305ae9b5edac7bd","High quality of security service is increasingly critical for Cloud workflow applications. However, existing scheduling strategies for Cloud systems disregard security requirements of workflow applications and only consider CPU time neglecting other resources like memory, storage capacities. These resource competition could noticeably affect the computation time and monetary cost of both submitted tasks and their required security services. To address this issue, in this paper, we introduce immoveable dataset concept which constrains the movement of certain datasets due to security and cost considerations and propose a new scheduling model in the context of Cloud systems. Based on the concept, we propose a Security-Aware and Budget-Aware workflow scheduling strategy (SABA), which holds an economical distribution of tasks among the available CSPs (Cloud Service Providers) in the market, to provide customers with shorter makespan as well as security services. We conducted extensive simulation studies using six different workflows from real world applications as well as synthetic ones. Results indicate that the scheduling performance is affected by immoveable datasets in Clouds and the proposed scheduling strategy is highly effective under a wide spectrum of workflow applications. © 2014 Elsevier Inc. All rights reserved.","Budget; Cloud computing; Scheduling; Security; Workflow"
"PERP: Attacking the balance among energy, performance and recovery in storage systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939938718&doi=10.1016%2fj.jpdc.2014.11.007&partnerID=40&md5=a1c948d803a20ad5177bc968703de5bf","Most recently, an important metric called ""energy proportional"" is presented as a guideline for energy efficiency systems (Barroso and Hölzle, 2007), which advocates that energy consumption should be in proportion to system performance/utilization. However, this tradeoff metric is only defined for normal mode where the system is functioning normally without node failures. When node failure occurs, the system enters degradation mode during which node reconstruction is initiated. This very process needs to wake/spin up a number of disks and takes a substantial amount of I/O bandwidth, which will not only compromise energy efficiency but also performance. Moreover, as in replication-based storage such as Google File System (Sanjay Ghemawat, Gobioff, 2003 [10]) and Hadoop Distributed File System (Borthakur, 2007), systems are adopting a recovery policy that defines a deadline for recovery rather than simply recovering the data as soon as possible. Given the flexibility of the recovery time, this makes it possible to reduce energy consumption with respect to the performance and recovery requirements. This raises a natural problem: how to balance the performance, energy, and recovery in degradation mode for an energy efficient storage system? Without considering the I/O bandwidth contention between recovery and performance, we find that the current energy proportional solutions cannot answer this question accurately. This paper presents a mathematical model named Perfect Energy, Recovery and Performance (PERP) which provides guidelines for provisioning the number of active nodes as well as the assigned recovery bandwidth at each time slot with respect to the performance and recovery constraints. To utilize PERP in storage systems, we take data layouts into consideration and propose a node selection algorithm named ""Gradual Increase/decrease"" Algorithm (GIA) to select the active nodes based on PERP results. We apply PERP and GIA to current popular power proportional layouts and test their effectiveness on a 25 nodes in-house CASS cluster. Experimental results validate that while meeting both performance and recovery constraints, PERP helps realize 25% power savings compared with maximum recovery policy from Sierra (Thereska et al., 2011) and 20% power savings compared with recovery group policy from Rabbit (Amur et al., 2010).","Data layout; Energy; Performance; Power proportional; Recovery"
"Research on the conjugate gradient algorithm with a modified incomplete Cholesky preconditioner on GPU","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891630422&doi=10.1016%2fj.jpdc.2013.10.002&partnerID=40&md5=f64692639eed2670cc64de339bc41f14","In this study, we discover the parallelism of the forward/backward substitutions (FBS) for two cases and thus propose an efficient preconditioned conjugate gradient algorithm with the modified incomplete Cholesky preconditioner on the GPU (GPUMICPCGA). For our proposed GPUMICPCGA, the following are distinct characteristics: (1) the vector operations are optimized by grouping several vector operations into single kernels, (2) a new kernel of inner product and a new kernel of the sparse matrix-vector multiplication with high optimization are presented, and (3) an efficient parallel implementation of FBS on the GPU (GPUFBS) for two cases are suggested. Numerical results show that our proposed kernels outperform the corresponding ones presented in CUBLAS or CUSPARSE, and GPUFBS is almost 3 times faster than the implementation of FBS using the CUSPARSE library. Furthermore, GPUMICPCGA has better behavior than its counterpart implemented by the CUBLAS and CUSPARSE libraries. © 2013 Elsevier Inc. All rights reserved.","Conjugate gradient algorithm; CUDA; GPU; Modified incomplete Cholesky preconditioner"
"Optimal metadata replications and request balancing strategy on cloud data centers","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905032272&doi=10.1016%2fj.jpdc.2014.06.010&partnerID=40&md5=03295ea9e24f3c8edab779f57392034c","In large-scale cloud data centers, metadata accesses will very likely become a severe performance bottleneck as metadata-based transactions account for over 50% of all file system operations. Clusters of Metadata Servers (MDS) that provide metadata searching service can improve the system performance significantly. For a data stored in cloud data centers, there may be several MDS storing the metadata replicas. Therefore, when a data request arrives, it has many potential metadata paths, one of which shall be chosen to obtain the best performance. In this paper, we attempt to determine the number of MDS that each data object in the system shall have and the request rates that each MDS shall serve, in order to achieve the minimum mean response time (MRT) of all the metadata requests. The target optimal constrained function has been formulated and a novel metadata request balancing algorithm based on request arrival rates has been proposed, which can find near-optimal solutions by a theoretical proof. In our experiments, we compare our algorithm with widely used hashing functions that have 0, 1, 2, 3 replicas, respectively. We validate our findings via simulations with respect to several influencing factors and prove that our proposed strategy is scalable, flexible and efficient for the real-life applications. Some interesting perspectives of the work are also presented at the end of this paper. © 2014 Elsevier Inc. All rights reserved.","Distributed system; Mean response time; Metadata server; Queueing theory; Request balancing"
"A uniform approach for programming distributed heterogeneous computing systems","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910032554&doi=10.1016%2fj.jpdc.2014.08.002&partnerID=40&md5=4779a19aa3897a2f9a947df600b0c6b3","Large-scale compute clusters of heterogeneous nodes equipped with multi-core CPUs and GPUs are getting increasingly popular in the scientific community. However, such systems require a combination of different programming paradigms making application development very challenging. In this article we introduce libWater, a library-based extension of the OpenCL programming model that simplifies the development of heterogeneous distributed applications. libWater consists of a simple interface, which is a transparent abstraction of the underlying distributed architecture, offering advanced features such as inter-context and inter-node device synchronization. It provides a runtime system which tracks dependency information enforced by event synchronization to dynamically build a DAG of commands, on which we automatically apply two optimizations: collective communication pattern detection and device-host-device copy removal. We assess libWater's performance in three compute clusters available from the Vienna Scientific Cluster, the Barcelona Supercomputing Center and the University of Innsbruck, demonstrating improved performance and scaling with different test applications and configurations. © 2014 The Authors.","Distributed computing; Heterogeneous computing; MPI; OpenCL; Programming model; Runtime system"
"Self-scaling cooperative discovery of service compositions in unstructured P2P networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956571765&doi=10.1016%2fj.jpdc.2014.06.006&partnerID=40&md5=903bafe118123318fa9949b1f4ede877","We propose an efficient technique for improving the performance of automatic and cooperative compositions in unstructured Peer-to-Peer networks during service discovery. The technique exploits a probabilistic forwarding algorithm that uses different sources of knowledge, such as network density and service grouping, to reduce the amount of messages exchanged in the network. The technique, analysed in several network configurations by using a simulator to observe resolution time, recall and message overhead, presents good performance especially in dense and large-scale service networks. To further improve performance and effectiveness of service discovery, we propose a bidirectional search strategy for distributed service composition. It enables concurrent searches over the Peer-to-Peer network exploring the service space in two search directions, hence reducing the response time when solutions are present; in case the requests for a service cannot be completely satisfied, discovered partial solutions may be analysed to identify service gaps that suggest future service implementations and consequently new opportunities for service providers. This technique further reduces the time for discovering compositions, highlighting only a limited increment, when compared with the unidirectional search, of the number of messages exchanged. © 2014 Elsevier Inc. All rights reserved..","Composition overlay networks; Peer-to-Peer computing; Semantic overlay networks; Semantic web services; Service composition; Service discovery"
"High quality real-time Image-to-Mesh conversion for finite element simulations","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891624679&doi=10.1016%2fj.jpdc.2013.11.002&partnerID=40&md5=3f761af3d541e417512f7a996e36a386","In this paper, we present a parallel Image-to-Mesh Conversion (I2M) algorithm with quality and fidelity guarantees achieved by dynamic point insertions and removals. Starting directly from an image, its implementation is capable of recovering the isosurface and meshing the volume with tetrahedra of good shape. Our tightly-coupled shared-memory parallel speculative execution paradigm employs carefully designed contention managers, load balancing, synchronization and optimizations schemes. These techniques are shown to boost not only the parallel but also the single-threaded efficiency of our code. Specifically, our single-threaded performance is faster than both CGAL and TetGen, the state of the art sequential open source meshing tools we are aware of. The effectiveness of our method is demonstrated on Blacklight, the Pittsburgh Supercomputing Center's cache-coherent NUMA machine. We observe a more than 82% strong scaling efficiency for up to 64 cores, and a more than 82% weak scaling efficiency for up to 144 cores, reaching a rate of more than 14.3 million elements per second. This is the fastest 3D Delaunay mesh generation and refinement algorithm, to the best of our knowledge. © 2013 Elsevier Inc. All rights reserved.","Fidelity; Parallel Delaunay mesh refinement; Quality; Scalability; Shared-memory"
"SHadoop: Improving MapReduce performance by optimizing job execution mechanism in Hadoop clusters","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891620235&doi=10.1016%2fj.jpdc.2013.10.003&partnerID=40&md5=123dfbd027169baa03b3895ce3992417","As a widely-used parallel computing framework for big data processing today, the Hadoop MapReduce framework puts more emphasis on high-throughput of data than on low-latency of job execution. However, today more and more big data applications developed with MapReduce require quick response time. As a result, improving the performance of MapReduce jobs, especially for short jobs, is of great significance in practice and has attracted more and more attentions from both academia and industry. A lot of efforts have been made to improve the performance of Hadoop from job scheduling or job parameter optimization level. In this paper, we explore an approach to improve the performance of the Hadoop MapReduce framework by optimizing the job and task execution mechanism. First of all, by analyzing the job and task execution mechanism in MapReduce framework we reveal two critical limitations to job execution performance. Then we propose two major optimizations to the MapReduce job and task execution mechanisms: first, we optimize the setup and cleanup tasks of a MapReduce job to reduce the time cost during the initialization and termination stages of the job; second, instead of adopting the loose heartbeat-based communication mechanism to transmit all messages between the JobTracker and TaskTrackers, we introduce an instant messaging communication mechanism for accelerating performance-sensitive task scheduling and execution. Finally, we implement SHadoop, an optimized and fully compatible version of Hadoop that aims at shortening the execution time cost of MapReduce jobs, especially for short jobs. Experimental results show that compared to the standard Hadoop, SHadoop can achieve stable performance improvement by around 25% on average for comprehensive benchmarks without losing scalability and speedup. Our optimization work has passed a production-level test in Intel and has been integrated into the Intel Distributed Hadoop (IDH). To the best of our knowledge, this work is the first effort that explores on optimizing the execution mechanism inside map/reduce tasks of a job. The advantage is that it can complement job scheduling optimizations to further improve the job execution performance. © 2013 Elsevier Inc. All rights reserved.","Cloud computing; Distributed processing; MapReduce; Parallel computing; Performance optimization"
"Robust network supercomputing with unreliable workers","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918803604&doi=10.1016%2fj.jpdc.2014.10.002&partnerID=40&md5=a1d4e161939baaa40b8ec1f10b355b8e","Internet supercomputing is becoming a powerful tool for harnessing massive amounts of computational resources. However in typical master-worker settings the correctness of the results of the computation crucially relies on the ability of the master to depend on the computation performed by the workers. We consider a distributed system consisting of a master process and a collection of synchronous worker processes that can execute tasks on behalf of the master and that may act nefariously by deliberately returning fallacious results. The master decides on the correctness of the results by assigning the same task to several workers. For such a setting with n processes and t tasks we study the problem of collectively performing the tasks under two different failure models: model Fa, where some fraction f of workers that are prone to returning arbitrary (e.g., incorrect) results and the probability p of such faulty behavior are not known a priori to the master, and model Fb when these quantities are known to the master. Previous works assume that the number of faulty processes or the probability of a process acting maliciously is known to the master, e.g., as in model Fb. In this paper this assumption is removed in model Fa. First, for model Fa we provide an efficient algorithm-based on the Stopping Rule Algorithm by Dagum et al. (1995) - that can estimate f and p with (ε,δ)-approximation, for any 0<δ<1 and ε>0. We also provide a randomized algorithm for detecting the faulty processes for model Fa. Finally, we provide algorithms to perform t tasks, with n workers, in models Fa and Fb. © 2014 Elsevier Inc. All rights reserved.","Distributed algorithms; Fault-tolerance; Internet supercomputing; Randomized algorithms; Reliability"
"A survey of high level frameworks in block-structured adaptive mesh refinement packages","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910096726&doi=10.1016%2fj.jpdc.2014.07.001&partnerID=40&md5=798b484aee640567c80f83cf4f46fde9","Over the last decade block-structured adaptive mesh refinement (SAMR) has found increasing use in large, publicly available codes and frameworks. SAMR frameworks have evolved along different paths. Some have stayed focused on specific domain areas, others have pursued a more general functionality, providing the building blocks for a larger variety of applications. In this survey paper we examine a representative set of SAMR packages and SAMR-based codes that have been in existence for half a decade or more, have a reasonably sized and active user base outside of their home institutions, and are publicly available. The set consists of a mix of SAMR packages and application codes that cover a broad range of scientific domains. We look at their high-level frameworks, their design trade-offs and their approach to dealing with the advent of radical changes in hardware architecture. The codes included in this survey are BoxLib, Cactus, Chombo, Enzo, FLASH, and Uintah.","BoxLib; Cactus; Chombo; Enzo; FLASH; SAMR; Uintah"
"Distributed algorithm for the maximal 2-packing in geometric outerplanar graphs","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891811255&doi=10.1016%2fj.jpdc.2013.12.002&partnerID=40&md5=f6259052a73b662177964c9477ac6277","In this paper, we present a deterministic distributed algorithm that computes the maximal 2-packing set in a geometric outerplanar graph. In a geometric outerplanar graph, all the vertices have location coordinates in the plane and lie on the boundary of the graph. Our algorithm consists of three phases. First, it elects a vertex as the leader. Second, it explores the graph to determine relevant information about the structure of the input graph. Third, with this information, it computes a maximal 2-packing set. When the input graph is a ring, the algorithm computes a maximum 2-packing set. The execution time of this algorithm is O(n) steps and it uses O(nlogn) messages. This algorithm does not require knowledge of the size of the input graph. To the best of our knowledge, this is the first deterministic distributed algorithm that solves such a problem for a geometric outerplanar graph in a linear number of steps. © 2013 Elsevier Inc. All rights reserved.","2-packing set; Distributed algorithm; Ear decomposition; Geometric graph; Outerplanar graph"
"Exploiting multi-core nodes in peer-to-peer grids","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894636935&doi=10.1016%2fj.jpdc.2013.12.001&partnerID=40&md5=dff147efe7c572d2f13b58d851022565","While the majority of CPUs now sold contain multiple computing cores, current grid computing systems either ignore the multiplicity of cores, or treat them as distinct, independent machines. The latter approach ignores the resource contention present between cores in a single CPU, while the former approach fails to take advantage of significant computing power. We provide a decentralized resource management framework for exploiting multi-core nodes to run multi-threaded applications in peer-to-peer grids. We present two new load-balancing schemes that explicitly account for the resource sharing and contention of multiple cores, and propose a parameterized performance prediction model that can represent a continuum of resource sharing among cores of a CPU. We use extensive simulation to confirm that our two algorithms match jobs with computing nodes efficiently, and balance load during the lifetime of the computing jobs. © 2013 Elsevier Inc. All rights reserved.","Multi-core; Peer-to-peer grid; Resource contention; Resource management"
"A massively parallel tensor contraction framework for coupled-cluster computations","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027958735&doi=10.1016%2fj.jpdc.2014.06.002&partnerID=40&md5=c7e1b189b18cfcf11108b172bd2a6aa5","Precise calculation of molecular electronic wavefunctions by methods such as coupled-cluster requires the computation of tensor contractions, the cost of which has polynomial computational scaling with respect to the system and basis set sizes. Each contraction may be executed via matrix multiplication on a properly ordered and structured tensor. However, data transpositions are often needed to reorder the tensors for each contraction. Writing and optimizing distributed-memory kernels for each transposition and contraction is tedious since the number of contractions scales combinatorially with the number of tensor indices. We present a distributed-memory numerical library (Cyclops Tensor Framework (CTF)) that automatically manages tensor blocking and redistribution to perform any user-specified contractions. CTF serves as the distributed-memory contraction engine in Aquarius, a new program designed for high-accuracy and massively-parallel quantum chemical computations. Aquarius implements a range of coupled-cluster and related methods such as CCSD and CCSDT by writing the equations on top of a C++ templated domain-specific language. This DSL calls CTF directly to manage the data and perform the contractions. Our CCSD and CCSDT implementations achieve high parallel scalability on the BlueGene/Q and Cray XC30 supercomputer architectures showing that accurate electronic structure calculations can be effectively carried out on top of general distributed-memory tensor primitives. © 2014 Elsevier Inc.","Communication-avoiding algorithms; Coupled-cluster; Matrix multiplication; Tensor contractions; Topology-aware mapping"
"Towards efficient and fair resource trading in community-based cloud computing","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927805095&doi=10.1016%2fj.jpdc.2014.07.005&partnerID=40&md5=4cc7e3f13b45107b5c2dfa31e58802a9","In this paper, we investigate the resource trading problem in a community-based cloud computing setting where multiple tenants communicate in a peer-to-peer (P2P) fashion. Enabling resource trading in a community cloud unleashes the untapped cloud resources, thus presents a flexible solution for managing resource allocation. However, finding an efficient and fair resource allocation is challenging mainly due to the heterogeneity of tenants. Our work first develops a market-oriented model to support resource negotiation and trading. Based on this model, we adopt a multiagent-based technique that allows a group of autonomous tenants to reach an efficient and fair resource allocation. Further, when budget constraint presents, we propose a directed hypergraph model to facilitate resource trading amongst heterogeneous tenants. We analyze the application of the directed hypergraph model to trading decision making, and design a series of heuristic-based resource trading protocols for both budget-unaware and budget-aware scenarios. The performances of the proposed protocols are validated through simulations. The results are in tune with the theoretical analysis and provide insights into practical application issues. © 2015 Elsevier B.V. All rights reserved.","Cloud computing; Envy-free; Hypergraph; Resource trading; Social welfare"
"Memory-aware tree traversals with pre-assigned tasks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918809786&doi=10.1016%2fj.jpdc.2014.10.004&partnerID=40&md5=104154b7438be35c6c21872e139edbb0","We study the complexity of traversing tree-shaped workflows whose tasks require large I/O files. We target a heterogeneous architecture with two resource types, each with a different memory, such as a multicore node equipped with a dedicated accelerator (FPGA or GPU). The tasks in the workflow are colored according to their type and can be processed if all their input and output files can be stored in the corresponding memory. The amount of used memory of each type at a given execution step strongly depends upon the ordering in which the tasks are executed, and upon when communications between both memories are scheduled. The objective is to determine an efficient traversal that minimizes the maximum amount of memory of each type needed to traverse the whole tree. In this paper, we establish the complexity of this two-memory scheduling problem, and provide inapproximability results. In addition, we design several heuristics, based on both post-order and general traversals, and we evaluate them on a comprehensive set of tree graphs, including random trees as well as assembly trees arising in the context of sparse matrix factorizations. © 2014 Elsevier Inc. All rights reserved.","Bi-objective optimization; Memory-aware; Multifrontal method; Scheduling; Sparse matrix factorization; Tree traversal"
"Hypergraph partitioning for multiple communication cost metrics: Model and methods","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923675667&doi=10.1016%2fj.jpdc.2014.12.002&partnerID=40&md5=1edae58cbe032047e3edf0d4119f8549","We investigate hypergraph partitioning-based methods for efficient parallelization of communicating tasks. A good partitioning method should divide the load among the processors as evenly as possible and minimize the inter-processor communication overhead. The total communication volume is the most popular communication overhead metric which is reduced by the existing state-of-the-art hypergraph partitioners. However, other metrics such as the total number of messages, the maximum amount of data transferred by a processor, or a combination of them are equally, if not more, important. Existing hypergraph-based solutions use a two phase approach to minimize such metrics where in each phase, they minimize a different metric, sometimes at the expense of others. We propose a one-phase approach where all the communication cost metrics can be effectively minimized in a multi-objective setting and reductions can be achieved for all metrics together. For an accurate modeling of the maximum volume and the number of messages sent and received by a processor, we propose the use of directed hypergraphs. The directions on hyperedges necessitate revisiting the standard partitioning heuristics. We do so and propose a multi-objective, multi-level hypergraph partitioner called UMPa. The partitioner takes various prioritized communication metrics into account, and optimizes all of them together in the same phase. Compared to the state-of-the-art methods which only minimize the total communication volume, we show on a large number of problem instances that UMPa produces better partitions in terms of several communication metrics. © 2014 Elsevier Inc. All rights reserved.","Graph; Hypergraph; Load balancing; Partitioning"
"Task scheduling using NSGA II with fuzzy adaptive operators for computational grids","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894495965&doi=10.1016%2fj.jpdc.2014.01.006&partnerID=40&md5=54dd3c4ea483db89ad4b86502211e369","Scheduling algorithms have an essential role in computational grids for managing jobs, and assigning them to appropriate resources. An efficient task scheduling algorithm can achieve minimum execution time and maximum resource utilization by providing the load balance between resources in the grid. The superiority of genetic algorithm in the scheduling of tasks has been proven in the literature. In this paper, we improve the famous multi-objective genetic algorithm known as NSGA-II using fuzzy operators to improve quality and performance of task scheduling in the market-based grid environment. Load balancing, Makespan and Price are three important objectives for multi-objective optimization in the task scheduling problem in the grid. Grid users do not attend load balancing in making decision, so it is desirable that all solutions have good load balancing. Thus to decrease computation and ease decision making through the users, we should consider and improve the load balancing problem in the task scheduling indirectly using the fuzzy system without implementing the third objective function. We have used fuzzy operators for this purpose and more quality and variety in Pareto-optimal solutions. Three functions are defined to generate inputs for fuzzy systems. Variance of costs, variance of frequency of involved resources in scheduling and variance of genes values are used to determine probabilities of crossover and mutation intelligently. Variance of frequency of involved resources with cooperation of Makespan objective satisfies load balancing objective indirectly. Variance of genes values and variance of costs are used in the mutation fuzzy system to improve diversity and quality of Pareto optimal front. Our method conducts the algorithm towards best and most appropriate solutions with load balancing in less iteration. The obtained results have proved that our innovative algorithm converges to Pareto-optimal solutions faster and with more quality. © 2014 Elsevier Inc. All rights reserved.","Grid computing; Load balancing; Multi-objective optimization; Non-dominated sorting genetic algorithm II; Task scheduling; Variance-based fuzzy operators"
"A GEMM interface and implementation on NVIDIA GPUs for multiple small matrices","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918832712&doi=10.1016%2fj.jpdc.2014.09.003&partnerID=40&md5=6d2f383d46ffa70e763b43e7ab786e6b","We present an interface and an implementation of the General Matrix Multiply (GEMM) routine for multiple small matrices processed simultaneously on NVIDIA graphics processing units (GPUs). We focus on matrix sizes under 16. The implementation can be easily extended to larger sizes. For single precision matrices, our implementation is 30% to 600% faster than the batched cuBLAS implementation distributed in the CUDA Toolkit 5.0 on NVIDIA Tesla K20c. For example, we obtain 104 GFlop/s and 216 GFlop/s when multiplying 100,000 independent matrix pairs of size 10 and 16, respectively. Similar improvement in performance is obtained for other sizes, in single and double precisions for real and complex types, and when the number of matrices is smaller. Apart from our implementation, our different function interface also plays an important role in the improved performance. Applications of this software include finite element computation on GPUs. © 2014 Elsevier Inc. All rights reserved.","BLAS; cuBLAS; Dense linear algebra; GEMM; GPU; NVIDIA CUDA; Parallel programming"
"Wihidum: Distributed complex event processing","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930375639&doi=10.1016%2fj.jpdc.2015.03.002&partnerID=40&md5=bcea6d061a9c78297e3d8ff8c45c04c1","In the last few years, we have seen much interest in data processing technologies. Although initial interests focused on batch processing technologies like MapReduce, people have realized the need for more responsive technologies such as stream processing and complex event processing. Complex event processing has been historically used within a single node or a cluster of tightly interconnected nodes. However, to be effective with Big Data use-cases, CEP technologies need to be able to scale up to handle large use-cases. This paper presents several approaches to scale complex event processing by distributing it across several nodes. Wihidum discusses how to balance the workload among nodes efficiently, how complex event processing queries can be broken up into simple sub queries, and how queries can be efficiently deployed in the cluster. The paper focuses on three techniques used for scaling queries: pipelining, partitioning and distributed operators. Then it discusses in detail the distribution of few CEP operators: filters, joins, pattern matching, and partitions. Empirical results show that the techniques followed in Wihidum have improved the performance of the CEP solution. © 2015 Elsevier Inc. All rights reserved.","Complex event processing; Distributed systems"
"Cluster-to-cluster data transfer with data compression over wide-area networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930375239&doi=10.1016%2fj.jpdc.2014.09.008&partnerID=40&md5=e0217eb3afead77a27e285183c67a806","The recent emergence of ultra high-speed networks up to 100 Gb/s has posed numerous challenges and has led to many investigations on efficient protocols to saturate 100 Gb/s links. However, end-to-end data transfers involve many components, not only protocols, affecting overall transfer performance. These components include disk I/O subsystem, additional computation associated with data streams, and network adapters. For example, achievable bandwidth by TCP may not be implementable if disk I/O or CPU becomes a bottleneck in end-to-end data transfer. In this paper, we first model all the system components involved in end-to-end data transfer as a graph. We then formulate the problem whose goal is to achieve maximum data transfer throughput using parallel data flows. We also propose a variable data flow GridFTP XIO stack to improve data transfer with data compression. Our contributions lie in how to optimize data transfers considering all the system components involved rather than in accurately modeling all the system components involved. Our proposed formulations and solutions are evaluated through experiments on the ESnet 100G testbed and a wide-area cluster-to-cluster testbed. The experimental results on the ESnet 100G testbed show that our approach is several times faster than Globus Online - 8 x faster for datasets with many 10 MB files and 3-4 x faster for other datasets of larger size files. The experimental results on the cluster-to-cluster testbed show that our variable data flow approach is up to 4 x faster than a normal cluster data transfer. © 2014 Elsevier Inc. All rights reserved.","Disk-to-disk data transfer; High-speed networks; Optimization; System modeling"
"Peer-to-peer bichromatic reverse nearest neighbours in mobile ad-hoc networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027933295&doi=10.1016%2fj.jpdc.2014.07.007&partnerID=40&md5=d4199536e8f979d93889f392afe9ecc8","The increasing use of mobile communications has raised many issues of decision support and resource allocation. A crucial problem is how to solve queries of Reverse Nearest Neighbour (RNN). An RNN query returns all objects that consider the query object as their nearest neighbour. Existing methods mostly rely on a centralised base station. However, mobile P2P systems offer many benefits, including self-organisation, fault-tolerance and load-balancing. In this study, we propose and evaluate 3 distinct P2P algorithms focusing on bichromatic RNN queries, in which mobile query peers and static objects of interest are of two different categories, based on a time-out mechanism and a boundary polygon around the mobile query peers. The Brute-Force Search Algorithm provides a naive approach to exploit shared information among peers whereas two other Boundary Search Algorithms filter a number of peers involved in query processing. The algorithms are evaluated in the MiXiM simulation framework with both real and synthetic datasets. The results show the practical feasibility of the P2P approach for solving bichromatic RNN queries for mobile networks. © 2015 Elsevier B.V. All rights reserved.","Collaborative caching; Mobile ad-hoc networks; Mobile database; P2P spatial queries; Reverse nearest neighbours"
"The numerical template toolbox: A modern C++ design for scientific computing","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910025411&doi=10.1016%2fj.jpdc.2014.07.002&partnerID=40&md5=8d9de78ed0b039b0cb1e1dbe2d885b8e","The design and implementation of high level tools for parallel programming is a major challenge as the complexity of modern architectures increases. Domain Specific Languages (or DSL) have been proposed as a solution to facilitate this design but few of those DSLs actually take full advantage of said parallel architectures. In this paper, we propose a library-based solution by designing a C++ DSLs using generative programming: NT2. By adapting generative programming idioms so that architecture specificities become mere parameters of the code generation process, we demonstrate that our library can deliver high performance while featuring a high level API and being easy to extend over new architectures. © 2014 Elsevier Inc. All rights reserved.","C++; Embedded domain specific languages; Generative programming; Generic programming; Parallel skeletons"
"IMSuite: A benchmark suite for simulating distributed algorithms","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918783881&doi=10.1016%2fj.jpdc.2014.10.010&partnerID=40&md5=313845354b80803305bf98fbf4ea1b3e","Considering the diverse nature of real-world distributed applications that makes it hard to identify a representative subset of distributed benchmarks, we focus on their underlying distributed algorithms. We present and characterize a new kernel benchmark suite (named IMSuite) that simulates some of the classical distributed algorithms in task parallel languages. We present multiple variations of our kernels, broadly categorized under two heads: (a) varying synchronization primitives (with and without fine grain synchronization primitives); and (b) varying forms of parallelization (data parallel and recursive task parallel). Our characterization covers interesting aspects of distributed applications such as distribution of remote communication requests, number of synchronization, task creation, task termination and atomic operations. We study the behavior (execution time) of our kernels by varying the problem size, the number of compute threads, and the input configurations. We also present an involved set of input generators and output validators. © 2014 Elsevier Inc. All rights reserved.","Benchmarks; Data parallelism; Distributed algorithms; Performance evaluation; Recursive task parallelism; Task parallelism"
"The IceProd framework: Distributed data processing for the IceCube neutrino observatory","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915776438&doi=10.1016%2fj.jpdc.2014.08.001&partnerID=40&md5=54822c2fcaf350a5637564c564bab320","IceCube is a one-gigaton instrument located at the geographic South Pole, designed to detect cosmic neutrinos, identify the particle nature of dark matter, and study high-energy neutrinos themselves. Simulation of the IceCube detector and processing of data require a significant amount of computational resources. This paper presents the first detailed description of IceProd, a lightweight distributed management system designed to meet these requirements. It is driven by a central database in order to manage mass production of simulations and analysis of data produced by the IceCube detector. IceProd runs as a separate layer on top of other middleware and can take advantage of a variety of computing resources, including grids and batch systems such as CREAM, HTCondor, and PBS. This is accomplished by a set of dedicated daemons that process job submission in a coordinated fashion through the use of middleware plugins that serve to abstract the details of job submission and job management from the framework. © 2014 Elsevier Inc. All rights reserved.","Data management; Distributed computing; Grid computing; Monitoring"
"Distributed channel assignment algorithms for 802.11n WLANs with heterogeneous clients","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894419241&doi=10.1016%2fj.jpdc.2014.01.009&partnerID=40&md5=5c9b413ecccfa29b9f03ff0843a49e8a","As the latest IEEE 802.11 standard, 802.11n applies several new technologies, such as multiple input multiple output (MIMO), channel bonding, and frame aggregation to greatly improve the rate, range and reliability of wireless local area networks (WLANs). In 802.11n WLANs, access points (APs) are often densely deployed to provide satisfactory coverage. Thus nearby APs should operate at non-overlapping channels to avoid mutual interference. It is challenging to assign channels in legacy 802.11a/b/g WLANs due to the limited number of channels. Channel assignment becomes more complex in 802.11n WLANs, as the channel bonding in 802.11n allows WLAN stations (APs and clients) to combine two adjacent, non-overlapping 20MHz channels together for transmission. On the other hand, IEEE 802.11n is backward compatible, such that 802.11n clients will coexist with legacy clients in 802.11n WLANs. Legacy clients may affect the performance of nearby 802.11n clients, and reduce the effectiveness of channel bonding. Based on these observations, in this paper, we study channel assignment in 802.11n WLANs with heterogeneous clients. We first present the network model, interference model, and throughput estimation model to estimate the throughput of each client. We then formulate the channel assignment problem into an optimization problem, with the objective of maximizing overall network throughput. Since the problem is NP-hard, we give a distributed channel assignment algorithm based on the throughput estimation model. We then present another channel assignment algorithm with lower complexity, and aim at minimizing interference experienced by high-rate, 802.11n clients. We have carried out extensive simulations to evaluate the proposed algorithms. Simulation results show that our algorithms can significantly improve the network throughput of 802.11n WLANs, compared with other channel assignment algorithms. © 2014 Elsevier Inc. All rights reserved.","Channel assignment; Heterogeneous clients; IEEE 802.11n; Wireless local area networks (WLANs)"
"Competitive online adaptive scheduling for sets of parallel jobs with fairness and efficiency","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891711423&doi=10.1016%2fj.jpdc.2013.12.003&partnerID=40&md5=23a237b5e5fbb84f96c1c81fa8cdd755","We study online adaptive scheduling for multiple sets of parallel jobs, where each set may contain one or more jobs with time-varying parallelism. This two-level scheduling scenario arises naturally when multiple parallel applications are submitted by different users or user groups in large parallel systems, where both user-level fairness and system-wide efficiency are of important concerns. To achieve fairness, we use the well-known equi-partitioning algorithm to distribute the available processors among the active job sets at any time. For efficiency, we apply a feedback-driven adaptive scheduler that periodically adjusts the processor allocations within each set by consciously exploiting the jobs' execution history. We show that our algorithm achieves asymptotically competitive performance with respect to the set response time, which incorporates two widely used performance metrics, namely, total response time and makespan, as special cases. Both theoretical analysis and simulation results demonstrate that our algorithm improves upon an existing scheduler that provides only fairness but lacks efficiency. Furthermore, we provide a generalized framework for analyzing a family of scheduling algorithms based on feedback-driven policies with provable efficiency. Finally, we consider an extended multi-level hierarchical scheduling model and present a fair and efficient solution that effectively reduces the problem to the two-level model. © 2013 Elsevier Inc. All rights reserved.","Adaptive scheduling; Competitive analysis; Efficiency; Fairness; Hierarchical scheduling; Multiprocessors; Online algorithm; Parallel jobs; Set response time"
"Efficient random walk sampling in distributed networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924727224&doi=10.1016%2fj.jpdc.2015.01.002&partnerID=40&md5=43c01697796eb5d5bd50d6a084c031c0","Performing random walks in networks is a fundamental primitive that has found numerous applications in communication networks such as token management, load balancing, network topology discovery and construction, search, and peer-to-peer membership management. While several such algorithms are ubiquitous, and use numerous random walk samples, the walks themselves have always been performed naively. In this paper, we focus on the problem of performing random walk sampling efficiently in a distributed network. Given bandwidth constraints, the goal is to minimize the number of rounds and messages required to obtain several random walk samples in a continuous online fashion. We present the first round and message optimal distributed algorithms that present a significant improvement on all previous approaches. The theoretical analysis and comprehensive simulations of our algorithms show that they perform very well in different types of networks of differing topologies. In particular, our results show how several random walks can be performed continuously (when source nodes are provided only at runtime, i.e., online), such that each walk of length ℓ can be performed exactly in just Õ(√ℓD) rounds1 (where D is the diameter of the network), and O(ℓ) messages. This significantly improves upon both, the naive technique that requires O(ℓ) rounds and O(ℓ) messages, and the sophisticated algorithm of Das Sarma et al. (2013) that has the same round complexity as this paper but requires Ω(m√ℓ) messages (where m is the number of edges in the network). Our theoretical results are corroborated through extensive simulations on various topological data sets. Our algorithms are fully decentralized, lightweight, and easily implementable, and can serve as building blocks in the design of topologically-aware networks. © 2015 Elsevier Inc. All rights reserved.","Decentralized computation; Distributed algorithms; Random sampling; Random walks; Self-aware network"
"Multi-tier service differentiation by coordinated learning-based resource provisioning and admission control","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894476672&doi=10.1016%2fj.jpdc.2014.01.004&partnerID=40&md5=eb0cc0807097c8cb68b401fdfecde1c2","Multiple Internet applications are often hosted in one datacenter, sharing underlying virtualized server resources. It is important to provide differentiated treatment to co-hosted applications and to improve overall system performance by efficient use of shared resources. Challenges arise due to multi-tier service architecture, virtualized server infrastructure, and highly dynamic and bursty workloads. We propose a coordinated admission control and adaptive resource provisioning approach for multi-tier service differentiation and performance improvement in a shared virtualized platform. We develop new model-independent reinforcement learning based techniques for virtual machine (VM) auto-configuration and session based admission control. Adaptive VM auto-configuration provides proportional service differentiation between co-located applications and improves application response time simultaneously. Admission control improves session throughput of the applications and minimizes resource wastage due to aborted sessions. A shared reward actualizes coordination between the two learning modules. For system agility and scalability, we integrate the reinforcement learning approach with cascade neural networks. We have implemented the integrated approach in a virtualized blade server system hosting RUBiS benchmark applications. Experimental results demonstrate that the new approach meets differentiation targets accurately and achieves performance improvement of applications at the same time. It reacts to dynamic and bursty workloads in an agile and scalable manner. © 2014 Elsevier Inc. All rights reserved.","Cascade neural networks; Performance differentiation; Reinforcement learning; Scalable Internet services; Virtualized servers"
"Assessing the role of mini-applications in predicting key performance characteristics of scientific and engineering applications","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918818527&doi=10.1016%2fj.jpdc.2014.09.006&partnerID=40&md5=b11915f80227691dbfd44ebe93cc3e04","Computational science and engineering application programs are typically large, complex, and dynamic, and are often constrained by distribution limitations. As a means of making tractable rapid explorations of scientific and engineering application programs in the context of new, emerging, and future computing architectures, a suite of ""miniapps"" has been created to serve as proxies for full scale applications. Each miniapp is designed to represent a key performance characteristic that does or is expected to significantly impact the runtime performance of an application program. In this paper we introduce a methodology for assessing the ability of these miniapps to effectively represent these performance issues. We applied this methodology to three miniapps, examining the linkage between them and an application they are intended to represent. Herein we evaluate the fidelity of that linkage. This work represents the initial steps required to begin to answer the question, ""Under what conditions does a miniapp represent a key performance characteristic in a full app?"" © 2014 Elsevier Inc. All rights reserved.","High performance computing; Scientific computing; Validation"
"Accelerating sequential programs on commodity multi-core processors","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894624564&doi=10.1016%2fj.jpdc.2013.12.009&partnerID=40&md5=8fe99e4bdb70f2ef7a6cb5dfec672ee4","A recently proposed pipelined multithreading (PMT) technique exhibits wide applicability in parallelizing general sequential programs on multi-core processors. However, significant inter-core communication overhead limits PMT performance and prevents its commercial utilization. A simple and effective clustered pipelined multithreading (CPMT) approach is presented to accelerate sequential programs on commodity multi-core processors. This CPMT technique adopts a clustered communication mechanism that can yield very low average communication overhead by eliminating false sharing as well as reducing communication operation and transit delays in the software-only approach. A single-producer/single-consumer concurrent lock-free clusteredQueue algorithm based on a two-level queue structure is also proposed. The accuracy of CPMT is theoretically demonstrated. The performances of the algorithm and CPMT are evaluated on a commodity AMD Phenom four-core processor. The number of enqueue and dequeue times of the algorithm are 20.8 and 23 cycles given an appropriate parameter, respectively. The speedup of CPMT ranges from 13.1% to 119.8% for typical loops extracted from the SPEC CPU 2000 benchmark suite. © 2013 Elsevier Inc. All rights reserved.","Clustered communication mechanism; Commodity multi-core processors; Pipeline parallelism"
"Time hybrid total order broadcast: Exploiting the inherent synchrony of broadcast networks","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027955096&doi=10.1016%2fj.jpdc.2014.10.012&partnerID=40&md5=2a290b4d126753dc1f50cd7f85e24362","Total order broadcast is a fundamental communication primitive for the construction of highly-available systems. Informally, the primitive guarantees that messages sent by a group of processes are delivered to all processes in the same order. This paper investigates the design and performance of a very simple synchronous total order broadcast that is built atop of an asynchronous distributed system based on a broadcast network. Our Time Hybrid Total Order Broadcast (THyTOB) explores the inherent synchrony of the broadcast network to build a total order for the messages, while ensuring safety under asynchrony and in the presence of process failures. We assess the performance of THyTOB in an Ethernet-based commodity cluster, and show that it is on a par with the performance of other well-known, and more complex total order broadcast protocols inherently designed for the asynchronous model. © 2014 Elsevier Inc. All rights reserved.","Asynchronous systems; Broadcast networks; Cluster environments; Consensus; Fault tolerance; Synchronous systems; Total order broadcast"
"Checkpointing algorithms and fault prediction","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891630180&doi=10.1016%2fj.jpdc.2013.10.010&partnerID=40&md5=e74751a90cc6a3e42dd72d38207a05a5","This paper deals with the impact of fault prediction techniques on checkpointing strategies. We extend the classical first-order analysis of Young and Daly in the presence of a fault prediction system, characterized by its recall and its precision. In this framework, we provide optimal algorithms to decide whether and when to take predictions into account, and we derive the optimal value of the checkpointing period. These results allow us to analytically assess the key parameters that impact the performance of fault predictors at very large scale. © 2013 Elsevier Inc. All rights reserved.","Algorithms; Checkpoint; Exascale; Fault-tolerance; Prediction; Resilience"
"Direct distributed memory access for CMPs","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891633293&doi=10.1016%2fj.jpdc.2013.11.004&partnerID=40&md5=3fac5ba8b699626550da55c8a22f0789","On-chip distributed memory has emerged as a promising memory organization for future many-core systems, since it efficiently exploits memory level parallelism and can lighten off the load on each memory module by providing a comparable number of memory interfaces with on-chip cores. The packet-based memory access model (PDMA) has provided a scalable and flexible solution for distributed memory management, but suffers from complicated and costly on-chip network protocol translation and massive interferences among packets, which leads to unpredictable performance. In this paper we propose a direct distributed memory access (DDMA) model, in which remote memory can be directly accessed by local cores via remote-to-local virtualization, without network protocol translation. From the perspective of local cores, remote memory controllers (MC) can be directly manipulated through accessing the local agent MC, which is responsible for accessing remote memory through high-performance inter-tile communication. We further discuss some detailed architecture supports for the DDMA model, including the memory interface design, work flow and the protocols involved. Simulation results of executing PARSEC benchmarks show that our DDMA architecture outperforms PDMA in terms of both average memory access latency and IPC by 17.8% and 16.6% respectively on average. Besides, DDMA can better manage congested memory traffic, since a reduction of bandwidth in running memory-intensive SPEC2006 workloads only incurs 18.9% performance penalty, compared with 38.3% for PDMA. © 2013 Elsevier Inc. All rights reserved.","Architecture; Memory interface; On-chip distributed memory; Performance"
"A grand spread estimator using a graphics processing unit","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891634263&doi=10.1016%2fj.jpdc.2013.10.007&partnerID=40&md5=2263a6d66425cf5c32cdfef123407698","The spread of a source is defined as the number of distinct destinations to which the source has sent packets during a measurement period. Spread estimation is essential in traffic monitoring, measurement, intrusion detection, to mention a few. To support high speed networking, recent research suggests implementing a spread estimator in fast but small on-chip memory such as SRAM. A state-of-the-art estimator can hold succinct information about 10 million distinct packets using 1 MB SRAM. This implies that a measurement period should restart whenever every 10 million distinct packets fill up the SRAM. Spread estimation is a challenging problem because two spread values from different measurement periods cannot be aggregated to derive the total value. Therefore, current spread estimators have a serious limitation concerning the length of the measurement period because SRAM is available a few megabytes at most. In this paper, we propose a spread estimator that utilizes a large memory space of a graphics processing unit on a commodity PC. The proposed estimator utilizes a 1 GB memory, a hundred times larger than those of current spread estimators, and its throughput is still around 160 Gbps. According to our experiments, the proposed scheme can cover a measurement period of a few dozen hours while the current state-of-the-art can cover only one hour. To the best of our knowledge, this has not been achieved by any spread estimators thus far. © 2013 Elsevier Inc. All rights reserved.","Graphics processing unit; Intrusion detection; Software router; Spread estimation; Traffic measurement"
"Regularizing graph centrality computations","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027948323&doi=10.1016%2fj.jpdc.2014.07.006&partnerID=40&md5=376107a168d462f537c461c8c849816e","Centrality metrics such as betweenness and closeness have been used to identify important nodes in a network. However, it takes days to months on a high-end workstation to compute the centrality of today's networks. The main reasons are the size and the irregular structure of these networks. While today's computing units excel at processing dense and regular data, their performance is questionable when the data is sparse. In this work, we show how centrality computations can be regularized to reach higher performance. For betweenness centrality, we deviate from the traditional fine-grain approach by allowing a GPU to execute multiple BFSs at the same time. Furthermore, we exploit hardware and software vectorization to compute closeness centrality values on CPUs, GPUs and Intel Xeon Phi. Experiments show that only by reengineering the algorithms and without using additional hardware, the proposed techniques can speed up the centrality computations significantly: an improvement of a factor 5.9 on CPU architectures, 70.4 on GPU architectures and 21.0 on Intel Xeon Phi. © 2014 Elsevier Inc. All rights reserved.","Betweenness centrality; BFS; Closeness centrality; CPU; GPU; Intel Xeon Phi; Vectorization"
"A general purpose lossless data compression method for GPU","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918787064&doi=10.1016%2fj.jpdc.2014.09.016&partnerID=40&md5=ab993a4845c6b5187d316038a64a025d","The paper describes a parallel method for a lossless data compression that uses graphical processing units (GPUs). Two commonly used statistical and dictionary approaches to data compression have been applied in our method. The reduction of compression time was possible due to the implementation of multi level parallel methods that use a single GPU or a set of GPUs efficiently. The base of our method is a search for repetitions in data that is executed in parallel with the use of sorted suffix tables. On the second level of concurrency operations on different data blocks: data file reading, match search, coding, compression and data file writing are performed in parallel. The methods proposed, supplying a comparable compression ratio, achieve a better compression speed than a standard CPU-based compression tools used in personal computers. Experiments performed in technologically comparable systems showed that our approach is similar or even better in terms of power and cost efficiency. © 2014 Elsevier Inc. All rights reserved.","GPU; Lossless data compression; Parallel processing"
"Heterogeneity-driven end-to-end synchronized scheduling for precedence constrained tasks and messages on networked embedded systems","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2015.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929619660&doi=10.1016%2fj.jpdc.2015.04.005&partnerID=40&md5=2401006dd3ebc651d5198df2db8f8191","Scheduling for a directed acyclic graph (DAG) on networked embedded systems is to maximize concurrency and minimize inter-processor communication for minimum end-to-end worst-case response time (WCRT). Time accuracy and synchronization are critical for scheduling on heterogeneous networked embedded systems, where computing and networking are both heterogeneous and deeply jointed. Most algorithms use the upward rank value for task prioritization, and the earliest finish time for processor selection. In order to obtain accurate and efficient schedules in heterogeneous networked systems, the above approaches can be improved. Moreover, synchronization with tasks and messages is critical for end-to-end WCRT. However, task scheduling and message scheduling are isolated in most approaches in communication contention environments. In this paper, a heterogeneity-driven task scheduling algorithm called Heterogeneous Selection Value (HSV) based on the classic model, and a heterogeneity-driven end-to-end synchronized scheduling algorithm called Heterogeneous Selection Value on Communication Contention (HSV-CC) based on the communication contention model are proposed to address the above problems. Both benchmark and extensive experimental evaluation demonstrate significant performance improvement of the proposed algorithms. © 2015 Elsevier Inc.All rights reserved.","Communication contention; DAG; Heterogeneity; Networked embedded systems; Synchronized scheduling"
"Byzantine broadcast with fixed disjoint paths","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927657257&doi=10.1016%2fj.jpdc.2014.07.010&partnerID=40&md5=41f3999214aa601ea233b8d6cd53e631","We consider the problem of reliably broadcasting a message in a multihop network. We assume that some nodes may be Byzantine, and behave arbitrarily. We focus on cryptography-free solutions. We propose a protocol for sparse networks (such as grids or tori) where the nodes are not aware of their position. Our protocol uses a fixed number of disjoint paths to accept and forward the message to be broadcast. It can be tuned to significantly improve the number of Byzantine nodes tolerated. We present both theoretical analysis and experimental evaluation. © 2015 Elsevier B.V. All rights reserved.","Asynchronous networks; Byzantine failures; Distributed computing; Fault tolerance; Multihop networks; Protocol; Random failures; Reliable broadcast"
"Dynamic task scheduling using a directed neural network","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918818599&doi=10.1016%2fj.jpdc.2014.09.015&partnerID=40&md5=7e5df7865c08ff481570b37e40f6e2cf","This article is based on the problem of work flow scheduling in grid environment of multi-processors. We, in this paper, introduce three novel approaches for the task scheduling problem using recently proposed Directed Search Optimization (DSO). In the first attempt, task scheduling is framed as an optimization problem and solved by DSO. Next, this paper makes use of DSO as a training algorithm to train (a) a three layer Artificial Neural Network (ANN) and then (b) Radial Basis Function Neural Networks (RBFNN). These DSO trained networks are used for task scheduling and interestingly yield better performance than contemporary algorithms as evidenced by simulation results. © 2014 Elsevier Inc. All rights reserved.","Directed search optimization; Neural network; Task scheduling"
"Reputation systems: A survey and taxonomy","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918779043&doi=10.1016%2fj.jpdc.2014.08.004&partnerID=40&md5=d5a7e142cc089e471687f309ef49b21e","In our increasingly interconnected world, the need for reputation is becoming more important as larger numbers of people and services interact online. Reputation is a tool to facilitate trust between entities, as it increases the efficiency and effectiveness of online services and communities. As most entities will not have any direct experience of other entities, they must increasingly come to rely on reputation systems. Such systems allow the prediction who is likely to be trustworthy based on feedback from past transactions. In this paper we introduce a new taxonomy for reputation systems, along with: a reference model for reputation context, a model of reputation systems, a substantial survey, and a comparison of existing reputation research and deployed reputation systems. © 2014 Elsevier Inc. All rights reserved.","Feedback; Reputation system; Survey; Taxonomy"
"Multi-threaded modularity based graph clustering using the multilevel paradigm","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027951398&doi=10.1016%2fj.jpdc.2014.09.012&partnerID=40&md5=fc361ebf1be79aa7dd25978f8ed7a30a","Graphs are an important tool for modeling data in many diverse domains. Recent increase in sensor technology and deployment, the adoption of online services, and the scale of VLSI circuits has caused the size of these graphs to skyrocket. Finding clusters of highly connected vertices within these graphs is a critical part of their analysis. In this paper we apply the multilevel paradigm to the modularity graph clustering problem. We improve upon the state of the art by introducing new efficient methods for coarsening graphs, creating initial clusterings, and performing local refinement on the resulting clusterings. We establish that for a graph with n vertices and m edges, these algorithms have an O(m+n) runtime complexity and an O(m+n) space complexity, and show that in practice they are extremely fast. We present shared-memory parallel formulations of these algorithms to take full advantage of modern architectures, which we show have a parallel runtime of O(m/p+n/p+k), where p is the number of threads and k is the number of clusters. Finally, we present the product of this research, the clustering tool Nerstrand.1 In serial mode, Nerstrand runs in a fraction of the time of current methods and produces results of equal quality. When run in parallel mode, Nerstrand exhibits significant speedup with less than one percent degradation of clustering quality. Nerstrand works well on large graphs, clustering a graph with over 105 million vertices and 3.3 billion edges in 90 s. © 2014 Elsevier Inc. All rights reserved.","Graph clustering; Multi-threading; Multilevel paradigm; Shared-memory parallel"
"Scaling Support Vector Machines on modern HPC platforms","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027956569&doi=10.1016%2fj.jpdc.2014.09.005&partnerID=40&md5=7b7f3e08c6ec3705f7593d5df09b2165","Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4-84× and 18-47× speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns. © 2014 Elsevier Inc. All rights reserved.","Dynamic modeling; Machine learning models; Multi- & many-core architectures; Optimization techniques; Performance analysis; Support Vector Machine"
"Scalable real-time OLAP on cloud architectures","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930637605&doi=10.1016%2fj.jpdc.2014.08.006&partnerID=40&md5=e0d682a2aa3ac413b3be0082865f4dff","In contrast to queries for on-line transaction processing (OLTP) systems that typically access only a small portion of a database, OLAP queries may need to aggregate large portions of a database which often leads to performance issues. In this paper we introduce CR-OLAP, a scalable Cloud based Real-time OLAP system based on a new distributed index structure for OLAP, the distributed PDCR tree. CR-OLAP utilizes a scalable cloud infrastructure consisting of multiple commodity servers (processors). That is, with increasing database size, CR-OLAP dynamically increases the number of processors to maintain performance. Our distributed PDCR tree data structure supports multiple dimension hierarchies and efficient query processing on the elaborate dimension hierarchies which are so central to OLAP systems. It is particularly efficient for complex OLAP queries that need to aggregate large portions of the data warehouse, such as ""report the total sales in all stores located in California and New York during the months February-May of all years"". We evaluated CR-OLAP on the Amazon EC2 cloud, using the TPC-DS benchmark data set. The tests demonstrate that CR-OLAP scales well with increasing number of processors, even for complex queries. For example, for an Amazon EC2 cloud instance with 16 processors, a data warehouse with 160 million tuples, and a TPC-DS OLAP query stream where each query aggregates between 60% and 95% of the database, CR-OLAP achieved a query latency of below 0.3 s which can be considered a real time response. © 2014 Elsevier Inc. All rights reserved.","Cloud architecture; Real-time OLAP; Scalability; TPC-DS benchmark"
"Hybrid parallel task placement in irregular applications","2015","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027953610&doi=10.1016%2fj.jpdc.2014.09.014&partnerID=40&md5=8467f5ebfd3c004080c3d53478e47acb","What are the performance benefits of selectively relaxing the locality preferences of some tasks in parallel applications? Can load-balancing algorithms for a distributed-memory cluster benefit from this relaxation? This work investigates these ideas by employing application-level task locality for selection of tasks rather than hardware memory topology as is the norm in the literature. A prototype designed to evaluate these ideas is implemented in X10, a realization of the asynchronous partitioned global address space programming model. This evaluation reveals the applicability of this new approach to several real-world applications chosen from the Cowichan and the Lonestar suites. On a cluster of 128 processors, the new work-stealing strategy demonstrates a speedup between 12% and 32% over X10's existing scheduler. Moreover, the new strategy does not degrade the performance of any of the applications studied. © 2014 Elsevier Inc. All rights reserved.","APGAS languages; Locality-aware scheduling; Task placement; X10"
"Privacy-preserving and verifiable protocols for scientific computation outsourcing to the cloud","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891322736&doi=10.1016%2fj.jpdc.2013.11.007&partnerID=40&md5=99da695c1aa4eead7c5fa19c3c23ad04","Computation outsourcing to the cloud has become a popular application in the age of cloud computing. Recently, two protocols for secure outsourcing scientific computations, i.e., linear equation solving and linear programming solving, to the cloud were proposed. In this paper, we improve the work by proposing new protocols that achieve significant performance gains. For linear equation solving outsourcing, we achieve the improvement by proposing a completely new protocol. The new protocol employs some special linear transformations and there are no homomorphic encryptions and interactions between the client and the cloud, compared with the previous protocol. For linear programming outsourcing, we achieve the improvement by reformulating the linear programming problem in the standard and natural form. We also introduce a method to reduce the key size by using a pseudorandom number generator. The design of the newly proposed protocols also sheds some insight on constructing secure outsourcing protocols for other scientific computations. Comparisons between our protocols and the previous protocols are given, which demonstrate significant improvements of our proposed protocols. We also carry out numerical experiments to validate the efficiency of our protocols for secure linear equation solving and linear programming outsourcing. © 2013 Elsevier Inc. All rights reserved.","Cloud computing; Computation outsourcing; Distributed computing; Linear equation solving; Linear programming"
"Flexible rerouting schemes for reconfiguration of multiprocessor arrays","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904079712&doi=10.1016%2fj.jpdc.2014.06.009&partnerID=40&md5=09a80fe771cf85395bf1acdaa19d5b24","In a multiprocessor array, some processing elements (PEs) fail to function normally due to hardware defects or soft faults caused by overheating, overload or occupancy by other running applications. Fault-tolerant reconfiguration reorganizes fault-free PEs to a new regular topology by changing the interconnection among PEs. This paper investigates the problem of constructing as large as possible logical array with short interconnects from a physical array with faults. A flexible rerouting scheme is developed to improve the efficiency of utilizing fault-free PEs. Under the scheme, two efficient reconfiguration algorithms are proposed. The first algorithm is able to generate the maximum logical array (MLA) in linear time. The second algorithm reduces the interconnect length of the MLA, and it is capable of producing nearly optimal logical arrays in comparison to the lower bound of the interconnect length, that is also proposed in this paper. Experimental results validate the efficiency of the flexible rerouting schemes and the proposed algorithms. For 128×128 host arrays with 30% unavailable PEs, the proposed approaches improve existing algorithm up to 44% in terms of logical array size, while reducing the interconnection redundancy by 49.6%. In addition, the proposed algorithms are more scalable than existing approaches. On host arrays with 50% unavailable PEs, our algorithms can produce logical arrays with harvest over 56% while existing approaches fail to construct a feasible logical array. © 2014 Elsevier Inc. All rights reserved.","Algorithm; Fault tolerance; Interconnection length; Processor array; Reconfiguration; Rerouting scheme"
"Robust static resource allocation of DAGs in a heterogeneous multicore system","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885956453&doi=10.1016%2fj.jpdc.2013.08.007&partnerID=40&md5=d15aa3c53867d603fc13067a0c6c6d5a","In this study, we consider an environment composed of a heterogeneous cluster of multicore-based machines used to analyze satellite data. The workload involves large data sets and is subject to a deadline constraint. Multiple applications, each represented by a directed acyclic graph (DAG), are allocated to a dedicated heterogeneous distributed computing system. Each vertex in the DAG represents a task that needs to be executed and task execution times vary substantially across machines. The goal of this research is to assign the tasks in applications to a heterogeneous multicore-based parallel system in such a way that all applications complete before a common deadline, and their completion times are robust against uncertainties in execution times. We define a measure that quantifies robustness in this environment. We design, compare, and evaluate five static resource allocation heuristics that attempt to maximize robustness. We consider six different scenarios with different ratios of computation versus communication, and loose and tight deadlines. © 2013 Elsevier Inc. All rights reserved.","Directed acyclical graph; Heterogeneous computing; Resource allocation"
"Efficient allocation of resources in multiple heterogeneous Wireless Sensor Networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887031117&doi=10.1016%2fj.jpdc.2013.09.012&partnerID=40&md5=63154542f941071054a12e8893322d4c","Wireless Sensor Networks (WSNs) are useful for a wide range of applications, from different domains. Recently, new features and design trends have emerged in the WSN field, making those networks appealing not only to the scientific community but also to the industry. One such trend is the running different applications on heterogeneous sensor nodes deployed in multiple WSNs in order to better exploit the expensive physical network infrastructure. Another trend deals with the capability of accessing sensor generated data from the Web, fitting WSNs in novel paradigms of Internet of Things (IoT) and Web of Things (WoT). Using well-known and broadly accepted Web standards and protocols enables the interoperation of heterogeneous WSNs and the integration of their data with other Web resources, in order to provide the final user with value-added information and applications. Such emergent scenarios where multiple networks and applications interoperate to meet high level requirements of the user will pose several changes in the design and execution of WSN systems. One of these challenges regards the fact that applications will probably compete for the resources offered by the underlying sensor nodes through the Web. Thus, it is crucial to design mechanisms that effectively and dynamically coordinate the sharing of the available resources to optimize resource utilization while meeting application requirements. However, it is likely that Quality of Service (QoS) requirements of different applications cannot be simultaneously met, while efficiently sharing the scarce networks resources, thus bringing the need of managing an inherent tradeoff. In this paper, we argue that a middleware platform is required to manage heterogeneous WSNs and efficiently share their resources while satisfying user needs in the emergent scenarios of WoT. Such middleware should provide several services to control running application as well as to distribute and coordinate nodes in the execution of submitted sensing tasks in an energy-efficient and QoS-enabled way. As part of the middleware provided services we present the Resource Allocation in Heterogeneous WSNs (SACHSEN) algorithm. SACHSEN is a new resource allocation heuristic for systems composed of heterogeneous WSNs that effectively deals with the tradeoff between possibly conflicting QoS requirements and exploits heterogeneity of multiple WSNs. © 2013 Published by Elsevier Inc. All rights reserved.","Resource allocation; Task allocation; Wireless sensor network"
"A parallel algorithm with enhancements via partial objective value cuts for cluster-based wireless sensor network design","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901635141&doi=10.1016%2fj.jpdc.2014.03.008&partnerID=40&md5=c66b1e6654f13b95b861fe5b13525a1d","In this paper, we develop a parallel algorithm for the solution of an integrated topology control and routing problem in Wireless Sensor Networks (WSNs). After presenting a mixed-integer linear optimization formulation for the problem, for its solution, we develop an effective parallel algorithm in a Master-Worker model that incorporates three parallelization strategies, namely low-level parallelism, domain decomposition, and multiple search (both cooperative and independent) in a single Master-Worker framework. For improved algorithmic efficiency, we introduce three reduced subproblems and devise partial objective value cuts from these reduced models. We utilize both the reduced models, for which we suggest efficient approaches for their solution, and the associated cuts in our parallel algorithm. We observe that the reduced models provide valuable information on the optimal design variables for the original model and we exploit this fact in our parallel algorithm. Our overall parallelization scheme utilizes exact optimization models and solutions as its components and allows cooperation among multiple worker processors via communication of partial solution and cut information. Computational study shows that our approach is very effective in addressing this complex problem. Parallel implementation not only achieves a speed-up of the computations, but also yields better solutions as it can explore the solution space more effectively. © 2014 Elsevier Inc. All rights reserved.","Network design; Parallel algorithms; Wireless sensor networks"
"Parameter variation sensing and estimation in nanoscale fabrics","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899066087&doi=10.1016%2fj.jpdc.2013.08.005&partnerID=40&md5=ad79c13b6ea06e7df27a2dc5b2c62aba","Parameter variations introduced by manufacturing imprecision are becoming more influential on circuit performance. This is especially the case in emerging nanoscale computing fabrics due to unconventional manufacturing steps and aggressive scaling. On-chip variation sensors are gaining in importance since post-fabrication compensation techniques can be employed. In estimation with on-chip variation sensors, however, random variations are masked because of well-known averaging effects during measurements. We propose a new on-chip sensor for nanoscale computing fabrics to estimate random variations in physical parameters. We show detailed estimation methodology and validate it with Monte Carlo simulations. The results show the sensor estimation error to be 8% on average and 12.7% in the worst case. In comparison to the well-known ring-oscillator based approach developed for CMOS, the estimation accuracy is 1.6× better and requires 40× less devices in on-chip sensors. © 2013 Elsevier Inc. All rights reserved.","Nanowire fabric; NASIC; On-chip variation sensor; Parameter variation; Random variation"
"Analysis of the impact of spatial and temporal variations on the stability of SRAM arrays and the mitigation technique using independent-gate devices","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899129340&doi=10.1016%2fj.jpdc.2013.07.009&partnerID=40&md5=899f8c85582df7d4747b47a82a3ba01b","As planar MOSFET is approaching its physical scaling limits, FinFET becomes one of the most promising alternative structure to keep on the industry scaling-down trend for future technology generations of 22 nm and beyond. In this paper, we investigate the influence of NBTI degradation induced variation and random process variations on the stability of the FinFET-based 6T-SRAM cell. The contributions of transistor threshold voltage variation ΔVth on the stability of the SRAM cell and the corresponding compensating bias schemes are thoroughly examined by means of SPICE simulations. A mitigation method for memory stability management under spatial and temporal variations is demonstrated by taking advantage of the independent-gate FinFET device structure in order to perform threshold voltage adjustment. The proposed technique allows for a practical compensation strategy able to preserve the SRAM cell stability while balancing performance and leakage power consumption. We demonstrate that the standby leakage current IDDQ value can be utilized to assess the consequences of parameter variations and NBTI on the circuit performance and propose a model that captures this. We evaluate the impact of our proposal on the SRAM cell stability by means of SPICE simulations for 20 nm FinFET devices. Simulation results indicate that the proposed technique can effectively maintain stability of an SRAM array within the desired range during its operational life under both spatial and temporal variations, hence improve the system performance and reliability. Our method allows for maintaining the Static Noise Margin (SNM) degradation of SRAM cells under a certain range, e.g., 2% of fresh device after 1 year operation, which is about 55.56% improvement when compared with the 4.5% degradation corresponding to the uncompensated case. © 2013 Elsevier Inc. All rights reserved.","Dynamic reliability management; Negative Bias Temperature Instability (NBTI); Variation tolerance"
"RRAM-based FPGA for ""normally Off, Instantly On"" applications","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898810841&doi=10.1016%2fj.jpdc.2013.08.003&partnerID=40&md5=34c769d396c355c9d30daa30de5eb8b0","""Normally Off, Instantly On"" applications are becoming common in our environment. They range from healthcare to video surveillance. As the number of applications and their associated performance requirements grow rapidly, more and more powerful, flexible, and power efficient computing units are necessary. In such a context, Field Programmable Gate Arrays (FPGA) architectures present a good trade-off between performance and flexibility. However, they consume high static power and can hardly be associated with power-gating techniques due to their long context-restoring phase. In this paper, we propose to integrate non-volatile resistive memories in the configuration cells and registers in order to instantly restore the FPGA context. If the circuit is in the 'ON' state for less than 42% of time, non-volatile FPGA starts saving energy compared to classical FPGA. Finally, when context-saving functionality is included, for a typical application with only 1% of time spent in the 'ON' state, the energy gain exceeds 40%.© 2013 Elsevier B.V. All rights reserved.","FPGA; Non-volatile; Normally-off, instantly-on; OxRRAM; Power-gating; Resistive RAM"
"Bone structure analysis on multiple GPGPUs","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904967959&doi=10.1016%2fj.jpdc.2014.06.014&partnerID=40&md5=b8cf42df599fb53900642eb076097126","Osteoporosis is a disease that affects a growing number of people by increasing the fragility of their bones. To improve the understanding of the bone quality, large scale computer simulations are applied. A fast, scalable and memory efficient solver for such problems is ParOSol. It uses the preconditioned conjugate gradient algorithm with a multigrid preconditioner. A modification of ParOSol is presented that profits from the exorbitant compute capabilities of recent general-purpose graphics processing units (GPGPUs). Adaptations of data structures for the GPGPU are discussed. The fastest implementation on a GPGPU achieves a speedup of more than five compared with the CPU implementation and scales from 1 to at least 256 GPGPUs. © 2014 Elsevier Inc. All rights reserved.","Element-by-element sparse matrix-vector multiplication; Micro finite element analysis; Multiple GPGPUs; Voxel-based computation"
"Accident aware localization mechanism for wireless sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902675814&doi=10.1016%2fj.jpdc.2014.05.006&partnerID=40&md5=f12b17fca6f87d3dbfd4557da3617944","Accurate location information is important for event reporting, coverage estimation, and location-aware routing in a Wireless Sensor Network (WSN). Recently, a number of range-free localization schemes have been proposed to provide each static sensor with location information, which is represented by a rectangular region. However, most WSN applications are applied in outdoor environments where the sensors' location regions could be incorrect due to sudden accidents. This paper proposes an Active Location Correction Protocol, called ALCP, for detecting and correcting the occurrence of location error based on the bounding box technology. Performance study reveals that applying the ALCP to improve the location accuracies can enhance the performance of the well-known GPSR routing in terms of routing length, sensing coverage, and packet arrival rate. © 2014 Elsevier Inc. All rights reserved.","Bounding box; Localization; Location correction; Wireless sensor networks"
"Group-based memory oversubscription for virtualized clouds","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893193765&doi=10.1016%2fj.jpdc.2014.01.001&partnerID=40&md5=d2cb9cda7da227ea9e2c2fa0f048793c","As memory resource is a primary inhibitor of oversubscribing data centers in virtualized clouds, efficient memory management has been more appealing to public cloud providers. Although memory oversubscription improves overall memory efficiency, existing schemes lack isolation support, which is crucial for clouds to provide pay-per-use services on multi-tenant resource pools. This paper presents group-based memory oversubscription that confines both mechanism and policy of memory oversubscription into a group of virtual machines. A group is specified as one of service level agreements so that a cloud customer can control the memory management mechanism within its own isolated domain. We introduce group-based memory deduplication and reprovisioning with several policies based on per-group workload behaviors. The proposed scheme is implemented on the KVM-based prototype and evaluated with realistic cloud workloads such as MapReduce and MPI applications. The evaluation results show that our group-based memory oversubscription ensures strict inter-group isolation while achieving intra-group memory efficiency, compared to a system-wide scheme, by adapting oversubscription policies based on per-group workload characteristics. © 2014 Elsevier Inc. All rights reserved.","Cloud computing; Memory oversubscription; Resource isolation; Virtualization"
"Design space exploration of on-chip ring interconnection for a CPU-GPU heterogeneous architecture","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885951676&doi=10.1016%2fj.jpdc.2013.07.014&partnerID=40&md5=93289dfe776cf2ec8095fc11513e8402","Incorporating a GPU architecture into CMP, which is more efficient with certain types of applications, is a popular architecture trend in recent processors. This heterogeneous mix of architectures will use an on-chip interconnection to access shared resources such as last-level cache tiles and memory controllers. The configuration of this on-chip network will likely have a significant impact on resource distribution, fairness, and overall performance. The heterogeneity of this architecture inevitably exerts different pressures on the interconnection due to the differing characteristics and requirements of applications running on CPU and GPU cores. CPU applications are sensitive to latency, while GPGPU applications require massive bandwidth. This is due to the difference in the thread-level parallelism of the two architectures. GPUs use more threads to hide the effect of memory latency but require massive bandwidth to supply those threads. On the other hand, CPU cores typically running only one or two threads concurrently are very sensitive to latency. This study surveys the impact and behavior of the interconnection network when CPU and GPGPU applications run simultaneously. Among our findings, we observed that significant interference exists between CPU and GPU applications and resource partitioning, in particular virtual and physical channel partitioning, shows effectiveness to solve the interference problem. Also, heterogeneous link configurations show promising results by optimizing traffic hotspots in the network. Finally, we evaluated different placement policies and found that how to place different components in the network significantly affects the performance. Based on these findings, we suggest an optimal ring interconnect network. Our study will shed light on other architectural interconnection studies on CPU-GPU heterogeneous architectures. © 2013 Elsevier Inc. All rights reserved.","Design space exploration; Heterogeneous architecture; On-chip interconnection network"
"Multi-path utility maximization and multi-path TCP design","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890554074&doi=10.1016%2fj.jpdc.2013.07.005&partnerID=40&md5=93e75d60231dad119c72e785cbba1654","The canonical multi-path network utility maximization (NUM) model which is extended directly from the single-path NUM has been studied widely in the literature. Most of the previous approaches do not specify the case of subflows on paths with different characteristics. Moreover, the transport protocol derived from the canonical multi-path NUM exhibits flappiness in the subflows because of the non-strictly convexity of the optimization problem. This paper introduces a modified multi-path NUM model and proposes a novel approach to overcome the mentioned issues. Using Jensen's inequality, the multi-path NUM is approximated to a strictly convex and separable problem which can be solved efficiently by dual-based decomposition method. The algorithm successively solving a sequence of approximation problems is proven to converge at the global optimum of the original problem. Moreover, considering the separable form of the approximation utility and the dual-based nature of the proposed algorithm, the reverse engineering frameworks of the current TCPs are used to develop a series of multi-path TCPs that are compatible with corresponding regular single-path TCPs. © 2013 Elsevier Inc. All rights reserved.","Multi-path NUM; Multi-path TCP; Successive approximation"
"Oblivious algorithms for multicores and networks of processors","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879123404&doi=10.1016%2fj.jpdc.2013.04.008&partnerID=40&md5=e1395a096ba873bd484d17993b0a05d5","We address the design of algorithms for multicores that are oblivious to machine parameters. We propose HM, a multicore model consisting of a parallel shared-memory machine with hierarchical multi-level caching, and we introduce a multicore-oblivious approach to algorithms and schedulers for HM. A multicore-oblivious algorithm is specified with no mention of any machine parameters, such as the number of cores, number of cache levels, cache sizes and block lengths. However, it is equipped with a small set of instructions that can be used to provide hints to the run-time scheduler on how to schedule parallel tasks. We present efficient multicore-oblivious algorithms for several fundamental problems including matrix transposition, FFT, sorting, the Gaussian Elimination Paradigm, list ranking, and connected components. The notion of a multicore-oblivious algorithm is complementary to that of a network-oblivious algorithm, introduced by Bilardi et al. (2007) [8] for parallel distributed-memory machines where processors communicate point-to-point. We show that several of our multicore-oblivious algorithms translate into efficient network-oblivious algorithms, adding to the body of known efficient network-oblivious algorithms. © 2013 Elsevier Ltd. All rights reserved.","Cache; Gaussian elimination paradigm; List ranking; Multicore; Network; Oblivious algorithm"
"Enabling design and simulation of massive parallel nanoarchitectures","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899100766&doi=10.1016%2fj.jpdc.2013.07.010&partnerID=40&md5=c605ddf67523822de657f87f84f2b530","A common element in emerging nanotechnologies is the increasing complexity of the problems to face when attempting the design phase, because issues related to technology, specific application and architecture must be evaluated simultaneously. In several cases faced problems are known, but require a fresh re-think on the basis of different constraints not enforced by standard design tools. Among the emerging nanotechnologies, the two-dimensional structures based on nanowire arrays is promising in particular for massively parallel architectures. Several studies have been proposed on the exploration of the space of architectural solutions, but only a few derived high-level information from the results of an extended and reliable characterization of low-level structures. The tool we present is of aid in the design of circuits based on nanotechnologies, here discussed in the specific case of nanowire arrays, as best candidate for massively parallel architectures. It enables the designer to start from a standard High-level Description Languages (HDLs), inherits constraints at physical level and applies them when organizing the physical implementation of the circuit elements and of their connections. It provides a complete simulation environment with two levels of refinement. One for DC analysis using a fast engine based on a simple switch level model. The other for obtaining transient performance based on automatic extraction of circuit parasitics, on detailed device (nanowire-FET) information derived by experiments or by existing accurate models, and on spice-level modeling of the nanoarray. Results about the method used for the design and simulation of circuits based on nanowire-FET and nanoarray will be presented. © 2013 Elsevier Inc. All rights reserved.","CAD design tool; Device and parasitic extraction; Massive parallelism; Nanoarchitecture; Nanofabric; Place and route; Simulations; Table model"
"Energy-efficient clustering in lossy wireless sensor networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879612010&doi=10.1016%2fj.jpdc.2013.02.012&partnerID=40&md5=04a3ee22f00ef649e1b27117439c7df7","Recent experimental studies have revealed that a large percentage of wireless links are lossy and unreliable for data delivery in wireless sensor networks (WSNs). Such findings raise new challenges for the design of clustering algorithms in WSNs in terms of data reliability and energy efficiency. In this paper, we propose distributed clustering algorithms for lossy WSNs with a mobile collector, where the mobile collector moves close to each cluster head to receive data directly and then uploads collected data to the base station. We first consider constructing one-hop clusters in lossy WSNs where all cluster members are within the direct communication range of their cluster heads. We formulate the problem into an integer program, aiming at maximizing the network lifetime, which is defined as the number of rounds of data collection until the first node dies. We then prove that the problem is NP-hard. After that, we propose a metric-based distributed clustering algorithm to solve the problem. We adopt a metric called selection weight for each sensor node that indicates both link qualities around the node and its capability of being a cluster head. We further extend the algorithm to multi-hop clustering to achieve better scalability. We have found out that the performance of the one-hop clustering algorithm in small WSNs is very close to the optimal results obtained by mathematical tools. We have conducted extensive simulations for large WSNs and the results demonstrate that the proposed clustering algorithms can significantly improve the data reception ratio, reduce the total energy consumption in the network and prolong network lifetime compared to a typical distributed clustering algorithm, HEED, that does not consider lossy links. © 2013 Elsevier Inc. All rights reserved.","Clustering algorithms; Clusters; Lossy wireless links; Wireless sensor networks"
"GPU-based iterative transmission reconstruction in 3D ultrasound computer tomography","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886011896&doi=10.1016%2fj.jpdc.2013.09.007&partnerID=40&md5=c8e29353afa3068f8a4119415faadb62","As today's standard screening methods frequently fail to detect breast cancer before metastases have developed, early diagnosis is still a major challenge. With the promise of high-quality volume images, three-dimensional ultrasound computer tomography is likely to improve this situation, but has high computational needs. In this work, we investigate the acceleration of the ray-based transmission reconstruction by a GPU-based implementation of the iterative numerical optimization algorithm TVAL3. We identified the regular and transposed sparse-matrix-vector multiply as the performance limiting operations. For accelerated reconstruction we propose two different concepts and devise a hybrid scheme as optimal configuration. In addition we investigate multi-GPU scalability and derive the optimal number of devices for our two primary use-cases: a fast preview mode and a high-resolution mode. In order to achieve a fair estimation of the speedup, we compare our implementation to an optimized CPU version of the algorithm. Using our accelerated implementation we reconstructed a preview 3D volume with 24,576 unknowns, a voxel size of (8 mm)3 and approximately 200,000 equations in 0.5 s. A high-resolution volume with 1,572,864 unknowns, a voxel size of (2mm)3 and approximately 1.6 million equations was reconstructed in 23 s. This constitutes an acceleration of over one order of magnitude in comparison to the optimized CPU version. © 2013 Elsevier Inc. All rights reserved.","Application acceleration; GPU; Numerical optimization; SpMV; Ultrasound imaging"
"Dual partitioning multicasting for high-performance on-chip networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890566220&doi=10.1016%2fj.jpdc.2013.07.002&partnerID=40&md5=3ecae9ea53cb086418444b63c8d5e66d","As the number of cores integrated onto a single chip increases, power dissipation and network latency become ever-increasingly stringent. On-chip network provides an efficient and scalable interconnection paradigm for chip multiprocessors (CMPs), wherein one-to-many (multicast) communication is universal for such platforms. Without efficient multicasting support, traditional unicasting on-chip networks will be low efficiency in tackling such multicast communication. In this paper, we propose Dual Partitioning Multicasting (DPM) to reduce packet latency and balance network resource utilization. Specifically, DPM scheme adaptively makes routing decisions based on the network load-balance level as well as the link sharing patterns characterized by the distribution of the multicasting destinations. Extensive experimental results for synthetic traffic as well as real applications show that compared with the recently proposed RPM scheme, DPM significantly reduces the average packet latency and mitigates the network power consumption. More importantly, DPM is highly scalable for future on-chip networks with heavy traffic load and varieties of traffic patterns. © 2013 Elsevier Inc. All rights reserved.","Latency-aware; Load-balance; Multicast routing; On-chip network; Rectilinear Steiner tree"
"Google hostload prediction based on Bayesian model with optimized feature combination","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890460297&doi=10.1016%2fj.jpdc.2013.10.001&partnerID=40&md5=8faa704957eafdaf161aeebcf24ee200","We design a novel prediction method with Bayes model to predict a load fluctuation pattern over a long-term interval, in the context of Google data centers. We exploit a set of features that capture the expectation, trend, stability and patterns of recent host loads. We also investigate the correlations among these features and explore the most effective combinations of features with various training periods. All of the prediction methods are evaluated using Google trace with 10,000+ heterogeneous hosts. Experiments show that our Bayes method improves the long-term load prediction accuracy by 5.6%-50%, compared to other state-of-the-art methods based on moving average, auto-regression, and/or noise filters. Mean squared error of pattern prediction with Bayes method can be approximately limited in [10-8,10 -5]. Through a load balancing scenario, we confirm the precision of pattern prediction in finding a set of idlest/busiest hosts from among 10,000+ hosts can be improved by about 7% on average. © 2013 Elsevier Inc. All rights reserved.","Bayesian model; Google data center; Hostload prediction"
"Online auto-tuning for the time-step-based parallel solution of ODEs on shared-memory systems","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902296690&doi=10.1016%2fj.jpdc.2014.03.006&partnerID=40&md5=ebbe5052a60f61212721636fffe74787","This article considers automatic performance tuning of time-step-based parallel solution methods for initial value problems (IVPs) of systems of ordinary differential equations (ODEs). We apply auto-tuning to the parallel execution of a class of explicit predictor-corrector (PC) methods of Runge-Kutta (RK) type on shared-memory architectures. The performance of parallel multi-threaded implementation variants of these methods depends on various factors only known at runtime, for example, the coupling structure of the ODE system to be solved, the memory access pattern resulting from this coupling structure, and the number of threads executing the program. We propose an online auto-tuning approach that exploits the time-stepping nature of ODE methods by selecting the best parallel implementation variant from a set of candidate implementations at runtime during the first time steps. Thus, the auto-tuning process is not isolated from the computation, but rather contributes to the progress of the solution process. The search space of candidate implementations is a priori reduced by estimating the synchronization overhead of each implementation variant. For implementation variants containing tiled loops, suitable tile sizes are selected using a heuristic empirical search guided by an analytical model. Runtime experiments with two different test problems show the efficiency of the online auto-tuning approach on two different shared-memory systems equipped with 48 and 1040 cores. © 2014 Elsevier Inc. All rights reserved.","Auto-tuning; Ordinary differential equations; Parallelization; Predictor-corrector methods; Tile size selection"
"A survey of Cloud monitoring tools: Taxonomy, capabilities and objectives","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905003324&doi=10.1016%2fj.jpdc.2014.06.007&partnerID=40&md5=16cad4a776d02853dd15aca2e9e87a3d","The efficient management of Cloud infrastructure and deployments is a topic that is currently attracting significant interest. Complex Cloud deployments can result in an intricate layered structure. Understanding the behaviour of these hierarchical systems and how to manage them optimally are challenging tasks that can be facilitated by pervasive monitoring. Monitoring tools and techniques have an important role to play in this area by gathering the information required to make informed decisions. A broad variety of monitoring tools are available, from general-purpose infrastructure monitoring tools that predate Cloud computing, to high-level application monitoring services that are themselves hosted in the Cloud. Surveying the capabilities of monitoring tools can identify the fitness of these tools in serving certain objectives. Monitoring tools are essential components to deal with various objectives of both Cloud providers and consumers in different Cloud operational areas. We have identified the practical capabilities that an ideal monitoring tool should possess to serve the objectives in these operational areas. Based on these identified capabilities, we present a taxonomy and analyse the monitoring tools to determine their strength and weaknesses. In conclusion, we present our reflections on the analysis, discuss challenges and identify future research trends in the area of Cloud monitoring. © 2014 Elsevier Inc. All rights reserved.","Capabilities; Cloud management; Cloud operational areas; Monitoring tools; Survey; Taxonomy"
"Modeling and simulation of a nanoscale optical computing system","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899129617&doi=10.1016%2fj.jpdc.2013.07.006&partnerID=40&md5=d86163ae7e8c06680171da6e206dcfa7","Optical nanoscale computing is one promising alternative to the CMOS process. In this paper we explore the application of Resonance Energy Transfer (RET) logic to common digital circuits. We propose an Optical Logic Element (OLE) as a basic unit from which larger systems can be built. An OLE is a layered structure that works similar to a lookup table but instead uses wavelength division multiplexing for its inputs and output. Waveguides provide a convenient mechanism to connect multiple OLEs into large circuits. We build a SPICE model from first principles for each component to estimate the timing and power behavior of the OLE system. We analyze various logic circuits and the simulation results show that the components are theoretically correct and that the models faithfully reproduce the fundamental phenomena; the power-delay product of OLE systems is at least 2.5× less than the 14 nm CMOS technology with 100× better density. © 2013 Elsevier Inc. All rights reserved.","DNA self-assembly; Emerging technology; Nanophotonics; Optical computing; RET logic; SPICE"
"What is ahead for parallel computing","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901424062&doi=10.1016%2fj.jpdc.2014.02.005&partnerID=40&md5=7ce4a1824eb56f28f750a5b16bf7c351","With the industry-wide switch to multicore and manycore architectures, parallel computing has become the only venue in sight for continued growth in application performance. In order for the performance of an application to grow with future generations of hardware, a significant portion of its computation must be done with scalable parallel algorithms. It is therefore important to develop and deploy as many scalable parallel algorithms as possible. This paper takes a critical look at the major challenges involved in the development of scalable parallel algorithms and points to needs for compiler tool innovations to help address these challenges. © 2014 Elsevier Inc. All rights reserved.","Algorithm library; Algorithm optimization; Data layout; GPU; Locality; Memoryicore bandwidth; Multicore; Parallel algorithms; Parallel data structures"
"Self-stabilizing with service guarantee construction of 1-hop weight-based bounded size clusters","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890550278&doi=10.1016%2fj.jpdc.2013.09.004&partnerID=40&md5=8358828ae508d4d48c0208fae092388c","This paper makes contributions in two areas. First, we introduce an extended approach of self-stabilization, called self-stabilization with service guarantee. A self-stabilizing system tolerates transient faults: it automatically recovers to a correct behavior after a stabilization period. However, during stabilization periods, no property on the system behavior is guaranteed. A self-stabilizing protocol with service guarantee quickly provides a useful minimal service, and it maintains the minimal service during stabilization despite the occurrence of some disruptions. To illustrate our approach, we propose a clustering protocol (called SG-BSC), that builds 1-hop, bounded size and weight based clusters. SG-BSC protocol is self-stabilizing with service guarantee: it quickly reaches, in 4 rounds, a safe configuration from any arbitrary one. In a safe configuration, the following useful minimal service is provided: ""each node belongs to a cluster, and each cluster has a leader and at most SizeBound ordinary nodes that are at most at distance 1 from their cluster-head"". The convergence to a legitimate configuration (optimum service) is done in at most 7*N/2+4 rounds, where N is the number of nodes. We prove that any self-stabilizing protocol building weight-based clusters requires O(N) rounds to stabilize. Once the optimum service is reached, any cluster-head has the highest weight in its cluster, and the number of clusters is locally optimal. During the stabilization period, the minimal service is preserved; so, the hierarchical organization stays available throughout the entire network. Simulation results show the interest of self-stabilization with service guarantee compared to self-stabilization. © 2013 Elsevier Inc. All rights reserved.","Bounded clusters; Clustering; Minimal service; Safety; Self-stabilization; Service guarantee"
"Efficient data partitioning for the GPU computation of moment functions","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890534841&doi=10.1016%2fj.jpdc.2013.07.008&partnerID=40&md5=7bb6db2917f66591dbd92c1284023e06","In our previous work, we have provided tools for an efficient characterization of biomedical images using Legendre and Zernike moments, showing their relevance as biomarkers for classifying image tiles coming from bone tissue regeneration studies (Ujaldón, 2009) [24]. As part of our research quest for efficiency, we developed methods for accelerating those computations on GPUs (Martín-Requena and Ujaldón, 2011) [10,9]. This new stage of our work focuses on the efficient data partitioning to optimize the execution on many-cores and clusters of GPUs to attain gains up to three orders of magnitude when compared to the execution on multi-core CPUs of similar age and cost using 1 Mpixel images. We deploy a successive and successful chain of optimizations which exploit symmetries in trigonometric functions and access patterns to image pixels which are effectively combined with massive data parallelism on GPUs to enable (1) real-time processing for our set of input biomedical images, and (2) the use of high-resolution images in clinical practice. © 2013 Elsevier Inc. All rights reserved.","Data partitioning; GPU; High performance computing; Image features; Zernike moments"
"Amdahl's law for multithreaded multicore processors","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904073738&doi=10.1016%2fj.jpdc.2014.06.012&partnerID=40&md5=7f6757a0daf1e4963de3785ce5c2a2f9","In this paper, we conduct performance scaling analysis of multithreaded multicore processors (MMPs) for parallel computing. We propose a thread-level closed-queuing network model covering a fairly large design space, accounting for hardware scaling models, coarse-grain, fine-grain, and simultaneous multithreading (SMT) cores, shared resources, including cache, memory, and critical sections. We then derive a closed-form solution for this model in terms of speedup performance measure. This solution makes it possible to analyze performance scaling properties of MMPs along multiple dimensions. In particular, we show that for the parallelizable part of the workload, the speedup, in the absence of resource contention, is no longer just a linear function of parallel processing unit counts, as predicted by Amdahl's law, but also a strong function of workload characteristics, ranging from strong memory-bound to strong CPU-bound workloads. We also find that with core multithreading, super linear speedup, higher than that predicted by Amdahl's law, may be achieved for the parallelizable part of the workload, if core threads exhibit strong cache affinity and the workload is strongly memory-bound. Then, we derive a tight speedup upper bound in the presence of both memory resource contention and critical section for multicore processors with single-threaded cores. This speedup upper bound indicates that with resource contention among threads, whether it is due to shared memory or critical section, a sequential term is guaranteed to emerge from the parallelizable part of the workload, fundamentally limiting the scalability of multicore processors for parallel computing, in addition to the sequential part of the workload, as dictated by Amdahl's law. As a result, to improve speedup performance for MMPs, one should strive to enhance memory parallelism and confine critical sections as locally as possible, e.g., to the smallest possible number of threads in the same core. © 2014 Elsevier Inc. All rights reserved.","Amdahl's law; Closed queuing network; Multithreaded multicore processor; Speedup"
"Approximate Byzantine consensus in sparse, mobile ad-hoc networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903558200&doi=10.1016%2fj.jpdc.2014.05.005&partnerID=40&md5=feadb45494e752200bfc860021b5558d","We consider the problem of approximate consensus in mobile ad-hoc networks in the presence of Byzantine nodes. Due to nodes' mobility, the topology is dynamic. We propose a protocol based on the linear iteration method. The nodes collect information during several consecutive rounds: moving gives them the opportunity to gather progressively enough values. A novel sufficient and necessary condition guarantees the final convergence: from time to time only the correct nodes that own a value equal to (or very close to) either the minimum or the maximum value have to receive enough messages (quantity constraint) with either higher or lower values (quality constraint). Of course, nodes' motion should not prevent this requirement to be fulfilled. New concepts are introduced to prove the correctness of the protocol. Based on particular mobility scenarios, simulations are conducted to analyze the impact of some parameters on three variants of the protocol. © 2014 Elsevier Inc. All rights reserved.","Ad-hoc network; Approximate consensus; Mobility"
"Optimal number of annuli for maximizing the lifetime of sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886041971&doi=10.1016%2fj.jpdc.2013.09.010&partnerID=40&md5=540a5a04c653136408aef3a9fcb8a4ed","The most effective way to maximize the lifetime of a wireless sensor network (WSN) is to allocate initial energy to sensors such that they exhaust their energy at the same time. The lifetime of a WSN as well as an optimal initial energy allocation are determined by a network design. The main contribution of the paper is to show that the lifetime of a WSN can be maximized by an optimal network design. We represent the network lifetime as a function of the number m of annuli and show that m has significant impact on network lifetime. We prove that if the energy consumed by data transmission is proportional to dα+c, where d is the distance of data transmission and α and c are some constants, then for a circular area of interest with radius R, the optimal number of annuli that maximizes the network lifetime is m=R((α-1)/c)1/α for an arbitrary sensor density function. © 2013 Elsevier Inc. All rights reserved.","Lifetime maximization; Network design; Optimal energy allocation; Wireless sensor network"
"Obtaining the optimal configuration of high-radix Combined switches","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879283305&doi=10.1016%2fj.jpdc.2013.04.009&partnerID=40&md5=b75ff16f2d9c83c60c838c96f2987a70","High-radix switches reduce network cost and improve network performance, especially in large switch-based interconnection networks. However, there are some problems related to the integration scale to implement such switches in a single chip. An interesting alternative for building high-radix switches consists of combining several current smaller single-chip switches to obtain switches with a greater number of ports. A key design issue of this kind of high-radix switches is the internal switch configuration, specifically, the correspondence between the ports of these high-radix switches and the ports of their smaller internal single-chip switches. In this paper we use artificial intelligence and data mining techniques in order to obtain the optimal internal configuration of all the switches in the network of large supercomputers running parallel applications. Simulation results show that using the resultant switch configurations, it is possible to achieve similar performance as with single-chip switches with the same radix, which would be unfeasible with the current integration scale. © 2013 Elsevier Inc. All rights reserved.","High-radix switches; Interconnection networks; Parallel computing systems; Search algorithms"
"TLA: Temporal look-ahead processor allocation method for heterogeneous multi-cluster systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885954138&doi=10.1016%2fj.jpdc.2013.07.018&partnerID=40&md5=fb7b1558a86d09762f8edf3ffc966458","In a heterogeneous multi-cluster (HMC) system, processor allocation is responsible for choosing available processors among clusters for job execution. Traditionally, processor allocation in HMC considers only resource fragmentation or processor heterogeneity, which leads to heuristics such as Best-Fit (BF) and Fastest-First (FF). However, those heuristics only favor certain types of workloads and cannot be changed adaptively. In this paper, a temporal look-ahead (TLA) method is proposed, which uses an allocation simulation process to guide the decision of processor allocation. Thus, the allocation decision is made dynamically according to the current workload and system configurations. We evaluate the performance of TLA by simulations, with different workloads and system configurations, in terms of average turnaround time. Simulation results indicate that, with precise runtime information, TLA outperforms traditional processor allocation methods and has up to an 87% performance improvement. © 2013 Elsevier Inc. All rights reserved.","Heterogeneity; Look-ahead; Multi-cluster; Parallel job scheduling; Processor allocation"
"Symbol-level reliable broadcasting of sensitive data in error-prone wireless networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901610417&doi=10.1016%2fj.jpdc.2014.02.004&partnerID=40&md5=3735d393c5bc584639a0f8dd05f247c3","Reliable packet transmission over error-prone wireless networks has received a lot of attention from the research community. In this paper, instead of using simple packet retransmissions to provide reliability, we consider a novel retransmission approach, which is based on the importance of bits (symbols). We study the problem of maximizing the total gain in the case of partial data delivery in error-prone wireless networks, in which each set of bits (called symbols) has a different weight. We first address the case of one-hop single packet transmission, and prove that the optimal solution that maximizes the total gain has a round-robin symbol transmission pattern. Then, we extend our solution to the case of multiple packets. We also enhance the expected gain using random linear network coding. Our simulation results show that our proposed multiple packets transmission mechanism can increase the gain up to 60%, compared to that of a simple retransmission. Moreover, our network coding scheme enhances the expected total gain up to 15%, compared to our non-coding mechanism. © 2014 Elsevier Inc. All rights reserved.","Broadcasting; Error-prone channel; Random linear network coding; Reliability; Symbol-level coding; Weight; Wireless networks"
"Adaptive thread mapping strategies for transactional memory applications","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903137827&doi=10.1016%2fj.jpdc.2014.05.008&partnerID=40&md5=5972c8119ac3710e0ccdccea9440c4d6","Transactional Memory (TM) is a programmer friendly alternative to traditional lock-based concurrency. Although it intends to simplify concurrent programming, the performance of the applications still relies on how frequent they synchronize and the way they access shared data. These aspects must be taken into consideration if one intends to exploit the full potential of modern multicore platforms. Since these platforms feature complex memory hierarchies composed of different levels of cache, applications may suffer from memory latencies and bandwidth problems if threads are not properly placed on cores. An interesting approach to efficiently exploit the memory hierarchy is called thread mapping. However, a single fixed thread mapping cannot deliver the best performance when dealing with a large range of transactional workloads, TM systems and platforms. In this article, we propose and implement in a TM system a set of adaptive thread mapping strategies for TM applications to tackle this problem. They range from simple strategies that do not require any prior knowledge to strategies based on Machine Learning techniques. Taking the Linux default strategy as baseline, we achieved performance improvements of up to 64.4% on a set of synthetic applications and an overall performance improvement of up to 16.5% on the standard STAMP benchmark suite. © 2014 Elsevier Inc. All rights reserved.","Adaptivity; Multicore; Thread mapping; Transactional memory"
"A few bad ideas on the way to the triumph of parallel computing","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901621301&doi=10.1016%2fj.jpdc.2013.10.006&partnerID=40&md5=4673de269f3f697cab37543c5d262781","Parallelism has become mainstream, in the multicore chip, the GPU, and the internet datacenter running MapReduce. In my field, large-scale scientific computing, parallelism now reigns triumphant. It was no simple, direct route that led to this triumph. Along the way, we were confused by ideas that, in retrospect, turned out to be distractions and errors. The thinking behind them was reasonable, but wrong. One can learn from a dissection of mistakes, so I will retell part of the story here. © 2013 Elsevier Inc. All rights reserved.","Accelerators; Amdahl; Automatic parallelization; Exascale; Parallelism"
"EnhancedBit: Unleashing the potential of the unchoking policy in the BitTorrent protocol","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890459996&doi=10.1016%2fj.jpdc.2013.08.008&partnerID=40&md5=f20e00b0b95ad8f13c8086a716471ead","In this paper, we propose a modification to the BitTorrent protocol related to its peer unchoking policy. In particular, we apply a novel optimistic unchoking approach that improves the quality of inter-connections amongst peers, i.e., increases the number of directly-connected and interested-in-cooperation peers without penalizing underutilized and/or idle peers. Our optimistic unchoking policy takes into consideration the number of clients currently interested in downloading from a peer that is to be unchoked. Our conjecture is that peers having few clients interested in downloading data from them, should be favored with optimistic unchoke intervals. This enables the peers in question to receive data since they become unchoked faster and in turn, they will trigger the interest of additional clients. In contrast, peers with plenty of ""interested"" clients should enjoy a lower priority to be selected as planned optimistic unchoked, since these peers likely have enough data to forward; nevertheless, they receive enough data due to tit-for-tat peer reciprocation and are not in need of optimistic unchoking slots. Armed with this realization, we establish an analytical model and prove a significant performance improvement under our modified BitTorrent protocol. Experimental results, also, indicate that our approach significantly outperforms the existing optimistic unchoking policy in three important aspects: first, there is a higher number of interested-in-cooperation and directly-connected peers. Second, since leechers now act as data intermediaries, the load on seeders eases up considerably. Last, a shorter bootstrapping period for fresh peers is achieved. Hence, we claim that our approach helps implement an enhanced BitTorrent protocol and we name it ""EnhancedBit"".","BitTorrent; Content distribution; Incentive protocols; Peer-to-peer (P2P)"
"A block-asynchronous relaxation method for graphics processing units","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885954257&doi=10.1016%2fj.jpdc.2013.05.008&partnerID=40&md5=f6289a9d5e4970de7c8a9294aff105ce","In this paper, we analyze the potential of asynchronous relaxation methods on Graphics Processing Units (GPUs). We develop asynchronous iteration algorithms in CUDA and compare them with parallel implementations of synchronous relaxation methods on CPU- or GPU-based systems. For a set of test matrices from UFMC we investigate convergence behavior, performance and tolerance to hardware failure. We observe that even for our most basic asynchronous relaxation scheme, the method can efficiently leverage the GPUs computing power and is, despite its lower convergence rate compared to the Gauss-Seidel relaxation, still able to provide solution approximations of certain accuracy in considerably shorter time than Gauss-Seidel running on CPUs- or GPU-based Jacobi. Hence, it overcompensates for the slower convergence by exploiting the scalability and the good fit of the asynchronous schemes for the highly parallel GPU architectures. Further, enhancing the most basic asynchronous approach with hybrid schemes-using multiple iterations within the ""subdomain"" handled by a GPU thread block-we manage to not only recover the loss of global convergence but often accelerate convergence of up to two times, while keeping the execution time of a global iteration practically the same. The combination with the advantageous properties of asynchronous iteration methods with respect to hardware failure identifies the high potential of the asynchronous methods for Exascale computing. © 2013 Elsevier Inc. All rights reserved.","Asynchronous relaxation; Chaotic iteration; Graphics processing units (GPUs); Jacobi method"
"Online traffic-aware fault detection for networks-on-chip","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890562136&doi=10.1016%2fj.jpdc.2013.09.001&partnerID=40&md5=3b2321147befa979d97bbd8795f3c56a","A key requirement for modern Networks-on-Chip (NoC) is the ability to detect and diagnose faults and failures. This paper addresses the challenge of fault diagnosis using online testing where the interruption of the runtime operation (performance) under diagnosis is minimised. A novel Monitor Module (MM) is proposed to detect NoC interconnect faults which minimise the intrusion of the regular NoC traffic throughput by (1) using a channel tester which only examines NoC channels when they are idle; and (2) using a testing interval parameter based on the Binary Exponential Back off algorithm to dynamically balance the level of testing when recovering from temporary faults. The paper presents results on the minimal impact on NoC throughput for a range of testing conditions and also highlights the minimal area overhead of the MM (11.56%) compared with an adaptive NoC router implemented on FPGA hardware. Simulation results demonstrate non-intrusion of the NoC runtime traffic throughput when channel are fault free, and also how throughput loss is minimised when faults are identified. © 2013 Elsevier Inc. All rights reserved.","Adaptive fault detection scheme; Interconnect crosstalk fault detection; Networks-on-chip; Online fault testing"
"Causality, influence, and computation in possibly disconnected synchronous dynamic networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890549219&doi=10.1016%2fj.jpdc.2013.07.007&partnerID=40&md5=13c375d8339a71d8f35d77fd441bf1f8","In this work, we study the propagation of influence and computation in dynamic distributed computing systems that are possibly disconnected at every instant. We focus on a synchronous message-passing communication model with broadcast and bidirectional links. Our network dynamicity assumption is a worst-case dynamicity controlled by an adversary scheduler, which has received much attention recently. We replace the usual (in worst-case dynamic networks) assumption that the network is connected at every instant by minimal temporal connectivity conditions. Our conditions only require that another causal influence occurs within every time window of some given length. Based on this basic idea, we define several novel metrics for capturing the speed of information spreading in a dynamic network. We present several results that correlate these metrics. Moreover, we investigate termination criteria in networks in which an upper bound on any of these metrics is known. We exploit our termination criteria to provide efficient (and optimal in some cases) protocols that solve the fundamental counting and all-to-all token dissemination (or gossip) problems. © 2013 Elsevier Inc. All rights reserved.","Adversarial schedule; Counting; Dynamic graph; Information dissemination; Mobile computing; Optimal protocol; Temporal connectivity; Termination; Worst-case dynamicity"
"PMSS: A programmable memory system and scheduler for complex memory patterns","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904081816&doi=10.1016%2fj.jpdc.2014.06.005&partnerID=40&md5=0aab7b089c7b204bf00a22d78ec7952d","HPC industry demands more computing units on FPGAs, to enhance the performance by using task/data parallelism. FPGAs can provide its ultimate performance on certain kernels by customizing the hardware for the applications. However, applications are getting more complex, with multiple kernels and complex data arrangements, generating overhead while scheduling/managing system resources. Due to this reason all classes of multi threaded machines- minicomputer to supercomputer-require to have efficient hardware scheduler and memory manager that improves the effective bandwidth and latency of the DRAM main memory. This architecture could be a very competitive choice for supercomputing systems that meets the demand of parallelism for HPC benchmarks. In this article, we proposed a Programmable Memory System and Scheduler (PMSS), which provides high speed complex data access pattern to the multi threaded architecture. This proposed PMSS system is implemented and tested on a Xilinx ML505 evaluation FPGA board. The performance of the system is compared with a microprocessor based system that has been integrated with the Xilkernel operating system. Results show that the modified PMSS based multi-accelerator system consumes 50% less hardware resources, 32% less on-chip power and achieves approximately a 19x speedup compared to the MicroBlaze based system. © 2014 Elsevier Inc. All rights reserved.","DRAM; FPGA; HPC; Xilkernel"
"Using an adversary simulator to evaluate global EDF scheduling of sporadic task sets on multiprocessors","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904073753&doi=10.1016%2fj.jpdc.2014.06.011&partnerID=40&md5=6e453ff51c264d9f5b99bc9e81810e8e","Schedulability analysis of real-time multiprocessor systems is usually based on sufficient but not necessary tests that produce pessimistic results. One difficulty in evaluating the effectiveness of sufficient schedulability tests has been distinguishing the cause of a task set failing the test, i.e., finding out whether the task set is in fact not schedulable or it is actually schedulable but the test itself is too pessimistic. Necessary schedulability tests help to distinguish between these two situations, since if a task set fails in the test then it is guaranteed to be unschedulable. An adversary simulator is a scheduling simulator that uses the non-determinism of the task model to generate scenarios that will stress a specific scheduling algorithm, improving the odds of a deadline miss. In this paper we describe a new adversary simulator algorithm for sporadic task sets executed on multiprocessors scheduled by Global Earliest Deadline First (G-EDF). It is shown that this new adversary simulator is more effective as a necessary test than existing approaches. We also estimate the uncertainty regarding G-EDF by applying to the same task sets a well-known sufficient schedulability test from the literature and the necessary schedulability test based on the adversary simulator. © 2014 Elsevier Inc. All rights reserved.","Adversary simulator; Global EDF; Multiprocessor; Real-time"
"A queueing theoretic approach for performance evaluation of low-power multi-core embedded systems","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890552189&doi=10.1016%2fj.jpdc.2013.07.003&partnerID=40&md5=27f120274a4bf23169d15a8ecb924405","With Moore's law supplying billions of transistors on-chip, embedded systems are undergoing a transition from single-core to multi-core to exploit this high transistor density for high performance. However, the optimal layout of these multiple cores along with the memory subsystem (caches and main memory) to satisfy power, area, and stringent real-time constraints is a challenging design endeavor. The short time-to-market constraint of embedded systems exacerbates this design challenge and necessitates the architectural modeling of embedded systems to reduce the time-to-market by expediting target applications to device/architecture mapping. In this paper, we present a queueing theoretic approach for modeling multi-core embedded systems that provides a quick and inexpensive performance evaluation both in terms of time and resources as compared to the development of multi-core simulators and running benchmarks on these simulators. We verify our queueing theoretic modeling approach by running SPLASH-2 benchmarks on the SuperESCalar simulator (SESC). Results reveal that our queueing theoretic model qualitatively evaluates multi-core architectures accurately with an average difference of 5.6% as compared to the architectures' evaluations from the SESC simulator. Our modeling approach can be used for performance per watt and performance per unit area characterizations of multi-core embedded architectures, with varying number of processor cores and cache configurations, to provide a comparative analysis. © 2013 Elsevier Inc. All rights reserved.","Embedded systems; Low-power; Multi-core; Performance evaluation; Queueing theory"
"Exploiting hierarchy parallelism for molecular dynamics on a petascale heterogeneous system","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885953712&doi=10.1016%2fj.jpdc.2013.07.015&partnerID=40&md5=11ecb187e51073038d8d32272361a09e","Heterogeneous systems with nodes containing more than one type of computation units, e.g., central processing units (CPUs) and graphics processing units (GPUs), are becoming popular because of their low cost and high performance. In this paper, we have developed a Three-Level Parallelization Scheme (TLPS) for molecular dynamics (MD) simulation on heterogeneous systems. The scheme exploits multi-level parallelism combining (1) inter-node parallelism using spatial decomposition via message passing, (2) intra-node parallelism using spatial decomposition via dynamically scheduled multi-threading, and (3) intra-chip parallelism using multi-threading and short vector extension in CPUs, and employing multiple CUDA threads in GPUs. By using a hierarchy of parallelism with optimizations such as communication hiding intra-node, and memory optimizations in both CPUs and GPUs, we have implemented and evaluated a MD simulation on a petascale heterogeneous supercomputer TH-1A. The results show that MD simulations can be efficiently parallelized with our TLPS scheme and can benefit from the optimizations. © 2013 Elsevier Inc. All rights reserved.","GPU computing; Heterogeneous system; Molecular dynamics"
"Design and analysis of crossbar architecture based on complementary resistive switching non-volatile memory cells","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899103419&doi=10.1016%2fj.jpdc.2013.08.004&partnerID=40&md5=0954eae1f8e709d9e5df9bbadf577c2f","Emerging non-volatile memories (e.g. STT-MRAM, OxRRAM and CBRAM) based on resistive switching are under intense research and development investigation by both academics and industries. They provide high performance such as fast write/read speed, low power and good endurance (e.g. >1012), and could be used as both computing and storage memories beyond flash memories. However the conventional access architecture based on 1 transistor + 1 memory cell limits its storage density as the selection transistor should be large enough to ensure enough current for the switching operation. This paper presents the design and analysis of crossbar architecture based on complementary resistive switching non-volatile memory cells with a particular focus on reliability and power performance investigation. This architecture allows fewer selection transistors, and minimum contacts between memory cells and CMOS control circuits. The complementary cell and parallel data sensing mitigate the impact of sneak currents in the crossbar architecture and provide fast data access for computing purpose. We perform transient and statistical simulations based on two memory technologies: STT-MRAM and OxRRAM to validate the functionality of this design by using CMOS 40 nm design kit and memory compact models, which were developed based on relative physics and experimental parameters. © 2013 Elsevier Inc. All rights reserved.","Complementary cell; Crossbar array; Non-volatile; Parallel sensing; Performance analysis; Resistive switching; Sneak current mitigation"
"Optimized FFT computations on heterogeneous platforms with application to the Poisson equation","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902291410&doi=10.1016%2fj.jpdc.2014.03.009&partnerID=40&md5=a32558c04ebb8036f83599f04a6bea8b","We develop optimized multi-dimensional FFT implementations on CPU-GPU heterogeneous platforms for the case when the input is too large to fit on the GPU global memory, and use the resulting techniques to develop a fast Poisson solver. The solver involves memory bound computations for which the large 3D data may have to be transferred over the PCIe bus several times during the computation. We develop a new strategy to decompose and allocate the computation between the GPU and the CPU such that the 3D data is transferred only once to the device memory, and the executions of the GPU kernels are almost completely overlapped with the PCI data transfer. We were able to achieve significantly better performance than what has been reported in previous related work, including over 145 GFLOPS for the three periodic boundary conditions (single precision version), and over 105 GFLOPS for the two periodic, one Neumann boundary conditions (single precision version). The effective bidirectional PCIe bus bandwidth achieved is 9-10 GB/s, which is close to the best possible on our platform. For all the cases tested, the single 3D data PCIe transfer time, which constitutes a lower bound on what is possible on our platform, takes almost 70% of the total execution time of the Poisson solver. © 2014 Elsevier Inc. All rights reserved.","CUDA GPU; Fast Fourier transforms; Parallel and vector implementations; Poisson equations"
"O (log m.log N) routing algorithm for (2log N-1) -stage switching networks and beyond","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903870610&doi=10.1016%2fj.jpdc.2014.06.004&partnerID=40&md5=7ee7f1b9abc78222ca8b1b5c00f6f92a","This paper addresses routing algorithm for a classic network called rearrangeable network with a complexity which is minimum than any other reported algorithms in this class. A new routing algorithm is presented for symmetric rearrangeable networks built with 2×2 switching elements. This new algorithm is capable of connection setup for partial permutation, m̄=ρN, where N is the total input numbers and m̄ is the number of active inputs. Overall the serial time complexity of this method is O(NlogN)1 and O(m̄.logN) where all N inputs are active and with m̄<N active inputs respectively. The time complexity of this algorithm in a parallel machine with N completely connected processors is O(log 2N). With m̄ active requests the time complexity goes down to O(logm̄.logN), which is better than the O(log2m̄+logN), reported in the literature for 212[(≤ρ≤;1. In later half of this paper, modified rearrangeable networks have been demonstrated built with bigger switching elements (>2×2) with shorter network depth. Routing algorithm for these new networks have been proposed by modifying the proposed algorithm for smaller switching elements networks. Also we shall look into the application of these networks in optical domain for crosstalk free routing. © 2014 Elsevier Inc. All rights reserved.","Complexity; Interconnection networks; Permutation; Rearrangeable networks; Routing tags"
"Power-aware optimization for heterogeneous multi-tier clusters","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890570840&doi=10.1016%2fj.jpdc.2013.09.003&partnerID=40&md5=05b040a9b307c85a73ce8c4d23b83885","Complex web applications are usually served by multi-tier web clusters. With the growing cost of energy, the importance of reducing power consumption in server systems is now well-known and has become a major research topic. However, most existing research focused solely on homogeneous clusters. This paper addresses the challenge of power management in Heterogeneous Multi-tier Web Clusters. We apply Generalized Benders Decomposition (GBD) to decompose the global optimization problem into small sub-problems. This algorithm achieves the optimal solution in an iterative fashion. The evaluation results show that our algorithm achieves more energy conservation than the previous work. © 2013 Elsevier Inc. All rights reserved.","Generalized Benders Decomposition; Heterogeneous multi-tier clusters; Non-Linear programming; Power management"
"Finding extremal sets on the GPU","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890569873&doi=10.1016%2fj.jpdc.2013.07.004&partnerID=40&md5=39d80aadd1dc32432ebb8290c064c1f3","The extremal sets of a family F of sets consist of all sets of F that are maximal or minimal with respect to the partial order induced by the subset relation in F. In this paper we present efficient parallel GPU-based algorithms, designed under CUDA architecture, for finding the extremal sets of a family F of sets. The complexity analysis of the presented algorithms together with experimental results showing the efficiency and scalability of the approach is provided. © 2013 Elsevier Inc. All rights reserved.","Algorithms; CUDA; Extremal sets; Graphics Processing Unit (GPU)"
"Hyperspherical cluster based distributed anomaly detection in wireless sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890562284&doi=10.1016%2fj.jpdc.2013.09.005&partnerID=40&md5=54d24a65141e1146782541b427915cc6","This article describes a distributed hyperspherical cluster based algorithm for identifying anomalies in measurements from a wireless sensor network, and an implementation on a real wireless sensor network testbed. The communication overhead incurred in the network is minimised by clustering sensor measurements and merging clusters before sending a compact description of the clusters to other nodes. An evaluation on several real and synthetic datasets demonstrates that the distributed hyperspherical cluster-based scheme achieves comparable detection accuracy with a significant reduction in communication overhead compared to a centralised scheme, where all the sensor node measurements are communicated to a central node for processing. © 2013 Elsevier Inc. All rights reserved.","Anomaly detection; Distributed processing; Wireless sensor networks"
"Efficient breadth first search on multi-GPU systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879465869&doi=10.1016%2fj.jpdc.2013.05.007&partnerID=40&md5=78abd7d61b08c2eb6938ac1ee12a48a7","Simple algorithms for the execution of a Breadth First Search on large graphs lead, running on clusters of GPUs, to a situation of load unbalance among threads and un-coalesced memory accesses, resulting in pretty low performances. To obtain a significant improvement on a single GPU and to scale by using multiple GPUs, we resort to a suitable combination of operations to rearrange data before processing them. We propose a novel technique for mapping threads to data that achieves a perfect load balance by leveraging prefix-sum and binary search operations. To reduce the communication overhead, we perform a pruning operation on the set of edges that needs to be exchanged at each BFS level. The result is an algorithm that exploits at its best the parallelism available on a single GPU and minimizes communication among GPUs. We show that a cluster of GPUs can efficiently perform a distributed BFS on graphs with billions of nodes. © 2013 Elsevier Inc. All rights reserved.","BFS; CUDA; Distributed algorithm; GPU; Graph 500 benchmark; Large graphs"
"Static load-balanced routing for slimmed fat-trees","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897115784&doi=10.1016%2fj.jpdc.2014.02.001&partnerID=40&md5=99b5227ac53d83e9aef53f065e661472","Slimmed fat-trees have recently been proposed and deployed to reduce costs in High Performance Computing (HPC) clusters. While existing static routing schemes such as destination-mod-k (D-mod-k) routing are load-balanced and effective for full bisection bandwidth fat-trees, they incur significant load imbalance in many slimmed fat-trees. In this work, we propose a static load balanced routing scheme, called Round-Robin Routing (RRR), for 2- and 3-level extended generalized fat-trees (XGFTs), which represent many fat-tree variations including slimmed fat-trees. RRR achieves near perfect load-balancing for any such XGFT in that links at the same level of a tree carry traffic from almost the same number of source-destination pairs. Our evaluation results indicate that on many slimmed fat-trees, RRR is significantly better than D-mod-k for dense traffic patterns due to its better load-balancing property, but performs worse for sparse patterns. We develop a combined routing scheme that enjoys the strengths of both RRR and D-mod-k by using RRR in conjunction with D-mod-k. The combined routing is a robust load-balanced routing scheme for slimmed fat-trees: it performs similar to D-mod-k for sparse traffic patterns and to RRR for dense patterns. © 2014 Elsevier Inc. All rights reserved.","Fat-tree; Interconnect; Single-path routing; Static routing"
"A new proposal to deal with congestion in InfiniBand-based fat-trees","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887417941&doi=10.1016%2fj.jpdc.2013.09.002&partnerID=40&md5=d29ce9349d39c08aef0a98634dea5429","The overall performance of High-Performance Computing applications may depend largely on the performance achieved by the network interconnecting the end-nodes; thus high-speed interconnect technologies like InfiniBand are used to provide high throughput and low latency. Nevertheless, network performance may be degraded due to congestion; thus using techniques to deal with the problems derived from congestion has become practically mandatory. In this paper we propose a straightforward congestion-management method suitable for fat-tree topologies built from InfiniBand components. Our proposal is based on a traffic-flow-to-service-level mapping that prevents, as much as possible with the resources available in current InfiniBand components (basically Virtual Lanes), the negative impact of the two most common problems derived from congestion: head-of-line blocking and buffer-hogging. We also provide a mathematical approach to analyze the efficiency of our proposal and several ones, by means of a set of analytical metrics. In certain traffic scenarios, we observe up to a 68% of the ideal performance gain that could be achieved in HoL-blocking and buffer-hogging prevention. © 2013 Elsevier Inc. All rights reserved.","Congestion management; Fat-trees; High-performance computing; InfiniBand; Interconnection networks"
"Efficient fault-tolerant collision-free data aggregation scheduling for wireless sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887028975&doi=10.1016%2fj.jpdc.2013.09.011&partnerID=40&md5=b3110349174744a12b07bf7b069c3198","This paper investigates the design of fault-tolerant TDMA-based data aggregation scheduling (DAS) protocols for wireless sensor networks (WSNs). DAS is a fundamental pattern of communication in wireless sensor networks where sensor nodes aggregate and relay data to a sink node. However, any such DAS protocol needs to be cognisant of the fact that crash failures can occur. We make the following contributions: (i) we identify a necessary condition to solve the DAS problem, (ii) we introduce a strong and weak version of the DAS problem, (iii) we show several impossibility results due to the crash failures, (iv) we develop a modular local algorithm that solves stabilising weak DAS and (v) we show, through simulations and an actual deployment on a small testbed, how specific instantiations of parameters can lead to the algorithm achieving very efficient stabilisation. © 2013 Elsevier Inc. All rights reserved.","Collision freedom; Correctness; Crashes; Data aggregation scheduling; Fault tolerance; Impossibility; Wireless sensor networks"
"Wimpy or brawny cores: A throughput perspective","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880392819&doi=10.1016%2fj.jpdc.2013.06.001&partnerID=40&md5=010e869a52a824b69c57f0d6b6792dd1","In this paper, we conduct a coarse-granular comparative analysis of wimpy (i.e., simple) fine-grain multicore processors against brawny (i.e., complex) simultaneous multithreaded (SMT) multicore processors for server applications with strong request-level parallelism. We explore a large design space along multiple dimensions, including the number of cores, the number of threads, and a wide range of workloads. For strong CPU-bound workload, a 2R-core wimpy-multicore processor is found to be on par with an R-core brawny-multicore processor in terms of throughput performance. For strong memory-bound workload, core-level multithreading is largely ineffective for both wimpy-multicore and brawny-multicore processors, except for the case of low core and thread counts per memory/disk interface. For both wimpy-multicore and brawny-multicore, there is an optimal core number at which the highest throughput performance is achieved, which reduces, as the workload becomes deeper memory-bound. Moreover, there is a threshold core number for a wimpy-multicore, beyond which it is outperformed by its brawny-multicore counterpart. These behaviors indicate that brawny-multicores are better choices than wimpy-multicores in terms of throughput performance. © 2013 Elsevier Inc. All rights reserved.","Closed queuing network models; Multicore processors; Performance evaluation; Request-level parallelism; Server applications"
"Fault-tolerant oblivious assignment with m slots in synchronous systems","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901586890&doi=10.1016%2fj.jpdc.2014.02.008&partnerID=40&md5=50a16544cf2a6e12b7f97a69a817b9ee","Preserving anonymity and privacy of customer actions within a complex software system, such as a cloud computing system, is one of the main issues that should be addressed to boost private computation outsourcing. In this paper, we propose a coordination paradigm, namely oblivious assignment with m slots of a resource R © 2014 Elsevier Inc. All rights reserved.","Distributed coordination abstractions; Distributed systems; Failures; Mutual exclusion; Secure computations"
"Balls into non-uniform bins","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891634192&doi=10.1016%2fj.jpdc.2013.10.008&partnerID=40&md5=31ecd60430912d71a5caa7f5a9af3e70","Balls-into-bins games for uniform bins are widely used to model randomised load balancing strategies. Recently, balls-into-bins games have been analysed under the assumption that the selection probabilities for bins are not uniformly distributed. These new models are motivated by properties of many peer-to-peer (P2P) networks. In this paper we consider scenarios in which non-uniform selection probabilities help to balance the load among the bins. While previous evaluations try to find strategies for identical bins, we investigate heterogeneous bins where the ""capacities"" of the bins might differ significantly. We look at the allocation of m balls into n bins of total capacity C where each ball has d random bin choices. For such heterogeneous environments we show that the maximum load remains bounded by lnln(n)/ln(d)+O(1) w.h.p. if the number of balls m equals the total capacity C. Further analytical and simulative results show better bounds and values for the maximum loads in special cases. © 2013 Elsevier Inc. All rights reserved.","Balls-into-bins games; Load balancing; Randomised algorithms"
"Enhancing throughput of the Hadoop Distributed File System for interaction-intensive tasks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902251337&doi=10.1016%2fj.jpdc.2014.03.010&partnerID=40&md5=0107ec57b3894c527becebc56e386b01","The Hadoop Distributed File System (HDFS) is designed to run on commodity hardware and can be used as a stand-alone general purpose distributed file system (Hdfs user guide, 2008). It provides the ability to access bulk data with high I/O throughput. As a result, this system is suitable for applications that have large I/O data sets. However, the performance of HDFS decreases dramatically when handling the operations of interaction-intensive files, i.e., files that have relatively small size but are frequently accessed. The paper analyzes the cause of throughput degradation issue when accessing interaction-intensive files and presents an enhanced HDFS architecture along with an associated storage allocation algorithm that overcomes the performance degradation problem. Experiments have shown that with the proposed architecture together with the associated storage allocation algorithm, the HDFS throughput for interaction-intensive files increases 300% on average with only a negligible performance decrease for large data set tasks. © 2014 Elsevier Inc. All rights reserved.","Cache; HDFS; Hierarchical structure; Interaction intensive task; PSO; Storage allocation algorithm"
"IPACS: Power-aware covering sets for energy proportionality and performance in data parallel computing clusters","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887290146&doi=10.1016%2fj.jpdc.2013.09.006&partnerID=40&md5=84ea0ccf42518575ba44422408d7045d","Energy consumption in datacenters has recently become a major concern due to the rising operational costs and scalability issues. Recent solutions to this problem propose the principle of energy proportionality, i.e., the amount of energy consumed by the server nodes must be proportional to the amount of work performed. For data parallelism and fault tolerance purposes, most common file systems used in MapReduce-type clusters maintain a set of replicas for each data block. A covering subset is a group of nodes that together contain at least one replica of the data blocks needed for performing computing tasks. In this work, we develop and analyze algorithms to maintain energy proportionality by discovering a covering subset that minimizes energy consumption while placing the remaining nodes in low-power standby mode in a data parallel computing cluster. Our algorithms can also discover covering subset in heterogeneous computing environments. In order to allow more data parallelism, we generalize our algorithms so that it can discover k-covering subset, i.e., a set of nodes that contain at least k replicas of the data blocks. Our experimental results show that we can achieve substantial energy saving without significant performance loss in diverse cluster configurations and working environments. © 2013 Elsevier Inc. All rights reserved.","Covering subset; Data parallel computing; Energy proportionality; MapReduce"
"Detecting similarities in virtual machine behavior for cloud monitoring using smoothed histograms","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902272038&doi=10.1016%2fj.jpdc.2014.02.006&partnerID=40&md5=bbd8f414f45328f73ed49964f280a140","The growing size and complexity of cloud systems determine scalability issues for resource monitoring and management. While most existing solutions consider each Virtual Machine (VM) as a black box with independent characteristics, we embrace a new perspective where VMs with similar behaviors in terms of resource usage are clustered together. We argue that this new approach has the potential to address scalability issues in cloud monitoring and management. In this paper, we propose a technique to cluster VMs starting from the usage of multiple resources, assuming no knowledge of the services executed on them. This innovative technique models VMs behavior exploiting the probability histogram of their resources usage, and performs smoothing-based noise reduction and selection of the most relevant information to consider for the clustering process. Through extensive evaluation, we show that our proposal achieves high and stable performance in terms of automatic VM clustering, and can reduce the monitoring requirements of cloud systems. © 2014 Elsevier Inc. All rights reserved.","Bhattacharyya distance; Cloud computing; Histogram smoothing; Spectral clustering; Virtual machine clustering"
"Bitonic sort on a chained-cubic tree interconnection network","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886029425&doi=10.1016%2fj.jpdc.2013.09.008&partnerID=40&md5=69a180f8a3846c577fd5d3dd2a86e528","Bitonic sort is one of the fastest oblivious parallel sorting algorithms known so far. Due to its high modularity, bitonic sort can be mapped to different interconnection networks. In this paper, the bitonic sort algorithm is mapped to the chained-cubic tree (CCT) interconnection network. It is shown that the computation time of the bitonic sort on a CCT (BSCCT) algorithm is O((n/p)×log(np)) and that the communication cost is O(plog2p), assuming that n keys are evenly distributed among p processors that comprise a given CCT network. Simulation is implemented and used to assess the performance of the BSCCT algorithm in terms of computation time, communication cost, message delay, and key comparisons. Simulation results showed that the BSCCT algorithm achieves a speedup that is almost 12-fold relative to a bitonic sort on a single processor, when 1024 processors were used to sort 32M keys. © 2013 Elsevier Inc. All rights reserved.","Bitonic sort; Interconnection networks; Parallel algorithms; Performance evaluation; Sorting and searching"
"Generating data transfers for distributed GPU parallel programs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885949286&doi=10.1016%2fj.jpdc.2013.07.022&partnerID=40&md5=28966031406e7945d623fe8208ddcf37","Nowadays, high performance applications exploit multiple level architectures, due to the presence of hardware accelerators like GPUs inside each computing node. Data transfers occur at two different levels: inside the computing node between the CPU and the accelerators and between computing nodes. We consider the case where the intra-node parallelism is handled with HMPP compiler directives and message-passing programming with MPI is used to program the inter-node communications. This way of programming on such an heterogeneous architecture is costly and error-prone. In this paper, we specifically demonstrate the transformation of HMPP programs designed to exploit a single computing node equipped with a GPU into an heterogeneous HMPP + MPI exploiting multiple GPUs located on different computing nodes. The STEP tool focuses on generating communications combining both powerful static analyses and runtime execution to reduce the volume of communications. Our source-to-source transformation is implemented inside the PIPS workbench. We detail the generated source program of the Jacobi kernel and show that the execution times and speedups are encouraging. At last we give some directions for the improvement of the tool. © 2013 Elsevier Inc. All rights reserved.","Compiler directives; Data transfer; Distributed memory; GPU; Parallel execution; Source-to-source transformation"
"Critical transistors nexus based circuit-level aging assessment and prediction","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899055703&doi=10.1016%2fj.jpdc.2013.08.006&partnerID=40&md5=f3357473f859036d64499096179178de","Accurate age modeling, and fast, yet robust reliability sign-off emerged as mandatory constraints in Integrated Circuits (ICs) design for advanced process technology nodes. In this paper we introduce a novel method to assess and predict the circuit reliability at design time as well as at run-time. The main goal of our proposal is to allow for: (i) design time reliability optimization; (ii) fine tuning of the run-time reliability assessment infrastructure, and (iii) run-time aging assessment. To this end, we propose to select a minimum-size kernel of critical transistors and based on them to assess and predict an IC End-Of-Life (EOL) via two methods: (i) as the sum of the critical transistors end-of-life values, weighted by fixed topology-dependent coefficients, and (ii) by a Markovian framework applied to the critical transistors, which takes into account the joint effects of process, environmental, and temporal variations. The former model exploits the aging dependence on the circuit topology to enable fast run-time reliability assessment with minimum aging sensors requirements. By allowing the performance boundary to vary in time such that both remnant and nonremnant variations are encompassed, and imposing a Markovian evolution, the probabilistic model can be better fitted to various real conditions, thus enabling at design-time appropriate guardbands selection and effective aging mitigation/compensation techniques. The proposed framework has been validated for different stress conditions, under process variations and aging effects, for the ISCAS-85 c499 circuit, in PTM 45 nm technology. From the total of 1526 transistors, we obtained a kernel of 15 critical transistors, for which the set of topology dependent weights were derived. Our simulation results for 15 critical transistors kernel indicate a small approximation error (i.e., mean smaller than 15% and standard deviation smaller than 6%) for the considered circuit estimated end-of-life (EOL), when comparing to the end-of-life values obtained from Cadence simulation, which quantitatively confirm the accuracy of the IC lifetime evaluation. Moreover, as the number of critical transistors determines the area overhead, we also investigated the implications of reducing their number on the reliability assessment accuracy. When only 5 transistors are included into the critical set instead of 15, which results in a 66% area overhead reduction, the EOL estimation accuracy diminished with 18%. This indicates that area vs. accuracy trade-offs are possible, while maintaining the aging prediction accuracy within reasonable bounds. © 2013 Elsevier Inc. All rights reserved.","Circuit aging; Design for reliability; FEOL reliability; Markovian aging model; Transistor aging"
"Energy and transition-aware runtime task scheduling for multicore processors","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879204400&doi=10.1016%2fj.jpdc.2013.05.003&partnerID=40&md5=1ff25b42c08cf8977716ca97b230f140","Many embedded or portable devices have large demands on running real-time applications. The designers start to adopt the multicore processors in these devices. The multi-core processors, however, cause much higher power consumption than ever before. To resolve this problem, many researchers have focused their studies on designing the energy-aware task scheduling algorithms for multicore processors. Conventional scheduling algorithms assumed that each core can operate under different voltage levels. However, they have not considered the effects of voltage transition overheads, which may defeat the benefit of task scheduling. In this paper, we aim to resolve this scheduling problem with voltage transition overhead consideration. We formalize this problem by an integer linear programming model and propose a heuristic algorithm for a runtime environment. The experimental results show that the proposed online heuristic algorithm can obtain the comparable results with the optimal scheduling derived by the offline integer linear programming approach. © 2013 Published by Elsevier Inc. All rights reserved.","Dynamic voltage scaling; Integer linear programming; Multicore real-time systems; Transition-aware scheduling; Voltage transition overheads"
"Trends in big data analytics","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901640093&doi=10.1016%2fj.jpdc.2014.01.003&partnerID=40&md5=f6b1501958762a152084a3254a88bc71","One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications' considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics. © 2014 Elsevier Inc. All rights reserved.","Analytics; Big-data; Data centers; Distributed systems"
"Data broadcasting for dependent information using multiple channels in wireless broadcast environments","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902488515&doi=10.1016%2fj.jpdc.2014.05.002&partnerID=40&md5=8cb4aeb13fd6a8963daedf2b65dc0e14","Data broadcasting is an effective approach to disseminating information to mobile clients and has attracted much research attention in recent years. In many applications, the access pattern among the data can be represented by a weighted DAG. In this paper, we consider the problem of efficiently generating the broadcast schedules on multiple channels when the data set has a DAG access pattern. We show that it is NP-hard to find an optimal broadcast schedule which not only minimizes the latency but also satisfies the ancestor property that retains the data dependency. We further derive a condition for the input DAGs under which one can generate an optimal broadcast schedule in linear time and propose an algorithm to generate the schedule. Due to the NP-completeness, we provide three heuristics for general DAGs based on the level of a vertex in the input DAGs and each heuristic uses a different policy to place vertices into the broadcast channels. There are two categories for the policies. The first category mainly considers the probability for a process to stop at a considered vertex. The second category takes the vertices which are affected most when assigning a vertex into consideration. We analyze and discuss these heuristics. A short experimental simulation is given for supporting and validating the discussion. In particular, the experimental results indicate that roughly considering the whole posterior vertices of each vertex is not precise and may not lead to good results and considering the vertices affected most when assigning a vertex will help reducing the latency. © 2014 Elsevier B.V. All rights reserved.","DAGs; Data broadcasting; Latency; Multiple channels; NP-Completeness"
"Hybrid circuit-switched network for on-chip communication in large-scale chip-multiprocessors","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902532152&doi=10.1016%2fj.jpdc.2014.05.003&partnerID=40&md5=6fa73d9afcece57a0d9cf680047c76e3","Large-scale chip-multiprocessors (CMPs) need a scalable communication structure characterized by low cost, low power, and high performance to meet their on-chip communication requirements. This paper presents a hybrid circuit-switched (HCS) network for on-chip communication in the large-scale CMPs. The HCS network, which is Advanced Microcontroller Bus Architecture (AMBA) compatible, is composed of bufferless switches, pipeline channels, and network interfaces. Furthermore, packets are transferred in a hybrid transmission scheme. If a message has only one packet, the transmission scheme for this message is packet switching. Conversely, if a message contains multiple packets, the transmission scheme for this message is circuit switching. We evaluate HCS networks with different channel depths and then compare the HCS network with the Stanford elastic buffer (EB) network. Our results show that the HCS network with two-depth channel requires 83% less power and occupies 32% less area compared with the EB network. Furthermore, under maximum frequency and single traffic, the HCS network with two-depth channel provides 37% lower zero-load latency, 390% higher maximum throughput per unit power, and 19% higher maximum throughput per unit area compared with the EB network. © 2014 Elsevier B.V. All rights reserved.","AMBA compatibility; Chip-multiprocessors; Hybrid packet-circuit switching; Network-on-chip; Pipeline channel"
"Highly scalable computational algorithms on emerging parallel machine multicore architectures II: Development and implementation in the CSD and FSI contexts","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902532153&doi=10.1016%2fj.jpdc.2014.05.001&partnerID=40&md5=967314e6ac9bc449ebaf8f48fab84e06","In this paper, the second in a series, the authors have extended and implemented their computational algorithms for improving the scalability of CSD (Computational Structural Dynamics) and FSI (Fluid-Structure Interaction) simulations on emerging architectures like multicore High Performance Computing (HPC) platforms. These algorithmic developments and extensions are classified into two categories: (i) enhanced scalability for CSD simulations on multicore platforms, (ii) newer ideas for running FSI simulations. In the first category, the authors employed the ideas developed in the first paper of this series including the multilevel partitioning strategy, next generation optimized communication procedure and better memory management to get enhanced scalability for CSD simulations. In the second category, the authors came up with a novel solver specific multicore-FSI optimal partitioning so as to improve the overall FSI scalability. After implementing the new ""intelligent partitioning"" algorithm, a speedup ratio of nearly 2.5x was obtained for the total time. The intelligent partitioning algorithm optimizes the number of solid domains relative to the number of fluid domains to optimize the overall FSI solution, irrespective of the type of the flow solver. In general, the authors have demonstrated (i) good, almost linear scalability for aeroelastic applications with several millions of cells on multicore platforms with thousands of cores, (ii) significant improvement in the scalability for smaller FSI problems using the intelligent partitioning. © 2014 Elsevier Inc. All rights reserved.","Communication; HPC; Memory management; Multicore; Partitioning"
"Heterogeneous graphene-CMOS ternary content addressable memory","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899124882&doi=10.1016%2fj.jpdc.2013.08.002&partnerID=40&md5=6d84664a7f69b12bb32a3540cfdae79d","Leveraging nanotechnology for computing opens up exciting new avenues for breakthroughs. For example, graphene is an emerging nanoscale material and is believed to be a potential candidate for post-Si nanoelectronics due to high carrier mobility and extreme scalability. Recently, a new graphene nanoribbon crossbar (xGNR) device was proposed which exhibits negative differential resistance (NDR). In this paper we propose a novel graphene nanoribbon tunneling ternary content addressable memory (GNTCAM) enabled by xGNR device, featuring heterogeneous integration with CMOS transistors and routing. Benchmarking with respect to 16nm CMOS TCAM (which uses two binary SRAMs to store ternary information) shows that GNTCAM is up to 1.82× denser, up to 9.42× more power-efficient during stand-by, and has up to 1.6× faster performance during match operation. Thus, GNTCAM has the potential to realize low-power high-density nanoscale TCAMs. Further improvements may be possible by using graphene more extensively, as graphene transistors become available in future. © 2013 Elsevier Inc. All rights reserved.","CAM; GNTCAM; Graphene nanoribbons; Heterogeneous integration; NDR; TCAM; Ternary memory"
"Energy-efficient multithreading for a hierarchical heterogeneous multicore through locality-cognizant thread generation","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885957635&doi=10.1016%2fj.jpdc.2013.07.011&partnerID=40&md5=ef51f2cdb5a2634f7e3c82f123908fbc","Energy costs have become increasingly problematic for high performance processors, but the rising number of cores on-chip offers promising opportunities for energy reduction. Further, emerging architectures such as heterogeneous multicores present new opportunities for improved energy efficiency. While previous work has presented novel memory architectures, multithreading techniques, and data mapping strategies for reducing energy, consideration to thread generation mechanisms that take into account data locality for this purpose has been limited. This study presents methodologies for the joint partitioning of data and threads to parallelize sequential codes across an innovative heterogeneous multicore processor called the Passive/Active Multicore (PAM) for reducing energy consumption from on-chip data transport and cache access components while also improving execution time. Experimental results show that the design with automatic thread partitioning offered reductions in energy-delay product (EDP) of up to 48%. © Published by Elsevier Inc.","Energy-efficient processor design; Heterogeneous multicore; Parallel computer architecture"
"Efficient heterogeneous execution on large multicore and accelerator platforms: Case study using a block tridiagonal solver","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885960645&doi=10.1016%2fj.jpdc.2013.07.012&partnerID=40&md5=a3e9299c50db3204089bded64fa4cf6a","The algorithmic and implementation principles are explored in gainfully exploiting GPU accelerators in conjunction with multicore processors on high-end systems with large numbers of compute nodes, and evaluated in an implementation of a scalable block tridiagonal solver. The accelerator of each compute node is exploited in combination with multicore processors of that node in performing block-level linear algebra operations in the overall, distributed solver algorithm. Optimizations incorporated include: (1) an efficient memory mapping and synchronization interface to minimize data movement, (2) multi-process sharing of the accelerator within a node to obtain balanced load with multicore processors, and (3) an automatic memory management system to efficiently utilize accelerator memory when sub-matrices spill over the limits of device memory. Results are reported from our novel implementation that uses MAGMA and CUBLAS accelerator software systems simultaneously with ACML (2013) [2] for multithreaded execution on processors. Overall, using 940 nVidia Tesla X2090 accelerators and 15,040 cores, the best heterogeneous execution delivers a 10.9-fold reduction in run time relative to an already efficient parallel multicore-only baseline implementation that is highly optimized with intra-node and inter-node concurrency and computation-communication overlap. Detailed quantitative results are presented to explain all critical runtime components contributing to hybrid performance. © 2013 Elsevier Inc. All rights reserved.","Accelerator; GPU; Heterogeneous execution; Linear algebra; Memory management; Tridiagonal solver"
"Hints to improve automatic load balancing with LeWI for hybrid applications","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902337781&doi=10.1016%2fj.jpdc.2014.05.004&partnerID=40&md5=5911264b6fb42cd781bd5747c12849a0","The DLB (Dynamic Load Balancing) library and LeWI (LEnd When Idle) algorithm provide a runtime solution to deal with the load imbalance of parallel applications independently of the source of imbalance. DLB relies on the usage of hybrid programming models and exploits the malleability of the second level of parallelism to redistribute computation power across processes. When executing real applications with LeWI, although application's performance is significantly improved, we have observed in some cases efficiency values between 60% and 70%, far from our theoretical limit. This work is a deep analysis of the sources of efficiency loss correlated with application characteristics, parallelization schemes and programming models. We have based our analysis in fine grain monitoring tools and metrics and validated our conclusions by reproducing them in synthetic experiments. As a result, this work teaches us some lessons that can be seen as hints to programmers to help LeWI make an efficient use of computational resources and obtain the maximum performance. © © 2014 Elsevier Inc. All rights reserved.","Application performance analysis; Hybrid parallel programming; Load balancing"
"Shield: A stackable secure storage system for file sharing in public storage","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903827089&doi=10.1016%2fj.jpdc.2014.06.003&partnerID=40&md5=2b9f3502d61906372b2e44ef756bc2a7","With the increasing amount of personal data stored in public storage, users are losing control of their physical data, putting their data information at risk of theft or being compromised. Traditional secure storage systems either require users to completely trust the storage provider or impose the considerable burden of managing files on file owners; such systems are inapplicable in the practical cloud environment. This paper addresses these challenging problems by proposing a new secure system architecture and implementing a stackable secure storage system named Shield, in which a proxy server is introduced to be in charge of authentication and access control. We propose a new variant of the Merkle Hash Tree to support efficient integrity checking and file content update; further, we have designed a hierarchical key organization to achieve convenient keys management and efficient permission revocation. Shield supports concurrent write access by employing a virtual linked list; it also provides secure file sharing without any modification to the underlying file systems. A series of evaluations over various real benchmarks show that Shield causes about 7%∼13% performance degradation when compared with eCryptfs but provides enhanced security for user's data. © 2014 Elsevier Inc. All rights reserved.","Concurrent writes; Cryptographic controls; Keys management; Permission revocation; Proxy server; Secure sharing; Storage system"
"An execution time and energy model for an energy-aware execution of a conjugate gradient method with CPU/GPU collaboration","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904811134&doi=10.1016%2fj.jpdc.2014.06.001&partnerID=40&md5=41f13a5e86fc18947abc7ae21174dc14","The parallel preconditioned conjugate gradient method (CGM) is used in many applications of scientific computing and often has a critical impact on their performance and energy consumption. This article investigates the energy-aware execution of the CGM on multi-core CPUs and GPUs used in an adaptive FEM. Based on experiments, an application-specific execution time and energy model is developed. The model considers the execution speed of the CPU and the GPU, their electrical power, voltage and frequency scaling, the energy consumption of the memory as well as the time and energy needed for transferring the data between main memory and GPU memory. The model makes it possible to predict how to distribute the data to the processing units for achieving the most energy efficient execution: the execution might deploy the CPU only, the GPU only or both simultaneously using a dynamic and adaptive collaboration scheme. The dynamic collaboration enables an execution minimising the execution time. By measuring execution times for every FEM iteration, the data distribution is adapted automatically to changing properties, e.g. the data sizes. © 2014 Elsevier Inc. All rights reserved.","Conjugate gradient method; Energy awareness; Energy model; Execution time model; GPU; RAPL"
"Experience with using the Parallel Workloads Archive","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904084381&doi=10.1016%2fj.jpdc.2014.06.013&partnerID=40&md5=13fed2425655509be36231f788cd8b6a","Science is based upon observation. The scientific study of complex computer systems should therefore be based on observation of how they are used in practice, as opposed to how they are assumed to be used or how they were designed to be used. In particular, detailed workload logs from real computer systems are invaluable for research on performance evaluation and for designing new systems. Regrettably, workload data may suffer from quality issues that might distort the study results, just as scientific observations in other fields may suffer from measurement errors. The cumulative experience with the Parallel Workloads Archive, a repository of job-level usage data from large-scale parallel supercomputers, clusters, and grids, has exposed many such issues. Importantly, these issues were not anticipated when the data was collected, and uncovering them was not trivial. As the data in this archive is used in hundreds of studies, it is necessary to describe and debate procedures that may be used to improve its data quality. Specifically, we consider issues like missing data, inconsistent data, erroneous data, system configuration changes during the logging period, and unrepresentative user behavior. Some of these may be countered by filtering out the problematic data items. In other cases, being cognizant of the problems may affect the decision of which datasets to use. While grounded in the specific domain of parallel jobs, our findings and suggested procedures can also inform similar situations in other domains.© 2014 Elsevier Inc. All rights reserved.","Data quality; Parallel job scheduling; Workload log"
"DOpenCL: Towards uniform programming of distributed heterogeneous multi-/many-core systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885954459&doi=10.1016%2fj.jpdc.2013.07.021&partnerID=40&md5=66839936c30b4b355397b94ce13bf98f","Modern computer systems become increasingly distributed and heterogeneous by comprising multi-core CPUs, GPUs, and other accelerators. Current programming approaches for such systems usually require the application developer to use a combination of several programming models (e.g., MPI with OpenCL or CUDA) in order to exploit the system's full performance potential. In this paper, we present dOpenCL (distributed OpenCL) - a uniform approach to programming distributed heterogeneous systems with accelerators. dOpenCL allows the user to run unmodified existing OpenCL applications in a heterogeneous distributed environment. We describe the challenges of implementing the OpenCL programming model for distributed systems, as well as its extension for running multiple applications concurrently. Using several example applications, we compare the performance of dOpenCL with MPI + OpenCL and standard OpenCL implementations. © 2013 Elsevier Inc. All rights reserved.","Distributed systems; dOpenCL; GPU computing; Heterogeneous systems; Many-cores; Multi-cores; OpenCL"
"Partitionable group membership for Mobile Ad hoc Networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902258252&doi=10.1016%2fj.jpdc.2014.03.003&partnerID=40&md5=2385b18deaac14b363d5c962950ff3dd","Group membership is a fundamental building block that facilitates the development of fault-tolerant systems. The specification of group membership in partitionable systems has not yet reached the same level of maturity as in primary partition systems. Existing specifications do not satisfy the following two antagonistic requirements: (i) the specification must be weak enough to be solvable (implementable); (ii) it must be strong enough to simplify the design of fault-tolerant distributed applications in partitionable systems. In this article, we propose: (1) a new distributed system model that takes into account the formation of dynamic paths, (2) a specification of partitionable group membership for MANETS called PGM, and (3) an implementation of PGM designed by adapting a well-known solution of primary partition group membership, namely Paxos. This results in the specification of an abortable consensus as the combination of two abstractions: an eventual α partition-participant detector and an eventual register per partition that guarantee liveness and safety per partition, respectively. Then, partitionable group membership is solved by transformation into a sequence of abortable consensus. © 2014 Elsevier Inc. All rights reserved.","Abortable consensus; Dynamic partitionable systems; MANETs; Partitionable group membership"
"Approximation algorithms for sweep coverage in wireless sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902257617&doi=10.1016%2fj.jpdc.2014.02.009&partnerID=40&md5=e0928fa95f5956e17bfbbc5ce561a306","Periodic monitoring is sufficient for sweep coverage with a small number of mobile sensor nodes, whereas a continuous monitoring with static sensor nodes is required for the coverage problem in wireless sensor networks. Finding the minimum number of mobile sensor nodes with a uniform speed to guarantee sweep coverage is NP-hard and it cannot be approximated within a factor of 2 (Li et al., 2011). In this paper we have proposed a 2-approximation algorithm for solving the sweep coverage for a given set of points of interest (PoIs). The best known approximation factor is 3 for this problem (Li et al., 2011). Modification of the algorithm is also proposed for extending lifetime of the mobile sensors, sweep coverage in the presence of obstacles and failure of mobile sensors. When all PoIs are static sensor nodes, we have proposed a distributed approximation algorithm with approximation ratio 2, where the static sensor nodes compute the number of mobile sensor nodes and their positions. We have introduced sweep coverage for a given area of interest (AoI) and proved that the problem is NP-complete. A 22-approximation algorithm is designed in order to solve the problem for a square region. An improvement over the approximation factor is designed for any rectangular region. Based on our simulation study we have shown that less number mobile sensor nodes are required for our point sweep coverage algorithm compared to the algorithm proposed in Du et al. (2010). © 2014 Elsevier Inc. All rights reserved.","Approximation algorithm; Euler tour; Mobile sensor; MST; Sweep coverage; TSP; Wireless networks"
"Parallel photon-mapping rendering on a mesh-NoC-based MPSoC platform","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901640238&doi=10.1016%2fj.jpdc.2014.03.005&partnerID=40&md5=aac38ac68c16e8b35a7c1dacabef6d92","High demand 3-D scenes on embedded systems draw the developers' attention to use the whole resources of current low-power processors and add dedicated hardware as a graphic accelerator unit to deal with real-time realistic scene rendering. Photon mapping, as one of the most powerful techniques to render highly realistic 3-D images by high amounts of floating-point operations, is very time-consuming. To use the advantages of multiprocessor systems to make 3-D scenes, parallel photon-mapping rendering on a homogeneous multiprocessor SoC (MPSoC) platform along with a mesh NoC by an adaptive wormhole routing method to communicate packets among cores is proposed in this paper. To make efficient use of the MPSoC platform to carry out photon-mapping rendering, many methods concerning the increase of load balancing, the efficient use of memory, and the decrease of communication cost to achieve a scalable application are explored in this paper. The resulting MPSoC platform is verified and evaluated by cycle-accurate simulations for different sizes of the mesh NoC. As expected, the proposed methods can obtain excellent load balancing and achieve a maximum of 44.3 times faster on an 8-by-8 MPSoC platform than on a single-core MPSoC platform. © 2014 Elsevier Inc. All rights reserved.","Global illumination; Mesh; MPSoC; NoC; Parallel rendering; Photon mapping"
"BiELL: A bisection ELLPACK-based storage format for optimizing SpMV on GPUs","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901616007&doi=10.1016%2fj.jpdc.2014.03.002&partnerID=40&md5=920a7cb4f07f894188492337b05392da","Sparse matrix-vector multiplication (SpMV) is one of the most important high level operations for basic linear algebra. Nowadays, the GPU has evolved into a highly parallel coprocessor which is suited to compute-intensive, highly parallel computation. Achieving high performance of SpMV on GPUs is relatively challenging, especially when the matrix has no specific structure. For these general sparse matrices, a new data structure based on the bisection ELLPACK format, BiELL, is designed to realize the load balance better, and thus improve the performance of the SpMV. Besides, based on the same idea of JAD format, the BiJAD format can be obtained. Experimental results on various matrices show that the BiELL and BiJAD formats perform better than other similar formats, especially when the number of non-zero elements per row varies a lot. © 2014 Elsevier Inc. All rights reserved.","BiELL; BiJAD; GPU; Sparse matrix-vector multiplication"
"Flooding in dynamic graphs with arbitrary degree sequence","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897114923&doi=10.1016%2fj.jpdc.2014.01.007&partnerID=40&md5=3b75975586629f0857c187f011bffad7","This paper addresses the flooding problem in dynamic graphs, where flooding is the basic mechanism in which every node becoming aware of a piece of information at step t forwards this information to all its neighbors at all forthcoming steps t′>t. We show that a technique developed in a previous paper, for analyzing flooding in a Markovian sequence of Erdös-Rényi graphs, is robust enough to be used also in different contexts. We establish this fact by analyzing flooding in a sequence of graphs drawn independently at random according to a model of random graphs with given expected degree sequence. In the prominent case of power-law degree distributions, we prove that flooding takes almost surely O(logn) steps even if, almost surely, none of the graphs in the sequence is connected. In the general case of graphs with an arbitrary degree sequence, we prove several upper bounds on the flooding time, which depend on specific properties of the degree sequence. © 2014 Elsevier Inc. All rights reserved.","Distributed computing; Dynamic graphs; Flooding protocol; Power-law graphs"
"A class of almost-optimal size-independent parallel prefix circuits","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876247167&doi=10.1016%2fj.jpdc.2013.03.012&partnerID=40&md5=1ee1362f7f212ffbb557a36d37753a29","Prefix computation is one of the fundamental problems that can be used in many applications such as fast adders. Most proposed parallel prefix circuits assume that the circuit is of the same width as the input size. In this paper, we present a class of parallel prefix circuits that perform well when the input size, n, is more than the width of the circuit, m. That is, the proposed circuit is almost optimal in speed when n>m. Specifically, we derive a lower bound for the depth of the circuit, and prove that the circuit requires one time step more than the optimal number of time steps needed to generate its first output. We also show that the size of the circuit is optimal within one. The input is divided into subsets, each of width m-1, and presented to the circuit in subsequent time steps. The circuit is compared with functionally similar circuits of (Lin, 1999 [9]; Lin and Hung, 2009 [12]) to show its outperforming speed. When n>m, the circuit is shown to be faster than the family of waist-size optimal circuits with waist 1 (WSO-1) of the same width and fan-out. © 2013 Elsevier Inc. All rights reserved.","Depth-size optimal; Parallel algorithms; Prefix operations; Problem-size independent"
"Line coverage measures in wireless sensor networks","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901634514&doi=10.1016%2fj.jpdc.2014.03.004&partnerID=40&md5=4957633d425441f5a4bf42244fdd7112","The coverage problem in wireless sensor networks addresses the problem of covering a region with sensors. Many different definitions of coverage are there in the literature depending on the goal of the coverage. In this paper, we address the problem of determining the quality of a sensor deployment against an intruder who can walk along a straight line. A line segment is said to be k-covered if it intersects the sensing regions of at least k sensors distributed in R. Similarly, it is said to be k-uncovered if it intersects the sensing regions, that is assumed to be circular, of at most k-1 sensors. We introduce two new metrics, smallest k-covered line segment and longest k-uncovered line segment, for measuring the quality of line coverage achieved by a sensor deployment. The intruder can walk a distance less than the smallest k-covered line segment without ever being detected by k sensors. So, this metric gives an estimate on the distance an intruder can walk in a straight line path before being detected by k sensors. On the other side, the defender would want to deploy sensors so that the length of the longest k-uncovered line segment is minimized. Given a deployment of n sensors, we propose deterministic algorithms to determine the smallest k-covered line segment and longest k-uncovered line segment where the line segments can be of the following types: (i) axis-parallel (horizontal and vertical) line segments, (ii) line segments whose one endpoint is fixed and is of arbitrary orientation and (iii) arbitrary line segments. The time complexities for the first and second types of line segments are O((n+χ)logn) for both smallest k-covered line segment and longest k-uncovered line segment, where χ is the number of intersections among n circles. For the arbitrary line segment case, the smallest k-covered segment can be determined in O(χ2logn+n113+μ) time, whereas, the longest k-uncovered segment can be determined in O( χ2logn+n2+β+μ) time, where β=log 2(1+5)-1 and μ is a small value greater than or equal to 0. All our algorithms take linear space. © 2014 Elsevier Inc. All rights reserved.","Coverage measures; Geometry; Line coverage; Wireless sensor network"
"Combining multi-core and GPU computing for solving combinatorial optimization problems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885960130&doi=10.1016%2fj.jpdc.2013.07.023&partnerID=40&md5=5f20741163fdf7820b72ec172c3060c3","In this paper, we revisit the design and implementation of Branch-and-Bound (B&B) algorithms for solving large combinatorial optimization problems on GPU-enhanced multi-core machines. B&B is a tree-based optimization method that uses four operators (selection, branching, bounding and pruning) to build and explore a highly irregular tree representing the solution space. In our previous works, we have proposed a GPU-accelerated approach in which only a single CPU core is used and only the bounding operator is performed on the GPU device. Here, we extend the approach (LL-GB&B) in order to minimize the CPU-GPU communication latency and thread divergence. Such an objective is achieved through a GPU-based fine-grained parallelization of the branching and pruning operators in addition to the bounding one. The second contribution consists in investigating the combination of a GPU with multi-core processing. Two scenarios have been explored leading to two approaches: a concurrent (RLL-GB&B) and a cooperative one (PLL-GB&B). In the first one, the exploration process is performed concurrently by the GPU and the CPU cores. In the cooperative approach, the CPU cores prepare and off-load to GPU pools of tree nodes using data streaming while the GPU performs the exploration. The different approaches have been extensively experimented on the Flowshop scheduling problem. Compared to a single CPU-based execution, LL-GB&B allows accelerations up to (×160) for large problem instances. Moreover, when combining multi-core and GPU, we figure out that using RLL-GB&B is not beneficial while PLL-GB&B enables an improvement up to 36% compared to LL-GB&B. © 2013 Elsevier Inc. All rights reserved.","Flowshop scheduling problem; GPU accelerators; Multi-core computing; Parallel branch-and-bound"
"Proactive scheduling in distributed computing - A reinforcement learning approach","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901586227&doi=10.1016%2fj.jpdc.2014.03.007&partnerID=40&md5=52f8b263137e57c44390dea2c404cc2f","In distributed computing such as grid computing, online users submit their tasks anytime and anywhere to dynamic resources. Task arrival and execution processes are stochastic. How to adapt to the consequent uncertainties, as well as scheduling overhead and response time, are the main concern in dynamic scheduling. Based on the decision theory, scheduling is formulated as a Markov decision process (MDP). To address this problem, an approach from machine learning is used to learn task arrival and execution patterns online. The proposed algorithm can automatically acquire such knowledge without any aforehand modeling, and proactively allocate tasks on account of the forthcoming tasks and their execution dynamics. Under comparison with four classic algorithms such as Min-Min, Min-Max, Suffrage, and ECT, the proposed algorithm has much less scheduling overhead. The experiments over both synthetic and practical environments reveal that the proposed algorithm outperforms other algorithms in terms of the average response time. The smaller variance of average response time further validates the robustness of our algorithm. © 2014 Elsevier Inc. All rights reserved.","Distributed computing; Markov decision process; Queueing model; Reinforcement learning; Task scheduling"
"Resource management policies for real-time Java remote invocations","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890572154&doi=10.1016%2fj.jpdc.2013.08.001&partnerID=40&md5=b18ade8965ba9ac66e1e8a73d3a18c73","A way to deal with the increasing cost of next generation real-time applications is to extend middleware and high-level general-purpose programming languages, e.g. Java, with real-time support that reduces development, deployment, and maintenance costs. In the particular path towards a distributed real-time Java technology, some important steps have been given into centralized systems to produce real-time Java virtual machines. However, the integration with traditional remote invocation communication paradigms is far from producing an operative solution that may be used to develop final products. In this context, the paper studies how The Real-Time Specification for Java (RTSJ), the leading effort in real-time Java, may be integrated with Java's Remote Method Invocation (RMI) in order to support real-time remote invocations. The article details a specific approach towards the problem of producing a predictable mechanism for the remote invocation-the core communication mechanism of RMI-via having control on the policies used in the remote invocation. Results obtained in a software prototype help understand how the key entities defined to control the performance of the remote invocation influence in the end-to-end response time of a distributed real-time Java application. © 2013 Elsevier Inc. All rights reserved.","DRTSJ; Real-time Java; Real-time middleware; RT-RMI; RTSJ"
"A DAG scheduling scheme on heterogeneous computing systems using double molecular structure-based chemical reaction optimization","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879610933&doi=10.1016%2fj.jpdc.2013.05.005&partnerID=40&md5=853e390ac62e2a045015e1c0941b8580","A new meta-heuristic method, called Chemical Reaction Optimization (CRO), has been proposed very recently. The method encodes solutions as molecules and mimics the interactions of molecules in chemical reactions to search the optimal solutions. The CRO method has demonstrated its capability in solving NP-hard optimization problems. In this paper, the CRO scheme is used to formulate the scheduling of Directed Acyclic Graph (DAG) jobs in heterogeneous computing systems, and a Double Molecular Structure-based Chemical Reaction Optimization (DMSCRO) method is developed. There are two molecular structures in DMSCRO: one is used to encode the execution order of the tasks in a DAG job, and the other to encode the task-to-computing-node mapping. The DMSCRO method also designs four elementary chemical reaction operations and the fitness function suitable for the scenario of DAG scheduling. In this paper, we have also conducted the simulation experiments to verify the effectiveness and efficiency of DMSCRO over a large set of randomly generated graphs and the graphs for real-world problems. © 2013 Elsevier Inc. All rights reserved.","Chemical reaction optimization; Makespan; Meta-heuristic approaches; NP-hard problem; Task scheduling"
"Exploiting heterogeneous parallelism with the Heterogeneous Programming Library","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885951736&doi=10.1016%2fj.jpdc.2013.07.013&partnerID=40&md5=2af198f2d5f879e4b41461c3757684c6","While recognition of the advantages of heterogeneous computing is steadily growing, the issues of programmability and portability hinder its exploitation. The introduction of the OpenCL standard was a major step forward in that it provides code portability, but its interface is even more complex than that of other approaches. In this paper, we present the Heterogeneous Programming Library (HPL), which permits the development of heterogeneous applications addressing both portability and programmability while not sacrificing high performance. This is achieved by means of an embedded language and data types provided by the library with which generic computations to be run in heterogeneous devices can be expressed. A comparison in terms of programmability and performance with OpenCL shows that both approaches offer very similar performance, while outlining the programmability advantages of HPL. © 2013 Elsevier Inc. All rights reserved.","Code generation; Heterogeneity; Libraries; OpenCL; Parallelism; Portability; Programmability"
"A multicast reprogramming protocol for wireless sensor networks based on small world concepts","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879491531&doi=10.1016%2fj.jpdc.2013.05.006&partnerID=40&md5=1170a2ef9c49ea62d3820c7f05a736e0","Automatic reprogramming is an important and challenging issue in wireless sensor networks (WSNs). A usual approach is the over-the-air programming (OAP), which is a fundamental service based on reliable broadcast for efficient code dissemination. However, existing OAP protocols do not enable the reprogramming of a subset of the sensor nodes in a WSN. Hence, in this work we propose a multicast-based over-the-air programming protocol that considers a small world infrastructure (MOAP-SW). The small world model is used to create shortcuts toward the sink in the communication infrastructure of sensor networks. The endpoints of these shortcuts are more powerful nodes, resulting in a heterogeneous wireless sensor network. Simulation results show the feasibility of the protocol regarding the number of messages transmitted, the energy consumption and the time to reconfigure the network. © 2013 Elsevier Inc. All rights reserved.","Multicast reprogramming; Over-the-air programming; Wireless sensor network"
"Distributed and hardware accelerated computing for clinical medical imaging using proton computed tomography (pCT)","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885954728&doi=10.1016%2fj.jpdc.2013.07.016&partnerID=40&md5=101168c72e873bf7d34b0be00863e007","Proton computed tomography (pCT) is an imaging modality that has been in development to support targeted dose delivery in proton therapy. It aims to accurately map the distribution of relative stopping power. Because protons traverse material media in non-linear paths, pCT requires individual proton processing. Image reconstruction then becomes a time-consuming process. Clinical-use scenarios that require images from billions of protons in less than ten or fifteen minutes have motivated us to use distributed and hardware-accelerated computing methods to achieve fast image reconstruction. Combined use of MPI and GPUs demonstrates that clinically viable image reconstruction is possible. On a 60-node CPU/GPU computer cluster, we achieved efficient strong and weak scaling when reconstructing images from two billion histories in under seven minutes. This represents a significant improvement over the previous state-of-the-art in pCT, which took almost seventy minutes to reconstruct an image from 131 million histories on a single-CPU, single-GPU computer. © 2013 Elsevier Inc. All rights reserved.","Computed tomography; CUDA; GPU; Image reconstruction; Iterative reconstruction; MLP; MPI; PCT; Proton computed tomography; Relative stopping power; RSP; Scaling"
"Spintronic Threshold Logic Array (STLA) - A compact, low leakage, non-volatile gate array architecture","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898835090&doi=10.1016%2fj.jpdc.2013.09.013&partnerID=40&md5=678ed1acf83e9991323e780e2227f710","This paper describes a novel, first of its kind architecture for a threshold logic gate using conventional MOSFETs and an STT-MTJ (Spin Transfer Torque-Magnetic Tunneling Junction) device. The resulting cell, called STL which is extremely compact can be programmed to realize a large number of threshold functions, many of which would require a multilevel network of conventional CMOS logic gates. Next, we describe a novel array architecture consisting of STL cells onto which complex logic networks can be mapped. The resulting array, called STLA has several advantages not available with conventional logic. This type of logic (1) is non-volatile, (2) is structurally regular and operates like DRAM, (3) is fully observable and controllable, (4) has zero standby power. These advantages are demonstrated and compared by implementing a 16-bit carry look-ahead adder and a 32-bit Wallace tree multiplier in STLA and FPGA. © 2013 Elsevier B.V. All rights reserved.","Gate array architecture; Low leakage magnetic circuits; Spin Transfer Torque-Magnetic Tunneling Junction (STT-MTJ); Threshold logic"
"A cooperative pursuit-evasion game in wireless sensor and actor networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879363014&doi=10.1016%2fj.jpdc.2013.05.009&partnerID=40&md5=3dd7f296a90c6467756425128bea87af","This paper studies the problem of the pursuit-evasion game under the wireless sensor and actor networks (WSANs). In order to plan paths for pursuers to capture an evader in the pursuit-evasion game, a novel multi-step cooperative strategy is presented. Under this strategy, the pursuit-evasion game is studied in two stages. In the first stage we assume that the evader is always static in the workplace, and in the second stage the evader will move once it senses the existence of pursuers. A Daisy-Chain Formation algorithm and a sliding mode-based method are presented to control the pursuit. Based on Lyapunov stability theory, the proposed algorithm is proved to be convergent. At last, simulation results are provided to demonstrate the effectiveness of the proposed method. © 2013 Elsevier Inc. All rights reserved.","Multi-vehicle systems; Path planning; Pursuit-evasion game; Wireless sensor and actor networks (WSANs)"
"On minimizing the resource consumption of cloud applications using process migrations","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885950760&doi=10.1016%2fj.jpdc.2013.07.020&partnerID=40&md5=057b84cd2245c7062763b71936ba17c5","According to the pay-per-use model adopted in clouds, the more resources an application running in a cloud computing environment consumes, the greater the amount of money the owner of the corresponding application will be charged. Therefore, applying intelligent solutions to minimize the resource consumption is of great importance. In this paper, we study the problem of identifying an assignment scheme between the interacting components of an application, such as processes and virtual machines, and the computing nodes of a cloud system, such that the total amount of resources consumed by the respective application is minimized. Because centralized solutions are deemed unsuitable for large distributed systems or large-scale applications, we propose a fully distributed algorithm (called DRA) to overcome scalability issues. DRA takes decisions concerning the transition from one assignment scheme to another in a dynamic way, based solely on local information. We also propose and test two modifications of the basic DRA algorithm to deal better with the heterogeneity of cloud servers in terms of capacity constraints. We must note that we capture heterogeneity regarding the network model. Through theoretical analysis, we formally prove that DRA achieves convergence and always provides an optimal solution for tree-based networks in the uncapacitated case. Moreover, we prove through experimental evaluation that DRA achieves up to 55% network cost reduction when compared to the most recent algorithm in the literature. We also show that the proposed modifications of DRA improve the algorithm's performance considerably in the case where servers have limited capacity. © 2013 Elsevier Inc. All rights reserved.","Cloud computing; Network flow; Virtual machine placement"
"Searching for a black hole in interconnected networks using mobile agents and tokens","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890561133&doi=10.1016%2fj.jpdc.2013.08.009&partnerID=40&md5=ddc5efccccd68a68acca29f92deb778f","We study the impact of the topological structure on the complexity of the Black hole search (Bhs) problem using mobile agents that communicate via tokens. First, we show that the token model can support the same cost as in the whiteboard model, despite the fact that communication between mobile agents is considerably more restricted (and complex) in a token model than in a whiteboard one. More precisely, in this paper, we focus on three specific topologies, namely: an asynchronous (i) hypercube, (ii) torus and (iii) complete network. With knowledge of which of these topologies is being used, we present token-based solutions for Bhs where the number of moves executed by a team of two co-located anonymous agents can be reduced to Θ(n). These proposed solutions do not require the availability of a map and do not assume FIFO on either nodes or links. Second, we consider the use of scattered agents for Bhs in an asynchronous (i) torus and (ii) complete network. We show that, using 3 scattered agents and 7 tokens in total, a black hole can be located with Θ(n) moves in an oriented asynchronous torus. Again, the solution does not assume FIFO on the links and nodes. If the number of scattered agents in a torus increases, the cost is reduced but communication between these agents becomes significantly more complicated. We propose an algorithm that solves Bhs using k (k>3) scattered agents, with only 1 token per agent, with O( k2n2) moves. Beyond theoretical proofs, we also discuss simulations of an actual system in order to evaluate our proposed solutions. © 2013 Elsevier Inc. All rights reserved.","Black hole; Co-located; Mobile agents; Scattered; Simulation; Tokens; Un-oriented"
"Adaptive atomic capture of multiple molecules","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879288068&doi=10.1016%2fj.jpdc.2013.03.010&partnerID=40&md5=facf22e3891078499a67a7a1c175e940","Facing the scale, heterogeneity and dynamics of the global computing platform emerging on top of the Internet, autonomic computing has been raised recently as one of the top challenges of computer science research. Such a paradigm calls for alternative programming abstractions, able to express autonomic behaviours. In this quest, nature-inspired analogies regained a lot of interest. More specifically, the chemical programming paradigm, which envisions a program's execution as a succession of reactions between molecules representing data to produce a result, has been shown to provide some adequate abstractions for the high-level specification of autonomic systems. However, conceiving a runtime able to run such a model over large-scale platforms raises several problems, hindering this paradigm to be actually leveraged. Among them, the atomic capture of multiple molecules participating in concurrent reactions is one of the most significant. In this paper, we propose a protocol for the atomic capture of these molecules distributed and evolving over a large-scale platform. As the density of potential reactions has a significant impact on the liveness and efficiency of such a capture, the protocol proposed is made up of two sub-protocols, each of them aimed at addressing different levels of densities of potential reactions in the solution. While the decision to choose one or the other is local to each node participating in a program's execution, a global coherent behaviour is obtained. We also give an overview of the course of execution when a program contains multiple rules and provide a rule-changing mechanism. The proof of correctness, as well as intensive simulation results showing the efficiency and limited overhead of the protocol are given. © 2013 Elsevier Inc. All rights reserved.","Autonomic computing; Capture protocol; Chemical programming paradigm; Distributed systems; Large-scale platforms; Mutual exclusion; Nature-inspired models"
"Fair scheduling of bag-of-tasks applications using distributed Lagrangian optimization","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890563279&doi=10.1016%2fj.jpdc.2013.08.011&partnerID=40&md5=e746ae6383c191a2c1053b8a1e2a8391","Large scale distributed systems typically comprise hundreds to millions of entities (applications, users, companies, universities) that have only a partial view of resources (computers, communication links). How to fairly and efficiently share such resources between entities in a distributed way has thus become a critical question. Although not all applications are suitable for execution on large scale distributed computing platform, ideal are the Bag-of-Tasks (BoT) applications. Hence a large fraction of jobs in workloads imposed on Grids is made of sequential applications submitted in the form of BoTs. Up until now, mainly simple mechanisms have been used to ensure a fair sharing of resources among these applications. Although these mechanisms are proved to be efficient for CPU-bound applications, they are known to be ineffective in the presence of network-bound applications. A possible answer resorts to Lagrangian optimization and distributed gradient descent. Under certain conditions, the resource sharing problem can be formulated as a global optimization problem, which can be solved by a distributed self-stabilizing supply and demand algorithm. In the last decade, this technique has been applied to design various network protocols (variants of TCP, multi-path network protocols, wireless network protocols) and even distributed algorithms for smart grids. In this article, we explain how to use this technique for fairly scheduling concurrent BoT applications with arbitrary communication-to- computation ratio on a Grid. Yet, application heterogeneity raises severe convergence and stability issues that did not appear in the previous contexts and need to be addressed by non-trivial modifications. The effectiveness of our proposal is assessed through an extensive set of complex and realistic simulations. © 2013 Elsevier Inc. All rights reserved.","Distributed scheduling; Grid computing; Lagrangian optimization; Steady-state scheduling"
"Detecting stable locality-aware predicates","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890549666&doi=10.1016%2fj.jpdc.2013.09.009&partnerID=40&md5=26945376d2141869a6706026a1cf0258","In a large-scale locality-driven network such as in modular robotics and wireless sensor networks, knowing the state of a local area is sometimes necessary due to either interactions being local and driven by neighborhood proximity or the users being interested in the state of a certain region. We define locality-aware predicates (LAP) that aim at detecting a predicate within a specified area. We model the area of interest as the set of processes that are within a breadth-first search tree (BFST) of height k rooted at the initiator process. Although a locality-aware predicate specifies a predicate only within a local area, observing the area consistently requires considering the entire system in a consistent manner. This raises the challenge of making the complexities of the corresponding predicate detection algorithms scale-free, i.e., independent of the size of the system. Since all existing algorithms for getting a consistent view of the system require either a global snapshot of the entire system or vector clocks of the size of the system, a new solution is needed. We focus on stable LAP, which are those LAP that remain true once they become true. We propose a scale-free algorithm to detect stable LAP within a k-height BFST. Our algorithm can detect both stable conjunctive LAP and stable relational LAP. In the process of designing our algorithm, we also propose the first distributed algorithm for building a BFST within an area of interest in a graph, and the first distributed algorithm for recording a consistent sub-cut within the area of interest. This paper demonstrates that LAPs are a natural fit for detecting distributed properties in large-scale distributed systems, and stable LAPs can be practically detected at low cost. © 2013 Elsevier Inc. All rights reserved.","Distributed computing; Locality-aware; Modular robotics; Predicate detection; Scale-free; Wireless sensor networks"
"Streaming data analytics via message passing with application to graph algorithms","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2014.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902250397&doi=10.1016%2fj.jpdc.2014.04.001&partnerID=40&md5=9860c311f0f7323889a76a0a4f85457c","The need to process streaming data, which arrives continuously at high-volume in real-time, arises in a variety of contexts including data produced by experiments, collections of environmental or network sensors, and running simulations. Streaming data can also be formulated as queries or transactions which operate on a large dynamic data store, e.g. a distributed database. We describe a lightweight, portable framework named PHISH which provides a communication model enabling a set of independent processes to compute on a stream of data in a distributed-memory parallel manner. Datums are routed between processes in patterns defined by the application. PHISH provides multiple communication backends including MPI and sockets/ZMQ. The former means streaming computations can be run on any parallel machine which supports MPI; the latter allows them to run on a heterogeneous, geographically dispersed network of machines. We illustrate how streaming MapReduce operations can be implemented using the PHISH communication model, and describe streaming versions of three algorithms for large, sparse graph analytics: triangle enumeration, sub-graph isomorphism matching, and connected component finding. We also provide benchmark timings comparing MPI and socket performance for several kernel operations useful in streaming algorithms. © 2014 Elsevier Inc. All rights reserved.","Graph algorithms; MapReduce; Message passing; MPI; Sockets; Streaming data"
"Efficient asynchronous executions of AMR computations and visualization on a GPU system","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876143563&doi=10.1016%2fj.jpdc.2013.03.002&partnerID=40&md5=cc516274c7232f26846f4ca64bb51716","Adaptive Mesh Refinement is a method which dynamically varies the spatio-temporal resolution of localized mesh regions in numerical simulations, based on the strength of the solution features. In-situ visualization plays an important role for analyzing the time evolving characteristics of the domain structures. Continuous visualization of the output data for various timesteps results in a better study of the underlying domain and the model used for simulating the domain. In this paper, we develop strategies for continuous online visualization of time evolving data for AMR applications executed on GPUs. We reorder the meshes for computations on the GPU based on the users input related to the subdomain that he wants to visualize. This makes the data available for visualization at a faster rate. We then perform asynchronous executions of the visualization steps and fix-up operations on the CPUs while the GPU advances the solution. By performing experiments on Tesla S1070 and Fermi C2070 clusters, we found that our strategies result in 60% improvement in response time and 16% improvement in the rate of visualization of frames over the existing strategy of performing fix-ups and visualization at the end of the timesteps. © 2013 Elsevier Inc. All rights reserved.","Adaptive Mesh Refinement; CPU-GPU asynchronism; GAMER; GPGPU"
"Sparse matrix-vector multiplication on the Single-Chip Cloud Computer many-core processor","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885948161&doi=10.1016%2fj.jpdc.2013.07.017&partnerID=40&md5=173c4bc242dd74819bfc052e3a09cb5d","The microprocessor industry has responded to memory, power and ILP walls by turning to many-core processors, increasing parallelism as the primary method to improve processor performance. These processors are expected to consist of tens or even hundreds of cores. One of these future processors is the 48-core experimental processor Single-Chip Cloud Computer (SCC). The SCC was created by Intel Labs as a platform for many-core software research. In this work we study the behavior of an important irregular application such as the Sparse Matrix-Vector multiplication (SpMV) on the SCC processor in terms of performance and power efficiency. In addition, some of the most successful optimization techniques for this kernel are evaluated. In particular, reordering, blocking and data compression techniques have been considered. Our experiments give some key insights that can serve as guidelines for the understanding and optimization of the SpMV kernel on this architecture. Furthermore, an architectural comparison of the SCC processor with several leading multicore processors and GPUs is performed, including the new Intel Xeon Phi coprocessor. The SCC only outperforms the Itanium2 multicore processor. Best performance results are observed for the high-end GPUs and the Phi, while reaching low values with respect to their peak performance. In terms of power efficiency, we must highlight the good behavior of the ATI GPUs. © 2013 Elsevier Inc. All rights reserved.","Many-core; Optimization; Performance; Power efficiency; Sparse matrix"
"How many cores do we need to run a parallel workload: A test drive of the Intel SCC platform?","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901593493&doi=10.1016%2fj.jpdc.2013.12.011&partnerID=40&md5=6c31203a0be636cb74fd4cce9175ff98","As semiconductor manufacturing technology continues to improve, it is possible to integrate more and more transistors onto a single processor. Many-core processor design has resulted in part from the search to utilize this enormous transistor real estate. The Single-Chip Cloud Computer (SCC) is an experimental many-core processor created by Intel Labs. In this paper we present a study in which we analyze this innovative many-core system by running several workloads with distinctive parallelism characteristics. We investigate the effect on system performance by monitoring specific hardware performance counters. Then, we experiment on varying different hardware configuration parameters such as number of cores, clock frequency and voltage levels. We execute the chosen workloads and collect the timing, power consumption and energy consumption information on such a many-core research platform. Thus, we can comprehensively analyze the behavior and scalability of the Intel SCC system with the introduced workload in terms of performance and energy consumption. Our results show that the profiled parallel workload execution has a communication bottleneck on the Intel SCC system. Moreover, our results indicate that we should carefully choose the number of cores to execute different workloads in order to yield a balance between execution performance and energy efficiency for different applications. © 2014 Elsevier Inc. All rights reserved.","Dynamic voltage and frequency scaling; Many-core processor; Message passing; Single-chip cloud computer"
"Looking back at dense linear algebra software","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901587022&doi=10.1016%2fj.jpdc.2013.10.005&partnerID=40&md5=b8fe23bfd9b7f1f345c7c8d9a7f23a3f","Over the years, computational physics and chemistry served as an ongoing source of problems that demanded the ever increasing performance from hardware as well as the software that ran on top of it. Most of these problems could be translated into solutions for systems of linear equations: the very topic of numerical linear algebra. Seemingly then, a set of efficient linear solvers could be solving important scientific problems for years to come. We argue that dramatic changes in hardware designs precipitated by the shifting nature of the marketplace of computer hardware had a continuous effect on the software for numerical linear algebra. The extraction of high percentages of peak performance continues to require adaptation of software. If the past history of this adaptive nature of linear algebra software is any guide then the future theme will feature changes as well-changes aimed at harnessing the incredible advances of the evolving hardware infrastructure. © 2013 Elsevier Inc. All rights reserved.","Decompositional approach; Dense linear algebra; Parallel algorithms"
"Maximum-throughput mapping of SDFGs on multi-core SoC platforms","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880373319&doi=10.1016%2fj.jpdc.2013.05.004&partnerID=40&md5=a2b99d2fb6ea5e92a1eaa6c0af990bc2","Data-Flow models are attracting renewed attention because they lend themselves to efficient mapping on multi-core architectures. The key problem of finding a maximum-throughput allocation and scheduling of Synchronous Data-Flow graphs (SDFGs) onto a multi-core architecture is NP-hard and has been traditionally solved by means of heuristic (incomplete) algorithms with no guarantee of global optimality. In this paper we propose an exact (complete) algorithm for the computation of a maximum-throughput mapping of applications specified as SDFG onto multi-core architectures. This is, to the best of our knowledge, the first complete algorithm for generic SDF graphs, including those with loops and a finite iteration bound. Our approach is based on Constraint Programming, it guarantees optimality and can handle realistic instances in terms of size and complexity. Extensive experiments on a large number of SDFGs demonstrate that our approach is effective and robust. © 2013 Elsevier Inc. All rights reserved.","Acceleration of parallel execution; Constraint programming; Mapping; Multi-core platforms; Scheduling"
"Emitter-coupled spin-transistor logic","2014","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.08.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899077101&doi=10.1016%2fj.jpdc.2013.08.012&partnerID=40&md5=3831d1de5a0ce441d93e282251a7e09b","The recent invention of magnetoresistive bipolar spin-transistors makes possible the creation of new spintronic logic families. Here we propose the first logic family exploiting these devices, extending emitter-coupled logic (ECL) to achieve a greater range of basis logic functions. By placing the wire from the output stage of ECL logic elements near spin-transistors in other logic stages throughout the circuit, additional basis logic elements can be realized. These new logic elements support greater logic minimization, resulting in enhanced speed, area, and power characteristics. A novel magnetic shielding structure provides this logic family with the crucial ability to cascade logic stages. This logic family potentially achieves a power-delay product 10-25 times smaller than conventional ECL, and can therefore be exploited to increase the performance of very high-speed logic circuits while broadening the range of design choices for a variety of electronic applications. © 2013 Elsevier B.V. All rights reserved.","Beyond-CMOS; Bipolar magnetoresistive semiconductor heterojunction transistor; Magnetic bipolar transistor; Magnetoresistance; Magnetoresistive transistor; Spintronic logic; Spintronics"
"Dimension-adjacent trees and parallel construction of independent spanning trees on crossed cubes","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893666698&doi=10.1016%2fj.jpdc.2013.01.009&partnerID=40&md5=4de4957e2f384934115eb961e5da9868","Independent spanning trees (ISTs) have increasing applications in fault-tolerance, bandwidth, and security. In this paper, we study the problem of parallel construction of ISTs on crossed cubes. We first propose the definitions of dimension-adjacent walk and dimension-adjacent tree along with a dimension property of crossed cubes. Then, we consider the parallel construction of ISTs on crossed cubes. We show that there exist n general dimension-adjacent trees which are independent of the addresses of vertices in the n-dimensional crossed cube CQn. Based on n dimension-adjacent trees and an arbitrary root vertex, a parallel algorithm with the time complexity O(2 n) is proposed to construct n ISTs on CQn, where n ≥ 1. © 2013 Elsevier Inc. All rights reserved.","Crossed cube; Dimension-adjacent tree; Independent spanning tree; Internally disjoint path; Parallel construction"
"Accelerating text mining workloads in a MapReduce-based distributed GPU environment","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870898987&doi=10.1016%2fj.jpdc.2012.10.001&partnerID=40&md5=a3ba9524432e34b465ab188e795143a9","Scientific computations have been using GPU-enabled computers successfully, often relying on distributed nodes to overcome the limitations of device memory. Only a handful of text mining applications benefit from such infrastructure. Since the initial steps of text mining are typically data intensive, and the ease of deployment of algorithms is an important factor in developing advanced applications, we introduce a flexible, distributed, MapReduce-based text mining workflow that performs I/O-bound operations on CPUs with industry-standard tools and then runs compute-bound operations on GPUs which are optimized to ensure coalesced memory access and effective use of shared memory. We have performed extensive tests of our algorithms on a cluster of eight nodes with two NVidia Tesla M2050s attached to each, and we achieve considerable speedups for random projection and self-organizing maps. © 2012 Elsevier Inc. All rights reserved.","GPU computing; MapReduce; Random projection; Self-organizing maps; Text mining"
"Parallel design for error-resilient entropy coding algorithm on GPU","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872786934&doi=10.1016%2fj.jpdc.2012.12.008&partnerID=40&md5=7ab1e2c513d726082548b658741cd61d","The error-resilient entropy coding (EREC) algorithm is an effective method for combating error propagation at low cost in many compression methods using variable-length coding (VLC). However, the main drawback of the EREC is its high complexity. In order to overcome this disadvantage, a parallel EREC is implemented on a graphics processing unit (GPU) using the NVIDIA CUDA technology. The original EREC is a finer-grained parallel at each stage which brings additional communication overhead. To achieve high efficiency of parallel EREC, we propose partitioning the EREC (P-EREC) algorithm, which splits variable-length blocks into groups and then every group is coded using the EREC separately. Each GPU thread processes one group so as to make the EREC coarse-grained parallel. In addition, some optimization strategies are discussed in order to obtain higher performance using the GPU. In the case that the variable-length data blocks are divided into 128 groups (256 groups, resp.), experimental results show that the parallel P-EREC achieves 32× to 123× (54× to 350×, resp.) speedup over the original C code of EREC compiled with the O2 optimization option. Higher speedup can even be obtained with more groups. Compared to the EREC, the P-EREC not only achieves a good speedup performance, but it also slightly improves the resilience of the VLC bit-stream against burst or random errors. © 2013 Elsevier Inc. All rights reserved.","CUDA; EREC; GPU; P-EREC; Parallel processing"
"Virtual Tree: A robust architecture for interval valid queries in dynamic distributed systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885180350&doi=10.1016%2fj.jpdc.2013.03.017&partnerID=40&md5=903841ee963625fe2759ddd64b29abb4","This paper studies the problem of answering aggregation queries, satisfying the interval validity semantics, in a distributed system prone to continuous arrival and departure of participants. The interval validity semantics states that the query answer must be calculated considering contributions of at least all processes that remained in the distributed system for the whole query duration. Satisfying this semantics in systems experiencing unbounded churn is impossible due to the lack of connectivity and path stability between processes. This paper presents a novel architecture, namely Virtual Tree, for building and maintaining a structured overlay network with guaranteed connectivity and path stability in settings characterized by bounded churn rate. The architecture includes a simple query answering algorithm that provides interval valid answers. The overlay network generated by the Virtual Tree architecture is a tree-shaped topology with virtual nodes constituted by clusters of processes and virtual links constituted by multiple communication links connecting processes located in adjacent virtual nodes. We formally prove a bound on the churn rate for interval valid queries in a distributed system where communication latencies are bounded by a constant unknown by processes. Finally, we carry out an extensive experimental evaluation that shows the degree of robustness of the overlay network generated by the virtual tree architecture under different churn rates. © 2013 Elsevier Inc.","Distributed query answering; Dynamic distributed systems; Node clustering; Overlay networks; Peer-to-peer systems"
"Towards scalable model checking of self-stabilizing programs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872805810&doi=10.1016%2fj.jpdc.2012.12.009&partnerID=40&md5=0e3ab74df233a6442982b226f7c25775","Existing approaches for verifying self-stabilization with a symbolic model checker have relied on the use of weak fairness. We point out that this approach has limited scalability. To overcome this limitation, first, we show that if self-stabilization is possible without fairness then the cost of verifying self-stabilization is substantially lower. In fact, we observe from several case studies that the cost of verification under weak fairness is more than 1000 times that of the cost without fairness. For the case where weak fairness is essential for self-stabilization, we demonstrate the feasibility of two approaches for improving scalability: (1) decomposition and (2) utilizing the weaker version of self-stabilization, namely weak stabilization. In the first approach, the designer partitions the program into components where each component satisfies its property without fairness. We show that the first approach enables us to verify Huang's mutual exclusion program for uniform rings with 31 processes (state space 10138) whereas without this approach, it was not possible to verify the same program with 5 processes (state space 1010). In the second approach, a weaker version of self-stabilization is verified. For Hoepman's ring-orientation program on odd-length ring, we show that it is possible to verify weak stabilization for 301 processes (state space 10181) whereas self-stabilization could not be verified for 9 processes (state space 105) under weak fairness. Furthermore, one can utilize transformation algorithms to convert weak stabilizing programs to probabilistically stabilizing programs. Hence, for the case where it is not possible to verify deterministic self-stabilization, one can obtain the assurance provided by probabilistic self-stabilization at a significantly reduced cost. Finally, we also present 5 case studies to illustrate the scalability of stabilization with techniques suggested in this paper. © 2012 Elsevier Inc. All rights reserved.","Fairness; Fault-tolerance; Model checking; Self-stabilization; Verification"
"Trellis: Portability across architectures with a high-level framework","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885176360&doi=10.1016%2fj.jpdc.2013.07.001&partnerID=40&md5=9148882ace9980f5efe443d8dbe5cd33","The increasing computational needs of parallel applications inevitably require portability across parallel architectures, which now include heterogeneous processing resources, such as CPUs and GPUs, and multiple SIMD/SIMT widths. However, the lack of a common parallel programming paradigm that provides predictable, near-optimal performance on each resource leads to the use of low-level frameworks with architecture-specific optimizations, which in turn cause the code base to diverge and makes porting difficult. Our experiences with parallel applications and frameworks lead us to the conclusion that achieving performance portability requires a common set of high-level directives and efficient mapping onto each architecture. In order to demonstrate this concept, we develop Trellis, a prototype programming framework that allows the programmer to maintain only a single generic and structured codebase that executes efficiently on both the CPU and the GPU. Our approach annotates such code with a single set of high-level directives, derived from both OpenMP and OpenACC, that is made compatible for both architectures. Most importantly, motivated by the limitations of the OpenACC compiler in transforming such code into a GPU kernel, we introduce a thread synchronization directive and a set of transformation techniques that allow us to obtain the GPU code with the desired parallelization that yields more optimal performance. While a common high-level programming framework for both CPU and GPU is not yet available, our analysis shows that even obtaining the best-case GPU performance with OpenACC, state-of-the-art solution, requires modifications to the structure of codes to properly exploit braided parallelism, and cope with conditional statements or serial sections. While this already requires prior knowledge of compiler behavior the optimal performance is still unattainable due to the lack of synchronization. We describe the contributions of Trellis in addressing these problems by showing how it can achieve correct parallelization of the original codes for three parallel applications, with performance competitive to that of OpenMP and CUDA, improved programmability and reduced overall code length. © 2013 Elsevier Ltd.","Loop mapping; Parallel architectures; Parallel computation; Parallel frameworks"
"Programming support and scheduling for communicating parallel tasks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870945933&doi=10.1016%2fj.jpdc.2012.09.017&partnerID=40&md5=2cd65895321640b55b60dff5a8186381","Task-based programming models are beneficial for the development of parallel programs for several reasons. They provide a decoupling of the specification of parallelism from the scheduling and mapping to execution resources of a specific hardware platform, thus allowing a flexible and individual mapping. For platforms with a distributed address space, the use of parallel tasks, instead of sequential tasks, adds the additional advantage of a structuring of the program into communication domains that can help to reduce the overall communication overhead. In this article, we consider the parallel programming model of communicating parallel tasks (CM-tasks), which allows both task-internal communication as well as communication between concurrently executed tasks at arbitrary points of their execution. We propose a corresponding scheduling algorithm and describe how the scheduling is supported by a transformation tool. An experimental evaluation using synthetic task graphs as well as several complex application programs shows that employing the CM-task model may lead to significant performance improvements compared to other parallel execution schemes. © 2012 Elsevier Inc. All rights reserved.","Algorithms; Mixed parallelism; Parallel tasks; Scalability; Scheduling; Tool support"
"Parallel partitioning for distributed systems using sequential assignment","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870946628&doi=10.1016%2fj.jpdc.2012.09.019&partnerID=40&md5=a6fb253cdb5ddb75905bacd8d20181c6","This paper introduces a method to combine the advantages of both task parallelism and fine-grained co-design specialisation to achieve faster execution times than either method alone on distributed heterogeneous architectures. The method uses a novel mixed integer linear programming formalisation to assign code sections from parallel tasks to share computational components with the optimal trade-off between acceleration from component specialism and serialisation delay. The paper provides results for software benchmarks partitioned using the method and formal implementations of previous alternatives to demonstrate both the practical tractability of the linear programming approach and the increase in program acceleration potential deliverable. © 2012 Elsevier Inc. All rights reserved.","Heterogeneous computing; High-performance computing; Parallel partitioning; Sequential assignment; Write-Only Architecture"
"Foundations of distributed multiscale computing: Formalization, specification, and analysis","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893671761&doi=10.1016%2fj.jpdc.2012.12.011&partnerID=40&md5=717172fb719e67cd36e8d0a26a271d3f","Inherently complex problems from many scientific disciplines require a multiscale modeling approach. Yet its practical contents remain unclear and inconsistent. Moreover multiscale models can be very computationally expensive and may have potential to be executed on distributed infrastructure. In this paper we propose firm foundations for multiscale modeling and distributed multiscale computing. Useful interaction patterns of multiscale models are made predictable with a submodel execution loop (SEL) four coupling templates and coupling topology properties. We enhance a high-level and well-defined Multiscale Modeling Language (MML) that describes and specifies multiscale models and their computational architecture in a modular way. The architecture is analyzed using directed acyclic task graphs facilitating validity checking scheduling distributed computing resources estimating computational costs and predicting deadlocks. Distributed execution using the multiscale coupling library and environment (MUSCLE) is outlined. The methodology is applied to two selected applications in nanotechnology and biophysics showing its capabilities. © 2013 Elsevier Inc. All rights reserved.","Coupling template; Coupling topology; Distributed multiscale computing; MML; Modeling methodology; Multiscale modeling; MUSCLE; Submodel execution loop; Task graph"
"BlobCR: Virtual disk based checkpoint-restart for HPC applications on IaaS clouds","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893708621&doi=10.1016%2fj.jpdc.2013.01.013&partnerID=40&md5=4e37dd0e1c6ca8efba6058fd1f1e6d2d","Infrastructure-as-a-Service (IaaS) cloud computing is gaining significant interest in industry and academia as an alternative platform for running HPC applications. Given the need to provide fault tolerance, support for suspend-resume and offline migration, an efficient Checkpoint-Restart mechanism becomes paramount in this context. We propose BlobCR, a dedicated checkpoint repository that is able to take live incremental snapshots of the whole disk attached to the virtual machine (VM) instances. BlobCR aims to minimize the performance overhead of checkpointing by persisting VM disk snapshots asynchronously in the background using a low overhead technique we call selective copy-on-write. It includes support for both application-level and process-level checkpointing, as well as support to roll back filesystem changes. Experiments at large scale demonstrate the benefits of our proposal both in synthetic settings and for a real-life HPC application. © 2013 Elsevier Inc. All rights reserved.","Checkpoint-restart; Fault tolerance; High performance computing; IaaS clouds; Rollback of filesystem changes; Virtual disk snapshots"
"The failure trace archive: Enabling the comparison of failure measurements and models of distributed systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885176762&doi=10.1016%2fj.jpdc.2013.04.002&partnerID=40&md5=91abf80ae630905651afef8edbaaa644","With the increasing presence, scale, and complexity of distributed systems, resource failures are becoming an important and practical topic of computer science research. While numerous failure models and failure-aware algorithms exist, their comparison has been hampered by the lack of public failure data sets and data processing tools. To facilitate the design, validation, and comparison of fault-tolerant models and algorithms, we have created the Failure Trace Archive (FTA)-an online, public repository of failure traces collected from diverse parallel and distributed systems. In this work, we first describe the design of the archive, in particular of the standard FTA data format, and the design of a toolbox that facilitates automated analysis of trace data sets. We also discuss the use of the FTA for various current and future purposes. Second, after applying the toolbox to nine failure traces collected from distributed systems used in various application domains (e.g., HPC, Internet operation, and various online applications), we present a comparative analysis of failures in various distributed systems. Our analysis presents various statistical insights and typical statistical modeling results for the availability of individual resources in various distributed systems. The analysis results underline the need for public availability of trace data from different distributed systems. Last, we show how different interpretations of the meaning of failure data can result in different conclusions for failure modeling and job scheduling in distributed systems. Our results for different interpretations show evidence that there may be a need for further revisiting existing failure-aware algorithms, when applied for general rather than for domain-specific distributed systems. © 2013 Elsevier Inc.","Distributed systems; Failure model; Failure trace archive; Resource failures; Statistical analysis"
"Software-based dynamic-warp scheduling approach for load-balancing the Viola-Jones face detection algorithm on GPUs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893691369&doi=10.1016%2fj.jpdc.2013.01.012&partnerID=40&md5=707741b0b70f26579bb618ca2466edb6","Face detection is a key component in applications such as security surveillance and human-computer interaction systems, and real-time recognition is essential in many scenarios. The Viola-Jones algorithm is an attractive means of meeting the real time requirement, and has been widely implemented on custom hardware, FPGAs and GPUs. We demonstrate a GPU implementation that achieves competitive performance, but with low development costs. Our solution treats the irregularity inherent to the algorithm using a novel dynamic warp scheduling approach that eliminates thread divergence. This new scheme also employs a thread pool mechanism, which significantly alleviates the cost of creating, switching, and terminating threads. Compared to static thread scheduling, our dynamic warp scheduling approach reduces the execution time by a factor of 3. To maximize detection throughput, we also run on multiple GPUs, realizing 95.6 FPS on 5 Fermi GPUs. © 2013 Elsevier Inc. All rights reserved.","Dynamic warp scheduling; GPUs; SIMD; Viola-Jones"
"Extreme scale computing: Modeling the impact of system noise in multi-core clustered systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879098640&doi=10.1016%2fj.jpdc.2013.01.016&partnerID=40&md5=38910c27b04ec62bfd52311868af3a81","System noise or Jitter is the activity of hardware, firmware, operating system, runtime system, and management software events. It is shown to disproportionately impact application performance in current generation large-scale clustered systems running general-purpose operating systems (GPOS). Jitter mitigation techniques such as co-scheduling jitter events across operating systems improve application performance but their effectiveness on future petascale systems is unknown. To understand if existing jitter mitigation solutions enable scalable petascale performance, we construct two complementary jitter models based on detailed analysis of system noise from the nodes of a large-scale system running a GPOS. We validate these two models using experimental data from a system consisting of 256 GPOS instances with 8192 CPUs. Based on our models, we project a minimum slowdown of 1.8%, 4.1%, and 6.5% for applications executing on a similar one petaflop system running 1024 GPOS instances and having global synchronization operations once every 100 ms, 10 ms, and 1 ms, respectively. Our projections indicate that-although existing mitigation solutions enable scalable petascale performance-additional techniques are required to contain the impact of jitter on multi-petaflop systems, especially for tightly synchronized applications. © 2013 Elsevier Ltd. All rights reserved.","General-purpose operating systems; Jitter; Models; Multi-core clustered systems; System noise"
"Parallel multitask cross validation for Support Vector Machine using GPU","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886781685&doi=10.1016%2fj.jpdc.2012.02.011&partnerID=40&md5=17360c904e8987cf1a589cb286929f23","The Support Vector Machine (SVM) is an efficient tool in machine learning with high accuracy performance. However, in order to achieve the highest accuracy performance, n-fold cross validation is commonly used to identify the best hyperparameters for SVM. This becomes a weak point of SVM due to the extremely long training time for various hyperparameters of different kernel functions. In this paper, a novel parallel SVM training implementation is proposed to accelerate the cross validation procedure by running multiple training tasks simultaneously on a Graphics Processing Unit (GPU). All of these tasks with different hyperparameters share the same cache memory which stores the kernel matrix of the support vectors. Therefore, this heavily reduces redundant computations of kernel values across different training tasks. Considering that the computations of kernel values are the most time consuming operations in SVM training, the total time cost of the cross validation procedure decreases significantly. The experimental tests indicate that the time cost for the multitask cross validation training is very close to the time cost of the slowest task trained alone. Comparison tests have shown that the proposed method is 10 to 100 times faster compared to the state of the art LIBSVM tool. © 2012 Elsevier Inc. All rights reserved.","Cross validation; GPU; Parallel computing; Support Vector Machine"
"Multicasting in the presence of aggregated deliveries","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893686976&doi=10.1016%2fj.jpdc.2012.12.004&partnerID=40&md5=2f880009b53e89e3425c57376fb463a2","An increasing number of distributed systems relies on forms of message correlation, which result in atomic delivery of multiple messages aggregated by following process-specific criteria. Generally, more than one process is aggregating messages, implying that messages are multicast. While delivery guarantees for multicast scenarios with single message delivery are well understood, existing systems and models for aggregated deliveries either consider only unicast, centralized setups, or focus on efficiency thus providing only best-effort guarantees. This paper investigates the foundations of Multi-Delivery Multicast (MDMcast) in asynchronous distributed systems with crash-stop failures. We first describe a succinct aggregation model with a concise and generic predicate grammar for expressing conjunctions on messages and properties for a corresponding multicast primitive, which we term Conjunction-MDMcast (C-MDMcast). We show that for processes interested in identical conjunctions to achieve agreement on delivered messages, a total order on individual messages (or equivalent oracle) is not only useful but necessary, which is clearly opposed to single-message deliveries. We show this indirectly by exhibiting an algorithm implementing C-MDMcast on top of Total Order Broadcast (TOBcast) and vice-versa for a majority of correct processes. Then, we extend our predicate grammar with disjunctions, leading to the specification of Disjunction-MDMcast (D-MDMcast). We exhibit an algorithm implementing D-MDMcast, derived from our algorithm implementing C-MDMcast. We formalize several additional properties for both of our specifications, including ordering properties on aggregated messages and a notion of agreement capturing non-identical yet ""related"" conjunctions, and show how our respective algorithms implement these. © 2012 Elsevier Inc. All rights reserved.","Aggregation; Multicast; Order; Properties"
"Comparisons of air traffic control implementations on an associative processor with a MIMD and consequences for parallel computing","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870864282&doi=10.1016%2fj.jpdc.2012.05.009&partnerID=40&md5=eb5fd8e717d722e5c8adb82c6b3c00f8","This paper has two complementary focuses. The first is the system design and algorithmic development for air traffic control (ATC) using an associative SIMD processor (AP). The second is the comparison of this implementation with a multiprocessor implementation and the implications of these comparisons. This paper demonstrates how one application, ATC, can more easily, more simply, and more efficiently be implemented on an AP than is generally possible on other types of traditional hardware. The AP implementation of ATC will take advantage of its deterministic hardware to use static scheduling. The software will be dramatically smaller and cheaper to create and maintain. Likewise, a large AP system will be considerably simpler and cheaper than the MIMD hardware currently used. While APs were used for ATC-type applications earlier, these are no longer available. We use a ClearSpeed CSX600 accelerator to emulate the AP solutions of ATC on an ATC prototype consisting of eight data-intensive ATC real-time tasks. Its performance is compared with an 8-core multiprocessor (MP) using OpenMP. Our extensive experiments show that the AP implementation meets all deadlines while the MP will regularly miss a large number of deadlines. The AP code will be similar in size to sequential code for the same tasks and will avoid all of the additional support software needed with an MP to handle dynamic scheduling, load balancing, shared resource management, race conditions, false sharing, etc. At this point, essentially only MIMD systems are built. Many of the advantages of using an AP to solve an ATC problem would carry over to other applications. AP solutions for a wide variety of applications will be cited in this paper. Applications that involve a high degree of data parallelism such as database management, text processing, image processing, graph processing, bioinformatics, weather modeling, managing UAS (Unmanned Aircraft Systems or drones) etc., are good candidates for AP solutions. This raises the issue of whether we should routinely consider using non-multiprocessor hardware like the AP for applications where substantially simpler software solutions will normally exist. It also raises the question of whether the use of both AP and MIMD hardware in a single hetergeneous system could provide more versatility and efficiency. Either the AP or MIMD could serve as the primary system, but could hand off jobs it could not handle efficiently to the other system. © 2012 Elsevier Inc. All rights reserved.","Air traffic control (ATC); ASPRO; Associative processor (AP); Associative SIMD; ClearSpeed; Conflict detection and resolution (CD&R); Deterministic hardware; Federal Aviation Administration (FAA); Instruction stream (IS); MIMD; Multicore processor; Multiprocessor; NP-complete; OpenMP; Predictability; Real-time systems; SIMD; STARAN; Static scheduling; Worst case execution time"
"Upper and lower bounds for dynamic cluster assignment for multi-target tracking in heterogeneous WSNs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885178133&doi=10.1016%2fj.jpdc.2013.04.007&partnerID=40&md5=d538e5ab0cfcdb6ae4755a59de790a2f","In this paper, we consider the problem of cluster task assignment to maximize total utilities of nodes for target coverage in heterogeneous Wireless Sensor Networks. We define this problem as assigning the tasks of Cluster Head (CH) and Cluster Members (CM) to nodes for each target and requiring communication connectivity between every CH with its members. The utility of each node for each target is defined as a function of its distance to the target and its remaining energy. We propose an upper bound based on Lagrangian Relaxation (LR) and a lower bound by Linear Programming (LP) relaxation with a combination of Randomized Rounding (RR) and a greedy-based heuristic. Furthermore, we propose a distributed heuristic algorithm based on matching and a general assignment problem. Dynamic movements of targets are taken into account by intra/inter-cluster task reassignments. Simulation results, compared with optimal values, reveal that the LR upper bound performs better than the bound reached by pure LP relaxation. The lower bound obtained by LP relaxation combined with the RR technique provides close results in comparison with the distributed heuristic algorithm. Furthermore, the results of the distributed heuristic algorithm remain between the upper and lower bounds and close to optimal values. © 2013 Elsevier Ltd.","Clustering; Heterogeneous nodes; Lagrangian relaxation; LP relaxation; Randomized rounding; Task assignment; Task reassignment; WSN"
"A bottom-up model for heterogeneous BitTorrent systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885181337&doi=10.1016%2fj.jpdc.2013.04.003&partnerID=40&md5=ffe55c5541f16e66e3806060da84112c","BT system modeling has great importance for understanding the system performance and improving the protocol design. The challenge of accurate BT system modeling lies in the complicated peer behavior in the dynamic and heterogeneous system. In this paper, we propose a bottom-up model to simplify the performance analysis and increase the modeling accuracy, in which local transitions of the system lead to global macroscopic descriptions by integration. To be specific, we evaluate the download rate of each individual peer by estimating the probability of a connection being made, and the probability of a peer being unchoked, and then later integrate the analysis of each individual peer into the description of the whole system. Such a model characterizes both core parts of the protocol and main features of the system and requires far fewer restrictive assumptions than that used before. By using the model, we provide some interesting insights into setting of parameters, power of free-riding, effect of bandwidth distribution and effectiveness of unchoking. We also validate our model through both simulations and experiments. The preliminary experimental results show that our model is significantly more accurate than existing ones. © 2013 Elsevier Inc.","BitTorrent; Bottom-up; Heterogeneous; Model; Probabilistic"
"Parallel globally optimal structure learning of Bayesian networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885184846&doi=10.1016%2fj.jpdc.2013.04.001&partnerID=40&md5=9742898b6899fb7cc1a4ca86164151c7","Given n random variables and a set of m observations of each of the n variables, the Bayesian network structure learning problem is to learn a directed acyclic graph (DAG) on the n variables such that the implied joint probability distribution best explains the set of observations. Bayesian networks are widely used in many fields including data mining and computational biology. Globally optimal (exact) structure learning of Bayesian networks takes O(n2 · 2n) time plus the cost of O(n · 2n) evaluations of an applicationspecific scoring function whose run-time is at least linear in m. In this paper, we present a parallel algorithm for exact structure learning of a Bayesian network that is communication- efficient and workoptimal up to O(1/n · 2n) processors. We further extend this algorithm to the important restricted case of structure learning with bounded node in-degree and investigate the performance gains achievable because of limiting node in-degree. We demonstrate the applicability of our method by implementation on an IBM Blue Gene/P system and an AMD Opteron InfiniBand cluster and present experimental results that characterize run-time behavior with respect to the number of variables, number of observations, and the bound on in-degree. © 2013 Elsevier Inc.","Bayesian networks; Graphical models; Machine learning; Parallel algorithm; Structure learning"
"Requirement-aware strategies for scheduling real-time divisible loads on clusters","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885179626&doi=10.1016%2fj.jpdc.2013.03.013&partnerID=40&md5=0cd3aec8669d6b1b1b5dcaeba5906f0f","This paper investigates the real-time scheduling problem for handling heterogeneous divisible loads on cluster systems. Divisible load applications occur in many fields of science and engineering. Such applications can be easily parallelized in a master-worker fashion, but pose several scheduling challenges. We consider divisible loads associated with deadlines to enhance quality-of-service (QoS) and provide performance guarantees in distributed computing environments. In addition, since the divisible loads to be performed may widely vary in terms of their required hardware and software, we capture the loads' various processing requirements in our load distribution strategies, a unique feature that is applicable for running proprietary applications only on certain eligible processing nodes. Thus in our problem formulation each load can only be processed by certain processors as both the loads and processors are heterogeneous. We propose scheduling algorithms referred to as Requirements-Aware Real-Time Scheduling (RARTS) algorithms, which consist of a novel scheduling policy, referred to as Minimum Slack Capacity First (MSCF), and two multi-round load distribution strategies, referred to as All Eligible Processors (AEP) and Least Capability First (LCF). We perform rigorous performance evaluation studies to quantify the performance of our strategies on a variety of scenarios. © 2013 Elsevier Inc.","Cluster computing; Communication delay; Divisible loads; Parallel processing; Real-time scheduling"
"Resource-efficient authentic key establishment in heterogeneous wireless sensor networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870951943&doi=10.1016%2fj.jpdc.2012.10.004&partnerID=40&md5=262d15ff0562e93831abb41faeecfbbc","A key challenge for the protection of heterogeneous wireless sensor networks (HWSNs) is how to enable sensor nodes to establish shared cryptographic keys in an authentic and resource-efficient manner for their secure communications. So far, a number of schemes have been developed to address this challenge. However, a main shortcoming of these schemes is that their use of sensor resources is not properly balanced. This has motivated us to propose a new lightweight scheme to rectify the problem. The scheme only employs symmetric cryptosystems for its design. It takes advantage of the hierarchical clustering feature of a HWSN to deliver a novel way of building vertical key shareability before sensor deployment to enable horizontal key shareability after the deployment for authentic shared key establishment. The scheme evaluation shows that it offers strong authenticity and resilience against various security threats, and is more resource-efficient, flexible and scalable than related work. © 2012 Elsevier Inc. All rights reserved.","Heterogeneous wireless sensor network; Key establishment; Node authentication"
"Parallel approaches to machine learning - A comprehensive survey","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880780351&doi=10.1016%2fj.jpdc.2012.11.001&partnerID=40&md5=ae8eaade2b17a70a029d93062cd56078","Literature has always witnessed efforts that make use of parallel algorithms / parallel architecture to improve performance; machine learning space is no exception. In fact, a considerable effort has gone into this area in the past fifteen years. Our report attempts to bring together and consolidate such attempts. It tracks the development in this area since the inception of the idea in 1995, identifies different phases during the time period 1995-2011 and marks important achievements. When it comes to performance enhancement, GPU platforms have carved a special niche for themselves. The strength of these platforms comes from the capability to speed up computations exponentially by way of parallel architecture / programming methods. While it is evident that computationally complex processes like image processing, gaming etc. stand to gain much from parallel architectures; studies suggest that general purpose tasks such as machine learning, graph traversal, and finite state machines are also identified as the parallel applications of the future. Map reduce is another important technique that has evolved during this period and as the literature has it, it has been proved to be an important aid in delivering performance of machine learning algorithms on GPUs. The report summarily presents the path of developments. © 2012 Elsevier Inc. All rights reserved.","Distributed and parallel machine learning; GPU; Map reduce"
"Parallel multi-objective Ant Programming for classification using GPUs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875170671&doi=10.1016%2fj.jpdc.2013.01.017&partnerID=40&md5=ba2b7ed2bc31f062c36a7347ab1e18f2","Classification using Ant Programming is a challenging data mining task which demands a great deal of computational resources when handling data sets of high dimensionality. This paper presents a new parallelization approach of an existing multi-objective Ant Programming model for classification, using GPUs and the NVIDIA CUDA programming model. The computational costs of the different steps of the algorithm are evaluated and it is discussed how best to parallelize them. The features of both the CPU parallel and GPU versions of the algorithm are presented. An experimental study is carried out to evaluate the performance and efficiency of the interpreter of the rules, and reports the execution times and speedups regarding variable population size, complexity of the rules mined and dimensionality of the data sets. Experiments measure the original single-threaded and the new multi-threaded CPU and GPU times with different number of GPU devices. The results are reported in terms of the number of Giga GP operations per second of the interpreter (up to 10 billion GPops/s) and the speedup achieved (up to 834× vs CPU, 212× vs 4-threaded CPU). The proposed GPU model is demonstrated to scale efficiently to larger datasets and to multiple GPU devices, which allows the expansion of its applicability to significantly more complicated data sets, previously unmanageable by the original algorithm in reasonable time.© 2013 Elsevier B.V. All rights reserved.","Ant colony optimization (ACO); Ant programming (AP); Classification; GPU; Parallel computing"
"A case study on expressiveness and performance of component-oriented parallel programming","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874096440&doi=10.1016%2fj.jpdc.2012.12.007&partnerID=40&md5=e6c4e0363c9bf8e5e4ba400f32932e2c","Component-oriented programming has been applied to address the requirements of large-scale applications from computational sciences and engineering that present high performance computing (HPC) requirements. However, parallelism continues to be a challenging requirement in the design of CBHPC (Component-Based High Performance Computing) platforms. This paper presents strong evidence about the efficacy and the efficiency of HPE (Hash Programming Environment), a CBHPC platform that provides full support for parallel programming, on the development, deployment and execution of numerical simulation code onto cluster computing platforms. © 2012 Elsevier Inc. All rights reserved.","Component-based software engineering; High performance computing; Parallel programming; Performance evaluation"
"Stochastic DAG scheduling using a Monte Carlo approach","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885960215&doi=10.1016%2fj.jpdc.2013.07.019&partnerID=40&md5=42dc27100e296d315f193234f8e4e653","In heterogeneous computing systems, there is a need for solutions that can cope with the unavoidable uncertainty in individual task execution times, when scheduling DAGs. When such uncertainties occur, static DAG scheduling approaches may suffer, and some rescheduling may be necessary. Assuming that the uncertainty in task execution times is modelled in a stochastic manner, we may be able to use this information to improve static DAG scheduling considerably. In this paper, a novel DAG scheduling approach is proposed to solve this stochastic scheduling problem, based on a Monte Carlo method. The approach is built on the top of a classic static DAG scheduling heuristic and evaluated through extensive simulation. Empirical results show that a significant improvement of average application performance can be achieved by the proposed approach at a reasonable execution time cost. © 2013 Elsevier Inc. All rights reserved.","DAG scheduling; Directed acyclic graphs; Heterogeneous computing; Monte Carlo methods"
"Improved group-based cooperative caching scheme for mobile ad hoc networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893666877&doi=10.1016%2fj.jpdc.2012.12.013&partnerID=40&md5=2412d80888b7f7f44ac4057c1d31c645","Data caching is a popular technique that improves data accessibility in wired or wireless networks. However, in mobile ad hoc networks, improvement in access latency and cache hit ratio may diminish because of the mobility and limited cache space of mobile hosts (MHs). In this paper, an improved cooperative caching scheme called group-based cooperative caching (GCC) is proposed to generalize and enhance the performance of most group-based caching schemes. GCC allows MHs and their neighbors to form a group, and exchange a bitmap data directory periodically used for proposed algorithms, such as the process of data discovery, and cache placement and replacement. The goal is to reduce the access latency of data requests and efficiently use available caching space among MH groups. Two optimization techniques are also developed for GCC to reduce computation and communication overheads. The first technique compresses the directories using an aggregate bitmap. The second employs multi-point relays to develop a forwarding node selection scheme to reduce the number of broadcast messages inside the group. Our simulation results show that the optimized GCC yields better results than existing cooperative caching schemes in terms of cache hit ratio, access latency, and average hop count. © 2013 Elsevier Inc. All rights reserved.","Cooperative caching; Mobile ad hoc networks; Multi-point relays"
"MapReduce with communication overlap (MaRCO)","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893670179&doi=10.1016%2fj.jpdc.2012.12.012&partnerID=40&md5=2ce3a5dd1164d7fa5e6aca6518d2d08e","MapReduce is a programming model from Google for cluster-based computing in domains such as search engines, machine learning, and data mining. MapReduce provides automatic data management and fault tolerance to improve programmability of clusters. MapReduce's execution model includes an all-mapto- all-reduce communication, called the shuffle, across the network bisection. Some MapReductions move large amounts of data (e.g., as much as the input data), stressing the bisection bandwidth and introducing significant runtime overhead. Optimizing such shuffle-heavy MapReductions is important because (1) they include key applications (e.g., inverted indexing for search engines and data clustering for machine learning) and (2) they run longer than shuffle-light MapReductions (e.g., 5x longer). In MapReduce, the asynchronous nature of the shuffle results in some overlap between the shuffle and map. Unfortunately, this overlap is insufficient in shuffle-heavy MapReductions. We propose MapReduce with communication overlap (MaRCO) to achieve nearly full overlap via the novel idea of including reduce in the overlap. While MapReduce lazily performs reduce computation only after receiving all the map data, MaRCO employs eager reduce to process partial data from some map tasks while overlapping with other map tasks' communication. MaRCO's approach of hiding the latency of the inevitably high shuffle volume of shuffle-heavy MapReductions is fundamental for achieving performance. We implement MaRCO in Hadoop's MapReduce and show that on a 128-node Amazon EC2 cluster, MaRCO achieves 23% average speed-up over Hadoop for shuffle-heavy MapReductions. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Distributed processing; Large-scale data processing; MapReduce; Parallel computing; Performance optimization"
"Object protection in distributed systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874608882&doi=10.1016%2fj.jpdc.2013.01.008&partnerID=40&md5=4e8107325b68902163aeb23b557ab8e7","With reference to a distributed system consisting of nodes connected by a local area network, we consider a salient aspect of the protection problem, the representation of access permissions and protection domains. We present a model of a protection system supporting typed objects. Possession of an access permission for a given object is certified by possession of an object pointer including the specification of a set of access rights. We associate an encryption key with each object and a password with each domain. Object pointers are stored in memory in a ciphertext form obtained by using the object key and including the value of the domain password. Each process is executed in a domain and can take advantage of a given object pointer only if this object pointer was encrypted by including the password of this domain. A set of protection primitives makes it possible to use object pointers for object reference and to control the movements of the objects across the network. The resulting protection environment is evaluated from a number of salient viewpoints, including ease of access right distribution and revocation, interprocess interaction and cooperation, protection against fraudulent actions of access right manipulation and stealing, storage overhead, and network traffic. © 2013 Elsevier Inc. All rights reserved.","Access right; Distributed system; Domain; Object; Protection; Symmetric-key cryptography"
"Fault tolerant decentralised K-Means clustering for asynchronous large-scale networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880481501&doi=10.1016%2fj.jpdc.2012.09.009&partnerID=40&md5=895555937f017c65f651f893662782eb","The K-Means algorithm for cluster analysis is one of the most influential and popular data mining methods. Its straightforward parallel formulation is well suited for distributed memory systems with reliable interconnection networks, such as massively parallel processors and clusters of workstations. However, in large-scale geographically distributed systems the straightforward parallel algorithm can be rendered useless by a single communication failure or high latency in communication paths. The lack of scalable and fault tolerant global communication and synchronisation methods in large-scale systems has hindered the adoption of the K-Means algorithm for applications in large networked systems such as wireless sensor networks, peer-to-peer systems and mobile ad hoc networks. This work proposes a fully distributed K-Means algorithm (Epidemic K-M eans) which does not require global communication and is intrinsically fault tolerant. The proposed distributed K-Means algorithm provides a clustering solution which can approximate the solution of an ideal centralised algorithm over the aggregated data as closely as desired. A comparative performance analysis is carried out against the state of the art sampling methods and shows that the proposed method overcomes the limitations of the sampling-based approaches for skewed clusters distributions. The experimental analysis confirms that the proposed algorithm is very accurate and fault tolerant under unreliable network conditions (message loss and node failures) and is suitable for asynchronous networks of very large and extreme scale. © 2012 Elsevier Inc. All rights reserved.","Distributed clustering; Epidemic protocols; Extreme scale computing; Gossip protocols; K-Means; Peer-to-peer data mining"
"A CPU-GPU framework for optimizing the quality of large meshes","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885188497&doi=10.1016%2fj.jpdc.2013.03.007&partnerID=40&md5=c75a311d8b02adbd6767bf9a8a8138b0","The automatic generation of 3D finite element meshes (FEM) is still a bottleneck for the simulation of large fluid dynamic problems. Although today there are several algorithms that can generate good meshes without user intervention, in cases where the geometry changes during the calculation and thousands of meshes must be constructed, the computational cost of this process can exceed the cost of the FEM. There has been a lot of work in FEM parallelization and the algorithms work well in different parallel architectures, but at present there has not been much success in the parallelization of mesh generation methods. This paper will present a massive parallelization scheme for re-meshing with tetrahedral elements using the local modification algorithm. This method is frequently used to improve the quality of elements once the mesh has been generated, but we will show it can also be applied as a regeneration process, starting with the distorted and invalid mesh of the previous step. The parallelization is carried out using OpenCL and OpenMP in order to test the method in a multiple CPU architecture and also in Graphics Processing Units (GPUs). Finally we present the speedup and quality results obtained in meshes with hundreds of thousands of elements and different parallel APIs. © 2013 Elsevier Inc.","GPU; Parallelism; Quality; Re-meshing"
"An effective Parallel Multistart Tabu Search for Quadratic Assignment Problem on CUDA platform","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884813524&doi=10.1016%2fj.jpdc.2012.07.014&partnerID=40&md5=22f4e68ba706423b52b424440850a772","NVidia's powerful GPU hardware and CUDA platform enables the design of very fast parallel algorithms. Relatively little research has been done so far on GPU implementations of algorithms for computationally demanding discrete optimisation problems. In this paper, the well-known NP-hard Quadratic Assignment Problem (QAP) is considered. Detailed analysis of parallelisation possibilities, memory organisation and access patterns, enables the implementation of fast and effective heuristics for QAP on the GPU - the Parallel Multistart Tabu Search (PMTS). Computational experiments show that PMTS is capable of providing good quality (often optimal or the best known) solutions in a short time, and even better quality solutions in longer runs. PMTS runs up to 420× faster than a single-core counterpart, or 70× faster than a parallel CPU implementation on a high-end six-core CPU. © 2013 Elsevier Ltd. All rights reserved.","CUDA; Discrete optimisation; GPGPU; Multistart Tabu Search; Parallel algorithms; Quadratic Assignment Problem"
"Design space exploration in many-core processors for sound synthesis of plucked string instruments","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884815158&doi=10.1016%2fj.jpdc.2012.07.013&partnerID=40&md5=74f1528521695fe901dcd2339bc11b97","Recent advances in physics-based sound synthesis have unveiled numerous possibilities for the creation of new musical instruments. Despite the fact that research on physics-based sound synthesis has been going on for three decades, its higher computational complexity compared to that of signal modeling has limited its use in real-time applications. This limitation has motivated research on parallel processing architectures that support the physics-based sound synthesis of musical instruments. In this paper, we present analytical results of the design space exploration of many-core processors for the physics-based sound synthesis of plucked-string instruments including acoustic guitar, classical guitar and the gayageum, which is representative of a Korean plucked-string instrument. We do so by quantitatively evaluating the significance of a sample-per-processing-element (SPE) ratio-i.e., the amount of sample data directly mapped to each processing element, which is equivalent to varying the number of processing elements for a fixed sample size on system performance and efficiency using architectural and workload simulations. The effect of the sample-to-processor ratio is difficult to analyze because it fundamentally affects both hardware and software design when varied. In addition, the optimal SPE ratio is not typically at either extreme of its range-i.e., one sample per processor or one processor per an entire sample. This paper illustrates the correlation between a fixed problem sample size, SPE ratio and processing element (PE) architecture for a target implementation in 130-nm CMOS technology. Experimental results indicate that an SPE in the range of 5513 to 2756, which is equivalent to 48 to 96 PEs for guitars and 96 to 192 PEs for the gayageum, provides the most efficient operation for the synthesis of musical sounds sampled at 44.1 kHz, yielding the highest task throughput per unit area or per unit energy. In addition, the produced synthesized sounds appear to be very similar to the original sounds, and the selected optimal many-core configurations outperform commercial processor architectures including DSPs, FPGAs, and GPUs in terms of area efficiency and energy efficiency. © 2013 Elsevier Ltd. All rights reserved.","Design space exploration; Many-core processor architecture; Musical instruments; Physics-based sound synthesis; Sample-per-processing-element"
"Towards accelerating smoothed particle hydrodynamics simulations for free-surface flows on multi-GPU clusters","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884818782&doi=10.1016%2fj.jpdc.2012.07.010&partnerID=40&md5=1102adfccbee437661a20e86e3f90914","Starting from the single graphics processing unit (GPU) version of the Smoothed Particle Hydrodynamics (SPH) code DualSPHysics, a multi-GPU SPH program is developed for free-surface flows. The approach is based on a spatial decomposition technique, whereby different portions (sub-domains) of the physical system under study are assigned to different GPUs. Communication between devices is achieved with the use of Message Passing Interface (MPI) application programming interface (API) routines. The use of the sorting algorithm radix sort for inter-GPU particle migration and sub-domain ""halo"" building (which enables interaction between SPH particles of different sub-domains) is described in detail. With the resulting scheme it is possible, on the one hand, to carry out simulations that could also be performed on a single GPU, but they can now be performed even faster than on one of these devices alone. On the other hand, accelerated simulations can be performed with up to 32 million particles on the current architecture, which is beyond the limitations of a single GPU due to memory constraints. A study of weak and strong scaling behaviour, speedups and efficiency of the resulting program is presented including an investigation to elucidate the computational bottlenecks. Last, possibilities for reduction of the effects of overhead on computational efficiency in future versions of our scheme are discussed. © 2013 Elsevier Ltd. All rights reserved.","Computational fluid dynamics; CUDA; GPU; Graphics processing unit; Molecular dynamics; Multi-GPU; Smoothed particle hydrodynamics; SPH"
"An effective and efficient parallel approach for random graph generation over GPUs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886776151&doi=10.1016%2fj.jpdc.2012.09.010&partnerID=40&md5=b3e514cf1ffe38168fde0722029b3fd8","The widespread usage of random graphs has been highlighted in the context of database applications for several years. This because such data structures turn out to be very useful in a large family of database applications ranging from simulation to sampling, from analysis of complex networks to study of randomized algorithms, and so forth. Amongst others, Erdo{combining double acute accent}s-Rényi Γv,p is the most popular model to obtain and manipulate random graphs. Unfortunately, it has been demonstrated that classical algorithms for generating Erdo{combining double acute accent}s-Rényi based random graphs do not scale well in large instances and, in addition to this, fail to make use of the parallel processing capabilities of modern hardware. Inspired by this main motivation, in this paper we propose and experimentally assess a novel parallel algorithm for generating random graphs under the Erdo{combining double acute accent}s-Rényi model that is designed and implemented in a Graphics Processing Unit (GPU), called PPreZER. We demonstrate the nice amenities due to our solution via a succession of several intermediary algorithms, both sequential and parallel, which show the limitations of classical approaches and the benefits due to the PPreZER algorithm. Finally, our comprehensive experimental assessment and analysis brings to light a relevant average speedup gain of PPreZER over baseline algorithms. © 2012 Elsevier Inc. All rights reserved.","GPU; Parallel algorithm; Random graph; Random graph generation"
"p-PIC: Parallel power iteration clustering for big data","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886291762&doi=10.1016%2fj.jpdc.2012.06.009&partnerID=40&md5=4e3e772dad1cf1e5a4cb8d649c506cf0","Power iteration clustering (PIC) is a newly developed clustering algorithm. It performs clustering by embedding data points in a low-dimensional subspace derived from the similarity matrix. Compared to traditional clustering algorithms, PIC is simple, fast and relatively scalable. However, it requires the data and its associated similarity matrix fit into memory, which makes the algorithm infeasible for big data applications. This paper attempts to expand PIC's data scalability by implementing a parallel power iteration clustering (p-PIC). While this paper focuses on exploring different parallelization strategies and implementation details for minimizing computation and communication costs, we have also paid great attention to ensuring the algorithm works well on low-end commodity computers (COTS-based clusters and general purpose servers found at most commercial cloud providers). The experimental results demonstrate that the proposed p-PIC algorithm is highly scalable to both data and compute resources. © 2012 Elsevier Inc. All rights reserved.","Big data; Cloud computing; Clustering; Data-mining; Distributed computing; Machine learning; Parallel computing; Spectral clustering"
"Design of an efficient communication infrastructure for highly contended locks in many-core CMPs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879099062&doi=10.1016%2fj.jpdc.2012.06.010&partnerID=40&md5=31a3cbb30280d47de88ef24ac491c070","Lock synchronization is a key programming primitive for shared-memory many-core CMPs. However, as the number of cores increases, conventional software implementations cannot meet the desirable levels of performance and scalability. Meanwhile, most existing hardware-supported lock proposals require modifications at some level of the memory hierarchy, thus degrading QoS of applications through synchronization traffic. In this paper, we propose GLock, a dedicated network infrastructure and a token-based message-passing protocol to provide a non-intrusive, extremely efficient and fair implementation for highly contended locks. Two implementations of GLock are considered. The first leverages current full-custom G-lines technology, whilst the second uses a cost-effective mainstream industrial toolflow with an advanced 45 nm technology. When compared with the most efficient software-based lock, both alternatives provide significant reductions in execution time, network traffic and power consumption, for a representative set of benchmarks, with negligible area overhead. © 2013 Elsevier Ltd. All rights reserved.","Cache coherence; Energy efficiency; Lock synchronization; Many-core CMPs; Scalability"
"Simulating cortical networks on heterogeneous multi-GPU systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879109448&doi=10.1016%2fj.jpdc.2012.02.006&partnerID=40&md5=5c0ed87f8ca6c338d56b1114da0ee3d4","Recent advances in neuroscientific understanding have highlighted the highly parallel computation power of the mammalian neocortex. In this paper we describe a GPGPU-accelerated implementation of an intelligent learning model inspired by the structural and functional properties of the neocortex. Furthermore, we consider two inefficiencies inherent to our initial implementation and propose software optimizations to mitigate such problems. Analysis of our application's behavior and performance provides important insights into the GPGPU architecture, including the number of cores, the memory system, atomic operations, and the global thread scheduler. Additionally, we create a runtime profiling tool for the cortical network that proportionally distributes work across the host CPU as well as multiple GPGPUs available to the system. Using the profiling tool with these optimizations on Nvidia's CUDA framework, we achieve up to 60× speedup over a single-threaded CPU implementation of the model. © 2013 Elsevier Ltd. All rights reserved.","Cortical learning algorithms; CUDA; GPGPU; Profiling systems"
"Energy saving strategies for parallel applications with point-to-point communication phases","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885176218&doi=10.1016%2fj.jpdc.2013.03.006&partnerID=40&md5=b000d7f7a1731c2e45e1e23cd35d530a","Although high-performance computing traditionally focuses on the efficient execution of large-scale applications, both energy and power have become critical concerns when approaching exascale. Drastic increases in the power consumption of supercomputers affect significantly their operating costs and failure rates. In modern microprocessor architectures, equipped with dynamic voltage and frequency scaling (DVFS) and CPU clock modulation (throttling), the power consumption may be controlled in software. Additionally, network interconnect, such as Infiniband, may be exploited to maximize energy savings while the application performance loss and frequency switching overheads must be carefully balanced. This paper advocates for a runtime assessment of such overheads by means of characterizing point-to-point communications into phases followed by analyzing the time gaps between the communication calls. Certain communication and architectural parameters are taken into consideration in the three proposed frequency scaling strategies, which differ with respect to their treatment of the time gaps. The experimental results are presented for NAS parallel benchmark problems as well as for the realistic parallel electronic structure calculations performed by the widely used quantum chemistry package GAMESS. For the latter, three different process-to-core mappings were studied as to their energy savings under the proposed frequency scaling strategies and under the existing state-of-theart techniques. Close to the maximum energy savings were obtained with a low performance loss of 2% on the given platform. © 2013 Elsevier Inc.","Dynamic voltage and frequency scaling; GAMESS; Multicore platforms; Point-to-point communications"
"A decentralized approach for mining event correlations in distributed system monitoring","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886794659&doi=10.1016%2fj.jpdc.2012.09.007&partnerID=40&md5=37e3f0d7828071a744216c15e2da7ea3","Nowadays, there is an increasing demand to monitor, analyze, and control large scale distributed systems. Events detected during monitoring are temporally correlated, which is helpful to resource allocation, job scheduling, and failure prediction. To discover the correlations among detected events, many existing approaches concentrate detected events into an event database and perform data mining on it. We argue that these approaches are not scalable to large scale distributed systems as monitored events grow so fast that event correlation discovering can hardly be done with the power of a single computer. In this paper, we present a decentralized approach to efficiently detect events, filter irrelative events, and discover their temporal correlations. We propose a MapReduce-based algorithm, MapReduce-Apriori, to data mining event association rules, which utilizes the computational resource of multiple dedicated nodes of the system. Experimental results show that our decentralized event correlation mining algorithm achieves nearly ideal speedup compared to centralized mining approaches. © 2012 Elsevier Inc. All rights reserved.","Data mining; Decentralized; Distributed system; Event correlations"
"PHAST: Hardware-accelerated shortest path trees","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879125071&doi=10.1016%2fj.jpdc.2012.02.007&partnerID=40&md5=8c3cea49e7f4b52a58b4e4029dd7fa66","We present a novel algorithm to solve the non-negative single-source shortest path problem on road networks and graphs with low highway dimension. After a quick preprocessing phase, we can compute all distances from a given source in the graph with essentially a linear sweep over all vertices. Because this sweep is independent of the source, we are able to reorder vertices in advance to exploit locality. Moreover, our algorithm takes advantage of features of modern CPU architectures, such as SSE and multiple cores. Compared to Dijkstra's algorithm, our method needs fewer operations, has better locality, and is better able to exploit parallelism at multi-core and instruction levels. We gain additional speedup when implementing our algorithm on a GPU, where it is up to three orders of magnitude faster than Dijkstra's algorithm on a high-end CPU. This makes applications based on all-pairs shortest-paths practical for continental-sized road networks. Several algorithms, such as computing the graph diameter, arc flags, or exact reaches, can be greatly accelerated by our method. © 2013 Elsevier Ltd. All rights reserved.","GPU; High performance computing; Route planning; Shortest paths"
"Universal adaptive self-stabilizing traversal scheme: Random walk and reloading wave","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870949658&doi=10.1016%2fj.jpdc.2012.10.006&partnerID=40&md5=20afc8b7b9dac57d1a3504548bd1c6a6","In this paper, we investigate random walk based token circulation in dynamic environments subject to faults. We describe hypotheses on the dynamic environment that allow random walks to meet the important property that the token visits any node infinitely often. The randomness of this scheme allows it to work on any topology, and requires no adaptation after a topological change, which is a desirable property for applications to dynamic systems. For random walks to be a traversal scheme and to solve the concurrency problem, one needs to guarantee that exactly one token circulates in the system. In the presence of transient faults, configurations with multiple tokens or with no token can occur. The meeting property of random walks solves the cases with multiple tokens. The reloading wave mechanism we propose, together with timeouts, allows us to detect and solve cases with no token. This traversal scheme is self-stabilizing, and universal, meaning that it needs no assumption on the system topology. We describe conditions on the dynamicity (with a local detection criterion) under which the algorithm is tolerant to dynamic reconfigurations. We conclude with a study on the time between two visits of the token to a node, which we use to tune the parameters of the reloading wave mechanism according to some system characteristics. © 2012 Elsevier Inc. All rights reserved.","Distributed algorithms; Fault-tolerance; Random walk; Self-stabilization; Token circulation; Universal traversal scheme"
"A cross-layer optimization based integrated routing and grooming algorithm for green multi-granularity transport networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875754541&doi=10.1016%2fj.jpdc.2013.02.010&partnerID=40&md5=da4439cd0deebeeb86dcb1ffeb951f1d","With the development of IP networks and intelligent optical switch networks, the backbone network tends to be a multi-granularity transport one. In a multi-granularity transport network (MTN), due to the rapid growth of various applications, the scale and complexity of network devices are significantly enhanced. Meanwhile, to deal with bursty IP traffic, the network devices need to provide continuous services along with excessive power consumption. It has attracted wide attention from both academic and industrial communities to build a power-efficient MTN. In this paper, we design an effective node structure for MTN. Considering the power savings on both IP and optical transport layers, we propose a mathematical model to achieve a cross-layer optimization objective for power-efficient MTN. Since this optimization problem is NP-hard (Hasan et al. (2010) [11]) and heuristic or intelligent optimization algorithms have been successfully applied to solve such kinds of problems in many engineering domains (Huang et al. (2011) [13], Li et al. (2011) [17] and Dong et al. (2011) [5]), a Green integrated Routing and Grooming algorithm based on Biogeography-Based Optimization (Simon (2008) [23]) (GRGBBO) is also presented. The simulation results demonstrate that, compared with the other BBO based and state-of-the-art power saving approaches, GRGBBO improves the power savings at a rate between 2%-15% whilst the high-level multi-user QoS (Quality of Services) satisfaction degree (MQSD) is guaranteed. GRGBBO is therefore an effective technique to build a power-efficient MTN. © 2013 Elsevier Inc. All rights reserved.","Biogeography-based optimization Multi-granularity transport network Green integrated routing and grooming Multi-user QoS satisfaction degree"
"Lowest priority first based feasibility analysis of real-time systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885181475&doi=10.1016%2fj.jpdc.2013.03.016&partnerID=40&md5=a0424f893bffecf7809177731ce28c6a","The feasibility problem of periodic tasks under fixed priority systems has always been a critical research issue in real-time systems and a number of feasibility tests have been proposed to guarantee the timing requirements of real-time systems. These tests can be broadly classified into: (a) inexact and (b) exact tests. The inexact tests are applied to the task sets that present lower utilization, while the exact tests become inevitable when system utilization is high. The exact tests can be further classified into: (a) Scheduling Points Tests (SPT) and (b) Response Time Tests (RTT). The SPT analyze task set feasibility at the arrival times while the RTT utilize fixed-point techniques to determine task feasibility. All of the available exact feasibility tests, whichever class it belongs to, share pseudo-polynomial complexity. Therefore, the aforementioned tests become impractical for online systems. Currently, both SPT and RTT employ the Highest Priority First (HPF) approach, which determines the system feasibility by testing the schedulability of individual tasks in the decreasing order of priority. In contrast, this work exploits the Lowest Priority First (LPF) alternative which is an aggressive solution based on the observation that the system infeasibility is primarily due to the lower priority tasks and not because of the higher priority tasks. For the average case analysis, our technique demonstrates promising results. Moreover, in the worst case scenario our solution is no inferior to the existing state of the art alternatives. We compare our proposed technique with the existing tests: (a) by counting the number of scheduling points used by a test that belongs to the SPT, (b) by counting the number of inner-most loops executed by an algorithm for the RTT, and (c) by measuring the actual running time of the existing alternatives. © 2013 Elsevier Inc.","Feasibility analysis; Fixed-priority scheduling; Online schedulability; Real-time systems"
"Computing resultants on Graphics Processing Units: Towards GPU-accelerated computer algebra","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884817600&doi=10.1016%2fj.jpdc.2012.07.015&partnerID=40&md5=9cbc75ae417803b72b14f20221f0a805","In this article we report on our experience in computing resultants of bivariate polynomials on Graphics Processing Units (GPU). Following the outline of Collins' modular approach [6], our algorithm starts by mapping the input polynomials to a finite field for sufficiently many primes m. Next, the GPU algorithm evaluates the polynomials at a number of fixed points xâ̂̂Zm, and computes a set of univariate resultants for each modular image. Afterwards, the resultant is reconstructed using polynomial interpolation and Chinese remaindering. The GPU returns resultant coefficients in the form of Mixed Radix (MR) digits. Finally, large integer coefficients are recovered from the MR representation on the CPU. All computations performed by the algorithm (except for, partly, Chinese remaindering) are outsourced to the graphics processor thereby minimizing the amount of work to be done on the host machine. The main theoretical contribution of this work is the modification of Collins' modular algorithm using the methods of matrix algebra to make an efficient realization on the GPU feasible. According to the benchmarks, our algorithm outperforms a CPU-based resultant algorithm from 64-bit Maple 14 by a factor of 100. © 2013 Elsevier Ltd. All rights reserved.","CUDA; GPU; Modular computations; Parallel computing; Resultants; Symbolic algorithms"
"Parallel multi-dimensional range query processing with R-trees on GPU","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885175081&doi=10.1016%2fj.jpdc.2013.03.015&partnerID=40&md5=e21fee0491b8004301a825c398543410","The general purpose computing on graphics processing unit (GP-GPU) has emerged as a new cost effective parallel computing paradigm in high performance computing research that enables large amount of data to be processed in parallel. Large scale scientific data intensive applications have been playing an important role in modern high performance computing research. A common access pattern into such scientific data analysis applications is multi-dimensional range query, but not much research has been conducted on multi-dimensional range query on the GPU. Inherently multi-dimensional indexing trees such as R-Trees are not well suited for GPU environment because of its irregular tree traversal. Traversing irregular tree search path makes it hard to maximize the utilization of massively parallel architectures. In this paper, we propose a novel MPTS (Massively Parallel Three-phase Scanning) R-tree traversal algorithm for multi-dimensional range query, that converts recursive access to tree nodes into sequential access. Our extensive experimental study shows that MPTS R-tree traversal algorithm on NVIDIA Tesla M2090 GPU consistently outperforms traditional recursive R-trees search algorithm on Intel Xeon E5506 processors. © 2013 Elsevier Inc.","CUDA; GPGPU; GPU; Parallel indexing; Parallel multi-dimensional indexing; Parallel R-tree"
"Building a reliable and high-performance content-based publish/subscribe system","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872818939&doi=10.1016%2fj.jpdc.2012.12.014&partnerID=40&md5=4b3d734157cd6f6f7da3e5061acac171","Provisioning reliability in a high-performance content-based publish/subscribe system is a challenging problem. The inherent complexity of content-based routing makes message loss detection and recovery, and network state recovery extremely complicated. Existing proposals either try to reduce the complexity of handling failures in a traditional network architecture, which only partially address the problem, or rely on robust network architectures that can gracefully tolerate failures, but perform less efficiently than the traditional architectures. In this paper, we present a hybrid network architecture for reliable and high-performance content-based publish/subscribe. Two overlay networks, a high-performance one with moderate fault tolerance and a highly-robust one with sufficient performance, work together to guarantee the performance of normal operations and reliability in the presence of failures. Our design exploits the fact that, in a high-performance content-based publish/subscribe system, subscriptions are broadcast to all brokers, to facilitate efficient backup routing when failures occur, which incurs a minimal overhead. Per-hop reliability is used to gracefully detect and recover lost messages that are caused by transit errors. Two backup routing methods based on DHT routing are proposed. Extensive simulation experiments are conducted. The results demonstrate the superior performance of our system compared to other state-of-the-art proposals. © 2013 Elsevier Inc. All rights reserved.","Content-based publish/subscribe; Fault tolerance; High-performance system design; Reliable distributed system"
"StreamTMC: Stream compilation for tiled multi-core architectures","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893651689&doi=10.1016%2fj.jpdc.2012.12.001&partnerID=40&md5=6414b469d78edeffd6a3b8a1ebbdf926","Tiled multi-core architectures have become an important kind of multi-core design for its good scalability and low power consumption. Stream programming has been productively applied to a number of important application domains. It provides an attractive way to exploit the parallelism. However the architecture characteristics of large amounts of cores memory hierarchy and exposed communication between tiles have presented a performance challenge for stream programs running on tiled multicores. In this paper we present StreamTMC an efficient stream compilation framework that optimizes the execution of stream applications for the tiled multi-core. This framework is composed of three optimization phases. First a software pipelining schedule is constructed to exploit the parallelism. Second an efficient hybrid of SPM and cache buffer allocation algorithm and data copy elimination mechanism is proposed to improve the efficiency of the data access. Last a communication aware mapping is proposed to reduce the network communication and synchronization overhead. We implement the StreamTMC compiler on Godson-T a 64-core tiled architecture and conduct an experimental study to verify the effectiveness. The experimental results indicate that StreamTMC can achieve an average of 58% improvement over the performance before optimization. © 2012 Elsevier Inc. All rights reserved.","Godson-T; Stream compilation; StreamTMC; Tiled multi-core"
"Reliability and performance optimization of pipelined real-time systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876105966&doi=10.1016%2fj.jpdc.2013.02.009&partnerID=40&md5=a72939821a9240a220900b4321868363","We consider pipelined real-time systems that consist of a chain of tasks executing on a distributed platform. The processing of the tasks is pipelined: each processor executes only one interval of consecutive tasks. We are interested in minimizing both the input-output latency and the period of application mapping. For dependability reasons, we are also interested in maximizing the reliability of the system. We therefore assign several processors to each interval of tasks, so as to increase the reliability of the system. Both processors and communication links are unreliable and subject to transient failures. We assume that the arrival of the failures follows a constant parameter Poisson law, and that the failures are statistically independent events. We study several variants of this multiprocessor mapping problem, with several hypotheses on the target platform (homogeneous/heterogeneous speeds and/or failure rates). We provide NP-hardness complexity results, and optimal mapping algorithms for polynomial problem instances. Efficient heuristics are presented to solve the general case, and experimental results are provided. © 2013 Elsevier Ltd. All rights reserved.","Complexity results; Interval mapping; Multi-criteria (reliability, latency, period) optimization; Pipelined real-time systems"
"Parallel rare term vector replacement: Fast and effective dimensionality reduction for text","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886794133&doi=10.1016%2fj.jpdc.2012.08.008&partnerID=40&md5=c39f3efa284d41a67a19f6857b508c5b","Dimensionality reduction is an established area in text mining and information retrieval. These methods convert the highly sparse corpus matrices into dense matrix format while preserving or improving the classification accuracy or retrieval performance. In this paper, we describe a novel approach to dimensionality reduction for text, along with a parallel algorithm suitable for private memory parallel computer systems. According to Zipf's law, the majority of indexing terms occurs only in a small number of documents. Our algorithm replaces rare terms by computing a vector which expresses their semantics in terms of common terms. This process produces a projection matrix, which can be applied to a corpus matrix and individual document and query vectors. We give an accurate mathematical and algorithmic description of our algorithms and present an experimental evaluation on two benchmark corpora. These experiments indicate that our algorithm can deliver a substantial reduction in the number of features, from 47,236 to 392 features on the Reuters corpus with a clear improvement in the retrieval performance. We have evaluated our parallel implementation using the message passing interface with up to 32 processes on a Nehalem Xeon cluster, computing the projection matrix for the dimensionality reduction for over 800,000 documents in just under 100 s. © 2012 Elsevier Inc. All rights reserved.","Dimensionality reduction; Information retrieval; Parallel algorithms"
"KNEM: A generic and scalable kernel-assisted intra-node MPI communication framework","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870930880&doi=10.1016%2fj.jpdc.2012.09.016&partnerID=40&md5=3fba25cd3d2c95ec4675fe30761435d2","The multiplication of cores in today's architectures raises the importance of intra-node communication in modern clusters and their impact on the overall parallel application performance. Although several proposals focused on this issue in the past, there is still a need for a portable and hardware-independent solution that addresses the requirements of both point-to-point and collective MPIoperations inside shared-memory computing nodes. This paper presents the KNEM module for the Linux kernel that provides MPI implementations with a flexible and scalable interface for performing kernel-assisted single-copy data transfers between local processes. It enables high-performance communication within most existing MPI implementations and brings significant application performance improvements thanks to more efficient point-to-point and collective operations. © 2012 Elsevier Inc. All rights reserved.","Collective operations; Hardware-independence; Intra-node communication; Kernel assistance; KNEM; MPI; Portability"
"Grex: An efficient MapReduce framework for graphics processing units","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893688166&doi=10.1016%2fj.jpdc.2013.01.004&partnerID=40&md5=4fc81ee31fbf1031e93035cdd1ce75bd","In this paper, we present a new MapReduce framework, called Grex, designed to leverage general purpose graphics processing units (GPUs) for parallel data processing. Grex provides several new features. First, it supports a parallel split method to tokenize input data of variable sizes, such as words in e-books or URLs in web documents, in parallel using GPU threads. Second, Grex evenly distributes data to map/reduce tasks to avoid data partitioning skews. In addition, Grex provides a new memory management scheme to enhance the performance by exploiting the GPU memory hierarchy. Notably, all these capabilities are supported via careful system design without requiring any locks or atomic operations for thread synchronization. The experimental results show that our system is up to 12.4× and 4.1× faster than two state-of-the-art GPU-based MapReduce frameworks for the tested applications. © 2013 Elsevier Inc. All rights reserved.","GPGPU; MapReduce; Shared memory; SIMT"
"Continuous data aggregation and capacity in probabilistic wireless sensor networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875169350&doi=10.1016%2fj.jpdc.2013.02.005&partnerID=40&md5=7b1fe0b83c3f20b0e1109577201d6bd5","Due to the existence of many probabilistic lossy links in Wireless Sensor Networks (WSNs) (Liu et al., 2010) [25], it is not practical to study the network capacity issue under the Deterministic Network Model (DNM). A more realistic one is actually the Probabilistic Network Model (PNM). Therefore, we study the Snapshot Data Aggregation (SDA) problem, the Continuous Data Aggregation (CDA) problem, and their achievable capacities for probabilistic WSNs under both the independent and identically distributed (i.i.d.) node distribution model and the Poisson point distribution model in this paper. First, we partition a network into cells and use two vectors to further partition these cells into equivalent color classes. Subsequently, based on the partitioned cells and equivalent color classes, we propose a Cell-based Aggregation Scheduling (CAS) algorithm for the SDA problem in probabilistic WSNs. Theoretical analysis of CAS and the upper bound capacity of the SDA problem show that the achievable capacities of CAS are all order optimal in the worst case, the average case, and the best case. For the CDA problem in probabilistic WSNs, we propose a Level-based Aggregation Scheduling (LAS) algorithm. LAS gathers the aggregation values of continuous snapshots by forming a data aggregation/transmission pipeline on the segments and scheduling all the cell-levels in a cell-level class concurrently. By theoretical analysis of LAS and the upper bound capacity of the CDA problem, we prove that LAS also successfully achieves order optimal capacities in all the cases. The extensive simulation results further validate the effectiveness of CAS and LAS.","Wireless sensor networks Probabilistic wireless sensor networks Snapshot data aggregation Continuous data aggregation Physical interference model Network capacity"
"An investigation of the performance portability of OpenCL","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884816817&doi=10.1016%2fj.jpdc.2012.07.005&partnerID=40&md5=698c4ba258a2c86861ebeb8a4350a070","This paper reports on the development of an MPI/OpenCL implementation of LU, an application-level benchmark from the NAS Parallel Benchmark Suite. An account of the design decisions addressed during the development of this code is presented, demonstrating the importance of memory arrangement and work-item/work-group distribution strategies when applications are deployed on different device types. The resulting platform-agnostic, single source application is benchmarked on a number of different architectures, and is shown to be 1.3-1.5× slower than native FORTRAN 77 or CUDA implementations on a single node and 1.3-3.1× slower on multiple nodes. We also explore the potential performance gains of OpenCL's device fissioning capability, demonstrating up to a 3× speed-up over our original OpenCL implementation. © 2013 Elsevier Ltd. All rights reserved.","GPU computing; High performance computing; Many-core computing; OpenCL; Optimisation"
"A segmentation approach for file broadcast scheduling","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885142798&doi=10.1016%2fj.jpdc.2013.06.003&partnerID=40&md5=19c71728bc2174d80e5726be535a9734","We study the broadcast scheduling problem in which clients send their requests to a server in order to receive some files available on the server. The server may be scheduled in a way that several requests are satisfied in one broadcast. When files are transmitted over computer networks, broadcasting the files by fragmenting them provides flexibility in broadcast scheduling that allows the optimization of per user response time. The broadcast scheduling algorithm, then, is in charge of determining the number of segments of each file and their order of transmission in each round of transmission. In this paper, we obtain a closed form approximation formula which approximates the optimal number of segments for each file, aiming at minimizing the total response time of requests. The obtained formula is a function of different parameters including those of underlying network as well as those of requests arrived at the server. Based on the obtained approximation formula we propose an algorithm for file broadcast scheduling which leads to total response time which closely conforms to the optimum one. We use extensive simulation and numerical study in order to evaluate the proposed algorithm which reveals high accuracy of obtained analytical approximation. We also investigate the impact of various headers that different network protocols add to each file segment. Our segmentation approach is examined for scenarios with different file sizes at the range of 100 KB to 1 GB. Our results show that for this range of file sizes the segmentation approach shows on average 13% tolerance from that of optimum in terms of total response time and the accuracy of the proposed approach is growing by increasing file size. Besides, using proposed segmentation in this work leads to a high Goodput of the scheduling algorithm. © 2013 Elsevier Ltd.","Broadcast; Scheduling; Segmentation"
"Estimating parallel performance","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876133890&doi=10.1016%2fj.jpdc.2013.01.011&partnerID=40&md5=f69a5fe1d2ce9406a0fe94ee09c7344a","In this paper we introduce our estimation method for parallel execution times, based on identifying separate ""parts"" of the work done by parallel programs. Our run time analysis works without any source code inspection. The time of parallel program execution is expressed in terms of the sequential work and the parallel penalty. We measure these values for different problem sizes and numbers of processors and estimate them for unknown values in both dimensions using statistical methods. This allows us to predict parallel execution time for unknown inputs and non-available processor numbers with high precision. Our prediction methods require orders of magnitude less data points than existing approaches. We verified our approach on parallel machines ranging from a multicore computer to a peta-scale supercomputer. Another useful application of our formalism is a new measure of parallel program quality. We analyse the values for parallel penalty for both growing input size and for increasing numbers of processing elements. From these data, conclusions on parallel performance and scalability are drawn. © 2013 Elsevier Inc. All rights reserved.","Parallel run time estimation; Scalability measure"
"C2FPGA - A dependency-timing graph design methodology","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884817625&doi=10.1016%2fj.jpdc.2012.09.001&partnerID=40&md5=1792a72aa04ff7c8859904ebc53a4926","In this paper, we present a design methodology that uses a combined graphical and scheduling technique to map C-based high level language (HLL) based applications to FPGA. Although there are a number of approaches addressing the mapping from HLL to hardware, many of these existing solutions either require a steep learning curve or do not produce an appropriate mapping pattern for the hardware platform. We provide a solution to this problem, by analyzing the data flow and data dependencies in the given code and proposing a scheduling patterns for the given algorithm. We then provide a suitable mapping pattern for the hardware platform. We use the mapping pattern to deliver synthesizable HDL (Verilog) code. We demonstrate our design methodology with results from different real-time case studies that are based on different algorithms. © 2013 Elsevier Ltd. All rights reserved.","Algorithms; Applications; Compiler; Data dependency; Scheduling"
"Energy-aware routing in hybrid optical network-on-chip for future multi-processor system-on-chip","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870942471&doi=10.1016%2fj.jpdc.2012.09.018&partnerID=40&md5=3fb24c0eff25a89c0bd205ebd7cad88e","With the development of Multi-Processor System-on-Chip (MPSoC) in recent years, the intra-chip communication is becoming the bottleneck of the whole system. Current electronic network-on-chip (NoC) designs face serious challenges, such as bandwidth, latency and power consumption. Optical interconnection networks are a promising technology to overcome these problems. In this paper, we study the routing problem in optical NoCs with arbitrary network topologies. Traditionally, a minimum hop count routing policy is employed for electronic NoCs, as it minimizes both power consumption and latency. However, due to the special architecture of current optical NoC routers, such a minimum-hop path may not be energy-wise optimal. Using a detailed model of optical routers we reduce the energy-aware routing problem into a shortest-path problem, which can then be solved using one of the many well known techniques. By applying our approach to different popular topologies, we show that the energy consumed in data communication in an optical NoC can be significantly reduced. We also propose the use of optical burst switching (OBS) in optical NoCs to reduce control overhead, as well as an adaptive routing mechanism to reduce energy consumption without introducing extra latency. Our simulation results demonstrate the effectiveness of the proposed algorithms. © 2012 Elsevier Inc. All rights reserved.","Energy aware; On-chip interconnection networks; Optical routers; Routing algorithm"
"Enhancing group communication with self-manageable behavior","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872899777&doi=10.1016%2fj.jpdc.2012.12.005&partnerID=40&md5=f87361ff9a828dbda19ccc44fd7e3a2d","Group communication protocols (GCPs) play an important role in the design of modern distributed systems. A typical GCP exchanges control messages to provide message delivery guarantees, and a key point in the configuration of such a protocol is to establish the right trade-off between message overhead and delivery latency. This trade-off becomes even a greater challenge in systems where computing resources and application requirements may change at runtime. In such scenarios, the configuration of a GCP must be continuously re-adjusted to attain certain performance goals, or to adapt to current resource availability. This paper addresses this challenge by proposing self-managing mechanisms based on feedback control theory to a GCP especially designed to be self-manageable; in the proposed protocol, message overhead and delivery latency can be adjusted at runtime to follow some new operating set-point. The evaluation performed under varied scenarios shows the effectiveness of our approach. © 2012 Elsevier Inc. All rights reserved.","Autonomic computing; Dependability; Dynamic adaptation; Group communication; Self-management"
"Design, implementation, and evaluation of a low-complexity vector-core for executing scalar/vector instructions","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875842636&doi=10.1016%2fj.jpdc.2013.02.003&partnerID=40&md5=561b22e1e18cc9c45a4be564440d896c","This paper proposes a low-complexity vector-core called LcVc for executing both scalar and vector instructions on the same execution datapath. A unified register file in the decode stage is used for storing both scalar operands and vector elements. The execution stage accepts a new set of operands each cycle and produces a new result. Rather than issuing a vector instruction (1-D operations) as a whole, each vector operation is issued sequentially with the existing scalar issue hardware. In the first implementation of LcVc, all loads and stores of registers take place from the data cache in the memory access stage in a rate of one element per clock cycle. The complete design of our proposed LcVc processor is implemented using VHDL targeting the Xilinx FPGA Spartan 3E, xc3s1600e-4-fg320 device. The total number of slices required for implementing LcVc is 1778, where the number of slice flip-flops is 538 and the number of 4-input LUTs is 3706: 1914 for logic and 1792 for RAMs. Moreover, our performance evaluation results show that the speedup of executing vector addition, vector scaling, SAXPY, and matrix-matrix multiplication on LcVc over the scalar execution are 2.3, 2.5, 1.9, and 3, respectively. The hardware required to support the enhanced vector capability is insignificant (5%), which results in reducing the area per core and increasing the number of cores available in a given chip area. © 2013 Elsevier Inc. All rights reserved.","Data-level parallelism; Pipelining; Unified datapath Unified register file Unified ISA Performance evaluation FPGA/VHDL implementation; Vector processing"
"Distributed anomaly detection for industrial wireless sensor networks based on fuzzy data modelling","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875522445&doi=10.1016%2fj.jpdc.2013.02.004&partnerID=40&md5=9dcddbf85e089bce8aed931c61a6a1f8","Modern infrastructure increasingly depends on large computerized systems for their reliable operation. Supervisory Control and Data Acquisition (SCADA) systems are being deployed to monitor and control large scale distributed infrastructures (e.g. power plants, water distribution systems). A recent trend is to incorporate Wireless Sensor Networks (WSNs) to sense and gather data. However, due to the broadcast nature of the network and inherent limitations in the sensor nodes themselves, they are vulnerable to different types of security attacks. Given the critical aspects of the underlying infrastructure it is an extremely important research challenge to provide effective methods to detect malicious activities on these networks. This paper proposes a robust and scalable mechanism that aims to detect malicious anomalies accurately and efficiently using distributed in-network processing in a hierarchical framework. Unsupervised data partitioning is performed distributively adapting fuzzy c-means clustering in an incremental model. Non-parametric and non-probabilistic anomaly detection is performed through fuzzy membership evaluations and thresholds on observed inter-cluster distances. Robust thresholds are determined adaptively using second order statistical knowledge at each evaluation stage. Extensive experiments were performed and the results demonstrate that the proposed framework achieves high detection accuracy compared to existing data clustering approaches with more than 96% less communication overheads opposed to a centralized approach. © 2013 Elsevier B.V.","Anomaly detection; Data clustering; networks; SCADA systems; Wireless sensor"
"Detecting Sybil attacks in VANETs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875145945&doi=10.1016%2fj.jpdc.2013.02.001&partnerID=40&md5=0df27e91cb8241c1a08f86ce7fe4bf2d","Sybil attacks have been regarded as a serious security threat to Ad hoc Networks and Sensor Networks. They may also impair the potential applications in Vehicular Ad hoc Networks (VANETs) by creating an illusion of traffic congestion. In this paper, we make various attempts to explore the feasibility of detecting Sybil attacks by analyzing signal strength distribution. First, we propose a cooperative method to verify the positions of potential Sybil nodes. We use a Random Sample Consensus (RANSAC)-based algorithm to make this cooperative method more robust against outlier data fabricated by Sybil nodes. However, several inherent drawbacks of this cooperative method prompt us to explore additional approaches. We introduce a statistical method and design a system which is able to verify where a vehicle comes from. The system is termed the Presence Evidence System (PES). With PES, we are able to enhance the detection accuracy using statistical analysis over an observation period. Finally, based on realistic US maps and traffic models, we conducted simulations to evaluate the feasibility and efficiency of our methods. Our scheme proves to be an economical approach to suppressing Sybil attacks without extra support from specific positioning hardware.© 2013 Elsevier Inc. All rights reserved.","Position verification; Sybil attacks; VANET"
"A metaheuristic framework for stochastic combinatorial optimization problems based on GPGPU with a case study on the probabilistic traveling salesman problem with deadlines","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869505110&doi=10.1016%2fj.jpdc.2012.05.004&partnerID=40&md5=0c9e9815dd72d61fadfaf6d6ba6b60a1","In this work we propose a general metaheuristic framework for solving stochastic combinatorial optimization problems based on general-purpose computing on graphics processing units (GPGPU). This framework is applied to the probabilistic traveling salesman problem with deadlines (PTSPD) as a case study. Computational studies reveal significant improvements over state-of-the-art methods for the PTSPD. Additionally, our results reveal the huge potential of the proposed framework and sampling-based methods for stochastic combinatorial optimization problems. © 2012 Elsevier Inc. All rights reserved.","CUDA; GPGPU; Monte Carlo sampling; Stochastic combinatorial optimization; Stochastic vehicle routing"
"Job scheduling with adjusted runtime estimates on production supercomputers","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879082931&doi=10.1016%2fj.jpdc.2013.02.006&partnerID=40&md5=eccd9ff64e5171eac7f0d2e0e0cbb003","The estimate of a parallel job's running time (walltime) is an important attribute used by resource managers and job schedulers in various scenarios, such as backfilling and short-job-first scheduling. This value is provided by the user, however, and has been repeatedly shown to be inaccurate. We studied the workload characteristic based on a large amount of historical data (over 275,000 jobs in two and a half years) from a production leadership-class computer. Based on that study, we proposed a set of walltime adjustment schemes producing more accurate estimates. To ensure the utility of these schemes on production systems, we analyzed their potential impact in scheduling and evaluated the schemes with an event-driven simulator. Our experimental results show that our method can achieve not only better overall estimation accuracy but also improved overall system performance. Specifically, the average estimation accuracy of the tested workload can be improved by up to 35%, and the system performance in terms of average waiting time and weighted average waiting time can be improved by up to 22% and 28%, respectively. © 2013 Elsevier Ltd. All rights reserved.","Job scheduling; Runtime estimates; Walltime prediction"
"Large-scale access scheduling in wireless mesh networks using social centrality","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885176168&doi=10.1016%2fj.jpdc.2013.03.011&partnerID=40&md5=8ae55f2cee6289cfa3a9a338e24e4f94","Wireless mesh networking is an economic and convenient way to provide last mile Internet access through ad hoc peer-to-peer communication links. However, without systematic network configuration and channel resource management, these networks suffer from scalability, performance degradation and service disruption issues due to overwhelming co-channel interference, unscrupulous channel utilization and inherent network mobility. The IEEE 802.11 DCF and EDCA mechanisms based on CSMA/CA are the most widely used random channel access mechanisms, but unfortunately these cannot effectively eliminate hidden terminal and exposed terminal problems in multi-hop scenarios. Social network analysis techniques proposed for economic and social studies have recently been shown to be a successful approach for characterizing information propagation in multi-hop wireless networks. We propose a set of efficient resource allocation algorithms and channel access scheduling protocols based on Latin squares and social centrality metrics for wireless mesh networks (WMNs) with multi-radio multichannel (MRMC) communication capabilities, called LaSo, which can coexist with IEEE 802.11 DCF and be effectively applied in large scale WMNs. Based on interference information provided by the interference graph, LaSo uses nodal degree centrality to form cliques for intra-cluster communication, and betweenness centrality to choose bridge nodes to form cliques for inter-cluster communication in WMNs, and then applies Latin squares to map the clique-based clustering structure to radios and channels for wireless communication purposes. Afterwards, LaSo again applies Latin squares to schedule the channel access amongst nodes within each cluster in a collision-free manner. We evaluate LaSo using simulations, and results show that LaSo achieves much better performance than existing IEEE 802.11 standards and other multi-channel access control protocols. © 2013 Elsevier Inc.","Latin squares; Multi-radio multi-channel; Multiple access scheduling; Social centrality; Wireless mesh networks"
"Benchmarking of communication techniques for GPUs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870952042&doi=10.1016%2fj.jpdc.2012.09.006&partnerID=40&md5=95593af90e9364562dc601b0ee7575c9","We report about the performances obtained, at the application level, by two MPI implementations for Infiniband that allow direct exchange of data stored in the global memory of Graphic Processing Units (GPU) based on the Nvidia CUDA. For the same purpose, we tested also the Application Programming Interface of APEnet, which is a custom, high performance interconnect technology. As a benchmark we consider the time required to update a single spin of the 3D Heisenberg spin glass model by using the over-relaxation algorithm. The results show that CUDA streams are instrumental in achieving the best possible performances. © 2012 Elsevier Inc. All rights reserved.","3D Torus; Asynchronous communication; GPU; MPI"
"More IMPATIENT: A gridding-accelerated Toeplitz-based strategy for non-Cartesian high-resolution 3D MRI on GPUs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893661530&doi=10.1016%2fj.jpdc.2013.01.001&partnerID=40&md5=af30393f8a2504ae843abde371205c58","Several recent methods have been proposed to obtain significant speed-ups in MRI image reconstruction by leveraging the computational power of GPUs. Previously, we implemented a GPU-based image reconstruction technique called the Illinois Massively Parallel Acquisition Toolkit for Image reconstruction with ENhanced Throughput in MRI (IMPATIENT MRI) for reconstructing data collected along arbitrary 3D trajectories. In this paper, we improve IMPATIENT by removing computational bottlenecks by using a gridding approach to accelerate the computation of various data structures needed by the previous routine. Further, we enhance the routine with capabilities for off-resonance correction and multi-sensor parallel imaging reconstruction. Through implementation of optimized gridding into our iterative reconstruction scheme, speed-ups of more than a factor of 200 are provided in the improved GPU implementation compared to the previous accelerated GPU code. © 2013 Elsevier Inc. All rights reserved.","CUDA; GPU; Gridding; MRI; Non-Cartesian; Toeplitz"
"A shared matrix unit for a chip multi-core processor","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885185573&doi=10.1016%2fj.jpdc.2013.03.004&partnerID=40&md5=a8fdb83b3bac6eba90af9fef4ed604a7","This paper proposes extending a multi-core processor with a common matrix unit to maximize onchip resource utilization and to leverage the advantages of the current multi-core revolution to improve the performance of data-parallel applications. Each core fetches scalar/vector/matrix instructions from its instruction cache. Scalar instructions continue the execution on the scalar datapath; however, vector/matrix instructions are issued by the decode stage to the shared matrix unit through the corresponding FIFO queue. Moreover, scalar results from reduction vector/matrix instructions are sent back from the matrix unit to the scalar core that sent these instructions. Some dense linear algebra kernels (scalar-vector multiplication, scalar times vector plus another, apply Givens rotation, rank-1 update, vector-matrix multiplication, and matrix-matrix multiplication) as well as discrete cosine transform, sum of absolute differences, and affine transformation are used in the performance evaluation. Our results show that the improvement in the utilization of the shared matrix unit with a dual-core ranges from 9% to 26% compared to extending a matrix unit to a single-core. Moreover, the average speedup of the dualcore shared matrix unit over a single-core extended with a matrix unit ranges from 6% to 24% and the maximum speedup ranges from 13% to 46%. © 2013 Elsevier Inc.","Multi-core processors; Parallel processing; SystemC implementation; Vector/matrix processing"
"Scalability study of molecular dynamics simulation on Godson-T many-core architecture","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884815836&doi=10.1016%2fj.jpdc.2012.07.007&partnerID=40&md5=93066be25308ae711866d08b4f341145","Molecular dynamics (MD) simulation has broad applications, and an increasing amount of computing power is needed to satisfy the large scale of the real world simulation. The advent of the many-core paradigm brings unprecedented computing power, but it remains a great challenge to harvest the computing power due to MD's irregular memory-access pattern. To address this challenge, this paper presents a joint application/architecture study to enhance the scalability of MD on Godson-T-like many-core architecture. First, a preprocessing approach leveraging an adaptive divide-and-conquer framework is designed to exploit locality through memory hierarchy with software controlled memory. Then three incremental optimization strategies-a novel data-layout to improve data locality, an on-chip locality-aware parallel algorithm to enhance data reuse, and a pipelining algorithm to hide latency to shared memory-are proposed to enhance on-chip parallelism for Godson-T many-core processor. Experiments on Godson-T simulator exhibit strong-scaling parallel efficiency of 0.99 on 64 cores, which is confirmed by a field-programmable gate array emulator. Also the performance per watt of MD on Godson-T is much higher than MD on a 16-cores Intel core i7 symmetric multiprocessor (SMP) and 26 times higher than MD on an 8-core 64-thread Sun T2 processor. Detailed analysis shows that optimizations utilizing architectural features to maximize data locality and to enhance data reuse benefit scalability most. Furthermore, a hierarchical parallelization scheme is designed to map the MD algorithm to Godson-T many-core cluster and a simple performance model is derived, which suggests that the optimization scheme is likely to scale well toward exascale. Certain architectural features are found essential for these optimizations, which could guide future hardware developments. © 2013 Elsevier Ltd. All rights reserved.","Many-core architecture; Molecular dynamics; Scalability"
"Gossip-based Cooperative Caching for mobile applications in mobile wireless networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893667919&doi=10.1016%2fj.jpdc.2013.01.006&partnerID=40&md5=e2191af24eb675f9504c33f5a2928e22","Cooperative caching is an efficient way to improve the performance of data access in mobile wireless networks, by cache nodes selecting different data items in their limited storage in order to reduce total access delay. With more demands on sharing a video or other data, especially for mobile applications in an Internet-based Mobile Ad Hoc Network, considering the relations among data items in cooperative caching becomes more important than before. However, most of the existing works do not consider these inherent relations among data items, such as the logical, temporal, or spatial relations. In this paper, we present a novel solution, Gossip-based Cooperative Caching (GosCC) to address the cache placement problem, and consider the sequential relation among data items. Each mobile node stores the IDs of data items cached locally and the ID of the data item in use into its progress report. Each mobile node also makes use of these progress reports to determine whether a data item should be cached locally. These progress reports are propagated within the network in a gossip-based way. To improve the user experience, GosCC aims to provide users with an uninterrupted data access service. Simulation results show that GosCC achieves better performance than Benefit-based Data Caching and HybridCache, in terms of average interruption intervals and average interruption times, while sacrificing message cost to a certain degree. © 2013 Elsevier Inc. All rights reserved.","Cache placement; Cooperative caching; Data relation; Gossip-based protocol"
"State-based predictions with self-correction on Enterprise Desktop Grid environments","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875425786&doi=10.1016%2fj.jpdc.2013.02.007&partnerID=40&md5=38fb21796b4a386cd0fcbc50935bf850","The abundant computing resources in current organizations provide new opportunities for executing parallel scientific applications and using resources. The Enterprise Desktop Grid Computing (EDGC) paradigm addresses the potential for harvesting the idle computing resources of an organization's desktop PCs to support the execution of the company's large-scale applications. In these environments, the accuracy of response-time predictions is essential for effective metascheduling that maximizes resource usage without harming the performance of the parallel and local applications. However, this accuracy is a major challenge due to the heterogeneity and non-dedicated nature of EDGC resources. In this paper, two new prediction techniques are presented based on the state of resources. A thorough analysis by linear regression demonstrated that the proposed techniques capture the real behavior of the parallel applications better than other common techniques in the literature. Moreover, it is possible to reduce deviations with a proper modeling of prediction errors, and thus, a Self-adjustable Correction method (SAC) for detecting and correcting the prediction deviations was proposed with the ability to adapt to the changes in load conditions. An extensive evaluation in a real environment was conducted to validate the SAC method. The results show that the use of SAC increases the accuracy of response-time predictions by 35%. The cost of predictions with self-correction and its accuracy in a real environment was analyzed using a combination of the proposed techniques. The results demonstrate that the cost of predictions is negligible and the combined use of the prediction techniques is preferable. © 2013 Elsevier Inc. All rights reserved.","System-generated predictions Instance-based learning Application modeling Dynamic prediction correction Enterprise Desktop Grids"
"RISE: A general simulation interoperability middleware container","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874600194&doi=10.1016%2fj.jpdc.2013.01.014&partnerID=40&md5=d4ca475349a0e9c7f788e9f1cb6fa2ed","In recent years, new services on the Internet have enabled global cooperation; in particular, the Web has enabled new distributed simulation technology. Much research has been devoted to develop middleware interoperability methods on the Web. However, most existing methods have constraints in the structural rules that are placed on the design of middleware interoperability methods. For example, such constraints make it difficult to enhance interoperability via decoupling systems implementations and design, which is essential in open computing networks, as in the case of the Web. In order to achieve such objectives we present the RISE (RESTful Interoperability Simulation Environment) middleware. This all-purpose WS-based distributed simulation middleware decouples design and implementation while allowing composition scalability and dynamicity. Furthermore, it supports experiment-oriented frameworks and has the ability to put Web 2.0 services in the simulation loop. RISE is the first existing middleware to achieve such objectives, and the first to employ RESTful Web-services. We present the foundations for meeting the above objectives, and the distinct characteristics of RISE from existing Web-based approaches. © 2013 Elsevier Inc. All rights reserved.","Experimental framework; Interoperability; Middleware; REST; Semantic Web; Simulation; Web services"
"Kernel-assisted and topology-aware MPI collective communications on multicore/many-core platforms","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879116111&doi=10.1016%2fj.jpdc.2013.01.015&partnerID=40&md5=21f08cc59eed66ddad1f70b5fb23f36a","Multicore Clusters, which have become the most prominent form of High Performance Computing (HPC) systems, challenge the performance of MPI applications with non-uniform memory accesses and shared cache hierarchies. Recent advances in MPI collective communications have alleviated the performance issue exposed by deep memory hierarchies by carefully considering the mapping between the collective topology and the hardware topologies, as well as the use of single-copy kernel assisted mechanisms. However, on distributed environments, a single level approach cannot encompass the extreme variations not only in bandwidth and latency capabilities, but also in the capability to support duplex communications or operate multiple concurrent copies. This calls for a collaborative approach between multiple layers of collective algorithms, dedicated to extracting the maximum degree of parallelism from the collective algorithm by consolidating the intra- and inter-node communications. In this work, we present HierKNEM, a kernel-assisted topology-aware collective framework, and the mechanisms deployed by this framework to orchestrate the collaboration between multiple layers of collective algorithms. The resulting scheme maximizes the overlap of intra- and inter-node communications. We demonstrate experimentally, by considering three of the most used collective operations (Broadcast, Allgather and Reduction), that (1) this approach is immune to modifications of the underlying process-core binding; (2) it outperforms state-of-art MPI libraries (Open MPI, MPICH2 and MVAPICH2) demonstrating up to a 30x speedup for synthetic benchmarks, and up to a 3x acceleration for a parallel graph application (ASP); (3) it furthermore demonstrates a linear speedup with the increase of the number of cores per compute node, a paramount requirement for scalability on future many-core hardware. © 2013 Elsevier Ltd. All rights reserved.","Cluster; Collective communication; Hierarchical; HPC; MPI; Multicore"
"Efficient autonomic cloud computing using online discrete event simulation","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875452140&doi=10.1016%2fj.jpdc.2013.02.008&partnerID=40&md5=c4c25e39d771ee7961b2fddbe3846654","Interest is growing in open source tools that let organizations build IaaS clouds using their own internal infrastructures, alone or in conjunction with external ones. A key component in such private/hybrid clouds is virtual infrastructure management, i.e., the dynamic orchestration of virtual machines, based on the understanding and prediction of performance at scale, with uncertain workloads and frequent node failures. Part of the research community is trying to solve this and other IaaS problems looking to Autonomic Computing techniques, that can provide, for example, better management of energy consumption, quality of service (QoS), and unpredictable system behaviors. In this context, we first recall the main features of the NAM framework devoted to the design of distributed autonomic systems. Then we illustrate the organization and policies of a NAM-based Workload Manager, focusing on one of its components, the Capacity Planner. We show that, when it is not possible to obtain optimal energy-aware plans analytically, sub-optimal plans can be autonomically obtained using online discrete event simulation. Specifically, the proposed approach allows to cope with a broader range of working conditions and types of workloads. © 2013 Elsevier Inc. All rights reserved.","Cloud management Autonomic computing Discrete event simulation"
"Solving very large instances of the scheduling of independent tasks problem on the GPU","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869507403&doi=10.1016%2fj.jpdc.2012.02.018&partnerID=40&md5=e415fe8aa792b772733d4b0ea7fd55f6","In this paper, we present two new parallel algorithms to solve large instances of the scheduling of independent tasks problem. First, we describe a parallel version of the Min-min heuristic. Second, we present GraphCell, an advanced parallel cellular genetic algorithm (CGA) for the GPU. Two new generic recombination operators that take advantage of the massive parallelism of the GPU are proposed for GraphCell. A speedup study shows the high performance of the parallel Min-min algorithm in the GPU versus several CPU versions of the algorithm (both sequential and parallel using multiple threads). GraphCell improves state-of-the-art solutions, especially for larger problems, and it provides an alternative to our GPU Min-min heuristic when more accurate solutions are needed, at the expense of an increased runtime. © 2012 Elsevier Inc. All rights reserved.","Cellular genetic algorithm; GPU; Scheduling"
"Designing OP2 for GPU architectures","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884819485&doi=10.1016%2fj.jpdc.2012.07.008&partnerID=40&md5=65e899442a6306d62e1dfd7e6bb5f523","OP2 is an ""active"" library framework for the solution of unstructured mesh applications. It aims to decouple the specification of a scientific application from its parallel implementation to achieve code longevity and near-optimal performance through re-targeting the back-end to different multi-core/many-core hardware. This paper presents the design of the current OP2 library for generating efficient code targeting contemporary GPU platforms. In this we focus on some of the software architecture design choices and low-level optimizations to maximize performance on NVIDIA's Fermi architecture GPUs. The performance impact of these design choices is quantified on two NVIDIA GPUs (GTX560Ti, Tesla C2070) using the end-to-end performance of an industrial representative CFD application developed using the OP2 API. Results show that for each system, a number of key configuration parameters need to be set carefully in order to gain good performance. Utilizing a recently developed auto-tuning framework, we explore the effect of these parameters, their limitations and insights into optimizations for improved performance. © 2013 Elsevier Ltd. All rights reserved.","Auto-tuning; CUDA; GPU; Performance; Unstructured mesh applications"
"Parallel Bayesian inference of range and reflectance from LaDAR profiles","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872799632&doi=10.1016%2fj.jpdc.2012.12.003&partnerID=40&md5=299e5a0f6c3170097d5bf212cc5eedf5","Bayesian analysis using reversible jump Markov chain Monte Carlo (RJMCMC) algorithms improves the measurement accuracy, resolution and sensitivity of full waveform laser detection and ranging (LaDAR), but at a significant computational cost. Parallel processing has the potential to significantly reduce the processing time, but although there have been several strategies for Markov chain Monte Carlo (MCMC) parallelization, adaptation of these strategies to RJMCMC may degrade parallel performance. In this paper, we describe an approach to parallel RJMCMC processing that combines data and sampling parallelism in a single framework. This approach, Data Parallel State Space Decomposed RJMCMC (DP SSD-RJMCMC), can be adapted to different parallel cluster size, improve sampling efficiency and maintain parameter estimation accuracy. Formally, it forms a group of parallel chains by decomposing the state space into subsets of parameter space. Each subset has different but restricted dimensionality, and is assigned with an independent chain of variable length. To further improve load balancing, we also employ data decomposition, forming a task queue and conducting dynamic task allocation. The MPI-based implementation on a 32-node Beowulf cluster leads to significant speedup, typically of the order of 15-25 times, while maintaining the estimation accuracy. copy; 2012 Elsevier Inc. All rights reserved.","Data parallelism; Distributed parallel computation; Dynamic task allocation; Full waveform; Laser radar; Reversible jump Markov chain Monte Carlo; Varying-dimensional signal analysis"
"Enhanced dynamic hierarchical replication and weighted scheduling strategy in data grid","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893687190&doi=10.1016%2fj.jpdc.2013.01.002&partnerID=40&md5=d7b5f15c17274c645202e1f487b50fb3","The Data Grid provides massive aggregated computing resources and distributed storage space to deal with data-intensive applications. Due to the limitation of available resources in the grid as well as production of large volumes of data, efficient use of the Grid resources becomes an important challenge. Data replication is a key optimization technique for reducing access latency and managing large data by storing data in a wise manner. Effective scheduling in the Grid can reduce the amount of data transferred among nodes by submitting a job to a node where most of the requested data files are available. In this paper two strategies are proposed, first a novel job scheduling strategy called Weighted Scheduling Strategy (WSS) that uses hierarchical scheduling to reduce the search time for an appropriate computing node. It considers the number of jobs waiting in a queue, the location of the required data for the job and the computing capacity of the sites Second, a dynamic data replication strategy, called Enhanced Dynamic Hierarchical Replication (EDHR) that improves file access time. This strategy is an enhanced version of the Dynamic Hierarchical Replication strategy. It uses an economic model for file deletion when there is not enough space for the replica. The economic model is based on the future value of a data file. Best replica placement plays an important role for obtaining maximum benefit from replication as well as reducing storage cost and mean job execution time. So, it is considered in this paper. The proposed strategies are implemented by OptorSim, the European Data Grid simulator. Experiment results show that the proposed strategies achieve better performance by minimizing the data access time and avoiding unnecessary replication. © 2013 Elsevier Inc. All rights reserved.","Data Grid; Data replication; Job scheduling; Simulation"
"On reducing energy management delays in disks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875692399&doi=10.1016%2fj.jpdc.2013.02.011&partnerID=40&md5=4ef7b0f24f3bcd8c311ff667c39facca","Enterprise computing systems consume a large amount of energy, the cost of which contributes significantly to the operating budget. Consequently, dynamic energy management techniques are prevalent. Unfortunately, dynamic energy management for disks impose delays associated with powering up the disks from a low-power state. Systems designers face a critical trade-off: saving energy reduces operating costs but may increase delays; conversely, reduced access latency makes the systems more responsive but may preclude energy management. In this paper, we propose a System-wide Alternative Retrieval of Data (SARD) scheme. SARD exploits the similarity in software deployment and configuration in enterprise computers to retrieve binaries transparently from other nodes, thus avoiding access delays when the local disk is in a low-power state. SARD uses a software-based approach to reduce spin-up delays while eliminating custom buffering, shared memory infrastructure, or the need for major changes in the operating system. SARD achieves over 71% reduction in delays on trace-driven simulations and in an actual implementation. This will encourage users to utilize energy management techniques more frequently. SARD also achieves an additional 5.1% average reduction in energy consumption for typical desktop applications compared to the widely-used timeout-based disk energy management. © 2013 Elsevier Inc. All rights reserved.","Spin-up delay reduction Disk energy management Peer memory sharing"
"Efficient distributed snapshots in an anonymous asynchronous message-passing system","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893672822&doi=10.1016%2fj.jpdc.2013.01.003&partnerID=40&md5=631992807e8deaa9c88cadfe892ecf1c","We present a global snapshot algorithm with concurrent initiators, with termination detection in an anonymous asynchronous distributed message-passing system having FIFO channels. In anonymous systems, process identifiers are not available and an algorithm cannot use process identifiers in its operation. Such systems arise in several domains due to a variety of reasons. In the proposed snapshot algorithm for anonymous systems, each instance of algorithm initiation is identified by a random number(nonce); however, this is not used as an address in any form of communication. In the algorithm, each process can determine an instant when the local snapshot recordings at all the processes have terminated. This is a challenging problem when an algorithm cannot use process identifiers and a process does not know the number of processes in the system or the diameter of the network and cannot use a predefined topology overlay on the network, because there is no easy way to identify the global termination condition. The message complexity of our algorithm is (cn2), where c is the number of concurrent initiators and n is the number of processes in the system, which is much better than that of the algorithm by Chalopin et al. (2012) [6]. Further, the algorithm by Chalopin et al. also requires knowledge of the network diameter. © 2013 Elsevier Inc. All rights reserved.","Anonymous systems; Causality; Distributed computing; Global snapshot; Global state; Termination detection"
"Inexact subgraph isomorphism in MapReduce","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870871123&doi=10.1016%2fj.jpdc.2012.10.005&partnerID=40&md5=19342bf2533268a56a2f92d71de838a9","Inexact subgraph matching based on type-isomorphism was introduced by Berry et al. [J. Berry, B. Hendrickson, S. Kahan, P. Konecny, Software and algorithms for graph queries on multithreaded architectures, in: Proc. IEEE International Parallel and Distributed Computing Symposium, IEEE, 2007, pp. 1-14] as a generalization of the exact subgraph matching problem. Enumerating small subgraph patterns in very large graphs is a core problem in the analysis of social networks, bioinformatics data sets, and other applications. This paper describes a MapReduce algorithm for subgraph type-isomorphism matching. The MapReduce computing framework is designed for distributed computing on massive data sets, and the new algorithm leverages MapReduce techniques to enable processing of graphs with billions of vertices. The paper also introduces a new class of walk-level constraints for narrowing the set of matches. Constraints meeting criteria defined in the paper are useful for specifying more precise patterns and for improving algorithm performance. Results are provided on a variety of graphs, with size ranging up to billions of vertices and edges, including graphs that follow a power law degree distribution. © 2012 Elsevier Inc. All rights reserved.","Graph mining; MapReduce; Pattern match; Subgraph isomorphism"
"Simple, space-efficient, and fairness improved FCFS mutual exclusion algorithms","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885174528&doi=10.1016%2fj.jpdc.2013.03.009&partnerID=40&md5=c311b7e011e556f53b57d4c7a8feca8b","Let n be the number of threads that can compete for a shared resource R. The mutual exclusion problem involves coordinating these n concurrent threads in accessing R in a mutually exclusive way. This paper addresses two basic questions related to the First-Come-First-Served (FCFS) mutual exclusion algorithms that use only read-write operations: one is regarding the lower bound on the shared space requirement and the other is about fairness. The current best FCFS algorithm using read-write operations requires 5n shared bits. Could the shared space requirement be further reduced? The existing FCFS mutual exclusion algorithms assure fairness only among the threads which cross the 'doorway' sequentially. In systems with multicore processors, which are becoming increasingly common nowadays, threads can progress truly in parallel. Therefore, it is quite likely that several threads can cross the doorway concurrently. In such systems, the threads which cross the doorway sequentially may constitute only a fraction of all competing threads. While this fraction of threads follow the FCFS order, the rest of the threads have to rely on a biased scheme which always favors threads with smaller identifiers. Is there a simpler way to remove this bias to assure global fairness? This paper answers the above two questions affirmatively by presenting simple FCFS mutual exclusion algorithms using only read-write operations-the first one using 3n shared bits and the latter algorithms using 4n shared bits. The resulting algorithms are simple, space-efficient, and highly fair. © 2013 Elsevier Inc.","Concurrent programming; FCFS fairness; Multicore processors; Mutual exclusion; Nonatomic algorithms; Process/thread synchronization; Space optimal"
"Fractal self-similarity measurements based clustering technique for SOAP Web messages","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893644687&doi=10.1016%2fj.jpdc.2013.01.005&partnerID=40&md5=9aed55bae3eebb9292fe7c6a0e336138","The significant increase in the usage of Web services has resulted in bottlenecks and congestion on bandwidth-constrained network links. Aggregating SOAP messages can be an effective solution that could potentially reduce the large amount of generated traffic. Although pairwise SOAP aggregation, that is grouping only two similar messages, has demonstrated significant performance improvement, additional improvements can be done by including similarity mechanisms. Such mechanisms cluster several SOAP messages that have high degree of similarity. This paper proposes a fractal self-similarity model that provides a novel way of computing the similarity of SOAP messages. Fractal is proposed as an unsupervised clustering technique that dynamically groups SOAP messages. Various experimentations have shown good performance results for the proposed fractal self-similarity model in comparison with some well-known clustering models by only consuming 31% of the clustering time required by the K-Means and 23% when using principle component analysis (PCA) combined with K-Means. Furthermore, the proposed technique has shown ""better"" quality clustering, as the aggregated SOAP messages have much smaller size than their counterparts. © 2013 Elsevier Inc. All rights reserved.","Clustering; Fractal; SOAP; Web services"
"Accelerating wildfire susceptibility mapping through GPGPU","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885172862&doi=10.1016%2fj.jpdc.2013.03.014&partnerID=40&md5=e044152718cf6ad46fd4f80353e1e881","In the field of wildfire risk management the so-called burn probability maps (BPMs) are increasingly used with the aim of estimating the probability of each point of a landscape to be burned under certain environmental conditions. Such BPMs are usually computed through the explicit simulation of thousands of fires using fast and accurate models. However, even adopting the most optimized algorithms, the building of simulation-based BPMs for large areas results in a highly intensive computational process that makes mandatory the use of high performance computing. In this paper, General-Purpose Computation with Graphics Processing Units (GPGPU) is applied, in conjunction with a wildfire simulation model based on the Cellular Automata approach, to the process of BPM building. Using three different GPGPU devices, the paper illustrates several implementation strategies to speedup the overall mapping process and discusses some numerical results obtained on a real landscape. © 2013 Elsevier Inc. All rights reserved.","Cellular automata; GPGPU; Hazard maps; Wildfire simulation; Wildfire susceptibility"
"Revisiting parallel cyclic reduction and parallel prefix-based algorithms for block tridiagonal systems of equations","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870923772&doi=10.1016%2fj.jpdc.2012.10.003&partnerID=40&md5=bae0b0443f4ed5878ce7714a39336cfc","Direct solvers based on prefix computation and cyclic reduction algorithms exploit the special structure of tridiagonal systems of equations to deliver better parallel performance compared to those designed for more general systems of equations. This performance advantage is even more pronounced for block tridiagonal systems. In this paper, we re-examine the performances of these two algorithms taking the effects of block size into account. Depending on the block size, the parameter space spanned by the number of block rows, size of the blocks and the processor count is shown to favor one or the other of the two algorithms. A critical block size that separates these two regions is shown to emerge and its dependence both on problem dependent parameters and on machine-specific constants is established. Empirical verification of these analytical findings is carried out on up to 2048 cores of a Cray XT4 system. © 2012 Elsevier Inc. All rights reserved.","Block tridiagonal matrix; Cyclic reduction; Parallel solver; Prefix computation"
"Improving performance of codes with large/irregular stride memory access patterns via high performance reconfigurable computers","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884812910&doi=10.1016%2fj.jpdc.2012.07.011&partnerID=40&md5=9d54f60c23ee07e83d517a6858566d71","Codes that have large-stride/irregular-stride (L/I) memory access patterns, e.g., sparse matrix and linked list codes, often perform poorly on mainstream clusters because of the general purpose processor (GPP) memory hierarchy. High performance reconfigurable computers (HPRC) contain both GPPs and field programmable gate arrays (FPGAs) connected via a high-speed network. In this research, simple 64-bit floating-point codes are used to illustrate the runtime performance impact of L/I memory accesses in both software-only and FPGA-augmented codes and to assess the benefits of mapping L/I-type codes onto HPRCs. The experiments documented herein reveal that large-stride software-only codes experience severe performance degradation. In contrast, large-stride FPGA-augmented codes experience minimal performance degradation. For experiments with large data sizes, the unit-stride FPGA-augmented code ran about two times slower than software. On the other hand, the large-stride FPGA-augmented code ran faster than software for all the larger data sizes. The largest showed a 17-fold runtime speedup. © 2013 Elsevier Ltd. All rights reserved.","FPGA; Heterogeneous computer; High performance computing; Irregular memory access pattern; Reconfigurable computer"
"Lightweight, efficient, robust epidemic dissemination","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879071459&doi=10.1016%2fj.jpdc.2013.01.018&partnerID=40&md5=d663b06cc389c862f36a308359e8a14c","Today's intensive demand for data such as live broadcast or news feeds requires efficient and robust dissemination systems. Traditionally, designs focus on extremes of the efficiency/robustness spectrum by either using structures, such as trees for efficiency or by using loosely-coupled epidemic protocols for robustness. We present Brisa, a hybrid approach combining the robustness of epidemics with the efficiency of structured approaches. Brisa implicitly emerges embedded dissemination structures from an underlying epidemic substrate. The structures' links are chosen with local knowledge only, but still ensuring connectivity. Failures can be promptly compensated and repaired thanks to the epidemic substrate, and their impact on dissemination delays masked by the use of multiple independent structures. Besides presenting the protocol design, we conduct an extensive evaluation in real environments, analyzing the effectiveness of the structure creation mechanism and its robustness under dynamic conditions. Results confirm Brisa as an efficient and robust approach to data dissemination in large dynamic environments. © 2013 Elsevier Ltd. All rights reserved.","Data dissemination; Distributed systems; Epidemic protocols; Gossip-based protocols; Peer-to-peer"
"Accurately modeling superscalar processor performance with reduced trace","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893685518&doi=10.1016%2fj.jpdc.2012.12.002&partnerID=40&md5=60e4e757858cc254eec7d048fba35a3f","Trace-driven simulation of out-of-order superscalar processors is far from straightforward. The dynamic nature of out-of-order superscalar processors combined with the static nature of traces can lead to large inaccuracies in the results when the traces contain only a subset of executed instructions for trace reduction. In this paper, we describe and comprehensively evaluate the pairwise dependent cache miss model (PDCM), a framework for fast and accurate trace-driven simulation of out-of-order superscalar processors. The model determines how to treat a cache miss with respect to other cache misses recorded in the trace by dynamically reconstructing the reorder buffer state during simulation and honoring the dependencies between the trace items. Our experimental results demonstrate that a PDCM-based simulator produces highly accurate simulation results (less than 3% error) with fast simulation speeds (62.5× on average) compared with an execution-driven simulator. Moreover, we observed that the proposed simulation method is capable of preserving a processor's dynamic off-core memory access behavior and accurately predicting the relative performance change when a processor's low-level memory hierarchy parameters are changed. © 2012 Elsevier Inc. All rights reserved.","Out-of-order superscalar processor; Simulation methodology; Trace-driven simulation"
"Fine-grained multi-phase array designs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885181265&doi=10.1016%2fj.jpdc.2013.03.003&partnerID=40&md5=c853d32a12f04e1ad88b6057cbb3c07e","Hybrid multiprocessor architectures which combine re-configurable computing and multiprocessors on a chip are being proposed to transcend the performance of standard multi-core parallel systems. Both finegrained and coarse-grained parallel algorithm implementations are feasible in such hybrid frameworks. A compositional strategy for designing fine-grained multi-phase regular processor arrays to target hybrid architectures is presented in this paper. The method is based on deriving component designs using classical regular array techniques and composing the components into a unified global design. Effective designs with phase-changes and data routing at run-time are characteristics of these designs. In order to describe the data transfer between phases, the concept of communication domain is introduced so that the producer-consumer relationship arising from multi-phase computation can be treated in a unified way as a data routing phase. This technique is applied to derive new designs of multi-phase regular arrays with different dataflow between phases of computation. © 2013 Elsevier Inc.","Multi-phase design; Polyhedral model; Systolic array"
"An effective iterated greedy algorithm for reliability-oriented task allocation in distributed computing systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885180572&doi=10.1016%2fj.jpdc.2013.03.008&partnerID=40&md5=56def26328953eab268698f275c17116","This paper investigates the problem of allocating parallel application tasks to processors in heterogeneous distributed computing systems with the goal of maximizing the system reliability. The problem of finding an optimal task allocation for more than three processors is known to be NP-hard in the strong sense. To deal with this challenging problem, we propose a simple and effective iterative greedy algorithm to find the best possible solution within a reasonable amount of computation time. The algorithm first uses a constructive heuristic to obtain an initial assignment and iteratively improves it in a greedy way. We study the performance of the proposed algorithm over a wide range of parameters including problem size, the ratio of average communication time to average computation time, and task interaction density. The viability and effectiveness of our algorithm is demonstrated by comparing it with recently proposed task allocation algorithms for maximizing system reliability available in the literature. © 2013 Elsevier Inc.","Distributed computing; Iterated greedy algorithm; Meta-heuristics; System reliability; Task allocation"
"Hint-based cache design for reducing miss penalty in HBS packet classification algorithm","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885178441&doi=10.1016%2fj.jpdc.2013.03.005&partnerID=40&md5=a35c5b446abe3e1662a4d711bfe7c01f","In this paper, we implement some notable hierarchical or decision-tree-based packet classification algorithms such as extended grid of tries (EGT), hierarchical intelligent cuttings (HiCuts), HyperCuts, and hierarchical binary search (HBS) on an IXP2400 network processor. By using all six of the available processing microengines (MEs), we find that none of these existing packet classification algorithms achieve the line speed of OC-48 provided by IXP2400. To improve the search speed of these packet classification algorithms, we propose the use of software cache designs to take advantage of the temporal locality of the packets because IXP network processors have no built-in caches for fast path processing in MEs. Furthermore, we propose hint-based cache designs to reduce the search duration of the packet classification data structure when cache misses occur. Both the header and prefix caches are studied. Although the proposed cache schemes are designed for all the dimension-by-dimension packet classification schemes, they are, nonetheless, the most suitable for HBS. Our performance simulations show that the HBS enhanced with the proposed cache schemes performs the best in terms of classification speed and number of memory accesses when the memory requirement is in the same range as those of HiCuts and HyperCuts. Based on the experiments with all the high and low locality packet traces, five MEs are sufficient for the proposed rule cache with hints to achieve the line speed of OC-48 provided by IXP2400. © 2013 Elsevier Inc.","Cache; Network processor; Packet classification"
"A concurrent red-black tree","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872911764&doi=10.1016%2fj.jpdc.2012.12.010&partnerID=40&md5=42c9a550041c120907ff977ed2f80fc4","With the proliferation of multiprocessor computers, data structures capable of supporting several processes are a growing need. Concurrent data structures seek to provide similar performance to sequential data structures while being accessible concurrently by several processes and providing synchronization mechanisms transparently to those processes. Red-black trees are an important data structure used in many systems. Unfortunately, it is difficult to implement an efficient concurrent red-black tree for shared memory processes; so most research efforts have been directed towards other dictionary data structures, mainly skip-lists and AVL trees. In this paper we present a new type of concurrent red-black tree that uses optimistic concurrency techniques and new balancing operations to scale well and support contention. Our tree performs favorably compared to other similar dictionaries; in particular, in high contention scenarios it performs up to 14% better than the best-known concurrent dictionary solutions. © 2013 Elsevier Inc. All rights reserved.","Algorithm design; Concurrent data structure; Red-black tree"
"Enhancing throughput for streaming applications running on cluster systems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885190013&doi=10.1016%2fj.jpdc.2013.04.006&partnerID=40&md5=85a7702793384a340e76f3ca76745e26","The exploitation of throughput in a parallel application that processes an input data stream is a difficult challenge. For typical coarse-grain applications, where the computation time of tasks is greater than their communication time, the maximum achievable throughput is determined by the maximum task computation time. Thus, the improvement in throughput above this maximum would eventually require the modification of the source code of the tasks. In this work, we address the improvement of throughput by proposing two task replication methodologies that have the target throughput to be achieved as an input parameter. They proceed by generating a new task graph structure that permits the target throughput to be achieved. The first replication mechanism, named DPRM (Data Parallel Replication Mechanism), exploits the inner task data parallelism. The second mechanism, named TCRM (Task Copy Replication Mechanism), creates new execution paths inside the application task graph structure that allows more than one instance of data to be processed concurrently. We evaluate the effectiveness of these mechanisms with three real applications executed in a cluster system: the MPEG2 video compressor, the IVUS (Intra-Vascular Ultra-Sound) medical image application and the BASIZ (Bright and SAtured Images Zone) video processing application. In all these cases, the obtained throughput was greater after applying the proposed replication mechanism than what the application could provide with the original implementation. © 2013 Elsevier Inc. All rights reserved.","Data parallelism; Pipeline execution; Streaming applications; Task parallelism; Task replication mechanisms"
"Dynamic Fault-Tolerant three-dimensional cellular genetic algorithms","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870939369&doi=10.1016%2fj.jpdc.2012.09.011&partnerID=40&md5=65cf649575a7c9767a62d864f061046a","This paper proposes a new dynamic and algorithm-based approach to achieve fault tolerance using 3D cellular genetic algorithms (Dynamic Fault-Tolerant 3D-cGA). The proposed algorithm is an improved version of our previous algorithm (Fault-Tolerant 3D-cGA) that introduces and utilizes a dynamic adaptation feature to achieve further improvement. In Dynamic Fault-Tolerant 3D-cGA, faulty individuals are isolated and the maximum number of fitness evaluations is recalculated to adapt to faults encountered. To improve the performance of the algorithm, a mitigation technique is integrated into our algorithm by introducing an explicit migration operator. A benchmark of well-known real-world and test problems is used to test the effectiveness of the algorithm in order to investigate the influence of adaptation schemes and migration on algorithm performance. Faulty critical system data is tackled in conjunction with various fault ratios. To illustrate the improvement achieved, Dynamic Fault-Tolerant 3D-cGA is compared with Fault-Tolerant 3D-cGA, the previously proposed algorithm. The overall results demonstrate the ability of Dynamic Fault-Tolerant 3D-cGA to maintain system's functionality despite an increasing number of faults with up to 40% of processing elements (PEs), and clearly illustrate the importance of migration. Significant improvements in the performance of the algorithm, measured as efficiency, efficacy, and speed, are achieved, especially when migration is employed. © 2012 Elsevier Inc. All rights reserved.","Cellular genetic algorithms (cGAs); Dynamic adaptation; Evolutionary algorithms (EAs); Fault tolerance; Migration"
"Software transactional memories for Scala","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870877879&doi=10.1016%2fj.jpdc.2012.09.015&partnerID=40&md5=0fb4eb45adb9c86ce8a9f11ebeba95c6","Transactional memory is an alternative to locks for handling concurrency in multi-threaded environments. Instead of providing critical regions that only one thread can enter at a time, transactional memory records sufficient information to detect and correct for conflicts if they occur. This paper surveys the range of options for implementing software transactional memory in Scala. Where possible, we provide references to implementations that instantiate each technique. As part of this survey, we document for the first time several techniques developed in the implementation of Manchester University Transactions for Scala. We order the implementation techniques on a scale moving from the least to the most invasive in terms of modifications to the compilation and runtime environment. This shows that, while the less invasive options are easier to implement and more common, they are more verbose and invasive in the codes using them, often requiring changes to the syntax and program structure throughout the code. © 2012 Elsevier Inc. All rights reserved.","Scala; Software transactional memory; Transactional memory"
"Avoiding disruptive failovers in transaction processing systems with multiple active nodes","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893662129&doi=10.1016%2fj.jpdc.2013.01.007&partnerID=40&md5=5707664d872b138e34cebaec17cff51d","We present a highly available system for environments such as stock trading, where high request rates and low latency requirements dictate that service disruption on the order of seconds in length can be unacceptable. After a node failure, our system avoids delays in processing due to detecting the failure or transferring control to a back-up node. We achieve this by using multiple primary nodes which process transactions concurrently as peers. If a primary node fails, the remaining primaries continue executing without being delayed at all by the failed primary. Nodes agree on a total ordering for processing requests with a novel low overhead wait-free algorithm that utilizes a small amount of shared memory accessible to the nodes and a simple compare-and-swap like protocol which allows the system to progress at the speed of the fastest node. We have implemented our system on an IBM z990 zSeries eServer mainframe and show experimentally that our system performs well and can transparently handle node failures without causing delays to transaction processing. The efficient implementation of our algorithm for ordering transactions is a critically important factor in achieving good performance. © 2013 Elsevier Inc. All rights reserved.","Computer-driven trading; Fault tolerance; High availability; Total ordering algorithm; Transaction processing"
"Research on IPv6 address configuration for a VANET","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2013.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875161145&doi=10.1016%2fj.jpdc.2013.02.002&partnerID=40&md5=d0698ee7ce466306bf42c4e004fae14c","The paper proposes an IPv6 address configuration scheme for a VANET (Vehicular Ad hoc Network). In the paper, the VANET architecture is presented, and based on the architecture the hierarchical IPv6 address structure is created. Based on the hierarchical address structure, the distributed IPv6 address configuration scheme is proposed. In the scheme, each AP (Access Point) has the unique address space and has the right to assign an IPv6 address for an OBU (Onboard Unit). In this way, the address configuration task is distributed around all APs in a VANET, so the distribution of the address configuration is achieved. Moreover, an OBU acquires a unique IPv6 address from a neighbor AP without DAD (Duplicate Address Detection), so the transmission of the control packets used for the address configuration is controlled within one-hop scope. The paper also proposes the address recovery algorithm. Through the algorithm, an AP can retrieve the IPv6 address resources released by OBUs timely and effectively, so it has always sufficient address space for assignment. The paper evaluates the performance of the proposed scheme. The data results show that the proposed scheme reduces the address configuration cost, shortens the address configuration delay and improves the address configuration success rate.","Access point; Address configuration; IPv6 address; Onboard unit; Vehicular ad hoc network"
"Double auction-inspired meta-scheduling of parallel applications on global grids","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893640754&doi=10.1016%2fj.jpdc.2012.09.012&partnerID=40&md5=d43d43d4402da900e841565c1ae414af","Meta-schedulers map jobs to computational resources that are part of a Grid, such as clusters, that in turn have their own local job schedulers. Existing Grid meta-schedulers either target system-centric metrics, such as utilisation and throughput, or prioritise jobs based on utility metrics provided by the users. The system-centric approach gives less importance to users' individual utility, while the user-centric approach may have adverse effects such as poor system performance and unfair treatment of users. Therefore, this paper proposes a novel meta-scheduler, based on the well-known double auction mechanism that aims to satisfy users' service requirements as well as ensuring balanced utilisation of resources across a Grid. We have designed valuation metrics that commodify both the complex resource requirements of users and the capabilities of available computational resources. Through simulation using real traces, we compare our scheduling mechanism with other common mechanisms widely used by both existing market-based and traditional meta-schedulers. The results show that our meta-scheduling mechanism not only satisfies up to 15% more user requirements than others, but also improves system utilisation through load balancing. © 2013 Elsevier B.V. All rights reserved.","Auction; Grid computing; Meta-scheduling; Resource allocation"
"An early-stopping protocol for computing aggregate functions in Sensor Networks","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870903011&doi=10.1016%2fj.jpdc.2012.09.013&partnerID=40&md5=981d745c52a66d310dd3713ced32eff4","In this paper, we study algebraic aggregate computations in Sensor Networks. The main contribution is the presentation of an early-stopping protocol that computes the average function under a harsh model of the conditions under which sensor nodes operate. This protocol is shown to be time-optimal in the presence of infrequent failures. The approach followed saves time and energy by the computation relying on a small network of delegate nodes that can be rebuilt fast in case of node failures and communicate using a collision-free schedule. Delegate nodes run two protocols simultaneously, namely, a collection/dissemination tree-based algorithm, which is shown to be optimal, and a mass-distribution algorithm. Both algorithms are analyzed under a model where the frequency of failures is a parameter. Other aggregate computation algorithms can be easily derived from this protocol. To the best of our knowledge, this is the first optimal early-stopping algorithm for aggregate computations in Sensor Networks. © 2012 Elsevier Inc. All rights reserved.","Aggregate computation; Average computing; Early-stopping algorithm; Failure model; Sensor networks"
"Combinatorial auction-based allocation of virtual machine instances in clouds","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893672683&doi=10.1016%2fj.jpdc.2012.12.006&partnerID=40&md5=713b3ee3a9900a850fbf95098b2163c8","Most of the current cloud computing providers allocate virtual machine instances to their users through fixed-price allocation mechanisms. We argue that combinatorial auction-based allocation mechanisms are especially efficient over the fixed-price mechanisms since the virtual machine instances are assigned to users having the highest valuation. We formulate the problem of virtual machine allocation in clouds as a combinatorial auction problem and propose two mechanisms to solve it. The proposed mechanisms are extensions of two existing combinatorial auction mechanisms. We perform extensive simulation experiments to compare the two proposed combinatorial auction-based mechanisms with the currently used fixed-price allocation mechanism. Our experiments reveal that the combinatorial auction-based mechanisms can significantly improve the allocation efficiency while generating higher revenue for the cloud providers. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Combinatorial auction; Resource allocation; Virtual machine allocation"
"MicroClAn: Microarray clustering analysis","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886768074&doi=10.1016%2fj.jpdc.2012.09.008&partnerID=40&md5=1cc4831b01a318d52c8e2c087b31c032","Evaluating clustering results is a fundamental task in microarray data analysis, due to the lack of enough biological knowledge to know in advance the true partition of genes. Many quality indexes for gene clustering evaluation have been proposed. A critical issue in this domain is to compare and aggregate quality indexes to select the best clustering algorithm and the optimal parameter setting for a dataset. Furthermore, due to the huge amount of data generated by microarray experiments and the requirement of external resources such as ontologies to compute biological indexes, another critical issue is the performance decline in term of execution time. Thus, the distributed computation of algorithms and quality indexes becomes essential. Addressing these issues, this paper presents the MicroClAn framework, a distributed system to evaluate and compare clustering algorithms using the most exploited quality indexes. The best solution is selected through a two-step ranking aggregation of the ranks produced by quality indexes. A new index oriented to the biological validation of microarray clustering results is also introduced. Several scheduling strategies integrated in the framework allow to distribute tasks in the grid environment to optimize the completion time. Experimental results show the effectiveness of our aggregation strategy in identifying the best rank among different clustering algorithms. Moreover, our framework achieves good performance in terms of completion time with few computational resources. © 2012 Elsevier Inc. All rights reserved.","Clustering analysis; Microarray; Quality indexes; Rank aggregation; Scheduling strategies"
"Hierarchical RAID: Design, performance, reliability, and recovery","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867873889&doi=10.1016%2fj.jpdc.2012.07.002&partnerID=40&md5=cea980acbab63d3a5f26f59a1005709c","Hierarchical RAID (HRAID) extends the RAID paradigm to mask the failure of whole Storage Nodes (SNs) or bricks, where each SN is a disk array with a certain RAID level. HRAIDkℓ with N SNs and M disks per SN tolerates k SN failures and ℓ disk failures per SN with Maximum Distance Separable (MDS) erasure codes, which introduce the minimum level of redundancy at each level. For N=M there are k internode and ℓ intranode check strips per SN, occupying the capacity of as many disks with storage redundancy (k+ℓ)N, but a higher storage redundancy is required for M>N. HRAIDkℓ tolerates all disk failures up to dmin=(k+1)(ℓ+1)-1, but up to dmax= Nℓ+Mk-kℓ disk failures can be tolerated. Three options for HRAID operation are: (I) Only intranode recovery. (II) Intranode and internode recovery on demand reconstruction of blocks and rebuild. (III) Multistep internode recovery with no rebuild processing. The I/Os Per Second (IOPS) metric is used to assess the cost of fault-tolerance for HRAIDkℓ against RAID(4+ℓ) and RAID0, for varying k and ℓ. The maximum IOPS is at its lowest in degraded mode, but even with fewer operational disks the normal mode IOPS may be exceeded after restriping. Asymptotic reliability analysis and simulation results show that HRAIDkℓ with ℓ>k provides a higher reliability when SN failures are due to disk rather than controller failures. Monte Carlo simulation is used to quantify the effect of various recovery options with varying k and ℓ and as the SN controller failure rate is varied with respect to disk failure rates on the Mean Time to Data Loss (MTTDL). The HRAID paradigm is justified by the fact that Options II attains a significantly higher MTTDL than Option I. Option III with no rebuild processing has an MTTDL exceeding Option II, but a poorer performance. 4 © 2012 Elsevier Inc. All rights reserved.","Concurrency control; Continuous time Markov chains; Decision trees; Erasure coding; Hierarchical RAID (HRAID); Monte-Carlo simulation; Performance analysis; Queueing theory; RAID; Rebuild processing; Reliability analysis; Storage systems; Storage transactions"
"Understanding the future of energy-performance trade-off via DVFS in HPC environments","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857792797&doi=10.1016%2fj.jpdc.2012.01.006&partnerID=40&md5=e3add416ae41fa32d7afa62b6b82d459","DVFS is a ubiquitous technique for CPU power management in modern computing systems. Reducing processor frequency/voltage leads to a decrease of CPU power consumption and an increase in the execution time. In this paper, we analyze which application/platform characteristics are necessary for a successful energy-performance trade-off of large scale parallel applications. We present a model that gives an upper bound on performance loss due to frequency scaling using the application parallel efficiency. The model was validated with performance measurements of large scale parallel applications. Then we track how application sensitivity to frequency scaling evolved over the last decade for different cluster generations. Finally, we study how cluster power consumption characteristics together with application sensitivity to frequency scaling determine the energy effectiveness of the DVFS technique. © 2012 Elsevier Inc. All rights reserved.","DVFS; Energy efficiency; High performance computing"
"Improving the localization accuracy of targets by using their spatial-temporal relationships in wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862000677&doi=10.1016%2fj.jpdc.2012.04.011&partnerID=40&md5=5c9efb1a8fa08c9d5c38ac80b83ded47","Due to the low cost and capabilities of sensors, wireless sensor networks (WSNs) are promising for military and civilian surveillance of people and vehicles. One important aspect of surveillance is target localization. A location can be estimated by collecting and analyzing sensing data on signal strength, time of arrival, time difference of arrival, or angle of arrival. However, this data is subject to measurement noise and is sensitive to environmental conditions, so its location estimates can be inaccurate. In this paper, we add a novel process to further improve the localization accuracy after the initial location estimates are obtained from some existing algorithm. Our idea is to exploit the consistency of the spatial-temporal relationships of the targets we track. Spatial relationships are the relative target locations in a group and temporal relationships are the locations of a target at different times. We first develop algorithms that improve location estimates using spatial and temporal relationships of targets separately, and then together. We prove mathematically that our methods improve the localization accuracy. Furthermore, we relax the condition that targets should strictly keep their relative positions in the group and also show that perfect time synchronization is not required. Simulations were also conducted to test the algorithms. They used initial target location estimates from existing signal-strength and time-of-arrival algorithms and implemented our own algorithms. The results confirmed improved localization accuracy, especially in the combined algorithms. Since our algorithms use the features of targets and not the underlying WSNs, they can be built on any localization algorithm whose results are not satisfactory. © 2012 Elsevier Inc. All rights reserved.","Localization; Spatial-temporal; Surveillance; Tracking; Wireless sensor networks"
"A fast algorithm for constructing inverted files on heterogeneous platforms","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859161233&doi=10.1016%2fj.jpdc.2012.02.005&partnerID=40&md5=bc984d4ba0faa43946e49d2788419954","Given a collection of documents residing on a disk, we develop a new strategy for processing these documents and building the inverted files extremely quickly. Our approach is tailored for a heterogeneous platform consisting of multicore CPUs and highly multithreaded GPUs. Our algorithm is based on a number of novel techniques, including a high-throughput pipelined strategy, a hybrid trie and B-tree dictionary data structure, dynamic work allocation to CPU and GPU threads, and optimized CUDA indexer implementation. We have performed extensive tests of our algorithm on a single node (two Intel Xeon X5560 Quad-core CPUs) with two NVIDIA Tesla C1060 GPUs attached to it, and were able to achieve a throughput of more than 262 MB/s on the ClueWeb09 dataset. Similar results were obtained for widely different datasets. The throughput of our algorithm is superior to the best known algorithms reported in the literature even when compared to those run on large clusters. © 2012 Elsevier Inc. All rights reserved.","GPU; Indexer; Inverted files; Multicore; Pipelined and parallel parsing and indexing"
"HiCOO: Hierarchical cooperation for scalable communication in Global Address Space programming models on Cray XT systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866133201&doi=10.1016%2fj.jpdc.2012.01.022&partnerID=40&md5=ed4f46356ae2cf4d30a83a219d80fd09","Global Address Space (GAS) programming models enable a convenient, shared-memory style addressing model. Typically this is realized through one-sided operations that can enable asynchronous communication and data movement. With the size of petascale systems reaching 10,000s of nodes and 100,000s of cores, the underlying runtime systems face critical challenges in (1) scalably managing resources (such as memory for communication buffers), and (2) gracefully handling unpredictable communication patterns and any associated contention. For any solution that addresses these resource scalability challenges, equally important is the need to maintain the performance of GAS programming models. In this paper, we describe a Hierarchical COOperation (HiCOO) architecture for scalable communication in GAS programming models. HiCOO formulates a cooperative communication architecture: with inter-node cooperation amongst multiple nodes (a.k.a multinode) and hierarchical cooperation among multinodes that are arranged in various virtual topologies. We have implemented HiCOO for a popular GAS runtime library, Aggregate Remote Memory Copy Interface (ARMCI). By extensively evaluating different virtual topologies in HiCOO in terms of their impact to memory scalability, network contention, and application performance, we identify MFCG as the most suitable virtual topology. The resulting HiCOO architecture is able to realize scalable resource management and achieve resilience to network contention, while at the same time maintaining or enhancing the performance of scientific applications. In one case, it reduces the total execution time of an NWChem application by 52%. © 2012 Elsevier Inc. All rights reserved.","ARMCI; Contention; GAS; Multicore; Multinode; Virtual Topology"
"Sophia: A local trust system to secure key-based routing in non-deterministic DHTs","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867885984&doi=10.1016%2fj.jpdc.2012.07.009&partnerID=40&md5=24ed20a743f5e5aa5dce83a249fe44bf","Today, many distributed applications are typically deployed at a large scale, including Grid, web search engines and content distribution networks, and it is expected for their scale to grow more in terms of number of machines, locations and administrative domains. This poses many scalability issues related to the scale of the environment they run in. To explicitly address these issues, many distributed systems and everyday services use peer-to-peer (P2P) overlays to allow other parts of the system to benefit from the fault-tolerance and scalability of P2P technology. In particular, Distributed Hash Tables (DHTs), which implement a simple put-and-get interface to a dictionary-like data structure, have been extensively used to overcome the current limitations associated with the centralized and hierarchical components of distributed systems, including data management, resource discovery, job scheduling etc. However, DHTs exhibit a number of security problems in large-scale systems, where a large number of users are unknown to administrators (e.g., desktop grids). This makes the detection of malicious behavior an extremely complex task. As a result, attackers can disrupt the system in very dangerous ways, leading ultimately to the failure of the routing service, which is catastrophic for any DHT. To address this issue, we introduce Sophia, a new security technique which combines iterative routing with local trust to implement a secure lookup service with almost zero overhead. The key aspect to incur zero overhead is the use of local trust. In Sophia, each user identifies which routing entries are cooperative based on the success and failure of his own lookups, so no trust information is shared. Our simulation results demonstrate that Sophia does better than existing state-of-the-art solutions for secure routing in DHTs, both in stable and high dynamic environments, and even for collusive threat models. © 2012 Elsevier Inc. All rights reserved.","Distributed hash tables; Large-scale distributed systems; Security; Trust and reputation systems"
"Fine grained load balancing in multi-hop wireless networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857785745&doi=10.1016%2fj.jpdc.2012.01.010&partnerID=40&md5=1b9ec7d67945af7efd271fc77d4284f6","In this paper we address the problem of local balancing in multi-hop wireless networks. We introduce the notion of proactive routing: after a short pre-processing phase in which nodes build their routing tables by exchanging messages with neighbors, we require that nodes decide the relay of each message without any further interaction with other nodes. Besides delivering very low communication overhead, proactive routing protocols are robust against some well known active attacks to network routing. In this framework, we develop a proactive routing protocol that is able to balance the local load. Experiments show that our protocol improves network lifetime up to 98% and that it delivers a network that is more robust against attacks that have the goal of getting control over a large part of the network traffic. © 2012 Elsevier Inc. All rights reserved.","Energy efficient; Load balancing; Multi-hop wireless networks; Routing; Security; Sensor networks"
"Optimal, quality-aware scheduling of data consumption in mobile ad hoc networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865064778&doi=10.1016%2fj.jpdc.2012.05.011&partnerID=40&md5=24339ea03e62b054ad6b5b0cec61e8fd","In this paper we study the delivery of quality contextual information in mobile ad-hoc networks. We consider that information has a certain quality level that fades over time. Mobile context-aware applications receive and process disseminated information given that the corresponding quality is above the lowest level. The necessity for optimally scheduling information delivery arises from the dynamic nature of the network, e.g., probabilistic spreading, caching, deferred delivery, and mobility of nodes. We propose two policies for optimal scheduling information delivery consumption based on the Optimal Stopping Theory. The mobile nodes delay the reporting of information to mobile context-aware applications in search for better quality. The proposed policies efficiently deal with the delivery of quality information in mobile ad-hoc networks. © 2012 Elsevier Inc. All rights reserved.","Mobile ad-hoc networks; Optimal stopping theory; Quality information delivery"
"Thread vulnerability in parallel applications","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865062359&doi=10.1016%2fj.jpdc.2012.05.002&partnerID=40&md5=5e288f63539a557fc40a023404b9fb08","Continuously reducing transistor sizes and aggressive low power operating modes employed by modern architectures tend to increase transient error rates. Concurrently, multicore machines are dominating the architectural spectrum today in various application domains. These two trends require a fresh look at resiliency of multithreaded applications against transient errors from a software perspective. In this paper, we propose and evaluate a new metric called the Thread Vulnerability Factor (TVF). A distinguishing characteristic of TVF is that its calculation for a given thread (which is typically one of the threads of a multithreaded application) does not depend on its code alone, but also on the codes of the threads that share resources and data with that thread. As a result, we decompose TVF of a thread into two complementary parts: local and remote. While the former captures the TVF induced by the code of the target thread, the latter represents the vulnerability impact of the threads that interact with the target thread. We quantify the local and remote TVF values for three architectural components (register file, ALUs, and caches) using a set of ten multithreaded applications from the Parsec and Splash-2 benchmark suites. Our experimental evaluation shows that TVF values tend to increase as the number of cores increases, which means the system becomes more vulnerable as the core count rises. We further discuss how TVF metric can be employed to explore performance-reliability tradeoffs in multicores. Reliability-based analysis of compiler optimizations and redundancy-based fault tolerance are also mentioned as potential usages of our TVF metric. © 2012 Elsevier Inc. All rights reserved.","Fault tolerance; Multicores; Reliability; Thread Vulnerability; TVF"
"Parallel ant colony optimization on graphics processing units","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869495318&doi=10.1016%2fj.jpdc.2012.01.003&partnerID=40&md5=a600384f63ecf9ff144a25156671092f","The purpose of this paper is to propose effective parallelization strategies for the Ant Colony Optimization (ACO) metaheuristic on Graphics Processing Units (GPUs). The Max-Min Ant System (MMAS) algorithm augmented with 3-opt local search is used as a framework for the implementation of the parallel ants and multiple ant colonies general parallelization approaches. The four resulting GPU algorithms are extensively evaluated and compared on both speedup and solution quality on a state-of-the-art Fermi GPU architecture. A rigorous effort is made to keep parallel algorithms true to the original MMAS applied to the Traveling Salesman Problem. We report speedups of up to 23.60 with solution quality similar to the original sequential implementation. With the intent of providing a parallelization framework for ACO on GPUs, a comparative experimental study highlights the performance impact of ACO parameters, GPU technical configuration, memory structures and parallelization granularity. © 2012 Elsevier Inc. All rights reserved.","Ant colony optimization; CUDA; GPU; MMAS; Multiple colonies; Parallel ants; Parallel metaheuristics"
"Optimizing the stretch of independent tasks on a cluster: From sequential tasks to moldable tasks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857793516&doi=10.1016%2fj.jpdc.2011.12.007&partnerID=40&md5=ca34a8f65ad43b1fb2ad0a5d4784b52a","This paper addresses the problem of scheduling non-preemptive moldable tasks to minimize the stretch of the tasks in an online non-clairvoyant setting. To the best of the authors' knowledge, this problem has never been studied before. To tackle this problem, first the sequential subproblem is studied through the lens of the approximation theory. An algorithm, called DASEDF, is proposed and, through simulations, it is shown to outperform the first-come, first-served scheme. Furthermore, it is observed that machine availability is the key to getting good stretch values. Then, the moldable task scheduling problem is considered, and, by leveraging the results from the sequential case, another algorithm, DBOS, is proposed to optimize the stretch while scheduling moldable tasks. This work is motivated by a task scheduling problem in the context of parallel short sequence mapping which has important applications in biology and genetics. The proposed DBOS algorithm is evaluated both on synthetic data sets that represent short sequence mapping requests and on data sets generated using log files of real production clusters. The results show that the DBOS algorithm significantly outperforms the two state-of-the-art task scheduling algorithms on stretch optimization. © 2012 Elsevier Inc. All rights reserved.","Approximation algorithm; Job scheduling; Maximum stretch; Moldable task; Online scheduling; Resource augmentation; Sequential task; Simulation"
"Scalable architecture for a contention-free optical network on-chip","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866056970&doi=10.1016%2fj.jpdc.2012.02.003&partnerID=40&md5=519dbd066d9961366c87a150f6c2cc30","This paper proposes CoNoC (Contention-free optical NoC) as a new architecture for on-chip routing of optical packets. CoNoC is built upon all-optical switches (AOSs) which passively route optical data streams based on their wavelengths. The key idea of the proposed architecture is the utilization of per-receiver wavelength in the data network to prevent optical contention at the intermediate nodes. Routing optical packets according to their wavelength eliminates the need for resource reservation at the intermediate nodes and the corresponding latency, power, and area overheads. Since passive architecture of the AOS confines the optical contention to the end-points, we propose an electrical arbitration architecture for resolving optical contention at the destination nodes. By performing a series of simulations, we study the efficiency of the proposed architecture, its power and energy consumption, and the data transmission latency. Moreover, we compare the proposed architecture with electrical NoCs and alternative ONoC architectures under various synthetic traffic patterns. Averaged across different traffic patterns, the proposed architecture reduces per-packet power consumption by 19%, 28%, 29%, and 91% and achieves per-packet energy reduction of 28%, 40%, 20%, and 99% over Columbia, Phastlane, λ-router, and electrical torus, respectively. © 2012 Elsevier Inc. All rights reserved.","Network-on-Chip; Optics; Passive router; Power consumption; Scalability; Wavelength routing"
"Direction-based adaptive data propagation for heterogeneous sensor mobility","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859893666&doi=10.1016%2fj.jpdc.2012.02.010&partnerID=40&md5=c4964ef354c041e2313abc4b8e18af3d","We consider sensor networks where the sensor nodes are attached on entities that move in a highly dynamic, heterogeneous manner. To capture this mobility diversity we introduce a new network parameter, the direction-aware mobility level, which measures how fast and close each mobile node is expected to get to the data destination (the sink). We then provide local, distributed data dissemination protocols that adaptively exploit the node mobility to improve performance. In particular, ""high"" mobility is used as a low cost replacement for data dissemination (due to the ferrying of data), while in the case of ""low"" mobility either (a) data propagation redundancy is increased (when highly mobile neighbors exist) or (b) long-distance data transmissions are used (when the entire neighborhood is of low mobility) to accelerate data dissemination toward the sink. An extensive performance comparison to relevant methods from the state of the art demonstrates significant improvements, i.e. latency is reduced by even four times while keeping energy dissipation and delivery success at very satisfactory levels. © 2012 Elsevier Inc. All rights reserved.","Adaptation; Data propagation; Mobile sensors; Performance evaluation; Wireless sensor networks"
"Concurrent face traversal for efficient geometric routing","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859159043&doi=10.1016%2fj.jpdc.2012.01.009&partnerID=40&md5=911a20a999fa6f84abe7e5b8eec253b8","We present a concurrent face routing CFR algorithm. We formally prove that the worst case latency of our algorithm is asymptotically optimal. Our simulation results demonstrate that, on average, the path stretch, i.e., the speed of message delivery, achieved by CFR is significantly better than by other known geometric routing algorithms. In fact, it approaches the shortest possible path. CFR maintains its advantage over the other algorithms in pure form as well as in combination with greedy routing. CFR displays this performance superiority both on planar and non-planar graphs. © 2012 Elsevier Inc. All rights reserved.","Ad hoc wireless routing; Geometric routing"
"Making-a-stop: A new bufferless routing algorithm for on-chip network","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862819671&doi=10.1016%2fj.jpdc.2012.01.001&partnerID=40&md5=836cb4dfafae6cb77b9c96e44d95b4cb","In the deep submicron regime, the power and area consumed by router buffers in network-on-chip (NoC) have become a primary concern. With buffers elimination, bufferless routing is emerging as a promising solution to provide power-and-area efficiency for NoC. In this paper, we present a new bufferless routing algorithm that can be coupled with any topology. The proposed routing algorithm is based on the concept of making-a-stop (MaS), aiming to deadlock and livelock freedom in wormhole-switched NoC. Performance evaluation is carried out by using a flit-level, cycle-accurate network simulator under synthetic traffic scenarios. Simulation results indicate that the proposed routing algorithm yields an improvement over the recent bufferless routing algorithm in average latency, power consumption, and area overhead by up to 10%, 9%, and 80%, respectively. © 2012 Elsevier Inc. All rights reserved.","Bufferless routing; Livelock freedom; Network-on-chip; Wormhole"
"A scalable and fault-tolerant network routing scheme for many-core and multi-chip systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866118627&doi=10.1016%2fj.jpdc.2012.01.015&partnerID=40&md5=933c7c5b5214696242eedc3ffaad8507","Current on-chip network and inter-chip interconnection are designed separately. However, this traditional design methodology faces a great challenge: in a multi-chip system, each many-core chip contains hundreds or thousands of processors. The increasing number of on-chip processors must share one input/output unit to interface with the inter-chip interconnection. The increased network usage at the chip interface may create an uneven traffic load in the on-chip network. That is, traffic jams could occur in the chip area around the input/output unit. New technologies, such as through silicon via and silicon interposer, can directly connect networks on chips. These technologies can improve communication performance and reduce power consumption by omitting the input/output unit. This paper proposes a novel routing scheme to deal with the network scalability issues related to the many-core and multi-chip system-in-package paradigm. The proposed scheme can also enhance the fault-tolerance of nano-scale communication in deep-submicron designs. © 2012 Elsevier Inc. All rights reserved.","Fault-tolerance; Many-core; Multi-chip; Network routing; Scalable system; System-in-package"
"Towards green data centers: A comparison of x86 and ARM architectures power efficiency","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867882102&doi=10.1016%2fj.jpdc.2012.08.005&partnerID=40&md5=e484de6ef596a45d3167092fd0b8f270","Servers and clusters are fundamental building blocks of high performance computing systems and the IT infrastructure of many companies and institutions. This paper analyzes the feasibility of building servers based on low power computers through an experimental comparison of server applications running on x86 and ARM computer architectures. The comparison executed on web and database servers includes power usage, CPU load, temperature, request latencies and the number of requests handled by each tested system. Floating point performance and power usage are also evaluated. The use of ARM based systems has shown to be a good choice when power efficiency is needed without losing performance.","Cluster; Energy efficiency; Low power; Performance evaluation; Power usage"
"Graphics processing unit (GPU) programming strategies and trends in GPU computing","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869502554&doi=10.1016%2fj.jpdc.2012.04.003&partnerID=40&md5=f352caab19c41735d266cea94f6a34d4","Over the last decade, there has been a growing interest in the use of graphics processing units (GPUs) for non-graphics applications. From early academic proof-of-concept papers around the year 2000, the use of GPUs has now matured to a point where there are countless industrial applications. Together with the expanding use of GPUs, we have also seen a tremendous development in the programming languages and tools, and getting started programming GPUs has never been easier. However, whilst getting started with GPU programming can be simple, being able to fully utilize GPU hardware is an art that can take months or years to master. The aim of this article is to simplify this process, by giving an overview of current GPU programming strategies, profile-driven development, and an outlook to future trends. © 2012 Elsevier Inc. All rights reserved.","Debugging; Future trends; GPU computing; Hardware; Heterogeneous computing; Optimization; Profiling"
"Interrupting snapshots and the Java TM size method","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861196271&doi=10.1016%2fj.jpdc.2012.03.007&partnerID=40&md5=55f8aaa47321a05d82a3a7bb804083ee","The JavaTM developers kit requires a size() operation for all objects, tracking the number of elements in the object. Unfortunately, the best known solution, available in the Java concurrency package, has a blocking concurrent implementation that does not scale. This paper presents a highly scalable wait-free implementation of a concurrent size() operation based on a new lock-free interrupting snapshots algorithm. The key idea behind the new algorithm is to allow snapshot scan methods to interrupt each other until they agree on a shared linearization point with respect to update methods. This contrasts sharply with past approaches to the classical atomic snapshot problem, that have had threads coordinate the collecting of a shared global view. As we show empirically, the new algorithm scales well, significantly outperforming existing implementations. © 2012 Elsevier Inc. All rights reserved.","Concurrent data-structures; Java concurrency; Lock-free; Multicore algorithms; Wait-free"
"G-MSA - A GPU-based, fast and accurate algorithm for multiple sequence alignment","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869493545&doi=10.1016%2fj.jpdc.2012.04.004&partnerID=40&md5=bcc41ba47b9e68384e557891a7951978","Multiple sequence alignment (MSA) methods are essential in biological analysis. Several MSA algorithms have been proposed in recent years. The quality of the results produced by those methods is reasonable, but there is no single method that consistently outperforms others. Additionally, the increasing number of sequences in the biological databases is perceived as one of the upcoming challenges for alignment methods in the nearest future. The lack of performance concerns not only the alignment problems, but may be observed in many areas of biologically related research. To overcome this problem in the field of pairwise alignment, several GPU (Graphics Processing Unit) computing approaches have been proposed lately. These solutions show a great potential of GPU platform. Therefore, our main idea was to design and implement an MSA method which can take advantage of modern graphics cards. Our solution is based on T-Coffee-well known for its high accuracy MSA algorithm. Its computational time, however, is often unacceptable. Performed tests show that our method, named G-MSA, is highly efficient achieving up to 193-fold speedup on a single GPU while the quality of its results remains very good. Due to effective memory usage the method can perform alignment for huge sets of sequences that previously could only be aligned on computer clusters. Moreover, multiple GPUs support with load balancing makes the application very scalable. © 2012 Elsevier Inc. All rights reserved.","Bioinformatics; GPU heuristics; Multiple GPUs applications; Multiple sequence alignment; The T-Coffee algorithm"
"An effective and robust two-phase resource allocation scheme for interdependent tasks in mobile ad hoc computational Grids","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867886664&doi=10.1016%2fj.jpdc.2012.07.012&partnerID=40&md5=9777dd4b6e03b4dfd183c07fd473dcb9","This paper addresses the problem of resource allocation to interdependent tasks in mobile ad hoc computational Grids. Dependencies between tasks imply that there can be heavy communication induced by data transfers between tasks executed on separate nodes. The communication in mobile ad hoc Grids is always expensive and unreliable, and therefore plays a critical role in application performance. There are several factors that contribute to communication cost. Unreliable and short-term connectivity can increase communication cost due to frequent failure and activation of links, and ineffective resource allocation can increase communication cost due to multi hop communication between dependent tasks. To reduce communication cost, an effective and robust resource allocation scheme is required. However, the design of such a scheme for mobile ad hoc computational Grids exhibits numerous difficulties due to the constrained communication environment, node mobility, and lack of pre-existing network infrastructure. In this paper, we propose a two-phase resource allocation scheme to reduce communication cost between dependent tasks. The scheme is divided into two phases. The first phase exploits the history of user mobility patterns to select nodes that provide long-term connectivity and the second phase takes into account the task and dependency types, and uses the distance information among the nodes selected in the first phase to reduce communication costs. The scheme is validated in a simulation environment using various workloads and parameters. © 2012 Elsevier Inc. All rights reserved.","Ad hoc networks; Computational Grid; Mobile Grids; Resource allocation; Task dependencies"
"A-GHSOM: An adaptive growing hierarchical self organizing map for network anomaly detection","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867826492&doi=10.1016%2fj.jpdc.2012.09.004&partnerID=40&md5=f8b9606b9c6e108a0420e98f3676e543","The growing hierarchical self organizing map (GHSOM) has been shown to be an effective technique to facilitate anomaly detection. However, existing approaches based on GHSOM are not able to adapt online to the ever-changing anomaly detection. This results in low accuracy in identifying intrusions, particularly ""unknown"" attacks. In this paper, we propose an adaptive GHSOM based approach (A-GHSOM) to network anomaly detection. It consists of four significant enhancements: enhanced threshold-based training, dynamic input normalization, feedback-based quantization error threshold adaptation, and prediction confidence filtering and forwarding. We first evaluate the A-GHSOM approach for intrusion detection using the KDD'99 dataset. Extensive experimental results demonstrate that compared with eight representative intrusion detection approaches, A-GHSOM achieves significant overall accuracy improvement and significant improvement in identifying ""unknown"" attacks while maintaining low false-positive rates. It achieves an overall accuracy of 99.63%, and 94.04% accuracy in identifying ""unknown"" attacks while the false positive rate is 1.8%. To avoid drawing research results and conclusions solely based on experiments with the KDD dataset, we have also built a dataset (TD-Sim) that consists of a mixture of live trace data from the Lawrence Berkeley National Laboratory and simulated traffic based on our testbed network, ensuring adequate coverage of a variety of attacks. Performance evaluation with the TD-Sim dataset shows that A-GHSOM adapts to live traffic and achieves an overall accuracy rate of 97.12% while maintaining the false positive rate of 2.6%. © 2012 Elsevier Inc. All rights reserved.","Detection accuracy; False positive rate; Growing hierarchical self organizing map; Network anomaly detection; Online adaptation"
"Dual time-scale distributed capacity allocation and load redirect algorithms for cloud systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859888515&doi=10.1016%2fj.jpdc.2012.02.014&partnerID=40&md5=474d2ae6e0a7a4573f3e51ecceba9b7a","Resource management remains one of the main issues of cloud computing providers because system resources have to be continuously allocated to handle workload fluctuations while guaranteeing Service Level Agreements (SLA) to the end users. In this paper, we propose novel capacity allocation algorithms able to coordinate multiple distributed resource controllers operating in geographically distributed cloud sites. Capacity allocation solutions are integrated with a load redirection mechanism which, when necessary, distributes incoming requests among different sites. The overall goal is to minimize the costs of allocated resources in terms of virtual machines, while guaranteeing SLA constraints expressed as a threshold on the average response time. We propose a distributed solution which integrates workload prediction and distributed non-linear optimization techniques. Experiments show how the proposed solutions improve other heuristics proposed in literature without pzing SLAs, and our results are close to the global optimum which can be obtained by an oracle with a perfect knowledge about the future offered load. © 2012 Elsevier Inc. All rights reserved.","Capacity allocation; Cloud systems; Load balancing; Performance modeling; Resource management; SLA"
"Channel switching control policy for wireless mesh networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865072937&doi=10.1016%2fj.jpdc.2012.06.008&partnerID=40&md5=5dbb386420523ebcefb7d28857ec591c","Dynamic channel assignment algorithms allow wireless nodes to switch channels when their traffic loads exceed certain thresholds. These thresholds represent estimations of their throughput capacities. Unfortunately, the threshold estimation may not be accurate due to co-channel interference (CCI) and adjacent-channel interference (ACI), especially with high traffic loads in dense networks. When the link capacity is over-estimated, these channel assignment algorithms are not effective. This is because the channel switch is not triggered even with overloaded data traffic and the link quality decreases significantly as the channel is overloaded. When the link capacity is under-estimated, the link is under-utilized. Moreover, when link traffic load increases from time to time, channel switch occurs frequently. Such frequent channel switches increase latency and degrade throughput, and can even cause network wide channel oscillations. In this paper, we propose a novel threshold-based control system, called balanced control system (BCS). The proposed threshold-based control policy consists of deciding, according to the real time traffic load and interference, whether to switch to another channel, which channel should be switched to and how to perform the switch. Our control model is based on a fuzzy logic control. The threshold which assists to make the channel switch decisions, could be deduced dynamically according to the real-time traffic of each node. We also design a novel dynamic channel assignment scheme, which is used for the selection of the new channel. The channel switch scheduler is provided to perform channel-switch processing for sender and receiver over enhanced routing protocols. We implement our system in NS2, and the simulation results show that with our proposed system, the performance improves by 12.3%-72.8% in throughput and reduces 23.2%-52.3% in latency. © 2012 Elsevier Inc. All rights reserved.","Adaptive; Balanced control system; Dynamic channel assignment; Real-time; Wireless mesh network"
"CEA: A Cyclic Expansion Algorithm for data migration in parallel video servers","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861190244&doi=10.1016%2fj.jpdc.2012.03.002&partnerID=40&md5=39f99ed13f12ca8d7d58252c2c50d74c","Parallel video servers can achieve highly storage-saving and granularly load-balancing, but they suffer from a system expansion problem. As the number of users continuously increases, the system inevitably needs to expand the number of video servers. However, the expansion of a parallel video server system is not as simple as that of a replicated video server system. Hence, this work develops an efficient expansion algorithm, called the Cyclic Expansion Algorithm (CEA), for parallel video servers. The proposed CEA algorithm has several good features. First, the data layout of each video content exhibits periodicity. Consequently, the meta-data size of each video and the complexity of the CEA algorithm are reduced. Second, the number of required data movements during a system expansion is optimized. Third, the total number of required XOR recomputations for updating parity blocks during an expansion is also minimized. Additionally, the new CEA can be applied to a variety of distributed storage systems, such as the cloud-based storage systems using striping and parity check techniques. © 2012 Elsevier Inc. All rights reserved.","Algorithm; Data migration; Fault-tolerant computing; Load balancing; Parallel video server; System expansion"
"An effective approximation algorithm for the Malleable Parallel Task Scheduling problem","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862784973&doi=10.1016%2fj.jpdc.2012.01.011&partnerID=40&md5=c44e7a9542c1293a1874690189b0f2b2","The Malleable Parallel Task Scheduling problem (MPTS) is an extension of one of the most classic scheduling problems (P∥C max). The only difference is that for MPTS, each task can be processed simultaneously by more than one processor. Such flexibility could dramatically reduce the makespan, but greatly increase the difficulty for solving the problem. By carefully analyzing some existing algorithms for MPTS, we find each of them suitable for some specific cases, but none is effective enough for all cases. Based on such observations, we introduce some optimization algorithms and improving techniques for MPTS, with their performance analyzed in theory. Combining these optimization algorithms and improving techniques gives rise to our novel scheduling algorithm OCM (Optimizations Combined for MPTS), a 2-approximation algorithm for MPTS. Extensive simulations on random datasets and SPLASH-2 benchmark reveal that for all cases, schedules produced by OCM have smaller makespans, compared with other existing algorithms. © 2012 Elsevier Inc. All rights reserved.","Approximation algorithm; Parallel computing; Scheduling algorithm"
"Weak atomicity for the x86 memory consistency model","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865063261&doi=10.1016%2fj.jpdc.2012.06.001&partnerID=40&md5=ed5175c7886b5a1061e77fafc821cb6e","We consider the interaction of weakly atomic Software Transactional Memory (STM) providing single global lock atomicity with the x86 memory consistency model. We show that a practical design for such an STM requires that some program behaviour be disallowed, due to the strictness of the x86 memory consistency model in comparison to the language level memory models hitherto considered in weakly atomic STM designs. We present the design and construction of such an STM that disallows races between a transactional read and a non-transactional write. We also report on a practical application of this STM to elide legacy locks in x86 binaries. This allows software transactional memory to be applied without requiring software to be a priori written with awareness of transactional memory and without any restriction on source language or compiler. As an example, we show how a mainstream multiplayer game can use transactional memory with zero changes and 11% overhead over language level transactional memory, which requires over 700 annotations and severely restricts software development. © 2012 Elsevier Inc. All rights reserved.","Software transactional memory; Weak atomicity; X86 memory consistency model"
"Cooperative private searching in clouds","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862011306&doi=10.1016%2fj.jpdc.2012.04.012&partnerID=40&md5=a896c0e71823962837b75f9801d849e3","With the increasing popularity of cloud computing, there is increased motivation to outsource data services to the cloud to save money. An important problem in such an environment is to protect user privacy while querying data from the cloud. To address this problem, researchers have proposed several techniques. However, existing techniques incur heavy computational and bandwidth related costs, which will be unacceptable to users. In this paper, we propose a cooperative private searching (COPS) protocol that provides the same privacy protections as prior protocols, but with much lower overhead. Our protocol allows multiple users to combine their queries to reduce the querying cost while protecting their privacy. Extensive evaluations have been conducted on both analytical models and on a real cloud environment to examine the effectiveness of our protocol. Our simulation results show that the proposed protocol reduces computational costs by 80% and bandwidth cost by 37%, even when only five users query data. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Cooperative; Cost-effective; Privacy preserving; Private searching"
"A reflective service gateway for integrating evolvable sensor-actuator networks with pervasive infrastructure","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865068837&doi=10.1016%2fj.jpdc.2012.06.011&partnerID=40&md5=4484f4852d542a40f94a3be573024805","As sensor and actuator networks (SANETs) evolve, sensor and actuator nodes (SANs) will be frequently added, removed, and upgraded as needed, which we call evolvable SANETs (ESANETs). However, existing service gateways for SANETs are assumed to know the operational environment at design time with limited awareness of operational SANET conditions and, in the meanwhile, provide a single particular form of access without knowledge of application activities. Therefore, current service gateways are infeasible to accommodate ESANETs. To cope with this, this paper presents a reflective service gateway (RSG), which self-adapts to ESANET changes and enables ESANETs to customize its activities according to the application services involved. The key to enabling this is a reflection mechanism whereby RSG is able to monitor network status of multi-hop ESANETs so that its internal structures and behaviors can be altered adaptively. In addition, RSG allows heterogeneous middleware systems (HMSs) to concurrently access ESANETs, promoting the flexibility of RSG to be integrated with HMSs. To evaluate RSG, we compare the ESANET messaging cost of RSG with that of the OSGi-based framework by mathematical models. In addition, we implement a RSG prototype and a set of middleware system case studies. Finally, we conduct empirical performance measurements with home sensor and actuator applications using ZigBee. © 2012 Elsevier Inc. All rights reserved.","Middleware; Network management; Reflection; Service gateway; Wireless sensor and actuator networks"
"An accurate performance model for network-on-chip and multicomputer interconnection networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865074959&doi=10.1016%2fj.jpdc.2012.05.005&partnerID=40&md5=4893bc76be51f2f656930f6fb9c42e28","In this paper, we present a mathematical background for a new approach for performances modeling of interconnection networks, based on analyzing the packet blocking and waiting time spent in each channel passing through all possible paths in the channel dependency graph. We have proposed a new, simple and very accurate analytical model for deterministic routing in wormhole networks, which is general in terms of the network topology and traffic distribution. An accurate calculation of the variance of the service time has been developed, which overcomes the rough approximation used, as a rule, in the existing models. The model supports two-dimensional mesh topologies, widely used in network-on-chip architectures, and multidimensional topologies, popular in multicomputer architectures. It is applicable even for irregular topologies and arbitrary application-specific traffic. Results obtained through simulation show that the model achieves a high degree of accuracy. © 2012 Elsevier Inc. All rights reserved.","Deterministic routing; Interconnection network; Network on chip; Performance modeling; Wormhole"
"A data dependence test based on the projection of paths over shape graphs","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867756763&doi=10.1016%2fj.jpdc.2012.08.004&partnerID=40&md5=fed049ae9f9830e201152ef06a0fb6e5","We propose a data dependence detection test based on a new conflict analysis algorithm for C codes which make intensive use of recursive data structures dynamically allocated in the heap. This algorithm requires two pieces of information from the code section under analysis (a loop or a recursive function): (i) abstract shape graphs that represent the state of the heap at the code section; and (ii) path expressions that collect the traversing information for each statement. Our algorithm projects the path expressions on the shape graphs and checks over the graphs to ascertain whether one of the sites reached by a write statement matches one of the sites reached by another statement on a different loop iteration (or on a different call instance in a recursive function), in which case a conflict between the two statements is reported. Although our algorithm presents exponential complexity, we have found that in practice the parameters that dominate the computational cost have very low values, and to the best of our knowledge, all the other related studies involve higher costs. In fact, our experimental results show reductions in the data dependence analysis times of one or two orders of magnitude in some of the studied benchmarks when compared to a previous data dependence algorithm. Thanks to the information on uncovered data dependences, we have manually parallelized these codes, achieving speedups of 2.19 to 3.99 in four cores. © 2012 Elsevier Inc. All rights reserved.","Data dependencies; Data types and structures; Languages and compilers; Linked representations; Program analysis"
"Fat-tree routing and node ordering providing contention free traffic for MPI global collectives","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866076393&doi=10.1016%2fj.jpdc.2012.01.018&partnerID=40&md5=e897a0c6a4f66e4e473db8cd7eed448f","As the size of High Performance Computing clusters grows, so does the probability of interconnect hot spots that degrade the latency and effective bandwidth the network provides. This paper presents a solution to this scalability problem for real life constant bisectional-bandwidth fat-tree topologies. It is shown that maximal bandwidth and cut-through latency can be achieved for MPI global collective traffic. To form such a congestion-free configuration, MPI programs should utilize collective communication, MPI-node-order should be topology aware, and the packet routing should match the MPI communication patterns. First, we show that MPI collectives can be classified into unidirectional and bidirectional shifts. Using this property, we propose a scheme for congestion-free routing of the global collectives in fully and partially populated fat trees running a single job. The no-contention result is then obtained for multiple jobs running on the same fat-tree by applying some job size and placement restrictions. Simulation results of the proposed routing, MPI-node-order and communication patterns show no contention which provides a 40% throughput improvement over previously published results for all-to-all collectives. © 2012 Elsevier Inc. All rights reserved.","Collective communication; Network topologies; Routing algorithms and techniques"
"Selecting proper wireless network interfaces for user experience enhancement with guaranteed probability","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867824134&doi=10.1016%2fj.jpdc.2012.08.006&partnerID=40&md5=4f6178a68c44f4507d611bfbdf177542","With the increasing capabilities of mobile phones, mobile users access data via wireless interfaces pervasively. Although WiFi has limited coverage and resulted in a bigger delay of data access, it is not uncommon that mobile users are willing to use WiFi to transmit data to decrease communication costs instead of 3G. Hence it is reasonable to use delay tolerance strategies to balance execution time, energy consumption, and communication cost. In this paper, we model mobile user experience as a combination of three random variables (energy consumption, execution time and communication cost). We present a wireless interface scheduling algorithm to select proper wireless interfaces for a set of data-dependent sporadic tasks to enhance user experience under the constraints of execution time, energy consumption, and communication cost with a guaranteed confidence probability in a delay-tolerant environment. The experimental results show that our approach can effectively enhance the user experience. © 2012 Elsevier Inc. All rights reserved.","Mobile user experience; Probabilistic assignment; Wireless network interfaces"
"On the impact of serializing contention management on STM performance","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859927476&doi=10.1016%2fj.jpdc.2012.02.009&partnerID=40&md5=4da206e121bdf176f748d6c1f64597fb","Transactional memory (TM) is an emerging concurrent programming abstraction. Numerous software-based transactional memory (STM) implementations have been developed in recent years. STM implementations must guarantee transaction atomicity and isolation. In order to ensure progress, an STM implementation must resolve transaction collisions by consulting a contention manager (CM). Recent work established that serializing contention management-a technique in which the execution of colliding transactions is serialized for eliminating repeat-collisions-can dramatically improve STM performance in high-contention workloads. In low-contention and highly-parallel workloads, however, excessive serialization of memory transactions may limit concurrency too much and hurt performance. It is therefore important to better understand how the impact of serialization on STM performance varies as a function of workload characteristics. We investigate how serializing CM influences the performance of STM systems. Specifically, we study serialization's influence on STM throughput (number of committed transactions per time unit) and efficiency (ratio between the extent of ""useful"" work done by the STM and work ""wasted"" by aborts) as the workload's level of contention changes. Towards this goal, we implement CBench - a synthetic benchmark that generates workloads in which transactions have (parameter) pre-determined length and probability of being aborted in the lack of contention reduction mechanisms. CBench facilitates evaluating the efficiency of contention management algorithms across the full spectrum of contention levels. The characteristics of TM workloads generated by real applications may vary over time. To achieve good performance, CM algorithms need to monitor these characteristics and change their behavior accordingly. We implement adaptive algorithms that control the activation of serializing CM according to measured contention level, based on a novel low-overhead serialization mechanism. We then evaluate our new algorithms on CBench-generated workloads and on additional well-known STM benchmark applications. Our results shed light on the manner in which serializing CM should be used by STM systems. We show that adaptive contention managers are susceptible to a phenomenon of mode oscillations-in which serialization is repeatedly turned on and off-which hurts performance. We implement a simple stabilizing mechanism that solves this problem. We also compare the performance of local and global adaptive CM algorithms and demonstrate that local adaptive algorithms are superior for applications with asymmetric workloads. © 2012 Elsevier Inc. All rights reserved.","Conflict detection; Contention management; Transactional memory"
"Constructing sensor barriers with minimum cost in wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867885723&doi=10.1016%2fj.jpdc.2012.07.004&partnerID=40&md5=d9accb30911b4e47c579df09fdf64338","One major application category for wireless sensor networks is to detect intruders entering protected areas. Early research has studied the barrier coverage problem for intruder detection. However, an open problem is to build sensor barriers with minimum cost in wireless sensor networks. This is a critical problem (called minimum-cost barrier coverage), and its solution can be widely used in sensor barrier applications, such as border security and intruder detection. In this paper, we present a complete solution to the minimum-cost barrier coverage problem. The cost here can be any performance measurement and normally is defined as the resource consumed or occupied by the sensor barriers. Our algorithm, called the PUSH-PULL-IMPROVE algorithm, is the first one that provides a distributed solution to the minimum-cost barrier coverage problem in asynchronous wireless sensor networks. It can be used for protected areas of any size and shape with homogeneous or heterogeneous networks. In our algorithm, each node does not necessarily know its exact location and only needs to communicate with its neighbors. For a deployment of n sensors and a cost measurement with maximum value Cmax, our algorithm has O( n2log(n Cmax)) message complexity and O( n2log(n Cmax)) time complexity to find K barriers. Simulation results verify the performance of the algorithm. We observe that the actual number of messages sent in the simulations is much less than n2. © 2012 Elsevier Inc. All rights reserved.","Asynchronous communications; Barrier coverage; Complexity analysis; Distributed algorithm; Minimum cost flow; Wireless sensor network"
"Performance tradeoffs in structured peer to peer streaming","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856343764&doi=10.1016%2fj.jpdc.2011.12.006&partnerID=40&md5=c2abf49d2c1da667ee12b47b4211aad1","We consider the following basic question: a source node wishes to stream an ordered sequence of packets to a collection of receivers, which are in K clusters. A node may send a packet to another node in its own cluster in one time step and to a node in a different cluster in Tc time steps ( Tc>1). Each cluster has two special nodes. We assume that the source and the special nodes in each cluster have a higher capacity and thus can send multiple packets at each step, while all other nodes can both send and receive a packet at each step. We construct two (intra-cluster) data communication schemes, one based on multi-trees (using a collection of d-ary interior-disjoint trees) and the other based on hypercubes. The multi-tree scheme sustains streaming within a cluster with O(dlogN) maximum playback delay and O(dlogN) size buffers, while communicating with O(d) neighbors, where N is the maximum size of any cluster. We also show that this protocol is optimal when d=2 or 3. The hypercube scheme sustains streaming within a cluster, with O(log 2(N)) maximum playback delay and O(1) size buffers, while communicating with O(log(N)) neighbors, for arbitrary N. In addition, we extend our multi-tree scheme to work when receivers depart and arrive over time. We also evaluate our dynamic schemes using simulations. © 2011 Elsevier Inc. All rights reserved.","Peer-to-peer systems; Quality-of-service guarantees; Streaming"
"An efficient parallel construction of optimal independent spanning trees on hypercubes","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867871329&doi=10.1016%2fj.jpdc.2012.07.003&partnerID=40&md5=8cc2cee00e83c12d7455033eb4eebfcf","Reliable data broadcasting on parallel computers can be achieved by applying more than one independent spanning tree (IST). Using k-IST-based broadcasting from root r on an interconnection network (N=2 k) provides k-degree fault tolerance in broadcasting, while construction of optimal height k-ISTs needs more time than that of one IST. In the past, most research focused on constructing k ISTs on the hypercube Qk, an efficient communication network. One sequential approach utilized the recursive feature of Qk to construct k ISTs working on a specific root (r)=0 in O(kN) time. Another parallel approach was introduced for generating k ISTs with optimal height on Qk, based on HDLS (Hamming Distance Latin Square), single pointer jumping, which is applied for a source (r)=0 in O( k2) time for successful broadcasting in O(k). For broadcasting from r≠0, those existing approaches require a special routine to reassign new nodes' IDs for logical r=0. This paper proposes a flexible and efficient parallel construction of k ISTs with optimal height on Qk, a generalized approach, for an arbitrary root (r=0,1,2,..., or 2 k-1) in O(k) time. Our focus is to introduce the more efficient time (O(k)) of preprocessing, based on double pointer jumping over O( k2) of the HDLS approach. We also prove that our generalized parallel k-IST construction (arbitrary r) with optimal height on Qk is correctly set in efficient O(k) time. Finally, experiments were performed by simulation to investigate the fault-tolerance effect in reliable broadcasting. Experimental results showed that our efficient ISTs yielded 10%-20% fault tolerance for successful broadcasting (on N=16-1024 PEs). © 2012 Elsevier Inc. All rights reserved.","Double pointer jumping; Hypercube interconnection networks; ISTs (independent spanning trees); Multicomputers; Multicores; Parallel computers; Reliable and fault-tolerant broadcasting"
"Self-stabilizing byzantine asynchronous unison","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861197037&doi=10.1016%2fj.jpdc.2012.04.001&partnerID=40&md5=7281ecf06507697f63898e4b79dd76b9","We explore asynchronous unison in the presence of systemic transient and permanent Byzantine faults in shared memory. We observe that the problem is not solvable under a less than strongly fair scheduler or for system topologies with maximum node degree greater than two. We present then a self-stabilizing Byzantine-tolerant solution to asynchronous unison for chain and ring topologies under the central strongly fair daemon. Our algorithm has minimum possible containment radius and optimal stabilization time. © 2012 Elsevier Inc. All rights reserved.","Byzantine containment; Clock synchronization; Self-stabilization; Strict stabilization; Unison"
"A transactional runtime system for the Cell/BE architecture","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867790488&doi=10.1016%2fj.jpdc.2012.08.001&partnerID=40&md5=6f5e4a84710a29f82d1fe3104d6b1631","Single-core architectures have hit the end of the road and industry and academia are currently exploiting new multicore design alternatives. In special, heterogeneous multicore architectures have attracted a lot of attention but developing applications for such architectures is not an easy task due to the lack of appropriate tools and programming models. We present the design of a runtime system for the Cell/BE architecture that works with memory transactions. Transactional programs are automatically instrumented by the compiler, shortening development time and avoiding synchronization mistakes usually present in lock-based approaches (such as deadlock). Experimental results conducted with a prototype implementation and the STAMP benchmark show good scalability for applications with moderate to low contention levels, and whose transactions are not too small. For those cases in which a small performance loss is admissible, we believe that the ease of programming provided by transactions greatly pays off. © 2012 Elsevier Inc. All rights reserved.","Multiprocessors; Parallel programming; Transactional memory"
"Parallel differential evolution with self-adapting control parameters and generalized opposition-based learning for solving high-dimensional optimization problems","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869457784&doi=10.1016%2fj.jpdc.2012.02.019&partnerID=40&md5=4c3e7d035a3dc691d8dfe58c37817b74","Solving high-dimensional global optimization problems is a time-consuming task because of the high complexity of the problems. To reduce the computational time for high-dimensional problems, this paper presents a parallel differential evolution (DE) based on Graphics Processing Units (GPUs). The proposed approach is called GOjDE, which employs self-adapting control parameters and generalized opposition-based learning (GOBL). The adapting parameters strategy is helpful to avoid manually adjusting the control parameters, and GOBL is beneficial for improving the quality of candidate solutions. Simulation experiments are conducted on a set of recently proposed high-dimensional benchmark problems with dimensions of 100, 200, 500 and 1,000. Simulation results demonstrate that GjODE is better than, or at least comparable to, six other algorithms, and employing GPU can effectively reduce computational time. The obtained maximum speedup is up to 75. © 2012 Elsevier Inc. All rights reserved.","Differential evolution (DE); Generalized opposition-based learning; Graphics processing units (GPU); High-dimensional global optimization"
"Transactional scheduling for read-dominated workloads","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865070221&doi=10.1016%2fj.jpdc.2012.05.012&partnerID=40&md5=39e89bc8584c90aa1cd7280b71624bec","The transactional approach to contention management guarantees atomicity by aborting transactions that may violate consistency. A major challenge in this approach is to schedule transactions in a manner that reduces the total time to perform all transactions (the makespan), since transactions are often aborted and restarted. The performance of a transactional scheduler can be evaluated by the ratio between its makespan and the makespan of an optimal, clairvoyant scheduler that knows the list of resource accesses that will be performed by each transaction, as well as its release time and duration. This paper studies transactional scheduling in the context of read-dominated workloads; these common workloads include read-only transactions, i.e., those that only observe data, and late-write transactions, i.e., those that update only towards the end of the transaction. We present the Bimodal transactional scheduler, which is especially tailored to accommodate read-only transactions, without punishing transactions that write most of their duration (early-write transactions). It is evaluated by comparison with an optimal clairvoyant scheduler; we prove that Bimodal demonstrates the best competitive ratio achievable by a non-clairvoyant schedule for workloads consisting of early-write and read-only transactions. We also show that late-write transactions significantly deteriorate the competitive ratio of any non-clairvoyant scheduler, assuming it takes a conservative approach to conflicts. © 2012 Elsevier Inc. All rights reserved.","Competitive analysis; Read-dominated workload; Scheduling; Transactional memory"
"Audit: A new synchronization API for the GET/PUT protocol","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866079655&doi=10.1016%2fj.jpdc.2012.01.019&partnerID=40&md5=d9ed2ac30e1eaa6a097e831bec517004","The GET/PUT protocol is considered an effective communication API for parallel computing. However, the one-sided nature of the GET/PUT protocol lacks synchronization functionality for the target process. To date, several techniques have been proposed to tackle this problem. The APIs suggested thus far have failed to hide implementation details of the synchronization functionality. In this paper, a new synchronization API for the GET/PUT protocol is proposed. The central idea here is to associate synchronization flags with the G ET/PUT memory regions. Using this technique, synchronization flags are hidden from users, and they are freed from managing the associations between the memory regions and the synchronization flags. The proposed API, named Audit, does not incur additional programming and thus enables natural parallel programming. The evaluations show that Audit exhibits better performance compared to the Notify API proposed in ARMCI. © 2012 Elsevier Inc. All rights reserved.","GET/PUT protocol; One-sided communication; PGAS"
"A new distributed topology control algorithm based on optimization of delay and energy in wireless networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862003636&doi=10.1016%2fj.jpdc.2012.04.007&partnerID=40&md5=71556506223ba3db9053dd7fb99f7871","Topology Control (TC) is one of the most important techniques used in wireless networks to obtain the desired network property. Most existing works with regard to TC focus on reducing energy consumption. Even though there are some works to consider delay in their resulting topologies, they do not consider the effect of radio interference on delay. Aiming at wireless sensor networks, we model a link delay as a function of the signal to interference noise ratio of the receiving node in this link and its packet forwarding time, and take a weight sum of delay and energy consumption as weight of edge (or link). The minimum weight sum of any edge can be solved by using the Getmin-costofedge(i,j) algorithm proposed in this paper. An Optimal Edge-cost Topology Control (OETC) algorithm is proposed to ensure that all approximate minimum-edge-cost paths exist in final topology. We also propose a Distributed Symmetric Link Maintenance (DSLM) algorithm to ensure that all links are symmetric in final topology if all links in original topology are symmetric. We prove that the communication complexity and computational complexity in OETC+DLSM are O( Nu) and O( Ne*Nu2) respectively, where Nu denotes the number of any node u's neighbors and Ne denotes the times of executing the Getmin-costofedge(i,j) algorithm. Furthermore, we verify through simulation that the network topologies produced by OETC+DLSM show good performance in terms of expected average link delay and node hop-count while keeping average energy consumption at an acceptable level. © 2012 Elsevier Inc. All rights reserved.","Delay; Energy consumption; Optimization; Topology control"
"Performance evaluation of OpenMP-based algorithms for handling Kronecker descriptors","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859161221&doi=10.1016%2fj.jpdc.2012.02.001&partnerID=40&md5=dc7bbdd363cc024a5f43bc1099f06732","Numerical analysis of Markovian models is relevant for performance evaluation and probabilistic analysis of systems' behavior from several fields in science and engineering. These models can be represented in a compact fashion using Kronecker algebra. The Vector-Descriptor Product (VDP) is the key operation to obtain stationary and transient solutions of models represented by Kronecker-based descriptors. VDP algorithms are usually CPU intensive, requiring alternatives such as data partitioning to produce results in less time. This paper introduces a set of parallel implementations of a hybrid algorithm for handling descriptors and a detailed performance analysis on four real Markovian models. The implementations are based on different scheduling strategies using OpenMP and existing techniques of static and dynamic load balancing, along with data partitioning presented in the literature. The performance evaluation study contains analysis of speed-up, synchronization and scheduling overheads, task mapping policies, and memory affinity. The results presented here provide insights into different implementation choices for an application on shared-memory systems and how this application benefited from this architecture. © 2012 Elsevier Inc. All rights reserved.","Kronecker descriptors; Markovian models; NUMA machines; OpenMP; Parallel algorithms; Performance evaluation; Scientific computing"
"High performance network virtualization with SR-IOV","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866114929&doi=10.1016%2fj.jpdc.2012.01.020&partnerID=40&md5=f3e5b1d74dd11ced3721654291333b4b","Virtualization poses new challenges to I/O performance. The single-root I/O virtualization (SR-IOV) standard allows an I/O device to be shared by multiple Virtual Machines (VMs), without losing performance. We propose a generic virtualization architecture for SR-IOV-capable devices, which can be implemented on multiple Virtual Machine Monitors (VMMs). With the support of our architecture, the SR-IOV-capable device driver is highly portable and agnostic of the underlying VMM. Because the Virtual Function (VF) driver with SR-IOV architecture sticks to hardware and poses a challenge to VM migration, we also propose a dynamic network interface switching (DNIS) scheme to address the migration challenge. Based on our first implementation of the network device driver, we deployed several optimizations to reduce virtualization overhead. Then, we conducted comprehensive experiments to evaluate SR-IOV performance. The results show that SR-IOV can achieve a line rate throughput (9.48 Gbps) and scale network up to 60 VMs, at the cost of only 1.76% additional CPU overhead per VM, without sacrificing throughput and migration. © 2012 Elsevier Inc. All rights reserved.","Performance; SR-IOV; Virtual Machine; Virtualization; Xen"
"Group key agreement for secure group communication in dynamic peer systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865071791&doi=10.1016%2fj.jpdc.2012.06.004&partnerID=40&md5=713ab149b8e9ef53419226ee95e63a45","Self-organizing group key agreement protocols without a centralized administrator are essential to secure group communication in dynamic peer systems. In this paper, we propose a generic construction of a one-round self-organizing group key agreement protocol based on the Chinese Remainder Theorem. In the proposed construction, all group members contribute their own public keys to negotiate a shared encryption public key, which corresponds to all different decryption keys. Using his/her own secret key, each group member is able to decrypt any ciphertext encrypted by the shared encryption key. Following the generic construction, we instantiate a one-round self-organizing group key agreement protocol using the efficient and computationally inexpensive public key cryptosystem NTRU. Both the public key and the message in this protocol are secure against the known lattice attacks. Furthermore, we also briefly describe another concrete scheme with our generic idea, based on the ElGamal public key cryptosystem. © 2012 Elsevier Inc. All rights reserved.","Dynamic peer system; One-round group key agreement; Secure group communication"
"Peer-to-peer indirect reciprocity via personal currency","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862012743&doi=10.1016%2fj.jpdc.2012.04.008&partnerID=40&md5=3be50d3a5e0543808d14ae27e2ecc25b","Motivating peers to contribute services is critical to the success of peer-to-peer (P2P) systems. Incentive protocols use reciprocity to enforce contributions. Indirect reciprocity schemes are more efficient than direct reciprocity schemes for large-scale P2P systems under high churn rate. In this paper, we propose an indirect reciprocity scheme, called FairTrade, in which peers issue personal currencies to trade services in a P2P system. Personal currency enables indirect reciprocity without relying on any central banks or authorities. It wins extra robustness over global currency as well as much improved trading flexibility and efficiency over direct reciprocity schemes. The acceptance degree of a personal currency depends on the issuer's service capability and reliance. Peer credit limit is introduced to represent the amount of personal currency that will be accepted by other peers. Every peer as a creditor applies a Bayesian network model to setting peer credit limit for a trading partner peer as a creditee. The Bayesian network model learns the creditee's capability and reliability and anticipates the associated profits and risks for credit setting. Using simulations on a file-sharing P2P system, we demonstrate that FairTrade achieves 100% success rate of download requests without malicious peers, and maintains over 90% success rate even with 50% malicious nodes. The system warms up quickly and does not assume any altruistic service or other kind of help. On average, the system traffic stabilizes before peers issue their second download requests. All these good performances are achieved with extremely low trading overhead, which takes up less than 1% of the total traffic. © 2012 Elsevier Inc. All rights reserved.","Incentive protocols; Peer-to-peer (P2P) networks"
"Analyzing performance and power efficiency of network processing over 10 GbE","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866062791&doi=10.1016%2fj.jpdc.2012.02.016&partnerID=40&md5=c70b1e6a9ec6e490a0082dbbf71c3310","Ethernet continues to be the most widely used network architecture today for its low cost and backward compatibility with the existing Ethernet infrastructure. Driven by increasing networking demands of cloud workloads, network speed rapidly migrates from 1 to 10 Gbps and beyond. Ethernet's ubiquity and its continuously increasing rate motivate us to fully understand high speed network processing performance and its power efficiency. In this paper, we begin with per-packet processing overhead breakdown on Intel Xeon servers with 10 GbE networking. We find that besides data copy, the driver and buffer release, unexpectedly take 46% of the processing time for large I/O sizes and even 54% for small I/O sizes. To further understand the overheads, we manually instrument the 10 GbE NIC driver and OS kernel along the packet processing path using hardware performance counters (PMU). Our fine-grained instrumentation pinpoints the performance bottlenecks, which were never reported before. In addition to detailed performance analysis, we also examine power consumption of network processing over 10 GbE by using a power analyzer. Then, we use an external Data Acquisition System (DAQ) to obtain a breakdown of power consumption for individual hardware components such as CPU, memory and NIC, and obtain several interesting observations. Our detailed performance and power analysis guides us to design a more processing- and power-efficient server I/O architecture for high speed networks. © 2012 Elsevier Inc. All rights reserved.","10 GbE; Instrumentation; Network processing; NIC; Performance; Power"
"FREP: Energy proportionality for disk storage using replication","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862017759&doi=10.1016%2fj.jpdc.2012.03.010&partnerID=40&md5=b9b6f0a190f76b1f0b83e8fd20a0958d","Energy saving has become a crucial concern in datacenters as several reports predict that the anticipated energy costs over a three year period will exceed hardware acquisition. In particular, saving energy for storage is of major importance as storage devices (and cooling them off) may contribute over 25 percent of the total energy consumed in a datacenter. Recent work introduced the concept of energy proportionality and argued that it is a more relevant metric than just energy saving as it takes into account the tradeoff between energy consumption and performance. In this paper, we present a novel approach, called FREP (Fractional Replication for Energy Proportionality), for energy management in large datacenters. FREP includes a replication strategy and basic functions to enable flexible energy management. Specifically, our method provides performance guarantees by adaptively controlling the power states of a group of disks based on observed and predicted workloads. Our experiments using a set of real and synthetic traces show that FREP dramatically reduces energy requirements with a minimal response time penalty. © 2012 Elsevier Inc. All rights reserved.","Disk storage; Energy management; Fractional replication; Workload adaptation"
"Scalable communication architectures for massively parallel hardware multi-processors","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866126348&doi=10.1016%2fj.jpdc.2012.01.017&partnerID=40&md5=fdeb38dbdde532e0880698206d5554b7","Modern complex embedded applications in multiple application fields impose stringent and continuously increasing functional and parametric demands. To adequately serve these applications, massively parallel multi-processor systems on a single chip (MPSoCs) are required. This paper is devoted to the design of scalable communication architectures of massively parallel hardware multi-processors for highly-demanding applications. We demonstrated that in the massively parallel hardware multi-processors the communication network influence on both the throughput and circuit area dominates the processors influence, while the traditionally used flat communication architectures do not scale well with the increase of parallelism. Therefore, we propose to design highly optimized application-specific partitioned hierarchical organizations of the communication architectures through exploiting the regularity and hierarchy of the actual information flows of a given application. We developed related communication architecture synthesis strategies and incorporated them into our quality-driven model-based multi-processor design methodology and related automated architecture exploration framework. Using this framework we performed a large series of architecture synthesis experiments. Some of the results of the experiments are presented in this paper. They demonstrate many features of the synthesized communication architectures and show that our method and related framework are able to efficiently synthesize well scalable communication architectures even for the high-end massively parallel multi-processors that have to satisfy extremely stringent computation demands. © 2012 Elsevier Inc. All rights reserved.","4G communication systems; Design space exploration; EDA tools; Highly-demanding applications; LDPC decoding; Massively parallel MPSoCs; Scalable communication architectures"
"Failure-aware resource provisioning for hybrid Cloud infrastructure","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865067611&doi=10.1016%2fj.jpdc.2012.06.012&partnerID=40&md5=86b2d8cb23de01a40ad1c12b94c746f0","Hybrid Cloud computing is receiving increasing attention in recent days. In order to realize the full potential of the hybrid Cloud platform, an architectural framework for efficiently coupling public and private Clouds is necessary. As resource failures due to the increasing functionality and complexity of hybrid Cloud computing are inevitable, a failure-aware resource provisioning algorithm that is capable of attending to the end-users quality of service (QoS) requirements is paramount. In this paper, we propose a scalable hybrid Cloud infrastructure as well as resource provisioning policies to assure QoS targets of the users. The proposed policies take into account the workload model and the failure correlations to redirect users' requests to the appropriate Cloud providers. Using real failure traces and a workload model, we evaluate the proposed resource provisioning policies to demonstrate their performance, cost as well as performance-cost efficiency. Simulation results reveal that in a realistic working condition while adopting user estimates for the requests in the provisioning policies, we are able to improve the users' QoS about 32% in terms of deadline violation rate and 57% in terms of slowdown with a limited cost on a public Cloud. © 2012 Elsevier Inc. All rights reserved.","Deadline; Hybrid Cloud computing; Quality of service; Resource failures; Resource provisioning; Workload model"
"Info-based approach in distributed mutual exclusion algorithms","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859156485&doi=10.1016%2fj.jpdc.2012.01.005&partnerID=40&md5=46f8bf8ad437a3f8291f45baafa884d9","In this paper, we propose a token-based fully distributed algorithm with token-asking method for Distributed Mutual Exclusion (DME) in a computer network composed of N nodes that communicate by message exchanges. The main goal is to introduce a new class of token-based DME algorithms called info-based algorithms. In some previous algorithms, the request to enter a critical section is sent to all nodes because the token-holding node is unknown, but in this info-based algorithm some nodes know the token-holding node and lead critical section entering requests to it, directly. This algorithm uses a logical structure in the form of a wraparound two-dimensional array which is imposed on the interconnecting network. Usually, a request message for entering the critical section is sent vertically down in the array, and eventually sent to the token-holding node with the assistant of an informed-node (common node between the row consisting of the token-holding node and the column consisting of the requester node). The nodes invoking the critical section can obtain the token with fewer message exchanges in comparison with many other algorithms. Typically, the number of message exchanges is 4N+1 under light demand which reduces to approximately 2 message exchanges under heavy demand. A correctness proof is provided. © 2012 Elsevier Inc. All rights reserved.","Concurrency; Critical section; Distributed systems; Message exchange; Mutual exclusion; Token-based algorithm"
"On scheduling dag s for volatile computing platforms: Area-maximizing schedules","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865063121&doi=10.1016%2fj.jpdc.2012.06.007&partnerID=40&md5=5067e694c9fd29d5e2d4ea8bb02b5d12","Many modern computing platforms - notably clouds and desktop grids - exhibit dynamic heterogeneity: the availability and computing power of their constituent resources can change unexpectedly and dynamically, even in the midst of a computation. We introduce a new quality metric, area, for schedules that execute computations having interdependent constituent chores (jobs, tasks, etc.) on such platforms. Area measures the average number of tasks that a schedule renders eligible for execution at each step of a computation. Even though the definition of area does not mention and properties of host platforms (such as volatility), intuition suggests that rendering tasks eligible at a faster rate will have a benign impact on the performance of volatile platforms - and we report on simulation experiments that support this intuition. We derive the basic properties of the area metric and show how to efficiently craft area-maximizing (A-M) schedules for several classes of significant computations. Simulations that compare A-M scheduling against heuristics ranging from lightweight ones (e.g., FIFO) to computationally intensive ones suggest that A-M schedules complete computations on volatile heterogeneous platforms faster than their competition, by percentages that vary with computation structure and platform behavior - but are often in the double digits. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; dag scheduling; Desktop grids; Scheduling for dynamically heterogeneous platforms; Volunteer computing"
"Replicated partitioning for undirected hypergraphs","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857788491&doi=10.1016%2fj.jpdc.2012.01.004&partnerID=40&md5=f91466bf351b4671606f1a41b6796e5d","Hypergraph partitioning (HP) and replication are diverse but powerful tools that are traditionally applied separately to minimize the costs of parallel and sequential systems that access related data or process related tasks. When combined together, these two techniques have the potential of achieving significant improvements in performance of many applications. In this study, we provide an approach involving a tool that simultaneously performs replication and partitioning of the vertices of an undirected hypergraph whose vertices represent data and nets represent task dependencies among these data. In this approach, we propose an iterative-improvement-based replicated bipartitioning heuristic, which is capable of move, replication, and unreplication of vertices. In order to utilize our replicated bipartitioning heuristic in a recursive bipartitioning framework, we also propose appropriate cut-net removal, cut-net splitting, and pin selection algorithms to correctly encapsulate the two most commonly used cutsize metrics. We embed our replicated bipartitioning scheme into the state-of-the-art multilevel HP tool PaToH to provide an effective and efficient replicated HP tool, rpPaToH. The performance of the techniques proposed and the tools developed is tested over the undirected hypergraphs that model the communication costs of parallel query processing in information retrieval systems. Our experimental analysis indicates that the proposed technique provides significant improvements in the quality of the partitions, especially under low replication ratios. © 2012 Elsevier Inc. All rights reserved.","Hypergraph partitioning; Iterative improvement heuristic; Recursive bipartitioning; Replication; Undirected hypergraphs"
"Efficient implementation of globally-aware network flow control","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866060745&doi=10.1016%2fj.jpdc.2012.02.004&partnerID=40&md5=2820bda5419321d9c455245daa961acb","Network flow control mechanisms that are aware of global conditions potentially can achieve higher performance than flow control mechanisms that are only locally aware. Owing to high implementation overhead, globally-aware flow control mechanisms in their purest form are seldom adopted in practice, leading to less efficient simplified implementations. In this paper, we propose an efficient implementation of a globally-aware flow control mechanism, called Critical Bubble Scheme, for k-ary n-cube networks. This scheme achieves near-optimal performance with the same minimal buffer requirements of globally-aware flow control and can be further generalized to implement the general class of buffer occupancy-based network flow control. We prove deadlock freedom of the proposed scheme and exploit its use in handling protocol-induced deadlocks in on-chip environments. We evaluate the proposed scheme using both synthetic traffic and real application loads. Simulation results show that the proposed scheme can reduce the buffer access component of packet latency by as much as 62% over locally-aware flow control, and improve average packet latency by 18.8% and overall execution time by 7.2% in full system simulation. © 2012 Elsevier Inc. All rights reserved.","Adaptive routing; Bubble Flow Control; Globally-aware network flow control; Interconnection network; Protocol-induced or message-dependent deadlock avoidance"
"An accurate on-demand time synchronization protocol for wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865079549&doi=10.1016%2fj.jpdc.2012.06.003&partnerID=40&md5=c4ccd8e18167272ff91f493318c478f8","Time synchronization is a critical component in any wireless sensor network (WSN). In terms of energy consumption, on-demand time synchronization is better than continuous synchronization. However, currently existing on-demand time synchronization protocols have a very low accuracy and very strong spatial accumulative effect. These features are not suitable for several types of WSN applications, such as applications with stringent temporal requirements, or applications that have a large spatial region of interest. In this paper, we propose an on-demand time synchronization protocol, named AOTSP (Accurate On-demand Time Synchronization Protocol), which differs from other protocols of the same category by having the following advantages, as shown in our theoretical analysis and simulation results: (1) weak spatial accumulative effect; (2) fairly low communication cost; (3) low computational complexity; (4) high accuracy; (5) high scalability. Such features make AOTSP a suitable time synchronization protocol for a broad range of WSN applications. © 2012 Elsevier Inc. All rights reserved.","On-demand synchronization; Taylor expansion; Wireless sensor networks"
"Exploiting parallelism in deterministic shared memory multiprocessing","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862805216&doi=10.1016%2fj.jpdc.2012.02.008&partnerID=40&md5=c4ac18f0c03127f609b206764b025f8c","Multi-threaded programs on shared-memory hardware tend to be non-deterministic, which brings challenges to software debugging and testing. Current deterministic implementations eliminate nondeterminism of multi-threaded programs by trading much parallelism for determinism, which leads to low performance. Researchers typically improve parallelism by weakening determinism or introducing weak memory consistency models. However, weak determinism cannot deal with non-determinism caused by data races which are very common in multi-threaded programs. Weak memory consistency models impact the productivity of programming and may bring correctness problems of legacy programs. To address the problems, this paper presents a fully parallelized deterministic runtime, FPDet, which exploits parallelism of deterministic multi-threaded programs by preserving strong determinism and sequential memory consistency. FPDet creates a Working Set Memory (WSM) for each thread to make threads run independently for parallelism. FPDet guarantees determinism by redistributing memory blocks among threads' WSMs in specified synchronization points. As a result, FPDet obtains parallelism and determinism simultaneously. To further exploit parallelism, we propose an Adaptive Budget Adjustment (ABA) mechanism to minimize wait time caused by thread synchronization. We evaluated FPDet using benchmarks from both the SPLASH-2 and PARSEC suits. The results show that FPDet can effectively improve parallelism (the average speedup is more than 1.4 compared with existing approaches) without weakening determinism or memory consistency. © 2012 Elsevier Inc. All rights reserved.","Debugging; Determinism; FPDet; Parallelism; Shared memory; Weak memory consistency"
"A dynamic multicast tree based routing scheme without replication in delay tolerant networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856351778&doi=10.1016%2fj.jpdc.2011.11.007&partnerID=40&md5=f8a06f32072308ea61b97f3ba4eca80e","Delay tolerant networks (DTNs) are a special type of wireless mobile networks which may lack continuous network connectivity. Multicast is an important routing function that supports the distribution of data to a group of users: a service needed for many potential DTN applications. While multicasting in the Internet and in mobile ad hoc networks has been studied extensively, efficient multicasting in DTNs is a considerably different and challenging problem due to the probabilistic nature of contact among nodes. This paper aims to provide a non-replication multicasting scheme in DTNs while keeping the number of forwardings low. The address of each destination is not replicated, but is assigned to a particular node based on its contact rate level and active level. Our scheme is based on a dynamic multicast tree where each leaf node corresponds to a destination. Each tree branch is generated at a contact based on the comparesplit rule proposed in this paper. The compare part determines when a new search branch is needed, and the split part decides how the destination set should be partitioned. When only one destination is left in the destination set, we use either wait (no further relay) or focus (with further relay) to reach the final destination. The effectiveness of our approach is verified through extensive simulations. Ratio-based-split performs best in the comparesplit step, both in synthetic and real traces. Using the wait scheme can reduce the number of forwardings, while using the focus scheme can reduce the latency. © 2011 Elsevier Inc. All rights reserved.","Contact; Delay tolerant networks; Dynamic multicast tree; Efficient protocols; Multicast; Opportunistic routing"
"Scalable communications for a million-core neural processing architecture","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866102225&doi=10.1016%2fj.jpdc.2012.01.016&partnerID=40&md5=2d0f56365d2ceee026c79a54d8cc75b4","The design of a new high-performance computing platform to model biological neural networks requires scalable, layered communications in both hardware and software. SpiNNaker's hardware is based upon Multi-Processor System-on-Chips (MPSoCs) with flexible, power-efficient, custom communication between processors and chips. The architecture scales from a single 18-processor chip to over 1 million processors and to simulations of billion-neuron, trillion-synapse models, with tens of trillions of neural spike-event packets conveyed each second. The communication networks and overlying protocols are key to the successful operation of the SpiNNaker architecture, designed together to maximise performance and minimise the power demands of the platform. SpiNNaker is a work in progress, having recently reached a major milestone with the delivery of the first MPSoCs. This paper presents the architectural justification, which is now supported by preliminary measured results of silicon performance, indicating that it is indeed scalable to a million-plus processor system. © 2012 Elsevier Inc. All rights reserved.","GALS; HPC; Low-power; Network-on-Chip; Neuromorphic; Parallel architecture"
"Probabilistic resource allocation in heterogeneous distributed systems with random failures","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865065308&doi=10.1016%2fj.jpdc.2012.03.003&partnerID=40&md5=29ebcd05c8815bcc02884733d3e8208f","The problem of finding efficient workload distribution techniques is becoming increasingly important today for heterogeneous distributed systems where the availability of compute nodes may change spontaneously over time. Resource-allocation policies designed for such systems should maximize the performance and, at the same time, be robust against failure and recovery of compute nodes. Such a policy, based on the concepts of the Derman-Lieberman-Ross theorem, is proposed in this work, and is applied to a simulated model of a dedicated system composed of a set of heterogeneous image processing servers. Assuming that each image results in a ""reward"" if its processing is completed before a certain deadline, the goal for the resource allocation policy is to maximize the expected cumulative reward. An extensive analysis was done to study the performance of the proposed policy and compare it with the performance of some existing policies adapted to this environment. Our experiments conducted for various types of task-machine heterogeneity illustrate the potential of our method for solving resource allocation problems in a broad spectrum of distributed systems that experience high failure rates. © 2012 Elsevier Inc. All rights reserved.","Distributed systems; Fault tolerant systems; Load balancing; Resource allocation"
"Performance analysis of an adaptive, energy-efficient MAC protocol for wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857794373&doi=10.1016%2fj.jpdc.2012.01.021&partnerID=40&md5=cbaa29a4290443fa0c9b360736997048","We propose an adaptive and energy-efficient TDMA-based MAC protocol that significantly reduces energy consumption in the network, while efficiently handling network traffic load variations and optimizing channel utilization through a timeslot stealing mechanism and a timeslot reassignment procedure. We have analytically derived the average delay performance of our MAC protocol, with and without the timeslot stealing mechanism. Our delay model, validated via simulations, shows that the timeslot stealing mechanism can substantially improve the protocol throughput in scenarios with varying and asymmetric traffic patterns. Evaluation results show that the timeslot reassignment procedure is efficient in handling the longer timescale changes in the traffic load, while the timeslot stealing mechanism is better in handling the shorter timescale changes in the traffic patterns.","MAC protocol; TDMA; Timeslot stealing; Wireless sensor networks"
"Optimal energy allocation in heterogeneous wireless sensor networks for lifetime maximization","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861113199&doi=10.1016%2fj.jpdc.2012.03.001&partnerID=40&md5=835ad9fa5dd0333002080cd157a79dd7","We consider the problem of optimal energy allocation and lifetime maximization in heterogeneous wireless sensor networks. We construct a probabilistic model for heterogeneous wireless sensor networks where sensors can have different sensing range, different transmission range, different energy consumption for data sensing, and different energy consumption for data transmission, and the stream of data sensed and transmitted from a sensor and the stream of data relayed by a sensor to a base station are all treated as Poisson streams. We derive the probability distribution and the expectation of the number of data transmissions during the lifetime of each sensor and the probability distribution and the expectation of the lifetime of each sensor. In all these analysis, energy consumption of data sensing and data transmission and data relay are all taken into consideration. We develop an algorithm to find an optimal initial energy allocation to the sensors such that the network lifetime in the sense of the identical expected sensor lifetime is maximized. We show how to deal with a large amount of energy budget that may cause excessive computational time by developing accurate closed form approximate expressions of sensor lifetime and network lifetime and optimal initial energy allocation. We derive the expected number of working sensors at any time. Based on such results, we can find the latest time such that the expected number of sensors that are still functioning up to that time is above certain threshold. © 2012 Elsevier Inc. All rights reserved.","Data sensing; Data transmission; Energy consumption; Heterogeneous wireless sensor network; Lifetime maximization; Optimal energy allocation"
"Communication and energy efficient routing protocols for single-hop radio networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859905773&doi=10.1016%2fj.jpdc.2012.03.004&partnerID=40&md5=ad808d6075883dd4eff3524502978a3e","In this paper, we study the important problems of message routing, sorting, and selection in a radio network. A radio network consists of stations where each station is a hand-held device. We consider a single-hop radio network where it is assumed that each station is within the transmission range of every other station. Let RN(p,k) stand for a single-hop network that has p stations and k communication channels. The best known prior algorithm for sorting takes 4nk+o(nk) broadcast rounds on a RN(p,k). In this paper, we present a randomized algorithm that takes only 3nk+o(nk) broadcast rounds with high probability. For the selection problem, we present a randomized selection algorithm that takes O(pk) rounds on a RN(p,k) with high probability. The best known prior algorithms for the np-relations routing problem take nearly 2nk time slots (i.e., broadcast rounds). An important open question has been if there exist algorithms that take only close to nk time slots. Note that a trivial lower bound for routing is nk. The existence of such algorithms will be highly relevant, especially in emergencies and time-critical situations. In this paper, we answer this question by presenting a randomized algorithm that takes nearly nk rounds on the average. We also present a deterministic algorithm that takes nearly nk rounds. These routing algorithms are also shown to be energy efficient. © 2012 Elsevier Inc. All rights reserved.","Message routing; Radio networks; Randomized algorithms; Searching; Selection; Sorting"
"A flexible layered control policy for resource allocation in a sensor grid","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862017061&doi=10.1016%2fj.jpdc.2012.04.002&partnerID=40&md5=bffc4a6e497de828951a4da3f867d065","The paper proposes a flexible layered control policy for sensor resource allocation in a sensor grid. In order to allocate sensor resources in the system to maximize the sensor grid utility, different controllers are deployed at three levels: a job-level controller, an application group controller, and a sensor grid system controller. At the lowest levels, job-level controllers perform fast, frequent, local adaptation for optimizing a single sensor grid application at a time, while, at the highest levels, sensor grid system controllers perform less frequent control actions to optimize all applications. Sensor grid system control considers all sensor grid applications in response to large system changes at coarse time granularity. Sensor grid system control exploits the interlayer coupling of the resource layer and the application layer to achieve a system-wide optimization based on the sensor grid users' preferences. Job-level control adapts a single application to small changes at fine granularity. The layered control system uses a set of utility functions to evaluate the performance of sensor grid applications and groups. The control system chooses control actions that would result in a higher level of utility. In the simulation, a performance evaluation of the algorithm is carried out. © 2012 Elsevier Inc. All rights reserved.","Layered control; Optimization; Sensor grid"
"Accelerated parallel genetic programming tree evaluation with OpenCL","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869492090&doi=10.1016%2fj.jpdc.2012.01.012&partnerID=40&md5=578a1cf609a0ffa9e9becfe8ab3ca183","Inspired by the process of natural selection, genetic programming (GP) aims at automatically building arbitrarily complex computer programs. Being classified as an ""embarrassingly"" parallel technique, GP can theoretically scale up to tackle very diverse problems by increasingly adding computational power to its arsenal. With today's availability of many powerful parallel architectures, a challenge is to take advantage of all those heterogeneous compute devices in a portable and uniform way. This work proposes both (i) a transcription of existing GP parallelization strategies into the OpenCL programming platform; and (ii) a freely available implementation to evaluate its suitability for GP, by assessing the performance of parallel strategies on the CPU and GPU processors from different vendors. Benchmarks on the symbolic regression and data classification domains were performed. On the GPU we could achieve 13 billion node evaluations per second, delivering almost 10 times the throughput of a twelve-core CPU. © 2012 Elsevier Inc. All rights reserved.","Accelerated tree evaluation; GP-GPU; OpenCL; Parallel genetic programming"
"Efficient real-time divisible load scheduling","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867828228&doi=10.1016%2fj.jpdc.2012.09.003&partnerID=40&md5=537a00fe3b72baa16a6e932e2d93c185","Providing QoS and performance guarantees to arbitrarily divisible loads has become a significant problem for many cluster-based research computing facilities. While progress is being made in scheduling arbitrarily divisible loads, current approaches are not efficient and do not scale well. In this paper, we propose a linear algorithm for real-time divisible load scheduling. Unlike existing approaches, the new algorithm relaxes the tight coupling between the task admission controller and the task dispatcher. By eliminating the need to generate exact schedules in the admission controller, the algorithm avoids high overheads. We also proposed a hybrid algorithm that combines the best of our efficient algorithm and a previously best-known approach. We experimentally evaluate the new algorithm. Simulation results demonstrate that the algorithm scales well, can schedule large numbers of tasks efficiently, and performs similarly to existing approaches in terms of providing real-time guarantees. © 2012 Elsevier Inc. All rights reserved.","Arbitrarily divisible loads; Cluster computing; Real-time computing; Scheduling efficiency"
"Coordination of multi-link spectrum handoff in multi-radio multi-hop cognitive networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862794521&doi=10.1016%2fj.jpdc.2011.11.004&partnerID=40&md5=cb23e4ef178167478cdab0a241d69889","In cognitive networks, spectrum handoff occurs when primary users reclaim their rights to access their licensed spectrum. When links perform spectrum handoff, their communication may be interrupted for a certain period and incurs spectrum handoff delay. Existing work only considered the problem of minimizing spectrum handoff delay of a single link in single-hop cognitive networks, referred to as the SH-SLSH problem. This paper studies a more challenging problem (referred to as the SH-MLMH problem) in which multiple links perform spectrum handoff in multi-hop cognitive networks. Assuming each node is equipped with multiple radios and multi-path routing is adopted, the SH-MLMH problem targets at maintaining the network connectivity and minimizing the Total Handoff Completion Time (THCT) by coordinating multiple links to perform spectrum handoff. THCT is defined as the time for all links to finish spectrum handoff. We can keep the communication of switching links uninterrupted by maintaining network connectivity and adopting multi-path routing. To the best of our knowledge, we are the first to study the SH-MLMH problem. We make following contributions in this paper. We prove that the SH-MLMH problem is NP-hard. We propose both centralized and distributed algorithm to solve the SH-MLMH problem. We prove that the centralized algorithm can achieve a logarithmic approximation ratio. The simulation results show that our proposed algorithms not only improve the network throughput but also reduce THCT compared with spectrum handoff without coordination. © 2012 Elsevier Inc. All rights reserved.","Cognitive radio; Multi-hop; Multi-link; Spectrum handoff"
"Online optimization for scheduling preemptable tasks on IaaS cloud systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862806683&doi=10.1016%2fj.jpdc.2012.02.002&partnerID=40&md5=45fab890602b0641850d7bc0f8bf86fb","In Infrastructure-as-a-Service (IaaS) cloud computing, computational resources are provided to remote users in the form of leases. For a cloud user, he/she can request multiple cloud services simultaneously. In this case, parallel processing in the cloud system can improve the performance. When applying parallel processing in cloud computing, it is necessary to implement a mechanism to allocate resource and schedule the execution order of tasks. Furthermore, a resource optimization mechanism with preemptable task execution can increase the utilization of clouds. In this paper, we propose two online dynamic resource allocation algorithms for the IaaS cloud system with preemptable tasks. Our algorithms adjust the resource allocation dynamically based on the updated information of the actual task executions. And the experimental results show that our algorithms can significantly improve the performance in the situation where resource contention is fierce. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Feedback; Online scheduling; Preemptable scheduling"
"Tuple switching network - When slower may be better","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866070787&doi=10.1016%2fj.jpdc.2012.01.014&partnerID=40&md5=deafee51ebd943634cc8a48c2b04cf4e","This paper reports an application dependent network design for extreme scale high performance computing (HPC) applications. Traditional scalable network designs focus on fast point-to-point transmission of generic data packets. The proposed network focuses on the sustainability of high performance computing applications by statistical multiplexing of semantic data objects. For HPC applications using data-driven parallel processing, a tuple is a semantic object. We report the design and implementation of a tuple switching network for data parallel HPC applications in order to gain performance and reliability at the same time when adding computing and communication resources. We describe a sustainability model and a simple computational experiment to demonstrate extreme scale application's sustainability with decreasing system mean time between failures (MTBF). Assuming three times slowdown of statistical multiplexing and 35% time loss per checkpoint, a two-tier tuple switching framework would produce sustained performance and energy savings for extreme scale HPC application using more than 1024 processors or less than 6 hour MTBF. Higher processor counts or higher checkpoint overheads accelerate the benefits. © 2012 Elsevier Inc. All rights reserved.","Application dependent networking; Sustainable high performance computing"
"Ramos: Concurrent writing and reconfiguration for collaborative systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859155600&doi=10.1016%2fj.jpdc.2012.02.012&partnerID=40&md5=4061c69b3313b400bb1318ec67c63d50","Collaborative systems, with specific distributed systems allow multiple participants to work in a common virtual space, while reproducing the different ways to interact in a group. Such systems have to manage not only the sharing of context and particularly the context consistency, but also at the same time the fault tolerance. No system in the literature combines these two requirements. In this paper, we are proposing the new protocol Ramos which implements a fault-tolerant, and a context consistency (ensuring a total order of write operations) based on an asynchronous message-passing model. Communication takes place via gossip messages, which are sent at any frequency between a dynamic set of nodes. Ramos is based on the Rambo III algorithm for replicated data services. Rambo III provides two functions: reconfiguration of a dynamic set of nodes and reading/writing of a replicated object. In Ramos the reconfiguration process from Rambo III is adapted to the needs of collaborative systems and Paxos is used to execute concurrent write operations. It is assumed that from a total set of 2f+1 nodes, at most a subset of f nodes is allowed to fail simultaneously. Furthermore, it is assumed that the application using Ramos provides a leader-election method. Ramos, the algorithm proposed here, provides one significant feature: all write operations are totally ordered. © 2012 Elsevier Inc. All rights reserved.","Distributed computing theory; Fault-tolerance; Quorum system"
"Enhancing data parallelism for ant colony optimization on GPUs","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869488080&doi=10.1016%2fj.jpdc.2012.01.002&partnerID=40&md5=7691fa7cf7e983b8bfdca5df593d2bce","Graphics Processing Units (GPUs) have evolved into highly parallel and fully programmable architecture over the past five years, and the advent of CUDA has facilitated their application to many real-world applications. In this paper, we deal with a GPU implementation of Ant Colony Optimization (ACO), a population-based optimization method which comprises two major stages: tour construction and pheromone update. Because of its inherently parallel nature, ACO is well-suited to GPU implementation, but it also poses significant challenges due to irregular memory access patterns. Our contribution within this context is threefold: (1) a data parallelism scheme for tour construction tailored to GPUs, (2) novel GPU programming strategies for the pheromone update stage, and (3) a new mechanism called I-Roulette to replicate the classic roulette wheel while improving GPU parallelism. Our implementation leads to factor gains exceeding 20x for any of the two stages of the ACO algorithm as applied to the TSP when compared to its sequential counterpart version running on a similar single-threaded high-end CPU. Moreover, an extensive discussion focused on different implementation paths on GPUs shows the way to deal with parallel graph connected components. This, in turn, suggests a broader area of inquiry, where algorithm designers may learn to adapt similar optimization methods to GPU architecture. © 2012 Elsevier Inc. All rights reserved.","Ant Colony Optimization; GPU programming; Metaheuristics; Performance analysis; TSP"
"A dynamic and adaptive load balancing strategy for parallel file system with large-scale I/O servers","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865066342&doi=10.1016%2fj.jpdc.2012.05.006&partnerID=40&md5=af64ce9a676a3150021a8f90fc9c66c8","Many solutions have been proposed to tackle the load imbalance issue of parallel file systems. However, all these solutions either adopt centralized algorithms, or lack considerations for both the network transmission and the tradeoff between benefits and side-effects of each dynamic file migration. Therefore, existing solutions will be prohibitively inefficient in large-scale parallel file systems. To address this problem, this paper presents SALB, a dynamic and adaptive load balancing algorithm which is totally based on a distributed architecture. To be also aware of the network transmission, SALB on the one hand adopts an adaptively adjusted load collection threshold in order to reduce the message exchanges for load collection, and on the other hand it employs an on-line load prediction model with a view to reducing the decision delay caused by the network transmission latency. Moreover, SALB employs an optimization model for selecting the migration candidates so as to balance the benefits and the side-effects of each dynamic file migration. Extensive experiments are conducted to prove the effectiveness of SALB. The results show that SALB achieves an optimal performance not only on the mean response time but also on the resource utilization among the schemes for comparison. The simulation results also indicate that SALB is able to deliver high scalability. © 2012 Elsevier Inc. All rights reserved.","Adaptive algorithm; Distributed load balancing; Dynamic file migration; Load collection; On-line load prediction; Parallel file systems"
"Adaptive energy-efficient scheduling for real-time tasks on DVS-enabled heterogeneous clusters","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862830127&doi=10.1016%2fj.jpdc.2012.03.005&partnerID=40&md5=f0c85c2e3afcbc46994a2d4f677e48f3","Developing energy-efficient clusters not only can reduce power electricity cost but also can improve system reliability. Existing scheduling strategies developed for energy-efficient clusters conserve energy at the cost of performance. The performance problem becomes especially apparent when cluster computing systems are heavily loaded. To address this issue, we propose in this paper a novel scheduling strategy-adaptive energy-efficient scheduling or AEES-for aperiodic and independent real-time tasks on heterogeneous clusters with dynamic voltage scaling. The AEES scheme aims to adaptively adjust voltages according to the workload conditions of a cluster, thereby making the best trade-offs between energy conservation and schedulability. When the cluster is heavily loaded, AEES considers voltage levels of both new tasks and running tasks to meet tasks' deadlines. Under light load, AEES aggressively reduces the voltage levels to conserve energy while maintaining higher guarantee ratios. We conducted extensive experiments to compare AEES with an existing algorithm-MEG, as well as two baseline algorithms-MELV, MEHV. Experimental results show that AEES significantly improves the scheduling quality of MELV, MEHV and MEG. © 2012 Elsevier Inc. All rights reserved.","Adaptivity; Cluster; Dynamic voltage scaling; Energy-efficient; Real-time; Scheduling"
"Energy-efficient deadline scheduling for heterogeneous systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867889096&doi=10.1016%2fj.jpdc.2012.07.006&partnerID=40&md5=48064858939424ca179a71a4b8db606b","Energy efficiency is a major concern in modern high performance computing (HPC) systems and a power-aware scheduling approach is a promising way to achieve that. While there are a number of studies in power-aware scheduling by means of dynamic power management (DPM) and/or dynamic voltage and frequency scaling (DVFS) techniques, most of them only consider scheduling at a steady state. However, HPC applications like scientific visualization often need deadline constraints to guarantee timely completion. In this paper we present power-aware scheduling algorithms with deadline constraints for heterogeneous systems. We formulate the problem by extending the traditional multiprocessor scheduling and design approximation algorithms with analysis on the worst-case performance. We also present a pricing scheme for tasks in the way that the price of a task varies as its energy usage as well as largely depending on the tightness of its deadline. Last we extend the proposed algorithm to the control dependence graph and the online case which is more realistic. Through the extensive experiments, we demonstrate that the proposed algorithm achieves near-optimal energy efficiency, on average 16.4% better for synthetic workload and 12.9% better for realistic workload than the EDD (Earliest Due Date)-based algorithm; The extended online algorithm also outperforms the EDF (Earliest Deadline First)-based algorithm with an average up to 26% of energy saving and 22% of deadline satisfaction. It is experimentally shown as well that the pricing scheme provides a flexible trade-off between deadline tightness and price. © 2012 Elsevier Inc. All rights reserved.","Deadline scheduling; Energy efficiency; High performance computing; Pricing scheme"
"The importance of considering unauthentic transactions in trust management systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859894948&doi=10.1016%2fj.jpdc.2012.03.006&partnerID=40&md5=0497340cdde1c7629b093c251f06bcb7","In peer-to-peer (P2P) networks, trust management is a key tool to minimize the impact of malicious nodes. EigenTrust is claimed to be one of the most powerful distributed reputation management systems focused on P2P file-sharing applications. It is the theoretical base of other systems, and it has also been directly modified in an attempt to improve its performance. However, none of them give appropriate importance to all the information about transactions. This paper proposes an enhancement of EigenTrust, which considers unsatisfactory transactions in greater depth. Pos&Neg EigenTrust is able to obtain a blacklist of the identities of the malicious nodes. Therefore, it is able to significantly reduce the number of unsatisfactory transactions in the network. © 2012 Elsevier Inc. All rights reserved.","File sharing; P2P systems; Trust management"
"Optimal wake-up scheduling of data gathering trees for wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862781149&doi=10.1016%2fj.jpdc.2012.01.008&partnerID=40&md5=eb26a9cc6194da40c51ef7cc8ea1a780","In order to gather sensor data, a data gathering tree is commonly created as a subnetwork of a wireless sensor network. Power conservation is of paramount importance in such networks, and using periodic sleepwake cycles for sensor nodes is one of the most effective methods for power conservation. This paper addresses the problem of scheduling the sleepwake cycles of nodes in a data gathering tree under deadline constraints. After formally modeling the problem being addressed, an optimal wake-up frequency assignment (OWFA) algorithm, which takes into account the data rate at each node and the total permitted delay, is proposed. The results of simulations under various conditions showed that OWFA consumed about 8.6%∼24.3% less average power, and thus resulted in a 7.4%∼26.0% longer network lifetime, than a previously proposed method that did not consider individual data rates. © 2012 Elsevier Inc. All rights reserved.","Data gathering tree; Environmental monitoring; Wake-up scheduling; Wireless sensor network"
"Packet scheduling with joint design of MIMO and network coding","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862782996&doi=10.1016%2fj.jpdc.2011.12.001&partnerID=40&md5=3329e99a5c1c7faebeb907e8f22f2719","In this paper, we propose a joint design of MIMO technique and network coding (MIMO-NC) and apply it to improve the performance of wireless networks. We consider a system in which the packet exchange among multiple wireless users is forwarded by a relay node. In order to enjoy the benefit of MIMO-NC, all the nodes in the network are mounted with two antennas and the relay node possess the coding capability. For the cross traffic flows among any four users, the relay node not only can receive packets simultaneously from two compatible users in the uplink (users-to-relay node), but also can mix up distinct packets for four destined users into two coded packets and concurrently send them out in the same downlink (relay node-to-users), so that the information content is significantly increased in each transmission. We formalize the problem of finding a schedule to forward the buffered data of all the users in minimum number of transmissions in such a system as a problem of finding a maximum matching in a graph. We also provide an analytical model on maximum throughput and optimal energy efficiency, which explicitly measures the performance gain of the MIMO-NC enhancement. Our analytical and simulation results demonstrate that system performance can be greatly improved by the efficient utilization of MIMO and network coding opportunities. © 2011 Elsevier Inc. All rights reserved.","Compatibility; Multiple-input multiple output; Network coding; Scheduling"
"QoS-aware dynamic MAP selection schemes in HMIPv6 networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861186469&doi=10.1016%2fj.jpdc.2012.03.008&partnerID=40&md5=f2151d90ae209976677bb11e39d1fbba","We consider a dynamic Mobile Anchor Point (MAP) selection problem when there are both real-time and non-real time sessions in a Hierarchical Mobile IPv6 (HMIPv6) network. We propose schemes in which Mobile Nodes (MNs) holding real-time sessions register with the root MAP in a hierarchy of MAPs to reduce the inter-domain handovers while those with non-real time sessions select one either to balance the load or to reduce handover frequencies. Both the simulation and analytical results show the effectiveness of the proposed schemes with respect to the number of inter-domain handovers, to the average signaling cost, and to the load distribution. In addition, we could also confirm that our MAP selection schemes provide better QoS to the MNs holding real-time sessions, in that they reduce the inter-domain handovers for those MNs and the average handover delay, resulting in a shorter service disruption. © 2012 Elsevier Inc. All rights reserved.","Delay-sensitive session; Dynamic MAP selection; Handover delay; HMIPv6; Inter-domain handover; Load distribution; Mobility; Signaling cost"
"End-to-end maxmin fairness in multihop wireless networks: Theory and protocol","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856354025&doi=10.1016%2fj.jpdc.2011.11.012&partnerID=40&md5=5136d26a6cf6e71af58841a85888b4aa","To promote commercial deployment of multihop wireless networks, the research/industry communities must develop new theories and protocols for flexible traffic engineering in these networks in order to support diverse user applications. This paper studies an important traffic engineering problemhow to support fair bandwidth allocation among all end-to-end flows in a multihop wireless networkwhich, in a more precise term, is to achieve the global maxmin fairness objective in bandwidth allocation. There exists no distributed algorithm for this problem in multihop wireless networks using IEEE 802.11 DCF. We have two major contributions. The first contribution is to develop a novel theory that maps the global maxmin objective to four local conditions and prove their equivalence. The second contribution is to design a distributed rate adjustment protocol based on those local conditions to achieve the global maxmin objective through fully distributed operations. Comparing with the prior art, our protocol has a number of advantages. It is designed for the popular IEEE 802.11 DCF. It replaces per-flow queueing with per-destination queueing. It achieves far better fairness (or weighted fairness) among end-to-end flows. © 2011 Elsevier Inc. All rights reserved.","Distributed rate adjustment protocol; Maxmin fairness; Multihop wireless networks"
"From the happened-before relation to the causal ordered set abstraction","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859910515&doi=10.1016%2fj.jpdc.2012.02.015&partnerID=40&md5=39e27c33bcb0fcc4d9f4c64e979a3165","Several works in distributed systems have been designed based on the Happened-Before Relation (HBR). Most of these works intend to be efficient in their implementation by identifying and ensuring dependency constraints among single events. Even when the minimal causal dependencies among events have been clearly identified, the evolution of systems, which may involve a high number of processes and a high volume of transmitted data, calls for the need to design even more efficient approaches. This paper proposes the Causal Ordered Set Abstraction (CAOS) where the causally related events are arranged in sets that are strictly causally ordered. As for single events, CAOS establishes that any pair of resultant sets can be, and can only be, causally or concurrently related. We claim that our ordered set abstraction can be used to design more efficient algorithms based on the HBR principle. This assertion is based on two main properties. First, CAOS attains a consistent compact representation of a distributed computation. Second, as a consequence of the causal ordering of the events in the resultant sets, it is sufficient to verify only a pair of single events, one per each set, in order to determine whether these sets are causally or concurrently related, regardless of the cardinality of the sets. © 2012 Elsevier Inc. All rights reserved.","Distributed systems; Event ordering; Happened-Before Relation; Ordered sets"
"Globally Synchronized Frames for guaranteed quality-of-service in on-chip networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866062625&doi=10.1016%2fj.jpdc.2012.01.013&partnerID=40&md5=43ed426f9625a89dc184396e869c92ec","Future chip multiprocessors (CMPs) may have hundreds to thousands of threads competing to access shared resources, and will require quality-of-service (QoS) support to improve system utilization. This paper introduces Globally-Synchronized Frames (GSF), a framework for providing guaranteed QoS in on-chip networks in terms of minimum bandwidth and maximum delay bound. The GSF framework can be easily integrated in a conventional virtual channel (VC) router without significantly increasing the hardware complexity. We exploit a fast on-chip barrier network to efficiently implement GSF. Performance guarantees are verified by analysis and simulation. According to our simulations, all concurrent flows receive their guaranteed minimum share of bandwidth in compliance with a given bandwidth allocation. The average throughput degradation of GSF on an 8×8 mesh network is within 10% compared to the conventional best-effort VC router. © 2012 Elsevier Inc. All rights reserved.","Chip multiprocessors (CMP); On-chip networks; QoS; Router"
"Compiler-assisted energy optimization for clustered VLIW processors","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862018897&doi=10.1016%2fj.jpdc.2012.04.005&partnerID=40&md5=f76ec79b908dcddc4d3f3ec22f45039a","Clustered architecture processors are preferred for embedded systems because centralized register file architectures scale poorly in terms of clock rate, chip area, and power consumption. Although clustering helps by improving the clock speed, reducing the energy consumption of the logic, and making the design simpler, it introduces extra overheads by way of inter-cluster communication. This communication happens over long global wires having high load capacitance which leads to delay in execution and significantly high energy consumption. Inter-cluster communication also introduces many short idle cycles, thereby significantly increasing the overall leakage energy consumption in the functional units. The trend towards miniaturization of devices (and associated reduction in threshold voltage) makes energy consumption in interconnects and functional units even worse, and limits the usability of clustered architectures in smaller technologies. However, technological advancements now permit the design of interconnects and functional units with varying performance and power modes. In this paper, we propose scheduling algorithms that aggregate the scheduling slack of instructions and communication slack of data values to exploit the low-power modes of functional units and interconnects. Finally, we present a synergistic combination of these algorithms that simultaneously saves energy in functional units and interconnects to improves the usability of clustered architectures by achieving better overall energy-performance trade-offs. Even with conservative estimates of the contribution of the functional units and interconnects to the overall processor energy consumption, the proposed combined scheme obtains on average 8% and 10% improvement in overall energy-delay product with 3.5% and 2% performance degradation for a 2-clustered and a 4-clustered machine, respectively. We present a detailed experimental evaluation of the proposed schemes. Our test bed uses the Trimaran compiler infrastructure. © 2012 Elsevier Inc. All rights reserved.","Clustered VLIW processors; Energy-aware scheduling; Scheduling"
"Load-balancing spatially located computations using rectangular partitions","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865078396&doi=10.1016%2fj.jpdc.2012.05.013&partnerID=40&md5=ecf1b0272859dcf8cb32ef409d73a9d7","Distributing spatially located heterogeneous workloads is an important problem in parallel scientific computing. We investigate the problem of partitioning such workloads (represented as a matrix of non-negative integers) into rectangles, such that the load of the most loaded rectangle (processor) is minimized. Since finding the optimal arbitrary rectangle-based partition is an NP-hard problem, we investigate particular classes of solutions: rectilinear, jagged and hierarchical. We present a new class of solutions called m-way jagged partitions, propose new optimal algorithms for m-way jagged partitions and hierarchical partitions, propose new heuristic algorithms, and provide worst case performance analyses for some existing and new heuristics. Moreover, the algorithms are tested in simulation on a wide set of instances. Results show that two of the algorithms we introduce lead to a much better load balance than the state-of-the-art algorithms. We also show how to design a two-phase algorithm that reaches different time/quality tradeoffs. © 2012 Elsevier Inc. All rights reserved.","Dynamic programming; Heuristics; Hierarchical partitioning; Jagged partitioning; Load balancing; Mesh-based computation; Optimal algorithms; Particle-in-cell; Rectilinear partitioning; Spatial partitioning"
"GPGPU implementation of growing neural gas: Application to 3D scene reconstruction","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865074591&doi=10.1016%2fj.jpdc.2012.05.008&partnerID=40&md5=38ed184c31325667ea0faabe81c59b49","Self-organising neural models have the ability to provide a good representation of the input space. In particular the Growing Neural Gas (GNG) is a suitable model because of its flexibility, rapid adaptation and excellent quality of representation. However, this type of learning is time-consuming, especially for high-dimensional input data. Since real applications often work under time constraints, it is necessary to adapt the learning process in order to complete it in a predefined time. This paper proposes a Graphics Processing Unit (GPU) parallel implementation of the GNG with Compute Unified Device Architecture (CUDA). In contrast to existing algorithms, the proposed GPU implementation allows the acceleration of the learning process keeping a good quality of representation. Comparative experiments using iterative, parallel and hybrid implementations are carried out to demonstrate the effectiveness of CUDA implementation. The results show that GNG learning with the proposed implementation achieves a speed-up of 6× compared with the single-threaded CPU implementation. GPU implementation has also been applied to a real application with time constraints: acceleration of 3D scene reconstruction for egomotion, in order to validate the proposal. © 2012 Elsevier Inc. All rights reserved.","3D reconstruction; CUDA; Egomotion; GPU; Growing neural gas; Multicore; Parallel computing"
"Analyzing multi-hop routing feasibility for sensor data harvesting using mobile sinks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859915521&doi=10.1016%2fj.jpdc.2012.02.017&partnerID=40&md5=83195154bd9d1f4ea8f147784c5ce97f","This paper presents a modeling framework for characterizing the feasibility and impacts of multi-hop packet routing in sensor networks with mobile sinks. Data collection in sensor networks using mobile sinks has recently been investigated to improve energy performance at the cost of collection delay. Although the data collection can be accomplished with varying degrees of multi-hop routing, for a given data generation rate, as the extent of multi-hop routing increases, the round traversal time of the sink decreases. At the same time, the interference experienced by the mobile sink-to-sensor links and the consequent upload time go up. This paper characterizes these competing effects and develops a methodology for determining the extent of multi-hop routing that is feasible for given network and application parameters such as sensor data generation rate, wireless link capacity between sensors and mobile sink, the speed of the mobile sink and node density. © 2012 Elsevier Inc. All rights reserved.","Mobile sinks; Multi-hop hop routing; Network Assisted Navigation; Network life; Sensor networks"
"Low-contention data structures","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859157954&doi=10.1016%2fj.jpdc.2011.10.018&partnerID=40&md5=6152f8b53261974ef0b05d0daff32c37","We consider the problem of minimizing contention in static (read-only) dictionary data structures, where contention is measured with respect to a fixed query distribution by the maximum expected number of probes to any given cell. The query distribution is known by the algorithm that constructs the data structure but not by the algorithm that queries it. Assume that the dictionary has n items. When all queries in the dictionary are equiprobable, and all queries not in the dictionary are equiprobable, we show how to construct a data structure in O(n) space where queries require O(1) probes and the contention is O(1n). Asymptotically, all of these quantities are optimal. For arbitrary query distributions, we construct a data structure in O(n) space where each query requires O(lognloglogn) probes and the contention is O(logn(nloglogn)). The lack of knowledge of the query distribution by the query algorithm prevents perfect load leveling in this case: for a large class of algorithms, we present a lower bound, based on VC-dimension, that shows that for a wide range of data structure problems, achieving contention even within a polylogarithmic factor of optimal requires a cell-probe complexity of Ω(loglogn). © 2012 Elsevier Inc. All rights reserved.","Cell-probe model; Data structure; Memory contention"
"NAND flash memory-based hybrid file system for high I/O performance","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867855832&doi=10.1016%2fj.jpdc.2012.08.002&partnerID=40&md5=afccff05aaa38452fb404b54c2fbc5d8","As the performance potentials of SSD (Solid State Device) have been recognized, adopting SSD to IT products as HDD replacements is rapidly increasing. Since SSD is organized into multiple flash memory packages, it deploys peculiar device characteristics that do not occur in HDD, such as block-unit erasure overhead. Also, its high cost per capacity is the main obstacle to building a large-scale storage subsystem with only SSDs. An alternative is to build a hybrid storage subsystem where a small portion of SSDs are integrated with HDDs so as to utilize SSD's performance advantages in a cost-effective way. This study introduces a new form of file system, called N-hybrid (New-Form of hybrid file system), that enables us to support the hybrid device structure combined with both HDD and SSD. Our primary objectives in developing N-hybrid are to provide better I/O bandwidth by exploiting the characteristics of HDD and SSD and to provide a flexible data layout maximizing the usage of tight SSD storage resources. Several experiments were conducted to verify the effectiveness and suitability of N-hybrid. © 2012 Elsevier Inc. All rights reserved.","Data mapping; Device integration; Extent replacement; Extent size; Flexible data layout; Hybrid file system; In-core extent table; Multiple data sections; NAND flash-memory SSD; Write-through cache"
"A cost-effective cloud computing framework for accelerating multimedia communication simulations","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865094736&doi=10.1016%2fj.jpdc.2012.06.005&partnerID=40&md5=f28eefbe79ae5398ebd3ce9a55bd7284","Multimedia communication research and development often requires computationally intensive simulations in order to develop and investigate the performance of new optimization algorithms. Depending on the simulations, they may require even a few days to test an adequate set of conditions due to the complexity of the algorithms. The traditional approach to speed up this type of relatively small simulations, which require several develop-simulate-reconfigure cycles, is indeed to run them in parallel on a few computers and leaving them idle when developing the technique for the next simulation cycle. This work proposes a new cost-effective framework based on cloud computing for accelerating the development process, in which resources are obtained on demand and paid only for their actual usage. Issues are addressed both analytically and practically running actual test cases, i.e., simulations of video communications on a packet lossy network, using a commercial cloud computing service. A software framework has also been developed to simplify the management of the virtual machines in the cloud. Results show that it is economically convenient to use the considered cloud computing service, especially in terms of reduced development time and costs, with respect to a solution using dedicated computers, when the development time is longer than one hour. If more development time is needed between simulations, the economic advantage progressively reduces as the computational complexity of the simulation increases. © 2012 Elsevier Inc. All rights reserved.","Amazon EC2; Cloud computing; Cloud cost comparison; Multimedia simulations; Video communication"
"Mobility-assisted minimum connected cover in a wireless sensor network","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861186695&doi=10.1016%2fj.jpdc.2012.03.009&partnerID=40&md5=bd21c4f64bf898a860d05dea8744328d","All properties of mobile wireless sensor networks (MWSNs) are inherited from static wireless sensor networks (WSNs) and meanwhile have their own uniqueness and node mobility. Sensor nodes in these networks monitor different regions of an area of interest and collectively present a global overview of monitored activities. Since failure of a sensor node leads to loss of connectivity, it may cause a partitioning of the network. Adding mobility to WSNs can significantly increase the capability of the WSN by making it resilient to failures, reactive to events, and able to support disparate missions with a common set of sensor nodes. In this paper, we propose a new algorithm based on the divide-and-conquer approach, in which the whole region is divided into sub-regions and in each sub-region the minimum connected sensor cover set is selected through energy-aware selection method. Also, we propose a new technique for mobility assisted minimum connected sensor cover considering the network energy. We provide performance metrics to analyze the performance of our approach and the simulation results clearly indicate the benefits of our new approach in terms of energy consumption, communication complexity, and number of active nodes over existing algorithms. © 2012 Elsevier Inc. All rights reserved.","Energy-aware selection method; Minimum connected sensor cover set; Mobile wireless sensor networks; Redundant node; Relocation"
"Profit-driven scheduling for cloud services with data access awareness","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862817479&doi=10.1016%2fj.jpdc.2011.12.002&partnerID=40&md5=33e9b6c7203d697c2196b72778257375","Resource sharing between multiple tenants is a key rationale behind the cost effectiveness in the cloud. While this resource sharing greatly helps service providers improve resource utilization and increase profit, it impacts on the service quality (e.g., the performance of consumer applications). In this paper, we address the reconciliation of these conflicting objectives by scheduling service requests with the dynamic creation of service instances. Specifically, our scheduling algorithms attempt to maximize profit within the satisfactory level of service quality specified by the service consumer. Our contributions include (1) the development of a pricing model using processor-sharing for clouds (i.e., queuing delay is embedded in processing time), (2) the application of this pricing model to composite services with dependency consideration, (3) the development of two sets of service request scheduling algorithms, and (4) the development of a prioritization policy for data service aiming to maximize the profit of data service. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Data service; Market-based resource allocation; Mashup services; Processor-sharing; Scheduling"
"Efficient transformation of distance-2 self-stabilizing algorithms","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857791479&doi=10.1016%2fj.jpdc.2011.12.008&partnerID=40&md5=b0cb832669abb7b487bae1fb0479e783","Self-stabilizing algorithms for optimization problems can often be solved more easily using the distance-two model in which each vertex can instantly see the state information of all vertices up to distance two. This paper presents a new technique to emulate algorithms for the distance-two model on the distance-one model using the distributed scheduler with a slowdown factor of O(m) moves. Up until now the best transformer had a slowdown factor of O( n2m) moves. The technique is used to derive improved self-stabilizing algorithms for several graph domination problems. The paper also introduces a generalization of the distance-two model allowing a more space efficient transformer. © 2012 Elsevier Inc. All rights reserved.","Distributed algorithms; Fault tolerant algorithms; Self-stabilization"
"A parallel and distributed meta-heuristic framework based on partially ordered knowledge sharing","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862807150&doi=10.1016%2fj.jpdc.2012.01.007&partnerID=40&md5=7cfaf470aacf835c60bdfd5309f7b166","We propose a new distributed and parallel meta-heuristic framework to address the issues of scalability and robustness in the optimization problem. The proposed framework, named PADO (Parallel And Distributed Optimization framework), can utilize heterogeneous computing and communication resources to achieve scalable speedup while maintaining high solution quality. Specifically, we combine an existing meta-heuristic framework with a loosely coupled distributed island model for scalable parallelization. Based on a mature sequential optimization framework, we implement a population-based meta-heuristic algorithm with an island model for parallelization. The coordination overhead of previous approaches is significantly reduced by using a partially ordered knowledge sharing (POKS) model as an underlying model for distributed computing. The resulting framework can encompass many meta-heuristic algorithms and can solve a wide variety of problems with minimal configuration. We demonstrate the applicability and the performance of the framework with a traveling salesman problem (TSP), multi-objective design space exploration (DSE) problem of an embedded multimedia system, and a drug scheduling problem of cancer chemotherapy. © 2012 Elsevier Inc. All rights reserved.","Design space exploration; Knowledge sharing; Meta-heuristic; Parallel and distributed optimization framework"
"A hierarchical reliability-driven scheduling algorithm in grid systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857794863&doi=10.1016%2fj.jpdc.2011.12.004&partnerID=40&md5=2290c65adb249586d3e686502d42ce93","In a Grid computing system, many distributed scientific and engineering applications often require multi-institutional collaboration, large-scale resource sharing, wide-area communication, etc. Applications executing in such systems inevitably encounter different types of failures such as hardware failure, program failure, and storage failure. One way of taking failures into account is to employ a reliable scheduling algorithm. However, most existing Grid scheduling algorithms do not adequately consider the reliability requirements of an application. In recognition of this problem, we design a hierarchical reliability-driven scheduling architecture that includes both a local scheduler and a global scheduler. The local scheduler aims to effectively measure task reliability of an application in a Grid virtual node and incorporate the precedence constrained tasks' reliability overhead into a heuristic scheduling algorithm. In the global scheduler, we propose a hierarchical reliability-driven scheduling algorithm based on quantitative evaluation of independent application reliability. Our experiments, based on both randomly generated graphs and the graphs of some real applications, show that our hierarchical scheduling algorithm performs much better than the existing scheduling algorithms in terms of system reliability, schedule length, and speedup. © 2012 Elsevier Inc. All rights reserved.","Application; Grid computing; Hierarchical; Reliability; Scheduling algorithm"
"An efficient incentive scheme with a distributed authority infrastructure in peer-to-peer networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867865994&doi=10.1016%2fj.jpdc.2012.08.003&partnerID=40&md5=9a6b79a8cbd7ad0638bf2e39b8660e77","Today's peer-to-peer networks are designed based on the assumption that the participating nodes are cooperative, which does not hold in reality. Incentive mechanisms that promote cooperation must be introduced. However, the existing incentive schemes (using either reputation or virtual currency) suffer from various attacks based on false reports. Even worse, a colluding group of malicious nodes in a peer-to-peer network can manipulate the history information of its own members, and the damaging power increases dramatically with the group size. Such malicious nodes/collusions are difficult to detect, especially in a large network without a centralized authority. In this paper, we propose a new distributed incentive scheme, in which the amount that a node can benefit from the network is proportional to its contribution, malicious nodes can only attack others at the cost of their own interests, and a colluding group cannot gain advantage by cooperation regardless of its size. Consequently, the damaging power of colluding groups is strictly limited. The proposed scheme includes three major components: a distributed authority infrastructure, a key sharing protocol, and a contract verification protocol. © 2012 Elsevier Inc. All rights reserved.","Incentive scheme; Peer-to-peer networks; Threshold cryptography"
"An algorithmic strategy for in-network distributed spatial analysis in wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867808930&doi=10.1016%2fj.jpdc.2012.09.005&partnerID=40&md5=a64860f8564c71cb82ce28c2650472e4","A wireless sensor network (WSN) can be construed as an intelligent, largely autonomous, instrument for scientific observation at fine temporal and spatial granularities and over large areas. The ability to perform spatial analyses over sensor data has often been highlighted as desirable in areas such as environmental monitoring. Whilst there exists research on computing topological changes of dynamic phenomena, existing proposals do not allow for more expressive in-network spatial analysis. This paper addresses the challenges involved in using WSNs to identify, track and report topological relationships between dynamic, transient spatial phenomena and permanent application-specific geometries focusing on cases where the geometries involved can be characterized by sets of nodes embedded in a finite 2-dimensional space. The approach taken is algebraic, i.e., analyses are expressed as algebraic expressions that compose primitive operations (such as Adjacent, or AreaInside). The main contributions are distributed algorithms for the operations in the proposed algebra and an empirical evaluation of their performance in terms of bit complexity, response time, and energy consumption. © 2012 Elsevier Inc. All rights reserved.","Distributed spatial analysis; Environmental monitoring; In-network processing; Topological relationships"
"Network partitioning using harmony search and equivalencing for distributed computing","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862001419&doi=10.1016%2fj.jpdc.2012.04.006&partnerID=40&md5=3732e2dd6da986ddbb907d8105df2393","Power system has a highly interconnected network that requires intense computational effort and resources for centralized control. Distributed computing needs the systems to be partitioned optimally into clusters. The network partitioning is an optimization problem whose objective is to minimize the number of nodes in a cluster and the tie lines between the clusters. Harmony Search(HS) Algorithm is one of the recently developed meta heuristic algorithms that can be applied to optimization problems. In this work, the HS algorithm is applied to the network partitioning problem and power flow based equivalencing is done to represent the external system. Simulation is done on IEEE Standard Test Systems. The algorithm is found to be very effective in partitioning the system hierarchically and the equivalencing method gives accurate results in comparison to the centralized control. © 2012 Elsevier Inc. All rights reserved.","Distributed computing; Harmony Search Algorithm; Network decomposition; Network equivalencing; Optimal partitioning"
"Efficient local search on the GPU - Investigations on the vehicle routing problem","2013","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869496633&doi=10.1016%2fj.jpdc.2012.02.020&partnerID=40&md5=2b724ade2e48f7f7365371d521837f30","We study how to implement local search efficiently on data parallel accelerators such as Graphics Processing Units. The Distance-constrained Capacitated Vehicle Routing Problem, a computationally very hard discrete optimization problem with high industrial relevance, is the selected vehicle for our investigations. More precisely, we investigate local search with the Best Improving strategy for the 2-opt and 3-opt operators on a giant tour representation. Resource extension functions are used for constant time move evaluation. Using CUDA, a basic implementation called The Benchmark Version has been developed and deployed on a Fermi architecture Graphics Processing Unit. Both neighborhood setup and evaluation are performed entirely on the device. The Benchmark Version is the initial step of an incremental improvement process where a number of important implementation aspects have been systematically studied. Ten well-known test instances from the literature are used in computational experiments, and profiling tools are used to identify bottlenecks. In the final version, the device is fully saturated, given a large enough problem instance. A speedup of almost an order of magnitude relative to The Benchmark Version is observed. We conclude that, with some effort, local search may be implemented very efficiently on Graphics Processing Units. Our experiments show that a maximum efficiency, however, requires a neighborhood cardinality of at least one million. Full exploration of a billion neighbors takes a few seconds and may be deemed too expensive with the current technology. Reduced neighborhoods through filtering is an obvious remedy. Experiments on simple models of neighborhood filtering indicate, however, that the speedup effect is limited on data parallel accelerators. We believe these insights are valuable in the design of new metaheuristics that fully utilize modern, heterogeneous processors. © 2012 Elsevier Inc. All rights reserved.","CUDA; Discrete optimization; Efficient implementation; GPU; Local search; Parallel computing; Stream processing; VRP"
"Improving communication latency with the write-only architecture","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867781576&doi=10.1016%2fj.jpdc.2012.08.007&partnerID=40&md5=773392b95ac2cbeebf371beed094140c","This paper introduces a novel execution paradigm called the Write-Only Architecture (WOA) that reduces communication latency overheads by up to a factor of five over previous methods. The WOA writes data through distributed control flow logic rather than using a read-write paradigm or a centralised message hub which allows tasks to be partitioned at a fine-grained level without suffering from excessive communication overheads on distributed systems. In this paper we provide formal assignment results for software benchmarks partitioned using the WOA and previous execution paradigms for distributed heterogeneous architectures along with bounds and complexity information to demonstrate the robust performance improvements possible with the WOA. © 2012 Published by Elsevier Inc.","Communication latency reduction; Execution paradigm; Heterogeneous computing; High-performance computing"
"An incrementally deployable path address scheme","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865087648&doi=10.1016%2fj.jpdc.2012.05.001&partnerID=40&md5=525c4089e4f44daa7652ea72c8115235","The research community has proposed numerous network security solutions, each dealing with a specific problem such as address spoofing, denial-of-service attacks, denial-of-quality attacks, reflection attacks, viruses, or worms. However, due to the lack of fundamental support from the Internet, individual solutions often share little common ground in their design, which causes a practical problem: deploying all these vastly different solutions will add exceedingly high complexity to the Internet routers. In this paper, we propose a simple generic extension to the Internet, providing a new type of information, called path addresses, that simplify the design of security systems for packet filtering, fair resource allocation, packet classification, IP traceback, filter push-back, etc. IP addresses are owned by end hosts; path addresses are owned by the network core, which is beyond the reach of the hosts. We describe how to enhance the Internet protocols for path addresses that meet the uniqueness requirement, completeness requirement, safety requirement, and incrementally deployable requirement. We evaluate the performance of our scheme both analytically and by simulations, which show that, at small overhead, the false positive ratio and the false negative ratio can both be made negligibly small. © 2012 Elsevier Inc. All rights reserved.","Internet protocols; Network security; Path address"
"Hardware implementation study of several new egress link scheduling algorithms","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862018526&doi=10.1016%2fj.jpdc.2012.04.010&partnerID=40&md5=9684a79faaacea252d2d269d878b1c03","The provision of Quality of Service (QoS) in interconnection networks is required for new multimedia and time-sensitive applications, which are very important for recent utility computing data centers (UCDCs) using high performance networks. These interconnection networks support switch-based principles and establish high demands in terms of bandwidth, time-delay, and delivery over short distances. A key component for networks with QoS support is the egress link scheduling algorithm. Apart from providing a good performance in terms of, for example, good end-to-end delay (also called latency) and fair bandwidth allocation, an ideal scheduling algorithm implemented in a high-performance network with QoS support should satisfy another important property which is to have a low computational and implementation complexity. In this paper, we propose specific implementations (taking into account the characteristics of current high performance networks) of several fair-queuing scheduling algorithms and compare their complexity in terms of silicon area and computation delay. In order to carry out this comparison, we have devised our own hardware comparison methodology. Following this methodology, we have performed our own hardware implementation for the different schedulers. We have modeled the schedulers using the Handel-C language and employed the DK design suite tool from Celoxica in order to obtain hardware estimates on silicon area and arbitration time. © 2012 Elsevier Inc. All rights reserved.","Advanced Switching; Aggregated flows; Application requirements; Interconnection networks; Performance evaluation; Quality of Service; Scheduling"
"A complex event routing infrastructure for distributed systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856336652&doi=10.1016%2fj.jpdc.2011.11.005&partnerID=40&md5=c480a48d64e6aad7c364136672aa7407","With the growing number of mega services and cloud computing platforms, industrial organizations are utilizing distributed data centers at increasing rates. Rather than the request/reply model, these centers use an event-based communication model. Traditionally, the event-based middleware and the Complex Event Processing (CEP) engine are viewed as two distinct components within a distributed system's architecture. This division adds additional system complexity and reduces the ability for consuming applications to fully utilize the CEP toolset. This article will address these issues by proposing a novel event-based middleware solution. We introduce Complex Event Routing Infrastructure (CERI), a single event-based infrastructure that serves as an event bus and provides first class integration of CEP. An unstructured peer-to-peer network is exploited to allow for efficient event transmission. To reduce network flooding, superpeers and overlay network partitioning are introduced. Additionally, CERI provides each client node the capability of local complex query evaluation. As a result, applications can offload internal logic to the query evaluation engine in an efficient manner. Finally, as more client nodes and event types are added to the system, the CERI can scale up. Because of these favorable scaling properties, CERI serves as a foundational step in bringing event-based middleware and CEP closer together into a single unified infrastructure component. © 2011 Elsevier Inc. All rights reserved.","Cloud computing; Complex event processing; Distributed systems; Event-based middleware"
"Efficient data aggregation with in-network integrity control for WSN","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865071826&doi=10.1016%2fj.jpdc.2012.06.006&partnerID=40&md5=8fa252647096ec33e22af3249dbfdd2f","Energy is a scarce resource in Wireless Sensor Networks (WSN). Some studies show that more than 70% of energy is consumed in data transmission in WSN. Since most of the time, the sensed information is redundant due to geographically collocated sensors, most of this energy can be saved through data aggregation. Furthermore, data aggregation improves bandwidth usage and reduces collisions due to interference. Unfortunately, while aggregation eliminates redundancy, it makes data integrity verification more complicated since the received data is unique. In this paper, we present a new protocol that provides control integrity for aggregation in wireless sensor networks. Our protocol is based on a two-hop verification mechanism of data integrity. Our solution is essentially different from existing solutions in that it does not require referring to the base station for verifying and detecting faulty aggregated readings, thus providing a totally distributed scheme to guarantee data integrity. We carried out numerical analysis and simulations using the TinyOS environment. Results show that the proposed protocol yields significant savings in energy consumption while preserving data integrity, and outperforms comparable solutions with respect to some important performance criteria. © 2012 Elsevier Inc. All rights reserved.","Data aggregation; Integrity control; Wireless sensor networks"
"Flocking based distributed self-deployment algorithms in mobile sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856336895&doi=10.1016%2fj.jpdc.2011.11.013&partnerID=40&md5=2c4765550362781c837ae04eca02d2f6","In this paper, we address a novel deployment problem in isotropic mobile sensor networks. Sensors are to be relocated uniformly in a region of interest (ROI) centered at a target of interest (TOI) which could be stationary or mobile. With the assumption that relative direction of a sensor to the TOI can be recognized or inferred by devices equipped in the sensor, distributed control algorithms based on first-order and second-order dynamic models are proposed for both stationary and mobile TOI situations. The Lyapunov stabilities and coverage guarantee are provided. To further improve the deployment such as coverage holes inside the network and uniformity of the deployment, four assisted rules are also proposed. Then algorithms proposed for the situation of a stationary TOI are extended to anisotropic sensor networks. Simulations demonstrate the effective performances of the proposed algorithms. © 2011 Elsevier Inc. All rights reserved.","Consensus; Flocking; Mobile sensor network; Sensor deployment; Triangle tessellation"
"Switch-based packing technique to reduce traffic and latency in token coherence","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856340696&doi=10.1016%2fj.jpdc.2011.11.010&partnerID=40&md5=8def9c2499fdb1da2785917e262814c3","Token Coherence is a cache coherence protocol able to simultaneously capture the best attributes of traditional protocols: low latency and scalability. However it may lose these desired features when (1) several nodes contend for the same memory block and (2) nodes write highly-shared blocks. The first situation leads to the issue of simultaneous broadcast requests which threaten the protocol scalability. The second situation results in a burst of token responses directed to the writer, which turn it into a bottleneck and increase the latency. To address these problems, we propose a switch-based packing technique able to encapsulate several messages (while in transit) into just one. Its application to the simultaneous broadcasts significantly reduces their bandwidth requirements (up to 45%). Its application to token responses lowers their transmission latency (by 70%). Thus, the packing technique decreases both the latency and coherence traffic, thereby improving system performance (about 15% of reduction in runtime). © 2011 Elsevier Inc. All rights reserved.","Cache coherence protocol; Combining switches; Message packing; Scalability; Token coherence; Traffic reduction"
"Acoustic scattering solver based on single level FMM for multi-GPU systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865740153&doi=10.1016%2fj.jpdc.2011.07.013&partnerID=40&md5=105c94e49193ba7bb1d5230c97b3468e","In this paper, we present a heterogeneous parallel solver of a high frequency single level Fast Multipole Method (FMM) for the Helmholtz equation applied to acoustic scattering. The developed solution uses multiple GPUs to tackle the compute bound steps of the FMM (aggregation, disaggregation, and near interactions) while the CPU handles a memory bound step (translation) using OpenMP. The proposed solver performance is measured on a workstation with two GPUs (NVIDIA GTX 480) and is compared with that of a distributed memory solver run on a cluster of 32 nodes (HP BL465c) with an Infiniband network. Some energy efficiency results are also presented in this work. © 2011 Elsevier Inc. All rights reserved.","Acoustic scattering; FMM; GPGPU; Heterogeneous"
"VForce: An environment for portable applications on high performance systems with accelerators","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865722340&doi=10.1016%2fj.jpdc.2011.07.014&partnerID=40&md5=201e6435b277c49bdab63a078478c396","Special Purpose Processors (SPPs), including Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs), are increasingly being used to accelerate scientific applications. VForce aims to aid application programmers in using such accelerators with minimal changes in user code. VForce is an extensible middleware framework that enables VSIPL++ (the Vector Signal Image Processing Library extension) programs to transparently use Special Purpose Processors (SPPs) while maintaining portability across platforms with and without SPP hardware. The framework is designed to maintain a VSIPL++- like environment and hide hardware-specific details from the application programmer while preserving performance and productivity. VForce focuses on the interface between application code and accelerator code. The same application code can run in software on a general purpose processor or take advantage of SPPs if they are available. VForce is unique in that it supports calls to both FPGAs and GPUs while requiring no changes in user code. Results on systems with NVIDIA Tesla GPUs and Xilinx FPGAs are presented. This paper describes VForce, illustrates its support for portability, and discusses lessons learned for providing support for different hardware configurations at run time. Key considerations involve global knowledge about the relationship between processing steps for defining application mapping, memory allocation, and task parallelism. © 2011 Elsevier Inc. All rights reserved.","FPGA; GPU; Heterogeneous systems; Middleware; Portability"
"Help when needed, but no more: Efficient read/write partial snapshot","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455122849&doi=10.1016%2fj.jpdc.2011.08.005&partnerID=40&md5=89103dde53644757723eae559b167406","An atomic snapshot object is an object that can be concurrently accessed by asynchronous processes prone to crash. It is made of m components (base atomic registers) and is defined by two operations: an update operation that allows a process to atomically assign a new value to a component and a snapshot operation that atomically reads and returns the values of all the components. To cope with the net effect of concurrency, asynchrony and failures, the algorithm implementing the update operation has to help concurrent snapshot operations so that they always terminate. This paper is on partial snapshot objects. Such an object provides a snapshot operation that can take any subset of the components as input parameter, and atomically reads and returns the values of this subset of components. The paper has two contributions. The first is the introduction of two properties for partial snapshot object algorithms, called help-locality and freshness. Help-locality requires that an update operation helps only the concurrent partial snapshot operations that read the component it writes. When an update of a component r helps a partial snapshot, freshness requires that the update provides the partial snapshot with a value of the component r that is at least as recent as the value it writes into that component. (No snapshot algorithm proposed so far satisfies these properties). The second contribution consists of an update and a partial snapshot algorithm that are wait-free, linearizable and satisfy the previous efficiency properties. Interestingly, the principle that underlies the proposed algorithms is different from the one used so far, namely, it is based on the ""write first, and help later"" strategy. An improvement of the previous algorithms is also presented. Based on LL/SC atomic registers (instead of read/write registers), this improvement decreases the number of base registers from O(n2) to O(n). This shows an interesting tradeoff relating the synchronization power of the base operations and the number of base atomic registers when using the ""write first, and help later"" strategy. © 2011 Elsevier Inc. All rights reserved.","Adaptive algorithm; Asynchronous shared memory system; Asynchrony; Atomicity; Concurrency; Efficiency; Linearizability; LL/SC atomic registers; Locality; Partial snapshot; Process crash; Read/write atomic register; Wait-free algorithm"
"Decentralized proactive resource allocation for maximizing throughput of P2P Grid","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855405449&doi=10.1016%2fj.jpdc.2011.10.010&partnerID=40&md5=32b44662404c267ae94f230df6f8410a","Peer-to-peer Desktop Grids provide integrated computational resources by leveraging autonomous desktop computers located at the edge of the Internet to offer high computing power. The arbitrary arrival and serving rates of tasks on peers impedes the high throughput in large-scale P2P Grids. We propose a novel autonomous resource allocation scheme, which can maximize the throughput of self-organizing P2P Grid systems. Our design possesses three key features: (1) high adaptability to dynamic environment by proactive and convex-optimal estimation of nodes' volatile states; (2) minimized task migration conflict probability (upper bound can be limited to 2%) of over-utilized nodes individually shifting surplus loads; (3) a load-status conscious gossip protocol for optimizing distributed resource discovery effect. Based on a real-life user's workload and capacity distribution, the simulation results show that our approach could get significantly improved throughput with 23.647.1% reduction on unprocessed workload compared to other methods. We also observe high scalability of our solution under dynamic peer-churning situations. © 2011 Elsevier Inc. All rights reserved.","Autonomous decision conflict; Fully-decentralized resource allocation; P2P desktop Grid system; System throughput"
"On the efficiency of routing in sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861183851&doi=10.1016%2fj.jpdc.2012.02.021&partnerID=40&md5=cd3f2ef04d38b35e15bf65be102aeb0d","In sensor networks, a key efficiency measure for routing protocols is the stretch of the computed paths, where the stretch is the ratio of the path length and the Euclidean distance covered. In the literature, many protocols have been evaluated via extensive simulations, and often come without any theoretical guarantees. For those whose performances are theoretically guaranteed there is an important gap between the theoretical predictions and the experimental results. The contribution of this paper is twofold. First, we give theoretical results that explain the observed efficiency of many of the algorithms proposed in the literature. Second, we propose ROAM2, a deterministic routing protocol, that requires a single bit of memory at each node and that ensures, with high probability (depending on the node distribution), that the paths have a constant stretch. © 2012 Elsevier Inc. All rights reserved.","Geographic routing; Obstacle avoidance; Sensor network"
"Wukong: A cloud-oriented file service for mobile Internet devices","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855358258&doi=10.1016%2fj.jpdc.2011.10.017&partnerID=40&md5=f1596b1eed5bb95bc305b25b8a1e8f17","Along with the rapid growth of heterogeneous cloud services and network technologies, an increasing number of mobile devices use cloud storage services to enlarge their capacity and share data in our daily lives. We commonly use cloud service client-side software in a straightforward fashion. However, when more devices and users participate in heterogeneous services, the difficulty of managing these services efficiently and conveniently increases. In this paper, we report a novel cloud-oriented file service, Wukong, which provides a user-friendly and highly available facilitative data access method for mobile devices in cloud settings. Wukong supports mobile applications, which may access local files only, transparently accessing cloud services with a relatively high performance. To the best of our knowledge, Wukong is the first file service that supports heterogeneous cloud services for mobile devices by using the innovative storage abstraction layer. We have implemented a prototype with several plugins and evaluated it in a systematic way. We find that this easily operable file service has a high usability and extensibility. It costs about 50 to 150 lines of code to implement a new backend service support plugin. Wukong achieves an acceptable throughput of 179.11 kB/s in an ADSL environment and 80.68 kB/s under a countryside EVDO 3G network with negligible overhead. © 2011 Elsevier Inc. All rights reserved.","Cloud computing; File services; Mobile devices; Plugins"
"Coordination in wireless sensor-actuator networks: A survey","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.02.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861186182&doi=10.1016%2fj.jpdc.2012.02.013&partnerID=40&md5=5d3765d01d72f17189f58ee3cc6442cc","Wireless Sensor-Actuator Networks (WSANs) have a myriad of applications, ranging from pacifying bulls to controlling light intensity in homes automatically. An important aspect of WSANs is coordination. Unlike conventional Wireless Sensor Networks (WSNs), sensor and actuator nodes must work hand-in-hand to collect and forward data, and act on any sensed data collaboratively, promptly and reliably. To this end, this paper reviews current state-of-the-art techniques that address this fundamental problem. More specifically, we review techniques in the following areas: (i) sensor-actuator coordination, (ii) routing protocols, (iii) transport protocols, and (iv) actuator-to-actuator coordination protocols. We provide an extensive qualitative comparison of their key features, advantages and disadvantages. Finally, we present unresolved problems and future research directions. © 2012 Elsevier Inc. All rights reserved.","Actuators; Coordination; Energy efficiency; Wireless sensor and actuator networks"
"Low latency complex event processing on parallel hardware","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855350883&doi=10.1016%2fj.jpdc.2011.11.002&partnerID=40&md5=2c4c9b924c3571122b582e7d14554c85","Most complex information systems are event driven: each part of the system reacts to the events happening in the other parts, potentially generating new events. Complex event processing (CEP) engines in charge of interpreting, filtering, and combining primitive events to identify higher level composite events according to a set of rules are the new breed of message-oriented middleware, which is being proposed today to better support event-driven interactions. A key requirement for CEP engines is low latency processing, even in presence of complex rules and large numbers of incoming events. In this paper, we investigate how parallel hardware may speed up CEP processing. In particular, we consider the most common operators offered by existing rule languages (i.e., sequences, parameters, and aggregates); we consider different algorithms to process rules built using such operators; and we discuss how they can be implemented on a multi-core CPU and on CUDA, a widespread architecture for general-purpose programming on GPUs. Our analysis shows that the use of GPUs can bring impressive speedups in the presence of complex rules. On the other hand, it shows that multi-core CPUs scale better with the number of rules. Our conclusion is that an advanced CEP engine should leverage a multi-core CPU for processing the simplest rules, using the GPU as a coprocessor devoted to process the most complex ones. © 2011 Elsevier Inc. All rights reserved.","Complex event processing; General-purpose GPU computing; Multi-core CPUs; Parallel hardware"
"Regression-based resource provisioning for session slowdown guarantee in multi-tier internet servers","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856337684&doi=10.1016%2fj.jpdc.2011.11.011&partnerID=40&md5=0b59a96b28461f66693c7ec6131fbc6d","Autonomous management of a multi-tier Internet service involves two critical and challenging tasks, one understanding its dynamic behaviors when subjected to dynamic workloads and second adaptive management of its resources to achieve performance guarantees. We propose a statistical machine learning based approach to achieve session slowdown guarantees of a multi-tier Internet service. Session slowdown is the relative ratio of a session's total queueing delay to its total processing time. It is a compelling performance metric of session-based online transactions because it directly measures user-perceived relative performance and it is independent of the session length. However, there is no analytical model for session slowdown on multi-tier servers. We first conduct training to learn the statistical regression models that quantitatively capture an Internet service's dynamic behaviors as relationships between various service parameters. Then, we propose a dynamic resource provisioning approach that utilizes the learned regression models to efficiently achieve session slowdown guarantee under dynamic workloads. The approach is based on the combination of offline training and online monitoring of the Internet service behavior. Simulations using the industry standard TPC-W benchmark demonstrate the effectiveness and efficiency of the regression based resource provisioning approach for session slowdown oriented performance guarantee of a multi-tier e-commerce application. © 2011 Elsevier Inc. All rights reserved.","Autonomous resource provisioning; Performance guarantee; Regression modeling and analysis; Scalable multi-tier Internet services; Session slowdown"
"Performance models for asynchronous data transfers on consumer Graphics Processing Units","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865705401&doi=10.1016%2fj.jpdc.2011.07.011&partnerID=40&md5=73ead2d4935c927ed4985060d7567d2e","Graphics Processing Units (GPU) have impressively arisen as general-purpose coprocessors in high performance computing applications, since the launch of the Compute Unified Device Architecture (CUDA). However, they present an inherent performance bottleneck in the fact that communication between two separate address spaces (the main memory of the CPU and the memory of the GPU) is unavoidable. The CUDA Application Programming Interface (API) provides asynchronous transfers and streams, which permit a staged execution, as a way to overlap communication and computation. Nevertheless, a precise manner to estimate the possible improvement due to overlapping does not exist, neither a rule to determine the optimal number of stages or streams in which computation should be divided. In this work, we present a methodology that is applied to model the performance of asynchronous data transfers of CUDA streams on different GPU architectures. Thus, we illustrate this methodology by deriving expressions of performance for two different consumer graphic architectures belonging to the more recent generations. These models permit programmers to estimate the optimal number of streams in which the computation on the GPU should be broken up, in order to obtain the highest performance improvements. Finally, we have checked the suitability of our performance models with three applications based on codes from the CUDA Software Development Kit (SDK) with successful results. © 2011 Elsevier Inc. All rights reserved.","Asynchronous transfers; CUDA; GPU; Overlapping of communication and computation; Streams"
"Efficient and scalable scheduling for performance heterogeneous multicore systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856336253&doi=10.1016%2fj.jpdc.2011.12.005&partnerID=40&md5=38f6f456908a751784e8f46b1d5bf212","Performance heterogeneous multicore processors (HMP for brevity) consisting of multiple cores with the same instruction set but different performance characteristics (e.g., clock speed, issue width), are of great concern since they are able to deliver higher performance per watt and area for programs with diverse architectural requirements than comparable homogeneous ones. However, such power and area efficiencies of performance heterogeneous multicore systems can only be achieved when workloads are matched with cores according to both the properties of the workload and the features of the cores. Several heterogeneity-aware schedulers were proposed in the previous work. In terms of whether properties of workloads are obtained online or not, those scheduling algorithms can be categorized into two classes: online monitoring and offline profiling. The previous online monitoring approaches had to trace threads' execution on all core types, which is impractical as the number of core types grows. Besides, to trace all core types threads have to be migrated among cores, which may cause load imbalance and degrade the performance. The existing offline profiling approaches profile programs with a given input set before really executing them and thus remove the overhead associated with the number of core types. However, offline profiling approaches do not account for phase changes of threads. Moreover, since the properties they have collected are based on the given input set, those offline profiling approaches are hard to adapt to various input sets and therefore will drastically affect the program performance. To address the above problems in the existing approaches, we propose a new technique, ASTPI (Average Stall Time Per Instruction), to measure the efficiencies of threads in using fast cores. We design, implement and evaluate a new online monitoring approach called ESHMP, which is based on the metric. Our evaluation in the Linux 2.6.21 operating system shows that ESHMP delivers scalability while adapting to a wide variety of applications. Also, our experiment results show that among HMP systems in which heterogeneity-aware schedulers are adopted and there are more than one LLC (Last Level Cache), the architecture where heterogeneous cores share LLCs gain better performance than the ones where homogeneous cores share LLCs. © 2011 Elsevier Inc. All rights reserved.","Algorithm; Operating systems; Performance heterogeneous multicore; Scheduling"
"Independent spanning trees on twisted cubes","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455130980&doi=10.1016%2fj.jpdc.2011.09.002&partnerID=40&md5=a916962741daedcce3ee9a62979c7cc5","Multiple independent spanning trees have applications to fault tolerance and data broadcasting in distributed networks. There are two versions of the n independent spanning trees conjecture. The vertex (edge) conjecture is that any n-connected (n-edge-connected) graph has n vertex-independent spanning trees (edge-independent spanning trees) rooted at an arbitrary vertex. Note that the vertex conjecture implies the edge conjecture. The vertex and edge conjectures have been confirmed only for n-connected graphs with n≤4, and they are still open for arbitrary n-connected graph when n<5. In this paper, we confirm the vertex conjecture (and hence also the edge conjecture) for the n-dimensional twisted cube TQn by providing an O(NlogN) algorithm to construct n vertex-independent spanning trees rooted at any vertex, where N denotes the number of vertices in TQn. Moreover, all independent spanning trees rooted at an arbitrary vertex constructed by our construction method are isomorphic and the height of each tree is n+1 for any integer n<2. © 2011 Elsevier Inc. All rights reserved.","Broadcasting; Fault tolerance; Independent spanning tree; Twisted cube"
"Optimizing performance and reliability on heterogeneous parallel systems: Approximation algorithms and heuristics","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855339817&doi=10.1016%2fj.jpdc.2011.11.003&partnerID=40&md5=762b26c57bd14e68c4a00fe08bf1cce4","We study the problem of scheduling tasks (with and without precedence constraints) on a set of related processors which have a probability of failure governed by an exponential law. The goal is to design approximation algorithms or heuristics that optimize both makespan and reliability. First, we show that both objectives are contradictory and that the number of points of the Pareto-front can be exponential. This means that this problem cannot be approximated by a single schedule. Second, for independent unitary tasks, we provide an optimal scheduling algorithm where the objective is to maximize the reliability subject to makespan minimization. For the bi-objective optimization, we provide a (1+,1)-approximation algorithm of the Pareto-front. Next, for independent arbitrary tasks, we propose a 〈2,1〉-approximation algorithm (i.e. for any fixed value of the makespan, the obtained solution is optimal on the reliability and no more than twice the given makespan) that has a much lower complexity than the other existing algorithms. This solution is used to derive a (2+,1)-approximation of the Pareto-front of the problem. All these proposed solutions are discriminated by the value of the product failure rate × unitary instruction execution time of each processor, which appears to be a crucial parameter in the context of bi-objective optimization. Based on this observation, we provide a general method for converting scheduling heuristics on heterogeneous clusters into heuristics that take into account the reliability when there are precedence constraints. The average behavior is studied by extensive simulations. Finally, we discuss the specific case of scheduling a chain of tasks which leads to optimal results. © 2011 Elsevier Inc. All rights reserved.","Makespan; Pareto-front approximation; Precedence task graphs; Reliability; Scheduling"
"Long-term availability prediction for groups of volunteer resources","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855348685&doi=10.1016%2fj.jpdc.2011.10.007&partnerID=40&md5=1ea3cd16276f52f4be076b8703552a4a","Volunteer computing uses the free resources in Internet and Intranet environments for large-scale computation and storage. Currently, 70 applications use over 12 PetaFLOPS of computing power from such platforms. However, these platforms are currently limited to embarrassingly parallel applications. In an effort to broaden the set of applications that can leverage volunteer computing, we focus on the problem of predicting if a group of resources will be continuously available for a relatively long time period. Ensuring the collective availability of volunteer resources is challenging due to their inherent volatility and autonomy. Collective availability is important for enabling parallel applications and workflows on volunteer computing platforms. We evaluate our predictive methods using real availability traces gathered from hundreds of thousands of hosts from the SETI@home volunteer computing project. We show our prediction methods can guarantee reliably the availability of collections of volunteer resources. We show that this is particularly useful for service deployments over volunteer computing environments. © 2011 Elsevier Inc. All rights reserved.","Availability prediction; Reliability; Service deployment; Trace analysis; Volunteer computing"
"Microwave tomography for breast cancer detection on Cell broadband engine processors","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865800333&doi=10.1016%2fj.jpdc.2011.10.013&partnerID=40&md5=1783354a4f0c0ca586b693dd7d985fcf","Microwave tomography (MT) is a safe screening modality that can be used for breast cancer detection. The technique uses the dielectric property contrasts between different breast tissues at microwave frequencies to determine the existence of abnormalities. Our proposed MT approach is an iterative process that involves two algorithms: Finite-Difference Time-Domain (FDTD) and Genetic Algorithm (GA). It is a compute intensive problem: (i) the number of iterations can be quite large to detect small tumors; (ii) many fine-grained computations and discretizations of the object under screening are required for accuracy. In our earlier work, we developed a parallel algorithm for microwave tomography on CPU-based homogeneous, multi-core, distributed memory machines. The performance improvement was limited due to communication and synchronization latencies inherent in the algorithm. In this paper, we exploit the parallelism of microwave tomography on the Cell BE processor. Since FDTD is a numerical technique with regular memory accesses, intensive floating point operations and SIMD type operations, the algorithm can be efficiently mapped on the Cell processor achieving significant performance. The initial implementation of FDTD on Cell BE with 8 SPEs is 2.9 times faster than an eight node shared memory machine and 1.45 times faster than an eight node distributed memory machine. In this work, we modify the FDTD algorithm by overlapping computations with communications during asynchronous DMA transfers. The modified algorithm also orchestrates the computations to fully use data between DMA transfers to increase the computation-to-communication ratio. We see 54% improvement on 8 SPEs (27.9% on 1 SPE) for the modified FDTD in comparison to our original FDTD algorithm on Cell BE. We further reduce the synchronization latency between GA and FDTD by using mechanisms such as double buffering. We also propose a performance prediction model based on DMA transfers, number of instructions and operations, the processor frequency and DMA bandwidth. We show that the execution time from our prediction model is comparable (within 0.5 s difference) with the execution time of the experimental results on one SPE. © 2011 Elsevier Inc. All rights reserved.","Breast cancer detection; Cell BE processor; Finite-difference time-domain (FDTD); Microwave tomography (MT)"
"Balls into bins with related random choices","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855369827&doi=10.1016%2fj.jpdc.2011.10.006&partnerID=40&md5=50cfbc4b868eb9295d6493784390e343","We consider a variation of classical balls-into-bins games. We randomly allocate m balls into n bins. Following Godfrey's model (Godfrey, 2008) [7], we assume that each ball i comes with a β-balanced set of clusters Bi= B1,⋯, Bsi, each containing a logarithmic number of balls. The condition of β-balancedness essentially enforces a uniform-like selection of bins for the clusters, where the parameter β<1 governs the deviation from uniformity. Each ball i=1,⋯,m, in turn, runs the following protocol: (i) it i.u.r. (independently and uniformly at random) chooses a cluster of bins Bi∈ Bi, and (ii) it i.u.r. chooses one of the empty bins in Bi and allocates itself to it. Should the cluster not contain at least a single empty bin, then the protocol fails. If the protocol terminates successfully, that is, every ball has indeed been able to find at least one empty bin in its chosen cluster, then this will obviously result in a maximum load of one. The main goal is to find a tight bound on the maximum number of balls, m, so that the protocol terminates successfully with a high probability. In this paper, we improve on Godfrey's result and show that m=nΘ(β). We use a more relaxed notion of balancedness than (Godfrey, 2008) [7] and show that our upper bounds hold for this type of balancedness. It even holds when we generalise the model and allow runs where each ball i tosses a coin and it copies the previous ball's choice Bi- 1 with constant probability pi (0< pi≤ 1). With the remaining probability the ball uses the protocol as described above. © 2011 Elsevier Inc. All rights reserved.","Balls into bins; Load balancing; Peer-to-peer; Stochastic processes"
"Predictable service overlay networks: Predictability through adaptive monitoring and efficient overlay construction and management","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455123775&doi=10.1016%2fj.jpdc.2011.09.005&partnerID=40&md5=e4a11fb51faf1269a286cb49d7efd760","Providing bounded communication among participating nodes is significant for distributed systems. Internet-based applications suffer with lower performance due to absence of bounded latency. We describe PSON, an overlay network solution to this challenging problem. PSON has two components. The monitoring component, SyncProbe, utilizes efficient and adaptive monitoring techniques to measure latency, detect packet loss, and provide real-time estimates of maximum expected latency along paths of an Internet substrate. The QoSMap component constructs and manages overlay such that it yields application-level QoS and provides resilience against network failures. A distinctive feature of QoSMap is construction of QoS-compliant backup paths which facilitate in overlay management and operation during the period when primary overlay paths violate QoS. We evaluate PSON on PlanetLab to provide predictable communication for applications with different topology and QoS requirement. Our experiments confirm the effectiveness of PSON in providing an inexpensive and efficient application-layer solution to Internet's unpredictable behavior. © 2011 Elsevier Inc. All rights reserved.","Bounded communication; Effective network measurements; Overlay construction and management; Predictable communication; Quality of service; Reliable and dependable networks; Synchronous communication"
"An adaptive hierarchical masterworker (AHMW) framework for gridsApplication to B&B algorithms","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855342921&doi=10.1016%2fj.jpdc.2011.10.002&partnerID=40&md5=3724ad1f7c28c4f6cc88b3629cc0ac22","Well-suited to embarrassingly parallel applications, the masterworker (MW) paradigm has largely and successfully used in parallel distributed computing. Nevertheless, such a paradigm is very limited in scalability in large computational grids. A natural way to improve the scalability is to add a layer of masters between the master and the workers making a hierarchical MW (HMW). In most existing HMW frameworks and algorithms, only a single layer of masters is used, the hierarchy is statically built and the granularity of tasks is fixed. Such frameworks and algorithms are not adapted to grids which are volatile, heterogeneous and large scale environments. In this paper, we revisit the HMW paradigm to match such characteristics of grids. We propose a new dynamic adaptive multi-layer hierarchical MW (AHMW) dealing with the scalability, volatility and heterogeneity issues. The construction and deployment of the hierarchy and the task management (deployment, decomposition of work, distribution of tasks, ⋯) are performed in a dynamic collaborative distributed way. The framework has been applied to the parallel Branch and Bound algorithm and experimented on the Flow-Shop scheduling problem. The implementation has been performed using the ProActive grid middleware and the large experiments have been conducted using about 2000 processors from the Grid'5000 French nation-wide grid infrastructure. The results demonstrate the high scalability of the proposed approach and its efficiency in terms of deployment cost, decomposition and distribution of work and exploration time. The results show that AHMW outperforms HMW and MW in scalability and efficiency in terms of deployment and exploration time. © 2011 Elsevier Inc. All rights reserved.","Branch and bound; Grid computing; Hierarchical masterworker; Large scale experiments; Masterworker"
"Empirical performance model-driven data layout optimization and library call selection for tensor contraction expressions","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856322680&doi=10.1016%2fj.jpdc.2011.09.006&partnerID=40&md5=36842cb9f39f3aab537236fc87a5b46c","Empirical optimizers like ATLAS have been very effective in optimizing computational kernels in libraries. The best choice of parameters such as tile size and degree of loop unrolling is determined in ATLAS by executing different versions of the computation. In contrast, optimizing compilers use a model-driven approach to program transformation. While the model-driven approach of optimizing compilers is generally orders of magnitude faster than ATLAS-like library generators, its effectiveness can be limited by the accuracy of the performance models used. In this paper, we describe an approach where a class of computations is modeled in terms of constituent operations that are empirically measured, thereby allowing modeling of the overall execution time. The performance model with empirically determined cost components is used to select library calls and choose data layout transformations in the context of the Tensor Contraction Engine, a compiler for a high-level domain-specific language for expressing computational models in quantum chemistry. The effectiveness of the approach is demonstrated through experimental measurements on representative computations from quantum chemistry. © 2011 Elsevier Inc. All rights reserved.","Compiler optimization; Data layout optimization; Library call selection; Tensor contractions"
"Decentralized polling with respectable participants","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455160071&doi=10.1016%2fj.jpdc.2011.09.003&partnerID=40&md5=c0831bfe72744dd85d90ee4b7a98b40e","We consider the polling problem in a social network: participants express support for a given option and expect an outcome reflecting the opinion of the majority. Individuals in a social network care about their reputation: they do not want their vote to be disclosed or any potential misbehavior to be publicly exposed. We exploit this social aspect of users to model dishonest behavior, and show that a simple secret sharing scheme, combined with lightweight verification procedures, enables private and accurate polling without requiring any central authority or cryptography. We present DPol, a simple and scalable distributed polling protocol in which misbehaving nodes are exposed with positive probability and in which the probability of honest participants having their privacy violated is traded off against the impact of dishonest participants on the accuracy of the polling result. The trade-off is captured by a generic parameter of the protocol, an integer k called the privacy parameter. In a system of N nodes with B dishonest participants, the probability of disclosing a participant's vote is bounded by (BN)k+1, whereas the impact on the score of each polling option is at most (3k+2)B, with high probability when dishonest users are a minority (i.e., B<N2), assuming nodes are uniformly spread across groups used by the system. When dishonest users are few (i.e., B<N), the impact bound holds deterministically and our protocol is asymptotically accurate: there is negligible difference between the true result score of the poll and the outcome of our protocol. To demonstrate the practicality of DPol, we report on its deployment on 400 PlanetLab nodes. The relative error of the polling result is less than 10% when faced with the message loss, crashes and delays inherent in PlanetLab. Our experiments show that the impact on the score of each polling option by dishonest nodes is (2k+1)B on average, consistently lower that the theoretical bound of (3k+2)B. © 2011 Elsevier Inc. All rights reserved.","Distributed polling; Fault tolerance; Overlay networks; Security; Social networks"
"Drawing maps with advice","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855343804&doi=10.1016%2fj.jpdc.2011.10.004&partnerID=40&md5=0973273226377b6f73c2d80d328a8292","We study the problem of the amount of information required to draw a complete or a partial map of a graph with unlabeled nodes and arbitrarily labeled ports. A mobile agent, starting at any node of an unknown connected graph and walking in it, has to accomplish one of the following tasks: draw a complete map of the graph, i.e., find an isomorphic copy of it including port numbering, or draw a partial map, i.e., a spanning tree, again with port numbering. The agent executes a deterministic algorithm and cannot mark visited nodes in any way. None of these map drawing tasks is feasible without any additional information, unless the graph is a tree. Hence we investigate the minimum number of bits of information (minimum size of advice) that has to be given to the agent to complete these tasks. It turns out that this minimum size of advice depends on the number n of nodes or the number m of edges of the graph, and on a crucial parameter μ, called the multiplicity of the graph, which measures the number of nodes that have an identical view of the graph. We give bounds on the minimum size of advice for both above tasks. For μ=1 our bounds are asymptotically tight for both tasks and show that the minimum size of advice is very small. For μ>1 the minimum size of advice increases abruptly. In this case our bounds are asymptotically tight for topology recognition and asymptotically almost tight for spanning tree construction. © 2011 Elsevier Inc. All rights reserved.","Advice; Algorithm; Graph; Spanning tree; Topology recognition"
"Distributed approximation of cellular coverage","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856324418&doi=10.1016%2fj.jpdc.2011.12.003&partnerID=40&md5=abaa99dcc1cd7d8256f088a9931791df","We consider the following model of cellular networks. Each base station has a given finite capacity, and each client has some demand and profit. A client can be covered by a specific subset of the base stations, and its profit is obtained only if its demand is provided in full. The goal is to assign clients to base stations, so that the overall profit is maximized subject to base station capacity constraints. In this work, we present a distributed algorithm for the problem, that runs in polylogarithmic time, and guarantees an approximation ratio close to the best known ratio achievable by a centralized algorithm. © 2011 Elsevier Inc. All rights reserved.","Assignment problem; Cellular networks; Distributed algorithms"
"The FLAME approach: From dense linear algebra algorithms to high-performance multi-accelerator implementations","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865733490&doi=10.1016%2fj.jpdc.2011.10.014&partnerID=40&md5=3f11303abb9ab571e9ad1f641ac2b1b3","Parallel accelerators are playing an increasingly important role in scientific computing. However, it is perceived that their weakness nowadays is their reduced ""programmability"" in comparison with traditional general-purpose CPUs. For the domain of dense linear algebra, we demonstrate that this is not necessarily the case. We show how the libflame library carefully layers routines and abstracts details related to storage and computation, so that extending it to take advantage of multiple accelerators is achievable without introducing platform specific complexity into the library code base. We focus on the experience of the library developer as he develops a library routine for a new operation, reduction of a generalized Hermitian positive definite eigenvalue problem to a standard Hermitian form, and configures the library to target a multi-GPU platform. It becomes obvious that the library developer does not need to know about the parallelization or the details of the multi-accelerator platform. Excellent performance on a system with four NVIDIA Tesla C2050 GPUs is reported. This makes libflame the first library to be released that incorporates multi-GPU functionality for dense matrix computations, setting a new standard for performance. © 2011 Elsevier Inc. All rights reserved.","Dense linear algebra libraries; Graphics processors; High performance computing; Runtime systems"
"Utilization-based pricing for power management and profit optimization in data centers","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455176832&doi=10.1016%2fj.jpdc.2011.09.001&partnerID=40&md5=aea4faf71ca8908681a05f6746c3083a","Traditional load balancing approaches may spread the load on more computers as long as the performance in terms of response time or cost is minimized. Nowadays power is a growing cost factor for data centers. In this paper, from the service provider's point of view, the load balancing decision is made based on whether power consumption can be reduced or more profit can be earned. To achieve this, we design pricing algorithms to influence the load distribution. Both algorithms take into account the utilization of computers besides other factors, such as prices and power costs. In the first algorithm, we design pricing functions with respect to the computer utilization to encourage or discourage resource usage. In the second algorithm, we focus on the profit that a service provider can earn after deducting power cost from its revenue. We formulate this profit optimization problem and derive the optimum price solution. © 2011 Elsevier Inc. All rights reserved.","Cost; Load balancing; Power; Pricing; Revenue; Utilization"
"An incentive-based distributed mechanism for scheduling divisible loads in tree networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856318673&doi=10.1016%2fj.jpdc.2011.11.008&partnerID=40&md5=e5268a5eb2e0fd6bd115566fab83e611","The underlying assumption of Divisible Load Scheduling (DLS) theory is that the processors composing the network are obedient, i.e., they do not ""cheat"" the scheduling algorithm. This assumption is unrealistic if the processors are owned by autonomous, self-interested organizations that have no a priori motivation for cooperation and they will manipulate the algorithm if it is beneficial to do so. In this paper, we address this issue by designing a distributed mechanism for scheduling divisible loads in tree networks, called DLS-T, which provides incentives to processors for reporting their true processing capacity and executing their assigned load at full processing capacity. We prove that the DLS-T mechanism computes the optimal allocation in an ex post Nash equilibrium. Finally, we simulate and study the mechanism under various network structures and processor parameters. © 2011 Elsevier Inc. All rights reserved.","Distributed systems; Divisible load scheduling; Game theory; Mechanism design"
"Constructing minimum extended weakly-connected dominating sets for clustering in ad hoc networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455173984&doi=10.1016%2fj.jpdc.2011.07.002&partnerID=40&md5=e35f6c2e6fc711316cce923e509f67ce","Motivated by cooperative communication in ad hoc networks, Wu et al. proposed extended dominating set (EDS) where each node in an ad hoc network is covered by either a dominating neighbor or several 2-hop dominating neighbors, and defined two types of dominating sets: extended strongly connected dominating set (ECDS) and extended weakly connected dominating set (EWCDS), according to the success of a broadcast process. An EWCDS is an effective method for clustering. In this paper, we extend the dominative capabilities of nodes such that each forward node dominates not only itself and its regular neighbors fully, but also its quasi-neighbors partly. Based on this extension, three novel algorithms to find EWCDSs in ad hoc networks are proposed. The correctness and performance of our algorithms are confirmed through theoretical analysis and comprehensive simulations. © 2011 Elsevier Inc. All rights reserved.","Ad hoc networks; Clustering; Dominative capability; Extended dominating set; Extended weakly connected dominating set; Simulation"
"URL: A unified reinforcement learning approach for autonomic cloud management","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855359756&doi=10.1016%2fj.jpdc.2011.10.003&partnerID=40&md5=80afe8e7e937118ee4fb9d6d2e7b8341","Cloud computing is emerging as an increasingly important service-oriented computing paradigm. Management is a key to providing accurate service availability and performance data, as well as enabling real-time provisioning that automatically provides the capacity needed to meet service demands. In this paper, we present a unified reinforcement learning approach, namely URL, to automate the configuration processes of virtualized machines and appliances running in the virtual machines. The approach lends itself to the application of real-time autoconfiguration of clouds. It also makes it possible to adapt the VM resource budget and appliance parameter settings to the cloud dynamics and the changing workload to provide service quality assurance. In particular, the approach has the flexibility to make a good trade-off between system-wide utilization objectives and appliance-specific SLA optimization goals. Experimental results on Xen VMs with various workloads demonstrate the effectiveness of the approach. It can drive the system into an optimal or near-optimal configuration setting in a few trial-and-error iterations. © 2011 Elsevier Inc. All rights reserved.","Cloud computing; Reinforcement learning; Virtual machine autoconfiguration"
"Parallel discovery of network motifs","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855354933&doi=10.1016%2fj.jpdc.2011.08.007&partnerID=40&md5=75f91ad247f0e93b82d263d865cbbf30","Many natural structures can be naturally represented by complex networks. Discovering network motifs, which are overrepresented patterns of inter-connections, is a computationally hard task related to graph isomorphism. Sequential methods are hindered by an exponential execution time growth when we increase the size of motifs and networks. In this article we study the opportunities for parallelism in existing methods and propose new parallel strategies that adapt and extend one of the most efficient serial methods known from the Fanmod tool. We propose both a masterworker strategy and one with distributed control, in which we employ a randomized receiver initiated methodology capable of providing dynamic load balancing during the whole computation process. Our strategies are capable of dealing both with exact and approximate network motif discovery. We implement and apply our algorithms to a set of representative networks and examine their scalability up to 128 processing cores. We obtain almost linear speedups, showcasing the efficiency of our proposed approach and are able to reach motif sizes that were not previously achievable using conventional serial algorithms. © 2011 Elsevier Inc. All rights reserved.","Complex networks; Graph mining; Network motifs; Parallel algorithms"
"Proactive process-level live migration and back migration in HPC environments","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855350553&doi=10.1016%2fj.jpdc.2011.10.009&partnerID=40&md5=6f48cc680f4f067f7cde0543aca879a8","As the number of nodes in high-performance computing environments keeps increasing, faults are becoming common place. Reactive fault tolerance (FT) often does not scale due to massive I/O requirements and relies on manual job resubmission. This work complements reactive with proactive FT at the process level. Through health monitoring, a subset of node failures can be anticipated when one's health deteriorates. A novel process-level live migration mechanism supports continued execution of applications during much of process migration. This scheme is integrated into an MPI execution environment to transparently sustain health-inflicted node failures, which eradicates the need to restart and requeue MPI jobs. Experiments indicate that 16.5 s of prior warning are required to successfully trigger live process migration while similar operating system virtualization mechanisms require 1324 s. This self-healing approach complements reactive FT by nearly cutting the number of checkpoints in half when 70% of the faults are handled proactively. The work also provides a novel back migration approach to eliminate load imbalance or bottlenecks caused by migrated tasks. Experiments indicate the larger the amount of outstanding execution, the higher the benefit due to back migration. © 2011 Elsevier Inc. All rights reserved.","Back migration; Fault tolerance; Health monitoring; High-performance computing; Live migration"
"Upper bounds on the connection probability for 2-D meshes and tori","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855339886&doi=10.1016%2fj.jpdc.2011.11.006&partnerID=40&md5=64ac237b14b0e74a21f0a792ca057642","Mesh is an important and popular interconnection network topology for large parallel computer systems. A mesh can be divided into submeshes to obtain the upper bounds on the connection probability for the mesh. Combinatorial techniques are used to get closer upper bounds on the connection probability for 2-D meshes compared with the existing upper bounds we have known. Simulation results of meshes of various sizes show that our upper bounds are close to the exact connection probability. The combinatorial methods and tools used in this paper can be used to study the connection probabilities for other networks. © 2011 Elsevier Inc. All rights reserved.","Connection probability; Fault tolerance; Mesh; Parallel computer system; Submesh; Torus"
"Designing fast LTL model checking algorithms for many-core GPUs","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864845053&doi=10.1016%2fj.jpdc.2011.10.015&partnerID=40&md5=36557d7b11f0a5b8991fa22eb880c56a","Recent technological developments made various many-core hardware platforms widely accessible. These massively parallel architectures have been used to significantly accelerate many computation demanding tasks. In this paper, we show how the algorithms for LTL model checking can be redesigned in order to accelerate LTL model checking on many-core GPU platforms. Our detailed experimental evaluation demonstrates that using the NVIDIA CUDA technology results in a significant speedup of the verification process. Together with state space generation based on shared hash-table and DFS exploration, our CUDA accelerated model checker is the fastest among state-of-the-art shared memory model checking tools. The effective utilization of the CUDA technology, however, is quite often reduced by the costly preparation of suitable data structures and limited to small or middle-sized instances due to space limitations, which is also the case of our CUDA-aware LTL model checking solutions. Hence, we further suggest how to overcome these limitations by multi-core construction of the compact data structures and by employing multiple CUDA devices for acceleration of fine-grained communication-intensive parallel algorithms for LTL model checking. © 2011 Elsevier Inc. All rights reserved.","CUDA technology; Linear temporal logic; Massively parallel architectures; Multiple CUDA devices; Parallel model checking"
"Power efficient rate monotonic scheduling for multi-core systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455173943&doi=10.1016%2fj.jpdc.2011.07.005&partnerID=40&md5=e3539aa8ff9b033f0d29db0f60b2c42a","More computational power is offered by current real-time systems to cope with CPU intensive applications. However, this facility comes at the price of more energy consumption and eventually higher heat dissipation. As a remedy, these issues are being encountered by adjusting the system speed on the fly so that application deadlines are respected and also, the overall system energy consumption is reduced. In addition, the current state of the art of multi-core technology opens further research opportunities for energy reduction through power efficient scheduling. However, the multi-core front is relatively unexplored from the perspective of task scheduling. To the best of our knowledge, very little is known as of yet to integrate power efficiency component into real-time scheduling theory that is tailored for multi-core platforms. In this paper, we first propose a technique to find the lowest core speed to schedule individual tasks. The proposed technique is experimentally evaluated and the results show the supremacy of our test over the existing counterparts. Following that, the lightest task shifting policy is adapted for balancing core utilization, which is utilized to determine the uniform system speed for a given task set. The aforementioned guarantees that: (i) all the tasks fulfill their deadlines and (ii) the overall system energy consumption is reduced. © 2011 Elsevier Inc. All rights reserved.","Feasibility analysis; Fixed-priority scheduling; Non-preemptive scheduling; Online schedulability tests; Real-time systems"
"PC3: Principal Component-based Context Compression: Improving energy efficiency in wireless sensor networks","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855353422&doi=10.1016%2fj.jpdc.2011.10.001&partnerID=40&md5=78e2be67b6868928720078912a867db8","We focus on energy efficiency, which guarantees the operation of a Wireless Sensor Network for long. We propose a context compression model that works in an orthogonal fashion. We first reduce the dimensions of multivariate contextual information. This is achieved through the Principal Component Analysis (PCA), which determines the statistical dependencies between the different contextual components. We then suppress the transmission of the determined principal components through an extrapolation scheme that exploits the properties of each individual component. Our findings are quite promising for the broader domain of WSN-based application engineering and context awareness. © 2011 Elsevier Inc. All rights reserved.","Context compression; Energy efficiency; Extrapolation; Principal component analysis; Wireless sensor network"
"Automatic parallelisation for LTI MIMO state space systems using FPGAs. An optimisation for cost & performance","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2012.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862022815&doi=10.1016%2fj.jpdc.2012.04.009&partnerID=40&md5=443bb00ac162126947275abe87557a2d","The parallelism attained by the use of Field Programmable Gate Arrays (FPGAs) has shown remarkable potential for accelerating control systems applications. This comes at a time when well established methods based on inherited serial Central Processor Units (CPUs) cannot guarantee solutions for the increasing execution speed demands. However, the transition from serial to parallel architectures represents a tremendous challenge due to overwhelming numbers of unexplored options and conflicting factors. The work presented achieves a parallelisation characterisation for generic MIMO systems using stand-alone FPGA implementations. The main contribution is that a very fine subset of possible serial/parallel implementations is obtained. This is used to achieve a flexible trade-off between cost and performance. Automatic optimisation of latency, occupied FPGA area and execution speed is attained and justified in respect to most of the feasible scenarios. © 2012 Elsevier Inc. All rights reserved.","Automation; Control systems; FPGA; Optimisation; Parallel computing; Pipelining; Resource sharing; State space"
"Shot boundary detection using Zernike moments in Multi-GPU Multi-CPU architectures","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865716925&doi=10.1016%2fj.jpdc.2011.10.011&partnerID=40&md5=80faa5e3a69e82849280a4f2d31c1e07","This paper presents an analysis of a Multi-GPU Multi-CPU environment, along with the different possible hybrid combinations. The analysis has been performed for a shot boundary detection application, based on Zernike moments, although it is general enough to be applied to many different application areas. A deep study of the performance, bottlenecks and design challenges is carried out showing the validity of this approach and achieving very high frame per second rates. In this paper, Zernike calculations are carried out on GPUs, taking advantage of a packing strategy proposed to minimize host-device communication time. © 2011 Elsevier Inc. All rights reserved.","Multi-GPU multi-CPU environments; Shot boundary detection; Zernike moments in GPU"
"Orthogonal drawings and crossing numbers of the Kronecker product of two cycles","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855357644&doi=10.1016%2fj.jpdc.2011.11.009&partnerID=40&md5=bab4fde147c1ce9025946a5025d2ca0e","An orthogonal drawing of a graph is an embedding of the graph in the plane such that each edge is representable as a chain of alternately horizontal and vertical line segments. This style of drawing finds applications in areas such as optoelectronic systems, information visualization and VLSI circuits. We present orthogonal drawings of the Kronecker product of two cycles around vertex partitions of the graph into grids. In the process, we derive upper bounds on the crossing number of the graph. The resulting upper bounds are within a constant multiple of the lower bounds. Unlike the Cartesian product that is amenable to an inductive treatment, the Kronecker product entails a case-to-case analysis since the results depend heavily on the parameters corresponding to the lengths of the two cycles. © 2011 Elsevier Inc. All rights reserved.","Cartesian product; Crossing number; Cycles; Graph algorithms; Graph minor; Grids; Kronecker product; Orthogonal drawing; Vertex partition"
"QoS and preemption aware scheduling in federated and virtualized Grid computing environments","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855352196&doi=10.1016%2fj.jpdc.2011.10.008&partnerID=40&md5=ee4959087057253b80d7b0046d4bb0d4","Resource provisioning is one of the challenges in federated Grid environments. In these environments each Grid serves requests from external users along with local users. Recently, this resource provisioning is performed in the form of Virtual Machines (VMs). The problem arises when there are insufficient resources for local users to be served. The problem gets complicated further when external requests have different QoS requirements. Serving local users could be solved by preempting VMs from external users which impose overheads on the system. Therefore, the question is how the number of VM preemptions in a Grid can be minimized. Additionally, how we can decrease the likelihood of preemption for requests with more QoS requirements. We propose a scheduling policy in InterGrid, as a federated Grid, which reduces the number of VM preemptions and dispatches external requests in a way that fewer requests with QoS constraints get affected by preemption. Extensive simulation results indicate that the number of VM preemptions is decreased at least by 60%, particularly, for requests with more QoS requirements. © 2011 Elsevier Inc. All rights reserved.","Federated Grid; Preemption; Quality of Service (QoS); Queuing model; Scheduling; Virtual Machine (VM)"
"Business-driven short-term management of a hybrid IT infrastructure","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855354992&doi=10.1016%2fj.jpdc.2011.11.001&partnerID=40&md5=3ad7a94952eacff79997716dd1c3d4f4","We consider the problem of managing a hybrid computing infrastructure whose processing elements are comprised of in-house dedicated machines, virtual machines acquired on-demand from a cloud computing provider through short-term reservation contracts, and virtual machines made available by the remote peers of a best-effort peer-to-peer (P2P) grid. Each of these resources has different cost basis and associated quality of service guarantees. The applications that run in this hybrid infrastructure are characterized by a utility function: the utility gained with the completion of an application depends on the time taken to execute it. We take a business-driven approach to manage this infrastructure, aiming at maximizing the profit yielded, that is, the utility produced as a result of the applications that are run minus the cost of the computing resources that are used to run them. We propose a heuristic to be used by a contract planner agent that establishes the contracts with the cloud computing provider to balance the cost of running an application and the utility that is obtained with its execution, with the goal of producing a high overall profit. Our analytical results show that the simple heuristic proposed achieves very high relative efficiency in the use of the hybrid infrastructure. We also demonstrate that the ability to estimate the grid behaviour is an important condition for making contracts that allow such relative efficiency values to be achieved. On the other hand, our simulation results with realistic error predictions show only a modest improvement in the profit achieved by the simple heuristic proposed, when compared to a heuristic that does not consider the grid when planning contracts, but uses it, and another that is completely oblivious to the existence of the grid. This calls for the development of more accurate predictors for the availability of P2P grids, and more elaborated heuristics that can better deal with the several sources of non-determinism present in this hybrid infrastructure. © 2011 Elsevier Inc. All rights reserved.","Business-driven IT management; Capacity planning; Cloud computing; Grid computing; Peer-to-peer; Short-term management"
"Accelerating knowledge-based energy evaluation in protein structure modeling with Graphics Processing Units","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855346430&doi=10.1016%2fj.jpdc.2011.10.005&partnerID=40&md5=6c739bc07b3282d92e08202f5f2c2ad1","Evaluating the energy of a protein molecule is one of the most computationally costly operations in many protein structure modeling applications. In this paper, we present an efficient implementation of knowledge-based energy functions by taking advantage of the recent Graphics Processing Unit (GPU) architectures. We use DFIRE, a knowledge-based all-atom potential, as an example to demonstrate our GPU implementations on the latest NVIDIA Fermi architecture. A load balancing workload distribution scheme is designed to assign computations of pair-wise atom interactions to threads to achieve perfect or near-perfect load balancing in the symmetric N-body problem in DFIRE. Reorganizing atoms in the protein also improves the cache efficiency in Fermi GPU architecture, which is particularly effective for small proteins. Our DFIRE implementation on GPU (GPU-DFIRE) has exhibited a speedup of up to ∼150 on NVIDIA Quadro FX3800M and ∼250 on NVIDIA Tesla M2050 compared to the serial DFIRE implementation on CPU. Furthermore, we show that protein structure modeling applications, including a Monte Carlo sampling program and a local optimization program, can benefit from GPU-DFIRE with little programming modification but significant computational performance improvement. © 2011 Elsevier Inc. All rights reserved.","GPU; Knowledge-based Energy; Protein modeling; Symmetric N-body Problem"
"Immediate detection of predicates in pervasive environments","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855349964&doi=10.1016%2fj.jpdc.2011.09.004&partnerID=40&md5=7ad529a43a19384a81e1ba67211f3c92","An important problem in pervasive environments is detecting predicates on sensed variables in an asynchronous distributed setting to determine context and to respond. We do not assume the availability of synchronized physical clocks because they may not be available or may be too expensive for predicate detection in such environments with a (relatively) low event occurrence rate. We address the problem of detecting each occurrence of a global predicate, at the earliest possible instant, by proposing a suite of three on-line middleware protocols having varying degrees of accuracy. We analyze the degree of accuracy for the proposed protocols. The extent of false negatives and false positives is determined by the run-time message processing latencies. © 2011 Elsevier Inc. All rights reserved.","On-line algorithms; Pervasive computing; Predicate detection; Sensor networks"
"Data layout optimization for multi-valued containers in OpenCL","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865750562&doi=10.1016%2fj.jpdc.2011.10.012&partnerID=40&md5=07c74302031eca301b3f14ea6700e210","Scientific data is mostly multi-valued, e.g., coordinates, velocities, moments or feature components, and it comes in large quantities. The data layout of such containers has an enormous impact on the achieved performance, however, layout optimization is very time-consuming and error-prone because container access syntax in standard programming languages is not sufficiently abstract. This means that changing the data layout of a container necessitates syntax changes in all parts of the code where the container is used. Object oriented languages allow to solve this problem by hiding the data layout behind a class interface. However, the additional coding effort is enormous in comparison to a simple structure. A clever coding pattern, previously presented by the author, significantly reduces the code overhead, however, it relies heavily on advanced C++ features, a language that is not supported on most accelerators. This paper develops a concise macro based solution that requires only support for structures and unions and can therefore be utilized in OpenCL, a widely supported programming language for parallel processors. This enables the development of high performance code without an a-priori commitment to a certain layout and includes the possibility to optimize it subsequently. This feature is used to identify the best data layouts for different processing patterns of multi-valued containers on a multi-GPU system. © 2011 Elsevier Inc. All rights reserved.","AoS; Array of structures; Array of structures of arrays; ASA; Data layout; Multi-component; Multi-GPU; Multi-valued; OpenCL; SoA; Structure of arrays"
"GPU-based parallel algorithms for sparse nonlinear systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.10.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865755711&doi=10.1016%2fj.jpdc.2011.10.016&partnerID=40&md5=a51c03322a5abdb095266cad38dad4ad","In this work we describe some parallel algorithms for solving nonlinear systems using CUDA (Compute Unified Device Architecture) over a GPU (Graphics Processing Unit). The proposed algorithms are based on both the Fletcher-Reeves version of the nonlinear conjugate gradient method and a polynomial preconditioner type based on block two-stage methods. Several strategies of parallelization and different storage formats for sparse matrices are discussed. The reported numerical experiments analyze the behavior of these algorithms working in a fine grain parallel environment compared with a thread-based environment. © 2011 Elsevier Inc. All rights reserved.","Bratu problem; GPGPU; GPU libraries; Multicore architectures; Nonlinear conjugate gradient algorithms; Parallel preconditioners"
"An MPI-CUDA implementation of an improved Roe method for two-layer shallow water systems","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865753974&doi=10.1016%2fj.jpdc.2011.07.012&partnerID=40&md5=99b349aed2f72d29e0f35caf66ad18ea","The numerical solution of two-layer shallow water systems is required to simulate accurately stratified fluids, which are ubiquitous in nature: they appear in atmospheric flows, ocean currents, oil spills, etc. Moreover, the implementation of the numerical schemes to solve these models in realistic scenarios imposes huge demands of computing power. In this paper, we tackle the acceleration of these simulations in triangular meshes by exploiting the combined power of several CUDA-enabled GPUs in a GPU cluster. For that purpose, an improvement of a path conservative Roe-type finite volume scheme which is specially suitable for GPU implementation is presented, and a distributed implementation of this scheme which uses CUDA and MPI to exploit the potential of a GPU cluster is developed. This implementation overlaps MPI communication with CPU-GPU memory transfers and GPU computation to increase efficiency. Several numerical experiments, performed on a cluster of modern CUDA-enabled GPUs, show the efficiency of the distributed solver. © 2011 Elsevier Inc. All rights reserved.","CUDA; Finite volume schemes; GPU cluster computing; MPI; Shallow water simulation; Unstructured meshes"
"A high performance multiple sequence alignment system for pyrosequencing reads from multiple reference genomes","2012","Journal of Parallel and Distributed Computing","10.1016/j.jpdc.2011.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455122790&doi=10.1016%2fj.jpdc.2011.08.001&partnerID=40&md5=78b6202be19c426196e23635b3af1042","Genome resequencing with short reads generated from pyrosequencing generally relies on mapping the short reads against a single reference genome. However, mapping of reads from multiple reference genomes is not possible using a pairwise mapping algorithm. In order to align the reads w.r.t each other and the reference genomes, existing multiple sequence alignment(MSA) methods cannot be used because they do not take into account the position of these short reads with respect to the genome, and are highly inefficient for a large number of sequences. In this paper, we develop a highly scalable parallel algorithm based on domain decomposition, referred to as P-Pyro-Align, to align such a large number of reads from single or multiple reference genomes. The proposed alignment algorithm accurately aligns the erroneous reads, and has been implemented on a cluster of workstations using MPI library. Experimental results for different problem sizes are analyzed in terms of execution time, quality of the alignments, and the ability of the algorithm to handle reads from multiple haplotypes. We report high quality multiple alignment of up to 0.5 million reads. The algorithm is shown to be highly scalable and exhibits super-linear speedups with increasing number of processors. © 2011 Elsevier Inc. All rights reserved.","Computational biology; Genome alignment and mapping; Multiple sequence alignment; Parallel algorithms; Pyrosequencing"
