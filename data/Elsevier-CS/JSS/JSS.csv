"Title","Year","Source title","DOI","Link","Abstract","Author Keywords"
"Empirical analysis of security-related code reviews in npm packages","2023","Journal of Systems and Software","10.1016/j.jss.2023.111752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163308334&doi=10.1016%2fj.jss.2023.111752&partnerID=40&md5=56348d189f61596194c48ce2e0465768","Security issues are a major concern in software packages and their impact can be detrimental if exploited. Modern code review is a widely-used practice that project maintainers adopt to improve the quality of contributed code. Prior work has shown that code review has an important role in improving software quality, however, in-depth analyses on code review in relation to security issues are limited. Therefore, in this paper, we aim to explore the role of code review in finding and mitigating security issues. In particular, we investigate active and popular npm packages to understand what types of security issues are raised during code review, and what kind of mitigation strategies are employed by package maintainers to address them. With pull requests (PRs) being the medium of code review under study, we analyze 171 PRs with raised security issues. We find that such issues are discussed at length by package maintainers. Moreover, we find that code review is effective at identifying certain types of security concerns, e.g., Race Condition, Access Control, and ReDOS, as dealing with such issues requires in-depth knowledge of the project domain and implementation specifics. Interestingly, we also observe that some projects have automated tools integrated in the project development cycle, which enhances the identification of frequent cases of certain security issues. When analyzing how maintainers respond to the raised security issues, we find that most of the issues (55%) are frequently addressed and mitigated. In other cases, security concerns ended up not being fixed or are ignored by project maintainers. Leveraging our findings, we offer several implications for project maintainers to support the role of reviewing code in finding and fixing security concerns. © 2023 Elsevier Inc.","Code review; Open source software; Security; Third-party package"
"Study the correlation between the readme file of GitHub projects and their popularity","2023","Journal of Systems and Software","10.1016/j.jss.2023.111806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165923275&doi=10.1016%2fj.jss.2023.111806&partnerID=40&md5=7010b3d76320dd41af8a4a987a53c9d7","A readme file plays an important role in a GitHub repository to provide a starting point for developers to reuse and make contributions. A good readme could provide sufficient information for users to learn and start a GitHub repository and might be correlated to the popularity of a repository. Given the importance of the role that a readme file plays, we aim to study to understand the correlation between the readme file of GitHub repositories and their popularity. We analyze readme files of 5,000 GitHub repositories across more than 20 languages. We study the relationship between readme file related factors and the popularity of GitHub repositories. We observe that: (1) Most of the studied readme file related factors (e.g., the number of lists, the number and frequency of updates on the readme file) are statistically significantly different between popular and non-popular repositories with non-negligible effect size. (2) After controlling repository-specific factors (e.g., repository topics and license information), the number of lists and the frequency of updates are the most significantly important factors that discriminate between popular and non-popular repositories. (3) The most of updates were made to update references in popular repositories, while in non-popular repositories most updates are for the content of how to use the repository. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Documentation; Github; Random forest; Readme file"
"A comprehensive evaluation of SZZ Variants through a developer-informed oracle","2023","Journal of Systems and Software","10.1016/j.jss.2023.111729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159063717&doi=10.1016%2fj.jss.2023.111729&partnerID=40&md5=2b6be7305a35ff2e53197a50dabb2ed2","Automatically linking bug-fixing changes to bug-inducing ones (BICs) is one of the key data-extraction steps behind several empirical studies in software engineering. The SZZ algorithm is the de facto standard to achieve this goal, with several improvements proposed over time. Evaluating the performance of SZZ implementations is, however, far from trivial. In previous works, researchers (i) manually assessed whether the BICs identified by the SZZ implementation were correct or not, or (ii) defined oracles in which they manually determined BICs from bug-fixing commits. However, ideally, the original developers should be involved in defining a labeled dataset to evaluate SZZ implementations. We propose a methodology to define a “developer-informed” oracle for evaluating SZZ implementations, without requiring a manual inspection from the original developers. We use Natural Language Processing (NLP) to identify bug-fixing commits in which developers explicitly reference the commit(s) that introduced the fixed bug. We use the built oracle to extensively evaluate existing SZZ variants defined in the literature. We also introduce and evaluate two new variants aimed at addressing two weaknesses we observed in state-of-the-art implementations (i.e., processing added lines and handling of revert commits). © 2023 Elsevier Inc.","Defect prediction; Empirical study; SZZ"
"A hybrid grey wolf optimizer using opposition-based learning, sine cosine algorithm and reinforcement learning for reliable scheduling and resource allocation","2023","Journal of Systems and Software","10.1016/j.jss.2023.111801","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166227482&doi=10.1016%2fj.jss.2023.111801&partnerID=40&md5=e01f0109c9bb77785ae5b4973761aecc","As the number of space debris in geosynchronous Earth orbits continues to grow, the threat posed by space debris to satellites surveillance is increasing, and the available orbital resources are also decreasing. Thus, reasonably scheduling and allocating the resources for space object tracking has become vital. This paper establishes an optimization model for the resource allocation and scheduling problem for space debris tracking. A fusion algorithm that combines the grey wolf optimizer, opposition-based learning, sine cosine search strategy, and reinforcement learning was proposed and used to solve the problem. Six groups of realistic data were selected based on the relevant background information of space debris tracking to test the validity and effectiveness of the proposed algorithm. The performance of the state-of-the-art optimization algorithms was compared with that of the proposed algorithms. The result of the experiment indicates that the proposed algorithm effectively solves the resource allocation and scheduling problem for space debris tracking. © 2023 Elsevier Inc.","Grey wolf optimizer; Optimized scheduling; Reinforcement learning; Resource allocation; Space debris tracking"
"On the relation of method popularity to breaking changes in the Maven ecosystem","2023","Journal of Systems and Software","10.1016/j.jss.2023.111738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160015773&doi=10.1016%2fj.jss.2023.111738&partnerID=40&md5=6ff3fe1e0ec1cd4e68435cf2fb7f6ca1","Software reuse is a common practice in modern software engineering to save time and energy while accelerating software delivery. Dependency managers like MAVEN offer a large ecosystem of reusable libraries that build the backbone of software reuse. Breaking changes, i.e., when an update to a library introduces incompatible changes that break existing client programs, are troublesome barriers to this library reuse. Semantic Versioning has been proposed as a practice to make it easier for the users to find safe updates by encoding the change impact in the version number. While this practice is widely studied from the framework perspective, no detailed insights exist yet into the ecosystem perspective. In this work, we study violations of semantic versioning in the MAVEN ecosystem for 13,876 versions of 384 artifacts to better understand the impact these violations have on the 7,190 dependent versioned packages. We found that 67% of the artifacts introduce at least one type of semantic versioning violation, either a breaking change or an illegal API extension in their history. An impact analysis on breaking methods that (direct or transitive) dependents reference, revealed strong centralization: 87% of publicly accessible methods are never used by dependents and among methods with at least one usage, half of the unique calls from dependents concentrate on only 35% of the defined methods. We also studied method popularity and could not find an indication that popularity affects stability: even popular methods break frequently. Overall, we confirm the previous result that Semantic Versioning is violated repeatedly in practice. Our results suggest that the frequency of breaking changes might be a sign of insufficient change-impact awareness on the ecosystem and we believe that developers require more adequate information, like method popularity, to improve their update strategies. © 2023 The Author(s)","API evolution; Breaking changes; Compatibility; Maven Central; Method popularity; Semantic versioning"
"Studying the association between Gitcoin's issues and resolving outcomes","2023","Journal of Systems and Software","10.1016/j.jss.2023.111835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171843513&doi=10.1016%2fj.jss.2023.111835&partnerID=40&md5=e6f3f62f381539f11100a4af04c543f0","The development of open-source software (OSS) projects usually have been driven through collaborations among contributors and strongly relies on volunteering. Thus, allocating software practitioners (e.g., contributors) to a particular task is non-trivial and draws attention away from the development. Therefore, a number of bug bounty platforms have emerged to address this problem through bounty rewards. Especially, Gitcoin, a new bounty platform, introduces a bounty reward mechanism that allows individual issue owners (backers) to define a reward value using cryptocurrencies rather than using crowdfunding mechanisms. Although a number of studies have investigated the phenomenon on bounty platforms, those rely on different bounty reward systems. Our study thus investigates the association between the Gitcoin bounties and their outcomes (i.e., success and non-success). We empirically study over 4,000 issues with Gitcoin bounties using statistical analysis and machine learning techniques. We also conducted a comparative study with the Bountysource platform to gain insights into the usage of both platforms. Our study highlights the importance of factors such as the length of the project, issue description, type of bounty issue, and the bounty value, which are found to be highly correlated with the outcome of bounty issues. These findings can provide useful guidance to practitioners. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Bounty platform; Issue-addressing outcome; Open source software development"
"How do microservices evolve? An empirical analysis of changes in open-source microservice repositories","2023","Journal of Systems and Software","10.1016/j.jss.2023.111788","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163866409&doi=10.1016%2fj.jss.2023.111788&partnerID=40&md5=ebfcf31cec6013cc0427f7f2024da945","Context.: Microservice architectures are an emergent service-oriented paradigm widely used in industry to develop and deploy scalable software systems. The underlying idea is to design highly independent services that implement small units of functionality and can interact with each other through lightweight interfaces. Objective.: Even though microservices are often used with success, their design and maintenance pose novel challenges to software engineers. In particular, it is questionable whether the intended independence of microservices can actually be achieved in practice. Method.: So, it is important to understand how and why microservices evolve during a system's life-cycle, for instance, to scope refactorings and improvements of a system's architecture or to develop supporting tools. To provide insights into how microservices evolve, we report a large-scale empirical study on the (co-)evolution of microservices in 11 open-source systems, involving quantitative and qualitative analyses of 7,319 commits. Findings.: Our quantitative results show that there are recurring patterns of (co-)evolution across all systems, for instance, “shotgun surgery” commits and microservices that are largely independent, evolve in tuples, or are evolved in almost all changes. We refine our results by analyzing service-evolving commits qualitatively to explore the (in-)dependence of microservices and the causes for their specific evolution. Conclusion.: The contributions in this article provide an understanding for practitioners and researchers on how microservices evolve in what way, and how microservice-based systems may be improved. © 2023 The Author(s)","Microservices; Mining software repositories; Service-oriented architecture; Software architecture; Software evolution"
"Sentiment overflow in the testing stack: Analyzing software testing posts on Stack Overflow","2023","Journal of Systems and Software","10.1016/j.jss.2023.111804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166229419&doi=10.1016%2fj.jss.2023.111804&partnerID=40&md5=e8285035de9eb4eadb3627db61abc8f3","Software testing is an integral part of modern software engineering practice. Past research has not only underlined its significance, but also revealed its multi-faceted nature. The practice of software testing and its adoption is influenced by many factors that go beyond tools or technology. This paper sets out to investigate the context of software testing from the practitioners’ point of view by mining and analyzing sentimental posts on the widely used question and answer website Stack Overflow. By qualitatively analyzing sentimental expressions of practitioners, which we extract from the Stack Overflow dataset using sentiment analysis tools, we discern factors that help us to better understand the lived experience of software engineers with regards to software testing. Grounded in the data that we have analyzed, we argue that sentiments like insecurity, despair and aspiration, have an impact on practitioners’ attitude towards testing. We suggest that they are connected to concrete factors like the level of complexity of projects in which software testing is practiced. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)","Grounded theory; Sentiment analysis; Software testing; Stack Overflow"
"Deep learning with class-level abstract syntax tree and code histories for detecting code modification requirements","2023","Journal of Systems and Software","10.1016/j.jss.2023.111851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172183926&doi=10.1016%2fj.jss.2023.111851&partnerID=40&md5=f1f52cdab48b52afce74996ab966039c","Improving code quality is one of the most significant issues in the software industry. Deep learning is an emerging area of research for detecting code smells and addressing refactoring requirements. The aim of this study is to develop a deep learning-based system for code modification analysis to predict the locations and types of code modifications, while significantly reducing the need for manual labeling. We created an experimental dataset by collecting historical code data from open-source project repositories on the Internet. We introduce a novel class-level abstract syntax tree-based code embedding method for code analysis. A recurrent neural network was employed to effectively identify code modification requirements. Our system achieves an average accuracy of approximately 83% across different repositories and 86% for the entire dataset. These findings indicate that our system provides higher performance than the method-based and text-based code embedding approaches. In addition, we performed a comparative analysis with a static code analysis tool to justify the readiness of the proposed model for deployment. The correlation coefficient between the outputs demonstrates a significant correlation of 67%. Consequently, this research highlights that the deep learning-based analysis of code histories empowers software teams in identifying potential code modification requirements. © 2023 Elsevier Inc.","Abstract syntax tree; Code embedding; Code smell; Recurrent neural network; Refactoring"
"BPEL process defects prediction using multi-objective evolutionary search","2023","Journal of Systems and Software","10.1016/j.jss.2023.111767","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163204896&doi=10.1016%2fj.jss.2023.111767&partnerID=40&md5=0443ce816de90f6a5a548b35cc0cdbc1","Web services are becoming increasingly popular technologies for modern organizations to improve their cooperation and collaboration through building new software systems by composing pre-built services. Such services are typically composed and executed through BPEL (Business Process Execution Language) processes. Like any other software artifact, such processes are frequently changed to add new or modify existing functionalities or adapt to environmental changes. However, poorly planned changes may introduce BPEL process design defects known as anti-patterns or defects. The presence of defects often leads to a regression in software quality. In this paper, we introduce an automated approach to predict the presence of defects in BPEL code using Multi-Objective Genetic Programming (MOGP). Our approach consists of learning from real-world instances of each service-based business process defect (i.e., anti-pattern) type to infer prediction rules based on the combinations of process metrics and their associated threshold values. We evaluate our approach based on a dataset of 178 real-world business processes that belong to various application domains, and a variety of BPEL process defect types such as data flow and portability defects. The statistical analysis of the achieved results shows the effectiveness of our approach in identifying defects compared with state-of-the-art techniques with a median accuracy of 91%. © 2023 Elsevier Inc.","Anti-patterns; BPEL process; Genetic programming; Multi-objective algorithms"
"On the roles of software testers: An exploratory study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161533534&doi=10.1016%2fj.jss.2023.111742&partnerID=40&md5=32cf17b739a0182f4848a93fb7b61ebb","Context: Software development organizations need testers with high skill levels in a broad range of technical areas and application domains. Accordingly, we need a better understanding of how testers meet such skill demands in the practice of their role. Objective: This work aims to deepen the understanding of the typical tester role. Method: We performed a thematic analysis of 19 in-depth, semi-structured interviews with software testers working in various industries. To investigate employers’ views on such roles, we conducted a thematic analysis of 400 job ads. Results: From the interviews, we identified five subroles of software testers: domain-specific tester, test automation specialist, test infrastructure specialist, user experience tester, and test manager. Most of the practitioners preferred to develop skills and act in one subrole. In contrast, most of the job ads requested that testers act in multiple subroles. Conclusion: Our findings provide a deeper understanding of the tester role, which may guide testers in their acquisition of skills and employers in the recruiting of testers. © 2023 The Author(s)","Agile testing; Empirical software engineering; Software testing career; Software testing job advertisements; Software testing skills"
"Enhancing Web Applications Observability through Instrumented Automated Browsers","2023","Journal of Systems and Software","10.1016/j.jss.2023.111723","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159086424&doi=10.1016%2fj.jss.2023.111723&partnerID=40&md5=7fba76052e9251436c0504ce8e8c3afc","In software engineering, observability is the ability to determine the current state of a software system based on its external outputs or signals such as metrics, logs, or traces. Web engineers rely on the web browser console as the primary tool to monitor the client-side of web applications during end-to-end tests. However, this is a manual and time-consuming task due to the different browsers available. This paper presents BrowserWatcher, an open-source browser extension providing cross-browser capabilities to observe web applications and automatically gather browser console logs in different browsers (e.g., Chrome, Firefox, or Edge). We have leveraged this extension to conduct an empirical study analyzing the browser console of the top-50 public websites manually and automatically. The results show that BrowserWatcher gathers all the well-known log categories such as console or error traces. It also reveals that each web browser additionally includes other types of logs, which differ among browsers, thus providing distinct pieces of information for the same website. © 2023 The Author(s)","Browser automation; Empirical study; Log gathering"
"Learning a holistic and comprehensive code representation for code summarization","2023","Journal of Systems and Software","10.1016/j.jss.2023.111746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160673325&doi=10.1016%2fj.jss.2023.111746&partnerID=40&md5=3a33399e9c4c8121d2b2cd98656886c7","Code summarization is the task of describing the function of code snippets in natural language, which benefits program comprehension and boosts software productivity. Despite lots of effort made by previous studies, existing models are not comprehensive enough to represent code, and yet the literature does not consider to model source code with API usage from a holistic perspective. To this end, this paper proposes a novel multi-modal code summarization approach called HCCS (Learning a Holistic and Comprehensive code representation for Code Summarization). We first design a neural network based on the graph attention mechanism to encode API Context Graph (ACG), which highlights holistic information of source code. Then, a multi-modal framework with a tree encoder for Abstract Syntax Tree (AST) and a code encoder for code tokens is incorporated to learn a more comprehensive code representation. Afterwards, we propose a fusing layer to integrate the encodings, which are then passed to a joint-decoder to generate summaries. The experimental results show that HCCS achieves better performance than the state of the arts (i.e., HCCS scores 9.5% higher in terms of BLEU metric and 11.46% in terms of BERTScore). As a result, HCCS is an effective approach to generate high-quality summaries. © 2023 Elsevier Inc.","API; Code summarization; Deep learning; Program comprehension"
"How are websites used during development and what are the implications for the coding process?","2023","Journal of Systems and Software","10.1016/j.jss.2023.111803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167426305&doi=10.1016%2fj.jss.2023.111803&partnerID=40&md5=2aed0f21a253808684af5238168714ed","Websites are frequently used to support the development process. This paper investigates how websites are used when writing code and programmers’ perceptions of the potential impact of this on their behaviour and the quality of the resulting software. We interviewed 18 programmers (13 students enrolled in undergraduate computer science courses, and 5 experienced professionals), and analysed the data thematically. The findings were used to develop a survey, which was distributed to 276 programmers (251 students, 25 experienced professionals). The results indicate that use of websites, especially Stack Overflow, is viewed as an essential part of programming by both students completing coursework and professionals developing code in industry. We also found that developers have experience of encountering a diverse set of problematic code snippets online, that copying code from websites without checking its quality or understanding how it worked is common, and that using online resources in this way had a potentially counter-productive effect on learning. Based on these findings, we make a number of recommendations, including better consideration of online code reuse in taught programmes, co-development and code-reuse practices in professional settings, and software licensing training for professional developers. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)","Computer science education; Human memory; Online code snippets; Problematic code; Professional practice; Stack Overflow"
"Optimal dynamic partial order reduction with context-sensitive independence and observers","2023","Journal of Systems and Software","10.1016/j.jss.2023.111730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159175956&doi=10.1016%2fj.jss.2023.111730&partnerID=40&md5=eb7fe3cbe8767d526e5cfe93b556f5d0","Dynamic Partial Order Reduction (DPOR) algorithms are used in stateless model checking of concurrent programs to avoid the exploration of equivalent execution sequences. In order to detect equivalence, DPOR relies on the notion of independence between execution steps. As this notion must be approximated, it can lose precision and thus treat execution steps as interfering when they are not. Our work is inspired by recent progress in the area that has introduced more accurate ways to exploit conditional notions of independence: Context-Sensitive DPOR considers two steps p and t independent in the current state if the states obtained by executing p⋅t and t⋅p are the same; Optimal DPOR with Observers makes their dependency conditional to the existence of future events that observe their operations. This article introduces a new algorithm, Optimal Context-Sensitive DPOR with Observers, that combines these two notions of conditional independence, and goes beyond them by exploiting their synergies. The implementation of our algorithm has been undertaken within the Nidhugg model checking tool. Our experimental evaluation, using benchmarks from the previous works, shows that our algorithm is able to effectively combine the benefits of both context-sensitive and observers-based independence and that it can produce exponential reductions over both of them. © 2023 The Author(s)","Concurrent programs; Partial order reduction; Software verification; Stateless model checking"
"Human factors in developing automated vehicles: A requirements engineering perspective","2023","Journal of Systems and Software","10.1016/j.jss.2023.111810","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166738978&doi=10.1016%2fj.jss.2023.111810&partnerID=40&md5=a7e6d5d79bc57f87ebe0adaee8fd4182","Automated Vehicle (AV) technology has evolved significantly both in complexity and impact and is expected to ultimately change urban transportation. Due to this evolution, the development of AVs challenges the current state of automotive engineering practice, as automotive companies increasingly include agile ways of working in their plan-driven systems engineering—or even transition completely to scaled-agile approaches. However, it is unclear how knowledge about human factors (HF) and technological knowledge related to the development of AVs can be brought together in a way that effectively supports today's rapid release cycles and agile development approaches. Based on semi-structured interviews with ten experts from industry and two experts from academia, this qualitative, exploratory case study investigates the relationship between HF and AV development. The study reveals relevant properties of agile system development and HF, as well as the implications of these properties for integrating agile work, HF, and requirements engineering. According to the findings, which were evaluated in a workshop with experts from academia and industry, a culture that values HF knowledge in engineering is key. These results promise to improve the integration of HF knowledge into agile development as well as to facilitate HF research impact and time to market. © 2023 The Author(s)","Agile; Automated vehicles; Human factors; Requirements engineering"
"Variable-strength combinatorial testing of exported activities based on misexposure prediction","2023","Journal of Systems and Software","10.1016/j.jss.2023.111773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162867818&doi=10.1016%2fj.jss.2023.111773&partnerID=40&md5=4686197b417a2852b9a99220bfceb5f5","Exported Activity (EA), a kind of activities in Android apps that can be launched by external components, is one of the most important inter-component communication (ICC) mechanisms. In combinatorial testing of EAs, although exhaustive testing of all possible combinations of input elements is ideal, it is often not feasible due to the combinatorial explosion of test cases. This paper presents ExaDroid, a novel variable-strength combinatorial testing framework for generating test suites for exported activities. ExaDroid is based on two observations: many activities are unintentionally exposed, and the complexity of input interactions in activities can be very limited. ExaDroid uses misexposure prediction and complexity analysis to decide the (default) testing strength of an EA. It also leverages input interactions to focus testing resources on important combinations by setting stronger (variable) test strengths on certain attributes. Our experiments have confirmed that ExaDroid is capable of trigger many unique crashes using a dozen or so test cases. The tool successfully found 100 unique crashes across 135 EAs in 30 apps, at an average cost of 14.2 test cases per EA. © 2023 Elsevier Inc.","Android application; Combinatorial testing; Exported activity; Robustness evaluation; Static analysis"
"An empirical study of software architecture resilience evaluation methods","2023","Journal of Systems and Software","10.1016/j.jss.2023.111726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158845284&doi=10.1016%2fj.jss.2023.111726&partnerID=40&md5=2967ab57145406641c747116224befe4","Resilience is one of the most essential quality properties of software systems, the resilience of software architectures plays an important role in the security of a software system. However, even though there are some methods have been proposed for evaluating the resilience of software architecture in the past few years, most of them are validated only by case studies with some specific application scenarios. We do not find a work which has provided a wide empirical verification and comparison of these different methods. To fill this gap, we explore and compare five typical software architecture resilience evaluation methods by experiments in this paper, and try to find which methods are better in which aspects. We have obtained the following findings: first, the five methods studied in this paper are effective and consistent in the trend of resilience change; secondly, the change of architecture resilience is actually related to the specific attributes and component relationships in the architecture; finally, systems designed in an object-oriented style are generally more resilient than most other design styles studied. © 2023 Elsevier Inc.","Empirical study; Evaluation methods; Software architecture resilience; “4+1” view model"
"A MILP model on coordinated coverage path planning system for UAV-ship hybrid team scheduling software","2023","Journal of Systems and Software","10.1016/j.jss.2023.111854","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172212966&doi=10.1016%2fj.jss.2023.111854&partnerID=40&md5=31df4d825c775a8c9030a3e6f7f0df13","Shipborne unmanned aerial vehicles (UAVs) are safer and more flexible for maritime missions, but frequent recharging is needed during long-term patrols. A coordinated system for path planning between ships and electric UAVs is necessary for efficient large-area coverage. A two-stage approach is proposed to minimize the makespan overall UAVs’ flight and the move distance overall ships combinationally. First, the target space is triangularized corresponding to the UAV camera field of view for generating air waypoints. Second, a MILP model is designed to connect suitable air waypoints for UAVs and marine waypoints for ship(s) to form the optimal path for them coordinating the requirements of the area coverage and the UAV recharging. The simulation experiments show the proposed model works for the scenario of either the static or the dynamic motherships in a unified way. In the static mode, the vessels are not migrated and the number of vessels and ship calling points required is the same. In the dynamic model, the ship can be repositioned to recover and recharge the drone, and the task can be accomplished simply by repositioning the ship between waypoints. Dynamic models have better interaction patterns than static models. © 2023 The Author(s)","Coverage path planning system; Mixed integer linear programming; Scheduling software; Shipborne electric UAV; UAV-ship hybrid team"
"Application of metamorphic testing on UAV path planning software","2023","Journal of Systems and Software","10.1016/j.jss.2023.111769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162921675&doi=10.1016%2fj.jss.2023.111769&partnerID=40&md5=6a18306ee6910f6e45bb3901df570d98","Both the performance and reliability evaluation processes of the unmanned aerial vehicle path planning software rely on the determination of the correctness of the execution results of unmanned aerial vehicle path planning software. However, this task is hindered due to the testing oracle problem. In this paper, a framework is designed to overcome the oracle problem and verify the correctness of path planning software based on the grid searching algorithms. In this framework, a metamorphic testing-based method is proposed, and three operations-based metamorphic relations are proposed and proved towards the target software. While analysis of the software is conducted, the version with manually injected faults as well as the officially released version are both dealt with. It is shown that in the experimental results the injected faults can be effectively revealed by using the methods proposed in this paper. Besides, through the evaluation of different types of metamorphic relations, we find that the composed metamorphic relations have stronger fault detection capability compared to the individual ones. © 2023 Elsevier Inc.","Metamorphic testing; Oracle problem; Path planning; Software test"
"Vulnerable smart contract function locating based on Multi-Relational Nested Graph Convolutional Network","2023","Journal of Systems and Software","10.1016/j.jss.2023.111775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162267698&doi=10.1016%2fj.jss.2023.111775&partnerID=40&md5=8ea6223403b3ddd776aac19b9b55d132","The immutable and trustable characteristics of blockchain enable smart contracts to be applied in various fields. Unfortunately, smart contracts are subject to various vulnerabilities, which are frequently exploited by attackers, causing financial damage to users. Therefore, it is extremely important to perform effective vulnerability detection and locating to ensure the security of smart contracts. Deep learning has shown great advantages in smart contract vulnerability detection due to its powerful end-to-end feature learning. The previous deep learning based approaches to smart contract vulnerability detection focus on identifying whether there are vulnerabilities in a smart contract. However, this kind of detection cannot achieve fine-grained vulnerability detection, i.e., locating which function in the smart contract is vulnerable. In this paper, we study the problem of vulnerable smart contract function locating. We construct a novel Multi-Relational Nested contract Graph (MRNG) to better characterize the rich syntactic and semantic information in the smart contract code, including the relationships between data and instructions. An MRNG represents a smart contract, where each node represents a function in the smart contract and each edge describes the calling relationship between the functions. In addition, we create a Multi-Relational Function Graph (MRFG) for each function, which characterizes the corresponding function code. That is, each function is characterized as an MRFG, which corresponds to a node in the MRNG. Each MRFG uses different types of edges to represent the different control and data relationships between nodes within a function. We also propose a Multi-Relational Nested Graph Convolutional Network (MRN-GCN) to process the MRNG. MRN-GCN first extracts and aggregates features from each MRFG, using the edge-enhanced graph convolution network and self-attention mechanism. The extracted feature vector is then assigned to the corresponding node in the MRNG to obtain a new Featured Contract Graph (FCG) for the smart contract. Graph convolution is used to further extract features from the FCG. Finally, a feed forward network with a Sigmoid function is used to locate the vulnerable functions. Experimental results on the real-world smart contract datasets show that model MRN-GCN can effectively improve the accuracy, precision, recall and F1-score performance of vulnerable smart contract function locating. © 2023 Elsevier Inc.","Graph neural network; Self-attention mechanism; Smart contract; Vulnerable function locating"
"Adaptive robustness evaluation for complex system prognostics and health management software platform","2023","Journal of Systems and Software","10.1016/j.jss.2023.111768","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162162958&doi=10.1016%2fj.jss.2023.111768&partnerID=40&md5=5c6ea2bae0a6870b1e05b3e4f01245bc","With the quantity and complexity of various complex systems have increased, their safety and reliability face many challenges. The prognostics and health management (PHM) software platform for complex systems is one of the important tools to ensure the reliability and safety of them. On the other hand, due to the uncertainty of sensing data, the robustness of PHM software platforms needs to be evaluated to ensure their performance. However, it is highly subjective and time-consuming for the existing robustness assessment methods which rely on expert experience. To solve these problems, a novel adaptive robustness evaluation method with Genetic Algorithm (GA) and Random Forest Algorithm (RFA) for the complex system PHM software platform is proposed. Firstly, the basic robustness indicators are extracted based on the classical metrics of PHM software platforms. Secondly, more sensitive indicators to the robustness of PHM software platforms are selected by GA from basic robustness indicators. Then, the classification is conducted by the selected indicators through RFA. Finally, the robustness of PHM software platforms is evaluated by the classification accuracy. Experiments with simulation data show that the proposed method has better performance, which is suitable for the robustness evaluation on the PHM software platform of complex systems. © 2023","Complex system; Machine learning; PHM software platform; Robustness evaluation"
"Psychometric instruments in software engineering research on personality: Status quo after fifty years","2023","Journal of Systems and Software","10.1016/j.jss.2023.111740","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160018803&doi=10.1016%2fj.jss.2023.111740&partnerID=40&md5=6c3bf1af66e928a08285dcb409569af9","Context: Although software development is a human activity, Software Engineering (SE) research has focused mostly on processes and tools, making human factors underrepresented. This kind of research may be improved using knowledge from human-focused disciplines. An example of missed opportunities is how SE employs psychometric instruments. Objective: Provide an overview of psychometric instruments in SE research regarding personality and provide recommendations for adopting them. Method: We conducted a systematic mapping to build an overview of instruments used within SE for assessing personality and reviewed their use from a multidisciplinary perspective of SE and social science. Results: We contribute with a secondary study covering fifty years of research (1970 to 2020). One of the most adopted instruments (MBTI) faces criticism within social sciences, and we identified discrepancies between its application and existing recommendations. We emphasize that several instruments refer to the Five-Factor Model, which despite its relevance in social sciences, has no specific advice for its application within SE. We discuss general advice for its proper application. Conclusion: The findings show that the adoption of psychometric instruments regarding personality in SE needs to be improved, ideally with the support of social science researchers. We believe that the review presented in this study can help to understand limitations and to evolve in this direction. © 2023 Elsevier Inc.","Behavioral software engineering; Mapping study; Personality"
"Test flakiness’ causes, detection, impact and responses: A multivocal review","2023","Journal of Systems and Software","10.1016/j.jss.2023.111837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172182727&doi=10.1016%2fj.jss.2023.111837&partnerID=40&md5=d398dcd83bb53f59b089bf9cdb1d46e8","Flaky tests (tests with non-deterministic outcomes) pose a major challenge for software testing. They are known to cause significant issues, such as reducing the effectiveness and efficiency of testing and delaying software releases. In recent years, there has been an increased interest in flaky tests, with research focusing on different aspects of flakiness, such as identifying causes, detection methods and mitigation strategies. Test flakiness has also become a key discussion point for practitioners (in blog posts, technical magazines, etc.) as the impact of flaky tests is felt across the industry. This paper presents a multivocal review that investigates how flaky tests, as a topic, have been addressed in both research and practice. Out of 560 articles we reviewed, we identified and analysed a total of 200 articles that are focused on flaky tests (composed of 109 academic and 91 grey literature articles/posts) and structured the body of relevant research and knowledge using four different dimensions: causes, detection, impact and responses. For each of those dimensions, we provide categorization and classify existing research, discussions, methods and tools With this, we provide a comprehensive and current snapshot of existing thinking on test flakiness, covering both academic views and industrial practices, and identify limitations and opportunities for future research. © 2023 The Author(s)","Flaky tests; Multivocal review; Non-deterministic tests; Software testing; Test bugs"
"GRuM — A flexible model-driven runtime monitoring framework and its application to automated aerial and ground vehicles","2023","Journal of Systems and Software","10.1016/j.jss.2023.111733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159586736&doi=10.1016%2fj.jss.2023.111733&partnerID=40&md5=e0d0b9938b43287b4df491e2bc144c34","Runtime monitoring is critical for ensuring safe operation and for enabling self-adaptive behavior of Cyber-Physical Systems (CPS). Monitors are established by identifying runtime properties of interest, creating probes to instrument the system, and defining constraints to be checked at runtime. For many systems, implementing and setting up a monitoring platform can be tedious and time-consuming, as generic monitoring platforms do not adequately cover domain-specific monitoring requirements. This situation is exacerbated when the System under Monitoring (SuM) evolves, requiring changes in the monitoring platform. Most existing approaches lack support for the automated generation and setup of monitors for diverse technologies and do not provide adequate support for dealing with system evolution. In this paper, we present GRuM (Generating CPS Runtime Monitors), a framework that combines model-driven techniques and runtime monitoring, to automatically generate a customized monitoring platform for a given SuM. Relevant properties are captured in a Domain Model Fragment, and changes to the SuM can be easily accommodated by automatically regenerating the platform code. To demonstrate the feasibility and performance we evaluated GRuM against two different systems using TurtleBot robots and Unmanned Aerial Vehicles. Results show that GRuM facilitates the creation and evolution of a runtime monitoring platform with little effort and that the platform can handle a substantial amount of events and data. © 2023 The Author(s)","Cyber-physical systems; Model-driven engineering; Runtime monitoring"
"Backsourcing of IT with focus on software development—A systematic literature review","2023","Journal of Systems and Software","10.1016/j.jss.2023.111771","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164225673&doi=10.1016%2fj.jss.2023.111771&partnerID=40&md5=b4c1364eb0462d21b5de6514e26d9187","Context: Backsourcing is the process of insourcing previously outsourced activities. Backsourcing can be a viable alternative when companies experience environmental or strategic changes, or challenges with outsourcing. While outsourcing and related processes have been extensively studied, few studies report experiences with backsourcing. Objectives: We summarize the results of the research literature on backsourcing of IT, with a focus on software development. By identifying practically relevant experience, we present findings that may help companies considering backsourcing. In addition, we identify gaps in the current research literature and point out areas for future work. Method: Our systematic literature review (SLR) started with a search for empirical studies on the backsourcing of IT. From each study, we identified the context in which backsourcing occurred, the factors leading to the decision, the backsourcing process, and the outcomes of backsourcing. We employed inductive coding to extract textual data from the papers and qualitative cross-case analysis to synthesize the evidence. Results: We identified 17 papers that reported 26 cases of backsourcing, six of which were related to software development. The cases came from a variety of contexts. The most common reasons for backsourcing were improving quality, reducing costs, and regaining control of outsourced activities. We model the backsourcing process as containing five sub-processes: change management, vendor relationship management, competence building, organizational build-up, and transfer of ownership. We identified 14 positive outcomes and nine negative outcomes of backsourcing. We also aggregated the evidence and detailed three relationships of potential use to companies considering backsourcing. Finally, we have highlighted the knowledge areas of software engineering associated with the backsourcing of software development. Conclusion: The backsourcing of IT is a complex process; its implementation depends on the prior outsourcing relationship and other contextual factors. Our systematic literature review contributes to a better understanding of this process by identifying its components and their relationships based on the peer-reviewed literature. Our results can serve as a motivation and baseline for further research on backsourcing and provide guidelines and process fragments from which practitioners can benefit when they engage in backsourcing. © 2023 Elsevier Inc.","Backsourcing; Information technology; Software development; Software engineering management; Systematic literature review"
"An Investigation of confusing code patterns in JavaScript","2023","Journal of Systems and Software","10.1016/j.jss.2023.111731","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159758958&doi=10.1016%2fj.jss.2023.111731&partnerID=40&md5=9535686f9656e5c98949c3bc5ab596c1","Evolving software is particularly challenging when the code has been poorly written or uses confusing idioms and language constructs, which might increase maintenance efforts and impose a significant cognitive load on developers. Previous research has investigated possible sources of confusion in programs, including the impact of small code patterns (hereafter atoms of confusion) that contribute to misunderstanding the source code. Although researchers have explored atoms of confusion in code written in C, C+ +, and Java, different languages have different features, developer communities, and development cultures. This justifies the exploration of other languages to verify whether they also exhibit confusion-inducing patterns. In this paper we investigate the impact of atoms of confusion on understanding JavaScript code—a dynamically typed language whose popularity is growing in the most diverse application domains. We present the results of a mixed-methods research comprising a mining software repositories (MSR) study, two experiments, and a set of interviews with practitioners. Our MSR effort shows that atom candidates are frequent and used intensively in 72 popular open-source JavaScript projects: four atom candidates appear in 90% of them and two of them occur more than once for every 100 lines of code. This helps motivate the other three studies. The results of both experiments suggest that two code patterns that have been previously observed to confuse C programmers also confuse JavaScript programmers: the comma operator and assignments being used as values. In addition, some code patterns, such as omitted curly braces and change of literal encoding, have caused confusion in participants in one of the experiments. We discover that some JavaScript-specific elements, such as automatic semicolon insertion and object destructuring, also have the potential to cause confusion. For all these cases effect sizes were either medium or high. The interviews we conducted indicate other constructs and idioms that merit investigation in the future. © 2023 Elsevier Inc.","Atoms of confusion; JavaScript code; Program comprehension; Program understanding; Software maintenance"
"Comparing the intensity of variability changes in software product line evolution","2023","Journal of Systems and Software","10.1016/j.jss.2023.111737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160023337&doi=10.1016%2fj.jss.2023.111737&partnerID=40&md5=8b0f0d374ebc7e93f1e25112cdc7a0cc","The evolution of a Software Product Line (SPL) typically affects multiple kinds of artifacts. The intensity (frequency and amount) in which developers change variability information in them was unknown, until we introduced a fine-grained approach for the variability-centric extraction and analysis of changes to code, build, and variability model artifacts. Its application to the commits of the Linux kernel revealed that changes to variability information occur infrequently and only affect small parts of the analyzed artifacts. Further, we outlined how these results may improve certain analysis and verification tasks during SPL evolution. However, the sole analysis of a single SPL did not allow for generic conclusions. In this paper, we extend our previous work to a comparative evolution analysis of four SPLs. We provide a detailed analysis of the individual intensities of variability changes by applying our updated approach to each SPL. A comparison of these results confirms our findings of infrequent and small changes to variability information in our previous study. However, differences in the details of these changes exist, which we cannot explain solely by the characteristics of the SPLs or their development processes. We discuss their implications on supporting SPL evolution and on our previous optimization proposals. © 2023 Elsevier Inc.","Comparison; Evolution analysis; Intensity; Software product line evolution; Variability changes"
"Exploration of advanced computer technology to address analytical and noise improvement issues in machine learning","2023","Journal of Systems and Software","10.1016/j.jss.2023.111820","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168555016&doi=10.1016%2fj.jss.2023.111820&partnerID=40&md5=3befa87b2124c2375b8bffc24b155c91","Computer-based visual recognition technology combined with deep learning enables accurate image matching through interactive, multi-level comparison. Currently, popular search methods such as RNN, Faster RNN, etc. are used in computer vision to compute image information by performing hierarchical comparison of image objects. In today's machine learning is used to construct training curves to predict corresponding results, but the accuracy rate will cause some data distortion due to overformation. Therefore, to solve the problem of interference is now physical, the abandonment method is developed to extract certain neurons and reduce the number of feature values. In this study, a visual image recognition framework is designed that uses advanced computer technology to mark and compare images, and to mark and eliminate blurred images. The experimental method successfully improves the prediction of accuracy after a judgment error by comparing the results of training with the results of deep learning verification. The accuracy of matrix formation and result prediction can reach 0.9812 without eliminating the image produced by the light-emitting elements. We remove noisy images based on the same sampling information. After re-training and re-predicting, the accuracy can reach 0.9847. © 2023 The Authors","Computer vision; Edge computing; Image recognition; Labeling; Machine learning"
"A systematic review on security and safety of self-adaptive systems","2023","Journal of Systems and Software","10.1016/j.jss.2023.111716","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159762451&doi=10.1016%2fj.jss.2023.111716&partnerID=40&md5=718cf3a1baa430ca33da0f6d876d8a0a","Context: Cyber–physical systems (CPS) are increasingly self-adaptive, i.e. they have the ability to introspect and change their behavior. This self-adaptation process must be considered when modeling the safety and security aspects of the system. Objective: This study collects and compares security attacks and safety hazards on self-adaptive systems (SAS) described in the literature. In addition, mitigation and treatment strategies, as well as the modeling and analysis approaches, are investigated. Method: We conducted a systematic literature review on 21 selected papers. The selection process included a database search on four scientific databases using a common search string (1430 papers), forward and backward snowballing (1402 papers), and filtering the results based on predefined inclusion and exclusion criteria. The coding scheme to analyze the content of the papers was obtained through research questions, existing domain-specific taxonomies, and open coding. Results: Safety and security are not jointly modeled in the context of self-adaptive systems. The adaptation process is often not considered in the attack and hazard analysis due to naïve assumptions and modeling. The proposed approaches are mostly verified and validated through simulation often using simple use cases and scenarios. Conclusion: A thorough and joint modeling approach for safety and security in self-adaptive systems is still an open challenge that needs to be addressed. Further work is needed to address the gap between safety and security modeling in self-adaptive systems. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)","Attack mechanisms; MAPE-K; Safety hazards; Security and safety; Self-adaptive system"
"Identifying refactoring opportunities for large packages by analyzing maintainability characteristics in Java OSS","2023","Journal of Systems and Software","10.1016/j.jss.2023.111717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158906393&doi=10.1016%2fj.jss.2023.111717&partnerID=40&md5=87291f6dbbaec791eeb8214d3e784893","The source code of a Java-based software system is often structured into packages. When packages are large, they often carry maintainability quality issues. In the literature, there is a lack of empirical evidence on the specific maintainability issues that occur when packages become too large. Our study fills this gap by performing relationship analysis of package size with respect to internal maintainability characteristics (coupling, cohesion, and complexity) using package-level metrics collected from 111 open-source Java projects provided in Qualitas Corpus. Our results show significantly higher maintainability issues in large packages as indicated by the maintainability metrics. We also report strong relationships of package size with cohesion (represented by the number of connected components in a package) and complexity (measured by the number of internal relationships in a package). Based on these strong associations with package size, we show that these cohesion and complexity metrics can be used to identify large package refactoring opportunities. Furthermore, we also discuss why some maintainability metrics (e.g., coupling metrics) may not be useful for refactoring large packages. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Empirical analysis; Open-source software; Package maintainability; Package metrics; Package smells"
"Semantic feature learning for software defect prediction from source code and external knowledge","2023","Journal of Systems and Software","10.1016/j.jss.2023.111753","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162135348&doi=10.1016%2fj.jss.2023.111753&partnerID=40&md5=a9932d7b4fea55da8f7e67f4bdbab497","Software defects not only reduce operational reliability but also significantly increase overall maintenance costs. Consequently, it is necessary to predict software defects at an early stage. Existing software defect prediction studies work with artificially designed metrics or features extracted from source code by machine learning-based approaches to perform classification. However, these methods fail to make full use of the defect-related information other than code, such as comments in codes and commit messages. Therefore, in this paper, additional information extracted from natural language text is combined with the programming language codes to enrich the semantic features. A novel model based on Transformer architecture and multi-channel CNN, PM2-CNN, is proposed for software defect prediction. Pretrained language model and CNN-based classifier are utilized in the model to obtain context-sensitive representations and capture the local correlation of sequences. A large and widely used dataset is utilized to verify the effectiveness of the proposed method. The results show that the proposed method has improvements in generic evaluation metrics compared with the optimal baseline method. Accordingly, external information can have a positive impact on software defect prediction, and our model effectively incorporates such information to improve detection performance. © 2023 Elsevier Inc.","Semantic features; Software defect prediction; Transformer"
"Automatic extraction of security-rich dataflow diagrams for microservice applications written in Java","2023","Journal of Systems and Software","10.1016/j.jss.2023.111722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158001078&doi=10.1016%2fj.jss.2023.111722&partnerID=40&md5=00a440ad9dd79f871f96033aa8a780be","Dataflow diagrams (DFDs) are a valuable asset for securing applications, as they are the starting point for many security assessment techniques. Their creation, however, is often done manually, which is time-consuming and introduces problems concerning their correctness. Furthermore, as applications are continuously extended and modified in CI/CD pipelines, the DFDs need to be kept in sync, which is also challenging. In this paper, we present a novel, tool-supported technique to automatically extract DFDs from the implementation code of microservices. The technique parses source code and configuration files in search for keywords that are used as evidence for the model extraction. Our approach uses a novel technique that iteratively detects new keywords, thereby snowballing through an application's codebase. Coupled with other detection techniques, it produces a fully-fledged DFD enriched with security-relevant annotations. The extracted DFDs further provide full traceability between model items and code snippets. We evaluate our approach and the accompanying prototype for applications written in Java on a manually curated dataset of 17 open-source applications. In our testing set of applications, we observe an overall precision of 93% and recall of 85%. The dataset created for the evaluation is openly released to the research community, as additional contribution of this work. © 2023 Elsevier Inc.","Architecture reconstruction; Automatic extraction; Dataflow diagram; Feature detection; Microservices; Security"
"Legacy systems to cloud migration: A review from the architectural perspective","2023","Journal of Systems and Software","10.1016/j.jss.2023.111702","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153322411&doi=10.1016%2fj.jss.2023.111702&partnerID=40&md5=21273eda3366e54277f8cb61830fdace","Legacy systems are business-critical systems that hold the organization's core business functions developed in a traditional way using monolith architecture and usually deployed on-premises. Through time, this system is exposed to improvement changes, increasing its size and number of functionalities, thus increasing its complexity, and maintaining it becomes a disadvantage to the organization. Migration to the cloud environment becomes the primary option to improve legacy application agility, maintainability, and flexibility. However, to take advantage of the cloud environment, monolith legacy application needs to be rearchitected as microservice architecture to fully benefit from cloud advantages. This paper aims to understand the motivation for cloud migration, investigate existing cloud migration frameworks, identify the target architecture for the cloud, and establish any empirical quality issues in cloud migration from the implementation point of view. To achieve those objectives, we conducted a systematic literature review (SLR) of 47 selected studies from the most relevant scientific digital libraries covering pre-migration, migration, and post-migration stages. The SLR outcome provided us with the primary motivation for the cloud migration, existing cloud migration frameworks, targeted migration architecture patterns, and migration challenges. The results also highlight areas where more research is needed and suggest future research in this field. Furthermore, our analysis shows that current migration approaches lack quality consideration, thus contributing to post-migration quality concerns. © 2023 Elsevier Inc.","Cloud migration; Monolith; Software architecture; Systematic review"
"Microservice architecture recovery based on intra-service and inter-service features","2023","Journal of Systems and Software","10.1016/j.jss.2023.111754","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161961515&doi=10.1016%2fj.jss.2023.111754&partnerID=40&md5=da62d404553e75cb8439adbd3cad8070","Microservice architecture supports independent development and deployment; it facilitates software system design and co-development. However, it also brings new challenges to a variety of software engineering tasks, especially in reverse engineering. An improper design or maintenance routine may cause complex invocation, obscure code logic, and complicate service layers, which may lead to difficulties in understanding, even further testing, or maintenance. To reduce the severity of this problem, we present a novel microservice architecture recovery technique that parses the source code to build a fine-grained dependency graph. This process recovers six key information components of the microservice architecture, which helps developers understand the system. Experimental results based on 12 projects show that the recovered accuracy is 94% on average. The results benefit any engineer unfamiliar with the project, increases their answering accuracy by 23.81% on average, and reduces their training time by 65.43% on average. © 2023 Elsevier Inc.","Architecture recovery; Microservice architecture; Reverse engineering; Software understanding; System Dependency Graph"
"The impact of unequal contributions in student software engineering team projects","2023","Journal of Systems and Software","10.1016/j.jss.2023.111839","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171621732&doi=10.1016%2fj.jss.2023.111839&partnerID=40&md5=4fa8b84d3e41c14c0cce1f15a16cf782","Unequal distribution of work is a common problem in student team projects, undermining learning objectives and reducing student satisfaction with teamwork. More is needed to be known about the impact of unequal distribution of work on the performance of student software engineering teams and their members and the relationship between objective measures of unequal contribution and team-perceived unequal contribution. A greater understanding of these issues allows for targeted and personalised responses to these behaviours. We investigated several aspects of unequal contribution in student software engineering teams. We measured inequality of contribution using Git data from student software engineering teams, source code quality using code analyser SonarQube, and the team performance using grades. According to our results, most students under-contributed to their teams, and at least half the teams in each assignment had low equality of contribution or extreme inequality of contribution. Individual contribution styles did not strongly persist between modules (one-semester-long software engineering courses). There were no consistent associations between inequality of contribution in the student teams and performance or code quality. When the contribution of the least active team member was less than 14% of their fair share, teams were likely to perceive an unequal distribution of contributions. © 2023 Elsevier Inc.","Cooperative/collaborative learning; Post-secondary education; Student team projects; Team work; Work distribution"
"Continuous verification with acknowledged MAPE-K pattern and time logic-based slicing: A platooning system of systems case study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171453076&doi=10.1016%2fj.jss.2023.111840&partnerID=40&md5=df0a63e4f489fbf1a7821e1788cf03e2","A system of Systems (SoS) has emerged to achieve goals beyond the capabilities of a single system. Platooning is a representative SoS where vehicles are driven in a group for energy efficiency. A leader of a platoon can control followers, but the followers can also leave the platoon independently. During follower leave, energy efficiency and the independent operation of followers may conflict. To resolve the conflicts of the platooning SoS and operate safe platooning maneuvers, continuous verification of platooning is required. Continuous verification is performed repeatedly in a control loop that allows system monitoring and verification. However, there are two problems in the existing approaches: there are no suitable control loop patterns to support the resilient reconfiguration, and the SoS verification cost is high. We propose an approach, called continuous verification of platooning (CVP), that solves these two problems. CVP includes an acknowledged MAPE-K pattern for resilience and a fast and accurate slicing for low verification costs. The acknowledged MAPE-K pattern and slicing algorithm proposed in the paper can be independently used for other systems and models. In the case of the acknowledged MAPE-K pattern, we applied it to a mass casualty incident response SoS, which is another acknowledged type of SoS in the paper, and showed its effectiveness. Our experiments on CVP showed that the pattern reduced the incidence rate of 10 types of failure by 97.3%, and ensured the leave of followers. We also proved the correctness of slicing and demonstrated experimentally that it reduces the verification costs by 68.62. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Continuous verification; MAPE-K pattern; Model slicing; PCTL; System of systems"
"Open tracing tools: Overview and critical comparison","2023","Journal of Systems and Software","10.1016/j.jss.2023.111793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164222850&doi=10.1016%2fj.jss.2023.111793&partnerID=40&md5=bb484e4bb01b7047d1b0ff516e7f6038","Background: Coping with the rapid growing complexity in contemporary software architecture, tracing has become an increasingly critical practice and been adopted widely by software engineers. By adopting tracing tools, practitioners are able to monitor, debug, and optimize distributed software architectures easily. However, with excessive number of valid candidates, researchers and practitioners have a hard time finding and selecting the suitable tracing tools by systematically considering their features and advantages. Objective: To such a purpose, this paper aims to provide an overview of popular Open tracing tools via comparison. Methods: Herein, we first identified 30 tools in an objective, systematic, and reproducible manner adopting the Systematic Multivocal Literature Review protocol. Then, we characterized each tool looking at the 1) measured features, 2) popularity both in peer-reviewed literature and online media, and 3) benefits and issues. We used topic modeling and sentiment analysis to extract and summarize the benefits and issues. Specially, we adopted ChatGPT to support the topic interpretation. Results: As a result, this paper presents a systematic comparison amongst the selected tracing tools in terms of their features, popularity, benefits and issues. Conclusion: The result mainly shows that each tracing tool provides a unique combination of features with also different pros and cons. The contribution of this paper is to provide the practitioners better understanding of the tracing tools facilitating their adoption. © 2023 The Author(s)","ChatGPT; Multivocal literature review; Open tracing tool; Telemetry"
"“We do not appreciate being experimented on”: Developer and researcher views on the ethics of experiments on open-source projects","2023","Journal of Systems and Software","10.1016/j.jss.2023.111774","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165678019&doi=10.1016%2fj.jss.2023.111774&partnerID=40&md5=0b379485bc47d4970af84314d7e7fed5","A tenet of open source software development is to accept contributions from users–developers (typically after appropriate vetting). But should this also include interventions done as part of research on open source development? Following an incident in which buggy code was submitted to the Linux kernel to see whether it would be caught, we conduct a survey among open source developers and empirical software engineering researchers to see what behaviors they think are acceptable. This covers two main issues: the use of publicly accessible information, and conducting active experimentation. The survey had 224 respondents. The results indicate that open-source developers are largely open to research, provided it is done transparently. In other words, many would agree to experiments on open-source projects if the subjects were notified and provided informed consent, and in special cases also if only the project leaders agree. While researchers generally hold similar opinions, they sometimes fail to appreciate certain nuances that are important to developers. Examples include observing license restrictions on publishing open-source code and safeguarding the code. Conversely, researchers seem to be more concerned than developers about privacy issues. Based on these results, it is recommended that open source repositories and projects address use for research in their access guidelines, and that researchers take care to ask permission also when not formally required to do so. We note too that the open source community wants to be heard, so professional societies and IRBs should consult with them when formulating ethics codes. © 2023 Elsevier Inc.","Ethics; Experiments; Open source"
"A multi-objective evolutionary approach towards automated online controlled experiments","2023","Journal of Systems and Software","10.1016/j.jss.2023.111703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161336117&doi=10.1016%2fj.jss.2023.111703&partnerID=40&md5=d1537bf1948368bb887ecae10d4e1272","Due to the complexity of the web and mobile software applications, engineers rely heavily on A/B testing (i.e., online controlled experiments) to evaluate and measure the impact of the new changes. However, creating and evaluating A/B tests is a time-consuming, error-prone, and costly manual activity. For the problem of automating A/B testing, there is no solution in literature that evaluates A/B testing results as multi-criteria instead of single criteria. The current single criteria based methods overly simplify the A/B test results, as in the web/mobile application industry, a typical A/B test in practice has multiple metrics instead of one metric. In this paper, we describe the Variant Creation and Evaluation (VCE) problem in A/B testing and propose MOVSW (Multi-Objective Variant Selection Wrapper) that utilizes Multi-Objective Evolutionary Algorithm (MOEA) to automate the process of creating and selecting variants as launch candidates in A/B testing. Given a set of parameters we would like to optimize, the control group in the A/B test uses the existing values of these parameters in the system. The MOVSW automatically creates variants of different parameter values and conducts A/B testing to evaluate these variants against the control group with existing parameter values based on multiple measurements. The outputs of the MOVSW are a list of non-dominated candidate variants (also known as Pareto optimal set) that lead to potential improvements on key measures according to the A/B test results. These candidate variants could be then reviewed by stakeholders to determine the final variant to be launched. We designed and conducted a case study to comprehensively evaluate the effectiveness of five MOEAs and three baseline methods on MOVSW for automating A/B tests, using user clicks logging of news articles from the Addressa Dataset. Our results indicate that MOEA/D is the most stable method with fast convergence compared with other MOEAs, produces more high-quality solutions with high precision than the single-objective methods, and thus is promising to be used in the VCE problem of A/B testing. © 2023 Elsevier Inc.","A/B testing; Multiple objective evolutionary algorithm; Online controlled experiments"
"A uniqueness-based approach to provide descriptive JUnit test names","2023","Journal of Systems and Software","10.1016/j.jss.2023.111821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167962738&doi=10.1016%2fj.jss.2023.111821&partnerID=40&md5=b4aab008a36c99f6abb295462b5367e3","The descriptive naming of unit tests has always been a focal task in test maintenance. Previous work tried to use different methods to generate descriptive names for unit tests or provide suggestions to improve existing names, but they often neglected developers’ needs. Therefore, they are unlikely to be useful to provide descriptive test names for developers in real world. Based on a recent study that can identify uniqueness of JUnit tests, we propose a uniqueness-based name generation approach to generate descriptive test names that meet developers’ needs. Comparing with several alternative approaches, the generated name from our approach are preferred by professional developers, or at least at the same level of preference as the original test names. © 2023 Elsevier Inc.","Documentation; Software testing; Test maintenance"
"Comparing software product lines and Clone and Own for game software engineering under two paradigms: Model-driven development and code-driven development","2023","Journal of Systems and Software","10.1016/j.jss.2023.111824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172451301&doi=10.1016%2fj.jss.2023.111824&partnerID=40&md5=87bdc8bb1d714e98861d94dc8e5b324e","Game developers often face more challenges when reusing code compared to non-game developers, and Software Product Lines (SPLs) have been successful in addressing this issue. In this study, we compare different approaches to code reuse in Classic Software Engineering (CSE) and Game Software Engineering (GSE) through a commercial video game called Kromaia. We specifically focus on two development paradigms: Model-Driven Development (MDD) and Code-Driven Development (CDD). We conduct an empirical evaluation where subjects develop game elements using two approaches: Clone and Own (CaO) and SPLs. The results show that game elements developed using SPLs are more correct (over 23%) than those developed with CaO in both MDD and CDD paradigms. In CDD, there are significant improvements in efficiency (51%) and satisfaction (13%) when using SPLs compared to CaO. However, no improvements are observed when working under MDD. The impact of using SPLs or CaO is greater in CDD than in MDD for game developers. Our findings suggest that SPLs in GSE may have a different role compared to their traditional role in CSE. Specifically, SPLs can be valuable in balancing game difficulty or generating new video game content, such as the one present in the bosses of the game. © 2023 Elsevier Inc.","Code-driven development; Empirical evaluation; Game software engineering; Model-driven development; Software product line engineering"
"Perceived usability of collaborative modeling tools","2023","Journal of Systems and Software","10.1016/j.jss.2023.111807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167999960&doi=10.1016%2fj.jss.2023.111807&partnerID=40&md5=565fbeeffda38aa2fe0e2deb8f6bff19","Context: Online collaborative creation of models is becoming commonplace. Collaborative modeling using chatbots and natural language may lower the barriers to modeling for users from different domains. Objective: We compare the perceived usability of two similarly online collaborative modeling tools, the SOCIO chatbot and the Creately web-based tool. Method: We conducted a crossover experiment with 66 participants. The evaluation instrument was based on the System Usability Scale (SUS). We performed a quantitative and qualitative exploration, employing inferential statistics and thematic analysis. Results: The results indicate that chatbots enabling natural language communication enhance communication and collaboration efficiency and improve the user experience. Conclusion: Chatbots need to improve guidance and help for novices, but they appear beneficial for enhancing user experience. © 2023 Elsevier Inc.","Chatbot; Collaborative modeling tools; Creately; SOCIO; Thematic analysis; Usability"
"Out of the BLEU: How should we assess quality of the Code Generation models?","2023","Journal of Systems and Software","10.1016/j.jss.2023.111741","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159174041&doi=10.1016%2fj.jss.2023.111741&partnerID=40&md5=32b5fef8f88af8e03e39a8d4fc24670b","In recent years, researchers have created and introduced a significant number of various code generation models. As human evaluation of every new model version is unfeasible, the community adopted automatic evaluation metrics such as BLEU to approximate the results of human judgement. These metrics originate from the machine translation domain and it is unclear whether they are applicable for the code generation tasks and how well they agree with the human evaluation on this task. There are also other metrics, CodeBLEU and RUBY, developed to estimate the similarity of code, that take into account the properties of source code. However, for these metrics there are hardly any studies on their agreement with the human evaluation. Despite all that, minimal differences in the metric scores have been used in recent papers to claim superiority of some code generation models over the others. In this paper, we present a study on the applicability of six metrics—BLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, and RUBY—for evaluation of code generation models. We conduct a study on two different code generation datasets and use human annotators to assess the quality of all models run on these datasets. The results indicate that for the CoNaLa dataset of Python one-liners, none of the metrics can correctly emulate human judgement on which model is better with >95% certainty if the difference in model scores is less than 5 points. For the HearthStone dataset, which consists of classes of a particular structure, a difference in model scores of at least 2 points is enough to claim the superiority of one model over the other. Our findings suggest that the ChrF metric is a better fit for the evaluation of code generation models than the commonly used BLEU and CodeBLEU. Yet, finding a metric for code generation that closely agrees with humans requires additional work. © 2023 Elsevier Inc.","Code generation; Code similarity; Metrics; Neural networks"
"Intentions to continue using agile methods: The case of the Greek banking sector","2023","Journal of Systems and Software","10.1016/j.jss.2023.111685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153537423&doi=10.1016%2fj.jss.2023.111685&partnerID=40&md5=4bb0248a6f79e568f9aac44be15cb003","The purpose of this study is to examine the factors that influence team members of software development projects to continue using agile methodologies after their initial adoption. The research focuses on large-scale projects in the banking sector and uses the expectation-confirmation model (ECM) as a conceptual framework. The research model is tested by employing partial least square structural equation modeling (PLS-SEM). The findings validate the model and report statistically significant positive associations between all constructs. The results suggest that confirmation of expectations regarding perceived usefulness and satisfaction are key determinants of agile continuance intentions in software development large-scale projects. The study has both theoretical and practical implications and calls for further research in the field of agile post-adoption. © 2023 Elsevier Inc.","Agile methods; Agile software development; Continuance intentions; Expectation-confirmation model; Large-scale agile projects; PLS-SEM"
"CausalRCA: Causal inference based precise fine-grained root cause localization for microservice applications","2023","Journal of Systems and Software","10.1016/j.jss.2023.111724","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159396126&doi=10.1016%2fj.jss.2023.111724&partnerID=40&md5=5253e51dcba8db3c28b0ec8e1c695978","Effectively localizing root causes of performance anomalies is crucial to enabling the rapid recovery and loss mitigation of microservice applications in the cloud. Depending on the granularity of the causes that can be localized, a service operator may take different actions, e.g., restarting or migrating services if only faulty services can be localized (namely, coarse-grained) or scaling resources if specific indicative metrics on the faulty service can be localized (namely, fine-grained). Prior research mainly focuses on coarse-grained faulty service localization, and there is now a growing interest in fine-grained root cause localization to identify faulty services and metrics. Causal inference (CI) based methods have gained popularity recently for root cause localization, but currently used CI methods have limitations, such as the linear causal relations assumption and strict data distribution requirements. To tackle these challenges, we propose a framework named CausalRCA to implement fine-grained, automated, and real-time root cause localization. The CausalRCA uses a gradient-based causal structure learning method to generate weighted causal graphs and a root cause inference method to localize root cause metrics. We conduct coarse- and fine-grained root cause localization to evaluate the localization performance of CausalRCA. Experimental results show that CausalRCA has significantly outperformed baseline methods in localization accuracy, e.g., the average AC@3 of the fine-grained root cause metric localization in the faulty service is 0.719, and the average increase is 10% compared with baseline methods. In addition, the average Avg@5 has improved by 9.43%. Codes and data are open-sourced and can be found in our Github repository CausalRCA. © 2023 The Author(s)","Causal inference; Fine-grained; Microservice applications; Monitoring data; Root cause localization"
"Reliability in software engineering qualitative research through Inter-Coder Agreement","2023","Journal of Systems and Software","10.1016/j.jss.2023.111707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153374620&doi=10.1016%2fj.jss.2023.111707&partnerID=40&md5=e6cbb2226633387acbc4c1274aea5326","The research on empirical software engineering that uses qualitative data analysis is increasing. However, most of them do not deepen into the validity of the findings, specifically in the reliability of coding in which these methodologies rely on. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis through Inter-Coder Agreement (ICA), based on the use of coefficients to measure the degree of agreement in collaborative coding. We systematically review several existing variants of Krippendorff's α coefficients and provide a novel common mathematical framework to unify them. Finally, this paper illustrates the use of this theoretical framework in a large case study on DevOps culture. We expect that this work will help researchers who are committed to measuring consensus with quantitative techniques in collaborative coding, conducted as part of a qualitative research, to improve the rigor of their findings. © 2023 The Authors","Coding; Inter-Coder Agreement; Krippendorff's αcoefficient; Qualitative research; Software engineering"
"CoDEvo: Column family database evolution using model transformations","2023","Journal of Systems and Software","10.1016/j.jss.2023.111743","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160005806&doi=10.1016%2fj.jss.2023.111743&partnerID=40&md5=0faa0cc23e8ca87b1421a857576e3323","In recent years, software applications have been working with NoSQL databases as they have emerged to handle big data more efficiently than traditional databases. The data models of these databases are designed to satisfy the requirements of the software application, which means that the models must evolve when the requirements of the software application change. To avoid mistakes during the design and evolution of these NoSQL models, there are several methodologies that recommend using a conceptual model. This implies that consistency between the conceptual model and the schema must be maintained when either evolving the database or the software application. In this work, we propose CoDEvo, a model-driven engineering approach that uses model transformations to address the evolution of a NoSQL column family DBMS schema when the underlying conceptual model evolves due to software requirement changes, aiming to maintain consistency between the schema and conceptual model. We have addressed this problem by defining transformation rules that determine how to evolve the schema for a specific conceptual model change. To validate these transformations, we applied them to conceptual model changes from 9 open-source software applications, comparing the output schemas from CoDEvo with the schemas that were defined in these applications. © 2023 The Author(s)","Consistency; Evolution; MDE; Model transformation; NoSQL; Software requirements"
"Software runtime monitoring with adaptive sampling rate to collect representative samples of execution traces","2023","Journal of Systems and Software","10.1016/j.jss.2023.111708","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153682003&doi=10.1016%2fj.jss.2023.111708&partnerID=40&md5=28ef38746bb23ec137219666d02ea08d","Monitoring software systems at runtime is key for understanding workloads, debugging, and self-adaptation. It typically involves collecting and storing observable software data, which can be analyzed online or offline. Despite the usefulness of collecting system data, it may significantly impact the system execution by delaying response times and competing with system resources. The typical approach to cope with this is to filter portions of the system to be monitored and to sample data. Although these approaches are a step towards achieving a desired trade-off between the amount of collected information and the impact on the system performance, they focus on collecting data of a particular type or may capture a sample that does not correspond to the actual system behavior. In response, we propose an adaptive runtime monitoring process to dynamically adapt the sampling rate while monitoring software systems. It includes algorithms with statistical foundations to improve the representativeness of collected samples without compromising the system performance. Our evaluation targets five applications of a widely used benchmark. It shows that the error (RMSE) of the samples collected with our approach is 9%–54% lower than the main alternative strategy (sampling rate inversely proportional to the throughput), with 1%–6% higher performance impact. © 2023 Elsevier Inc.","Adaptation; Execution trace; Logging; Monitoring; Sampling; Self-adaptation"
"Integration test order generation based on reinforcement learning considering class importance","2023","Journal of Systems and Software","10.1016/j.jss.2023.111823","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169036658&doi=10.1016%2fj.jss.2023.111823&partnerID=40&md5=8e7cf615b416b4a4e6823b698eaba576","The task of ordering classes reasonably in the context of integration testing has been discussed by many researchers. Existing methods regard the class integration test order with the minimum stubbing cost as the optimal result. However, they ignore that class importance can also affect the class integration test order. This paper presents a design of a new algorithm, which considered the class importance, and its evaluation using computational experiments. Specifically, two novel reinforcement learning-based methods to generate class integration test orders are proposed, which aim to consider the class importance and minimize the stubbing cost. First, we advance the concept of class importance and optimize its measurement method. Then, we refine the calculation of stubbing complexity, which is the evaluation indicator of stubbing cost. After that, we combine both class importance and the stubbing complexity into the reinforcement learning algorithm to guide the agent to explore. Finally, we evaluate the proposed methods using computational experiments on five benchmark programs and four open-source programs. The experimental results show that our proposed methods can significantly reduce the stubbing cost while prioritizing the classes of high importance. © 2023 Elsevier Inc.","Class importance; Class integration test order; Integration testing; Reinforcement learning; Stubbing cost"
"A speech-enabled virtual assistant for efficient human–robot interaction in industrial environments","2023","Journal of Systems and Software","10.1016/j.jss.2023.111818","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168410918&doi=10.1016%2fj.jss.2023.111818&partnerID=40&md5=530c8dfded5074c74b94fcbf6f71e4fd","This paper presents a natural language-enabled virtual assistant (VA), named Max, developed to support flexible and scalable human–robot interactions (HRI) with industrial robots. Regardless of the numerous natural language interfaces already proposed for intuitive HRI on the industrial shop floor, most of those interfaces remain tightly bound with a specific robotic system. Besides, the lack of a natural and efficient human–robot communication protocol hinders the user experience. Therefore three key elements characterize the proposed framework. First, a Client–Server style architecture is introduced so Max can provide a centralized solution for managing and controlling various types of robots deployed on the shop floor. Second, inspired by human–human communication, two conversation strategies, lexical-semantic and general diversion strategies, are used to guide Max's response generation. These conversation strategies were embedded to improve the operator's engagement with the manufacturing tasks. Third, we fine-tuned the state-of-the-art (SOTA) pre-trained model, Bidirectional Encoder Representations from Transformers (BERT), to support a highly accurate prediction of requested intents from the operator and robot services. Multiple experiments were conducted using the latest iteration of our autonomous industrial mobile manipulator, “Little Helper (LH)”, to validate Max's performance in a real manufacturing environment. © 2023 The Author(s)","Client–server systems; Human–robot interaction; Interactive systems; Natural language processing"
"Investigating the role of Product Owner in Scrum teams: Differentiation between organisational and individual impacts and opportunities","2023","Journal of Systems and Software","10.1016/j.jss.2023.111841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171618057&doi=10.1016%2fj.jss.2023.111841&partnerID=40&md5=943ccee76d7e2bc6ed3c1f1407bf4056","The role of the Product Owner in agile software development is critical being accountable for maximising the value, although becoming proficient in this role is a complex process. The Product Owner's responsibilities vary between different industries. The literature has not fully explained the role of a Product Owner. We shed light on the factors that influence the role at levels of organisation, team, stakeholder, and the individual. We conduct a systematic literature review in combination with a focus group consisting of practitioners to better understand the problem and to address the persistent gaps in theory and practice. Rather than exposing generic competencies associated with the role of Product Owner, our findings show that the role is tailored to fit the unique organisational context, including at the team level, and involves managing the organisational environment, including the institutional culture and politics. We also find an inherent tension arising from the fact that although the role of Product Owner is affected by the organisational level, the performance of the role is strongly influenced by attributes at the individual level. In particular, the individual's ability to establish and manage networks and relationships within the organisation is mediated by his or her communication skills. © 2023 The Author(s)","Agile; Agile project management; Organisation; Product Owner; Role; Scrum"
"A benchmark generator framework for evolving variant-rich software","2023","Journal of Systems and Software","10.1016/j.jss.2023.111736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160018723&doi=10.1016%2fj.jss.2023.111736&partnerID=40&md5=09890a1f42581379e3dbba5108a1d13b","Software often needs to exist in different variants, which account for varying customer requirements, environments, or non-functional aspects, such as energy consumption. Unfortunately, the number of variants can grow exponentially with the number of features. As such, developing and evolving variant-rich systems is challenging, since they do not only evolve “in time” as single systems, but also “in space” with new variants. Fortunately, many different methods and tools for variant-rich systems have been proposed over the last decades, especially in the field of software product line engineering. However, their level of evaluation varies significantly, threatening their relevance for practitioners and that of future research. Many tools have only been evaluated on ad hoc datasets, minimal examples, or unrealistic and limited evolution scenarios, missing large parts of the actual evolution lifecycle of variant-rich systems. Our long-term goal is to provide benchmarks to increase the maturity of evaluation of methods and tools for evolving variant-rich systems. However, providing manually curated and sufficiently detailed benchmarks that cover the whole evolution lifecycle of variant-rich systems is challenging. We present the framework vpbench, which simulates the evolution of a variant-rich system and thereby generates an evolution enriched with metadata explaining the evolution. The generated benchmarks, i.e., the evolution histories and metadata, can serve as ground truth to check the results of tools applied on it. We formalize the claims we make about the generator and the generated benchmarks as requirements. The design of vpbench comprises modular generators and evolution operators that automatically evolve real codebases. We implement simple and advanced evolution operators—e.g., relying on code transplantation to incorporate features from real projects. We demonstrate how vpbench addresses its claimed requirements, also considering multiple degrees of realism, extensibility and language-independence of the generated benchmarks. © 2023 Elsevier Inc.","Evaluation; Generator; Product lines; Variants"
"Effective software security enhancement using an improved PointNet++","2023","Journal of Systems and Software","10.1016/j.jss.2023.111794","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165180051&doi=10.1016%2fj.jss.2023.111794&partnerID=40&md5=1461e4bca48410144af708a55de4d148","As common three-dimensional (3D) data, point clouds have wide application prospects in many fields. The point cloud disorder problem is solved by the PointNet neural network using a symmetric function, which insensitivity to the input order enables PointNet to process the original point cloud data directly. The ability to extract local features was enhanced by introducing the PointNet++, furnishing a better solving capacity of 3D vision problems and improved intelligent driving software security. This paper analyzes the PointNet++ implementation principles and improves its local feature extraction capability by the proposed density-related farthest point sampling (DR-FPS) algorithm, mitigating some limitations of the conventional FPS algorithm so that the sampling results can better express the feature information of point cloud data. The accuracy of the proposed DR-FPS algorithm applied to five-category and ten-category classification tasks exceeded that of the conventional FPS by 4.4 and 5.6%, respectively. Finally, a positive correlation between the accuracy increment and the number of classification categories was revealed. The results are instrumental to intelligent driving software security enhancement. © 2023 Elsevier Inc.","3D point cloud; Density related-farthest point sampling; Driving software security; Farthest point sampling; PointNet++"
"Architecting complex, long-lived scientific software","2023","Journal of Systems and Software","10.1016/j.jss.2023.111732","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165991559&doi=10.1016%2fj.jss.2023.111732&partnerID=40&md5=001818492d1785cdd515638abe3ffebb","Software is a critical aspect of large-scale science, providing essential capabilities for making scientific discoveries. Large-scale scientific projects are vast in scope, with lifespans measured in decades and costs exceeding hundreds of millions of dollars. Successfully designing software that can exist for that span of time, at that scale, is challenging for even the most capable software companies. Yet scientific endeavors face challenges with funding, staffing, and operate in complex, poorly understood software settings. In this paper we discuss the practice of early-phase software architecture in the Square Kilometre Array Observatory's Science Data Processor. The Science Data Processor is a critical software component in this next-generation radio astronomy instrument. We customized an existing set of processes for software architecture analysis and design to this project's unique circumstances. We report on the series of comprehensive software architecture plans that were the result. The plans were used to obtain construction approval in a critical design review with outside stakeholders. We conclude with implications for other long-lived software architectures in the scientific domain, including potential risks and mitigations. © 2023 Carnegie Mellon University","Radio astronomy; Scientific software; Software architecture; Software engineering in practice; Software process"
"ARIST: An effective API argument recommendation approach","2023","Journal of Systems and Software","10.1016/j.jss.2023.111786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165169413&doi=10.1016%2fj.jss.2023.111786&partnerID=40&md5=775de717e4cadd722534dd173f64b535","Learning and remembering to use APIs are difficult. Several techniques have been proposed to assist developers in using APIs. Most existing techniques focus on recommending the right API methods to call, but very few techniques focus on recommending API arguments. In this paper, we propose ARIST, a novel automated argument recommendation approach which suggests arguments by predicting developers’ expectations when they define and use API methods. To implement this idea in the recommendation process, ARIST combines program analysis (PA), language models (LMs), and several features specialized for the recommendation task which consider the functionality of formal parameters and the positional information of code elements (e.g., variables or method calls) in the given context. In ARIST, the LMs and the recommending features are used to suggest the promising candidates identified by PA. Meanwhile, PA navigates the LMs and the features working on the set of the valid candidates which satisfy syntax, accessibility, and type-compatibility constraints defined by the programming language in use. Our evaluation on a large dataset of real-world projects shows that ARIST improves the state-of-the-art approach by 19% and 18% in top-1 precision and recall for recommending arguments of frequently-used libraries. For general argument recommendation task, i.e., recommending arguments for every method call, ARIST outperforms the baseline approaches by up to 125% top-1 accuracy. Moreover, for newly-encountered projects, ARIST achieves more than 60% top-3 accuracy when evaluating on a larger dataset. For working/maintaining projects, with a personalized LM to capture developers’ coding practice, ARIST can productively rank the expected arguments at the top-1 position in 7/10 requests. © 2023 Elsevier Inc.","Code completion; Effective argument recommendation; Program analysis; Statistical language model"
"A systematic mapping study on group work research in computing education projects","2023","Journal of Systems and Software","10.1016/j.jss.2023.111795","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165061885&doi=10.1016%2fj.jss.2023.111795&partnerID=40&md5=6b2179515faf747a9bb978922087467f","Context: For developing students’ group- and teamwork skills needed in the team-oriented work environments of the software industry, the role of project-based learning is considered central. Yet there does not appear to be a proper mapping of the current group work research in the computer science project education literature. Thus, the current state of group work research in the research area is somewhat unknown. Objective: This study aims to form an overview of how research has addressed students’ group work in the field of computer science (CS) and software engineering (SE) to identify research gaps as well as suitable topics for more detailed literature reviews. Methods: A systematic mapping study was used to investigate how group work in tertiary education has been undertaken in the literature during the past decade. Results: Based on the selected papers, the most investigated group work areas were related to the assessment of groups, group formation, communication, and cooperation. The research appeared to be quite narrowly focused on a few areas. Most of the papers were experience or evaluation research. A case study using interviews or questionnaires to gather data from a single course was the most representative type of study. The papers were mainly published in scientific conferences. The use of theoretical frameworks was limited, with a focus on a few established frameworks. Tuckman's group development theory was the predominant framework, while other commonly used concepts and theories include social loafing, Kolb's learning style theory, and the Big Five personality traits model. Conclusion: Out of 7515 papers screened, 225 were deemed eligible and analyzed. We conclude a need for more focused group work research in CS/SE student projects, in which education is inspected from particular perspectives. This would create identifiable lines of research and structure the research area. Relatedly, we suggest that the underused theoretical frameworks can inspire important research: group interventions would benefit from a socially shared regulated learning perspective, explicit use of justice theories would improve theoretical understandings of group behavior, and transactional distance would help analyze how students adopt a software process. Moreover, the research area could be precipitated by novel theoretical perspectives. For practitioners, those implementing a group project course can benefit from a large amount of literature on assessment and group formation, which are issues on which the teacher must take a position. We also include a lessons-learned summary for teachers. Generally, the present results outlining the field in a structured way can facilitate research-based teaching. © 2023 The Author(s)","Group projects; Group work; PjBL; Project education; Systematic mapping study"
"Generic and robust root cause localization for multi-dimensional data in online service systems","2023","Journal of Systems and Software","10.1016/j.jss.2023.111748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159260376&doi=10.1016%2fj.jss.2023.111748&partnerID=40&md5=a34f64f2fbcfd657d777ad065803f3c3","Localizing root causes for multi-dimensional data is critical to ensure online service systems’ reliability. When a fault occurs, only the measure values within specific attribute combinations (e.g., Province = Beijing) are abnormal. Such attribute combinations are substantial clues to the underlying root causes and thus are called root causes of multi-dimensional data. This paper proposes a generic and robust root cause localization approach for multi-dimensional data, PSqueeze. We propose a generic property of root cause for multi-dimensional data, generalized ripple effect (GRE). Based on it, we propose a novel probabilistic cluster method and a robust heuristic search method. Moreover, we identify the importance of determining external root causes and propose an effective method for the first time in literature. Our experiments on two real-world datasets with 5400 faults show that the F1-score of PSqueeze outperforms baselines by 32.89%, while the localization time is around 10 s across all cases. The F1-score in determining external root causes of PSqueeze achieves 0.90. Furthermore, case studies in several production systems demonstrate that PSqueeze is helpful to fault diagnosis in the real world. © 2023 Elsevier Inc.","Multi-dimensional data; Online service system; Ripple effect; Root cause localization"
"On the outdatedness of workflows in the GitHub Actions ecosystem","2023","Journal of Systems and Software","10.1016/j.jss.2023.111827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170222561&doi=10.1016%2fj.jss.2023.111827&partnerID=40&md5=cd65397d50a08560f0171811975e101b","GitHub Actions was introduced as a way to automate CI/CD workflows in GitHub, the largest social coding platform. Thanks to its deep integration into GitHub, GitHub Actions can be used to automate a wide range of social and technical activities. Among its main features, it allows automation workflows to rely on reusable components – the so-called Actions – to enable developers to focus on the tasks that should be automated rather than on how to automate them. As any other kind of reusable software components, Actions are continuously updated, causing many automation workflows to use outdated versions of these Actions. Based on a dataset of nearly one million workflows obtained from 22K+ repositories between November 2019 and September 2022, we provide quantitative empirical evidence that reusing Actions in GitHub workflows is common practice, even if this reuse tends to concentrate on a limited number of Actions. We show that Actions are frequently updated, and we quantify to which extent automation workflows are outdated with respect to these Actions. Using two complementary metrics, technical lag and opportunity lag, we found that most of the workflows are using an outdated Action release, are lagging behind the latest available release for at least 7 months, and had the opportunity to be updated during at least 9 months. This calls for a more rigorous management of Action outdatedness in automation workflows, as well as for better policies and tooling to keep workflows up-to-date. © 2023 Elsevier Inc.","Collaborative software development; Continuous integration; Dependency management; Software ecosystem; Technical lag; Workflow automation"
"Characteristics, causes, and consequences of technical debt in the automation domain","2023","Journal of Systems and Software","10.1016/j.jss.2023.111725","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162113216&doi=10.1016%2fj.jss.2023.111725&partnerID=40&md5=52a4d9b355c9ec756d607d25599723ce","Technical Debt (TD) is a significant concern in software development, particularly when interdisciplinary teams collaborate and interact. The goal of the study is to investigate TD causal chains and patterns in the industrial automation sector by analyzing 123 mechatronic TD incidents from 47 expert interviews across ten companies. Findings reveal that Requirements, Process, and Test TD are most common, while Build, Versioning, Manufacturing, Code, and Maintenance/Service TD are less frequent. Key causes include ”other priorities”, ”lack of time”, ”historically grown products”, ”lack of market analysis” and ”copy-paste-modify without revising tolerances.” The research identifies correlations between TD subtypes and causes/consequences in relation to company size, experts’ experience, and position, utilizing the Chi-square test and PrefixSpan algorithm. The study also maps the contagious character of TD using Neo4J graphical representation. This first in-depth analysis of TD causal chains in industrial automation contributes qualitatively to understanding TD patterns, helping researchers and practitioners assess TD contagiousness, comprehend its effects, prevent diffusion, and develop repayment strategies To the best of our knowledge, this study's quantitative analysis approach provides the foundation that will enable future research identifying TD metrics and TD management in multidisciplinary engineering. © 2023 The Author(s)","Causes; Consequences; Life cycle; Mechatronic product; Mechatronics; Technical debt"
"On the granularity of linguistic reuse","2023","Journal of Systems and Software","10.1016/j.jss.2023.111704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160015846&doi=10.1016%2fj.jss.2023.111704&partnerID=40&md5=12016bd062169dd3baa26b32db8cb07d","Programming languages are complex software systems integrated across an ecosystem of different applications such as language compilers or interpreters but also an integrated development environment comprehensive of syntax highlighting, code completion, error recovery, and a debugger. The complexity of language ecosystems can be faced using language workbenches—i.e., tools that tackle the development of programming languages, domain specific languages and their ecosystems in a modular way. As with any other software system, one of the priorities that developers struggle to achieve when developing programming languages is reusability. After all, the capacity to easily reuse and adapt existing components to new scenarios can dramatically improve development times. Therefore, as programming languages offer features to reuse existing code, language workbenches should offer tools to reuse existing language assets. However, reusability can be achieved in many different ways. In this work, we identify six forms of linguistic reusability, ordered by level of granularity: (i) sub-languages composition, (ii) language features composition, (iii) syntax and semantics assets composition, (iv) semantic assets composition, (v) actions composition, and. (vi) action extension. We use these mechanisms to extend the taxonomy of language composition proposed by Erdweg et al. To show a concrete application of this taxonomy, we evaluate the capabilities provided by the Neverlang language workbench with regards to our taxonomy and extend it by adding explicit support for any granularity level that was originally not supported. This is done by instantiating two levels of reusability as actual operators—desugaring, and delegation. We evaluate these operators against the clone-and-own approach, which was the only form of reuse at that level of granularity prior to the introduction of explicit operators. We show that with the clone-and-own approach the design quality of the source code is negatively affected. We conclude that language workbenches can benefit from the introduction of mechanisms to explicitly support reuse at all granularity levels. © 2023 Elsevier Inc.","Domain specific languages; Feature modularity; Language composition; Language evolution; Language product lines; Reuse and evolution"
"ARRAY: Adaptive triple feature-weighted transfer Naive Bayes for cross-project defect prediction","2023","Journal of Systems and Software","10.1016/j.jss.2023.111721","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162188575&doi=10.1016%2fj.jss.2023.111721&partnerID=40&md5=4050f93a1079d41aac15cbf4f0c1fc22","Context: Cross-project defect prediction (CPDP) aims to predict defects of target data by using prediction models trained on the source dataset. However, owing to the huge distribution difference, it is still a challenge to build high-performance CPDP models. Objective: We propose a novel high-performance CPDP method named adaptive triple feature-weighted transfer naive Bayes (ARRAY). Methods: ARRAY is characterized by feature weighted similarity, feature weighted instance weight, and the model adaptive adjustment. Experiments are performed on 34 defect datasets. We compare ARRAY with seven state-of-the-art CPDP methods in terms of area under ROC curve (AUC), F1, and Matthews correlation coefficient (MCC) with statistical testing methods. Results: Experimental results show that: (1) on average, ARRAY separately improves MCC, AUC, and F1 over the baselines by at least 18.4%, 6.5%, and 4.5%; (2) ARRAY significantly performs better than each baseline on most datasets; (3) ARRAY significantly outperforms all baselines with non-negligible effect size according to post-hoc test. Conclusion: It can be concluded that: (1) the proposed feature weighted similarity, feature weighted instance weight, and the model adaptive adjustment are very helpful for improving the performance of CPDP models; (2) ARRAY is a more promising alternative for CPDP with common metrics. © 2023 Elsevier Inc.","Common metrics; Cross-project defect prediction; Feature weighting; Model adaptation; Transfer learning"
"VsusFL: Variable-suspiciousness-based Fault Localization for novice programs","2023","Journal of Systems and Software","10.1016/j.jss.2023.111822","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168555560&doi=10.1016%2fj.jss.2023.111822&partnerID=40&md5=0b676efcf6b3b9ed5e28319ef4c376b2","Automatically localizing faulty statements is a desired feature for effective learning programming. Most of the existing automated fault localization techniques are developed and evaluated on commercial or well-known open-source projects, which performed poorly on novice programs. In this paper, we propose a novel fault localization technique VsusFL (Variable-suspiciousness-based Fault Localization) for novice programs. VsusFL is inspired by simulating the manual program debugging process and takes advantage of variable value sequences. VsusFL can trace variable value changes, determine whether the intermediate state of the variables is correct, and report the potential faulty statements for novice programs. This paper presents the implementation of VsusFL and conducts empirical studies on 422 real faulty novice programs. Experimental results show that VsusFL performs much better than Grace, ANGELINA, VSBFL, Spectrum-Based Fault Localization (SBFL), and Variable-based Fault Localization (VFL) in terms of TOP-1, TOP-3, and TOP-5 metrics. Specifically, VsusFL can localize 90%, 35% and 9% more faulty statements than the best-performing baseline Grace. Moreover, We analyze the correlation between VsusFL and other techniques and find a weak correlation since they perform well on different programs, indicating the potential to further enhance fault localization performance through strategic integration of VsusFL with other methods. © 2023 Elsevier Inc.","Fault localization; Novice programs; Sequence mapping; Variable value sequence"
"Predicting resource consumption of Kubernetes container systems using resource models","2023","Journal of Systems and Software","10.1016/j.jss.2023.111750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160690261&doi=10.1016%2fj.jss.2023.111750&partnerID=40&md5=ee61f48a225bdaa3462db2f447ae24ec","Cloud computing has radically changed the way organizations operate their Software by allowing them to achieve high availability of services at affordable cost. Containerized microservices is an enabling technology for this change, and advanced container orchestration platforms such as Kubernetes are used for service management. Despite the flourishing ecosystem of monitoring tools for such orchestration platforms, service management is still mainly a manual effort. The modeling of cloud computing systems is an essential step towards automatic management, but the modeling of cloud systems of such complexity remains challenging and, as yet, unaddressed. In fact modeling resource consumption will be a key to comparing the outcome of possible deployment scenarios. This paper considers how to derive resource models for cloud systems empirically. We do so based on models of deployed services in a formal modeling language with explicit CPU and memory resources; once the adherence to the real system is good enough, formal properties can be verified in the model. Targeting a likely microservices application, we present a model of Kubernetes developed in Real-Time ABS. We report on leveraging data collected empirically from small deployments to simulate the execution of higher intensity scenarios on larger deployments. We discuss the challenges and limitations that arise from this approach, and identify constraints under which we obtain satisfactory accuracy. © 2023 The Author(s)","Cloud computing; Kubernetes; Microservices; Resource models; Resource prediction"
"CharM — Evaluating a model for characterizing service-based architectures","2023","Journal of Systems and Software","10.1016/j.jss.2023.111826","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170251056&doi=10.1016%2fj.jss.2023.111826&partnerID=40&md5=cbdd3e85c6f7788e758a400930110c14","Service-based architecture is an approach that emerged to overcome software development challenges such as difficulty to scale, low productivity, and strong dependence between elements. Microservice, an architectural style that follows this approach, offers advantages such as scalability, agility, resilience, and reuse. This architectural style has been well accepted and used in industry and has been the target of several academic studies. However, analyzing the state-of-the-art and -practice, we can notice a fuzzy limit when trying to classify and characterize the architecture of service-based systems. Furthermore, it is possible to realize that it is difficult to analyze the trade-offs to make decisions regarding the design and evolution of this kind of system. Some concrete examples of these decisions are related to how big the services should be, how they communicate, and how the data should be divided/shared. Based on this context, we developed the CharM, a model for characterizing the architecture of service-based systems that adopts microservices guidelines. To achieve this goal, we followed the guidelines of the Design Science Research in five iterations, composed of an ad-hoc literature review, discussions with experts, two case studies, and a survey. As a contribution, the CharM is an easily understandable model that helps professionals with different profiles to understand, document, and maintain the architecture of service-based systems. © 2023 Elsevier Inc.","Characterization model; Microservice; Service-based system; Software architecture"
"Factors that affect developers’ decision to participate in a Mobile Software Ecosystem","2023","Journal of Systems and Software","10.1016/j.jss.2023.111808","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167456899&doi=10.1016%2fj.jss.2023.111808&partnerID=40&md5=d2000c4ce905933ac0fa8d5fcf7dd277","In Mobile Software Ecosystem (MSECO), external developers build applications that meet the interest of users of mobile technologies. Researchers claim that MSECO sustainability depends on two capabilities: adoption of new technologies over time, and attraction and maintenance of people (i.e., developers and users) participating in these ecosystems. Our research focuses on the latter. We investigated factors that affect developers’ decision to participate in an MSECO. First, we analyzed the literature to identify such motivational factors. Second, we interviewed experts in MSECO aiming to refine the identified factors. Finally, we also conducted interviews to identify the opinion of MSECO developers regarding the refined list of factors. The final list of 29 factors were discussed, one by one, by the 20 interviewees. Each developer indicated which factor motivated him/her to join a certain MSECO and also which has kept him/her contributing to it. Results indicate that the lack of studies focusing on developers, not just on technologies or business rules of the ecosystem, is a key for understanding developers’ decision to participate in an MSECO. Moreover, developers become more concerned with their relationships in these ecosystems over time. As such, ou research contributes to the MSECO field with a list of motivating factors that should be considered by external developers from ecosystems towards MSECO sustainability. © 2023","Developers attraction; Empirical study; Mobile Software Ecosystems; Motivating factors"
"GitHub Copilot AI pair programmer: Asset or Liability?","2023","Journal of Systems and Software","10.1016/j.jss.2023.111734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159766448&doi=10.1016%2fj.jss.2023.111734&partnerID=40&md5=4ce303b685f57c075850408e58f92998","Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans’ solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans’ contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise. © 2023 Elsevier Inc.","Code completion; GitHub copilot; Language model; Testing"
"Generic and industrial scale many-criteria regression test selection","2023","Journal of Systems and Software","10.1016/j.jss.2023.111802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166227149&doi=10.1016%2fj.jss.2023.111802&partnerID=40&md5=5275a465587ebe3e7d96a542d31924ff","While several test case selection algorithms (heuristic and optimal) and formulations (linear and non-linear) have been proposed, no multi-criteria framework enables Pareto search — the state-of-the-art approach of doing multi-criteria optimization. Therefore, we introduce the highly parallelizable, openly available Many-Criteria Test-Optimization Algorithm (MC-TOA) framework that combines heuristic Pareto search and optimality gap knowledge per criterion. MC-TOA is largely agnostic to the criteria formulations and can incorporate many criteria where existing approaches offer limited scope (single or few objectives/constraints), lack flexibility in the expression and assurance of constraints, or run into problem complexity issues. For two large-scale systems with up to six criteria and thousands of system test cases, MC-TOA not only produces, over the board, superior Pareto fronts in terms of HVI score compared to the state-of-the-art many-objective heuristic baseline, it also does that within minutes of runtime for worst-case executions, i.e., assuming that a regression affects the entire test-suite. MC-TOA depends on convex solvers. We find that the evaluated open-source solvers are slower but suffice for smaller systems, while being less robust for larger systems. Linear formulations execute faster and obtain near-optimal results, which led to faster and better overall convergence of MC-TOA compared to integer formulations. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)","Industrial-scale optimization; Regression testing; Software testing; Test case selection"
"Agile software engineers’ affective states, their performance and software quality: A systematic mapping review","2023","Journal of Systems and Software","10.1016/j.jss.2023.111800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165527365&doi=10.1016%2fj.jss.2023.111800&partnerID=40&md5=de6d18ca8fa3a799612776b04d4b219c","Nowadays, software development companies use agile methodologies to increase the speed and flexibility required by unpredictable working environments and streamline the software delivery process. Agile methodologies emphasize the software engineers’ interactions rather than rigid software development processes. Therefore, agile software development processes (e.g., implementing user stories) should consider software engineers’ affective states since these influence agile software project activities (e.g., team composition or decision-making). We conducted a systematic mapping review following the guidelines of Petersen, K. and Kitchenham, B. to answer our research question: “What affective states influence agile software engineers’ performance and developed software quality?”. We retrieved over 16,000 articles published between January 2010 and September 2021 and after applying selection criteria, 24 primary articles were identified. The results show that the affective states of software engineers influence the activities of the software project and the software development process. Furthermore, we found that there is a lack of well-defined and standard metrics to study the influence of software engineers’ affective states on their performance and the quality of the resulting software. Finally, we concluded that studying and understanding the affective states of software engineers in agile environments is crucial to achieve their well-being at work and improve their performance. © 2023 Elsevier Inc.","Affective state; Agile methodology; Software Engineering; Software Engineers’ performance; Software quality"
"An empirical study on real bug fixes from solidity smart contract projects","2023","Journal of Systems and Software","10.1016/j.jss.2023.111787","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163799485&doi=10.1016%2fj.jss.2023.111787&partnerID=40&md5=df11431e72996e85c3794752e6a59cfa","Smart contracts are pieces of code that reside inside the blockchains and can be triggered to execute any transaction when specifically predefined conditions are satisfied. Being commonly used for commercial transactions in blockchain makes the security of smart contracts particularly important. Over the last few years, we have seen a great deal of academic and practical interest in detecting and fixing the bugs in smart contracts written by Solidity. But little is known about the real bug fixes in Solidity smart contract projects. To understand the bug fixes and enrich the knowledge of bug fixes in real-world projects, we conduct an empirical study on historical bug fixes from 46 real-world Solidity smart contract projects in this paper. We provide a multi-faceted discussion and mainly explore the following four questions: File Type and Amount, Fix Complexity, Bug distribution, and Fix Patches. We distill four findings during the process to explore these four questions. Finally, based on these findings, we provide actionable implications to improve the current approaches to fixing bugs in Solidity smart contracts from three aspects: Automatic repair techniques, Analysis tools, and Solidity developers. © 2023 Elsevier Inc.","Bug fix; Empirical study; Smart contract; Solidity"
"A catalogue of game-specific anti-patterns based on GitHub and Game Development Stack Exchange","2023","Journal of Systems and Software","10.1016/j.jss.2023.111789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165086375&doi=10.1016%2fj.jss.2023.111789&partnerID=40&md5=1fc93b773e3735a2d8a19f539110d159","With the ever-increasing use of games, game developers are expected to write efficient code and support several aspects such as security, maintainability, and performance. However, the need for frequent updates in game development may lead to quick-fix solutions and bad coding practices. Though unintentional, these bad practices may lead to poor program comprehension and can cause several issues during software maintenance. The quick-fix solutions might lead to technical debts due to the presence of anti-patterns and code smells, which may affect the functional and non-functional requirements of the game. To avoid such instances, game developers may need some guidelines to refer to during the game development process. Thus, to aid developers and researchers, in our previous work, we had presented an initial catalogue of anti-patterns in the domain of game development. To broaden the scope of the catalogue and diversify the instances of anti-patterns, we analyzed additional data from a Q&A platform. We present 15 game-specific anti-patterns based on thematic analysis of 189 issues, 892 commits, 104 pull requests from 100 open-source GitHub game repositories, and 971 questions from Game Development Stack Exchange. We see the catalogue as an effort towards improving the development and quality of the games. The catalogue containing a detailed description of every anti-pattern with the context, problem, solution, example(s), and their occurrences on GitHub and Game Development Stack Exchange is available at https://rishalab.github.io/Catalog-of-Game-Antipatterns/. © 2023 Elsevier Inc.","Anti-patterns; Catalogue; Games; Thematic analysis"
"Cognitive Driven Development helps software teams to keep code units under the limit!","2023","Journal of Systems and Software","10.1016/j.jss.2023.111830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171792938&doi=10.1016%2fj.jss.2023.111830&partnerID=40&md5=cf6457449a63186a499cc7d044a76aa0","Software design techniques are key elements in the process of designing good software. Over the years, a large number of design techniques have been proposed by both researchers and practitioners. Unfortunately, despite their uniqueness, it is not uncommon to find software products that make subpar design decisions, leading to design degradation challenges. One potential reason for this behavior is that developers do not have a clear vision of how much a code unit could grow; without this vision, a code unit can grow endlessly, even when developers are equipped with an arsenal of design practices. Different than other design techniques, Cognitive Driven Development (CDD for short) focuses on (1) defining and (2) limiting the number of coding elements that developers could use at a given code unit. In this paper, we report on the experiences of a software development team using CDD for building from scratch a learning management tool at Zup Innovation, a Brazilian tech company. By curating commit traces left in the repositories, combined with the developers’ perception, we organized a set of findings and lessons that could be useful for those interested in adopting CDD. For instance, we noticed that by using CDD, despite the evolution of the product, developers were able to keep the code units under a small amount of size (in terms of lines of code). Furthermore, although limiting the complexity is at the heart of CDD, we also discovered that developers tend to relax this notion of limit so that they can cope with the different complexities of the software. Still, we noticed that CDD could also influence testing practices; limiting the code units’ size makes testing easier to perform. © 2023 Elsevier Inc.","Cognitive Driven Development; Design techniques; Software design"
"A systematic literature review on the impact of formatting elements on code legibility","2023","Journal of Systems and Software","10.1016/j.jss.2023.111728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163388981&doi=10.1016%2fj.jss.2023.111728&partnerID=40&md5=1ac99e82c24196970128ea680e222e6b","Context: Software programs can be written in different but functionally equivalent ways. Even though previous research has compared specific formatting elements to find out which alternatives affect code legibility, seeing the bigger picture of what makes code more or less legible is challenging. Goal: We aim to find which formatting elements have been investigated in empirical studies and which alternatives were found to be more legible for human subjects. Method: We conducted a systematic literature review and identified 15 papers containing human-centric studies that directly compared alternative formatting elements. We analyzed and organized these formatting elements using a card-sorting method. Results: We identified 13 formatting elements (e.g., indentation) and 33 levels of formatting elements (e.g., two-space indentation), which are about formatting styles, spacing, block delimiters, long or complex code lines, and word boundary styles. While some levels were found to be statistically better than other equivalent ones in terms of code legibility, e.g., appropriate use of indentation with blocks, others were not, e.g., formatting layout. For identifier style, we found divergent results, where one study found a significant difference in favor of camel case, while another study found a positive result in favor of snake case. Conclusion: The number of identified papers, some of which are outdated, and the many null and contradictory results emphasize the relative lack of work in this area and underline the importance of more research. There is much to be understood about how formatting elements influence code legibility before the creation of guidelines and automated aids to help developers make their code more legible. © 2023","Code legibility; Formatting elements; Program understandability"
"Automatic prediction of developers’ resolutions for software merge conflicts","2023","Journal of Systems and Software","10.1016/j.jss.2023.111836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171848437&doi=10.1016%2fj.jss.2023.111836&partnerID=40&md5=d12a6420061e17bd2a98a54102010a5d","In collaborative software development, developers simultaneously work in parallel on different branches that they merge periodically. When edits from different branches textually overlap, conflicts may occur. Manually resolving conflicts can be tedious and error-prone. Researchers proposed tool support for conflict resolution, but these tools barely consider developers’ preferences. Conflicts can be resolved by: keeping the local version only KL, keeping the remote version only (KR), or manually editing them (ME). Recent studies show that developers resolved the majority of textual conflicts by KL or KR. Thus, we created a machine learning-based approach RPREDICTOR to predict developers’ resolution strategy (KL, KR, or ME) given a merge conflict. We did large-scale experiments on the historical resolution of 74,861 conflicts. Our experiments show that RPREDICTOR achieved 63% F-score for within-project prediction and 46% F-score for cross-project prediction. Compared with other classifiers, RPREDICTOR provides the highest effectiveness when using a random forest (RF) classifier. Finally, we proposed a variant technique RPREDICTORv, which enables developers to customize its prediction conservativeness. For a highly conservative setting, RPREDICTORv achieved 34% effort saving while minimizing the risk of producing incorrect prediction labels. © 2023 Elsevier Inc.","Conflict resolution; Prediction; Software merge; Textual conflicts"
"BiTCN_DRSN: An effective software vulnerability detection model based on an improved temporal convolutional network","2023","Journal of Systems and Software","10.1016/j.jss.2023.111772","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162106383&doi=10.1016%2fj.jss.2023.111772&partnerID=40&md5=44a49ee69a821c2d53b42b6afbf29cb0","The detection of software vulnerabilities is a challenging task in the field of security. With the increasing scale of software and the rapid development of artificial intelligence technology, deep learning has been extensively applied to automatic vulnerability detection. Temporal Convolutional Networks (TCNs) have been shown to perform well in tasks that can be processed in parallel; they can adaptively learn complex structures (including in-time series data); and they have exhibited stable gradients — they are relatively easier to train, and can quickly converge to an optimal solution. However, TCNs cannot simultaneously capture the bidirectional semantics of the source code, since they do not have a bidirectional network structure. Furthermore, because of the weak noise resistance of residual TCN connections, TCNs are also susceptible to learning features that are not related to vulnerabilities when learning the source code features. To overcome the limitations of the traditional TCN, we propose a bidirectional TCN model based on the Deep Residual Shrinkage Network (DRSN), namely BiTCN_DRSN. BiTCN_DRSN combines TCN and DRSN to enhance the noise immunity and make the network model more attentive to the features associated with vulnerabilities. In addition, addressing the limitation that the TCN is a unidirectional network structure, the forward and backward sequences are utilized for bidirectional source-code feature learning. The experimental results show that the proposed BiTCN_DRSN model can effectively improve the accuracy of source-code vulnerability detection, compared with some existing neural-network models. Compared with the traditional TCN, our model increases the accuracy by 4.22%, 2.42% and 2.66% on the BE-ALL, RM-ALL and HY-ALL datasets, respectively. The proposed BiTCN_DRSN model also exhibits improved detection stability. © 2023 Elsevier Inc.","Deep learning; Deep residual shrinkage network; Software security; Vulnerability detection"
"Automatic software vulnerability assessment by extracting vulnerability elements","2023","Journal of Systems and Software","10.1016/j.jss.2023.111790","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164217990&doi=10.1016%2fj.jss.2023.111790&partnerID=40&md5=97c8aacc33a7a6e9a167cbde3153db2f","Software vulnerabilities take threats to software security. When faced with multiple software vulnerabilities, the most urgent ones need to be fixed first. Therefore, it is critical to assess the severity of vulnerabilities in advance. However, increasing number of vulnerability descriptions do not use templates, which reduces the performance of the existing software vulnerability assessment approaches. In this paper, we propose an automated vulnerability assessment approach that using vulnerability elements for predicting the severity of six vulnerability metrics (i.e., Access Vector, Access Complexity, Authentication, Confidentiality Impact, Integrity Impact and Availability Impact). First, we use BERT-MRC to extract vulnerability elements from vulnerability descriptions. Second, we assess six metrics using vulnerability elements instead of full descriptions. We conducted experiments on our manually labeled dataset. The experimental results show that our approach has an improvement of 12.03%, 14.37%, and 38.65% on Accuracy over three baselines. © 2023 Elsevier Inc.","Deep learning; Mining software repositories; Multi-class classification; Vulnerability assessment"
"Data-driven agile software cost estimation models for DHS and DoD","2023","Journal of Systems and Software","10.1016/j.jss.2023.111739","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159633364&doi=10.1016%2fj.jss.2023.111739&partnerID=40&md5=b9f59864b503201ebbc0d9e1eeb829eb","Problem: Since the Manifesto for Agile Software Development, the cost analysis community has struggled to find the most appropriate size measures for developing accurate agile software development cost estimates at an early phase to establish baseline budgets and schedules, and for the selection of competitive bidders. At this time, common agile sizing measures such as Story Points are not practical since these are reported months after contract award in the Department of Homeland Security (DHS) and Department of Defense (DoD). The problem is compounded with the lack of data to build estimation models for informed decisions. Aims: The primary objective is to investigate how well two new size measures (Functional Story and Issues) accurately relate to total effort, and how these compare and rank against four popular software size measures (Story, Story Point, Unadjusted Function Points, and Simple Function Points). The second objective is to rank the six sizing measures based on how well each estimate software development effort at an early phase or after contract award. Method: The experimental framework relied on an analysis of variance and goodness-of-fit tests to examine and compare the accuracy of effort estimation models using six competing size measures. The analysis is based on data from 17 agile projects implemented between 2014 to 2021. Results: Our two new size measures (Functional Story, Issues) proved to be good predicters of total software development effort. Functional Story is better at predicting total effort at early phase than Function Points and Story. Functional Story and Function Points are better at predicting total effort after contract award than Story Point and Issues. Conclusion: The DHS and DoD cost community can choose one or more of these estimation models to evaluate agile software development cost proposals or track agile developer's progress after contract award. © 2023","Agile; Effort estimation; Function point; Functional story; Story point; User story"
"LWS: A framework for log-based workload simulation in session-based SUT","2023","Journal of Systems and Software","10.1016/j.jss.2023.111735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159469224&doi=10.1016%2fj.jss.2023.111735&partnerID=40&md5=180b88a8d8502aacfd206c9a9e34f115","Artificial intelligence for IT Operations (AIOps) plays a critical role in operating and managing cloud-native systems and microservice-based applications but is limited by the lack of high-quality datasets with diverse scenarios. Realistic workloads are the premise and basis of generating such AIOps datasets, with the session-based workload being one of the most typical examples. Due to privacy concerns, complexity, variety, and requirements for reasonable intervention, it is difficult to copy or generate such workloads directly, showing the importance of effective and intervenable workload simulation. In this paper, we formulate the task of workload simulation and propose a framework for Log-based Workload Simulation (LWS) in session-based systems. LWS extracts the workload specification including the user behavior abstraction based on agglomerative clustering as well as relational models and the intervenable workload intensity from session logs. Then LWS combines the user behavior abstraction with the workload intensity to generate simulated workloads. The experimental evaluation is performed on an open-source cloud-native application with both well-designed and public real-world workloads, showing that the simulated workload generated by LWS is effective and intervenable, which provides the foundation of generating high-quality AIOps datasets. © 2023 Elsevier Inc.","AIOps; Behavior model; Intensity modeling; Workload simulation"
"A systematic literature review on source code similarity measurement and clone detection: Techniques, applications, and challenges","2023","Journal of Systems and Software","10.1016/j.jss.2023.111796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165159999&doi=10.1016%2fj.jss.2023.111796&partnerID=40&md5=4c83bc80535b8ef9686208a3fb6fea19","Measuring and evaluating source code similarity is a fundamental software engineering activity that embraces a broad range of applications, including but not limited to code recommendation, duplicate code, plagiarism, malware, and smell detection. This paper proposes a systematic literature review and meta-analysis on code similarity measurement and evaluation techniques to shed light on the existing approaches and their characteristics in different applications. We initially found over 10,000 articles by querying four digital libraries and ended up with 136 primary studies in the field. The studies were classified according to their methodology, programming languages, datasets, tools, and applications. A deep investigation reveals 80 software tools, working with eight different techniques on five application domains. Nearly 49% of the tools work on Java programs and 37% support C and C++, while there is no support for many programming languages. A noteworthy point was the existence of 12 datasets related to source code similarity measurement and duplicate codes, of which only eight datasets were publicly accessible. The lack of reliable datasets, empirical evaluations, hybrid methods, and focuses on multi-paradigm languages are the main challenges in the field. Emerging applications of code similarity measurement concentrate on the development phase in addition to the maintenance. © 2023 Elsevier Inc.","Code clone; Code recommendation; Plagiarism detection; Source code similarity; Systematic literature review"
"DongTing: A large-scale dataset for anomaly detection of the Linux kernel","2023","Journal of Systems and Software","10.1016/j.jss.2023.111745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159766299&doi=10.1016%2fj.jss.2023.111745&partnerID=40&md5=2b9969b7731be234b7972e5a87fef518","Host-based intrusion detection systems (HIDS) can automatically identify adversarial applications by learning models from system events that represent normal system behaviors. The system call is the only way for applications to interact with the operating system (OS). Thus, system call sequences are traditionally used in HIDS to train models to detect novel attacks, and a wide range of datasets has been proposed for this task. However, existing datasets are either built for user-level applications (not for OS kernels), or completely outdated (proposed more than 20 years ago). To address this issue, this paper presents the first large-scale dataset specifically assembled for anomaly detection of the Linux kernel. The task of creating such a dataset is challenging due to the difficulty both in collecting a diversified set of programs that can trigger bugs in the kernel and in tracing events that may crash the kernel at runtime. In this paper, we describe in detail how to collect the data through an automated and efficient framework. The raw dataset is 85 GB in size, and contains 18,966 system call sequences that are labeled with normal and abnormal attributes. Our dataset covers more than 200 kernel versions (including major/minor releases and revisions) and 3,600 bug-triggering programs in the past five years. In addition, we conduct cross-dataset evaluation to demonstrate that training on our dataset enables superior generalization ability than other related datasets, and provide benchmark results for anomaly detection of Linux kernel on our dataset. Our extensive dataset is both useful for machine learning researchers focusing on algorithmic optimizations and practitioners in kernel development who are interested in deploying deep learning models in OS kernels. © 2023 Elsevier Inc.","Anomaly detection; Dataset; Deep learning; Kernel BUG; Linux kernel; System calls"
"On the impact of single and co-occurrent refactorings on quality attributes in android applications","2023","Journal of Systems and Software","10.1016/j.jss.2023.111817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172468450&doi=10.1016%2fj.jss.2023.111817&partnerID=40&md5=1ea52672c0f85a68896cac9a5a9ae1f4","Android applications must evolve quickly to meet new user requirements, to facilitate bug fixing or to adapt to technological changes. This evolution can lead to various software quality problems that may hinder maintenance and further evolution. Code refactoring is a key practice that is employed to ensure that the intent of a code change is properly achieved without compromising internal software quality. While the impact of refactoring on software quality has been widely studied in object-oriented software in general, its impact in the specific context of mobile applications is still unclear. This paper reports on a large empirical study that aims to understand the impact of single and co-occurrent refactorings on quality metrics in Android applications. We analyze the evolution history of 800 open-source Android applications containing a total of 84,841 refactoring operations. We first analyze the impact of single refactoring operations on 21 common quality metrics using the Difference-in-Difference (DiD) statistical model. Then, we identify the most common co-occurrent refactorings using association rule mining, and investigate their impact on quality metrics using the DiD model. Our investigations deliver several important findings. Our results reveal that co-occurrent refactorings are quite prevalent in Android applications. Overall, 60% of the total number of refactoring commits contain multiple refactoring types, and 16 co-occurrent refactoring pairs tend to be applied together leading to a higher impact than single refactorings. We found that single refactorings have no statistically significant impact on quality metrics in 74.7% of the cases, a positive impact in 23.1% of the cases, and a negative impact in 2.2% of the cases. Whereas, co-occurrent refactorings have no statistically significant impact on quality metrics in 54.3% of the cases, a positive impact in 42.4% of the cases, and a negative impact in 3.3% of the cases. Our findings provide practical insights and suggest directions for researchers, practitioners, and tool builders to improve refactoring practices in the context of Android applications development. © 2023 Elsevier Inc.","Android; Co-occurrence; Empirical study; Mobile app; Quality metrics; Refactoring"
"Sustaining human health: A requirements engineering perspective","2023","Journal of Systems and Software","10.1016/j.jss.2023.111792","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164214087&doi=10.1016%2fj.jss.2023.111792&partnerID=40&md5=0141a36d6325b80dcced0eca5b0fbde2","In our current day and age, Earth suffers under the human ecological footprint, which influences our health and well-being. Technological solutions, including software-related ones, may help tackle these concerns for humanity. However, the development of such solutions requires special attention and effort to overcome human, public, and social barriers that might prevent them from being effective. The Requirements Engineering for Well-Being, Aging, and Health (REWBAH) workshop gathering in 2021 focused on addressing the challenge of how Requirements Engineering (RE) knowledge and practices can be applied to the development of information systems that support and promote long-lasting, sustained, and healthier behavior and choices by individuals. An interactive discussion among subject matter experts and practitioners participating in the REWBAH’21 revolved around several questions. In a subsequent qualitative analysis, the emerging themes were arranged in the sustainable-health RE (SusHeRE) framework to describe RE processes that address both sustainability and health goals. In this vision paper, we present our framework, which includes four main SusHeRE goals defined according to the changes in RE that we deem necessary for achieving a positive contribution of RE on sustainability and health. These goals involve improved RE Techniques, Multidisciplinary Expertise, Education Agenda, and Public and Social Ecology. © 2023 Elsevier Inc.","Health; Requirements engineering; Sustainability; Well-being"
"An effective fault localization approach based on PageRank and mutation analysis","2023","Journal of Systems and Software","10.1016/j.jss.2023.111799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164750567&doi=10.1016%2fj.jss.2023.111799&partnerID=40&md5=c2d2cbf6e1d2670ea49693a5cd1ba05b","Mutation-based fault localization (MBFL) is a popular method based on mutation testing. MBFL applies a variety of operators to generate mutants and calculates the statement's suspiciousness by counting the execution results of the test cases on the mutants. However, the tie problem of MBFL creates obstacles to accurate fault localization. The tie problem refers to that many statements have the same suspiciousness. To solve the tie problem, we propose a fault localization approach based on the PageRank algorithm and mutation analysis (PRMA). We first apply the PageRank algorithm to calculate the faultiness scores of the statements. Then, we weight the suspicious value of the statements with faultiness scores to solve the tie problem. Finally, the weighted suspicious values are sorted in descending order to generate a list, which is provided to developers for fault localization. To evaluate our approach, we conduct experiments on the real fault benchmark Defects4J and the artificial fault dataset Siemens. We compare PRMA with the traditional MBFL techniques (Metallaxis and MUSE) and recently proposed MBFL methods (MCBFL-hybrid-avg, SMFL and SMBFL). The experimental results show that our approach outperforms above comparison methods and improves the effect of fault localization in both quantity and accuracy. © 2023 Elsevier Inc.","Mutation analysis; Mutation-based fault localization; PageRank algorithm; Software fault localization"
"Behaviour driven development: A systematic mapping study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111749","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160605559&doi=10.1016%2fj.jss.2023.111749&partnerID=40&md5=0d6c623a12e99c3edeb2a8a0624478da","Context: Behaviour Driven Development (BDD) uses scenarios written in semi-structured natural language to express software requirements in a way that can be understood by all stakeholders. The resulting natural language specifications can also be executed to reveal correct and problematic parts of a software. Although BDD was introduced about two decades ago, there is a lack of secondary studies in peer-reviewed scientific literature, making it difficult to understand the state of BDD research and existing gaps. Objective: To understand the current state of BDD research by conducting a systematic mapping study that covers studies published from 2006 (when BDD was introduced) to 2021. Method: By following the guidelines for conducting systematic mapping studies in software engineering, we sought to answer research questions on types of venues in which BDD papers have been published, research types, contribution types, studied topics and their evolution, as well as evaluation methods used in published BDD research. Results: The study identified 166 papers which were mapped. Key results include the following: the dominance of conference papers; scarcity of research with insights from the industry; shortage of philosophical papers on BDD; acute shortage of metrics for measuring various aspects of BDD specifications and the processes for producing BDD specifications; the dominance of studies on using BDD for facilitating various software development endeavours, improving the BDD process and associated artefacts, and applying BDD in different contexts; scarcity of studies on using BDD alongside other software techniques and technologies; increase in diversity of studied BDD topics; and notable use of case studies and experiments to study different BDD aspects. Conclusion: The paper improves our understanding of the state of the art of BDD, and highlights important areas of focus for future BDD research. © 2023 Elsevier Inc.","Behaviour Driven Development; Systematic mapping studies in software engineering; Systematic mapping study"
"Catalog and detection techniques of microservice anti-patterns and bad smells: A tertiary study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171991388&doi=10.1016%2fj.jss.2023.111829&partnerID=40&md5=161d6d39620f84573d51c09cc05c0deb","Background: Various works investigated microservice anti-patterns and bad smells in the past few years. We identified seven secondary publications that summarize these, but they have little overlap in purpose and often use different terms to describe the identified anti-patterns and smells. Objective: This work catalogs recurring bad design practices known as anti-patterns and bad smells for microservice architectures, and provides a classification into categories as well as methods for detecting these practices. Method: We conducted a systematic literature review in the form of a tertiary study targeting secondary studies identifying poor design practices for microservices. Results: We provide a comprehensive catalog of 58 disjoint anti-patterns, grouped into five categories, which we derived from 203 originally identified anti-patterns for microservices. Conclusion: The results provide a reference to microservice developers to design better-quality systems and researchers who aim to detect system quality based on anti-patterns. It also serves as an anti-pattern catalog for development-aiding tools, which are not currently available for microservice system development but could mitigate quality degradation throughout system evolution. © 2023 The Author(s)","Anti patterns; Anti-patterns; Antipatterns; Bad smells; Microservices; Software maintenance"
"A Grey Literature Review on Data Stream Processing applications testing","2023","Journal of Systems and Software","10.1016/j.jss.2023.111744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160024077&doi=10.1016%2fj.jss.2023.111744&partnerID=40&md5=279db1183abd50513795efc85821d5f3","Context: The Data Stream Processing (DSP) approach focuses on real-time data processing by applying specific techniques for capturing and processing relevant data for on-the-fly results, i.e. without necessarily requiring prior storage. Like in any other software, testing plays a vital role in the quality assurance of DSP applications. However, testing such kind of software is not a simple task. In this context, some factors that make challenging testing are message temporality, parallelism, data volume, complex infrastructure, variability, and speed of messages. Objective: This work aims to map and synthesize industry knowledge and experience regarding DSP application testing. Specifically, we want to know about challenges, test purposes, test approaches, test data sources, and adopted tools. Method: To achieve the objective, we performed a Grey Literature Review (e.g., blog posts, white papers, discussion lists, lecture themes at technical events, professional social networks, software repositories, and other web-published) on testing DSP applications. We searched the grey literature using Google's regular search engine in addition to specific searches on technical software development content websites. The selected studies were analyzed using qualitative and quantitative techniques. Results: Results are based on evidence from 154 selected sources. The challenges for testing DSP applications are the complexity of DSP applications, test infrastructure complexity, timing, and data acquisition issues. The main test objectives identified are functional suitability, performance efficiency, reliability, and maintainability. The main test approaches reported: Performance Testing, Regression Testing, Property-Based Testing, Chaos Testing, and Contract/Schema Testing. The strategies adopted by practitioners to obtain test data: Historical Data, Production Data Mirroring, Semi-Synthetic Data, and Synthetic Data. We also report 50 tools used in various testing activities, which are used for: automating infrastructure, generating test data, test utilities, dealing with timing issues, load generation, simulation, and others. Furthermore, we identified gaps and opportunities for future scientific work. Conclusion: This work selected and summarized content produced by practitioners regarding DSP application testing. We identified that knowledge, techniques, and tools intrinsic to the practice were not present in the formal literature, so this study helps reduce the gap between industry and academia on this topic. The document has delivered benefits to industry practitioners and academic researchers. © 2023 Elsevier Inc.","Data streams; Grey literature; Software testing"
"LCVD: Loop-oriented code vulnerability detection via graph neural network","2023","Journal of Systems and Software","10.1016/j.jss.2023.111706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85157985357&doi=10.1016%2fj.jss.2023.111706&partnerID=40&md5=9d08c9e5a66deffd43cab0392ed9ed3c","Due to the unique mechanism and complex structure, loops in programs can easily lead to various vulnerabilities such as dead loops, memory leaks, resource depletion, etc. Traditional approaches to loop-oriented program analysis (e.g. loop summarization) are costly with a high rate of false positives in complex software systems. To address the issues above, recent works have applied deep learning (DL) techniques to vulnerability detection. However, existing DL-based approaches mainly focused on the general characteristics of most vulnerabilities without considering the semantic information of specific vulnerabilities. As a typical structure in programs, loops are highly iterative with multi-paths. Currently, there is a lack of available approaches to represent loops, as well as useful methods to extract the implicit vulnerability patterns. Therefore, this paper introduces LCVD, an automated loop-oriented code vulnerability detection approach. LCVD represents the source code as the Loop-flow Abstract Syntax Tree (LFAST), which focuses on interleaving multi-paths around loop structures. Then a novel Loop-flow Graph Neural Network (LFGNN) is proposed to learn both the local and overall structure of loop-oriented vulnerabilities. The experimental results demonstrate that LCVD outperforms the three static analysis-based and four state-of-the-art DL-based vulnerability detection approaches across evaluation settings. © 2023 Elsevier Inc.","Code representation; Deep learning; Graph neural network; Loop-oriented vulnerability; Vulnerability detection"
"[Formula presented]: Software product lines extraction driven by language server protocol","2023","Journal of Systems and Software","10.1016/j.jss.2023.111809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170045439&doi=10.1016%2fj.jss.2023.111809&partnerID=40&md5=9606daacf25b0d9246651d21c045265b","Software product lines (SPL) describe highly-variable software systems as a family of similar products that differ in terms of the features they provide. The promise of SPL engineering is to enable massive software reuse by allowing software features to be reused across a variety of different products made for several customers. However, there are some disadvantages in the extraction of SPLs from standard applications. Most notably, approaches to the development of SPLs are not supported by the base language and use a syntax and composition techniques that require a deep understanding of the tools being used. Therefore, the same features cannot be used in a different application and developers must face a steep learning curve when developing SPLs for the first time or when switching from one approach to a different one. Ultimately, this problem is due to a lack of standards in the area of SPL engineering and in the way SPLs are extracted from variability-unaware applications. In this work, we present a framework based on LSP and dubbed [Formula presented] that aims at standardizing such a process by decoupling the refactoring operations made by the user from the effect they have on the source code. This way, the server for a specific SPL development approach can be used across several development environments that provide clients with customized refactoring options. Conversely, the developers can use the same client to refactor SPLs made according to different approaches without needing to learn the syntax of each approach. To showcase the applicability of the approach, we present an evaluation performed by refactoring four SPLs according to two different approaches: the results show that a minimal implementation of the [Formula presented] client and server applications can be used to reduce the effort of extracting an SPL up to the 93% and that it can greatly reduce or even completely hide the implementation details from the developer, depending on the chosen approach. © 2023 The Author(s)","Aspect-oriented programming; Design patterns; Feature-oriented programming; Language server protocol; Software product lines"
"Static vulnerability detection based on class separation","2023","Journal of Systems and Software","10.1016/j.jss.2023.111832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171347188&doi=10.1016%2fj.jss.2023.111832&partnerID=40&md5=123daba88e70d3b208cc76f727d3d022","Software vulnerability detection is a key step to prevent the system from being attacked. However, tens of thousands of codes have brought great challenges to engineers, so we urgently need an automatic and intelligent vulnerability detection method. The existing vulnerability detection model based on deep learning has the problem that it is difficult to separate the features of vulnerable and neutral code. Based on the code data drive, this paper proposes a static vulnerability detection method SDV(Statically Detecting Vulnerability) for C∖C++ programs. SDV is a function-level vulnerability code detection method. This paper uses a code property graph to represent the code and decouples the feature extractor and the classifier. In the graph feature extraction stage, we use Jump Graph Attention Network layers and convolutional pooling layers. Their combination can not only prevent the over-smoothing problem but also separate the sample classes deeply. Finally, on the chrdeb dataset, SDV outperforms state-of-the-art function-level vulnerability detection methods by 52.3%, 15.9%, and 39.6% in Precision, Recall, and F1-Score, respectively. On the real project sard, the number of vulnerabilities detected by SDV is 10.7 times more than Reveal. © 2023 Elsevier Inc.","Class separation; Code property graph; Graph attention network; Jump structure; Vulnerability detection"
"ProCon: An automated process-centric quality constraints checking framework","2023","Journal of Systems and Software","10.1016/j.jss.2023.111727","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156148987&doi=10.1016%2fj.jss.2023.111727&partnerID=40&md5=ed3568df1abcb8aa0be9579cf5cdbf41","When dealing with safety–critical systems, various regulations, standards, and guidelines stipulate stringent requirements for certification and traceability of artifacts, but typically lack details with regards to the corresponding software engineering process. Given the industrial practice of only using semi-formal notations for describing engineering processes – with the lack of proper tool mapping – engineers and developers need to invest a significant amount of time and effort to ensure that all steps mandated by quality assurance are followed. The sheer size and complexity of systems and regulations make manual, timely feedback from Quality Assurance (QA) engineers infeasible. In order to address these issues, in this paper, we propose a novel framework for tracking, and “passively” executing processes in the background, automatically checking QA constraints depending on process progress, and informing the developer of unfulfilled QA constraints. We evaluate our approach by applying it to three case studies: a safety–critical open-source community system, a safety–critical system in the air-traffic control domain, and a non-safety–critical, web-based system. Results from our analysis confirm that trace links are often corrected or completed after the work step has been considered finished, and the engineer has already moved on to another step. Thus, support for timely and automated constraint checking has significant potential to reduce rework as the engineer receives continuous feedback already during their work step. © 2023 The Author(s)","Constraint checking; Developer support; Process deviation; Quality assurance; Software engineering process; Traceability"
"Pragmatic evidence of cross-language link detection: A systematic literature review","2023","Journal of Systems and Software","10.1016/j.jss.2023.111825","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169977797&doi=10.1016%2fj.jss.2023.111825&partnerID=40&md5=69efa0cc50c9a272a719dae8997519d7","There is a rising trend for heterogeneous software applications involving multilingual source code. The key focus of reverse engineers is to unravel the cross-language links (XLLs) and their dependencies. This study aims to perform a systematic literature review (SLR) to compile different approaches, tools, techniques, and shortcomings of such techniques and understand the XLLs and their dependencies while performing reverse engineering on state-of-the-art software applications. This SLR selects 76 primary studies and uses them to create a ’go-to’ literature database, where professionals from software engineering could find all the content pertinent to the analysis and XLL detection for major multilingual applications like Java enterprise applications, Android applications, etc. It has been observed that traditional source code analysis mechanisms to reverse engineer contemporary software applications face scores of problems and limitations that need to be addressed. To assist the community in the above-mentioned goal, a general schema with definitions of XLLs and associated concepts is furnished. This study provides an SLR on XLLs, comprehensive taxonomy called cross-language analysis, which incorporates all the methods for XLL detection in multilingual source code. By pursuing future directions suggested in the end, researchers and practitioners can advance the field of multilingual applications; such as Enterprise resource planning (ERP) solutions, and cross-language software corpora, leading to improved software development practices and better understanding of language interactions in multilingual environments. The research data provided in the survey presents a comprehensive analysis of the complexities involved in working with diverse programming languages and frameworks, offering valuable insights for language technology researchers, software developers, academics, and decision-makers. This integration will enable them to identify and manage dependencies across diverse languages, leading to more efficient and reliable multilingual software systems. © 2023 Elsevier Inc.","Cross-language dependencies; Cross-language link detection; Graph databases; Machine learning in software engineering; Multilingual software applications; Multilingual source code; Reverse engineering; Software development; Software maintenance; Source code analysis; Systematic literature review"
"Empathy models and software engineering — A preliminary analysis and taxonomy","2023","Journal of Systems and Software","10.1016/j.jss.2023.111747","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159361523&doi=10.1016%2fj.jss.2023.111747&partnerID=40&md5=1e37d3835a1d097e889fa89027befbd4","Empathy is widely used in many disciplines such as philosophy, sociology, psychology, health care. Ability to empathise with software end-users seems to be a vital skill software developers should possess. This is because engineering successful software systems involves not only interacting effectively with users but also understanding their true needs. Empathy has the potential to address this situation. Empathy is a predominant human aspect that can be used to comprehend decisions, feelings, emotions and actions of users. However, to date empathy has been under-researched in software engineering (SE) context. In this position paper, we present our exploration of key empathy models from different disciplines and our analysis of their adequacy for application in SE. While there is no evidence for empathy models that are readily applicable to SE, we believe these models can be adapted and applied in SE context with the aim of assisting software engineers to increase their empathy for diverse end-user needs. We present a preliminary taxonomy of empathy by carefully considering the most popular empathy models from different disciplines. We encourage future research on empathy in SE as we believe it is an important human aspect that can significantly influence the relationship between developers and end-users. © 2023","Empathy; Empathy models; Human aspects; Human factors; Software engineering; Taxonomy"
"Using expression parsing and algebraic operations to generate test sequences.","2023","Journal of Systems and Software","10.1016/j.jss.2023.111798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165227537&doi=10.1016%2fj.jss.2023.111798&partnerID=40&md5=c3543d611fce47e4e04a4ffb618ab709","It has become a popular trend to build software's regular expression or extended regular expression models in order to generate test sequences from these models. Such test sequences tend to have promising test coverage and fault detection capability. During this process, one critical step is expression parsing based on algebraic operations. However, the parsing can be very challenging as different algebraic systems have different algebraic operators and algebraic operations. Besides, the parsing difficulty continues to grow as software complexity increases. To address the above challenges, this paper proposes a general expression parsing framework for test sequence generation. The proposed framework consists of three stages, expression decomposition, algebraic operations, and subexpression combination. To implement the framework, an expression parsing algorithm based on abstract syntax tree is developed. Case studies based on 117 expressions collected from the literature over the past 30 years as well as 13 software systems are conducted to evaluate the effectiveness of the proposed algorithm. The results indicate that our algorithm is superior to three existing and commonly used algorithms with respect to expression parsing and software fault detection. © 2023 Elsevier Inc.","Abstract syntax tree; Algebraic operation; Expression parsing; Test sequence generation"
"Bugs4Q: A benchmark of existing bugs to enable controlled testing and debugging studies for quantum programs","2023","Journal of Systems and Software","10.1016/j.jss.2023.111805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166471742&doi=10.1016%2fj.jss.2023.111805&partnerID=40&md5=8bd074598efd41124aa952a04cc713d5","Realistic benchmarks of reproducible bugs and fixes are vital to good experimental evaluation of debugging and testing approaches. However, there is no suitable bug benchmark suite that can systematically evaluate the debugging and testing methods of quantum programs until now. This paper proposes Bugs4Q, a benchmark of forty-two real, manually validated Qiskit bugs from three popular platforms (GitHub, StackOverflow, and Stack Exchange) in programming, supplemented with test cases to reproduce buggy behaviors. Bugs4Q also provides interfaces for accessing the buggy and fixed versions of the Qiskit programs and executing the corresponding source code and unit tests, facilitating the reproducible empirical studies and comparisons of Qiskit program debugging and testing tools. Bugs4Q is publicly available at https://github.com/Z-928/Bugs4Q-Framework. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Bug benchmark suite; Bugs4Q; Quantum program debugging; Quantum software testing"
"Transforming Numerical Feature Models into Propositional Formulas and the Universal Variability Language","2023","Journal of Systems and Software","10.1016/j.jss.2023.111770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162118938&doi=10.1016%2fj.jss.2023.111770&partnerID=40&md5=81e06e04773c26d8e43d015a23ff88fe","Real-world Software Product Lines (SPLs) need Numerical Feature Models (NFMs) whose features have not only boolean values that satisfy boolean constraints but also have numeric attributes that satisfy arithmetic constraints. An essential operation on NFMs finds near-optimal performing products, which requires counting the number of SPL products. Typical constraint satisfaction solvers perform poorly on counting and sampling. Nemo (Numbers, features, models) is a tool that supports NFMs by bit-blasting, the technique that encodes arithmetic expressions as boolean clauses. The newest version, Nemo2, translates NFMs to propositional formulas and the Universal Variability Language (UVL). By doing so, products can be counted efficiently by #SAT and Binary Decision Tree solvers, enabling finding near-optimal products. This article evaluates Nemo2 with a large set of synthetic and colossal real-world NFMs, including complex arithmetic constraints and counting and sampling experiments. We empirically demonstrate the viability of Nemo2 when counting and sampling large and complex SPLs. © 2023 The Authors","Bit-blasting; Feature model; Model counting; Numerical features; Propositional formula; Universal variability language"
"Boosting multi-objective just-in-time software defect prediction by fusing expert metrics and semantic metrics","2023","Journal of Systems and Software","10.1016/j.jss.2023.111853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172367390&doi=10.1016%2fj.jss.2023.111853&partnerID=40&md5=3c880f10bc3e08c2f96da70a737efe50","Just-in-time software defect prediction (JIT-SDP) aims to predict whether a code commit is defect-inducing or defect-clean immediately after developers submit their code commits. In our previous study, we modeled JIT-SDP as a multi-objective optimization problem by designing two potential conflict optimization objectives. By only considering expert metrics for code commits, our proposed multi-objective just-in-time software defect prediction (MOJ-SDP) approach can significantly outperform state-of-the-art supervised and unsupervised baselines. Recent studies have shown that deep learning techniques can be used to automatically extract semantic metrics from code commits and achieved promising performance for JIT-SDP. However, it is unclear how well MOJ-SDP performs when semantic metrics are used, and whether these two types of metrics are complementary and can be boosted by fusing them for MOJ-SDP. We conducted an extensive experiment using 27,319 code commits from 21 real-world open-source projects. Our results show that when using semantic features, the performance of MOJ-SDP can be slightly decreased for Popt, but greatly improved for Recall@20%Effort. However, when these two types of metrics are fused based on the model-level fusion with the maximum rule, the performance can be boosted by a large margin and outperform state-of-the-art JIT-SDP baselines. © 2023 Elsevier Inc.","Expert metrics; Just-in-time defect prediction; Metric fusion; Multi-objective optimization; Semantic metrics"
"Transitioning a project-based course between onsite and online. An experience report","2023","Journal of Systems and Software","10.1016/j.jss.2023.111828","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171371383&doi=10.1016%2fj.jss.2023.111828&partnerID=40&md5=cb37702cfd65d3b7015e5874787cd4bf","We present an investigation regarding the challenges faced by student teams across four consecutive iterations of a team-focused, project-based course in software engineering. The studied period includes the switch to fully online activities in the spring of 2020, and covers the return to face to face teaching two years later. We cover the feedback provided by over 1500 students, collected in free-text form on the basis of a survey. A qualitative research method was utilized to discern and examine the challenges and perceived benefits of a course that was conducted entirely online. We show that technical challenges remain a constant in project-based courses, with time management being the most affected by the move to online. Students reported that the effective use of collaborative tools eased team organization and communication while online. We conclude by providing a number of action points regarding the integration of online activities in face-to-face course unfolding related to project management, communication tools, the importance of teamwork, and of active mentor participation. © 2023 Elsevier Inc.","Online education; Soft skills; Software engineering education; Teamwork"
"Dialog summarization for software collaborative platform via tuning pre-trained models","2023","Journal of Systems and Software","10.1016/j.jss.2023.111763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162051230&doi=10.1016%2fj.jss.2023.111763&partnerID=40&md5=09de16f2205980742587e4b3de4f8b2f","Software collaborative platforms, e.g., Gitter live chat and GitHub Discussions, are essential in software maintenance. Summarizing the live chat logs is useful for extracting, retrieving, and sharing knowledge for software developers. Automatic text summarization has been studied in many areas such as code summarization, and title generation. However, the previous studies rely on rich collected labeled data for model training which are absent for the noisy interleaved dialogs, resulting in poor performance in the few-shot scenario. To tackle the issue, we propose a novel Automatic Dialog Summarization Approach based on pre-trained models, named ADSum. To alleviate the high-cost problem of the from-scratch manual annotation, ADSum finetunes the Text-To-Text Transfer Transformer (T5) model by exploiting the discussion posts on GitHub, and then recommends summaries for an annotator. To solve the poor performance in the few-shot scenario, we propose to employ the prompt tuning paradigm for tuning the T5 model by exploiting the disentangled dialog data on Gitter. Meanwhile, the soft prompt is used to avoid the manual effort of designing appropriate prompt templates. To verify the effectiveness of our approach, we extract 38,964 high-quality discussion posts from GitHub and manually annotate 3,039 dialog summarizations from Gitter. Experimental results show our approach achieves state-of-the-art performance in terms of three performance metrics. In particular, our proposed method outperformed the Transformer-based and other pre-training models by 39% and 14%, respectively, on the GitHub dataset regarding the Rouge-L metric. The experiments of handling data scarcity and a human evaluation also confirm the effectiveness of ADSum. © 2023 Elsevier Inc.","Live chat; Pre-trained language model; Prompt tuning; Software maintenance; Text summarization"
"A first look at bug report templates on GitHub","2023","Journal of Systems and Software","10.1016/j.jss.2023.111709","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158054175&doi=10.1016%2fj.jss.2023.111709&partnerID=40&md5=9f3aa5b0b7e7fccb982def6d511514eb","Bug reports which are written by different people have a variety of styles, and such various styles lead to difficulty in understanding bug reports. To enhance the comprehensibility of bug reports, GitHub has proposed a template mechanism to guide how to report the bugs. However, there is no study on the use of bug report templates on GitHub. In this paper, we conduct an empirical study on the bug report templates on GitHub, including the popularity, benefits, and content of the templates. Our empirical study finds that: (1) For popularity, more and more open source projects and bug reports are applying templates over time. (2) For benefits, bug reports written using templates will be resolved quicker and have a higher comment coverage. (3) For content, the most common items for templates are expected behavior, describe the bug and to reproduce etc. Additionally, we summarize a taxonomy of items for bug report templates. Finally, we propose an automatic templating approach for templating an un-templated bug report. Our approach achieves an accuracy of 0.718 and an F1-score of 0.717 on average, which shows that our approach can effectively templatize an un-templated bug report. © 2023 Elsevier Inc.","Bug reports; Empirical study; Template"
"On the maintenance support for microservice-based systems through the specification and the detection of microservice antipatterns","2023","Journal of Systems and Software","10.1016/j.jss.2023.111755","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162234261&doi=10.1016%2fj.jss.2023.111755&partnerID=40&md5=7c374b7440a16fedb8414b31c94192f2","The software industry is currently moving from monolithic to microservice architectures, which are made up of independent, reusable, and fine-grained services. A lack of understanding of the core concepts of microservice architectures can lead to poorly designed systems that include microservice antipatterns. These microservice antipatterns may affect the quality of services and hinder the maintenance and evolution of software systems. The specification and detection of microservice antipatterns could help in evaluating and assessing the design quality of systems. Several research works have studied patterns and antipatterns in microservice-based systems, but the automatic detection of these antipatterns is still in its infancy. We propose MARS (Microservice Antipatterns Research Software), a fully automated approach supported by a framework for specifying and identifying microservice antipatterns. Using MARS, we specify and identify 16 microservice antipatterns in 24 microservice-based systems. The results show that MARS can effectively detect microservice antipatterns with an average precision of 82% and a recall of 89%. Thus, our approach can help developers assert and improve the quality of their microservices and development practices. © 2023 Elsevier Inc.","Antipatterns; Detection; Maintenance; Microservices"
"An evidence-based roadmap for IoT software systems engineering","2023","Journal of Systems and Software","10.1016/j.jss.2023.111680","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150843938&doi=10.1016%2fj.jss.2023.111680&partnerID=40&md5=d1a5a6997e16b964231d78905e8ad4bf","Context: The Internet of Things (IoT) has brought expectations for software inclusion in everyday objects. However, it has challenges and requires multidisciplinary technical knowledge involving different areas that should be combined to enable IoT software systems engineering. Goal: To present an evidence-based roadmap for IoT development to support developers in specifying, designing, and implementing IoT systems. Method: An iterative approach based on experimental studies to acquire evidence to define the IoT Roadmap. Next, the Systems Engineering Body of Knowledge life cycle was used to organize the roadmap and set temporal dimensions for IoT software systems engineering. Results: The studies revealed seven IoT Facets influencing IoT development. The IoT Roadmap comprises 117 items organized into 29 categories representing different concerns for each Facet. In addition, an experimental study was conducted observing a real case of a healthcare IoT project, indicating the roadmap applicability. Conclusions: The IoT Roadmap can be a feasible instrument to assist IoT software systems engineering because it can (a) support researchers and practitioners in understanding and characterizing the IoT and (b) provide a checklist to identify the applicable recommendations for engineering IoT software systems. © 2023 Elsevier Inc.","Evidence-based software engineering; Internet of Things; System engineering"
"CCStokener: Fast yet accurate code clone detection with semantic token","2023","Journal of Systems and Software","10.1016/j.jss.2023.111618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147729023&doi=10.1016%2fj.jss.2023.111618&partnerID=40&md5=ecf696a177567044b747b824dacd9977","Code clone detection refers to the discovery of identical or similar code fragments in the code repository. AST-based, PDG-based, and DL-based tools can achieve good results on detecting near-miss clones (i.e., clones with small differences or gaps) by using syntax and semantic information, but they are difficult to apply to large code repositories due to high time complexity. Traditional token-based tools can rapidly detect clones by the low-cost index (i.e., low frequency or k-lines tokens) on sequential source code, but most of them have the poor capability on detecting near-miss clones because of the lack of semantic information. In this study, we propose a fast yet accurate code clone detection tool with the semantic token, called CCSTOKENER. The idea behind the semantic token is to enhance the detection capability of token-based tool via complementing the traditional token with semantic information such as the structural information around the token and its dependency with other tokens in form of n-gram. Specifically, we extract the type of relevant nodes in the AST path of every token and transform these types into a fixed-dimensional vector, then model its semantic information by applying n-gram on its related tokens. Meanwhile, our tool adopts and improves the location–filtration–verification process also used in CCALIGNER and LVMAPPER, during which process we build the low-cost k-tokens index to quickly locate the candidate code blocks and speed up detection efficiency. Our experiments show that CCSTOKENER achieves excellent accuracy on detecting more near-miss clone pairs, which exhibits the best recall on Moderately Type-3 clones and detects more true positive clones on four java open-source projects. Moreover, CCSTOKENER attains the best generalization and transferability compared with two DL-based tools (i.e., ASTNN, TBCCD). © 2023 Elsevier Inc.","Code clone detection; Near-miss clones; Scalable detection; Semantic token"
"Graph collaborative filtering-based bug triaging","2023","Journal of Systems and Software","10.1016/j.jss.2023.111667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149976690&doi=10.1016%2fj.jss.2023.111667&partnerID=40&md5=a858750643b69b78a3e921031c0498f4","Issue tracking systems are widely used for collecting bug reports. A target of intelligent software engineering is to automate assigning bugs to appropriate developers. Recently, the momentum of artificial intelligence has brought many successful studies that triage bugs by classifying their reports with NLP-based methods. Some studies also try to introduce context information to represent developers. Nevertheless, they take a fundamental assumption that developers and bugs, closely related entities in real-world scenarios, should be modeled independently. To capture the bug-developer correlations in bug triaging activities, we propose a Graph Collaborative filtering-based Bug Triaging framework: (1) bug-developer correlations are modeled as a bipartite graph; (2) natural language processing-based pre-training is implemented on bug reports to initialize bug nodes; (3) spatial–temporal graph convolution strategy is designed to learn the representation of developer nodes; (4) information retrieval-based classifier is proposed to match bugs and developers. Extensive experiments across mainstream datasets show the competence of our GCBT. Moreover, We believe that GCBT could generally benefit the modeling of correlations in other software engineering scenarios. © 2023 Elsevier Inc.","Bug triaging; Deep graph learning; Graph collaborative filtering; Software reliability engineering"
"The anatomy of a vulnerability database: A systematic mapping study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111679","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151296480&doi=10.1016%2fj.jss.2023.111679&partnerID=40&md5=505021e40954bfd1e883f60123ce0039","Software vulnerabilities play a major role, as there are multiple risks associated, including loss and manipulation of private data. The software engineering research community has been contributing to the body of knowledge by proposing several empirical studies on vulnerabilities and automated techniques to detect and remove them from source code. The reliability and generalizability of the findings heavily depend on the quality of the information mineable from publicly available datasets of vulnerabilities as well as on the availability and suitability of those databases. In this paper, we seek to understand the anatomy of the currently available vulnerability databases through a systematic mapping study where we analyze (1) what are the popular vulnerability databases adopted; (2) what are the goals for adoption; (3) what are the other sources of information adopted; (4) what are the methods and techniques; (5) which tools are proposed. An improved understanding of these aspects might not only allow researchers to take informed decisions on the databases to consider when doing research but also practitioners to establish reliable sources of information to inform their security policies and standards. © 2023 The Author(s)","Software evolution; Software security; Systematic mapping studies; Vulnerability databases"
"A compositional approach to creating architecture frameworks with an application to distributed AI systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111604","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146698508&doi=10.1016%2fj.jss.2022.111604&partnerID=40&md5=fc81e8b936ec5952daa6c023a2b499fd","Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). This poses additional architectural challenges. To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks in a use case of distributed AI systems. Then, we derive from the identified challenges four rules, and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project “Very efficient deep learning in the IoT” (VEDLIoT) in the form of a case study. © 2022 The Author(s)","AI systems; Architectural frameworks; Compositional thinking; Requirements engineering; Systems engineering"
"A systematic literature review on blockchain governance","2023","Journal of Systems and Software","10.1016/j.jss.2022.111576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145657949&doi=10.1016%2fj.jss.2022.111576&partnerID=40&md5=26c8df1929cfb11acea5b26deb491e18","Blockchain has been increasingly used as a component to enable decentralisation in software architecture for a variety of applications. Blockchain governance has received considerable attention to ensure the safe and appropriate use and evolution of blockchain, especially after the Ethereum DAO attack in 2016. However, there are no systematic efforts to analyse existing governance solutions. To understand the state-of-the-art of blockchain governance, we conducted a systematic literature review with 37 primary studies. The extracted data from primary studies are synthesised to answer identified research questions. The study results reveal several major findings: (1) governance can improve the adaptability and upgradability of blockchain, whilst the current studies neglect broader ethical responsibilities as the objectives of blockchain governance; (2) governance is along with the development process of a blockchain platform, while ecosystem-level governance process is missing, and; (3) the responsibilities and capabilities of blockchain stakeholders are briefly discussed, whilst the decision rights, accountability, and incentives of blockchain stakeholders are still under studied. We provide actionable guidelines for academia and practitioners to use throughout the lifecycle of blockchain, and discuss future trends to support researchers in this area. © 2022 Elsevier Inc.","Blockchain; Distributed ledger technology; DLT; Governance; SLR; Systematic literature review"
"Secondary studies on human aspects in software engineering: A tertiary study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149852630&doi=10.1016%2fj.jss.2023.111654&partnerID=40&md5=22ed14cde15b75971878f05ad82a950b","Context: This study compiles the evidence reported on the human aspects of software engineering in view of providing a comprehensive catalogue of human aspects that have been examined. Objective: To summarise the existing systematic literature on human aspects in software engineering. Method: This study employs published tertiary research guidelines to investigate secondary studies published between 1940 and 2021. Results: We identified 67 secondary studies concentrating on 16 different human aspects research categories, including Agile, Economic Factors, Environmental Factors to Productivity, Success, and Teams. Several trends reveal the topics that have received the least (e.g., “software engineer controllers”) and most (e.g., “individual human aspects”) attention in research. Outcomes show that the number of secondary studies on human aspects in software engineering continues to rise when compared to other software engineering topics, despite experiencing a significant drop in 2020. Many secondary studies implemented established guidelines, especially those published in scholarly journals. While there is variance in the quality of published secondary studies, the average quality score across the investigated studies was 3.09 out of 4.0, with journal-published studies and one thesis having higher quality than conference and workshops papers. Specific institutions are also more central to the publication of secondary studies. Furthermore, there has been noteworthy advancement in the consideration of human aspects across the domain. Finally, we discovered several relationships among human aspects investigated. For example, the “Industry” subject of investigation is strongly correlated with the “Theoretical” study type. Conclusion: The overview provided by this study allows researchers and practitioners to gain familiarity with the current state of research on human aspects, helping researchers to identify gaps for further study and allowing practitioners to discover high-quality, evidence-based approaches to harness the power of human aspects in software engineering. © 2023 Elsevier Inc.","Human aspects; Quality assessment; Secondary study; Software engineering; Tertiary study"
"Self-supervised log parsing using semantic contribution difference","2023","Journal of Systems and Software","10.1016/j.jss.2023.111646","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148333828&doi=10.1016%2fj.jss.2023.111646&partnerID=40&md5=99b05102395611343a1eb43801d21a36","Logs can help developers to promptly diagnose software system failures. Log parsers, which parse semi-structured logs into structured log templates, are the first component for automated log analysis. However, almost all existing log parsers have poor generalization ability and only work well for specific systems. In addition, some parsers cannot perform well based on partial data training and cannot support out-of-vocabulary (OOV) words. These limitations can cause erroneous log parsing results. We observe that logs are presented as semi-structured natural language, and we can treat log parsing as a natural language processing task. Thus, we propose Semlog, a novel log parser, requiring no domain knowledge about specific systems. For a log, constant and variable words contribute differently to the semantics of a log. We pretrain a self-attention based model to craft their semantic contribution difference, and then extract log templates based on the pretrained model. We have conducted extensive experiments on 16 benchmark datasets, and the results show that Semlog outperforms the state-of-the-art parsers in terms of average parsing accuracy, reaching 0.987. © 2023 Elsevier Inc.","Deep learning; Log parsing; Log semantics; Self-attention"
"An annotation-based approach for finding bugs in neural network programs","2023","Journal of Systems and Software","10.1016/j.jss.2023.111669","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150451924&doi=10.1016%2fj.jss.2023.111669&partnerID=40&md5=18a5e2fd0e8117128306a86962b18a60","As neural networks are increasingly included as core components of safety–critical systems, developing effective testing techniques specialized for them becomes crucial. The bulk of the research has focused on testing neural-network models; but these models are defined by writing programs, and there is growing evidence that these neural-network programs often have bugs too. This paper presents ANNOTEST: an approach to generating test inputs for neural-network programs. A fundamental challenge is that the dynamically-typed languages (e.g., Python) commonly used to program neural networks cannot express detailed constraints about valid function inputs (e.g., matrices with certain dimensions). Without knowing these constraints, automated test-case generation is prone to producing invalid inputs, which trigger spurious failures and are useless for identifying real bugs. To address this problem, we introduce a simple annotation language tailored for concisely expressing valid function inputs in neural-network programs. ANNOTEST takes as input an annotated program, and uses property-based testing to generate random inputs that satisfy the validity constraints. In the paper, we also outline guidelines that simplify writing ANNOTEST annotations. We evaluated ANNOTEST on 19 neural-network programs from Islam et al's survey. Islam et al. (2019), which we manually annotated following our guidelines — producing 6 annotations per tested function on average. ANNOTEST automatically generated test inputs that revealed 94 bugs, including 63 bugs that the survey reported for these projects. These results suggest that ANNOTEST can be a valuable approach to finding widespread bugs in real-world neural-network programs. © 2023 The Author(s)","Debugging; Neural networks; Python; Test generation"
"A taxonomy of assets for the development of software-intensive products and services","2023","Journal of Systems and Software","10.1016/j.jss.2023.111701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152899759&doi=10.1016%2fj.jss.2023.111701&partnerID=40&md5=321adeb00f841dd3ceb44331c9b54c79","Context: Developing software-intensive products or services usually involves a plethora of software artefacts. Assets are artefacts intended to be used more than once and have value for organisations; examples include test cases, code, requirements, and documentation. During the development process, assets might degrade, affecting the effectiveness and efficiency of the development process. Therefore, assets are an investment that requires continuous management. Identifying assets is the first step for their effective management. However, there is a lack of awareness of what assets and types of assets are common in software-developing organisations. Most types of assets are understudied, and their state of quality and how they degrade over time have not been well-understood. Methods: We performed an analysis of secondary literature and a field study at five companies to investigate and identify assets to fill the gap in research. The results were analysed qualitatively and summarised in a taxonomy. Results: We present the first comprehensive, structured, yet extendable taxonomy of assets, containing 57 types of assets. Conclusions: The taxonomy serves as a foundation for identifying assets that are relevant for an organisation and enables the study of asset management and asset degradation concepts. © 2023 The Author(s)","Asset management in software engineering; Assets for software-intensive products or services; Assets in software engineering; Taxonomy"
"Automatically generating test cases for safety-critical software via symbolic execution","2023","Journal of Systems and Software","10.1016/j.jss.2023.111629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149060202&doi=10.1016%2fj.jss.2023.111629&partnerID=40&md5=7c54edb0954aba5d5d2a19a87b03105a","Automated test generation based on symbolic execution can be beneficial for systematically testing safety-critical software, to facilitate test engineers to pursue the strict testing requirements mandated by the certification standards, while controlling at the same time the costs of the testing process. At the same time, the development of safety-critical software is often constrained with programming languages or coding conventions that ban linguistic features which are believed to downgrade the safety of the programs, e.g., they do not allow dynamic memory allocation and variable-length arrays, limit the way in which loops are used, forbid recursion, and bound the complexity of control conditions. As a matter of facts, these linguistic features are also the main efficiency-blockers for the test generation approaches based on symbolic execution at the state of the art. This paper contributes new evidence of the effectiveness of generating test cases with symbolic execution for a significant class of industrial safety critical-systems. We specifically focus on SCADE, a largely adopted model-based development language for safety-critical embedded software, and we report on a case study in which we exploited symbolic execution to automatically generate test cases for a set of safety-critical programs developed in SCADE. To this end, we introduce an original test generator that we developed in a recent industrial project on testing safety-critical railway software written in SCADE, and we report on our experience of using this test generator for testing a set of SCADE programs that belong to the development of an on-board signaling unit for high-speed rail. The results provide empirically evidence that symbolic execution is indeed a viable approach for generating high-quality test suites for the safety-critical programs considered in our case study. © 2023 Elsevier Inc.","Automated test generation; Safety-critical software; Symbolic execution"
"Deep learning framework testing via hierarchical and heuristic model generation","2023","Journal of Systems and Software","10.1016/j.jss.2023.111681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151257301&doi=10.1016%2fj.jss.2023.111681&partnerID=40&md5=b19c2af91d141c8dc0afb8263445e7ba","Deep learning frameworks are the foundation of deep learning model construction and inference. Many testing methods using deep learning models as test inputs are proposed to ensure the quality of deep learning frameworks. However, there are still critical challenges in model generation, model instantiation, and result analysis. To bridge the gap, we propose Ramos, a hierarchical heuristic deep learning framework testing method. To generate diversified models, we design a novel hierarchical structure to represent the building block of the model. Based on this structure, new models are generated by the mutation method. To trigger more precision bugs in deep learning frameworks, we design a heuristic method to increase the error triggered by models and guide the subsequent model generation. To reduce false positives, we propose an API mapping rule between different frameworks to aid model instantiation. Further, we design different test oracles for crashes and precision bugs respectively. We conduct experiments under three widely-used frameworks (TensorFlow, PyTorch, and MindSpore) to evaluate the effectiveness of Ramos. The results show that Ramos can effectively generate diversified models and detect more deep learning framework bugs, including crashes and precision bugs, with fewer false positives. Additionally, 14 of 15 are confirmed by developers. © 2023 Elsevier Inc.","Deep learning framework; Hierarchical and heuristic model generation; Precision bug; Software testing"
"Diverse title generation for Stack Overflow posts with multiple-sampling-enhanced transforme","2023","Journal of Systems and Software","10.1016/j.jss.2023.111672","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149984186&doi=10.1016%2fj.jss.2023.111672&partnerID=40&md5=f7049fcb22cb46aac461317e37e8ba95","Stack Overflow is one of the most popular programming communities where developers can seek help for their encountered problems. Nevertheless, if inexperienced developers fail to describe their problems clearly, it is hard for them to attract sufficient attention and get the anticipated answers. To address such a problem, we propose M3NSCT5, a novel approach to automatically generate multiple post titles from the given code snippets. Developers may take advantage of the generated titles to find closely related posts and complete their problem descriptions. M3NSCT5 employs the CodeT5 backbone, which is a pre-trained Transformer model with an excellent language understanding and generation ability. To alleviate the ambiguity issue that the same code snippets could be aligned with different titles under varying contexts, we propose the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from. We build a large-scale dataset with 890,000 question posts covering eight programming languages to validate the effectiveness of M3NSCT5. The automatic evaluation results on the BLEU and ROUGE metrics demonstrate the superiority of M3NSCT5 over six state-of-the-art baseline models. Moreover, a human evaluation with trustworthy results also demonstrates the great potential of our approach for real-world applications. © 2023 Elsevier Inc.","CodeT5; Maximal marginal ranking; Nucleus sampling; Stack Overflow; Title generation"
"Optimize along the way: An industrial case study on web performance","2023","Journal of Systems and Software","10.1016/j.jss.2022.111593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146054672&doi=10.1016%2fj.jss.2022.111593&partnerID=40&md5=5aea37f503918e3e395f7678e70568b9","Context: Fast loading web apps can be a key success factor in terms of user experience. However, improving the performance of a web app is not trivial, since it requires a deep understanding of both the browser engine and the specific usage scenarios of the web app under consideration. Aims: In this paper, we present an industrial case study at 30 MHz, an agricultural technology company, in which we target a large web-based dashboard, where its performance was improved via 13 distinct interventions over a four-month period. Moreover, we conduct a user study to analyse whether web performance metrics correlate with the user perceived page load time in optimization scenarios. Methods: First, we design a replicable performance engineering plan, where the technical realization of each intervention is reported in detail along with its development effort. Second, we develop a benchmarking tool that supports 11 widely used web performance metrics. Finally, we use the benchmarking tool to quantitatively evaluate the performance of the target web app and measure the effect of 13 interventions on both desktop and mobile devices. For the user study, we record six videos of different page loads and ask participants about their opinion about the time a web page is considered ready. We calculate the correlation of the user perceived data with each web performance metric. Results: We observe a considerable performance improvement over the course of the 13 interventions. Among others, we achieve 98.37% and 97.56% time reductions on desktop and mobile, respectively, for the First Contentful Paint metric. In addition, we achieve 48.25% and 19.85% improvements for the Speed Index (SI) metric on desktop and mobile, respectively. Our user study shows that the Lowest Time to Widget metric, a product-specific web performance metric, is perfectly correlated with perceived performance during the optimization process. Conclusion: This study shows the importance of a continuous focus on performance engineering in the context of large-scale web apps to improve user browsing experience. We recommend developers to carefully plan their performance engineering activities, since different interventions might require different efforts and can have different effects on the overall performance of the web application. © 2022 The Author(s)","Industrial case study; Performance; Web browsers"
"People want reassurance when making privacy-related decisions — Not technicalities","2023","Journal of Systems and Software","10.1016/j.jss.2023.111620","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148538445&doi=10.1016%2fj.jss.2023.111620&partnerID=40&md5=72c33fea1e9f981980dfeb601d9b7984","Online service users sometimes need support when making privacy-related decisions. Humans make decisions either slowly, by painstakingly consulting all possible information, or quickly, by relying on cues to trigger heuristics. Human emotions elicited by the decision context affects decisions, often without the decision maker being aware of it. We wanted to determine how an information-based decision can be supported, and also to understand which cues are used by a heuristics-based approach. Our first study enhanced understanding of underlying encryption mechanisms using metaphors. Our participants objected to efforts to make them ‘technical experts’, expressing a need for reassurance instead. We fed their free-text responses into a Q-sort, to determine which cues they rely on to make heuristic-based decisions. We confirmed the desire for reassurance. Our third study elicited ‘cyber stories’: Unprompted narratives about cyber-related experiences to detect emotional undertones in this domain. Responses revealed a general negativity, which is bound to influence cybersecurity-related decisions. © 2023 The Author(s)","Cyber; Decision-making; Privacy; Reassurance"
"Software practitioners’ point of view on technical debt payment","2023","Journal of Systems and Software","10.1016/j.jss.2022.111554","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142137065&doi=10.1016%2fj.jss.2022.111554&partnerID=40&md5=48549a0cdf69c057a836f7431c9da397","Context: Technical debt (TD) payment refers to the activity of expending maintenance effort and resources to make up for the effects of previous technical compromises. Aims: To investigate if software practitioners have paid debt items off in their projects, the practices that have been used for paying off debt items, and the issues that hamper the implementation of these practices. Method: We analyze 653 responses collected by surveying practitioners from six countries about TD payment. Results: Practitioners have not paid off TD items in most cases. We identified 27 reasons for not paying off those items and 32 payment-related practices. Practices are mainly related to internal quality issues, while reasons for not paying TD off are mostly associated with planning and management issues. Lastly, we identified relationships between practices and between reasons, indicating that both can appear in combination. Conclusion:. We use different views to consolidate the set of information on TD payment, extending the conceptual model for TD and organizing the set of practices and reasons into a TD payment map. We believe that the model and the map can support practitioners in planning their TD payment strategy. © 2022 Elsevier Inc.","Technical debt; Technical debt management; Technical debt payment"
"An empirical evaluation of the “Cognitive Complexity” measure as a predictor of code understandability","2023","Journal of Systems and Software","10.1016/j.jss.2022.111561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143625898&doi=10.1016%2fj.jss.2022.111561&partnerID=40&md5=562e7394f8c11d903c493405de3c08b0","Background: Code that is difficult to understand is also difficult to inspect and maintain and ultimately causes increased costs. Therefore, it would be greatly beneficial to have source code measures that are related to code understandability. Many “traditional” source code measures, including for instance Lines of Code and McCabe's Cyclomatic Complexity, have been used to identify hard-to-understand code. In addition, the “Cognitive Complexity” measure was introduced in 2018 with the specific goal of improving the ability to evaluate code understandability. Aims: The goals of this paper are to assess whether (1) “Cognitive Complexity” is better correlated with code understandability than traditional measures, and (2) the availability of the “Cognitive Complexity” measure improves the performance (i.e., the accuracy) of code understandability prediction models. Method: We carried out an empirical study, in which we reused code understandability measures used in several previous studies. We first built Support Vector Regression models of understandability vs. code measures, and we then compared the performance of models that use “Cognitive Complexity” against the performance of models that do not. Results: “Cognitive Complexity” appears to be correlated to code understandability approximately as much as traditional measures, and the performance of models that use “Cognitive Complexity” is extremely close to the performance of models that use only traditional measures. Conclusions: The “Cognitive Complexity” measure does not appear to fulfill the promise of being a significant improvement over previously proposed measures, as far as code understandability prediction is concerned. © 2022 Elsevier Inc.","Cognitive complexity; Complexity measures; Software code measures; Software understandability; Static code measures"
"Checklists to support decision-making in regression testing","2023","Journal of Systems and Software","10.1016/j.jss.2023.111697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153245617&doi=10.1016%2fj.jss.2023.111697&partnerID=40&md5=70c6d8715880e389385daba0918ac773","Context: Practitioners working in large-scale software development face many challenges in regression testing activities. One of the reasons is the lack of a structured regression testing process. In this regard, checklists can help practitioners keep track of essential regression testing activities and add structure to the regression testing process to a certain extent. Objective: This study aims to introduce regression testing checklists so test managers/teams can use them: (1) to assess whether test teams/members are ready to begin regression testing, and (2) to keep track of essential regression testing activities while planning and executing regression tests. Method: We used interviews, workshops, and questionnaires to design, evolve, and evaluate regression testing checklists. In total, 25 practitioners from 12 companies participated in creating the checklist. Twenty-three of them participated in checklists evolution and evaluation. Results: We identified activities practitioners consider significant while planning, performing, and analyzing regression testing. We designed regression testing checklists based on these activities to help practitioners make informed decisions during regression testing. With the help of practitioners, we evolved these checklists into two iterations. Finally, the practitioners provided feedback on the proposed checklists. All respondents think the proposed checklists are useful and customizable for their environments, and 80% think checklists cover aspects essential for regression testing. Conclusion: The proposed regression testing checklists can be useful for test managers to assess their team/team members’ readiness and decide when to start and stop regression testing. The checklists can be used to record the steps required while planning and executing regression testing. Further, these checklists can provide a basis for structuring the regression testing process in varying contexts. © 2023 The Author(s)","Checklists; Process improvement; Regression testing; Team readiness; Test manager"
"A symbolic algorithm for the case-split rule in solving word constraints with extensions","2023","Journal of Systems and Software","10.1016/j.jss.2023.111673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150809916&doi=10.1016%2fj.jss.2023.111673&partnerID=40&md5=5e75855fd25486d51919b08588dfaf34","Case split is a core proof rule in current decision procedures for the theory of string constraints. Its use is the primary cause of the state space explosion in string constraint solving, since it is the only rule that creates branches in the proof tree. Moreover, explicit handling of the case split rule may cause recomputation of the same tasks in multiple branches of the proof tree. In this paper, we propose a symbolic algorithm that significantly reduces such a redundancy. In particular, we encode a string constraint as a regular language and proof rules as rational transducers. This allows us to perform similar steps in the proof tree only once, alleviating the state space explosion. We also extend the encoding to handle arbitrary Boolean combinations of string constraints, length constraints, and regular constraints. In our experimental results, we validate that our technique works in many practical cases where other state-of-the-art solvers fail to provide an answer; our Python prototype implementation solved over 50% of string constraints that could not be solved by the other tools. © 2023 Elsevier Inc.","Finite automata; Monadic second-order logic over strings; Nielsen transformation; Regular model checking; Satisfiability modulo theories; String constraints"
"A model-based mode-switching framework based on security vulnerability scores","2023","Journal of Systems and Software","10.1016/j.jss.2023.111633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148325599&doi=10.1016%2fj.jss.2023.111633&partnerID=40&md5=b0aa5bec27f59205cc8c8dbe03fd754c","Software vulnerabilities can affect critical systems within an organization impacting processes, workflows, privacy, and safety. When a software vulnerability becomes known, affected systems are at risk until appropriate updates become available and eventually deployed. This period can last from a few days to several months, during which attackers can develop exploits and take advantage of the vulnerability. It is tedious and time-consuming to keep track of vulnerabilities manually and perform necessary actions to shut down, update, or modify systems. Vulnerabilities affect system components, such as a web server, but sometimes only target specific versions or component combinations. In this paper, we propose a novel approach for automated mode switching of software systems to support system administrators in dealing with vulnerabilities and reducing the risk of exposure. We rely on model-driven techniques and use a multi-modal architecture to react to discovered vulnerabilities and provide automated contingency support. We have developed a dedicated domain-specific language to describe potential mitigation as mode switches. We have evaluated our approach with a web server case study, analyzing historical vulnerability data. Based on the vulnerabilities scores sum, we demonstrated that switching to less vulnerable modes reduced the attack surface in 98.9% of the analyzed time. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)","Domain-specific languages; Mode switching; Resilience; Security; Vulnerabilities"
"ExTrA: Explaining architectural design tradeoff spaces via dimensionality reduction","2023","Journal of Systems and Software","10.1016/j.jss.2022.111578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144456299&doi=10.1016%2fj.jss.2022.111578&partnerID=40&md5=a910344cffb4a60dd452228482b18de2","In software design, guaranteeing the correctness of run-time system behavior while achieving an acceptable balance among multiple quality attributes remains a challenging problem. Moreover, providing guarantees about the satisfaction of those requirements when systems are subject to uncertain environments is even more challenging. While recent developments in architectural analysis techniques can assist architects in exploring the satisfaction of quantitative guarantees across the design space, existing approaches are still limited because they do not explicitly link design decisions to satisfaction of quality requirements. Furthermore, the amount of information they yield can be overwhelming to a human designer, making it difficult to see the forest for the trees. In this paper we present ExTrA (Explaining Tradeoffs of software Architecture design spaces), an approach to analyzing architectural design spaces that addresses these limitations and provides a basis for explaining design tradeoffs. Our approach employs dimensionality reduction techniques employed in machine learning pipelines like Principal Component Analysis (PCA) and Decision Tree Learning (DTL) to enable architects to understand how design decisions contribute to the satisfaction of extra-functional properties across the design space. Our results show feasibility of the approach in two case studies and evidence that combining complementary techniques like PCA and DTL is a viable approach to facilitate comprehension of tradeoffs in poorly-understood design spaces. © 2022 The Author(s)","Dimensionality reduction; Tradeoff analysis; Uncertainty"
"Status indicators in software engineering group projects","2023","Journal of Systems and Software","10.1016/j.jss.2023.111612","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146055808&doi=10.1016%2fj.jss.2023.111612&partnerID=40&md5=4011f6f999a2c9acc370b97e1d25bae2","A segment of studies on group structure and performance in software engineering (SE) project-based learning (PjBL) have focused on roles, including studies that use Belbin team roles and studies that address problematic roles such as social loafing. The present study focuses on the status, which is basically missing in SE PjBL studies, although relating to roles. The study investigates the aspects that students identified as indicators of rising or declining status in their project groups. The status theory was utilized as the framework that motivated the research and on which the results were reflected. An inductive qualitative content analysis was applied to learning reports in which students reflected on their statuses. The indicators of rising status included technical know-how, commitment, management responsibility, and idea ownership, while also group-level attributes such as a caring atmosphere and joint responsibility. The indicators of a declining status included aspects that appear as counterparts of rising status indicators, while also more refined aspects such as no one willing to be a leader or study background. The results are concluded to provide material for educating students about intra-group relations and promoting self-regulation for fruitful collaboration in groups. The authors believe that the results also initiate further PjBL research in which status theory can be utilized. © 2023 The Author(s)","Group work; Higher education; Status concept"
"A systematic literature review on Android-specific smells","2023","Journal of Systems and Software","10.1016/j.jss.2023.111677","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150822158&doi=10.1016%2fj.jss.2023.111677&partnerID=40&md5=481a5c2bf399fde678eb79539b329d75","Context: Code smells are well-known concepts in Object-Oriented (OO) programs as symptoms that negatively impact software quality and cause long-term issues. However, the domain-specific smells in Android have not yet been investigated well. Android smells often refer to the misuse of mobile SDK and causes of performance, accessibility, and efficiency issues that end-users can perceive. Objective: This study aims to provide a clear overview of state-of-the-art techniques for addressing Android-specific code smells to understand existing methods and open challenges, which help the community understand the significance of Android smells and the current status of research. Methods: We conducted a Systematic Literature Review of 4,820 distinct papers published until 2021, following a consolidated methodology applied in software engineering. 35 primary studies were selected. Results: The known Android smells cannot be treated equally in the proposed approaches, as they mainly focus on detecting performance-related smells. The proposed approaches capture various features to detect smell instances using different analysis techniques in Android applications. In addition, the Android community continuously identifies new types of smells to improve apps’ quality. Conclusion: The research community still encounters several challenges. Thus, this paper outlines various directions for the necessary investigation as future work. © 2023 Elsevier Inc.","Android; Code smell; Systematic literature review"
"Explaining quality attribute tradeoffs in automated planning for self-adaptive systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145663959&doi=10.1016%2fj.jss.2022.111538&partnerID=40&md5=94edc61b35ee36a6c95ed11b3f76ecd4","Self-adaptive systems commonly operate in heterogeneous contexts and need to consider multiple quality attributes. Human stakeholders often express their quality preferences by defining utility functions, which are used by self-adaptive systems to automatically generate adaptation plans. However, the adaptation space of realistic systems is large and it is obscure how utility functions impact the generated adaptation behavior, as well as structural, behavioral, and quality constraints. Moreover, human stakeholders are often not aware of the underlying tradeoffs between quality attributes. To address this issue, we present an approach that uses machine learning techniques (dimensionality reduction, clustering, and decision tree learning) to explain the reasoning behind automated planning. Our approach focuses on the tradeoffs between quality attributes and how the choice of weights in utility functions results in different plans being generated. We help humans understand quality attribute tradeoffs, identify key decisions in adaptation behavior, and explore how differences in utility functions result in different adaptation alternatives. We present two systems to demonstrate the approach's applicability and consider its potential application to 24 exemplar self-adaptive systems. Moreover, we describe our assessment of the tradeoff between the information reduction and the amount of explained variance retained by the results obtained with our approach. © 2022 The Author(s)","Automated planning; Clustering; Decision tree learning; Explainable software; Non-functional requirements; Principal component analysis; Quality attributes; Self-adaptation"
"Towards automated Android app internationalisation: An exploratory study","2023","Journal of Systems and Software","10.1016/j.jss.2022.111559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143723015&doi=10.1016%2fj.jss.2022.111559&partnerID=40&md5=537e9e0734f8ca9a035651f5d39fb3b1","Android has become the most popular mobile platform with over 2.5 billion active users who use many different languages across many different countries. In order for Android apps to be useable by all of them, app developers usually need to add an internationalisation feature that adapts the app to the users’ linguistic and cultural requirements. Such a process, including the translation from the default language to up to thousands of languages, is usually achieved via manual efforts and hence is resource-intensive, time-consuming, and error-prone. Automated approaches are hence in demand to help developers mitigate such manual efforts. Since there are millions of apps proposed already for Android users, we are interested in knowing to what extent internationalisation has been supported. Our experimental results show that Android apps, at least the ones released on online markets, have mostly been equipped with internationalisation features, with the number of supported languages varies significantly. By mapping the actual term translations among different languages, we further find that the translations tend to be consistent among different apps, suggesting the possibility to learn from this data to achieve automated app internalisation. To explore this idea we implemented a Transformer-based prototype approach Androi18n, that learns from developers’ practical translations to achieve automated mobile app text translations. Experimental results show that Androi18n is effective in achieving our objective, and its high performance is generic across the translations of different languages. © 2022 Elsevier Inc.","Android; Apps; Internationalisation; Languages"
"Efficient transformer with code token learner for code clone detection","2023","Journal of Systems and Software","10.1016/j.jss.2022.111557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142914543&doi=10.1016%2fj.jss.2022.111557&partnerID=40&md5=43fe0b0e9ccc4813ca9b50e9d6add056","Deep learning techniques have achieved promising results in code clone detection in the past decade. Unfortunately, current deep learning-based methods rarely explicitly consider the modeling of long codes. Worse, the code length is increasing due to the increasing requirement of complex functions. Thus, modeling the relationship between code tokens to catch their long-range dependencies is crucial to comprehensively capture the information of the code fragment. In this work, we resort to the Transformer to capture long-range dependencies within a code, which however requires huge computational cost for long code fragments. To make it possible to apply Transformer efficiently, we propose a code token learner to largely reduce the number of feature tokens in an automatic way. Besides, considering the tree structure of the abstract syntax tree, we present a tree-based position embedding to encode the position of each token in the input. Apart from the Transformer that captures the dependency within a code, we further leverage a cross-code attention module to capture the similarities between two code fragments. Our method significantly reduces the computational cost of using Transformer by 97% while achieves superior performance with state-of-the-art methods. Our code is available at https://github.com/ArcticHare105/Code-Token-Learner. © 2022 Elsevier Inc.","Code clone detection; Code token learner; Efficient transformer"
"Empirical investigation in embedded systems: Quality attributes in general, maintainability in particular","2023","Journal of Systems and Software","10.1016/j.jss.2023.111678","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150859186&doi=10.1016%2fj.jss.2023.111678&partnerID=40&md5=53e70b442dc91a65e333e5b2f9292971","The quality of software systems is an important aspect, especially for embedded systems, thus strategies and actions for analyzing the trade-off between various quality attributes need to be improved. Objectives: We target firstly to determine which quality attributes are important in embedded systems, and secondly to inquire about maintainability in particular, emphasizing the practices that are associated with it, i.e., coding rules, conventions, documentation, code review, and refactoring. Method: We used interviews and surveys as means to investigate practitioners’ points of view and practices. Applying quantitative and qualitative analysis, we explored a general perspective of quality attributes in embedded systems, followed by specific practices related to the maintainability attribute. Results: At the general perspective level, we learned that the importance of security and safety is extended to all embedded systems, while maintainability remains of major importance, and there is a diversity of methods used to assure the quality of systems throughout the development cycle. At the maintainability-specific level, we learned that code review and refactoring are the most used practices and that the related activities are performed in a variety of ways. Conclusions: Our work recognizes various quality attributes as being important with different priorities, respectively analyses which maintainability-related activities are used. © 2023 Elsevier Inc.","Embedded systems; Empirical study; Quality factors"
"Decentralized decision-making and scaled autonomy at Spotify","2023","Journal of Systems and Software","10.1016/j.jss.2023.111649","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149631583&doi=10.1016%2fj.jss.2023.111649&partnerID=40&md5=ecdb0f45516061b562ff0f2814a88aa7","While modern software companies strive to increase team autonomy to enable them to successfully operate the piece of software they develop and deploy, efficient ways to orchestrate the work of multiple autonomous teams working in parallel are still poorly understood. In this paper, we report how team autonomy is maintained at Spotify at scale, based on team retrospectives, interviews with team managers and archival analysis of corporate databases and work procedures. In particular, we describe how managerial authority is decentralized through various workgroups with collective authority, what compromises are made to team autonomy to ensure alignment and which team-related factors can further hinder autonomy. Our findings show that scaled autonomy at Spotify does not mean anarchy, or unlimited permissiveness. Instead, squads are expected to take responsibility for their work and coordinate, communicate and align their actions with others, and comply with a few enabling constraints. Further, squads take many decisions independently without management control or due to collective efforts that bypass formal boundary structures. Mechanisms and strategies that enable self-organization at Spotify are related to effective sharing of the codebase, achieving alignment, networking and knowledge sharing, and are described to guide other companies in their efforts to scale autonomy. © 2023 The Author(s)","Coordination; Enabling constraints; Large-scale software development; Scaled autonomy; Scaling agile; The Spotify model"
"An empirical evaluation of quasi-static executable slices","2023","Journal of Systems and Software","10.1016/j.jss.2023.111666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149910423&doi=10.1016%2fj.jss.2023.111666&partnerID=40&md5=c6405ff8ce6a4bdbef7b52b071eb1527","Program slicing aims to reduce a program to a minimal form that produces the same output for a given slicing criterion. Program slicing approaches divide into static and dynamic approaches: whereas static approaches generate an over-approximation of the slice that is valid for all possible program inputs, dynamic approaches rely on executing the program and thus generate an under-approximation of the slice that is valid for only a subset of the inputs. An important limitation of static approaches is that they often do not generate an executable program, but rather identify only those program components upon which the slicing criterion depends (referred to as a closure slice). In order to overcome this limitation, we propose a novel approach that combines static and dynamic slicing. We rely on observation-based slicing, a dynamic approach, but protect all statements that have been identified as part of the static slice by the static slicer CodeSurfer. As a result, we obtain slices that cover at least the behavior of the static slice, and that can be compiled and executed. We evaluated this new approach on a set of 62 C programs and report our findings. © 2023 Elsevier Inc.","Dynamic slicing; Program dependence analysis; Program slicing; Static slicing"
"On the dependency heaviness of CRAN/Bioconductor ecosystem","2023","Journal of Systems and Software","10.1016/j.jss.2023.111610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145972589&doi=10.1016%2fj.jss.2023.111610&partnerID=40&md5=07fbc8e071db0b2438aa17f0c35910a9","The R package ecosystem is expanding fast and dependencies among packages are becoming more complex in the ecosystem. In this study, we explored the package dependencies from a new aspect. We applied a new metric named “dependency heaviness” which measures the number of additional strong dependencies that a package uniquely contributes to its child or downstream packages. We systematically studied how the dependency heaviness spreads from parent to child packages, and how it further spreads to remote downstream packages in the CRAN/Bioconductor ecosystem. We extracted top packages and key paths that majorly transmit heavy dependencies in the ecosystem. Additionally, the dependency heaviness analysis on the ecosystem has been implemented as a web-based database that provides comprehensive tools for querying dependencies of individual R packages. © 2023 Elsevier Inc.","Bioconductor; CRAN; Dependency graph; Dependency heaviness; Software ecosystem; Software engineering"
"ExploitGen: Template-augmented exploit code generation based on CodeBERT","2023","Journal of Systems and Software","10.1016/j.jss.2022.111577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144848774&doi=10.1016%2fj.jss.2022.111577&partnerID=40&md5=4a61a8a61919f928ecb3320eaeca1a1b","Exploit code is widely used for detecting vulnerabilities and implementing defensive measures. However, automatic generation of exploit code for security assessment is a challenging task. In this paper, we propose a novel template-augmented exploit code generation approach ExploitGen based on CodeBERT. Specifically, we first propose a rule-based Template Parser to generate template-augmented natural language descriptions (NL). Both the raw and template-augmented NL sequences are encoded to context vectors by the respective encoders. For better learning semantic information, ExploitGen incorporates a semantic attention layer, which uses the attention mechanism to extract and calculate each layer's representational information. In addition, ExploitGen computes the interaction information between the template information and the semantics of the raw NL and designs a residual connection to append the template information into the semantics of the raw NL. Comprehensive experiments on two datasets show the effectiveness of ExploitGen after comparison with six state-of-the-art baselines. Apart from the automatic evaluation, we conduct a human study to evaluate the quality of generated code in terms of syntactic and semantic correctness. The results also confirm the effectiveness of ExploitGen. © 2022 Elsevier Inc.","Code generation; CodeBERT; Exploit code; Neural network; Template parser"
"Automated data validation: An industrial experience report","2023","Journal of Systems and Software","10.1016/j.jss.2022.111573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144420892&doi=10.1016%2fj.jss.2022.111573&partnerID=40&md5=99f44e03ae57463f2cfd37d0942c0177","There has been a massive explosion of data generated by customers and retained by companies in the last decade. However, there is a significant mismatch between the increasing volume of data and the lack of automation methods and tools. The lack of best practices in data science programming may lead to software quality degradation, release schedule slippage, and budget overruns. To mitigate these concerns, we would like to bring software engineering best practices into data science. Specifically, we focus on automated data validation in the data preparation phase of the software development life cycle. This paper studies a real-world industrial case and applies software engineering best practices to develop an automated test harness called RESTORE. We release RESTORE as an open-source R package. Our experience report, done on the geodemographic data, shows that RESTORE enables efficient and effective detection of errors injected during the data preparation phase. RESTORE also significantly reduced the cost of testing. We hope that the community benefits from the open-source project and the practical advice based on our experience. © 2022 Elsevier Inc.","Data science; Data validation; Software engineering best practice; Test automation"
"Goal model convergence and conflict detection for crossover services","2023","Journal of Systems and Software","10.1016/j.jss.2023.111625","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150357889&doi=10.1016%2fj.jss.2023.111625&partnerID=40&md5=84278cf11726dbab477dcf7122f4cec6","As a new form of service model, crossover services aim to aggregate service resources across multiple domains to meet the complex needs of users and provide value-added services. It has received extensive attention from industry and academia due to its advantages in promoting enterprise services innovation. Crossover services span various business domains intending to meet diverse and continuously changing user requirements across multiple domains. Requirements engineering for crossover services must model user goals in various domains and converge them deeply to drive the subsequent service realization. Due to the domain heterogeneity, however, conflicts may frequently arise after the convergence of goals. Towards this issue, we propose a goal decomposition path-based method to support goal convergence of multiple domains and a computation tree logic-based method to detect conflicts between goals in the converged goal model. We evaluate the proposed method using several real cases from the literature and industry and design a controlled experiment to further assess the method's performance. Experimental results show the effectiveness of our proposed method. © 2023 Elsevier Inc.","Conflict detection; Crossover services; Goal convergence; Goal modeling; Requirements engineering"
"Boosting source code suggestion with self-supervised Transformer Gated Highway","2023","Journal of Systems and Software","10.1016/j.jss.2022.111553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145576596&doi=10.1016%2fj.jss.2022.111553&partnerID=40&md5=d908580c7b4657222b2b768549937c97","Attention-based transformer language models have shown significant performance gains in various natural language tasks. In this work, we explore the impact of transformer language models on the task of source code suggestion. The core intention of this work is to boost the modeling performance for the source code suggestion task and to explore how the training procedures and model architectures impact modeling performance. Additionally, we propose a transformer-based self-supervised learning technique called Transformer Gated Highway that outperforms recurrent and transformer language models of comparable size. The proposed approach combines the Transformer language model with Gated Highway introducing a notion of recurrence. We compare the performance of the proposed approach with transformer-based BERT (CodeTran), RoBERTa (RoBERTaCode), GPT2 (TravTrans), CodeGen and recurrent neural language-based LSTM (CodeLSTM) models. Moreover, we have experimented with various architectural settings for the transformer models to evaluate their impact on modeling performance. The extensive evaluation of the presented approach exhibits better performance on two programming language datasets; Java and C#. Additionally, we have adopted the presented approach for the syntax error correction task to predict the correct syntax token to render its possible implications for other source code modeling tasks. © 2022 Elsevier Inc.","Deep learning; Source code modeling; Source code suggestion; Transformer models"
"Investigating acceptance behavior in software engineering—Theoretical perspectives","2023","Journal of Systems and Software","10.1016/j.jss.2022.111592","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146227386&doi=10.1016%2fj.jss.2022.111592&partnerID=40&md5=4df944097102b7c6c0fb2ecbb7397c86","Background: Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology. Objective: To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development. Method: We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking). Results: We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior. Conclusions: Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories. © 2022 The Author(s)","Acceptance behavior; Dual process theory; TAM; Technology acceptance; Theory; TPB; UTAUT"
"A component framework for the runtime enforcement of safety properties","2023","Journal of Systems and Software","10.1016/j.jss.2022.111605","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145768750&doi=10.1016%2fj.jss.2022.111605&partnerID=40&md5=9dc46a1729cfdb0937150262de63c1c7","Safety assurance of a complex system cannot be completely ensured at design/development time since most uncertainties and unknowns are revealed when the system is deployed in a real environment. Safety assurance at runtime can be addressed by using models formalizing those safety assertions the system has to guarantee during operation, and specifying enforcement strategies aimed at preserving or eventually restoring safety. This paper presents an approach to runtime safety enforcement of software systems based on the MAPE-K control loop architecture for system monitoring and control, and on the State Machine as runtime model to specify safety assertions and enforcement strategies for steering the correct system behavior. The enforcer software is designed to act as a proxy system which wraps around the software system to realize safety enforcement, both as black-box enforcement on unsafe I/O events and as gray-box enforcement on unsafe internal system changes. The proposed approach is supported by a component framework called RSE (Runtime Safety Enforcement) that is here illustrated by means of two real case studies in the health-care domain. © 2022 Elsevier Inc.","Abstract State Machines@run.time; MAPE-K; Runtime models; Runtime safety enforcement; Self-adaptation"
"A framework for analyzing context-oriented programming languages","2023","Journal of Systems and Software","10.1016/j.jss.2023.111614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146672061&doi=10.1016%2fj.jss.2023.111614&partnerID=40&md5=bd0a0fd1bb36338f21a26f28bb0dc338","Context-aware systems keep on emerging as an intrinsic part of everyday activities. To cope with such situations, programming languages are extended to support the notion of context. Although context-oriented programming languages exist for over 15 years, they were tested for their suitability for developing context-aware systems only to a limited extent. In this paper, we propose a framework for analyzing the suitability of context-oriented languages from a wider viewpoint. Using this framework, we are able to examine context definition and activation, reasoning capabilities, process aspects of how to work with the language, and other pragmatic considerations. To demonstrate the use of the framework, we apply it to analyze three context-oriented programming languages: ServalCJ, Subjective-C, and COBPjs which represent the major Context-Oriented Programming themes. We evaluate the capabilities of each language using the purposed framework. Developers of context-oriented programming languages can use the framework to improve their languages and the associated development and supporting tools. Furthermore, such analysis can support users of context-oriented programming languages in deciding the language that best suits their needs. © 2023 Elsevier Inc.","Comparison; Context-oriented programming; Evaluation"
"On the relationship between source-code metrics and cognitive load: A systematic tertiary review","2023","Journal of Systems and Software","10.1016/j.jss.2023.111619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147192804&doi=10.1016%2fj.jss.2023.111619&partnerID=40&md5=a41e5a7f03f63fe01e78aef0a2605150","The difficulty of software development tasks depends on several factors including the characteristics of the underlying source-code. These characteristics can be captured and measured using source-code metrics, which, in turn, can provide indications about the difficulty of the source-code. From a cognitive perspective, this difficulty is due to an increase in developers’ cognitive load, which can be estimated using psycho-physiological measures. Based on these measures, a handful of studies investigated the relationship between source-code metrics and cognitive load. For most of the metrics, such a relationship could not be established. While these studies used a small subset of metrics, the literature comprises hundreds of other metrics. Despite the existing reviews surveying these metrics, a consolidated overview is still needed to understand their properties and leverage their potential to align with cognitive load. This need is addressed in this paper through a Systematic Tertiary Review (STR) covering the full spectrum of source-code metrics, studying their properties and investigating their potential relationship to cognitive load. The outcome of this STR is intended to guide practitioners in choosing appropriate metrics, set the grounds for conceptualizing the relationship between source-code metrics and cognitive load and raise new research challenges for the future. © 2023 The Author","Cognitive load; Software quality; Source-code metrics; Source-code readability"
"IT managers’ perspective on Technical Debt Management","2023","Journal of Systems and Software","10.1016/j.jss.2023.111700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153040907&doi=10.1016%2fj.jss.2023.111700&partnerID=40&md5=02c8bc5cbf2279557fc119ad2d9c2099","Context: Technical Debt (TD) is a term for software solutions that are beneficial in the short-term but impede future change. Goal: Previous research on TD indicates various management-related causes. We analyze the perspective of IT managers on TD since they usually have a major influence on deadlines, the project's budget, and setting up a TD management (TDM) process. Method: To determine the IT managers’ perspective, we obtained and analyzed data from 16 semi-structured interviews and a three-person focus group discussion. Results: We found that all IT managers understood the TD concept. They consider TDM to be an essential topic, though nearly none of them had set up a TDM process so far. We identified three major concerns the IT managers had regarding TDM: communicating about TD, establishing a TDM process, and dealing with vintage systems, i.e., old legacy systems We developed a model specifying causes and consequences visible to business stakeholders, causal chains, and vicious cycles. Conclusions: Our research identifies new research gaps and demonstrates to practitioners that investing in a TDM process may be beneficial. It provides the V4CTD model of Visibility, Cycles & Chains of Causes & Consequences of TD, extending the TD conceptual model and facilitating communication on TD with business stakeholders. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Legacy systems; Technical Debt; Technical Debt causes; Technical Debt communication; Technical Debt consequences; Technical Debt Management"
"The uphill journey of FaaS in the open-source community","2023","Journal of Systems and Software","10.1016/j.jss.2022.111589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144619088&doi=10.1016%2fj.jss.2022.111589&partnerID=40&md5=523b457b5425842156e861adf0be3fdc","Since its introduction in 2014 by Amazon, the Function as a Service (FaaS) model of serverless computing has set the expectation to fulfill the promise of on-demand, pay-as-you-go, infrastructure-independent processing, originally formulated by cloud computing. Yet, serverless applications are fundamentally different than traditional service-oriented software in that they pose specific performance (e.g., cold start), design (e.g., stateless), and development challenges (e.g., debugging). A growing number of cloud solutions have been continuously attempting to address each of these challenges as a result of the increasing popularity of FaaS. Yet, the characteristics of this model have been poorly understood; therefore, the challenges are poorly tackled. In this paper, we assess the state of FaaS in open-source community with a study on almost 2K real-world serverless applications. Our results show a jeopardized ecosystem, where, despite the hype of serverless solutions in the last years, a number of challenges remain untackled, especially concerning component reuse, support for software development, and flexibility among different platforms — resulting in arguably slow adoption of the FaaS model. We believe that addressing the issues discussed in this paper may help researchers shaping the next generation of cloud computing models. © 2022 Elsevier Inc.","Cloud computing; FaaS; Serverless"
"Run-time failure detection via non-intrusive event analysis in a large-scale cloud computing platform","2023","Journal of Systems and Software","10.1016/j.jss.2023.111611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147093400&doi=10.1016%2fj.jss.2023.111611&partnerID=40&md5=5042c350e96fe2e871b6f73663270ebf","Cloud computing systems fail in complex and unforeseen ways due to unexpected combinations of events and interactions among hardware and software components. These failures are especially problematic when they are silent, i.e., not accompanied by any explicit failure notification, hindering the timely detection and recovery. In this work, we propose an approach to run-time failure detection tailored for monitoring multi-tenant and concurrent cloud computing systems. The approach uses a non-intrusive form of event tracing, without manual changes to the system's internals to propagate session identifiers (IDs), and builds a set of lightweight monitoring rules from fault-free executions. We evaluated the effectiveness of the approach in detecting failures in the context of the OpenStack cloud computing platform, a complex and “off-the-shelf” distributed system, by executing a campaign of fault injection experiments in a multi-tenant scenario. Our experiments show that the approach detects the failure with an F1 score (0.85) and accuracy (0.77) higher than the ones provided by the OpenStack failure logging mechanisms (0.53 and 0.50) and two non-session-aware run-time verification approaches (both lower than 0.15). Moreover, the approach significantly decreases the average time to detect failures at run-time (∼114 seconds) compared to the OpenStack logging mechanisms. © 2023 Elsevier Inc.","Cloud computing; Failure detection; Fault injection; OpenStack; Run-time verification"
"Analyzing the impact of API changes on Android apps","2023","Journal of Systems and Software","10.1016/j.jss.2023.111664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150301777&doi=10.1016%2fj.jss.2023.111664&partnerID=40&md5=46f94cc916416b7f9a89eca42f0ee647","The continuous evolution of Android mobile operating system leads to regular updates to its APIs, which may compromise the functionality of Android apps. Given the high frequency of Android API updates, analyzing the impact of API changes is vital to ensure the high reliability of Android apps. This paper introduces APICIA, a novel approach to analyzing the impact of API changes on Android apps. APICIA investigates the impact of changing the target API and identifies the affected program elements (i.e., classes, methods, and statements), the affected tests whose executions may exhibit changed behaviors as a result of the API update, as well as the app code that is not covered by the existing tests. We evaluate APICIA on 219 real-world Android apps. According to the results, API changes impact 46.30% of tests per app on average, and regression test selection based on APICIA can be cost effective. Moreover, many affected statements are not covered by existing tests, which indicates APICIA can help with test suite augmentation to achieve better coverage. These findings suggest that APICIA is a promising approach for assisting Android developers with understanding, testing, and debugging Android apps that are subject to rapid API updates. © 2023 Elsevier Inc.","Android; API evolution; Change impact analysis; Regression testing"
"Web API evolution patterns: A usage-driven approach","2023","Journal of Systems and Software","10.1016/j.jss.2023.111609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147090383&doi=10.1016%2fj.jss.2023.111609&partnerID=40&md5=93e60e9d1ab20eadc1340018e5b73305","As the use of Application Programming Interfaces (APIs) is increasingly growing, their evolution becomes more challenging in terms of the service provided according to consumers’ needs. In this paper, we address the role of consumers’ needs in WAPIs evolution and introduce a process mining pattern-based method to support providers in WAPIs evolution by analyzing and understanding consumers’ behavior, imprinted in WAPI usage logs. We take the position that WAPIs’ evolution should be mainly usage-based, i.e., the way consumers use them should be one of the main drivers of their changes. We start by characterizing the structural relationships between endpoints, and next, we summarize these relationships into a set of behavioral patterns (i.e., usage patterns whose occurrences indicate specific consumers’ behavior like repetitive or consecutive calls), that can potentially imply the need for changes (e.g., creating new parameters for endpoints, merging endpoints). We analyze the logs and extract several metrics for the endpoints and their relationships, to then detect the patterns. We apply our method in two real-world WAPIs from different domains, education, and health, respectively the WAPI of Barcelona School of Informatics at the Polytechnic University of Catalonia (Facultat d'Informàtica de Barcelona, FIB, UPC), and District Health Information Software 2 (DHIS2) WAPI. The feedback from consumers and providers of these WAPIs proved the effectiveness of the detected patterns and confirmed the promising potential of our approach. © 2023 The Authors","Process mining; Usage patterns; Web API evolution; Web API logs"
"Automatic creation of acceptance tests by extracting conditionals from requirements: NLP approach and case study","2023","Journal of Systems and Software","10.1016/j.jss.2022.111549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142730522&doi=10.1016%2fj.jss.2022.111549&partnerID=40&md5=624862150366014cba38e93f1c14e413","Acceptance testing is crucial to determine whether a system fulfills end-user requirements. However, the creation of acceptance tests is a laborious task entailing two major challenges: (1) practitioners need to determine the right set of test cases that fully covers a requirement, and (2) they need to create test cases manually due to insufficient tool support. Existing approaches for automatically deriving test cases require semi-formal or even formal notations of requirements, though unrestricted natural language is prevalent in practice. In this paper, we present our tool-supported approach CiRA (Conditionals in Requirements Artifacts) capable of creating the minimal set of required test cases from conditional statements in informal requirements. We demonstrate the feasibility of CiRA in a case study with three industry partners. In our study, out of 578 manually created test cases, 71.8% can be generated automatically. Additionally, CiRA discovered 80 relevant test cases that were missed in manual test case design. CiRA is publicly available at www.cira.bth.se/demo/. © 2022","Acceptance testing; Automatic test case creation; Causality extraction; Natural language processing; Requirements engineering"
"MSTIL: Multi-cue Shape-aware Transferable Imbalance Learning for effective graphic API recommendation","2023","Journal of Systems and Software","10.1016/j.jss.2023.111650","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149057204&doi=10.1016%2fj.jss.2023.111650&partnerID=40&md5=48631d16fa8636b3e3522fdebaa597a5","Application Programming Interface (API) recommendation based on graphs is a valuable task in the fields of data visualization and software engineering. However, this task was previously undefined until a recently published paper coining the task as Plot2API and utilizing a deep learning-based method named SPGNN. Compared to general image classification methods, this dedicated approach uses semantic parsing to exploit deep features and yields better performance. However, its performance declines sharply in unbalanced datasets, thus limiting its generalizability. To address this issue, we propose a method named Multi-cue Shape and software engineering-aware Transferable Imbalance Learning (MSTIL), consisting of three major components: Cross-Language Shape-Aware Plot Transfer Learning (CLSAPTL), Cross-Language API Semantic Similarity-based Data Augmentation (CLASSDA), and Imbalance Plot2API Learning (IPL). Motivated by the hierarchical classification of the graphs, CLSAPTL guides the model to learn the graphs’ class hierarchy and thereby enabling the model to learn more transferable visual features. Given that a graph can be associated with multiple APIs and motivated by the fact that many APIs that exert similar functions in different languages have semantically similar names, CLASSDA leverages the samples of APIs with semantically similar names to assist in feature learning. Finally, inspired by the essence of softmax cross entropy loss, IPL alleviates the imbalances between positive and negative samples during training. We conduct our experiments on two public datasets. Extensive experimental results shows that MSTIL improves the performance of classic CNNs along with the state-of-the-art method, demonstrating its effectiveness. Specifically, MSTIL has an average relative mAP improvement of 12.94% across the models on all datasets. © 2023 The Author(s)","API recommendation; Data augmentation; Data visualization; Semantic similarity; Transfer learning"
"Applications of natural language processing in software traceability: A systematic mapping study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111616","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146959703&doi=10.1016%2fj.jss.2023.111616&partnerID=40&md5=1c682249d8223f9e3af88961cbb4bbeb","A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability. © 2023 The Author(s)","Information retrieval; Natural language processing; Software traceability"
"Optimizing smart contract vulnerability detection via multi-modality code and entropy embedding","2023","Journal of Systems and Software","10.1016/j.jss.2023.111699","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152224005&doi=10.1016%2fj.jss.2023.111699&partnerID=40&md5=be27429f4f8df34910e66da79d0cb3c7","Smart contracts have been widely used in the blockchain world these years, and simultaneously vulnerability detection has gained more and more attention due to the staggering economic losses caused by the attacker. Existing tools that analyze vulnerabilities for smart contracts heavily rely on rules predefined by experts, which are labour-intense and require domain knowledge. Moreover, predefined rules tend to be misconceptions and increase the risk of crafty potential back-doors in the future. Recently, researchers mainly used static and dynamic execution analysis to detect the vulnerabilities of smart contracts and have achieved acceptable results. However, the dynamic method cannot cover all the program inputs and execution paths, which leads to some vulnerabilities that are hard to detect. The static analysis method commonly includes symbolic execution and theorem proving, which requires using constraints to detect vulnerability. These shortcomings show that traditional methods are challenging to apply and expand on a large scale. This paper aims to detect vulnerabilities via the Bug Injection framework and transfer learning techniques. First, we train a Transformer encoder using multi-modality code, which contains source code, intermediate representation, and assembly code. The input code consists separately of Solidity source code, intermediate representation, and assembly code. Specifically, we translate source code into the intermediate representation and decompile the byte code into assembly code by the EVM compiler. Then, we propose a novel entropy embedding technique, which combines token embedding, segment embedding, and positional embedding of the Transformer encoder in our approach. After that, we utilize the Bug Injection framework to automatically generate specific types of buggy code for fine-tuning and evaluating the performance of vulnerability detection. The experimental results show that our proposed approach improves the performance in detecting reentrancy vulnerabilities and timestamp dependence. Moreover, our approach is more flexible and scalable than static and dynamic analysis approaches in detecting smart contract vulnerabilities. Our approach improves the baseline approaches by an average of 11.89% in term of F1 score. © 2023 Elsevier Inc.","Bug injection; Smart contract; Transfer learning; Vulnerability detection"
"Streaming software development: Accountability, community, and learning","2023","Journal of Systems and Software","10.1016/j.jss.2023.111630","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147603660&doi=10.1016%2fj.jss.2023.111630&partnerID=40&md5=7fc983e426b58b3be92d5b75aa190261","People use the Internet to learn new skills, stay connected with friends, and find new communities to engage with. Live streaming platforms like Twitch.tv, YouTube Live, and Facebook Gaming provide a place where all three of these activities intersect and enable users to live-stream themselves playing a video game or live-coding software and game development, as well as the ability to participate in chat while watching someone else engage in an activity. Through fifteen interviews with software and game development streamers, we investigate why people choose to stream themselves programming and if they perceive themselves improving their programming skills by live streaming. We found that the motivations to stream included accountability, self-education, community, and visibility of the streamers’ work, and streamers perceived a positive influence on their ability to write source code. Our findings implicate that alternative learning methods like live streaming programming are a beneficial tool in the age of the virtual classroom. This work also contributes to and extends research efforts surrounding educational live streaming and collaboration in developer communities. © 2023 Elsevier Inc.","Collaborative learning; Developer communities; Gaming communities; Live coding; Live streaming; Online education"
"CM-CASL: Comparison-based performance modeling of software systems via collaborative active and semisupervised learning","2023","Journal of Systems and Software","10.1016/j.jss.2023.111686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151789671&doi=10.1016%2fj.jss.2023.111686&partnerID=40&md5=2b82a568e0b55ee5970b49d920b0c3b0","Configuration tuning for large software systems is generally challenging due to the complex configuration space and expensive performance evaluation. Most existing approaches follow a two-phase process, first learning a regression-based performance prediction model on available samples and then searching for the configurations with satisfactory performance using the learned model. Such regression-based models often suffer from the scarcity of samples due to the enormous time and resources required to run a large software system with a specific configuration. Moreover, previous studies have shown that even a highly accurate regression-based model may fail to discern the relative merit between two configurations, whereas performance comparison is actually one fundamental strategy for configuration tuning. To address these issues, this paper proposes CM-CASL, a Comparison-based performance Modeling approach for software systems via Collaborative Active and Semisupervised Learning. CM-CASL learns a classification model that compares the performance of two given configurations, and enhances the samples through a collaborative labeling process by both human experts and classifiers using an integration of active and semisupervised learning. Experimental results demonstrate that CM-CASL outperforms two state-of-the-art performance modeling approaches in terms of both classification accuracy and rank accuracy, and thus provides a better performance model for the subsequent work of configuration tuning. © 2023 Elsevier Inc.","Active learning; Comparison-based model; Performance modeling; Semisupervised learning; Software systems"
"Test scenario generation for feature-based context-oriented software systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111570","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145655485&doi=10.1016%2fj.jss.2022.111570&partnerID=40&md5=37833b8ba0b588e9d21c0ba98d14a5fe","Feature-based context-oriented programming reconciles ideas from context-oriented programming, feature modelling and dynamic software product lines. It offers a programming language, architecture, tools and methodology to develop software systems consisting of contexts and features that can become active at run-time to offer the most appropriate behaviour depending on the actual context of use. Due to their high run-time adaptivity, dedicated tool support to test such systems is needed. Building upon a pairwise combinatorial interaction testing approach from the domain of software product lines, we implement an algorithm to generate automatically a small set of relevant test scenarios, ordered to minimise the number of context activations between tests. We also explore how the generated scenarios can be enhanced incrementally when the software evolves, and how useful the proposed testing approach is in practice. © 2022 Elsevier Inc.","Combinatorial interaction testing; Context-oriented programming; Dynamic software product lines; Feature modelling; Satisfiability checking (SAT); Software testing"
"The vision of on-demand architectural knowledge systems as a decision-making companion","2023","Journal of Systems and Software","10.1016/j.jss.2022.111560","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144564662&doi=10.1016%2fj.jss.2022.111560&partnerID=40&md5=a8d1c4f804171a99cc09b31556b79be3","Cobbler's children do not wear shoes. Software engineers build sophisticated software but we often cannot find the needed information and knowledge for ourselves. Issues are the amount of development information that can be captured, organizing that information to make them useable for other developers as well as human decision-making issues. Current architectural knowledge management systems cannot handle these issues properly. In this paper, we outline a research agenda for intelligent tools to support the knowledge management and decision making of architects. The research agenda consists of a vision and research challenges on the way to realize this vision. We call our vision on-demand architectural knowledge systems (ODAKS). Based on literature review, analysis, and synthesis of past research works, we derive our vision of ODAKS as decision-making companions to architects. ODAKS organize and provide relevant information and knowledge to the architect through an assistive conversation. ODAKS use probing to understand the architects’ goals and their questions, they suggest relevant knowledge and present reflective hints to mitigate human decision-making issues, such as cognitive bias, cognitive limitations, as well as design process aspects, such as problem-solution co-evolution and the balance between intuitive and rational decision-making. We present the main features of ODAKS, investigate current potential technologies for the implementation of ODAKS and discuss the main research challenges. © 2022 The Authors","Decision-making; Human aspects; Knowledge management systems; Software architecture knowledge"
"Do names echo semantics? A large-scale study of identifiers used in C++’s named casts","2023","Journal of Systems and Software","10.1016/j.jss.2023.111693","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152475382&doi=10.1016%2fj.jss.2023.111693&partnerID=40&md5=914513d986688ad356725209135c452b","Developers relax restrictions on a type to reuse methods with other types. While type casts are prevalent, in weakly typed languages such as C++, they are also extremely permissive. Assignments where a source expression is cast into a new type and assigned to a target variable of the new type, can lead to software bugs if performed without care. In this paper, we propose an information-theoretic approach to identify poor implementations of explicit cast operations. Our approach measures accord between the source expression and the target variable using conditional entropy. We collect casts from 34 components of the Chromium project, which collectively account for 27MLOC and random-uniformly sample this dataset to create a manually labelled dataset of 271 casts. Information-theoretic vetting of these 271 casts achieves a peak precision of 81% and a recall of 90%. We additionally present the findings of an in-depth investigation of notable explicit casts, two of which were fixed in recent releases of the Chromium project. © 2023 The Author(s)","C++ type conversions; Languages; Programme analysis"
"The role of knowledge-based resources in Agile Software Development contexts","2023","Journal of Systems and Software","10.1016/j.jss.2022.111572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144082256&doi=10.1016%2fj.jss.2022.111572&partnerID=40&md5=a5525a75ddb1cde9ed80596ff4e54edb","The software value chain is knowledge-based since it is highly dependant on people. Consequently, a lack of practice in managing knowledge as a resource may jeopardise its application in software development. Knowledge-Based Resources (KBRs) relate to employees’ intangible knowledge that is deemed to be valuable to a company's competitive advantage. In this study, we apply a grounded theory approach to examine the role of KBRs in Agile Software Development (ASD). To this aim, we collected data from 18 practitioners from five companies. We develop the Knowledge-Push theory, which explains how KBRs boost the need for change in ASD. Our results show that the practitioners who participated in the study utilise, as primary strategies, task planning, resource management, and social collaboration. These strategies are implemented through the team environment and settings and incorporate an ability to codify and transmit knowledge. However, this process of codification is non-systematic, which consequently introduces inefficiency in the domain of knowledge resource utilisation, resulting in potential knowledge waste. This inefficiency can generate negative implications for software development, including meaningless searches in databases, frustration because of recurrent problems, the unnecessary redesign of solutions, and a lack of awareness of knowledge sources. © 2022 The Authors","Agile software development; Grounded theory; Knowledge management; Knowledge-based resources; Software development"
"Investigating end-users’ values in agriculture mobile applications development: An empirical study on Bangladeshi female farmers","2023","Journal of Systems and Software","10.1016/j.jss.2023.111648","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149907199&doi=10.1016%2fj.jss.2023.111648&partnerID=40&md5=0baacb12cd20c9587621d988aac1ef4d","The omnipresent nature of mobile applications (apps) in all aspects of daily lives raises the necessity of reflecting end-users’ values (e.g., fairness, honesty, social recognition, etc.) in apps. However, there are limited considerations of end-users’ values in apps development. Value violations by apps have been reported in the media and are responsible for end-users’ dissatisfaction and negative socio-economic consequences. Value violations may bring more severe and lasting problems for marginalized and vulnerable end-users of apps, which have been explored less (if at all) in the software engineering community. One of the main reasons behind value violations is the lack of understanding of human values due to their ill-defined, ambiguous, and implicit nature. Furthermore, addressing all the values of the end-users in a single app might cause dissatisfaction for some of the end-users if they have different values. Therefore, it is essential to identify if there are different groups of end-users of apps who share similar values and develop different sets of apps design strategies accordingly, which is the essential first step towards values-based apps development. This research aims to fill this gap by investigating different groups of Bangladeshi female farmers as a marginalized and vulnerable group of end-users of Bangladeshi agriculture apps based on their values. We conducted an empirical study that collected and analyzed data from a survey with 193 Bangladeshi female farmers to explore the underlying factor structure of Bangladeshi female farmers’ values and the significance of demographics on their values. The results identified three underlying factors of Bangladeshi female farmers. The first factor comprises of five values: benevolence, security, conformity, universalism, and tradition. The second factor consists of two values: self-direction and stimulation. The third factor includes three values: power, achievement, and hedonism. We also identified strong influences of demographics on some of the values of Bangladeshi female farmers. For example, area has significant impacts on three values: hedonism, achievement, and tradition. Similarly, there are also strong influences of household income on power and security. The results provide a direction for Bangladeshi agriculture app developers to develop different sets of apps design strategies for different groups of Bangladeshi female farmers based on their values. © 2023 Elsevier Inc.","Bangladeshi female farmers; Empirical study; Human values; Mobile applications"
"Smart contract vulnerability detection based on semantic graph and residual graph convolutional networks with edge attention","2023","Journal of Systems and Software","10.1016/j.jss.2023.111705","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153101603&doi=10.1016%2fj.jss.2023.111705&partnerID=40&md5=d1fccbf1bda760801372c3f73598ab4e","Smart contracts are becoming the forefront of blockchain technology, allowing the performance of credible transactions without third parties. However, smart contracts on blockchain are not immune to vulnerability exploitation and cannot be modified after being deployed on the blockchain. Therefore, it is imperative to assure the security of smart contracts via intelligent vulnerability detection tools with the exponential increase in the number of smart contracts. The remarkably developing deep learning technology provides a promising way to detect potential smart contract vulnerabilities. Nevertheless, existing deep learning-based approaches fail to effectively capture the rich syntax and semantic information embedded in smart contracts for vulnerability detection. In this paper, we tackle the problem of smart contract vulnerability detection at the function level by constructing a novel semantic graph (SG) for each function and learning the SGs using graph convolutional networks (GCNs) with residual blocks and edge attention. Our proposed method consists of three stages. In the first stage, we create the SG which contains rich syntax and semantic information including the data–data, instruction–instruction and instruction–data relationships, variables, operations, etc., by building an abstract syntax tree (AST) from the code of each function, removing the unimportant nodes in the AST, and adding edges between the nodes to represent the data flows and the execution sequence of the statements. In the second stage, we propose a new graph convolutional network model EA-RGCN to learn the content and semantic features of the code. EA-RGCN contains three parts: node and edge representation via word2vec, content feature extraction with a residual GCN (RGCN) module, and semantic feature extraction using an edge attention (EA) module. In the third stage, we concatenate the code content features and the semantic features to obtain the global code feature and use a classifier to identify whether the function is vulnerable. We conduct experiments on the datasets constructed from real-world smart contracts. Experimental results demonstrate that the proposed semantic graph and the EA-RGCN model can effectively improve the performance in terms of accuracy, precision, recall, and F1-score on smart contract vulnerability detection. © 2023 Elsevier Inc.","Code graph; Edge attention; Graph convolutional networks; Residual block; Smart contract vulnerability detection"
"Finding associations between natural and computer languages: A case-study of bilingual LDA applied to the bleeping computer forum posts","2023","Journal of Systems and Software","10.1016/j.jss.2023.111651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150300899&doi=10.1016%2fj.jss.2023.111651&partnerID=40&md5=04d03a1115eeeac512ce455f917bd1fe","In the context of technical support, trails of technical discussions often contain a mixture of natural language (e.g., English) and software log excerpts. Uncovering latent links between certain problems and log excerpts that are often requested during the discussions of those problems enables the construction of a valuable knowledge base. Nevertheless, uncovering such latent links is challenging because English and software logs are two fundamentally different languages. In this paper, we investigate the suitability of multilingual LDA models to address the problem at hand. We study three models, namely: enriched LDA (M+), two-layer LDA (M2L), and off-the-shelf bilingual LDA (Mbi). We use approximately 8K discussion threads from a Bleeping Computer forum as our dataset. We observe that M2L performs the best overall, although it yields a substantially coarser-grained view of the discussed themes in the threads (20 topics, 0.3% of the documents). We also note that M+ outperforms Mbiachieving higher coherence, lower perplexity, and higher cross-lingual coverage ratio. We invite future studies to qualitatively assess the quality of the topics produced by the LDA models, such that the feasibility of employing such models in practice can be better determined. © 2023 Elsevier Inc.","LDA; Logs; Multilingual LDA; Technical support; Topic models"
"Towards a cognitive engineering of transactional services in IoT based systems","2023","Journal of Systems and Software","10.1016/j.jss.2023.111634","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148321404&doi=10.1016%2fj.jss.2023.111634&partnerID=40&md5=78c28e0459b0b751c18cd39cfe1755d3","Cognitive computing is the capability of a system to mimic the ability of human brain to learn and adapt from the surroundings. Cognitive systems have decision-making capabilities based on new information, actions and outcomes. Similarly, Internet of Things (IoT) aims at making things smart, and enabling them to perform complex tasks. Reliability and flexibility are persistent challenges in the IoT context where the promise is managing a multitude of devices and delivering real-time responses for critical smart applications. A limited number of studies examine these challenges while considering cognitive capabilities of things and have failed to handle thing's specificities in terms of communication bandwidth, power availability and storage capacity. Following the service oriented architecture (SOA), the functionality can be encapsulated as services. Thus, automating the management of transactional services in smart IoT ecosystem can be fulfilled through the coupling of transactional properties to cognitive things. This paper provides a comprehensive approach to alleviate reliability restrictions in cognitive IoT service compositions. The concept of cognitive faculty (CF) is introduced to leverage transactional properties of services and can be customized to specific requirements of IoT applications. A proof-of-concept is included in this paper based on a self-monitoring IoT application for diabetic patients. © 2023 Elsevier Inc.","Cognitive computing; Cognitive things; Internet-of-Things; Service composition; Transactional properties"
"Characterizing architecture related posts and their usefulness in Stack Overflow","2023","Journal of Systems and Software","10.1016/j.jss.2023.111608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145980148&doi=10.1016%2fj.jss.2023.111608&partnerID=40&md5=edc7938ead7165b14aec3f727e22c407","Context: Stack Overflow (SO) has won the intention from software engineers (e.g., architects) to learn, practice, and utilize development knowledge, such as Architectural Knowledge (AK). But little is known about AK communicated in SO, which is a type of high-level but important knowledge in development. Objective: This study aims to investigate the AK in SO posts in terms of their categories and characteristics as well as their usefulness from the point of view of SO users. Methods: We conducted an exploratory study by qualitatively analyzing a statistically representative sample of 968 Architecture Related Posts (ARPs) from SO. Results: The main findings are: (1) architecture related questions can be classified into 9 core categories, in which “architecture configuration” is the most common category, followed by the “architecture decision” category, and (2) architecture related questions that provide clear descriptions together with architectural diagrams increase their likelihood of getting more than one answer, while poorly structured architecture questions tend to only get one answer. Conclusions: Our findings suggest that future research can focus on enabling automated approaches and tools that could facilitate the search and (re)use of AK in SO. SO users can refer to our proposed guidelines to compose architecture related questions with the likelihood of getting more responses in SO. © 2023 Elsevier Inc.","Architectural knowledge; Architectural level element; Architecture solution; Stack Overflow; Usefulness"
"Collaborative Model-Driven Software Engineering — A systematic survey of practices and needs in industry","2023","Journal of Systems and Software","10.1016/j.jss.2023.111626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147541731&doi=10.1016%2fj.jss.2023.111626&partnerID=40&md5=4e0f3f4937e5442860b3d976c8e02f9b","The engineering of modern software-intensive systems is carried out in collaboration among stakeholders with specialized expertise. The complexity of such systems often also necessitates employing more rigorous approaches, such as Model-Driven Software Engineering (MDSE). Collaborative MDSE is the combination of the two disciplines, with its specific opportunities and challenges. The rapid expansion and maturation of the field started attracting tool builders from outside of academia. However, available systematic studies on collaborative MDSE focus exclusively on mapping academic research and fail to identify how academic research aligns with industry practices and needs. To address this shortcoming, we have carried out a mixed-method survey on the practices and needs concerning collaborative MDSE. First, we carried out a qualitative survey in two focus group sessions, interviewing seven industry experts. Second, based on the results of the interviews, we constructed a questionnaire and carried out a questionnaire survey with 41 industry expert participants. In this paper, we report the results of our study, investigate the alignment of academic research with the needs of practitioners, and suggest directions on research and development of the supporting techniques of collaborative MDSE. © 2023 Elsevier Inc.","Collaborative software engineering; Industry survey; Model-driven engineering"
"Configuring mission-specific behavior in a product line of collaborating Small Unmanned Aerial Systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144386871&doi=10.1016%2fj.jss.2022.111543&partnerID=40&md5=2785574036b095dffb3ccf926002da39","In emergency response scenarios, autonomous small Unmanned Aerial Systems (sUAS) must be configured and deployed quickly and safely to perform mission-specific tasks. In this paper, we present Drone Response, a Software Product Line for rapidly configuring and deploying a multi-role, multi-sUAS mission whilst guaranteeing a set of safety properties related to the sequencing of tasks within the mission. Individual sUAS behavior is governed by an onboard state machine, combined with coordination handlers which are configured dynamically within seconds of launch and ultimately determine the sUAS’ behaviors, transition decisions, and interactions with other sUAS, as well as human operators. The just-in-time manner in which missions are configured precludes robust upfront testing of all conceivable combinations of features — both within individual sUAS and across cohorts of collaborating ones. To ensure the absence of common types of configuration failures and to promote safe deployments, we check vital properties of the dynamically generated sUAS specifications and coordination handlers before sUAS are assigned their missions. We evaluate our approach in two ways. First, we perform validation tests to show that the end-to-end configuration process results in correctly executed missions, and second, we apply fault-based mutation testing to show that our safety checks successfully detect incorrect task sequences. © 2022 Elsevier Inc.","Dynamic configuration; Emergency response; Product line; Small unmanned aerial system; sUAS"
"Aspects of modelling requirements in very-large agile systems engineering","2023","Journal of Systems and Software","10.1016/j.jss.2023.111628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147606615&doi=10.1016%2fj.jss.2023.111628&partnerID=40&md5=aeead7a1f60bbc033c40d0de1d1a1c3b","Using models for requirements engineering (RE) is uncommon in systems engineering, despite the widespread use of model-based engineering in general. One reason for this lack of use is that formal models do not match well the trend to move towards agile developing methods. While there exists work that investigates challenges in the adoption of requirements modelling and agile methods in systems engineering, there is a lack of work studying successful approaches of using requirements modelling in agile systems engineering. To address this gap, we conducted a case study investigating the application of requirements models at Ericsson AB, a Swedish telecommunications company. We studied a department using requirements models to bridge agile development and plan-driven development aspects. We find that models are used to understand how requirements relate to each other, and to keep track with the product's evolution. To cope with the effort to maintain models over time, study participants suggest to rely on text-based notations that bring the models closer to developers and allow integration into existing software development workflows. This results in tool trade-offs, e.g., losing the possibility to control diagram layout. © 2023 Elsevier Inc.","Agile; Modelling; Requirements"
"An empirical study of security practices for microservices systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111563","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144630737&doi=10.1016%2fj.jss.2022.111563&partnerID=40&md5=fd7865e9b51a9f61b4d2a72af4e4dfb2","Despite the numerous benefits of microservices systems, security has been a critical issue in such systems. Several factors explain this difficulty, including a knowledge gap among microservices practitioners on properly securing a microservices system. To (partially) bridge this gap, we conducted an empirical study. We first manually analyzed 861 microservices security points, including 567 issues, 9 documents, and 3 wiki pages from 10 GitHub open-source microservices systems and 306 Stack Overflow posts concerning security in microservices systems. In this study, a microservices security point is referred to as “a GitHub issue, a Stack Overflow post, a document, or a wiki page that entails 5 or more microservices security paragraphs”. Our analysis led to a catalog of 28 microservices security practices. We then ran a survey with 74 microservices practitioners to evaluate the usefulness of these 28 practices. Our findings demonstrate that the survey respondents affirmed the usefulness of the 28 practices. We believe that the catalog of microservices security practices can serve as a valuable resource for microservices practitioners to more effectively address security issues in microservices systems. It can also inform the research community of the required or less explored areas to develop microservices-specific security practices and tools. © 2022 Elsevier Inc.","Empirical study; Microservice; Practice; Practitioners; Security"
"Empirical research in software architecture — Perceptions of the community","2023","Journal of Systems and Software","10.1016/j.jss.2023.111684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152962290&doi=10.1016%2fj.jss.2023.111684&partnerID=40&md5=aa63bfe3ecad169b4452a097dc477180","Context: Previous research highlighted concerns about empirical research in software engineering (e.g., reproducibility, applicability of findings). It is unclear how these concerns reflect views of those who conduct and evaluate research. Objective: Focusing on software architecture, one subfield of software engineering, we study perceptions of the research community on (1) how empirical research is applied, (2) human participants, (3) internal and external validity, and (4) replications. Method: We collected responses from 105 key players in architecture research via a survey; we analyzed data quantitatively and qualitatively. Results: Although respondents do generally not prefer either quantitative or qualitative research, around 40% express a preference for various reasons. Professionals are the preferred participants; there is no consensus on the value of student participants. Also, there is no consensus on when to focus on internal or external validity. Most respondents value replications, but acknowledge difficulties. A comparison with published research shows differences between how the community thinks research should be done. Conclusions: We provide evidence that consensus about empirical research is limited. Findings have implications for conducting and reviewing empirical research (e.g., training researchers and reviewers), and call for reflection on empirical research (e.g., to resolve conflicts). We outline actions for the future. © 2023 The Author(s)","Empirical research; Literature review; Perceptions of community; Software architecture; Survey"
"PHP code smells in web apps: Evolution, survival and anomalies","2023","Journal of Systems and Software","10.1016/j.jss.2023.111644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149306575&doi=10.1016%2fj.jss.2023.111644&partnerID=40&md5=5921ddfdd0a1da30fa2144f7caa5bb61","Context: Code smells are symptoms of poor design, leading to future problems, such as reduced maintainability. Therefore, it becomes necessary to understand their evolution and how long they stay in code. This paper presents a longitudinal study on the evolution and survival of code smells (CS) for web apps built with PHP, the most widely used server-side programming language in web development and seldom studied. Objectives: We aimed to discover how CS evolve and what is their survival/lifespan in typical PHP web apps. Does CS survival depend on their scope or app life period? Are there sudden variations (anomalies) in the density of CS through the evolution of web apps? Method: We analyzed the evolution of 18 CS in 12 PHP web applications and compared it with changes in app and team size. We characterized the distribution of CS and used survival analysis techniques to study CS’ lifespan. We specialized the survival studies into localized (specific location) and scattered CS (spanning multiple classes/methods) categories. We further split the observations for each web app into two consecutive time frames. As for the CS evolution anomalies, we standardized their detection criteria. Results: The CS density trend along the evolution of PHP web apps is mostly stable, with variations, and correlates with the developer's numbers. We identified the smells that survived the most. CS live an average of about 37% of the life of the applications, almost 4 years on average in our study; around 61% of CS introduced are removed. Most applications have different survival times for localized and scattered CS, and localized CS have a shorter life. The CS survival time is shorter and more CS are introduced and removed in the first half of the life of the applications. We found anomalies in the evolution of 5 apps and show how a graphical representation of sudden variations found in the evolution of CS unveils the story of a development project. Conclusion: CS stay a long time in code. The removal rate is low and did not change substantially in recent years. An effort should be made to avoid this bad behavior and change the CS density trend to decrease. © 2023 The Author(s)","Code smells; PHP; Software evolution; Survival; Web apps"
"Simple stupid insecure practices and GitHub's code search: A looming threat?","2023","Journal of Systems and Software","10.1016/j.jss.2023.111698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152227507&doi=10.1016%2fj.jss.2023.111698&partnerID=40&md5=045eb0de266fa17528b74a53a7713e18","Insecure coding practices are a known, long-standing problem in open-source development, which takes on a new dimension with the current capabilities for mining open-source software repositories through version control systems. Although most insecure practices require a sequence of interlinked behaviour, prior work also determined that simpler, one-liner coding practices can introduce vulnerabilities in the code. Such simple stupid insecure practices (SSIPs) can have severe security implications for package-based software systems, as they are easily spread over version-control systems. Moreover, GitHub is piloting regular-expression-based code searches across public repositories through its “Code Search Technology”, potentially simplifying unearthing SSIPs. As an exploratory case study, we focused on popular PyPi packages and analysed their source code using regular expressions (as done by GitHub's incoming search engine). The goal was to explore how detectable these simple vulnerabilities are and how exploitable “Code Search” technology is. Results show that packages on lower versions are more vulnerable, that “code injection” is the most scattered issue, and that about 20% of the scouted packages have at least one vulnerability. Most concerningly, malicious use of this engine was straightforward, raising severe concerns about the implications of a publicly available “Code Search”. © 2023 Elsevier Inc.","GitHub code search; Python; Simple stupid insecure practices"
"Testing of highly configurable cyber–physical systems — Results from a two-phase multiple case study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147198055&doi=10.1016%2fj.jss.2023.111624&partnerID=40&md5=3234a7178e4b9c310fb4966d789bba80","Cyber–physical systems are commonly highly configurable. Testing such systems is particularly challenging because they comprise numerous heterogeneous components that can be configured and combined in different ways. Despite a plethora of work investigating software testing in general and software product line testing in particular, variability in tests and how it is applied in industry with cyber–physical systems is not well understood. In this paper, we report on a multiple case study with four companies maintaining highly configurable cyber–physical systems focusing on their testing practices, with a particular focus on variability. Based on the results of the multiple case study, we conducted an interactive survey with experienced engineers from eight companies, including the initial four. We reflect on the lessons learned from the multiple case study. We conclude that experience-based selection of configurations for testing is currently predominant. We learned that variability modeling techniques and tools are not utilized and the dependencies between configuration options are only partially modeled at best using custom artifacts such as spreadsheets or configuration files. Another finding is that companies have the need and desire to cover more configuration combinations by automated tests. Our findings raise many questions interesting to the scientific community and motivating future research. © 2023 Elsevier Inc.","Configurable software; Cyber–physical systems; Software product lines; Software testing; Variability"
"Input sensitivity on the performance of configurable systems an empirical study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151798480&doi=10.1016%2fj.jss.2023.111671&partnerID=40&md5=dc394daa72759aa3404b5cd9dea104b6","Widely used software systems such as video encoders are by necessity highly configurable, with hundreds or even thousands of options to choose from. Their users often have a hard time finding suitable values for these options (i.e., finding a proper configuration of the software system) to meet their goals for the tasks at hand, e.g., compress a video down to a certain size. One dimension of the problem is of course that performance depends on the input data: e.g., a video as input to an encoder like x264 or a file fed to a tool like xz. To achieve good performance, users should therefore take into account both dimensions of (1) software variability and (2) input data. This paper details a large study over 8 configurable systems that quantifies the existing interactions between input data and configurations of software systems. The results exhibit that (1) inputs fed to software systems can interact with their configuration options in non-monotonous ways, significantly impacting their performance properties (2) input sensitivity can challenge our knowledge of software variability and question the relevance of performance predictive models for a field deployment. Given the results of our study, we call researchers to address the problem of input sensitivity when tuning, predicting, understanding, and benchmarking configurable systems. Please refer to https://www.sciencedirect.com/science/article/pii/S0164121221002168 as an example of where to place this statement. “Editor s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.” © 2023","Configurable systems; Input sensitivity; Performance prediction"
"What makes test programs similar in microservices applications?","2023","Journal of Systems and Software","10.1016/j.jss.2023.111674","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150786348&doi=10.1016%2fj.jss.2023.111674&partnerID=40&md5=041c6b2b99d4f2214eba8579d486044f","The emergence of microservices architecture calls for novel methodologies and technological frameworks that support the design, development, and maintenance of applications structured according to this new architectural style. In this paper, we consider the issue of designing suitable strategies for the governance of testing activities within the microservices paradigm. We focus on the problem of discovering implicit relations between test programs that help to avoid re-running all the available test suites each time one of its constituents evolves. We propose a dynamic analysis technique and its supporting framework that collects information about the invocations of local and remote APIs. Information on test program execution is obtained in two ways: instrumenting the test program code or running a symbolic execution engine. The extracted information is processed by a rule-based automated reasoning engine, which infers implicit similarities among test programs. We show that our analysis technique can be used to support the reduction of test suites, and therefore has good application potential in the context of regression test optimisation. The proposed approach has been validated against two real-world microservices applications. © 2023 Elsevier Inc.","Automated reasoning; Microservices architecture; Program instrumentation; Software testing; Symbolic execution; Test program similarity"
"Runtime software patching: Taxonomy, survey and future directions","2023","Journal of Systems and Software","10.1016/j.jss.2023.111652","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149441502&doi=10.1016%2fj.jss.2023.111652&partnerID=40&md5=e1500d4afa91e865786926e591d4d08f","Runtime software patching aims to minimize or eliminate service downtime, user interruptions and potential data losses while deploying a patch. Due to modern software systems’ high variance and heterogeneity, no universal solutions are available or proposed to deploy and execute patches at runtime. Existing runtime software patching solutions focus on specific cases, scenarios, programming languages and operating systems. This paper aims to identify, investigate and synthesize state-of-the-art runtime software patching approaches and gives an overview of currently unsolved challenges. It further provides insights into multiple aspects of runtime patching approaches such as patch scales, general strategies and responsibilities. This study identifies seven levels of granularity, two key strategies providing a conceptual model of three responsible entities and four capabilities of runtime patching solutions. Through the analysis of the existing literature, this research also reveals open issues hindering more comprehensive adoption of runtime patching in practice. Finally, it proposes several crucial future directions that require further attention from both researchers and practitioners. © 2023 Elsevier Inc.","Dynamic patching; Hot patching; Live patching; Patch granularity; Patch strategy; Runtime patching"
"A modular metamodel and refactoring rules to achieve software product line interoperability","2023","Journal of Systems and Software","10.1016/j.jss.2022.111579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145263733&doi=10.1016%2fj.jss.2022.111579&partnerID=40&md5=5f725f3bf5b3069330e9710f2b1b8921","Emergent application domains, such as cyber–physical systems, edge computing or industry 4.0. present a high variability in software and hardware infrastructures. However, no single variability modeling language supports all language extensions required by these application domains (i.e., attributes, group cardinalities, clonables, complex constraints). This limitation is an open challenge that should be tackled by the software engineering field, and specifically by the software product line (SPL) community. A possible solution could be to define a completely new language, but this has a high cost in terms of adoption time and development of new tools. A more viable alternative is the definition of refactoring and specialization rules that allow interoperability between existing variability languages. However, with this approach, these rules cannot be reused across languages because each language uses a different set of modeling concepts and a different concrete syntax. Our approach relies on a modular and extensible metamodel that defines a common abstract syntax for existing variability modeling extensions. We map existing feature modeling languages in the SPL community to our common abstract syntax. Using our abstract syntax, we define refactoring rules at the language construct level that help to achieve interoperability between variability modeling languages. © 2022 The Author(s)","Edge computing; Interoperability; Model refactoring; Model specialization; Modular metamodel; Variability modeling language"
"Software engineering research on the Robot Operating System: A systematic mapping study","2023","Journal of Systems and Software","10.1016/j.jss.2022.111574","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143882663&doi=10.1016%2fj.jss.2022.111574&partnerID=40&md5=7bab099df1c294b3a232f46d480b5f68","The Robot Operating System (ROS) has become the de-facto standard framework for robotics software, and a great part of commercial robots is expected to have at least one ROS package on board in the coming years. For good quality, robotics software should rely on strong software engineering principles. In this paper, we perform a systematic mapping study on several works in software engineering on ROS, published at the top software engineering and robotics venues. Our goal is to analyze and evaluate such state-of-the-art regarding its relevance to the robotics software industry. The potentially-relevant studies are subject to a rigorously defined selection process. This results in a set of 63 primary studies on software engineering research on ROS. Those primary studies are then qualitatively analyzed according to a rigorously-defined classification framework. The results are of interest to both researchers and practitioners: (i) we provide an up-to-date overview of the state of the art on software engineering research on ROS and its potential for industrial adoption, (ii) a broad discussion of the research area as a whole, and (iii) point out routes of action for a better alignment between research and industry. © 2022 The Author(s)","Robotic systems; ROS; Software engineering; Systematic mapping study"
"Explaining software fault predictions to spreadsheet users","2023","Journal of Systems and Software","10.1016/j.jss.2023.111676","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150876932&doi=10.1016%2fj.jss.2023.111676&partnerID=40&md5=fac424a42f5e8e625cfe38cfcd136055","A variety of automated software fault prediction techniques was proposed in recent years, in particular for the important class of spreadsheet programs. Software fault prediction techniques commonly create ranked lists of “suspicious” program statements for developers to inspect. Existing research, however, suggests that solely providing such ranked lists may not always be effective. In particular, it was found that developers often seek for explanations for the outcomes provided by a debugging tool and that such explanations may be key for developers to trust and rely on the tool. Research on how to explain the outcomes of fault prediction techniques, which are often based on complex machine learning models, is scarce, and little is known regarding how such explanations are perceived by developers. With this work, we aim to narrow this research gap and study the perception of different forms of explanations by spreadsheet users in the context of a machine learning based fault prediction tool. A between-subjects user study (N=120) revealed significant differences between the explored explanation styles. In particular, we found that well-designed natural language explanations can indeed help users better understand why certain spreadsheet cells were marked by the debugging tool and that such explanations can be effective to increase the users’ trust compared to a black box system. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 The Author(s)","Explainable AI; Explanations; Fault prediction; Spreadsheets; User study"
"Can gamification help in software testing education? Findings from an empirical study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111647","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149184996&doi=10.1016%2fj.jss.2023.111647&partnerID=40&md5=3c0e790ba7c186de4508455845f46310","Software testing is an essential knowledge area required by industry for software engineers. However, software engineering students often consider testing less appealing than designing or coding. Consequently, it is difficult to engage students to create effective tests. To encourage students, we explored the use of gamification and investigated whether this technique can help to improve the engagement and performance of software testing students. We conducted a controlled experiment to compare the engagement and performance of two groups of students that took an undergraduate software testing course in different academic years. The experimental group is formed by 135 students from the gamified course whereas the control group is formed by 100 students from the non-gamified course. The data collected were statistically analyzed to answer the research questions of this study. The results show that the students that participated in the gamification experience were more engaged and achieved a better performance. As an additional finding, the analysis of the results reveals that a key aspect to succeed is the gamification experience design. It is important to distribute the motivating stimulus provided by the gamification throughout the whole experience to engage students until the end. Given these results, we plan to readjust the gamification experience design to increase student engagement in the last stage of the experience, as well as to conduct a longitudinal study to evaluate the effects of gamification. © 2023 The Author(s)","Empirical study; Engagement; Gamification; Performance; Software testing; Test effectiveness"
"Dynamic analysis of quantum annealing programs","2023","Journal of Systems and Software","10.1016/j.jss.2023.111683","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151435873&doi=10.1016%2fj.jss.2023.111683&partnerID=40&md5=df0a191675aebaca6d6951b47da42feb","Quantum software engineering is emerging as a relevant field, as it deals with the challenges of producing the new quantum software, whose adoption is steadily increasing. Quantum annealing software has gained a certain market penetration, demonstrating a good performance for optimization problems. However, there are no reverse engineering techniques with which to discover the underlying optimization problem definitions (the Hamiltonian functions to be minimized). Problem definitions are, in turn, dynamically defined using classical software, and can evolve over time, which make their accurate comprehension and abstract representation difficult. This paper, therefore, presents a dynamic analysis technique for D-Wave (Python) programs with which to reverse Hamiltonian expressions, and which are additionally represented according to the Knowledge Discovery Metamodel. The usage of this standard makes it possible to represent the reversed Hamiltonians in combination with other parts of classical–quantum software systems. In order to facilitate its adoption, the proposed technique has been empirically validated through a case study with 27 D-Wave programs that demonstrates its effectiveness and efficiency. The main implication of this research is that the proposed technique helps modernize quantum annealing software alongside hybrid software systems. © 2023 Elsevier Inc.","D-wave; Dynamic analysis; Knowledge discovery metamodel; Quantum annealing; Quantum software engineering; Reverse engineering"
"Leveraging multi-level embeddings for knowledge-aware bug report reformulation","2023","Journal of Systems and Software","10.1016/j.jss.2023.111617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146713464&doi=10.1016%2fj.jss.2023.111617&partnerID=40&md5=48a620d6d3a709c87fa35bfd0158e258","Software bug analysis based on the information retrieval (IR) technology is widely studied and used for bug understanding, localization and fixing. IR technology with various textual feature extraction methods formulates the textual information in a given new bug report (i.e., title and description) as an initial query. However, due to the low-quality content in the new bug report and improper representation to be used as a query, the retrieval results are usually not satisfactory. To alleviate these problems, we propose a novel knowledge-aware bug report reformulation approach (a.k.a, KABR) by leveraging multi-level embeddings from the bug data. First, we construct a bug-specific knowledge graph (KG) to manage and reuse prior knowledge extracted from historical bug reports. Then, we extract word embedding from the original bug data, entity embedding and context embedding from the bug-specific KG to enhance the initial query. Finally, a new query representation is generated by leveraging multi-level embeddings through Convolutional Neural Networks (CNN) with the self-attention mechanism. We evaluate KABR based on the duplicate bug report detection task, and the experimental results show that KABR achieves 6%–11% F1-measure improvement over the state-of-the-art approaches. © 2023 Elsevier Inc.","Bug analysis; Bug report reformulation; Information retrieval; Knowledge representation"
"A critical comparison on six static analysis tools: Detection, agreement, and precision","2023","Journal of Systems and Software","10.1016/j.jss.2022.111575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144822206&doi=10.1016%2fj.jss.2022.111575&partnerID=40&md5=7d40feaac35c2f25b5c56707c0cf4c5a","Background: Developers use Static Analysis Tools (SATs) to control for potential quality issues in source code, including defects and technical debt. Tool vendors have devised quite a number of tools, which makes it harder for practitioners to select the most suitable one for their needs. To better support developers, researchers have been conducting several studies on SATs to favor the understanding of their actual capabilities. Aims: Despite the work done so far, there is still a lack of knowledge regarding (1) what is their agreement, and (2) what is the precision of their recommendations. We aim at bridging this gap by proposing a large-scale comparison of six popular SATs for Java projects: Better Code Hub, CheckStyle, Coverity Scan, FindBugs, PMD, and SonarQube. Methods: We analyze 47 Java projects applying 6 SATs. To assess their agreement, we compared them by manually analyzing – at line – and class-level — whether they identify the same issues. Finally, we evaluate the precision of the tools against a manually-defined ground truth. Results: The key results show little to no agreement among the tools and a low degree of precision. Conclusion: Our study provides the first overview on the agreement among different tools as well as an extensive analysis of their precision that can be used by researchers, practitioners, and tool vendors to map the current capabilities of the tools and envision possible improvements. © 2022 The Author(s)","Empirical study; Software quality; Static analysis tools"
"Enhancing Ethereum smart-contracts static analysis by computing a precise Control-Flow Graph of Ethereum bytecode","2023","Journal of Systems and Software","10.1016/j.jss.2023.111653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150263642&doi=10.1016%2fj.jss.2023.111653&partnerID=40&md5=e1b96f2668b20e981b307bc2902b3624","The immutable nature of Ethereum transactions, and consequently Ethereum smart-contracts, has stimulated the proliferation of many approaches aiming at detecting defects and security issues before the deployment of smart-contracts on the blockchain. Indeed, the actions performed by smart-contracts instantiated on the blockchain, possibly involving substantial financial value, cannot be undone. Unfortunately, smart-contracts source code is not always available, hence approaches based on static analysis have very often to face the problem of inspecting the compiled Ethereum Virtual Machine (EVM) bytecode, retrieved directly from the blockchain. However, due to the intrinsic complexity of EVM bytecode (especially in jumps address resolution), the state-of-the-art static analysis-based solutions have poor accuracy in the automated detection of Ethereum smart-contracts programming defects and vulnerabilities. This paper presents a novel approach based on symbolic execution of the EVM operands stack that allows to resolve jumps address in the EVM bytecode and to construct a precise Control-Flow Graph (CFG) of compiled smart-contracts. Many static analysis techniques are based on a CFG-based representation of the smart-contract to validate, and would therefore benefit from our approach. We have implemented the CFG reconstruction algorithm in a tool called EtherSolve. Then, we have validated the tool on a large dataset of real-world Ethereum smart-contracts, showing that EtherSolve extracts more precise CFGs, w.r.t. state-of-the-art available approaches. Finally, we have extended EtherSolve with two detectors for two of the most prominent Ethereum smart-contracts vulnerabilities (Reentrancy and Tx.origin). Experimental results show that exploiting the proposed CFG reconstruction static analysis, leads to more accurate vulnerabilities detection, w.r.t. state-of-the-art security tools. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2023 Elsevier Inc.","Ethereum; Reverse engineering; Smart-contract; Static analysis"
"Trustworthiness models to categorize and prioritize code for security improvement","2023","Journal of Systems and Software","10.1016/j.jss.2023.111621","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146702351&doi=10.1016%2fj.jss.2023.111621&partnerID=40&md5=9bbdc90104847eb653d09f9d14ec84d6","The exploitation of software security vulnerabilities can have severe consequences. Thus, it is crucial to devise new processes, techniques, and tools to support teams in the development of secure code from the early stages of the software development process, while potentially reducing costs and shortening the time to market. In this paper, we propose an approach that uses security evidences (e.g., software metrics, bad smells) to feed a set of trustworthiness models, which allow characterizing code from a security perspective. In practice, the goal is to identify the code units that are more prone to be vulnerable (i.e., are less trustworthy from a security perspective), thus helping developers to improve their code. A clustering-based approach is used to categorize the code units based on the combination of the scores provided by several trustworthiness models and taking into account the criticality of the code. To instantiate our proposal, we use a dataset of software metrics (e.g., CountLine, Cyclomatic Complexity, Coupling Between Objects) for files and functions of the Linux Kernel and Mozilla Firefox projects, and a set of machine learning algorithms (i.e., Random Forest, Decision Tree, SVM Linear, SVM Radial, and Xboost) to build the trustworthiness models. Results show that code that is more prone to be vulnerable can be effectively distinguished, thus demonstrating the applicability and usefulness of the proposed approach in diverse scenarios. © 2023 Elsevier Inc.","Code categorization; Code security review; Machine learning; Software metrics; Software security; Trustworthiness model"
"A literature review of IoT and CPS—What they are, and what they are not","2023","Journal of Systems and Software","10.1016/j.jss.2023.111631","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150813746&doi=10.1016%2fj.jss.2023.111631&partnerID=40&md5=e393cb0c2e2fa6881742ee4b66fe00f3","Today, we are confronted with many concepts such as Cyber–Physical-Systems (CPS), Internet of Things (IoT), Industry 4.0, Industrial Internet, Ubiquitous Computing, Pervasive Computing and many more. Some researchers use all of these terms interchangeably, while others interpret them in ways that contradict each other. The inconsistent and interchangeable usage of these terms creates the impression that authors abuse them as buzzwords to attract attention. Hence, the question arises: Is the existence of all these terms justified? In this paper, we first look at the origin of the terms. Then, we focus on Internet of Things (IoT) and Cyber-Physical-Systems (CPS) as those terms are more often used as the others and further often seen as underlying technologies. We presentthe results of a literature review, including academic, industry and gray literature, with the objective to identify and discuss several clusters of similar statements on both terms. Building on this, we present definitions for IoT and CPS that reflect the core intuition of the terms as found in the literature review while providing a clear demarcation of the two terms. Then, we illustrate the applicability of our findings on several use cases. Finally, we discuss the relation to the other topics closely related to adaptive systems, namely Industry 4.0, Industrial Internet, Ubiquitous Computing, and Pervasive Computing. © 2023 Elsevier Inc.","CPS; Cyber Physical System; Internet of Things; IoT; Literature review"
"What are the characteristics of highly-selected packages? A case study on the npm ecosystem","2023","Journal of Systems and Software","10.1016/j.jss.2022.111588","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144557305&doi=10.1016%2fj.jss.2022.111588&partnerID=40&md5=dabb3f2bf70e7b634ad2697de0abcf44","With the popularity of software ecosystems, the number of open source components (known as packages) has grown rapidly. Identifying high-quality and well-maintained packages from a large pool of packages to depend on is a basic and important problem, as it is beneficial for various applications, such as package recommendation and package search. However, no systematic and comprehensive work focuses on addressing this problem except in online discussions or informal literature and interviews. To fill this gap, in this paper, we conducted a mixed qualitative and quantitative analysis to understand how developers identify and select relevant open source packages. In particular, we started by surveying 118 JavaScript developers from the npm ecosystem to qualitatively understand the factors that make a package to be highly-selected within the npm ecosystem. The survey results showed that JavaScript developers believe that highly-selected packages are well-documented, receive a high number of stars on GitHub, have a large number of downloads, and do not suffer from vulnerabilities. Then, we conducted an experiment to quantitatively validate the developers’ perception of the factors that make a highly-selected package. In this analysis, we collected and mined historical data from 2,527 packages divided into highly-selected and not highly-selected packages. For each package in the dataset, we collected quantitative data to present the factors studied in the developers’ survey. Next, we used regression analysis to quantitatively investigate which of the studied factors are the most important. Our regression analysis complements our survey results about highly-selected packages. In particular, the results showed that highly-selected packages tend to be correlated by the number of downloads, stars, and how large the package's readme file is. © 2022 Elsevier Inc.","highly-selected packages; npm; Package quality; Software ecosystem"
"Event-aware precise dynamic slicing for automatic debugging of Android applications","2023","Journal of Systems and Software","10.1016/j.jss.2023.111606","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146053890&doi=10.1016%2fj.jss.2023.111606&partnerID=40&md5=ccde9152e796de802736a97dce93f08e","Dynamic slicing aims to find the program statements that affect the values computed at some point of interest (i.e., a particular statement or variable) under a given program input. It is an enabling technique for many software engineering tasks (e.g., program understanding and debugging). Due to Android's event-driven nature, dynamic slicing for Android is more challenging than that for traditional Java programs. Its asynchronous events drive the execution of an app through inter-component communications. These non-deterministic user events often yield a large search space when applying existing dynamic slicing techniques, which introduce redundant statements into the resulting slice. We present ESDroid, an Event-aware dynamic Slicing technique for AnDroid applications. The novelty of our approach lies in the combination of segment-based delta debugging and backward dynamic slicing to narrow the search space to produce precise slices for Android. Our experiment across 38 apps shows that ESDroid can help with slicing buggy code from exception program points. We compare the effectiveness of ESDroid with the state-of-the-art dynamic slicing tools (AndroidSlicer and Mandoline). ESDroid outperforms both tools by reporting up to 72% fewer spurious statements than AndroidSlicer, and 50% fewer than Mandoline in the resulting slice (the number of instructions to be examined). © 2023 Elsevier Inc.","Android; Delta-debugging; Slicing"
"Does Deep Learning improve the performance of duplicate bug report detection? An empirical study","2023","Journal of Systems and Software","10.1016/j.jss.2023.111607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146288613&doi=10.1016%2fj.jss.2023.111607&partnerID=40&md5=620cb8740e982a7088cba95112a2fd6f","Do Deep Learning (DL) techniques actually help to improve the performance of duplicate bug report detection? Prior studies suggest that they do, if the duplicate bug report detection task is treated as a binary classification problem. However, in realistic scenarios, the task is often viewed as a ranking problem, which predicts potential duplicate bug reports by ranking based on similarities with existing historical bug reports. There is little empirical evidence to support that DL can be effectively applied to detect duplicate bug reports in the ranking scenario. Therefore, in this paper, we investigate whether well-known DL-based methods outperform classic information retrieval (IR) based methods on the duplicate bug report detection task. In addition, we argue that both IR- and DL-based methods suffer from incompletely evaluating the similarity between bug reports, resulting in the loss of important information. To address this problem, we propose a new method that combines IR and DL techniques to compute textual similarity more comprehensively. Our experimental results show that the DL-based method itself does not yield high performance compared to IR-based methods. However, our proposed combined method improves on the MAP metric of classic IR-based methods by a median of 7.09%–11.34% and a maximum of 17.228%–28.97%. © 2023 Elsevier Inc.","Deep learning; Duplicate bug report detection; Information retrieval; Realistic evaluation; Similarity measure"
"The impact of working from home on the success of Scrum projects: A multi-method study","2023","Journal of Systems and Software","10.1016/j.jss.2022.111562","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144069611&doi=10.1016%2fj.jss.2022.111562&partnerID=40&md5=3a5edcae676f7675fb0e7b4b2cc3e512","With the COVID-19 pandemic, Scrum teams had to switch abruptly from a traditional working setting into an enforced working from home one. This abrupt switch had an impact on software projects. Thus, it is necessary to understand how potential future disruptive events will impact Agile software teams’ ability to deliver successful projects while working from home. To investigate this problem, we used a two-phased Multi-Method study. In the first phase, we uncover how working from home impacted Scrum practitioners through semi-structured interviews. Then, in the second phase, we propose a theoretical model that we test and generalize using Partial Least Squares-Structural Equation Modeling (PLS-SEM) surveying 138 software engineers who worked from home within Scrum projects. We concluded that all the latent variables identified in our model are reliable, and all the hypotheses are significant. This paper emphasizes the importance of supporting the three innate psychological needs of autonomy, competence, and relatedness in the home working environment. We conclude that the ability of working from home and the use of Scrum both contribute to project success, with Scrum acting as a mediator. © 2022 The Author(s)","Multi-method study; Project success; Scrum; Working from home"
"A study of update request comments in Stack Overflow answer posts","2023","Journal of Systems and Software","10.1016/j.jss.2022.111590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144621236&doi=10.1016%2fj.jss.2022.111590&partnerID=40&md5=eb07ac37c1f2919e3d0943812d153646","Comments play an important role in updating Stack Overflow (SO) posts. They are used to point out a problem (e.g., obsolete answer and buggy code) in a SO answer or ask for more details about a proposed answer. We refer to this type of comment as update request comments (URCs), which may trigger an update to the answer post and thus improve its quality. In this study, we manually analyze a set of 384 sampled SO answer posts and their associated 1,221 comments to investigate the prevalence of URCs and how URCs are addressed. We find that around half of the analyzed comments are URCs. While 55.3% of URCs are addressed within 24 h, 36.5% of URCs remain unaddressed after a year. Moreover, we find that the current community-vote mechanism could not differentiate URCs from non-URCs. Thus many URCs might not be aware by users who can address the issue or improve the answer quality. As a first step to enhance the awareness of URCs and support future research on URCs, we investigate the feasibility of URC detection by proposing a set of features extracted from different aspects of SO comments and using them to build supervised classifiers that can automatically identify URCs. Our experiments on 377 and 289 comments posted on answers to JavaScript and Python questions show that the proposed URC classifier can achieve an accuracy of 90% and an AUC of 0.96, on average. © 2022 Elsevier Inc.","Answer quality; Classification; Commenting; Crowd-sourced knowledge sharing; Knowledge maintenance and update; Stack overflow"
"An estimation of distribution algorithm based on interactions between requirements to solve the bi-objective Next Release Problem","2023","Journal of Systems and Software","10.1016/j.jss.2023.111632","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147607370&doi=10.1016%2fj.jss.2023.111632&partnerID=40&md5=414ff8c59d77125749b4970758ef7cdc","Selecting the appropriate requirements to develop in the next release of an open market software product under evolution, is a compulsory step of each software development project. This selection should be done by maximizing stakeholders’ satisfaction and minimizing development costs, while keeping constraints. In this work we investigate what is the requirements interactions impact when searching for solutions of the bi-objective Next Release Problem. In one hand, these interactions are explicitly included in two algorithms: a branch and bound algorithm and an estimation of distribution algorithm (EDA). And on the other, we study the performance of these not previously used solving approaches by applying them in several instances of small, medium and large size data sets. We find that interactions inclusion do enhance the search and when time restrictions exists, as in the case of the bi-objective Next Release Problem, EDAs have proven to be stable and reliable locating a large number of solutions on the reference Pareto front. © 2023 The Author(s)","Estimation of distribution algorithms; Next release problem; Requirements interactions; Software requirements"
"Multi-objective empirical computational complexity of single-tenant service instances deployed at the Edge","2023","Journal of Systems and Software","10.1016/j.jss.2023.111665","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149436770&doi=10.1016%2fj.jss.2023.111665&partnerID=40&md5=64d4c2e8170e5d2b0b5dab757f719c13","The Edge has been recently introduced to augment the back-end of apps with service instances deployed to machines that are located close to end-users’ devices. However, given that machines at the Edge are usually resource constrained, a service instance that needs a big amount of hardware resources may be inefficiently executed at the Edge. We consider a service instance is Edge-efficient if the runtime consumption of hardware resources by a service instance is lower than the amount of the hardware resources provided by a machine at the Edge. The runtime consumption of hardware resources of a service instance depend on the computational complexity of the algorithms implemented by the service instance. The computational complexity should include multiple objectives like the time objective and the space objective. Each objective usually corresponds to a separate hardware resource. However, the automated calculation of the theoretical computational complexity is reduced to the insolvable halting problem. We propose an approach that learns the multi-objective empirical complexity of service instances deployed at the Edge as mathematical functions of the input parameters of the programming interface of services. We further contribute with an algorithm that decides on whether a service instance is Edge-efficient. We evaluate the accuracy of our approach via measuring the percentages of the correct decisions on Edge-efficient service instances. We use in our experiment a real-world mobile app that has been deployed to a resource-constrained machine at the Edge. We compare the accuracy of our approach against two baseline state-of-the-art approaches. The results show that the accuracy of our approach is higher than the accuracy of the baseline approaches. © 2023 Elsevier Inc.","Edge computing; Empirical complexity; Predictive model; Web services"
"A mutual embedded self-attention network model for code search","2023","Journal of Systems and Software","10.1016/j.jss.2022.111591","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146733183&doi=10.1016%2fj.jss.2022.111591&partnerID=40&md5=ffa751a5b888fcd1d4cff40a37086239","To improve the efficiency of program implementation, developers can selectively reuse the previously written code by searching the open-source codebase. To date, many code search methods have been proposed to actively push the limit of code search accuracy, where the methods designed using Self-Attention mechanism are particularly promising. However, while existing methods can improve the efficiency to capture textual semantics by attending significant words in the code component unit, they typically fail to capture the structural dependencies between the code components which may produce suboptimal search accuracy. In this paper, we propose a novel Self-Attention model termed MESN-CS which considers both word-level attention and code unit-level attention for code search. MESN-CS not only the attention weight of each word in the code component unit is calculated, but also the weight of the embedding between the code combination units is calculated. To verify the effectiveness of the proposed model, three benchmark models were compared on a large-scale code data and CodesearchNet. The experimental results show that the MESN-CS has better Recall@k, NDCG and MRR performance than baseline methods. the experiments also show that the semantic syntactic information between sequences can be effectively characterized in MESN-CS. © 2022 Elsevier Inc.","Code search; Code segments; Machine learning; MESN-CS; Self-attention"
"Software architecture for quantum computing systems — A systematic review","2023","Journal of Systems and Software","10.1016/j.jss.2023.111682","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151881159&doi=10.1016%2fj.jss.2023.111682&partnerID=40&md5=949a3516e7bfda8a68429eeb94166d2d","Quantum computing systems rely on the principles of quantum mechanics to perform a multitude of computationally challenging tasks more efficiently than their classical counterparts. The architecture of software-intensive systems can empower architects who can leverage architecture-centric processes, practices, description languages to model, develop, and evolve quantum computing software (quantum software for short) at higher abstraction levels. We conducted a Systematic Literature Review (SLR) to investigate (i) architectural process, (ii) modelling notations, (iii) architecture design patterns, (iv) tool support, and (iv) challenging factors for quantum software architecture. Results of the SLR indicate that quantum software represents a new genre of software-intensive systems; however, existing processes and notations can be tailored to derive the architecting activities and develop modelling languages for quantum software. Quantum bits (Qubits) mapped to Quantum gates (Qugates) can be represented as architectural components and connectors that implement quantum software. Tool-chains can incorporate reusable knowledge and human roles (e.g., quantum domain engineers, quantum code developers) to automate and customise the architectural process. Results of this SLR can facilitate researchers and practitioners to develop new hypotheses to be tested, derive reference architectures, and leverage architecture-centric principles and practices to engineer emerging and next generations of quantum software. © 2023 The Authors","Quantum computing; Quantum software architecture; Quantum software engineering; Systematic literature review"
"Timed pattern-based analysis of collaboration failures in system-of-systems","2023","Journal of Systems and Software","10.1016/j.jss.2023.111613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146673238&doi=10.1016%2fj.jss.2023.111613&partnerID=40&md5=ef6e5ff066329176daaa91cf45affdcc","A system-of-systems (SoS) tries to achieve prominent goals, such as increasing road capacity in platooning that groups driving vehicles in proximity, through interactions between constituent systems (CSs). However, during the collaboration of CSs, unintended interference in interactions causes collaboration failures that may lead to catastrophic damage, particularly for the safety–critical SoS. It is necessary to analyze the failure-inducing interactions (FIIs) during the collaboration and resolve the root causes of failures. Existing studies have utilized pattern-mining techniques to analyze system failures from logs. However, they have three limitations when applied to collaboration failures: (1) information loss caused by the limited capabilities of handling interaction logs; (2) limitations in identifying multiple failure patterns in a log; (3) absence of an end-to-end solution mapping patterns to faults. To overcome these limitations, we propose an FII pattern mining algorithm covering the main features of SoS interaction logs, an overlapping clustering technique for multiple pattern mining, and a pattern-based fault localization method. In experiments conducted on platooning and mass casualty incident-response SoS, the proposed approach exhibited the highest pattern mining and clustering accuracy and achieved feasible localization performance compared with existing methods. The findings of this study can facilitate the accurate analysis of collaboration failures. © 2023 Elsevier Inc.","Failure-inducing interaction; Fault localization; Pattern-based clustering; System-of-systems"
"Interaction detection in configurable systems – A formal approach featuring roles","2023","Journal of Systems and Software","10.1016/j.jss.2022.111556","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142472681&doi=10.1016%2fj.jss.2022.111556&partnerID=40&md5=72eaeb28282a728c7b0343dd149f6a5b","Modern software systems are increasingly complex due to their configurability and adaptivity. For modeling and implementing such systems, the concept of roles is particularly well-suited as it allows capturing context-dependent properties and behavior. Similar to other compositional approaches, notably the feature-oriented development approach, the detection of (unintended) interactions between roles is a challenging task. We consider two new aspects of interactions. Hierarchical interactions may occur between systems and active interplays describe the actual situations in a sequence of events where components interact. To reason about such interactions, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). Based on this formal foundation, we present a modeling language for succinctly describing RBAs and an implementation for translating this language into the input language of the probabilistic model checker PRISM. This enables a formal analysis of functional and non-functional properties including system dynamics, context changes, and interactions. We carry out three experimental studies as a proof of concept of such analyses: First, a peer-to-peer protocol study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput. Third, we illustrate how to incorporate feature-oriented system design in our role-oriented framework by means of the elevator community benchmark system. © 2022 Elsevier Inc.","Feature-oriented systems; Formal methods; Model checking; Role-oriented systems; Software product lines"
"Theory of constructed emotion meets RE: An industrial case study","2023","Journal of Systems and Software","10.1016/j.jss.2022.111544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144085233&doi=10.1016%2fj.jss.2022.111544&partnerID=40&md5=4246f1d93a6f2a8746aaf4917ab78e6a","Emotional requirements should be treated as first-class citizens rather than subsumed under “non-functional” requirements. This follows already from three primary elements of persuasion by Aristotle, being logos, ethos and pathos, which respectively stand for function, quality and emotion. Eliciting and representing emotional requirements should be based on up-to-date emotion theories, which are backed by cognitive psychology and neuroscience. The most promising among them is the theory of constructed emotion. Accordingly, this paper aims to find out what are the advantages of grounding requirements engineering in the theory of constructed emotion. We also aim to explore the possible methods or techniques that support the construction of emotions in the requirements engineering process for building emotion aware systems and how they could be utilised by stakeholders of a sociotechnical system with different backgrounds. By utilising the action design research method, we first formulate an appropriate methodology and then apply it for building and evaluating an artefact, which in our case study consists of the animations shown on the Media Wall. The main contribution of our paper is an original repeatable methodology for eliciting and representing requirements for interdisciplinary design projects aimed at designing software-intensive emotive artefacts. The methodology is rooted in the theory of constructed emotion. Although the proposed methodology can in principle be used for designing and developing any sociotechnical systems, a particular variation of the methodology proposed in this paper is geared towards designing and developing emotive artefacts that have the purpose to co-construct certain emotions among the stakeholders and the audience with the goal to further particular societal issues. © 2022 Elsevier Inc.","Animation; Design; Goal modelling; Requirements engineering; Sociotechnical system; Theory of constructed emotion"
"Architectural tactics in software architecture: A systematic mapping study","2023","Journal of Systems and Software","10.1016/j.jss.2022.111558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143753938&doi=10.1016%2fj.jss.2022.111558&partnerID=40&md5=c068860c55e58bf318d02df5885e201d","Architectural tactics are a key abstraction of software architecture, and support the systematic design and analysis of software architectures to satisfy quality attributes. Since originally proposed in 2003, architectural tactics have been extended and adapted to address additional quality attributes and newer kinds of systems, making quite hard for researchers and practitioners to master this growing body of specialized knowledge. This paper presents the design, execution and results of a systematic mapping study of architectural tactics in software architecture literature. The study found 552 studies in well-known digital libraries, of which 79 were selected and 12 more were added with snowballing, giving a total of 91 primary studies. Key findings are: (i) little rigor has been used to characterize and define architectural tactics; (ii) most architectural tactics proposed in the literature do not conform to the original definition; and (iii) there is little industrial evidence about the use of architectural tactics. This study organizes and summarizes the scientific literature to date about architectural tactics, identifies research opportunities, and argues for the need of more systematic definition and description of tactics. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 Elsevier Inc.","Architectural tactics; Quality attributes; Software architecture; Systematic mapping study"
"Automatic modelling and verification of AUTOSAR architectures","2023","Journal of Systems and Software","10.1016/j.jss.2023.111675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150886471&doi=10.1016%2fj.jss.2023.111675&partnerID=40&md5=ef1ddedad388779ea39dd57bfa79be94","AUTOSAR (AUTomotive Open System ARchitecture) is a development partnership whose primary goal is the standardization of basic system functions and functional interfaces for electronic control units in automobiles. As an open specification, its layered software architecture promotes the interoperability of real-time embedded vehicle systems and components. It also opens up the possibility of formal modelling and verification approaches, centred around the specification, that can be used to support analysis in the early stages of design. In this paper, we describe a methodology and associated tool, called A2A, that automatically models systems defined by the AUTOSAR specifications as timed automata, and then verifies their timing properties using UPPAAL. It contains 22 groups of timed automata templates, together with two auxiliary test templates, that model the AUTOSAR architecture and timing properties, allowing time-related behaviours to be extracted from the three-layer architecture, i.e., the AUTOSAR Software, AUTOSAR Runtime Environment, and Basic Software layers, and templates to be automatically instantiated. The timing properties are specified using timed computation tree logic (TCTL) in UPPAAL to verify the system model. We demonstrate the capabilities of the methodology by applying it to an AUTOSAR architecture that describes an internal vehicle light control system, thereby showing its effectiveness. © 2023 Elsevier Inc.","AUTOSAR; Formal modelling; Timed automata; Vehicle electronic system; Verification"
"On measuring coupling between microservices","2023","Journal of Systems and Software","10.1016/j.jss.2023.111670","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149440113&doi=10.1016%2fj.jss.2023.111670&partnerID=40&md5=6f986b71b835becb8740ff2cde17d46f","In software quality management, the selection strategy for proper metrics varies depending on the application scenarios and measurement objectives. MicroService Architecture (MSA), despite being commonly employed nowadays, still cannot be reliably measured and compared if the microservices in a system are independent. Software managers and architects need to understand whether their microservices are “decoupled enough”, if not, which ones are over-coupled, and by how much. In this paper, we contribute a novel set of metrics – Microservice Coupling Index (MCI) – derived from the relative measurement theory. Instead of measuring coupling evidence with simple counts, we measure how dependent and coupled the microservices are relative to the possible couplings between them. We measured the MCI metrics for 15 open source projects that involve 113 distinct microservices. Empirical investigation confirmed that MCIs differ quite significantly from existing coupling measures and that they are more discriminative than existing ones for separating high and low degrees of microservice couplings and thus more useful in comparing design alternatives. A series of experimental studies were conducted, showing that the larger the MCIs, the less likely the bugs and changes can be localized and separated, and the less likely that the individual microservices in a system can be independently developed and evolved. © 2023 Elsevier Inc.","Change impact; Coupling measure; Microservices architecture; Software quality"
"Colla-Config: A stakeholders preferences-based approach for product lines collaborative configuration","2023","Journal of Systems and Software","10.1016/j.jss.2022.111586","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144456878&doi=10.1016%2fj.jss.2022.111586&partnerID=40&md5=9ed43b3d07efdef349db8d2fc06882a8","During collaborative configuration of software product lines (SPL), multiple stakeholders contribute together in building a single product specification. Conflicting situations can arise during the configuration process due to contradictions between some/all stakeholders’ configuration choices. Detecting and resolving such situation rise two major challenges: choosing which choices to omit, and taking stakeholders’ preferences into account. Several SPL collaborative configuration approaches are available. However, they either do not present detailed information on the strategies for conflict resolution, or they rely on a systematic process that resolves conflicts by prioritizing configuration decisions made at earlier stage, constraining therefore some of stakeholders’ choices. The lack of flexibility may hinder conflict resolution as choices taken at earlier stages overlay those at later phases. To mitigate these limitations, we propose a new collaborative configuration approach (Colla-Config) that provides a preference-based conflict resolution method within a free-order configuration process; each stakeholder expresses his/her preferences through a set of substitution rules, and freely makes his/her configuration decisions towards the desired product, without being constrained by the configuration decisions made by the other ones. To assess the feasibility and the usability of the proposed approach, we conducted a usability test designed following the ISO/IEC 25062:2006 Common Industry Format for usability tests. Results of the experiments provide preliminary evidence of the approach feasibility and the tool ability to properly support the SPL collaborative configuration. © 2022 Elsevier Inc.","Collaborative configuration; Software product lines; Stakeholder preferences; Usability tool test"
"Distributed state model inference for scriptless GUI testing","2023","Journal of Systems and Software","10.1016/j.jss.2023.111645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148330004&doi=10.1016%2fj.jss.2023.111645&partnerID=40&md5=41f57295a4a662f0fd24dc7ed88c90e6","State model inference of software applications through the Graphical User Interface (GUI) is a technique that identifies GUI states and transitions, and maps them into a model. Scriptless GUI testing tools can benefit substantially from the availability of these state models, for example, to improve the exploration, or have sophisticated test oracles. However, inferring models for large systems requires a long execution time. Our goal is to improve the speed of the state model inference process. To achieve this goal, this paper presents a distributed state model inference approach with an open source scriptless GUI testing tool. Moreover, in order to be able to infer a suitable model, we design a set of strategies to deal with abstraction challenges and to distinguish GUI states and transitions in the model. To validate it, we conduct an experiment with two open source web applications that have been tested with the distributed architecture using one to six Docker containers sharing the same state model. With the obtained results, we can conclude that it is feasible to infer a model with a distributed approach and that using the distributed approach reduces the time required for inferring a state model. © 2023 The Author(s)","Distributed systems; Empirical evaluation; Scriptless testing; State model inference"
"The pipeline for the continuous development of artificial intelligence models—Current state of research and practice","2023","Journal of Systems and Software","10.1016/j.jss.2023.111615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153617843&doi=10.1016%2fj.jss.2023.111615&partnerID=40&md5=fbd1967d1386f4ce1d4aa634537ce2fb","Companies struggle to continuously develop and deploy Artificial Intelligence (AI) models to complex production systems due to AI characteristics while assuring quality. To ease the development process, continuous pipelines for AI have become an active research area where consolidated and in-depth analysis regarding the terminology, triggers, tasks, and challenges is required. This paper includes a Multivocal Literature Review (MLR) where we consolidated 151 relevant formal and informal sources. In addition, nine-semi structured interviews with participants from academia and industry verified and extended the obtained information. Based on these sources, this paper provides and compares terminologies for Development and Operations (DevOps) and Continuous Integration (CI)/Continuous Delivery (CD) for AI, Machine Learning Operations (MLOps), (end-to-end) lifecycle management, and Continuous Delivery for Machine Learning (CD4ML). Furthermore, the paper provides an aggregated list of potential triggers for reiterating the pipeline, such as alert systems or schedules. In addition, this work uses a taxonomy creation strategy to present a consolidated pipeline comprising tasks regarding the continuous development of AI. This pipeline consists of four stages: Data Handling, Model Learning, Software Development and System Operations. Moreover, we map challenges regarding pipeline implementation, adaption, and usage for the continuous development of AI to these four stages. © 2023 The Authors","CI/CD for AI; Continuous (end-to-end) lifecycle pipeline for AI; Continuous development of AI; DevOps for AI; MLOps; Multivocal literature review"
"A survey of blockchain consensus safety and security: State-of-the-art, challenges, and future work","2023","Journal of Systems and Software","10.1016/j.jss.2022.111555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142513420&doi=10.1016%2fj.jss.2022.111555&partnerID=40&md5=2d9804cf63245bbc6c67ba42574fd1ed","Blockchain technology has been widely used in finance, Internet of Things, insurance, and other fields since it was proposed in 2008. As a part of blockchain technology, the consensus protocol has a profound impact on the safety and security of blockchain systems. Therefore, there have been a lot of research on the safety and security of blockchain consensus protocols. However, the research shows us the diversity and complexity which motivate us to do this survey. In this paper, we provide a comprehensive survey focusing on the safety and security of blockchain consensus protocols, where we discuss in detail the classification of blockchain consensus protocols, the potential safety and security problems, safety and security assurance approaches, and validation methods etc. Furthermore, we also identify research challenges and current research gaps, and suggest future work to be further investigated in the end of this paper. © 2022 Elsevier Inc.","Blockchain consensus; Challenges; Consensus safety; Consensus security"
"Neural-FEBI: Accurate function identification in Ethereum Virtual Machine bytecode","2023","Journal of Systems and Software","10.1016/j.jss.2023.111627","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147551485&doi=10.1016%2fj.jss.2023.111627&partnerID=40&md5=f998eed484f4d2fd796349d2c5f9d878","Millions of smart contracts have been deployed onto the Ethereum platform, posing potential attack subjects. Therefore, analyzing contract binaries is vital since their sources are unavailable, involving identification comprising function entry identification and detecting its boundaries. Such boundaries are critical to many smart contract applications, e.g. reverse engineering and profiling. Unfortunately, it is challenging to identify functions from these stripped contract binaries due to the lack of internal function call statements and the compiler-inducing instruction reshuffling. Recently, several existing works excessively relied on a set of handcrafted heuristic rules which impose several faults. To address this issue, we propose a novel neural network-based framework for EVM bytecode Function Entries and Boundaries Identification (neural-FEBI) that does not rely on a fixed set of handcrafted rules. Instead, it used a two-level bi-Long Short-Term Memory network and a Conditional Random Field network to locate the function entries. The suggested framework also devises a control flow traversal algorithm to determine the code segments reachable from the function entry as its boundary. Several experiments on 38,996 publicly available smart contracts collected as binary demonstrate that neural-FEBI confirms the lowest and highest F1-scores for the function entries identification task across different datasets of 88.3 to 99.7, respectively. Its performance on the function boundary identification task is also increased from 79.4% to 97.1% compared with state-of-the-art. We further demonstrate that the identified function information can be used to construct more accurate intra-procedural CFGs and call graphs. The experimental results confirm that the proposed framework significantly outperforms state-of-the-art, often based on handcrafted heuristic rules. © 2023 The Authors","Binary analysis; Control flow traversal; Ethereum smart contract; Function identification; LSTM-CRF"
"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection","2023","Journal of Systems and Software","10.1016/j.jss.2023.111623","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147191808&doi=10.1016%2fj.jss.2023.111623&partnerID=40&md5=35cd17dd043c1d9487c1e1b02c7a2420","In order to secure software, it is critical to detect potential vulnerabilities. The performance of traditional static vulnerability detection methods is limited by predefined rules, which rely heavily on the expertise of developers. Existing deep learning-based vulnerability detection models usually use only a single sequence or graph embedding approach to extract vulnerability features. Sequence embedding-based models ignore the structured information inherent in the code, and graph embedding-based models lack effective node and graph embedding methods. As a result, we propose a novel deep learning-based approach, CSGVD (Combining Sequence and Graph embedding for Vulnerability Detection), which considers function-level vulnerability detection as a graph binary classification task. Firstly, we propose a PE-BL module, which inherits and enhances the knowledge from the pre-trained language model. It extracts the code's local semantic features as node embedding in the control flow graph by using sequence embedding. Secondly, CSGVD uses graph neural networks to extract the structured information of the graph. Finally, we propose a mean biaffine attention pooling, M-BFA, to better aggregate node information as a graph's feature representation. The experimental results show that CSGVD outperforms the existing state-of-the-art models and obtains 64.46% accuracy on the real-world benchmark dataset from CodeXGLUE for vulnerability detection. © 2023 Elsevier Inc.","Attention pooling; Graph embedding; Graph neural networks; Pre-trained language model; Sequence embedding; Vulnerability detection"
"Secure and flexible message-based communication for mobile apps within and across devices","2022","Journal of Systems and Software","10.1016/j.jss.2022.111460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135905688&doi=10.1016%2fj.jss.2022.111460&partnerID=40&md5=0878a11cb40393dfcd4f3068f555fc05","In modern mobile platforms, message-based communication is afflicted by data leakage attacks, through which untrustworthy apps access the transferred message data. Existing defenses are overly restrictive, as they block all suspicious message exchanges, thus preventing any app from receiving messages. To better secure message-based communication, we present a model that strengthens security, while also allowing untrusted-but-not-malicious apps to execute their business logic. Our model, HTPD, introduces two novel mechanisms: hidden transmission and polymorphic delivery. Sensitive messages are transmitted hidden in an encrypted envelope. Their delivery is polymorphic: as determined by the destination's trustworthiness, it can be delivered no data, raw data, or encrypted data. To allow an untrusted destination to operate on encrypted data deliveries, HTPD integrates homomorphic and convergent encryption. We concretely realize HTPD as POLICC, a plug-in replacement of Android Inter-Component Communication (ICC) middleware. POLICC mitigates three classic Android data leakage attacks, while allowing untrusted apps to perform useful operations on delivered messages. Our evaluation shows that POLICC supports secure message-based communication within and across devices by trading off performance costs, programming effort overheads, and security1. © 2022 Elsevier Inc.","Message-based communication; Mobile security; Secure inter-component communication"
"Update with care: Testing candidate bug fixes and integrating selective updates through binary rewriting","2022","Journal of Systems and Software","10.1016/j.jss.2022.111381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131734228&doi=10.1016%2fj.jss.2022.111381&partnerID=40&md5=e14d303b64a62b014d9835dcd7ac1388","Enterprise software updates depend on the interaction between user and developer organizations. This interaction becomes especially complex when a single developer organization writes software that services hundreds of different user organizations. Miscommunication during patching and deployment efforts lead to insecure or malfunctioning software installations. While developers oversee the code, the update process starts and ends outside their control. Since developer test suites may fail to capture buggy behavior finding and fixing these bugs starts with user generated bug reports and 3rd party disclosures. The process ends when the fixed code is deployed in production. Any friction between user, and developer results in a delay patching critical bugs. Two common causes for friction are a failure to replicate user specific circumstances that cause buggy behavior and incompatible software releases that break critical functionality. Existing test generation techniques are insufficient. They fail to test candidate patches for post-deployment bugs and to test whether the new release adversely effects customer workloads. With existing test generation and deployment techniques, users cannot choose (nor validate) compatible portions of new versions and retain their previous version's functionality. We present two new technologies to alleviate this friction. First, Test Generation for Ad Hoc Circumstances transforms buggy executions into test cases. Second, Binary Patch Decomposition allows users to select the compatible pieces of update releases. By sharing specific context around buggy behavior and developers can create specific test cases that demonstrate if their fixes are appropriate. When fixes are distributed by including extra context users can incorporate only updates that guarantee compatibility between buggy and fixed versions. We use change analysis in combination with binary rewriting to transform the old executable and buggy execution into a test case including the developer's prospective changes that let us generate and run targeted tests for the candidate patch. We also provide analogous support to users, to selectively validate and patch their production environments with only the desired bug-fixes from new version releases. This paper presents a new patching workflow that allows developers to validate prospective patches and users to select which updates they would like to apply, along with two new technologies that make it possible. We demonstrate our technique constructs tests cases more effectively and more efficiently than traditional test case generation on a collection of real world bugs compared to traditional test generation techniques, and provides the ability for flexible updates in real world scenarios. © 2022 Elsevier Inc.","Binary analysis; Binary rewriting; Change analysis; Test generation"
"An efficient heuristic algorithm for software module clustering optimization","2022","Journal of Systems and Software","10.1016/j.jss.2022.111349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134889221&doi=10.1016%2fj.jss.2022.111349&partnerID=40&md5=6c127a94a7780f09eaf4c0bfc86dadfa","In the lifecycle of software projects, maintenance tasks usually entail 75% of the total costs, where most efforts are spent in understanding the program. To improve the maintainability of software projects, the code is often divided into components, which are then grouped in different modules following good design principles, lowering coupling and increasing cohesion. The Software Module Clustering Problem (SMCP) is an optimization problem that looks for maximizing the modularity of software projects in the context of Search-Based Software Engineering. In the SMCP, projects are often modeled as graphs. Therefore, the SMCP can be interpreted as a graph partitioning problem, which is proved to be NP-hard. In this work, we propose a new heuristic algorithm for software modularization, based on a Greedy Randomized Adaptive Search Procedure with Variable Neighborhood Descent. We present a three-fold categorization of neighborhoods for the SMCP and leverage domain-specific information to filter unpromising solutions. Our proposal has been successfully tested over a dataset of real software projects, outperforming the previous state-of-the-art approach in terms of Modularization Quality in very short computing times. Therefore, it could be integrated in software development tools to improve the quality of software projects in real time. © 2022 Elsevier Inc.","Heuristics; Maintainability; Modularization Quality; Search-Based Software Engineering; Software Module Clustering"
"Reducing large adaptation spaces in self-adaptive systems using classical machine learning","2022","Journal of Systems and Software","10.1016/j.jss.2022.111341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130373437&doi=10.1016%2fj.jss.2022.111341&partnerID=40&md5=ac220b30620336ff8c9ef295d9032e9d","Modern software systems often have to cope with uncertain operation conditions, such as changing workloads or fluctuating interference in a wireless network. To ensure that these systems meet their goals these uncertainties have to be mitigated. One approach to realize this is self-adaptation that equips a system with a feedback loop. The feedback loop implements four core functions – monitor, analyze, plan, and execute – that share knowledge in the form of runtime models. For systems with a large number of adaptation options, i.e., large adaptation spaces, deciding which option to select for adaptation may be time consuming or even infeasible within the available time window to make an adaptation decision. This is particularly the case when rigorous analysis techniques are used to select adaptation options, such as formal verification at runtime, which is widely adopted. One technique to deal with the analysis of a large number of adaptation options is reducing the adaptation space using machine learning. State of the art has showed the effectiveness of this technique, yet, a systematic solution that is able to handle different types of goals is lacking. In this paper, we present ML2ASR+, short for Machine Learning to Adaptation Space Reduction Plus. Central to ML2ASR+ is a configurable machine learning pipeline that supports effective analysis of large adaptation spaces for threshold, optimization, and setpoint goals. We evaluate ML2ASR+ for two applications with different sizes of adaptation spaces: an Internet-of-Things application and a service-based system. The results demonstrate that ML2ASR+ can be applied to deal with different types of goals and is able to reduce the adaptation space and hence the time to make adaptation decisions with over 90%, with negligible effect on the realization of the adaptation goals. © 2022 The Authors","Adaptation space reduction; Analysis; Machine learning; Self-adaptation"
"Feature models to boost the vulnerability management process","2023","Journal of Systems and Software","10.1016/j.jss.2022.111541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140303293&doi=10.1016%2fj.jss.2022.111541&partnerID=40&md5=f7baa8db4d1e6d53d54fffffaf0c460c","Vulnerability management is a critical and very challenging process that allows organisations to design a procedure to identify potential vulnerabilities, assess the level of risk, and define remediation mechanisms to address threats. Thus, the large number of configuration options in systems makes it extremely difficult to identify which configurations are affected by vulnerabilities and even assess how systems may be affected. There are several repositories to store information on systems, software vulnerabilities, and exploits. However, they are largely scattered, offer different formats and information, and their use has limitations, complicating vulnerability management automation. For this reason, we introduce a discussion concerning modelling in vulnerability management and the proposal of feature models as a means to collect the variability of software and system configurations to facilitate the vulnerability management process. This paper presents AMADEUS-Exploit, a feature model-based solution that provides query and reasoning mechanisms that make it easier for vulnerability management experts. The power of AMADEUS-Exploit is shown and evaluated in three different ways: first, the solution is compared with other vulnerability management tools; second, the solution is faced with another in a complex scenario with 4,000 vulnerabilities and 700 exploits; and finally, our solution was used in a real project demonstrating the usability of reasoning operations to determine potential vulnerabilities. © 2022 Elsevier Inc.","Cybersecurity; Exploits; Feature model; Reasoning; Vulnerability; Vulnerable management process"
"Burr-type NHPP-based software reliability models and their applications with two type of fault count data","2022","Journal of Systems and Software","10.1016/j.jss.2022.111367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131463204&doi=10.1016%2fj.jss.2022.111367&partnerID=40&md5=986e487192db15e722df6e19b24381ec","In this paper, we summarize the so-called Burr-type software reliability models (SRMs) based on the non-homogeneous Poisson process (NHPP) and comprehensively evaluate the model performances by comparing them with the existing NHPP-based SRMs. Two kinds of software fault count data are considered; fault-detection time-domain data and fault-detection time-interval data (group data). For 8 data sets in each fault count type, we estimate the model parameters by means of the maximum likelihood estimation and evaluate the performance metrics in terms of goodness-of-fit and prediction. It is shown that the Burr-type NHPP-based SRMs could show the better performances than the existing NHPP-based SRMs in many cases. The main contribution of the paper consists in suggesting that the Burr-type NHPP-based SRMs should be the possible candidates for selecting the best SRM in terms of goodness-of-fit and predictive performances. © 2022 Elsevier Inc.","Burr-type distributions; Goodness-of-fit performance; Non-homogeneous Poisson processes; Predictive performance; Software reliability models"
"Explainable persuasion for interactive design: The case of online gambling","2023","Journal of Systems and Software","10.1016/j.jss.2022.111517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140471131&doi=10.1016%2fj.jss.2022.111517&partnerID=40&md5=80304a16915d0066e79c4b67306b41e0","Persuasive technology refers to digital means that influence attitude behaviour, and decisions. While the professional design of persuasive interfaces considers user interests and freedom of choice a primary requirement, principles and methods to achieve it are yet to be introduced. In the design of persuasive interfaces, fulfilling conditions of informed consent can help establish transparency and address such ethical issues. This paper defined explainable persuasion, its potential form, and benefits and explored whether explainable persuasion is a user requirement on demand. This paper further examined explainable persuasion design from the user's perspective and reported on acceptance and rejection factors, as well as possible design tensions and solutions. In this study, we took online gambling as a case study. A total of 250 UK-based users of gambling platforms (age range 18 – 75, 18–75, 127 female) completed our online survey based on principles of persuasion and explainability. Findings showed that players were aware of the use, persuasive intent, and potential harm of various persuasive design techniques used in online gambling platforms (e.g., the use of in-game rewards, reminders, and praise to encourage further gambling). Despite this awareness, they agreed that explainable persuasion can still help users stay in control of their online experience, increase their positive attitude towards the online system, and keep them reminded of the potential side effects of persuasive interfaces. Future research is required to enhance the design and implementation of explainable persuasion in persuasive interfaces. © 2022 The Author(s)","Ethical design; Explainability; Informed consent; Online gambling; Persuasive systems"
"Mutation-based analysis of queueing network performance models","2022","Journal of Systems and Software","10.1016/j.jss.2022.111385","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132376448&doi=10.1016%2fj.jss.2022.111385&partnerID=40&md5=0fcc20442329b9a57648c4a0c7922a54","Performance models have been used in the past to understand the performance characteristics of software systems. However, the identification of performance criticalities is still an open challenge, since there might be several system components contributing to the overall system performance. This work combines two different areas of research to improve the process of interpreting model-based performance analysis results: (i) software performance engineering that provides the ground for the evaluation of the system's performance; (ii) mutation-based techniques that nicely supports the experimentation of changes in performance models and contribute to a more systematic assessment of performance indices. We propose mutation operators for specific performance models, i.e., queueing networks, that resemble changes commonly made by designers when exploring the properties of a system's performance. Our approach consists in introducing a mutation-based approach that generates a set of mutated queueing network models. The performance of these mutated networks is compared to that of the original network to better understand the effect of variations in the different components of the system. A set of benchmarks is adopted to show how the technique can be used to get a deeper understanding of the performance characteristics of software systems. © 2022 The Author(s)","Model-based performance analysis; Mutation; Queueing Networks"
"Automating Feature Model maintainability evaluation using machine learning techniques","2023","Journal of Systems and Software","10.1016/j.jss.2022.111539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141259843&doi=10.1016%2fj.jss.2022.111539&partnerID=40&md5=3aab9ec451d5f24dd960c3791922d4c5","Context: Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. Objective: This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. Methods: To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. Results: The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures. Furthermore, we created a mechanism to suggest FM refactorings to improve the maintainability of this artifact. FM maintainability evaluation and the refactoring suggestion mechanism were automated in the DyMMer tool. Conclusion: We conclude this work by presenting a way to combine FM maintainability assessment with FM refactorings. The results of this work provide to domain engineers inputs that will allow them to carry out a continuous improvement of an SPL. © 2022 Elsevier Inc.","Feature Model; Machine Learning; Quality Assessment; Software Product Lines"
"A qualitative analysis of themes in instant messaging communication of software developers","2022","Journal of Systems and Software","10.1016/j.jss.2022.111397","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133742387&doi=10.1016%2fj.jss.2022.111397&partnerID=40&md5=e1f0b6ba995baf35de64de28e3d5ed9e","Software developers use instant messaging (e.g., Slack, Gitter) to collaboratively discuss software engineering problems and solutions. This communication takes place in chat rooms that generally contain a description of the main topic of discussion and the messages exchanged. To analyze whether and how the knowledge accumulated in these chat rooms is relevant to other developers, we first need to understand the themes discussed in these chat rooms. In this paper, we used thematic analysis to manually identify software engineering themes in the description of 87 chat rooms of Gitter, an instant messaging tool for software developers. Then, we checked whether these themes also occur in 184 public chat rooms of Slack, another instant messaging tool. We identified 47 themes in Gitter chat rooms, and regarding the applicability of themes, we could relate 36 of our themes to 173 Slack chat rooms. Our results indicate that, in the context of our study, chat rooms in developer instant messaging communication are mostly about software development technologies and practices rather than development processes. Furthermore, most chat rooms are topic- rather than project-related (e.g., a chat room used by developers of a particular software development project). © 2022 The Authors","Developer communication; Instant messaging; Reusable knowledge; Software engineering themes; Thematic analysis"
"Automated repair of resource leaks in Android applications","2022","Journal of Systems and Software","10.1016/j.jss.2022.111417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134325836&doi=10.1016%2fj.jss.2022.111417&partnerID=40&md5=bb8fe86c5326b16b9929f92affe5f0b2","Resource leaks – a program does not release resources it previously acquired – are a common kind of bug in Android applications. Even with the help of existing techniques to automatically detect leaks, writing a leak-free program remains tricky. One of the reasons is Android's event-driven programming model, which complicates the understanding of an application's overall control flow. In this paper, we present [Formula presented]: a technique to automatically detect and fix resource leaks in Android applications. [Formula presented] builds a succinct abstraction of an app's control flow, and uses it to find execution traces that may leak a resource. The information built during detection also enables automatically building a fix – consisting of release operations performed at appropriate locations – that removes the leak and does not otherwise affect the application's usage of the resource. An empirical evaluation on resource leaks from the [Formula presented] curated collection demonstrates that [Formula presented] ’s approach is scalable, precise, and produces correct fixes for a variety of resource leak bugs: [Formula presented] automatically found and repaired 50 leaks that affect 9 widely used resources of the Android system, including all those collected by [Formula presented] for those resources; on average, it took just 2 min to detect and repair a leak. [Formula presented] also compares favorably to Relda2/RelFix – the only other fully automated approach to repair Android resource leaks – since it can often detect more leaks with higher precision and producing smaller fixes. These results indicate that [Formula presented] can provide valuable support to enhance the quality of Android applications in practice. © 2022 The Author(s)","Android applications; Automated program repair; Program analysis; Static analysis"
"Factors affecting Agile adoption: An industry research study of the mobile app sector in Saudi Arabia","2022","Journal of Systems and Software","10.1016/j.jss.2022.111347","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129758848&doi=10.1016%2fj.jss.2022.111347&partnerID=40&md5=d2e5981b65ffd5c65e3da53369b4c987","Agile is an established software development methodology that helps develop software by improving time to market, quality, customer engagement and reducing costs. Factors underpinning its adoption have been widely researched. However, most of these studies have been conducted in developed countries, particularly Europe and North America, with only a handful carried out in developing countries, including the Middle East. This is problematic given the strategic and economic importance of the software industry in such places as Saudi Arabia, where Agile adoption remains in the early stages, despite heavy investment in this industry in recent years to diversify its oil-dependent economy. Therefore, this study empirically investigates the factors influencing Agile adoption by Saudi Arabian software small and medium-sized enterprises (SMEs). To this end, in-depth interviews were conducted with 12 software practitioners from three software SMEs. Our findings reveal that human factors, such as customer involvement, team capability, organisational culture and national culture, are considered the most impactful factors affecting the adoption of Agile as opposed to technical ones, such as the availability of specific tools or techniques. © 2022 Elsevier Inc.","Adoption; Agile adoption; Agile software development; Influential factors; Saudi software SMEs; software engineering"
"Identification and measurement of Requirements Technical Debt in software development: A systematic literature review","2022","Journal of Systems and Software","10.1016/j.jss.2022.111483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136734876&doi=10.1016%2fj.jss.2022.111483&partnerID=40&md5=909b9bd77d3ee232970fab3ecc4ee05c","Context: Requirements Technical Debt are related to the distance between the ideal value of the specification and the actual implementation of the system, which are consequences of strategic decisions for immediate gains, or unintended changes in context. To ensure the evolution of the software, it is necessary to to manage TD. Identification and measurement are the first two stages of the management process; however, they are poorly explored in academic research in requirements engineering. Objective: We aimed to investigating which evidence helps to strengthen the TD requirements management process, including identification and measurement. Method: We conducted a Systematic Literature Review through manual and automatic searches considering 7499 studies from 2010 to 2020, and including 66 primary studies. Results: We identified some causes related to Technical Debt requirements, existing strategies to help in the identification and measurement, and metrics to support the measurement stage. Conclusion: The studies on Requirements Technical Debt are still preliminary, especially regarding management software. Yet, however, the interpersonal aspects that prove difficult in the implementation of such activities are not sufficiently addressed. Finally, the provision of metrics to help measure technical debt is part of the contribution of this search, providing insights into the application in its requirements context. © 2022 The Author(s)","Identification; Measurement; Requirements Technical Debt; Systematic literature review; Technical debt"
"Software engineering for quantum programming: How far are we?","2022","Journal of Systems and Software","10.1016/j.jss.2022.111326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129805418&doi=10.1016%2fj.jss.2022.111326&partnerID=40&md5=438738007aade0db08ea6d89a60cc1d0","Quantum computing is no longer only a scientific interest but is rapidly becoming an industrially available technology that can potentially overcome the limits of classical computation. Over the last years, all major companies have provided frameworks and programming languages that allow developers to create their quantum applications. This shift has led to the definition of a new discipline called quantum software engineering, which is demanded to define novel methods for engineering large-scale quantum applications. While the research community is successfully embracing this call, we notice a lack of systematic investigations into the state of the practice of quantum programming. Understanding the challenges that quantum developers face is vital to precisely define the aims of quantum software engineering. Hence, in this paper, we first mine all the GITHUB repositories that make use of the most used quantum programming frameworks currently on the market and then conduct coding analysis sessions to produce a taxonomy of the purposes for which quantum technologies are used. In the second place, we conduct a survey study that involves the contributors of the considered repositories, which aims to elicit the developers’ opinions on the current adoption and challenges of quantum programming. On the one hand, the results highlight that the current adoption of quantum programming is still limited. On the other hand, there are many challenges that the software engineering community should carefully consider: these do not strictly pertain to technical concerns but also socio-technical matters. © 2022 Elsevier Inc.","Empirical software engineering; Quantum computing; Software engineering for quantum programming"
"An architecture for mission coordination of heterogeneous robots","2022","Journal of Systems and Software","10.1016/j.jss.2022.111363","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131043669&doi=10.1016%2fj.jss.2022.111363&partnerID=40&md5=4952663124d5e97c540c646c26b116e7","Context: Robots can potentially collaborate to execute a variety of tasks in the service robots domain. However, developing applications of service robots can be complex due to the high level of uncertainty and required level of autonomy. Objective: We aim at contributing an architecture for the development of applications, capable of coordinating multi-robot missions, and that promotes modifiability and seamless integration of independently developed components. Method: In this work, we introduce MissionControl: an ensemble-based architecture to coordinate missions of heterogeneous robots to autonomously form coalitions. MissionControl comprises a component model and a runtime environment. The component model specifies how the system can be extended for different robot's behaviors and environments. The runtime environment provides the processes required for coordinating the execution of missions at runtime. Results: We evaluated MissionControl in a simulated environment in the healthcare domain. We randomly generated 81 scenarios with uncertainty in the robots’ initial configurations. Then, each scenario was executed 8 times (i.e. 648 runs), where we evaluated the feasibility and efficiency of MissionControl for autonomously forming coalitions against a baseline approach that uses a random robot allocation. Statistical hypotheses testing yielded that MissionControl was able to achieve higher success rates while reducing the required time to conclude a mission, when compared to a baseline approach. We also perform an evaluation of the key quality attributes of the architecture, i.e. modifiability and integrability. Conclusions: MissionControl demonstrated itself able to coordinate multi-robot missions by autonomously assigning missions. Despite the error-prone robotic mission environment and demanding computational resources, MissionControl led to a significant increase in the success rate, while also decreasing the time required to conclude robotic missions when compared to a baseline approach. © 2022 Elsevier Inc.","Cooperative heterogeneous robots; Cyber–physical systems; Ensemble-based software architecture; Multi-robots systems; Robotic missions"
"Creativity Triggers: Extension and empirical evaluation of their effectiveness during requirements elicitation","2022","Journal of Systems and Software","10.1016/j.jss.2022.111365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131431628&doi=10.1016%2fj.jss.2022.111365&partnerID=40&md5=467a2ae0380233584dfc7104b7d4a34c","Creativity helps organization to produce novel solutions to complex and sometimes enduring problems. By breaking traditional ways of looking at a given problem and facilitating the design of alternative approaches, creativity contributes to the creation of value-adding solutions within an organization. This is true for any type of problem, including the problem of specifying the requirements for a new software or system. A number of creativity methods, techniques and tools have been proposed as a way to be more creative during Requirements Engineering (RE). They are however often demanding in terms of time, human involvement and resources, thereby reducing their attractiveness for RE practitioners and their stakeholders. Our previous research has led to the proposition of a lightweight tool to support creativity in RE; the Creativity Triggers (CTs). CTs are cards to be used during requirements elicitation to foster creativity from stakeholders and help them uncover novel features of a system-to-be. This paper builds on – and extends – our early conceptualization of CTs to produce a more comprehensive and empirically grounded proposal. Our contribution is twofold; first, we conduct a large-scale and systematic exploration of the qualities underlying the CTs. The objective is to improve the completeness of the tool in order to produce a final set of CTs. Second, we conduct a validation of CTs in different contexts and with different viewpoints to evaluate its usefulness in supporting creativity during requirements elicitation. We end-up with a set of 22 CTs that provided evidence for supporting creativity during RE. © 2022 Elsevier Inc.","Creative thinking; Creativity technique; Lightweight technique; Requirements elicitation; Requirements engineering"
"Test case prioritization using partial attention","2022","Journal of Systems and Software","10.1016/j.jss.2022.111419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132899926&doi=10.1016%2fj.jss.2022.111419&partnerID=40&md5=31f2c862f42310cbc5f0b1f7c87138ef","Test case prioritization (TCP) aims to reorder the regression test suite with a goal of increasing the fault detection rate. Various TCP techniques have been proposed based on different prioritization strategies. Among them, the greedy-based techniques are the most widely-used TCP techniques. However, existing greedy-based techniques usually reorder all candidate test cases in prioritization iterations, resulting in both efficiency and effectiveness problems. In this paper, we propose a generic partial attention mechanism, which adopts the previous priority values (i.e., the number of additionally-covered code units) to avoid considering all candidate test cases. Incorporating the mechanism with the additional-greedy strategy, we implement a novel coverage-based TCP technique based on partition ordering (OCP). OCP first groups the candidate test cases into different partitions and updates the partitions on the descending order. We conduct a comprehensive experiment on 19 versions of Java programs and 30 versions of C programs to compare the effectiveness and efficiency of OCP with six state-of-the-art TCP techniques: total-greedy, additional-greedy, lexicographical-greedy, unify-greedy, art-based, and search-based. The experimental results show that OCP achieves a better fault detection rate than the state-of-the-arts. Moreover, the time costs of OCP are found to achieve 85%–99% improvement than most state-of-the-arts. © 2022 Elsevier Inc.","Greedy algorithm; Regression testing; Software testing; Test case prioritization"
"On the benefits and problems related to using Definition of Done — A survey study","2022","Journal of Systems and Software","10.1016/j.jss.2022.111479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136454909&doi=10.1016%2fj.jss.2022.111479&partnerID=40&md5=f005562903682ceed556d706e3877374","Context: Definition of Done (DoD) is one of the fundamental concepts of Scrum. It expresses a shared view of a Scrum Team on what makes an increment of their product complete. DoDs are often defined as checklists with items being requirements towards software (e.g., quality requirements) or towards activities performed to make the increment shippable (e.g., code reviews, testing). Unfortunately, the knowledge about the usefulness of DoD is still very limited. Objective: The goal is to study what benefits using the DoD practice can bring to an agile project, what problems it may trigger, and how it is created and maintained. Methods: In the survey among members of agile software development projects, 137 practitioners from all over the globe shared their experience with us. Results: 93% of the respondents perceive DoD as at least valuable for their ventures. It helps them to make work items complete, assure product quality, and ensure the needed activities are executed. However, they indicated that every second project struggles with infeasible, incorrect, unavailable, or creeping DoD. Conclusions: It follows from the study that DoD is important but not easy to use and more empirical studies are needed to identify best practices in this area. © 2022 Elsevier Inc.","Agile; Definition of Done; DoD; Scrum; Survey"
"Spectrum-based feature localization for families of systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140380777&doi=10.1016%2fj.jss.2022.111532&partnerID=40&md5=28ce35f5e350521c1d215928e0ac56ed","In large code bases, locating the elements that implement concrete features of a system is challenging. This information is paramount for maintenance and evolution tasks, although not always explicitly available. In this work, motivated by the needs of locating features as a first step for feature-based Software Product Line adoption, we propose a solution for improving the performance of existing approaches. For this, relying on an automatic feature localization approach to locate features in single-systems, we propose approaches to deal with feature localization in the context of families of systems, e.g., variants created through opportunistic reuse such as clone-and-own. Our feature localization approaches are built on top of Spectrum-based feature localization (SBFL) techniques, supporting both dynamic feature localization (i.e., using execution traces as input) and static feature localization (i.e., relying on the structural decomposition of the variants’ implementation). Concretely, we provide (i) a characterization of different settings for dynamic SBFL in single systems, (ii) an approach to improve accuracy of dynamic SBFL for families of systems, and (iii) an approach to use SBFL as a static feature localization technique for families of systems. The proposed approaches are evaluated using the consolidated ArgoUML SPL feature localization benchmark. The results suggest that some settings of SBFL favor precision such as using the ranking metrics Wong2, Ochiai2, or Tarantula with high threshold values, while most of the ranking metrics with low thresholds favor recall. The approach to use information from variants increase the precision of dynamic SBFL while maintaining recall even with few number of variants, namely two or three. Finally, the static SBFL approach performs equally in terms of accuracy to other state-of-the-art approaches, such as Formal Concept Analysis and Interdependent Elements. © 2022 The Author(s)","Clone-and-own; Feature localization; Software product lines; Spectrum-based localization"
"Maximizing integrative learning in software development teams: A systematic review of key drivers and future research agenda","2022","Journal of Systems and Software","10.1016/j.jss.2022.111345","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130416304&doi=10.1016%2fj.jss.2022.111345&partnerID=40&md5=c487d235b22bf6296baea75906bc3671","Software development is a complex phenomenon that requires software teams to integrate diverse knowledge and expertise to provide innovative solutions. A critical success factor for software teams is their integrative learning capability which is driven by multiple factors. We systematically review and synthesize extant literature to identify critical antecedents of integrative learning in software teams and uncover knowledge gaps to inform future research. Searching multiple databases, 447 papers were identified, of which 32 were selected for the final analysis after a rigorous data extraction process. We performed open and axial coding and affinity diagramming to thematically analyze each study for antecedents associated with integrative learning. We also triangulated our results by content analyzing the studies using a software-based text analysis tool. The findings indicate five crucial drivers of integrative learning in software teams: social harmony, process agility, team design, technological maturity, and project environment. The study also highlights the value of recognizing the inter-team and intra-team contexts in team learning. Based on the study findings, we propose a model of integrative learning in software teams, discuss future research agendas, and offer practical insights and recommendations to software professionals. © 2022 Elsevier Inc.","Boundary spanning; Literature review; Social capital; Software development teams; Team design; Team integrative learning"
"On the subjectivity of emotions in software projects: How reliable are pre-labeled data sets for sentiment analysis?","2022","Journal of Systems and Software","10.1016/j.jss.2022.111448","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134891383&doi=10.1016%2fj.jss.2022.111448&partnerID=40&md5=33d54852e9571710366eec48ab8f709c","Social aspects of software projects become increasingly important for research and practice. Different approaches analyze the sentiment of a development team, ranging from simply asking the team to so-called sentiment analysis on text-based communication. These sentiment analysis tools are trained using pre-labeled data sets from different sources, including GitHub and Stack Overflow. In this paper, we investigate if the labels of the statements in the data sets coincide with the perception of potential members of a software project team. Based on an international survey, we compare the median perception of 94 participants with the pre-labeled data sets as well as every single participant's agreement with the predefined labels. Our results point to three remarkable findings: (1) Although the median values coincide with the predefined labels of the data sets in 62.5% of the cases, we observe a huge difference between the single participant's ratings and the labels; (2) there is not a single participant who totally agrees with the predefined labels; and (3) the data set whose labels are based on guidelines performs better than the ad hoc labeled data set. © 2022 Elsevier Inc.","Communication; Development team; Polarity; Sentiment analysis; Software projects"
"Automatically recognizing the semantic elements from UML class diagram images","2022","Journal of Systems and Software","10.1016/j.jss.2022.111431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134431833&doi=10.1016%2fj.jss.2022.111431&partnerID=40&md5=211336186f3b531961d4349b630f00c9","Context: Design models are essential for multiple tasks in software engineering, such as consistency checking, code generation, and design-to-code tracing. Almost all of these works need a semantically analyzable model to represent the software architecture design, e.g., a UML class diagram. Unfortunately, many design models are stored as images and embedded in text-based documentations, impeding the usage and evolution of these models. Thus, identifying the semantic elements of design models from images is important. However, there are lots of design models with different elements in diverse representations, which ask for different approaches for semantic elements extraction. Objective: In order to grasp an overview of the commonly used design model types, we conduct a survey on both open-source communities and industry. We find that design model diagrams are usually embedded in documents as pictures (73.72%), and UML class diagrams are the most used type (55.43%). Considering that there are limited studies on automatically recognizing the semantic elements from class diagram images, we propose an approach, which we call ReSECDI. Method: ReSECDI includes our customized design for extracting UML class diagram elements based on image processing technologies. We design a rectangle clustering method for class recognition, to address the challenge that the presentation of classes may vary due to the UML constraints and tools’ styles. We design a polygonal line merging method and double-recognition-approximation method for relationship recognition to deal with the impact of low resolution on the detection. Results: We evaluate the applicability of ReSECDI on 30 images drawn by three popular UML tools and 50 diagrams collected from the open-source communities, and get promising performances. Conclusion: ReSECDI can recognize all types of semantic elements commonly used. It has well applicability and can be used to process the images drawn by the mainstream tools and stored in different resolutions. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 The Author(s)","Class diagram; Inconsistency check; UML image recognition"
"Technical debts and faults in open-source quantum software systems: An empirical study","2022","Journal of Systems and Software","10.1016/j.jss.2022.111458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135699101&doi=10.1016%2fj.jss.2022.111458&partnerID=40&md5=9290b68aebc2338832c078b171b8b2cc","Quantum computing is a rapidly growing field attracting the interest of both researchers and software developers. Supported by its numerous open-source tools, developers can now build, test, or run their quantum algorithms. Although the maintenance practices for traditional software systems have been extensively studied, the maintenance of quantum software is still a new field of study but a critical part to ensure the quality of a whole quantum computing system. In this work, we set out to investigate the distribution and evolution of technical debts in quantum software and their relationship with fault occurrences. Understanding these problems could guide future quantum development and provide maintenance recommendations for the key areas where quantum software developers and researchers should pay more attention. In this paper, we empirically studied 118 open-source quantum projects, which were selected from GitHub. The projects are categorized into 10 categories. We found that the studied quantum software suffers from the issues of code convention violation, error-handling, and code design. We also observed a statistically significant correlation between code design, redundant code or code convention, and the occurrences of faults in quantum software. © 2022 Elsevier Inc.","Quantum computing; Software bugs; Software maintenance; Software reliability; Technical debts"
"Mutation testing of PL/SQL programs","2022","Journal of Systems and Software","10.1016/j.jss.2022.111399","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132689786&doi=10.1016%2fj.jss.2022.111399&partnerID=40&md5=17d087b3ec4ffda997ff18af7f19f4be","Mutation testing is a prominent technique for evaluating the effectiveness of a test suite. Existing tools developed for supporting this technique are applicable for mainstream programming languages like C and Java. Mutation testing tools and mutation operators used by these tools are inherently language-specific. Moreover, there is a lack of industrial case studies for evaluating mutation testing tools and techniques in practice. In this article, we introduce muPLSQL, a tool for applying mutation testing on PL/SQL programs, facilitating automation for both mutant generation and test execution. We utilized existing mutation operators that are applicable for PL/SQL. In addition, we introduced some operators specifically for this language. We conducted an industrial case study for evaluating the applicability and usefulness of our tool and mutation testing in general. We applied mutation testing on a business support software system. muPLSQL generated a total of 5,939 mutants. The number of live mutants was 680. Manual inspection of live mutants led to improvements of the existing test suite. In addition, we found 8 faults in source code during the inspection process. Test execution against the mutants required around 40 h. The overall effort was almost one person month. © 2022 Elsevier Inc.","Industrial case study; Mutation analysis; Mutation testing; PL/SQL; Software testing"
"Missing standard features compared with similar apps? A feature recommendation method based on the knowledge from user interface","2022","Journal of Systems and Software","10.1016/j.jss.2022.111435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134876724&doi=10.1016%2fj.jss.2022.111435&partnerID=40&md5=25d4ef461de1727ae3cc262f2448e44c","To attract and retain users, deciding what features should be added in the next release of apps becomes very crucial. Different from traditional software, there are rich data resources in app markets to perform market-wide analysis. Considering that capturing key features that apps lack compared with its similar products and making up for them can be conducive to enhance the competitiveness, we propose a method to establish the feature relationships from the level of UI pages and recommend missing key features for the pages of apps based on these relationships. Firstly, we utilize the UI testing tool to collect UI pages for apps in the repository, and give the method to gain the feature information in them. Then, we identify the products similar to the analyzed app based on topic modeling technique. Finally, we establish the relationships between features by analyzing UI pages gained for the analyzed app as well as its similar products, and identify suitable features recommended to UI pages of the analyzed app based on these relationships. The experiment based on Google Play shows that our method can recommend features for apps from the level of UI pages effectively. © 2022 Elsevier Inc.","Android apps; Feature recommendation; User interface"
"Automated Web application testing driven by pre-recorded test cases","2022","Journal of Systems and Software","10.1016/j.jss.2022.111441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134892412&doi=10.1016%2fj.jss.2022.111441&partnerID=40&md5=067a56a6825a015a5ccea196b2901794","There are fully automated approaches proposed for Web application testing. These approaches mainly rely on tools that explore an application by crawling it. The crawling process results in a state transition model, which is used for generating test cases. Although these approaches are fully automated, they consume too much time and they usually require manual configuration. This is due to the lack of insight and domain knowledge of crawling tools regarding the application under test. We propose a semi-automated approach instead. We introduce a tool that takes a set of recorded event sequences as input. These sequences can be captured during exploratory tests. They are replayed as pre-recorded test cases. They are also exploited for steering the crawling and test case generation process. We performed a case study with 5 Web applications. These applications were randomly tested with state-of-the-art tools. Our approach can reduce the crawling time by hours, while compromising the coverage achieved by 0.2% to 7.43%. In addition, our tool does not require manual configuration before crawling. The input for the tool was created within 15 min of exploratory testing. © 2022 Elsevier Inc.","Exploratory testing; Model-based testing; Test automation; Test case generation; Web application testing"
"EdgeWorkflow: One click to test and deploy your workflow applications to the edge","2022","Journal of Systems and Software","10.1016/j.jss.2022.111456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135591312&doi=10.1016%2fj.jss.2022.111456&partnerID=40&md5=79481480483b1fd2c77349145db6076c","In recent years, edge computing has become the ideal computing paradigm for various smart systems, such as smart logistics, smart health and smart transportation. This is due to its advantages including fast response times, energy efficiency and cost effectiveness over conventional cloud computing platforms. However, running complex computational scientific workflow tasks is still a very challenging issue at the edge, due to its typical three-layered computing environment consisting of an end device layer, an edge server layer, and a cloud server layer. A large number of recent studies have proposed different solutions for optimizing such computing resource management problems in an edge computing environment. However, since evaluation of most such studies is conducted through simulation, the effectiveness cannot be guaranteed in a real world environment. Therefore, to advance research on efficient execution and deployment problems for real world workflow applications using edge computing, an open-source edge workflow management system with comprehensive empirical evaluation capabilities is urgently required. This paper presents the first edge workflow system (named EdgeWorkflow) that is able to deploy user-created workflow applications to a real-world edge computing environment with “one-click” after optimizing the configuration with the simulation tool. With the aid of EdgeWorkflow, the user can automate the generation of specific edge computing environments, easily model and generate executable workflow applications with a visual modelling tool, effectively select various resource management methods included in the systems or apply their own resource management and task scheduling algorithms, efficiently monitor the statuses of computational tasks and obtain comprehensive reports on the execution results (such as those regarding time, cost and energy). We use an edge computing-based unmanned aerial vehicle (UAV) last-mile delivery system as a real-world case study, and a number of representative scientific workflows are employed for our experiments. Our experimental results show that EdgeWorkflow can effectively evaluate the performance of different resource management and workflow task scheduling algorithms and efficiently deploy and execute user-defined scientific workflow applications to user-specified edge computing environments. © 2022 The Author(s)","Edge computing; One-click deployment; Resource management; Workflow system"
"Using deep temporal convolutional networks to just-in-time forecast technical debt principal","2022","Journal of Systems and Software","10.1016/j.jss.2022.111481","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136614376&doi=10.1016%2fj.jss.2022.111481&partnerID=40&md5=6c676d95e43f78692316b4691625146c","Technical debt is a widely used metaphor to summarize all the consequences of poorly written code. Managing technical debt is important for software developers to allow adequate planning for software maintenance and improvement activities, such as refactoring and preventing system degradation. Several studies in the literature investigate the identification of technical debt and its consequences. This work aims to explore a deep learning approach to just-in-time predict the impact on technical debt when changes are performed on the source code. In this way the developer can work better, trying to improve the quality of the code that is being modified. Knowing what the TD trend will be in just-in-time source code with the change made is the key to avoiding a project taking a long time to remediate or improve. The model exploits the knowledge of quality and ad-hoc process metrics evolution over time. To validate the approach, a large dataset, including metrics evaluated from commits of ten Java software projects, was built. The results obtained show the effectiveness of the proposed approach in predicting the Technical Debt accumulation within the source code. © 2022 Elsevier Inc.","Deep learning; Process metrics; Software quality metrics; Technical debt; Technical debt forecasting; Temporal convolutional network"
"Evaluating the layout quality of UML class diagrams using machine learning","2022","Journal of Systems and Software","10.1016/j.jss.2022.111413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133496498&doi=10.1016%2fj.jss.2022.111413&partnerID=40&md5=a82062bef3d2f28092e193c44f621b0a","UML is the de facto standard notation for graphically representing software. UML diagrams are used in the analysis, construction, and maintenance of software systems. Mostly, UML diagrams capture an abstract view of a (piece of a) software system. A key purpose of UML diagrams is to share knowledge about the system among developers. The quality of the layout of UML diagrams plays a crucial role in their comprehension. In this paper, we present an automated method for evaluating the layout quality of UML class diagrams. We use machine learning based on features extracted from the class diagram images using image processing. Such an automated evaluator has several uses: (1) From an industrial perspective, this tool could be used for automated quality assurance for class diagrams (e.g., as part of a quality monitor integrated into a DevOps toolchain). For example, automated feedback can be generated once a UML diagram is checked in the project repository. (2) In an educational setting, the evaluator can grade the layout aspect of student assignments in courses on software modeling, analysis, and design. (3) In the field of algorithm design for graph layouts, our evaluator can assess the layouts generated by such algorithms. In this way, this evaluator opens up the road for using machine learning to learn good layouting algorithms. Approach.: We use machine learning techniques to build (linear) regression models based on features extracted from the class diagram images using image processing. As ground truth, we use a dataset of 600+ UML Class Diagrams for which experts manually label the quality of the layout. Contributions.: This paper makes the following contributions: (1) We show the feasibility of the automatic evaluation of the layout quality of UML class diagrams. (2) We analyze which features of UML class diagrams are most strongly related to the quality of their layout. (3) We evaluate the performance of our layout evaluator. (4) We offer a dataset of labeled UML class diagrams. In this dataset, we supply for every diagram the following information: (a) a manually established ground truth of the quality of the layout, (b) an automatically established value for the layout-quality of the diagram (produced by our classifier), and (c) the values of key features of the layout of the diagram (obtained by image processing). This dataset can be used for replication of our study and others to build on and improve on this work. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 The Author(s)","Machine learning; Quality of layout; Quality of UML class diagrams"
"iFogSim2: An extended iFogSim simulator for mobility, clustering, and microservice management in edge and fog computing environments","2022","Journal of Systems and Software","10.1016/j.jss.2022.111351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130094579&doi=10.1016%2fj.jss.2022.111351&partnerID=40&md5=b650072584d3ec15cdfca92a7638dd74","Internet of Things (IoT) has already proven to be the building block for next-generation Cyber–Physical Systems (CPSs). The considerable amount of data generated by the IoT devices needs latency-sensitive processing, which is not feasible by deploying the respective applications in remote Cloud datacentres. Edge/Fog computing, a promising extension of Cloud at the IoT-proximate network, can meet such requirements for smart CPSs. However, the structural and operational differences of Edge/Fog infrastructure resist employing Cloud-based service regulations directly to these environments. As a result, many research works have been recently conducted, focusing on efficient application and resource management in Edge/Fog computing environments. Scalable Edge/Fog infrastructure is a must to validate these policies, which is also challenging to accommodate in the real-world due to high cost and implementation time. Considering simulation as a key to this constraint, various software have been developed that can imitate the physical behavior of Edge/Fog computing environments. Nevertheless, the existing simulators often fail to support advanced service management features because of their monolithic architecture, lack of actual dataset, and limited scope for a periodic update. To overcome these issues, we have developed modular simulation models for service migration, dynamic distributed cluster formation, and microservice orchestration for Edge/Fog computing based on real datasets and extended the basic components of iFogSim, a widely used Edge/Fog computing simulator for their ease of adoption as iFogSim2. The performance of iFogSim2 and its built-in service management policies are evaluated using three use case scenarios and compared with the contemporary simulators and benchmark policies under different settings. Results indicate that our simulator consumes less memory and minimizes simulation time by an average of 28% when compared to other simulators. © 2022 Elsevier Inc.","Clustering; Edge/Fog computing; Internet of Things; Microservices; Mobility; Simulation"
"TitleGen-FL: Quality prediction-based filter for automated issue title generation","2023","Journal of Systems and Software","10.1016/j.jss.2022.111513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138760257&doi=10.1016%2fj.jss.2022.111513&partnerID=40&md5=fa5bea9cf3212638a0145195644a5067","To automatically generate the issue title, researchers formulated the problem into a one-sentence summarization problem and then proposed an effective method iTAPE. However, after analyzing the quality of the titles generated by the method iTAPE, which is measured by ROUGE-L, we find the ROUGE-L scores of only 42.7% of titles can exceed 0.3. This means the quality of the generated titles is not satisfactory and can limit the practicability of the method iTAPE. Therefore, we propose a quality prediction-based filter TitleGen-FL, which can predict whether the method iTAPE can generate a high-quality title after analyzing the issue body. To achieve this goal, TitleGen-FL includes the deep learning-based (DL) module and the information retrieval-based (IR) module. If either of these two modules predict that the high-quality title cannot be generated by iTAPE, TitleGen-FL can automatically filter this issue and return a warning message. To evaluate the effectiveness of our proposed filter, we select the benchmark dataset gathered from real-world open-source projects as our experimental subject. Both automatic evaluation and human study show that our proposed filter TitleGen-FL can effectively filter the issues, which cannot generate high-quality titles by iTAPE. © 2022 Elsevier Inc.","Deep learning; Filter; Information retrieval; Issue title generation; Quality prediction"
"The PREVENT-Model: Human and Organizational Factors Fostering Engineering of Safe and Secure Robotic Systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141494843&doi=10.1016%2fj.jss.2022.111548&partnerID=40&md5=d4b7ade5ae2682d56169a23236536f52","The rise of robotic systems demands increasingly elaborated safety and security measures in order to prevent damage to property as well as human and economic harm. While technical expertise of engineers is a prerequisite and international standards constitute an overall frame for safety and security by design, human and organizational factors also shape the development of responsible technology. The current study derives an overarching model covering both human and organizational factors facilitating the development of safe and secure robotic systems. In a qualitative interview study, we conducted three focus groups with experts from robotics, software engineering, and related domains. The interviews were analyzed by three independent coders using a qualitative content analytical approach. The resulting integrative PREVENT-Model comprises 17 individual factors and 13 organizational factors that enhance safety and security by design. We embed each factor into existing research and derive implications for practitioners and possible courses of action. The PREVENT-Model serves as a roadmap to develop tailored measures. Based on this framework, practitioners can improve personnel development programs, organizational structures, working environments, and other aspects crucial for the development of safe and secure robotic systems. © 2022 The Author(s)","Human factors; Human resource management; Robotic systems; Safety; Security; Software engineering"
"The language mutation problem: Leveraging language product lines for mutation testing of interpreters","2023","Journal of Systems and Software","10.1016/j.jss.2022.111533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140082147&doi=10.1016%2fj.jss.2022.111533&partnerID=40&md5=1474b93f73b90876a24cd4937aee6cb0","Compilers translate programs from a high level of abstraction into a low level representation that can be understood and executed by the computer; interpreters directly execute instructions from source code to convey their semantics. Undoubtedly, the correctness of both compilers and interpreters is fundamental to reliably execute the semantics of any software developed by means of high-level languages. Testing is one of the most important methods to detect errors in any software, including compilers and interpreters. Among testing methods, mutation testing is an empirically effective technique often used to evaluate and improve the quality of test suites. However, mutation testing imposes severe demands in computing resources due to the large number of mutants that need to be generated, compiled and executed. In this work, we introduce a mutation approach for programming languages that mitigates this problem by leveraging the properties of language product lines, language workbenches and separate compilations. In this approach, the base language is taken as a black-box and mutated by means of mutation operators performed at language feature level to create a family of mutants of the base language. Each variant of the mutant family is created at runtime, without any access to the source code and without performing any additional compilation. We report results from a preliminary case study in which mutants of an ECMAScript interpreter are tested against the Sputnik conformance test suite for the ECMA-262 specification. The experimental data indicates that this approach can be used to create generally non-trivial mutants. © 2022 Elsevier Inc.","Language product lines; Language testing; Mutation testing"
"Using mutual information to test from Finite State Machines: Test suite generation","2022","Journal of Systems and Software","10.1016/j.jss.2022.111391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133760992&doi=10.1016%2fj.jss.2022.111391&partnerID=40&md5=8ed1da22aed5051fad6a4d29c13b3f11","Mutual Information is an information theoretic measure designed to quantify the amount of similarity between two random variables ranging over two sets. In recent work we have use it as a base for a measure, called Biased Mutual Information, to guide the selection of a test suite among different possibilities. In this paper, we adapt this concept and show how it can be used to address the problem of generating a test suite with high fault finding capability, in a black-box scenario and following a maximise diversity approach. Additionally, we present a new Grammar-Guided Genetic Programming Algorithm that uses Biased Mutual Information to guide the generation of such test suites. Our experimental results clearly show the potential value of our measure when used to generate test suites. Moreover, they show that our measure is better in guiding test generation than current state-of-the-art measures, like Test Set Diameter (TSDm) measures. Additionally, we compared our proposal with classical completeness-oriented methods, like the H-Method and the Transition Tour method, and found that our proposal produces smaller test suites with high enough fault finding capability. Therefore, our methodology is preferable in an scenario where a compromise is necessary between fault detection and execution time. © 2022 Elsevier Inc.","Finite State Machines; Formal approaches to testing; Information Theory; Mutual information"
"Revisiting the practices and pains of microservice architecture in reality: An industrial inquiry","2023","Journal of Systems and Software","10.1016/j.jss.2022.111521","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140322733&doi=10.1016%2fj.jss.2022.111521&partnerID=40&md5=d44437c5efc10654e95dff0f778bd73d","Background: Seeking an appropriate architecture for the design of software is always a challenge. Although microservices are claimed to be a lightweight architecture style that can improve current practices with several characteristics, many practices are based on different circumstances and reflect variant effects. Empirical inquiry gives us a systematic insight into industrial practices and sufferings on microservices. Objective: This study is to investigate the gaps between ideal visions and real industrial practices in microservices and what expenses microservices bring to industrial practitioners. Method: We carried out a series of industrial interviews with practitioners from 20 software companies. The collected data were then codified using qualitative methods. Results: Eight pairs of common practices and pains of microservices in industry were obtained after synthesizing the rich and detailed data collected. Five aspects that require careful decisions were extracted to help practitioners balance the possible benefits and pains of MSA. Five research directions that need further exploration were identified based on the pains associated with MSA. Conclusion: While the benefits of microservices are confirmed from the point of view of practitioners, decisions should be carefully made and the possible problems identified must be addressed with additional expense from experience. Furthermore, some of the topics and pains outlined, e.g., systematic evaluation and assessment, organizational transformation, decomposition, distributed monitoring, and bug localization, may inspire researchers to conduct further research. © 2022 Elsevier Inc.","Empirical study; Interview; Microservices; Software architecture"
"AMon: A domain-specific language and framework for adaptive monitoring of Cyber–Physical Systems","2023","Journal of Systems and Software","10.1016/j.jss.2022.111507","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139235846&doi=10.1016%2fj.jss.2022.111507&partnerID=40&md5=10103ed674f98cda7fceb723e38ab670","Cyber–Physical Systems (CPS) are increasingly used in safety–critical scenarios where ensuring their correct behavior at runtime becomes a crucial task. Therefore, the behavior of the CPS needs to be monitored at runtime so that violations of requirements can be detected. With the inception of edge devices that facilitate runtime analysis at the edge and the increasingly diverse environments that CPS operate in, flexible monitoring approaches are needed that consider the data that needs to be monitored and the analyses performed on that data. In this paper, we propose AMon, a flexible adaptive monitoring framework that supports the specification and validation of monitoring adaptation rules, using a domain-specific language. Based on these rules, AMon automatically generates code for direct deployment onto devices. We evaluated AMon by applying it to TurtleBot Robots and a fleet of Unmanned Aerial Vehicles. Furthermore, we conducted a user study assessing the understandability and ease of use of our language. Results show that creating multiple adaptation rules with our DSL is feasible with minimal effort, and that adaptive monitoring can reduce the amount of runtime data transmitted from the edge device according to the current state of the system and its monitoring needs. © 2022 The Author(s)","Adaptive monitoring; Cyber–Physical Systems; Domain-specific language; Runtime monitoring"
"Mapping the structure and evolution of software testing research over the past three decades","2023","Journal of Systems and Software","10.1016/j.jss.2022.111518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139315300&doi=10.1016%2fj.jss.2022.111518&partnerID=40&md5=33d44ca39355c546c4dbfbec699af11f","Background: The field of software testing is growing and rapidly-evolving. Aims: Based on keywords assigned to publications, we seek to identify predominant research topics and understand how they are connected and have evolved. Methods: We apply co-word analysis to map the topology of testing research as a network where author-assigned keywords are connected by edges indicating co-occurrence in publications. Keywords are clustered based on edge density and frequency of connection. We examine the most popular keywords, summarize clusters into high-level research topics examine how topics connect, and examine how the field is changing. Results: Testing research can be divided into 16 high-level topics and 18 subtopics. Creation guidance, automated test generation, evolution and maintenance, and test oracles have particularly strong connections to other topics, highlighting their multidisciplinary nature. Emerging keywords relate to web and mobile apps, machine learning, energy consumption, automated program repair and test generation, while emerging connections have formed between web apps, test oracles, and machine learning with many topics. Random and requirements-based testing show potential decline. Conclusions: Our observations, advice, and map data offer a deeper understanding of the field and inspiration regarding challenges and connections to explore. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 The Author(s)","Bibliometrics; Co-word analysis; Software testing"
"Conformance assessment of Architectural Design Decisions on API endpoint designs derived from domain models","2022","Journal of Systems and Software","10.1016/j.jss.2022.111433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135305434&doi=10.1016%2fj.jss.2022.111433&partnerID=40&md5=a6323119e265eb87f21ad84deca3195a","Context: Domain-driven design (DDD) is commonly used to design microservices. A crucial aspect of microservice design is API design, which includes the design of API endpoints. Objective: Our objective is to automate the assessment of conformance to Architectural Design Decisions (ADDs) on the interrelation of DDD and APIs. In particular, we studied link mapping, API operation design, and resource segregation as API endpoint design issues that are linked to domain model design. We particularly aim to address conformance checking in the context of frequent release practices, as frequent manual conformance checking is difficult or infeasible. Methods: We suggest a new approach for the automated assessment of conformance to ADD options. The approach suggests automated detectors to detect ADD options selected in a given API endpoint design, as well as an assessment scoring scheme based on empirical results. For the evaluation of our approach, we first manually created a ground truth for 12 cases in a multi-case study, and then compared the results of our automated detectors to the ground truth for each of those cases. Results: With our approach, all ADD options in our multi-case study possibly can be automatically detected. Without further improvements, our approach identifies 83% of the decision points in the multi-case study correctly. A statistical analysis of our data shows only a negligible effect size for differences to the ground truth. Conclusion: Our new approach provides a pragmatic method for automated detection of conformance to ADDs on the interrelation of DDD and APIs. The approach can support the continuous analysis of API endpoint designs. © 2022 The Authors","API design; Architecture conformance assessment; Domain driven design; Microservice architecture"
"When traceability goes awry: An industrial experience report","2022","Journal of Systems and Software","10.1016/j.jss.2022.111389","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132339754&doi=10.1016%2fj.jss.2022.111389&partnerID=40&md5=9b7fe76251df4e2e55dd551733c794e8","The concept of traceability between artifacts is considered an enabler for software project success. This concept has received plenty of attention from the research community and is by many perceived to always be available in an industrial setting. In this industry-academia collaborative project, a team of researchers, supported by testing practitioners from a large telecommunication company, sought to investigate the partner company's issues related to software quality. However, it was soon identified that the fundamental traceability links between requirements and test cases were missing. This lack of traceability impeded the implementation of a solution to help the company deal with its quality issues. In this experience report, we discuss lessons learned about the practical value of creating and maintaining traceability links in complex industrial settings and provide a cautionary tale for researchers. © 2022 The Author(s)","Industry-academia collaboration; Software quality; Traceability"
"Impact of individualism and collectivism cultural profiles on the behaviour of software developers: A study of stack overflow","2022","Journal of Systems and Software","10.1016/j.jss.2022.111427","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133428899&doi=10.1016%2fj.jss.2022.111427&partnerID=40&md5=673e235ba0dc1e9a8309eff24961b5ed","Literature is scarce on culture and its impact on the behavioural patterns within software development communities. However, globalisation in software development has intensified the need for software development teams to navigate culture issues to ensure the successful implementation of projects. Therefore, the current study examines whether the effects of culture on software developers conform to Hofstede's individualism cultural dimension. Individualism is studied because of its established negative impacts on teamwork, which is central to software development. Data comprised artefacts from Stack Overflow, a popular online programming community. Developers were from the United States (US), China, and Russia, three countries that differ in terms of their individualistic or collectivistic cultures. Data mining techniques, as well as statistical, linguistic, and content analysis were used to compare the orientation, attitudes, interaction, and knowledge sharing patterns of the three groups of developers. Differences revealed among the three groups were consistent with their cultural backgrounds. US developers, who are from a more individualistic culture, had higher average reputations, used the pronoun “I” more frequently, and were more task-focused. Conversely, Chinese developers, who are from a more collectivistic culture, used the pronouns “we” and “you” more frequently, and were more likely to engage in information exchange. Russian developers had been using Stack Overflow the longest and were the most reflective. The cultural patterns identified in this study have implications for enhancing in-group interactions and team behaviour management during software development, especially when global teams assemble. © 2022 Elsevier Inc.","Collectivism; Cultural dimension; Individualism; Software development communities"
"Hierarchical semantic-aware neural code representation","2022","Journal of Systems and Software","10.1016/j.jss.2022.111355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130588010&doi=10.1016%2fj.jss.2022.111355&partnerID=40&md5=61f8d917a5f3c0cee1bbf8809d24a295","Code representation is a fundamental problem in many software engineering tasks. Despite the effort made by many researchers, it is still hard for existing methods to fully extract syntactic, structural and sequential features of source code, which form the hierarchical semantics of the program and are necessary to achieve a deeper code understanding. To alleviate this difficulty, we propose a new supervised approach based on the novel use of Tree-LSTM to incorporate the sequential and the global semantic features of programs explicitly into the representation model. Unlike previous techniques, our proposed model can not only learn low-level syntactic information within each statement but also the high-level semantic information between statements over the constructed semantic graph. Besides, considering that the sequential semantics is also critical for developers to understand the dependency path and data flow transmission, we propose a DFS-based method to generate the topological order of statements being processed, and then feed them as well as their in-neighboring information and syntactic embeddings into the proposed model to learn richer statement-level semantic features. Extensive experiments on multiple program comprehension tasks, e.g., code clone detection, demonstrate that our method achieves promising performance compared with other existing baselines. © 2022 Elsevier Inc.","Clone detection; Code representation; Deep learning; Graph-LSTM; Hierarchical semantics; Program classification; Vulnerability detection"
"GT-SimNet: Improving code automatic summarization via multi-modal similarity networks","2022","Journal of Systems and Software","10.1016/j.jss.2022.111495","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138104120&doi=10.1016%2fj.jss.2022.111495&partnerID=40&md5=0e59fb86763acb5e938ab65dbf64742f","Code summarization aims to generate high-quality functional summaries of code snippets to improve the efficiency of program development and maintenance. It is a pressing challenge for code summarization models to capture more comprehensive code knowledge by integrating the feature correlations between the semantics and syntax of the code. In this paper, we propose a multi-modal similarity network based code summarization method: GT-SimNet. It proposes a novel code semantic modelling method based on a local application programming interface (API) dependency graph (Local-ADG), which exhibits an excellent ability to mask irrelevant semantics outside the current code snippet. For code feature fusion, GT-SimNet uses the SimNet network to calculate the correlation coefficients between Local-ADG and abstract syntax tree (AST) nodes and performs fusion under the influence of the correlation coefficients. Finally, it completes the prediction of the target summary by the generator. We conduct extensive experiments to evaluate the performance of GT-SimNet on two language datasets (Java and Solidity). The results show that GT-SimNet achieved BLEU scores of 38.73% and 41.36% on the two datasets, 1.47%∼2.68% higher than the best existing baseline. Importantly, GT-SimNet reduces the BLEU scores by 7.28% after removing Local-ADG. This indicates that Local-ADG is effective for the semantic representation of the code. © 2022 Elsevier Inc.","AST; Code summarization; GCN; Local API dependency graph"
"Transformation-based model checking temporal trust in multi-agent systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111383","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132222421&doi=10.1016%2fj.jss.2022.111383&partnerID=40&md5=67b6df80ee11fc5853146cfc7b231a3a","Several formal trust frameworks have been introduced in the area of Multi-Agent Systems (MASs). However, the problem of model checking trust logics is still a challenging research topic. In this paper, we address this challenge by proposing a formal and fully automatic model checking technique for two temporal logics of trust. We start by reviewing TCTL, a Computation Tree Logic of Trust, which has been recently proposed. We also introduce TCTLC for conditional trust. Then, we introduce sound and complete transformation-based algorithms that automatically transform the problem of model checking TCTL and TCTLC into the problem of model checking CTL. Moreover, we prove that although TCTL and TCTLC extend CTL, their model checking algorithms still have the same time complexity for explicit models, which is P-complete with regard to the size of the model and length of the formula, and the same space complexity for concurrent programs, which is PSPACE-complete with regard to the size of the components of these programs. Finally, experiments conducted on a standard industrial case study of auto-insurance claim processing demonstrate the efficiency and scalability of our approach in verifying TCTL and TCTLC formulae. © 2022 Elsevier Inc.","Model checking; Multi_Agent systems; Temporal logic; Trust"
"Assessing industrial end-user programming of robotic production cells: A controlled experiment","2023","Journal of Systems and Software","10.1016/j.jss.2022.111547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145775843&doi=10.1016%2fj.jss.2022.111547&partnerID=40&md5=cfd981139b33677f726ade49b3ffdf9e","Adapting the behavior of robots and their interaction with other machines on the shop floor is typically accomplished by non-programmers. Often these non-programmers use visual languages to specify the robot's and/or machine's control logic. While visual languages are explored as a means to enable novices to program, there is little understanding of what problems novices face when tasked with realistic adaptation programming tasks on the shop floor. In this paper, we report the results of a controlled experiment where domain experts in the injection molding industry inspected and changed realistic programs involving a robot, injection molding machine, and additional external machines. We found that participants were comparably quick to understand the program behavior with a familiar sequential function chart-based language and a Blockly-based language used for the first time. We also observed that these non-programmers had difficulty in multiple aspects independent of language due to the interweaving of physical and software-centric interaction between robot and machine. We conclude that assistance needs to go beyond optimizing available language elements to include suggesting relevant programming elements and their sequence. © 2022 The Author(s)","Block-based programming languages; End-user programming; Manufacturing automation; Robot programming; Sequential function charts"
"Work-from-home is here to stay: Call for flexibility in post-pandemic work policies","2023","Journal of Systems and Software","10.1016/j.jss.2022.111552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142142102&doi=10.1016%2fj.jss.2022.111552&partnerID=40&md5=fea3ecd7454e461c1b34dcf6dcef39ee","In early 2020, the Covid-19 pandemic forced employees in tech companies worldwide to abruptly transition from working in offices to working from their homes. During two years of predominantly working from home, employees and managers alike formed expectations about what post-pandemic working life should look like. Many companies are experimenting with new work policies that balance employee- and manager expectations regarding where, when and how work should be done in the future. In this article, we gather experiences of the new trend of remote working based on the synthesis of 22 company-internal surveys of employee preferences for WFH, and 26 post-pandemic work policies from 17 companies and their sites, covering 12 countries in total. Our results are threefold. First, through the new work policies, all companies formally give employees more flexibility regarding working time and location. Second, there is a great variation in how much flexibility the companies are willing to yield to the employees. The paper details the different formulations that companies adopted to document the extent of permitted WFH, exceptions, relocation permits and the authorisation procedures. Third, we document a change in the psychological contract between employees and managers, where the option of working from home is converted from an exclusive perk that managers could choose to give to the few, to a core privilege that all employees feel they are entitled to. Finally, there are indications that as the companies learn and solicit feedback regarding the efficiency of the chosen strategies, we will see further developments and changes in the work policies concerning how much flexibility to work whenever and from wherever they grant. Through these findings, the paper contributes to a growing literature about the new trends emerging from the pandemic in tech companies and spells out practical implications onwards. © 2022 The Author(s)","Hybrid workplace; Post-pandemic; Remote work; Survey; Work from anywhere; Work from home"
"A Cross-Domain Systematic Mapping Study on Software Engineering for Digital Twins","2022","Journal of Systems and Software","10.1016/j.jss.2022.111361","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135797712&doi=10.1016%2fj.jss.2022.111361&partnerID=40&md5=12dd4cfd94f899b6c7803df707575659","Digital Twins are currently investigated as the technological backbone for providing an enhanced understanding and management of existing systems as well as for designing new systems in various domains, e.g., ranging from single manufacturing components such as sensors to large-scale systems such as smart cities. Given the diverse application domains of Digital Twins, it is not surprising that the characterization of the term Digital Twin, as well as the needs for developing and operating Digital Twins are multi-faceted. Providing a better understanding what the commonalities and differences of Digital Twins in different contexts are, may allow to build reusable support for developing, running, and managing Digital Twins by providing dedicated concepts, techniques, and tool support. In this paper, we aim to uncover the nature of Digital Twins based on a systematic mapping study which is not limited to a particular application domain or technological space. We systematically retrieved a set of 1471 unique publications of which 356 were selected for further investigation. In particular, we analyzed the types of research and contributions made for Digital Twins, the expected properties Digital Twins have to fulfill, how Digital Twins are realized and operated, as well as how Digital Twins are finally evaluated. Based on this analysis, we also contribute a novel feature model for Digital Twins from a software engineering perspective as well as several observations to further guide future software engineering research in this area. © 2022 Elsevier Inc.","Digital Twins; Industry 4.0; Manufacturing; Software Engineering"
"From forced Working-From-Home to voluntary working-from-anywhere: Two revolutions in telework","2023","Journal of Systems and Software","10.1016/j.jss.2022.111509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139327922&doi=10.1016%2fj.jss.2022.111509&partnerID=40&md5=6136e27e1581e91542792a83de2437c7","The COVID-19 outbreak has admittedly caused interruptions to production, transportation, and mobility, therefore, having a significant impact on the global supply and demand chain's well-functioning. But what happened to companies developing digital services, such as software? How has the enforced Working-From-Home (WFH) mode impacted their ability to deliver software, if at all? This article shares our findings from monitoring the WFH during 2020 in an international software company with engineers located in Sweden, the USA, and the UK. We analyzed different aspects of productivity, such as developer job satisfaction and well-being, activity, communication and collaboration, efficiency and flow based on the archives of commit data, calendar invites, Slack communication, the internal reports of WFH experiences, and 30 interviews carried out in April/May and September 2020. We add more objective evidence to the existing COVID-19 studies the vast majority of which are based on self-reported productivity from the early months of the pandemic. We find that engineers continue committing code and carrying out their daily duties, as their routines adjust to “the new norm”. Our key message is that software engineers can work from home and quickly adjust their tactical approaches to the changes of unprecedented scale. Further, WFH has its benefits, including better work-life balance, improved flow, and improved quality of distributed meetings and events. Yet, WFH is not challenge free: not everybody feels equally productive working from home, work hours for many increased, while physical activity, socialization, pairing and opportunities to connect to unfamiliar colleagues decreased. Information sharing and meeting patterns also changed. Finally, experiences gained during the pandemic will have a lasting impact on the future of the workplace. The results of an internal company-wide survey suggest that only 9% of engineers will return to work in the office full time. Our article concludes with the InterSoft's strategy for work from anywhere (WFX), and a list of useful adjustments for a better WFH. © 2022 The Author(s)","Case study; COVID-19; Empirical study; Software engineering; Telework; WFH; Working from home"
"On the use of deep learning in software defect prediction","2023","Journal of Systems and Software","10.1016/j.jss.2022.111537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140965251&doi=10.1016%2fj.jss.2022.111537&partnerID=40&md5=abc6530954079e153d281e71264be476","Context: Automated software defect prediction (SDP) methods are increasingly applied, often with the use of machine learning (ML) techniques. Yet, the existing ML-based approaches require manually extracted features, which are cumbersome, time consuming and hardly capture the semantic information reported in bug reporting tools. Deep learning (DL) techniques provide practitioners with the opportunities to automatically extract and learn from more complex and high-dimensional data. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of the utilization of DL algorithms for SDP in the literature. Method: We systematically selected a pool of 102 peer-reviewed studies and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: Main highlights include: (1) most studies applied supervised DL; (2) two third of the studies used metrics as an input to DL algorithms; (3) Convolutional Neural Network is the most frequently used DL algorithm. Conclusion: Based on our findings, we propose to (1) develop more comprehensive DL approaches that automatically capture the needed features; (2) use diverse software artifacts other than source code; (3) adopt data augmentation techniques to tackle the class imbalance problem; (4) publish replication packages. © 2022 The Authors","Deep learning; Quality assurance; Software defect prediction; Systematic literature review"
"Systematic literature review of domain-oriented specification techniques","2022","Journal of Systems and Software","10.1016/j.jss.2022.111415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133420021&doi=10.1016%2fj.jss.2022.111415&partnerID=40&md5=d130391db867a8da18d9adbe9e839920","Context: The popularity of domain-specific languages and model driven development has made the tacit use of domain knowledge in system development more tangible. Our vision is a development process where a (software) system specification is based on multiple domain models, and where the specification method is built from cognitive concepts, presumably derived from natural language. Goal: To realize this vision, we evaluate and reflect upon the existing literature in domain-oriented specification techniques. Method: We designed and conducted a systematic literature review on domain-oriented specification techniques. Results: We identified 53 primary studies, populated the classification framework for each study, and summarized our findings per classification aspect. We found many approaches for creating domain models or domain-specific languages. Observations include: (i) most methods are defined incompletely; (ii) none offers methodical support for the use of domain models or domain-specific languages to create other specifications; (iii) there are specification techniques to integrate models in general, but no study offers methodical support for multiple domain models. Conclusion: The results indicate which topics need further research and which can instead be reused to realize our vision on system development. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 The Author(s)","Domain model; Domain-specific language; Method comparison; Modeling language; Specification method; Systematic literature review"
"Applying Inter-Rater Reliability and Agreement in collaborative Grounded Theory studies in software engineering","2023","Journal of Systems and Software","10.1016/j.jss.2022.111520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139877335&doi=10.1016%2fj.jss.2022.111520&partnerID=40&md5=b57e2f544245dee62239bdff313db74c","Context: The qualitative research on empirical software engineering that uses Grounded Theory is increasing (GT). The trustworthiness, rigor, and transparency of GT qualitative data analysis can benefit, among others, when multiple analysts juxtapose diverse perspectives and collaborate to develop a common code frame based on a consensual and consistent interpretation. Inter-Rater Reliability (IRR) and/or Inter-Rater Agreement (IRA) are commonly used techniques to measure consensus, and thus develop a shared interpretation. However, minimal guidance is available about how and when to measure IRR/IRA during the iterative process of GT, so researchers have been using ad hoc methods for years. Objective: This paper presents a process for systematically measuring IRR/IRA in GT studies, when appropriate, which is grounded in a previous systematic mapping study on collaborative GT in the field of software engineering. Methods: Meta-science guided us to analyze the issues and challenges of collaborative GT and formalize a process to measure IRR/IRA in GT. Results: This process guides researchers to incrementally generate a theory while ensuring consensus on the constructs that support it, improving trustworthiness, rigor, and transparency, and promoting the communicability, reflexivity, and replicability of the research. Conclusion: The application of this process to a GT study seems to support its feasibility. In the absence of further confirmation, this would represent the first step in a de facto standard to be applied to those GT studies that may benefit from IRR/IRA techniques. © 2022","Grounded Theory; Inter-Rater Agreement; Inter-Rater Reliability"
"Diversity-driven unit test generation","2022","Journal of Systems and Software","10.1016/j.jss.2022.111442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136455761&doi=10.1016%2fj.jss.2022.111442&partnerID=40&md5=1d131432993ca103be1c1ee53d002a03","The goal of automated unit test generation tools is to create a set of test cases for the software under test that achieve the highest possible coverage for the selected test quality criteria. The most effective approaches for achieving this goal at the present time use meta-heuristic optimization algorithms to search for new test cases using fitness functions defined on existing sets of test cases and the system under test. Regardless of how their search algorithms are controlled, however, all existing approaches focus on the analysis of exactly one implementation, the software under test, to drive their search processes, which is a limitation on the information they have available. In this paper we investigate whether the practical effectiveness of white box unit test generation tools can be increased by giving them access to multiple, diverse implementations of the functionality under test harvested from widely available Open Source software repositories. After presenting a basic implementation of such an approach, DivGen (Diversity-driven Generation), on top of the leading test generation tool for Java (EvoSuite), we assess the performance of DivGen compared to EvoSuite when applied in its traditional, mono-implementation oriented mode (MonoGen). The results show that while DivGen outperforms MonoGen in 33% of the sampled classes for mutation coverage (+16% higher on average), MonoGen outperforms DivGen in 12.4% of the classes for branch coverage (+10% higher average). © 2022 Elsevier Inc.","Automation; Behavior; Diversity; Evaluation; Experiment; Test amplification; Test generation; Test quality"
"Graph-based visualization of merge requests for code review","2023","Journal of Systems and Software","10.1016/j.jss.2022.111506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139263765&doi=10.1016%2fj.jss.2022.111506&partnerID=40&md5=7fd8bcf210da7b2548327d7814a426cf","Code review is a software development practice aimed at assessing code quality, finding defects, and sharing knowledge among developers. Despite its wide adoption, code review is a challenging task for developers, who often struggle to understand the content of a review change-set. Visualization techniques represent a promising approach to support reviewers. In this paper we present a new visualization approach that displays classes and methods in review changes as nodes in a graph. Then, we implemented our graph-based approach in a tool (ReviewVis) and performed a two-step feedback collection phase to assess the developers’ perceptions on the tool's benefits through (1) an in-company study with nine professional software developers and (2) an online survey with 37 participants. Given the positive results obtained by this first evaluation, we performed a second survey with 31 participants with a specific focus on supporting developers’ understanding of a review change-set. The collected feedback showed that the developers indeed perceive that ReviewVis can help them navigate and understand the changes under review. The results achieved also indicate possible future paths to use software visualization for code review. Data and Materials: https://doi.org/10.5281/zenodo.7047993 © 2022 The Author(s)","Empirical software engineering; Modern code review; Software visualization"
"EUDability: A new construct at the intersection of End-User Development and Computational Thinking","2023","Journal of Systems and Software","10.1016/j.jss.2022.111516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139597445&doi=10.1016%2fj.jss.2022.111516&partnerID=40&md5=d184d2bb5f76cfb3ea1187ebd827e79a","The sustainable and digital future of work may imply a dramatic equilibrium change between social factors and technological ones. We argue that providing suitable tools to support End-User Development (EUD) in the workplace could represent a way to cope with such future changes. The contributions of this paper include the analysis and characterization of the most used EUD techniques and their crossover with a new conveyed model of Computational Thinking. The synthesis between these aspects is made explicit in the construct of EUDability, which is designed to capture the quality dimensions of EUD systems suitable to work scenarios where better roles and better tools for individuals may be shaped. EUDability has to do with identifying and assessing the difficulties of EUD techniques on one side and the Computational Thinking skills held by individuals on the other side. © 2022 Elsevier Inc.","Computational Thinking; Digital transformation; End-User Development; EUDability; Life-long learning; Sustainable workplace"
"How have views on Software Quality differed over time? Research and practice viewpoints","2023","Journal of Systems and Software","10.1016/j.jss.2022.111524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140313968&doi=10.1016%2fj.jss.2022.111524&partnerID=40&md5=e4388fc3234b17723764df2587024515","Context: Over the years, there has been debate about what constitutes software quality and how it should be measured. This controversy has caused uncertainty across the software engineering community, affecting levels of commitment to the many potential determinants of quality among developers. An up-to-date catalogue of software quality views could provide developers with contemporary guidelines and templates. In fact, it is necessary to learn about views on the quality of code on frequently used online collaboration platforms (e.g., Stack Overflow), given that the quality of code snippets can affect the quality of software products developed. If quality models are unsuitable for aiding developers because they lack relevance, developers will hold relaxed or inappropriate views of software quality, thereby lacking awareness and commitment to such practices. Objective: We aim to explore differences in interest in quality characteristics across research and practice. We also seek to identify quality characteristics practitioners consider important when judging code snippet quality. First, we examine the literature for quality characteristics used frequently for judging software quality, followed by the quality characteristics commonly used by researchers to study code snippet quality. Finally, we investigate quality characteristics used by practitioners to judge the quality of code snippets. Methods: We conducted two systematic literature reviews followed by semi-structured interviews of 50 practitioners to address this gap. Results: The outcomes of the semi-structured interviews revealed that most practitioners judged the quality of code snippets using five quality dimensions: Functionality, Readability, Efficiency, Security and Reliability. However, other dimensions were also considered (i.e., Reusability, Maintainability, Usability, Compatibility and Completeness). This outcome differed from how the researchers judged code snippet quality. Conclusion: Practitioners today mainly rely on code snippets from online code resources, and specific models or quality characteristics are emphasised based on their need to address distinct concerns (e.g., mobile vs web vs standalone applications, regular vs machine learning applications, or open vs closed source applications). Consequently, software quality models should be adapted for the domain of consideration and not seen as one-size-fits-all. This study will lead to targeted support for various clusters of the software development community. © 2022 Elsevier Inc.","Code snippet quality; Software quality; Stack Overflow"
"A decade of code comment quality assessment: A systematic literature review","2023","Journal of Systems and Software","10.1016/j.jss.2022.111515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140462744&doi=10.1016%2fj.jss.2022.111515&partnerID=40&md5=82c0cc052cac46d1923bb69c7daa63a8","Code comments are important artifacts in software systems and play a paramount role in many software engineering (SE) tasks related to maintenance and program comprehension. However, while it is widely accepted that high quality matters in code comments just as it matters in source code, assessing comment quality in practice is still an open problem. First and foremost, there is no unique definition of quality when it comes to evaluating code comments. The few existing studies on this topic rather focus on specific attributes of quality that can be easily quantified and measured. Existing techniques and corresponding tools may also focus on comments bound to a specific programming language, and may only deal with comments with specific scopes and clear goals (e.g., Javadoc comments at the method level, or in-body comments describing TODOs to be addressed). In this paper, we present a Systematic Literature Review (SLR) of the last decade of research in SE to answer the following research questions: (i) What types of comments do researchers focus on when assessing comment quality? (ii) What quality attributes (QAs) do they consider? (iii) Which tools and techniques do they use to assess comment quality?, and (iv) How do they evaluate their studies on comment quality assessment in general? Our evaluation, based on the analysis of 2353 papers and the actual review of 47 relevant ones, shows that (i) most studies and techniques focus on comments in Java code, thus may not be generalizable to other languages, and (ii) the analyzed studies focus on four main QAs of a total of 21 QAs identified in the literature, with a clear predominance of checking consistency between comments and the code. We observe that researchers rely on manual assessment and specific heuristics rather than the automated assessment of the comment quality attributes, with evaluations often involving surveys of students and the authors of the original studies but rarely professional developers. © 2022 The Author(s)","Code comments; Documentation quality; Systematic literature review"
"SCGRU: A general approach for identifying multiple classes of self-admitted technical debt with text generation oversampling","2023","Journal of Systems and Software","10.1016/j.jss.2022.111514","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140061860&doi=10.1016%2fj.jss.2022.111514&partnerID=40&md5=9329cc3dbfc999f365f92b66463c4b1b","Identifying self-admitted technical debt (SATD) plays an important role in maintaining software stability and improving software quality. Although existing methods can detect SATD and researchers have identified design debt and requirement debt, an approach to realize multiple classification of SATD, including defect, test, and documentation, is still lacking. In this paper, we combine text generation oversampling and the Convolutional Neural Networks-Gated Recurrent Unit (CNNGRU) model, and propose an approach called SCGRU to classify multiple debt, including defect, test, documentation, design, and requirement. First, SeqGAN-based text generation is employed to generate new samples by learning the original SATD data, thereby increasing the number of SATD samples such as defect debt and reducing data imbalance. Then, we apply the CNNGRU model to refine SATD into multiple classes. An experiment with cross-project identification of 10 projects shows that our approach is more effective than existing methods such as CNN and text mining. The proposed SCGRU approach has strong advantages especially in cases of flawed debt with very unbalanced data such as test debt and documention debt. © 2022 Elsevier Inc.","Multi-classification; Self-admitted technical debt; Software quality; Text generation oversampling"
"CIT-daily: A combinatorial interaction testing-based daily build process","2022","Journal of Systems and Software","10.1016/j.jss.2022.111353","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130042029&doi=10.1016%2fj.jss.2022.111353&partnerID=40&md5=31316aff25f4339439fbc15a3f54807f","In this work, we introduce an approach, called CIT-daily, which integrates combinatorial interaction testing (CIT) with the daily build processes to systematically test the interactions between the factors/parameters affecting the system's behaviors, on a daily basis. We also develop a number of CIT-daily strategies and empirically evaluate them on highly-configurable systems. The first strategy tests the same t-way covering array every day throughout the process, achieving a t-way coverage on a daily basis by covering each possible combination of option settings for every combination of t options. The other strategies, on the other hand, while guaranteeing a t-way coverage on a daily basis, aim to cover higher order interactions between the configuration options over time by varying the t-way covering arrays tested. In the experiments, we observed that the proposed approach significantly improved the effectiveness (i.e., fault revealing abilities) of the daily build processes; randomizing the coverage of higher order interactions between the configuration options while guaranteeing a base t-way coverage every day, further improved the effectiveness; and the more the higher order interactions covered during the process, the higher the fault revealing abilities tended to be. © 2022 Elsevier Inc.","Combinatorial interaction testing; Covering arrays; Daily build processes; Software testing"
"Variability modules","2023","Journal of Systems and Software","10.1016/j.jss.2022.111510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139034915&doi=10.1016%2fj.jss.2022.111510&partnerID=40&md5=bfd314d8df082fdda5808df9cc2f19f3","A Software Product Line (SPL) is a family of similar programs, called variants, generated from a common artifact base. A Multi SPL (MPL) is a set of interdependent SPLs: each variant can depend on variants from other SPLs. MPLs are challenging to model and to implement efficiently, especially when different variants of the same SPL must coexist and interoperate. We address this challenge by introducing the concept of a variability module (VM), a new language construct. A VM constitutes at the same time a module and an SPL of standard (variability-free), possibly interdependent, modules. Generating a variant of a VM triggers the generation of all variants required to satisfy its dependencies. Consequentially, a set of interdependent VMs represents an MPL that can be compiled into a set of standard modules. We illustrate the VM concept with an example from an industrial modeling scenario and formalize it in a core calculus. We define family-based analyses to check that a VM satisfies certain well-formedness conditions and whether all variants can be generated. Finally, we provide an implementation of VM for the Java-like modeling language ABS, and evaluate it with case studies. © 2022 The Author(s)","Delta-oriented programming; Family-based analysis; Language design; Modules; Multi product line; Variant generation"
"A comprehensive empirical investigation on failure clustering in parallel debugging","2022","Journal of Systems and Software","10.1016/j.jss.2022.111452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135835969&doi=10.1016%2fj.jss.2022.111452&partnerID=40&md5=6a136e720899c5e8c3147f102502c02e","The clustering technique has attracted a lot of attention as a promising strategy for parallel debugging in multi-fault scenarios, this heuristic approach (i.e., failure indexing or fault isolation) enables developers to perform multiple debugging tasks simultaneously through dividing failed test cases into several disjoint groups. When using statement ranking representation to model failures for better clustering, several factors influence clustering effectiveness, including the risk evaluation formula (REF), the number of faults (NOF), the fault type (FT), and the number of successful test cases paired with one individual failed test case (NSP1F). In this paper, we present the first comprehensive empirical study of how these four factors influence clustering effectiveness. We conduct extensive controlled experiments on 1060 faulty versions of 228 simulated faults and 141 real faults, and the results reveal that: (1) GP19 is highly competitive across all REFs, (2) clustering effectiveness decreases as NOF increases, (3) higher clustering effectiveness is easier to achieve when a program contains only predicate faults, and (4) clustering effectiveness remains when the scale of NSP1F is reduced to 20%. © 2022 Elsevier Inc.","Failure clustering; Fault isolation; Multiple-fault; Parallel debugging"
"Exploring multi-programming-language commits and their impacts on software quality: An empirical study on Apache projects","2022","Journal of Systems and Software","10.1016/j.jss.2022.111508","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138185453&doi=10.1016%2fj.jss.2022.111508&partnerID=40&md5=4973a97ef53382eb1abfa96c96ec2362","Context: Modern software systems (e.g., Apache Spark) are usually written in multiple programming languages (PLs). There is little understanding on the phenomenon of multi-programming-language commits (MPLCs), which involve modified source files written in multiple PLs. Objective: This work aims to explore MPLCs and their impacts on development difficulty and software quality. Methods: We performed an empirical study on eighteen non-trivial Apache projects with 197,566 commits. Results:: (1) the most commonly used PL combination consists of all the four PLs, i.e., C/C++, Java, JavaScript, and Python; (2) 9% of the commits from all the projects are MPLCs, and the proportion of MPLCs in 83% of the projects goes to a relatively stable level; (3) more than 90% of the MPLCs from all the projects involve source files in two PLs; (4) the change complexity of MPLCs is significantly higher than that of non-MPLCs; (5) issues fixed in MPLCs take significantly longer to be resolved than issues fixed in non-MPLCs in 89% of the projects; (6) MPLCs do not show significant effects on issue reopen; (7) source files undergoing MPLCs tend to be more bug-prone; and (8) MPLCs introduce more bugs than non-MPLCs. Conclusions: MPLCs are related to increased development difficulty and decreased software quality. © 2022 Elsevier Inc.","Bug introduction; Bug proneness; Change complexity; Issue reopen; Multi-programming-language commit; Open source software"
"A mobile intelligent guide system for visually impaired pedestrian","2023","Journal of Systems and Software","10.1016/j.jss.2022.111546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141496962&doi=10.1016%2fj.jss.2022.111546&partnerID=40&md5=03d4ed4c6801a1f206354fab2796423b","Traditionally, tactile walking surface indicators (TWSIs) have been used as guide tools for visually impaired pedestrians, but the bumpy bricks also bring bad experiences to other users on the road, such as the elderly, people in wheelchairs, babies in strollers and ladies wearing high heels. In this paper, we propose an intelligent guide system based on a smartphone. Videos containing information about tactile paving captured by the phone camera are processed and analyzed by the system, and guide messages are sent to the user in the form of sound or vibration. In our system, the MobileNet model fine-tuned by transfer learning is used to perform feature extraction on overlapping grids. Single Shot MultiBox Detector (SSD) is then used for TWSI detection. Finally, the user's position is determined by the Score Voting algorithm, and corresponding guide information is given. In order to further improve the real-time performance of the system, we quantize the model to compress it while ensuring accuracy. The results of experiments on real tactile paving show that our system has high accuracy and real-time performance. With our system, bumpy tactile paving bricks can be replaced with flat stickers or paint with TWSI patterns. This is comforting for other road users, and it will be easy to set up and keep up. Moreover, the types of patterns can be extended for further applications. © 2022 Elsevier Inc.","Navigation system; Tactile paving detection; Transfer learning; Visually impaired pedestrian"
"Is it a case study?—A critical analysis and guidance","2022","Journal of Systems and Software","10.1016/j.jss.2022.111395","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132744710&doi=10.1016%2fj.jss.2022.111395&partnerID=40&md5=70b6759f1cb692388a45ecf68b2edd80","The term “case study” is not used consistently when describing studies and, most importantly, is not used according to the established definitions. Given the misuse of the term “case study”, we critically analyse articles that cite case study guidelines and report case studies. We find that only about 50% of the studies labelled “case study” are correctly labelled, and about 40% of studies labelled “case study” are actually better understood as “small-scale evaluations”. Based on our experiences conducting the analysis, we formulate support for ensuring and assuring the correct labelling of case studies. We develop a checklist and a self-assessment scheme. The checklist is intended to complement existing definitions and to encourage researchers to use the term “case study” correctly. The self-assessment scheme is intended to help the researcher identify when their empirical study is a “small-scale evaluation” and, again, encourages researchers to label their studies correctly. Finally, we develop and evaluate a smell indicator to automatically suggest when a reported case study may not actually be a case study. These three instruments have been developed to help ensure and assure that only those studies that are actually case studies are labelled as “case study”. © 2022 The Author(s)","Case study; Checklist; Citation analysis; Guidelines; Small-scale evaluation; Smell indicator"
"Scalability testing automation using multivariate characterization and detection of software performance antipatterns","2022","Journal of Systems and Software","10.1016/j.jss.2022.111446","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135348448&doi=10.1016%2fj.jss.2022.111446&partnerID=40&md5=a3bfb23c3b14aeb767dece086e63bb7b","Context: Software Performance Antipatterns (SPAs) research has focused on algorithms for their characterization, detection, and solution. Existing algorithms are based on the analysis of runtime behavior to detect trends on several monitored variables, such as system response time and CPU utilization. However, the lack of computationally efficient methods currently limits their integration into modern agile practices to detect SPAs in large scale systems. Objective: In this paper, we extended our previously proposed approach for the automated SPA characterization and detection designed to support continuous integration/delivery/deployment (CI/CDD) pipelines, with the goal of addressing the lack of computationally efficient algorithms. Method: We introduce a machine learning-based approach to improve the detection of SPA and interpretation of approach's results. The approach is complemented with a simulation-based methodology to analyze different architectural alternatives and measure the precision and recall of our approach. Our approach includes SPA statistical characterization using a multivariate analysis of load testing experimental results to identify the services that have the largest impact on system scalability. Results: To show the effectiveness of our approach, we have applied it to a large complex telecom system at Ericsson. We have built a simulation model of the Ericsson system and we have evaluated the introduced methodology by using simulation-based SPA injection. For this system, we are able to automatically identify the top five services that represent scalability choke points. We applied two machine learning algorithms for the automated detection of SPA. Conclusion: We contributed to the state-of-the-art by introducing a novel approach to support computationally efficient SPA characterization and detection that has been applied to a large complex system using performance testing data. We have compared the computational efficiency of the proposed approach with state-of-the-art heuristics. We have found that the approach introduced in this paper grows linearly, which is a significant improvement over existing techniques. © 2022 Elsevier Inc.","Characterization; Detection; Multivariate analysis; Software Performance Antipatterns"
"Data management for production quality deep learning models: Challenges and solutions","2022","Journal of Systems and Software","10.1016/j.jss.2022.111359","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130588457&doi=10.1016%2fj.jss.2022.111359&partnerID=40&md5=10f0337eeae50ed950297181897085ac","Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions. © 2022 The Authors","Challenges; Data management; Deep learning; Production quality DL models; Solutions; Validation"
"Fault localization using function call frequencies","2022","Journal of Systems and Software","10.1016/j.jss.2022.111429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134433546&doi=10.1016%2fj.jss.2022.111429&partnerID=40&md5=85853abaf046d7bdbd0850bf85a8a109","In traditional Spectrum-Based Fault Localization (SBFL), hit-based spectrum is used to estimate a program element's suspiciousness to contain a fault, i.e., only the binary information is used if the code element was executed by the test case or not. Count-based spectra can potentially improve the localization effectiveness due to the number of executions also being available. In this work, we use function-level granularity and define count-based spectra which use function call frequencies. We investigate the naïve approach, which simply counts the function call instances. We also define a novel method which is based on counting the different function call contexts, i.e., the frequency of the investigated function occurring in unique call stack instances during test execution. The basic intuition is that if a function is called in many different contexts during a failing test case, it will be more probable to be accountable for the fault. We empirically evaluated the fault localization capability of different variations of the approach and compared them to 9 traditional SBFL techniques using the Defects4J benchmark. We show that: (i) naïve counts result in worse rank positions than the hit-based approach, but (ii) unique counts produce better rank positions with some of the algorithm variants. © 2022 The Authors","Call stacks; Debugging; Function call frequency; Spectrum-Based Fault Localization; Testing"
"Combine sliced joint graph with graph neural networks for smart contract vulnerability detection","2023","Journal of Systems and Software","10.1016/j.jss.2022.111550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141321797&doi=10.1016%2fj.jss.2022.111550&partnerID=40&md5=71e722ea23da022d024f0445104f170d","Smart contract security has drawn extensive attention in recent years because of the enormous economic losses caused by vulnerabilities. Even worse, fixing bugs in a deployed smart contract is difficult, so developers must detect security vulnerabilities in a smart contract before deployment. Existing smart contract vulnerability detection efforts heavily rely on fixed rules defined by experts, which are inefficient and inflexible. To overcome the limitations of existing vulnerability detection approaches, we propose a GNN based approach for smart contract vulnerability detection. First, we construct a graph representation for a smart contract function with syntactic and semantic features by combining abstract syntax tree (AST), control flow graph (CFG), and program dependency graph (PDG). To further strengthen the presentation ability of our approach, we perform program slicing to normalize the graph and eliminate the redundant information unrelated to vulnerabilities. Then, we use a Bidirectional Gated Graph Neural-Network model with hybrid attention pooling to identify potential vulnerabilities in smart contract functions. Empirical results show that our approach can achieve 89.2% precision and 92.9% recall in smart contract vulnerability detection on our dataset and reveal the effectiveness and efficiency of our approach. © 2022 Elsevier Inc.","Code representation; Graph neural network; Smart contract; Vulnerability detection"
"Online malicious domain name detection with partial labels for large-scale dependable systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129557779&doi=10.1016%2fj.jss.2022.111322&partnerID=40&md5=d5fa5c8e53b95e4a80fb73ef74d44061","Detecting malicious non-existent domain names (NXDomains) in a real-time manner is vitally important to the security of large-scale dependable systems. Existing detection methods are trained based on the assumption that the NXDomains, which cannot be recognized by the domain generation algorithm (DGA) archive, are benign. However, new types of malicious NXDomains are continuously generated, and the DGA archive cannot cover all of them, making the NXDomains partially labeled. Additionally, extracting all the features for distinguishing malicious and benign NXDomains is computationally inefficient and inappropriate for online detection in large-scale dependable systems. This work proposes a framework, PUFS, to train an accurate malicious NXDomain detection model according to partial labels and conduct efficient online detection for large-scale dependable systems. PUFS adopts a novel, simple, yet effective three-step strategy to combine PU learning and feature selection. We conduct extensive experiments using real-world data collected from a top-tier global online bank. PUFS achieves 99.19% of F1-Score, and improves the feature extraction efficiency by 1153%, making it suitable for online detection scenarios. © 2022 Elsevier Inc.","DGA; DNS; Feature selection; PU learning; Reinforcement learning"
"Revealing the state of the art of large-scale agile development research: A systematic mapping study","2022","Journal of Systems and Software","10.1016/j.jss.2022.111473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136684806&doi=10.1016%2fj.jss.2022.111473&partnerID=40&md5=a5ab67afe6721726e249604cf6b70e6a","Context: Success with agile methods in the small scale has led to an increasing adoption also in large development undertakings and organizations. Recent years have also seen an increasing amount of primary research on the topic, as well as a number of systematic literature reviews. However, there is no systematic overview of the whole research field. Objective: This work identifies, classifies, and evaluates the state of the art of research in large-scale agile development. Methods: We conducted a systematic mapping study and rigorously selected 136 studies. We designed a classification framework and extracted key information from the studies. We synthesized the obtained data and created an overview of the state of the art. Results: This work contributes with (i) a description of large-scale agile endeavors reported in the industry, (ii) a systematic map of existing research in the field, (iii) an overview of influential studies, (iv) an overview of the central research themes, and (v) a research agenda for future research. Conclusion: This study portrays the state of the art in large-scale agile development and offers researchers and practitioners a reflection of the past thirteen years of research and practice on the large-scale application of agile methods. © 2022 Elsevier Inc.","Agile software development; Large-scale agile development; Systematic mapping study"
"Privacy explanations – A means to end-user trust","2023","Journal of Systems and Software","10.1016/j.jss.2022.111545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141330390&doi=10.1016%2fj.jss.2022.111545&partnerID=40&md5=2735f730ffb997b270f2ee44eccfd549","Software systems are ubiquitous, and their use is ingrained in our everyday lives. They enable us to get in touch with people quickly and easily, support us in gathering information, and help us perform our daily tasks. In return, we provide these systems with a large amount of personal information, often unaware that this is jeopardizing our privacy. End users are typically unaware of what data is collected, for what purpose, who has access to it, and where and how it is stored. To address this issue, we looked into how explainability might help to tackle this problem. We created privacy explanations that aim to help to clarify to end users why and for what purposes specific data is required. We asked end users about privacy explanations in a survey and found that the majority of respondents (91.6 %) are generally interested in receiving privacy explanations. Our findings reveal that privacy explanations can be an important step towards increasing trust in software systems and can increase the privacy awareness of end users. These findings are a significant step in developing privacy-aware systems and incorporating usable privacy features into them, assisting users in protecting their privacy. © 2022 Elsevier Inc.","Explainability; Online privacy; Privacy awareness; Privacy definition; Privacy explanations"
"A survey of software architectural change detection and categorization techniques","2022","Journal of Systems and Software","10.1016/j.jss.2022.111505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138322340&doi=10.1016%2fj.jss.2022.111505&partnerID=40&md5=dee1d5a976361fa08e28cbf9d03dbdc6","Software architecture is defined as the structural construction, design decisions implementation, evolution and knowledge sharing mechanisms of a system. Software architecture documentation help architects with decision making, guide developers during implementation, and preserve architectural decisions so that future caretakers are able to better understand an architect's solution. Many modern-day software development teams are focusing more on architectural consistency of software design to better cope with the cost-time-efforts, continuous integration, software glitches, security backdoors, regulatory inspections, human values, and so on. Therefore, in order to better reflect the software design challenges, the development teams review the architectural design either on a regular basis or after completing certain milestones or releases. However, many studies have focused on architectural change detection and classification as the essential steps for reviewing design, discovering architectural tactics and knowledge, analyzing software stability, tracing and auditing software development history, recovering design decisions, generating design summary, and so on. In this paper, we survey state-of-the-art architectural change detection and categorization techniques and identify future research directions. To the best of our knowledge, our survey is the first comprehensive report on this area. However, in this survey, we compare available techniques using various quality attributes relevant to software architecture for different implementation levels and types. Moreover, our analysis shows that there is a lack of lightweight techniques (in terms of human intervention, algorithmic complexity, and frequency of usage) feasible to process hundreds and thousands of change revisions of a project. We also realize that rigorous focuses are required for capturing the design decision associativity of the architectural change detection techniques for practical use in the design review process. However, our survey on architectural change classification shows that existing automatic change classification techniques are not promising enough to use for real-world scenarios and reliable post analysis of causes of architectural change is not possible without manual intervention. There is also a lack of empirical data to construct an architectural change taxonomy, and further exploration in this direction would add much value to architectural change management. © 2022 Elsevier Inc.","Abstraction; Change detection; Classification; Design review; Software architecture"
"Visualization of aggregated information to support class-level software evolution","2022","Journal of Systems and Software","10.1016/j.jss.2022.111421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133234762&doi=10.1016%2fj.jss.2022.111421&partnerID=40&md5=8ec348e18ea1db2f888ca60b4ee6424f","Context: Software is inherently prone to constant change, especially in the source code, making it difficult for developers to keep track of changes performed over time and to fully understand their implications. Objective: To this end, we present an Eclipse plug-in for visualizing heterogeneous information, collected from multiple sources, at different levels of granularity. This visualization provides a single graphical representation of a system's change histories over multiple versions, allowing developers to identify the previous and present dependencies in the system, while adding new or removing and modifying the current functionalities of a software. Summarizing and associating the relevant changes in a single graph, further supports developers, not familiar with the overall system, to conduct a self-study and explore the systems design and changes of its functionality over time. Method: Our tool, DejaVu, initially infers and further visualizes change scenarios that have been applied to a given class in source code, across multiple versions of a software. DejaVu additionally augments the change information with prior commits from GitHub repositories, as well as associated issues from Jira issue tracking system. Evaluation: As part of the evaluation, we conducted a controlled experiment, recruiting participants with research or industrial programming experiences. The participants were asked to investigate and assess a set of change stories with and without the use of the DejaVu. As such, we empirically evaluated the impact of DejaVu in alleviating developers’ understanding of code class-level changes across multiple versions. Results: Our results showed an average of 52% reduction in completion time and a 51% increase in correctness of several change-comprehension tasks once, users adopted DejaVu in comparison to the manual completion of the same tasks. A student's t-test verified the significant improvement in time and correctness of the tasks with p-values of 0.01 and 0.002. Conclusion: Visualizing aggregated information from multiple sources provides developers with a more comprehensive intuition of the change and its rationale, facilitating software maintenance tasks. © 2022 Elsevier Inc.","Change visualization; Code changes; Software evolution"
"What factors affect the UX in mobile apps? A systematic mapping study on the analysis of app store reviews","2022","Journal of Systems and Software","10.1016/j.jss.2022.111462","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135708476&doi=10.1016%2fj.jss.2022.111462&partnerID=40&md5=989c314641427d6334d042e1f97bc0ad","Researchers and practitioners are becoming aware of the importance of User eXperience (UX) in mobile app development. Developing merely usable apps became insufficient to meet users’ needs, requiring developers to focus on promoting pleasurable experiences to get a competitive advantage. To that end, it is crucial to understand what factors can lead to positive or negative UX. In this scenario, app store reviews emerged as a valuable source to address UX issues from analyzing several self-reports of end-users experiences in the wild. Many researchers have proposed approaches to analyze such reviews and investigate the effect of factors related to the user (e.g., gender, culture) and the app (e.g., bugs, features) on UX. However, the fragmentation of the results into various studies makes it difficult to draw conclusions that can support the development process and advance the research in the field. This paper presents a systematic mapping study to address publications that analyze app store reviews and identify the factors affecting UX reflected on users’ ratings and sentiment. From 25 accepted publications, we extracted 31 factors and their associated polarities. We also identified research gaps and future work opportunities with implications for practitioners and researchers. © 2022 Elsevier Inc.","App store reviews; Influencing factors; Systematic mapping study; User experience"
"The perspective of Brazilian software developers on data privacy","2023","Journal of Systems and Software","10.1016/j.jss.2022.111523","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139857676&doi=10.1016%2fj.jss.2022.111523&partnerID=40&md5=71520166cd525311dbacf5e95125ee1e","Context: Maintaining the privacy of user data is a concern in software development to satisfy customer needs or to comply with privacy laws. Recent studies have shown that software development approaches still neglect non-functional requirements, including privacy. Concern about privacy may increase in the period between when a privacy law is initially announced and when it is passed into law. During this period, companies will be challenged to comply with the new law. Research has shown that many developers do not have sufficient knowledge to develop privacy-preserving software systems. Objective: We investigate the level of knowledge and understanding that developers possess regarding privacy. We explore the personal, behavioural, and external environmental factors affecting a developer's decision-making regarding privacy requirements. Methods: We replicated a study by means of in-depth, semi-structured interviews with thirteen practitioners at six companies. Our data analysis is based on the principles of ‘grounded theory codification’. Results: We identified nine personal factors, five behavioural factors, and seven external environment factors that are relevant to how software developers make decisions regarding. Conclusion: Our identification of factors that influence the development of privacy-preserving software systems can be seen as a contribution to the specification of effective methods for securing privacy. © 2022 Elsevier Inc.","Empirical study; Privacy requirements; Software development"
"Architectural patterns for the design of federated learning systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130338616&doi=10.1016%2fj.jss.2022.111357&partnerID=40&md5=0b01334bb06be844f4684740cd833b8e","Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from the machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, four model aggregation patterns, and one configuration pattern. The patterns are associated to the particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems. © 2022 Elsevier Inc.","Artificial intelligence; Federated learning; Machine learning; Pattern; Software architecture"
"How far does the predictive decision impact the software project? The cost, service time, and failure analysis from a cross-project defect prediction model","2023","Journal of Systems and Software","10.1016/j.jss.2022.111522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139824509&doi=10.1016%2fj.jss.2022.111522&partnerID=40&md5=d9c7427f2002afde5a9ab9cf9bb8bcb5","Context: Cross-project defect prediction (CPDP) models are being developed to optimise the testing resources. Objectives: Proposing an ensemble classification framework for CPDP as many existing models are lacking with better performances and analysing the main objectives of CPDP from the outcomes of the proposed classification framework. Method: For the classification task, we propose a bootstrap aggregation based hybrid-inducer ensemble learning (HIEL) technique that uses probabilistic weighted majority voting (PWMV) strategy. To know the impact of HIEL on the software project, we propose three project-specific performance measures such as percent of perfect cleans (PPC), percent of non-perfect cleans (PNPC), and false omission rate (FOR) from the predictions to calculate the amount of saved cost, remaining service time, and percent of the failures in the target project. Results: On many target projects from PROMISE, NASA, and AEEEM repositories, the proposed model outperformed recent works such as TDS, TCA+, HYDRA, TPTL, and CODEP in terms of F-measure. In terms of AUC, the TCA+ and HYDRA models stand as strong competitors to the HIEL model. Conclusion: For better predictions, we recommend ensemble learning approaches for the CPDP models. And, to estimate the benefits from the SDP models, we recommend the above project-specific performance measures. © 2022 Elsevier Inc.","Cross-project defect prediction; Ensemble learning; Machine learning; Prediction analysis; Probabilistic weighted majority voting"
"An empirical study on the challenges that developers encounter when developing Apache Spark applications","2022","Journal of Systems and Software","10.1016/j.jss.2022.111488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136639314&doi=10.1016%2fj.jss.2022.111488&partnerID=40&md5=99fdd6af8129332eff9edcfa7f6687b4","Apache Spark is one of the most popular big data frameworks that abstract the underlying distributed computation details. However, even though Spark provides various abstractions, developers may still encounter challenges related to the peculiarity of distributed computation and environment. To understand the challenges that developers encounter, and provide insight for future studies, in this paper, we conduct an empirical study on the questions that developers encounter. We manually analyze 1,000 randomly selected questions that we collected from Stack Overflow. We find that: 1) questions related to data processing (e.g., transforming data format) are the most common among the 11 types of questions that we uncovered. 2) Even though data processing questions are the most common ones, they require the least amount of time to receive an answer. Questions related to configuration and performance require the most time to receive an answer. 3) Most of the issues are caused by developers’ insufficient knowledge in API usages, data conversation across frameworks, and environment-related configurations. We also discuss the implication of our findings for researchers and practitioners. In summary, our work provides insights for future research directions and highlight the need for more software engineering research in this area. © 2022 Elsevier Inc.","Big data system; Empirical study; Stack Overflow"
"Antipatterns in software classification taxonomies","2022","Journal of Systems and Software","10.1016/j.jss.2022.111343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129468608&doi=10.1016%2fj.jss.2022.111343&partnerID=40&md5=610d309bdf6605c9d74711080c781a8b","Empirical results in software engineering have long started to show that findings are unlikely to be applicable to all software systems, or any domain: results need to be evaluated in specified contexts, and limited to the type of systems that they were extracted from. This is a known issue, and requires the establishment of a classification of software types. This paper makes two contributions: the first is to evaluate the quality of the current software classifications landscape. The second is to perform a case study showing how to create a classification of software types using a curated set of software systems. Our contributions show that existing, and very likely even new, classification attempts are deemed to fail for one or more issues, that we named as the ‘antipatterns’ of software classification tasks. We collected 7 of these antipatterns that emerge from both our case study, and the existing classifications. These antipatterns represent recurring issues in a classification, so we discuss practical ways to help researchers avoid these pitfalls. It becomes clear that classification attempts must also face the daunting task of formulating a taxonomy of software types, with the objective of establishing a hierarchy of categories in a classification. © 2022 The Author(s)","Antipattern; Classification; Machine learning; Natural language processing; Software types; Taxonomy"
"VIBE: Looking for Variability In amBiguous rEquirements","2023","Journal of Systems and Software","10.1016/j.jss.2022.111540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141305255&doi=10.1016%2fj.jss.2022.111540&partnerID=40&md5=74cdd6899f2cb701c9d32abbe8cce19c","Variability is a characteristic of a software project and describes the fact that a system can be configured in different ways, obtaining different products (variants) from a common code base, accordingly to the software product line paradigm. This paradigm can be conveniently applied in all phases of the software process, starting from the definition and analysis of the requirements. We observe that often requirements contain ambiguities which can reveal an unintentional and implicit source of variability, that has to be detected. To this end we define VIBE, a tool supported process to identify variability aspects in requirements documents. VIBE is defined on the basis of a study of the different sources of ambiguity in natural language requirements documents that are useful to recognize potential variability, and is characterized by the use of a NLP tool customized to detect variability indicators. The tool to be used in VIBE is selected from a number of ambiguity detection tools, after a comparison of their customization features. The validation of VIBE is conducted using real-world requirements documents. © 2022 Elsevier Inc.","Ambiguity; Natural language processing tools; Natural language requirements documents; Software product lines; Variability detection"
"Towards using visual, semantic and structural features to improve code readability classification","2022","Journal of Systems and Software","10.1016/j.jss.2022.111454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135389780&doi=10.1016%2fj.jss.2022.111454&partnerID=40&md5=426374699ee51b3986f01ddbbedbb3b1","Context: Code readability, which correlates strongly with software quality, plays a critical role in software maintenance and evolvement. Although existing deep learning-based code readability models have reached a rather high classification accuracy, only structural features are utilized which inevitably limits their model performance. Objective: To address this problem, we propose to extract readability-related features from visual, semantic, and structural aspects from source code in an attempt to further improve code readability classification. Method: First, we convert a code snippet into a RGB matrix (for visual feature extraction), a token sequence (for semantic feature extraction) and a character matrix (for structural feature extraction). Then, we input them into a hybrid neural network that is composed of BERT, CNN, and BiLSTM for feature extraction. Finally, the extracted features are concatenated and input into a classifier to make a code readability classification. Result: A series of experiments are conducted to evaluate our method. The results show that the average accuracy could reach 85.3%, which outperforms all existing models. Conclusion: As an innovative work of extracting readability-related features automatically from visual, semantic, and structural aspects, our method is proved to be effective for the task of code readability classification. © 2022 Elsevier Inc.","Code readability classification; Code representation; Neural networks; Program comprehension; Software analysis"
"A systematic literature review on benchmarks for evaluating debugging approaches","2022","Journal of Systems and Software","10.1016/j.jss.2022.111423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134429445&doi=10.1016%2fj.jss.2022.111423&partnerID=40&md5=dcd67be2033b0a186d222b9d943fade0","Bug benchmarks are used in development and evaluation of debugging approaches, e.g. fault localization and automated repair. Quantitative performance comparison of different debugging approaches is only possible when they have been evaluated on the same dataset or benchmark. However, benchmarks are often specialized towards usage for certain debugging approaches in their contained data, metrics, and artifacts. Such benchmarks cannot be easily used on debugging approaches outside their scope as such approach may rely on specific data such as bug reports or code metrics that are not included in the dataset. Furthermore, benchmarks vary in their size w.r.t. the number of subject programs and the size of the individual subject programs. For these reasons, we have performed a systematic literature review where we have identified 73 benchmarks that can be used to evaluate debugging approaches. We compare the different benchmarks w.r.t. their size and the provided information such as bug reports, contained test cases, and other code metrics. This comparison is intended to help researchers to quickly identify all suitable benchmarks for evaluating their specific debugging approaches. Furthermore, we discuss reoccurring issues and challenges in selection, acquisition, and usage of such bug benchmarks, i.e., data availability, data quality, duplicated content, data formats, reproducibility, and extensibility. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 The Author(s)","Automatic repair; Benchmark; Debugging; Fault localization"
"Ensemble Effort Estimation: An updated and extended systematic literature review","2023","Journal of Systems and Software","10.1016/j.jss.2022.111542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141926894&doi=10.1016%2fj.jss.2022.111542&partnerID=40&md5=958caf444bda96685309b21a0360acf3","Ensemble Effort Estimation (EEE) techniques combine several individual software estimation methods in order to address the weaknesses of individual methods for prediction tasks. A systematic review published in 2016 analyzed empirical studies on EEE techniques published between 2010 and (January) 2016. The research on EEE has continuously evolved over the past five years (2016–2020), generating new findings that should be aggregated to the existing body of evidence on the subject. The goal of this paper is to update the systematic review from 2016 with new findings from studies published between 2016 (full year) and 2020 (inclusive). To conduct our review update, we followed existing guidelines for updating systematic reviews in software engineering and other fields. We started with an appraisal of the background and methods of the 2016 review, which resulted in the updated review protocol used to conduct our study. We retrieved 3,682 papers using automatic searching techniques, from which we selected 30 papers for data extraction and analysis. Our findings reinforce the results of the previous review in that machine learning is still the technique most common to construct EEE and that the ensemble techniques have outperformed the individual models. We added new evidence showing that there is no clear superiority of an EEE model over the others. Also, we found that ensemble dynamic selection is still little used in Software Effort Estimation (SEE). This review adds new evidence about the use of EEE techniques in software development which reinforces previous findings and also shows research opportunities in constructing more effective EEE. Besides, ensemble dynamic selection appears as a promising area of research which still is underexplored. © 2022","Machine learning; Software effort estimation; Software Engineering; Systematic literature review, Ensemble Effort Estimation"
"Cost-effective load testing of WebRTC applications","2022","Journal of Systems and Software","10.1016/j.jss.2022.111439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136143061&doi=10.1016%2fj.jss.2022.111439&partnerID=40&md5=e1c0f50e22ce58006c6ab7283ab814bf","Background: Video conference applications and systems implementing the WebRTC W3C standard are becoming more popular and demanded year after year, and load testing them is of paramount importance to ensure they can cope with demand. However, this is an expensive activity, usually involving browsers to emulate users. Goal:: to propose browser-less alternative strategies for load testing WebRTC services, and to study performance and costs of those strategies when compared with traditional ones. Method: (a) Exploring the limits of existing and novel strategies for load testing WebRTC services from a single machine. (b) Comparing the common strategy of using browsers with the best of our proposed strategies in terms of cost in a load testing scenario. Results: We observed that, using identical machines, our proposed strategies are able to emulate more users than traditional strategies. We also found a huge saving in expenditure for load testing, as our strategy suppose a saving of 96% with respect to usual browser-based strategies. We also found there are almost no differences between the traditional strategies considered. Conclusion: We provide details on scalability of different load testing strategies in terms of users emulated, as well as CPU and memory used. We could reduce the expenditure of load tests of WebRTC applications. © 2022 The Author(s)","Load testing; Testing; WebRTC"
"Locating and categorizing inefficient communication patterns in HPC systems using inter-process communication traces","2022","Journal of Systems and Software","10.1016/j.jss.2022.111494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138112404&doi=10.1016%2fj.jss.2022.111494&partnerID=40&md5=bc2ed09c1d5ff3fb61430f92b3fa142b","High Performance Computing (HPC) systems are used in a variety of industrial and research sectors to solve complex problems that require powerful computing platforms. For these systems to remain reliable, we should be able to debug and analyze their behavior in order to detect root causes of potential poor performance. Execution traces hold important information regarding the events and interactions among communicating processes, which are essential for the debugging of inter-process communication. Traces, however, tend to be considerably large, hindering their applicability. In previous work, we presented an approach for automatically detecting communication patterns and segmenting large HPC traces into execution phases. The goal is to reduce the effort of analyzing traces by allowing software analysts to focus on smaller parts of interest. In this paper, we propose an approach for detecting and localizing inefficient communication patterns using statistical and trace segmentation methods. In addition, we use the Analytic Hierarchy Process to categorize slow communication patterns based on their severity and complexity levels. Using our approach, an analyst can quickly locate slow communication patterns that may be the cause of important performance problems. We show the effectiveness of our approach by applying it to large traces from three HPC systems. © 2022 Elsevier Inc.","Data Analytics; Debugging; Distributed systems; Message Passing Interface; Performance analysis; Software tracing"
"End-users’ knowledge and perception about security of clinical mobile health apps: A case study with two Saudi Arabian mHealth providers","2023","Journal of Systems and Software","10.1016/j.jss.2022.111519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141292404&doi=10.1016%2fj.jss.2022.111519&partnerID=40&md5=f814508fccb5b9c49a6f8f8ee5f16461","Mobile health apps (mHealth apps) are being increasingly adopted in the healthcare sector, enabling stakeholders such as medics and patients, to utilize health services in a pervasive manner. Despite having several benefits, mHealth apps entail significant security and privacy challenges that can lead to data breaches with serious social, legal, and financial consequences. This research presents an empirical investigation into security awareness of end-users of mHealth apps that are available on major mobile platforms. We conducted end-users’ survey-driven case study research in collaboration with two mHealth providers in Saudi Arabia to survey 101 end-users, investigating their security awareness about (i) existing and desired security features, (ii) security-related issues, and (iii) methods to improve security knowledge. The results indicate that while security awareness among the different demographic groups was statistically significant based on their IT knowledge level and education level, security awareness based on gender, age, and frequency of mHealth app usage was not statistically significant. We also found that the majority of the end-users are unaware of the existing security features provided (e.g., restricted app permissions); however, they desire usable security (e.g., biometric authentication) and are concerned about the privacy of their health information (e.g., data anonymization). End-users suggested that protocols such as two-factor authentication positively impact security but compromise usability. Security-awareness via peer guidance, or training from app providers can increase end-users’ trust in mHealth apps. This research investigates human-centric knowledge based on a case study and provides a set of guidelines to develop secure and usable mHealth apps. © 2022 Elsevier Inc.","Case study; Mobile healthcare; Mobile security; Software engineering"
"SuMo: A mutation testing approach and tool for the Ethereum blockchain","2022","Journal of Systems and Software","10.1016/j.jss.2022.111445","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135400429&doi=10.1016%2fj.jss.2022.111445&partnerID=40&md5=8c1f3b7065e819fe1ee09d01693f482f","Blockchain technologies have had a rather disruptive impact on many sectors of the contemporary society. The establishment of virtual currencies is probably the most representative case. Nonetheless, the inherent support to trustworthy electronic interactions has widened the possible adoption contexts. In the last years, the introduction of Smart Contracts has further increased the potential impact of such technologies. These self-enforcing programs have interesting peculiarities (e.g., code immutability) that require innovative testing strategies. This paper presents a mutation testing approach for assessing the quality of test suites accompanying Smart Contracts written in Solidity, the language used by the Ethereum Blockchain. Specifically, we propose a novel suite of mutation operators capable of simulating a wide variety of traditional programming errors and Solidity-specific faults. The operators come in two flavors: Optimized, for faster mutation testing campaigns, and Non-Optimized, for performing a more thorough adequacy assessment. We implemented our approach in a proof-of-concept work, SuMo (SOlidity MUtator), and we evaluated its effectiveness on a set of real-world Solidity projects. The experiments highlighted a recurrent low Mutation Score for the test suites shipped with the selected applications. Moreover, analyzing the surviving mutants of a selected project helped us to identify faulty test cases and Smart Contract code. These results suggest that SuMo can concretely improve the fault-detection capabilities of a test suite, and help to deliver more reliable Solidity code. © 2022 Elsevier Inc.","Blockchain; Mutation testing; Smart contract; Solidity; Test automation"
"QExplore: An exploration strategy for dynamic web applications using guided search","2023","Journal of Systems and Software","10.1016/j.jss.2022.111512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139193350&doi=10.1016%2fj.jss.2022.111512&partnerID=40&md5=90738e14866b01592254b972140a84bd","Dynamic exploration approaches play an important role in automating web testing and analysis. They are extensively used to explore the state-space of a web application for achieving complete coverage of the application's functionality. Dynamic exploration approaches support end-to-end automation of testing to verify the correct behavior of a web application. However, existing approaches failed to explore the states behind the web forms and can get stuck in dynamic regions of web applications resulting in poor functionality coverage and diversity. Consequently, existing approaches are regressive in nature and sensitive to small DOM mutations which may not be interesting from a testing perspective. In this paper, we propose a dynamic exploration approach using guided search inspired by Q-learning that systematically explores dynamic web applications requiring less or no prior knowledge about the application. Our approach is implemented in a tool called QExplore and is empirically evaluated with six popular open-source and one real industrial application. The results show that QExplore achieved higher coverage with more diverse DOM than the existing state-of-the-art tools Crawljax and WebExplor. QExplore also results in a greater number of navigational paths, error states and distinct DOM states when compared with the existing tools. © 2022 Elsevier Inc.","Automated testing; Coverage; Dynamic exploration; Guided search; Model generation; Q-learning; Reinforcement learning; Web application testing"
"VULPEDIA: Detecting vulnerable ethereum smart contracts via abstracted vulnerability signatures","2022","Journal of Systems and Software","10.1016/j.jss.2022.111410","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133492932&doi=10.1016%2fj.jss.2022.111410&partnerID=40&md5=3a8a268bfd44cdd182fe5ec04aece649","Recent years have seen smart contracts are getting increasingly popular in building trustworthy decentralized applications. Previous research has proposed static and dynamic techniques to detect vulnerabilities in smart contracts. These tools check vulnerable contracts against several predefined rules. However, the emerging new vulnerable types and programming skills to prevent possible vulnerabilities emerging lead to a large number of false positive and false negative reports of tools. To address this, we propose VULPEDIA, which mines expressive vulnerability signatures from contracts. VULPEDIA is based on the relaxed assumption that the owner of contract is not malicious. Specifically, we extract structural program features from vulnerable and benign contracts as vulnerability signatures, and construct a systematic detection method based on detection rules composed of vulnerability signatures. Compared with the rules defined by state-of-the-arts, our approach can extract more expressive rules to achieve better completeness (i.e., detection recall) and soundness (i.e., precision). We further evaluate VULPEDIA with four baselines (i.e., Slither, Securify, SmartCheck and Oyente) on the testing dataset consisting of 17,770 contracts. The experiment results show that VULPEDIA achieves best performance of precision on 4 types of vulnerabilities and leading recall on 3 types of vulnerabilities meanwhile exhibiting the great efficiency performance. © 2022 Elsevier Inc.","Blockchain security; Smart contract; Software analysis; Software clone analysis; Software testing"
"Analysis of vulnerability fixing process in the presence of incorrect patches","2023","Journal of Systems and Software","10.1016/j.jss.2022.111525","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140045519&doi=10.1016%2fj.jss.2022.111525&partnerID=40&md5=4b5a490c2d4b3abf8a4aca9a49480551","Software vulnerabilities or security breaches can have consequences like leakage of sensitive information and malware execution, which are critical to network security. Consequently, eliminating security loopholes and vulnerabilities is imperative for the system administrator to counteract security attacks. Software should be thoroughly reviewed before it is released to uncover these security invasions. However, it is not feasible to identify and overcome all software failures during software testing due to external instances of software development, implementation costs, execution time, and unanticipated modifications to the specification. Security patching is a viable solution for such software systems to prevent attackers from exploiting existing vulnerabilities. Even after patch distribution and installation, it is crucial to determine whether the patch has effectively eliminated the vulnerability. Incorrect patches may lead to new security bugs, which may be malicious and disastrous for developing businesses and users. The present research aims to model the trend of patched vulnerabilities methodically by incorporating the generation of new vulnerabilities due to unsuccessful updations and encompassed bug fixes. The proposed analytical model is validated on the vulnerability databases obtained from the Common Vulnerabilities and Exposures repository. The empirical analysis yields that the present research has better forecasting efficacy than the benchmark studies. © 2022 Elsevier Inc.","Intrusion detection; Patch correctness; Patch management; Security bugs; Software vulnerabilities"
"IADA: A dynamic interference-aware cloud scheduling architecture for latency-sensitive workloads","2022","Journal of Systems and Software","10.1016/j.jss.2022.111491","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137153558&doi=10.1016%2fj.jss.2022.111491&partnerID=40&md5=e92e85c328c78fdbf698bec329382140","Cloud computing allows several applications to share physical resources, yielding rapid provisioning and improving hardware utilization. However, multiple applications contending for shared resources are susceptible to interference, which might lead to significant performance degradation and consequently an increase in Service Level Agreements violations. In previous work, we started to analyze resource contention and its impact on performance degradation and hardware utilization. Then, we created an interference-aware application classifier based on machine learning techniques and evaluated it comparing two classification strategies: (i) unique, when a single classification is performed over the entire applications’ execution; and (ii) segmented, when the classification is carried out over multiple static-defined intervals. Moving towards a dynamic scheduling solution, we combine and improve on previous work findings and, in this work, we present IADA, a full-fledged dynamic interference-aware cloud scheduling architecture for latency-sensitive workloads. Our approach consists in improving on a segmented interference classification of applications to a dynamic classification scheme based on workload variations. Aiming at using the available resource more efficiently and respecting Quality of Services requirements, the proposed architecture was developed supported by machine learning techniques, heuristics, and a bayesian changepoint detection algorithm for online inference. We conducted a set of real and simulated experiments, utilizing a developed extension of CloudSim Toolkit to analyze and compare the proposed architecture efficiency with related studies. Results evidenced that IADA reduces by 25%, on average, the overall performance degradation. © 2022 Elsevier Inc.","Cloud computing; Dynamic workloads; Interference-aware; Machine learning; Resource management"
"Perceptions of the human and social factors that influence the productivity of software development teams in Colombia: A statistical analysis","2022","Journal of Systems and Software","10.1016/j.jss.2022.111408","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132928272&doi=10.1016%2fj.jss.2022.111408&partnerID=40&md5=adc3fdec9f7b0ad4ebc82d51e6a76f03","This research aims to know if software engineering professionals consider that social and human factors (SHF) influence the productivity of a work team. A survey-based study was conducted among 112 members of software development teams. Empirical results show professionals agree with the SHF in the context of software development influence in the productivity of work teams. It was identified that the 13 SHFs have a weak or moderate correlation with each other. Additionally, the results of the exploratory factorial analysis suggest categorizing the factors into those associated with the individual, those associated with team interaction, and those related to capabilities and experience. This categorization reduced the number of items in the original questionnaire while preserving the variability explained in the latent variables, which will require a shorter response time. Our results broaden the understanding of the SHFs that influence software development team productivity and open up new research opportunities. Measuring the perception of these factors can be used to identify which SHFs should be prioritized in a strategy to improve productivity. In addition, this knowledge can help software organizations appropriately manage their development teams and propose innovative work approaches that have a positive impact on the success of their projects. © 2022 Elsevier Inc.","Human factors; Social factors; Software development productivity; Statistical analysis"
"The effect of advice network connectedness on problem-solving competence among software developers","2022","Journal of Systems and Software","10.1016/j.jss.2022.111489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137166582&doi=10.1016%2fj.jss.2022.111489&partnerID=40&md5=ea78bd83504583bc246ffbb6602bcbb1","Software development requires software developers to share knowledge and solve problems together. Although researchers have considered the business and technical knowledge germane to performing software development tasks, empirical studies investigating business and technical advice networks on problem-solving competence is scarce. Using social network theory, we argue that software developers must be embedded for knowledge brokering within and across business and technical advice connectedness for improving problem-solving competence. Moreover, we argue that contact quality matters in increasing or decreasing individual problem-solving competence. We present data collected via an online survey from 153 respondents in a professional software organisation. Our findings suggest that software developers who engage in knowledge brokering in business and technical advice connectedness will increase problem-solving competence in the software development effort. Our findings also reveal no significant effect of contact quality between these advice networks and problem-solving competence. We discuss our findings’ implications for theory and practice. © 2022 Elsevier Inc.","Advice network; Brokerage; Contact quality; Problem-solving competence; Software developers; Software development"
"Assessing the linguistic quality of REST APIs for IoT applications","2022","Journal of Systems and Software","10.1016/j.jss.2022.111369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131218016&doi=10.1016%2fj.jss.2022.111369&partnerID=40&md5=629a268070d3e446658b88a37e86cd6e","Internet of Things (IoT) is a growing technology that relies on connected ‘things’ that gather data from peer devices and send data to servers via APIs (Application Programming Interfaces). The design quality of those APIs has a direct impact on their understandability and reusability. This study focuses on the linguistic design quality of REST APIs for IoT applications and assesses their linguistic quality by performing the detection of linguistic patterns and antipatterns in REST APIs for IoT applications. Linguistic antipatterns are considered poor practices in the naming, documentation, and choice of identifiers. In contrast, linguistic patterns represent best practices to APIs design. The linguistic patterns and their corresponding antipatterns are hence contrasting pairs. We propose the SARAv2 (Semantic Analysis of REST APIs version two) approach to perform syntactic and semantic analyses of REST APIs for IoT applications. Based on the SARAv2 approach, we develop the REST-Ling tool and empirically validate the detection results of nine linguistic antipatterns. We analyse 19 REST APIs for IoT applications. Our detection results show that the linguistic antipatterns are prevalent and the REST-Ling tool can detect linguistic patterns and antipatterns in REST APIs for IoT applications with an average accuracy of over 80%. Moreover, the tool performs the detection of linguistic antipatterns on average in the order of seconds, i.e., 8.396 s. We found that APIs generally follow good linguistic practices, although the prevalence of poor practices exists. © 2022 The Author(s)","Antipattern; Detection; IoT applications; Linguistic quality; Pattern; REST APIs"
"Graph4Web: A relation-aware graph attention network for web service classification","2022","Journal of Systems and Software","10.1016/j.jss.2022.111324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129456347&doi=10.1016%2fj.jss.2022.111324&partnerID=40&md5=d1ee3c4d7db360934c312053a0be1dee","Software reuse is a popular way to utilize existing software components to ensure the quality of newly developed software in service-oriented architecture. However, how to find a suitable web service from existing repositories to meet requirements is still an open issue. Among others, web service classification is one of the most essential and effective means for web service recommendation. Previous studies have concerned this problem, but a critical issue, i.e., the semantic and syntactic information for the web service, is often ignored. To address such an issue, in this work, we propose Graph4Web, which uses a relation-aware graph attention network for web service classification. Specifically, we first parse the web service description sequence into the dependency graph and initialize the embedding vector of each node in the graph by tuning the pre-trained BERT model. We further propose a relation-aware graph attention layer to learn and update the node embedding vector by aggregating the information of neighborhood nodes and the distinct types of relationships between nodes. In addition, we introduce the self-attention mechanism to acquire the high-level global representation for web service classification. Various experiments demonstrate that Graph4Web has better classification performance compared with seven baseline methods with three indicators. © 2022 Elsevier Inc.","Attention mechanism; Graph neural network; Service discovery; Web services"
"Fuzzing with automatically controlled interleavings to detect concurrency bugs","2022","Journal of Systems and Software","10.1016/j.jss.2022.111379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131423083&doi=10.1016%2fj.jss.2022.111379&partnerID=40&md5=41b8dbb6697565e6d6fc53f6e0bb6163","Concurrency vulnerabilities are an irresistible threat to security, and detecting them is challenging. Triggering the concurrency vulnerabilities requires a specific thread interleaving and a bug-inducing input. Existing methods have focused on one of these but not both or have experimented with small programs, which raises scalability issues. This paper introduces AutoInter-fuzzing, a fuzzer controlling thread interleavings elaborately and providing an interleaving-aware power schedule to detect vulnerabilities in a multi-threaded program. AutoInter-fuzzing consists of static analysis and dynamic fuzzing. At the static analysis, the fuzzer extracts and optimizes the interleaving space to be explored and adds instrumentation to control thread interleavings. We apply the power schedule in the dynamic fuzzing to focus on the seeds that reveal the new interleaving space. The fuzzer records the interleaving information in a log when a crash occurs and uses it to reproduce and validate the crash. Experiments with 13 real-world multi-threaded programs show that the interleaving-aware power schedule effectively enlarges the untested interleaving space, and AutoInter-fuzzing outperforms AFL and ConAFL in detecting interleaving-relevant vulnerabilities. AutoInter-fuzzing has detected six interleaving-relevant vulnerabilities, including two new vulnerabilities and four interleaving-irrelevant vulnerabilities. © 2022","Bug detection; Concurrency vulnerabilities; Fuzzing; Multi-threading; Reliability"
"An empirical characterization of software bugs in open-source Cyber–Physical Systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133458370&doi=10.1016%2fj.jss.2022.111425&partnerID=40&md5=fdb3f83f11fb1fbf43e02bba5dc74be9","Background: Cyber-Physical Systems (CPSs) are systems in which software and hardware components interact with each other. Understanding the specific nature and root cause of CPS bugs would help to design better verification and validation (V&V) techniques for these systems such as domain-specific mutants. Aim: We look at CPS bugs from an open-source perspective, trying to understand what kinds of bugs occur in a set of open-source CPSs belonging to different domains. Method: We analyze 1151 issues from 14 projects related to drones, automotive, robotics, and Arduino. We apply a hybrid card-sorting procedure to create a taxonomy of CPS bugs, by extending a previously proposed taxonomy specific to the automotive domain. Results: We provide a taxonomy featuring 22 root causes, grouped into eight high-level categories. Our qualitative and quantitative analyses suggest that 33.4% of the analyzed bugs occurring in CPSs are peculiar to those and, consequently, require specific care during verification and validation activities. Conclusion: The taxonomy provides an overview of the root causes related to bugs found in open-source CPSs belonging to different domains. Such root causes are related to different components of a CPS, including hardware, interface, configuration, network, data, and application logic. © 2022 Elsevier Inc.","Bugs; Cyber-Physical Systems; Defects taxonomy; Open-source"
"Testing anticipatory systems: A systematic mapping study on the state of the art","2022","Journal of Systems and Software","10.1016/j.jss.2022.111387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134641034&doi=10.1016%2fj.jss.2022.111387&partnerID=40&md5=77b86d05d7c000808ee660ed9ba08212","Context: Systems exhibiting anticipatory behavior are controlling devices that are influencing decisions critical to business with increasing frequency, but testing such systems has received little attention from the artificial intelligence or software engineering communities. Goal: In this article, we describe research activities being carried out to test anticipatory systems and explore how this research contributes to the body of knowledge. In addition, we review the types of addressed anticipatory applications and point out open issues and trends. Method: This systematic mapping study was conducted to classify and analyze the literature on testing anticipatory systems, enabling us to highlight the most relevant topics and potential gaps in this field. Results: We identified 206 studies that contribute to the testing of systems that exhibit anticipatory behavior. The papers address testing at stages such as context sensing, inferring higher-level concepts from the sensed data, predicting the future context, and intelligent decision-making. We also identified agent testing as a trend, among others. Conclusion: The existing literature on testing anticipatory systems has originated from various research communities, such as those on autonomous agents and quality engineering. Although researchers have recently exhibited increasing interest in testing anticipatory systems, theoretical knowledge about testing such systems is lacking. © 2022 The Author(s)","Anticipatory systems; Artificial intelligence; Mapping study; Software testing; Validation; Verification"
"On the use of artificial intelligence to deal with privacy in IoT systems: A systematic literature review","2022","Journal of Systems and Software","10.1016/j.jss.2022.111475","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136467284&doi=10.1016%2fj.jss.2022.111475&partnerID=40&md5=9e3462d0a71d4fe859f0ab8331727169","The Internet of Things (IoT) refers to a network of Internet-enabled devices that can make different operations, like sensing, communicating, and reacting to changes arising in the surrounding environment. Nowadays, the number of IoT devices is already higher than the world population. These devices operate by exchanging data between them, sometimes through an intermediate cloud infrastructure, and may be used to enable a wide variety of novel services that can potentially improve the quality of life of billions of people. Nonetheless, all that glitters is not gold: the increasing adoption of IoT comes with several privacy concerns due to the lack or loss of control over the sensitive data exchanged by these devices. This represents a key challenge for software engineering researchers attempting to address those privacy concerns by proposing (semi-)automated solutions to identify sources of privacy leaks. In this respect, a notable trend is represented by the adoption of smart solutions, that is, the definition of techniques based on artificial intelligence (AI) algorithms. This paper proposes a systematic literature review of the research in smart detection of privacy concerns in IoT devices. Following well-established guidelines, we identify 152 primary studies that we analyze under three main perspectives: (1) What are the privacy concerns addressed with AI-enabled techniques; (2) What are the algorithms employed and how they have been configured/validated; and (3) Which are the domains targeted by these techniques. The key results of the study identified six main tasks targeted through the use of artificial intelligence, like Malware Detection or Network Analysis. Support Vector Machine is the technique most frequently used in literature, however in many cases researchers do not explicitly indicate the domain where to use artificial intelligence algorithms. We conclude the paper by distilling several lessons learned and implications for software engineering researchers. © 2022","Artificial intelligence; Data privacy; Internet-of-Things; Software engineering for IoT"
"VulSlicer: Vulnerability detection through code slicing","2022","Journal of Systems and Software","10.1016/j.jss.2022.111450","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135387191&doi=10.1016%2fj.jss.2022.111450&partnerID=40&md5=105df97aa8853b1812b8f44b29b48566","There has been a multitude of techniques proposed for identifying vulnerabilities in software. Forcing a program into a vulnerable state has become increasingly unscalable, given the size of the programs and the number of possible execution states. At the same time, techniques that are looking for vulnerability signatures are marred with weak and incomplete signatures. This is not to say that such techniques have failed to identify previously unknown vulnerabilities in the code. However, they have inherent weaknesses, which result in identifying vulnerabilities that are limited in type and complexity. We propose a novel technique to extract succinct vulnerability-relevant statements representing the self-contained nature of vulnerabilities and reproduce the vulnerable behavior independently of the rest of the program. We also introduce an innovative technique to slice target programs and search for similar vulnerability-relevant statements in them. We developed VulSlicer, a prototype system capable of extracting vulnerability-relevant statements from vulnerable programs and searching for them on target programs at scale. Furthermore, we have examined four candidate open-source projects and have been able to identify 118 potential vulnerabilities, out of which 94 were found to be silently patched, and from the remaining reported cases, three were confirmed by obtaining a CVE designation. © 2022 Elsevier Inc.","Code slicing; Static analysis; Vulnerability detection"
"A Monte Carlo tree search conceptual framework for feature model analyses","2023","Journal of Systems and Software","10.1016/j.jss.2022.111551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141531929&doi=10.1016%2fj.jss.2022.111551&partnerID=40&md5=3729043ca611de8c505bc6fd663f8fe4","Challenging domains of the future such as Smart Cities, Cloud Computing, or Industry 4.0 expose highly variable systems with colossal configuration spaces. The automated analysis of those systems’ variability has often relied on SAT solving and constraint programming. However, many of the analyses have to deal with the uncertainty introduced by the fact that undertaking an exhaustive exploration of the whole configuration space is usually intractable. In addition, not all analyses need to deal with the configuration space of the feature models, but with different search spaces where analyses are performed over the structure of the feature models, the constraints, or the implementation artifacts, instead of configurations. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo tree search methods, which have proven to succeed in vast search spaces (e.g., game theory, scheduling tasks, security, program synthesis, etc.). Our general framework is formally described, and its flexibility to cope with a diversity of analysis problems is discussed. We provide a Python implementation of the framework that shows the feasibility of our proposal, identifying up to 11 lessons learned, and open challenges about the usage of the Monte Carlo methods in the software product line context. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state-of-the-art one step forward. © 2022 Elsevier Inc.","Automated analysis; Configurable systems; Feature models; Monte Carlo tree search; Software product lines; Variability"
"Smells and refactorings for microservices security: A multivocal literature review","2022","Journal of Systems and Software","10.1016/j.jss.2022.111393","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132709336&doi=10.1016%2fj.jss.2022.111393&partnerID=40&md5=09b2abaf1ac557d5ef96d15d43319ee3","Context: Securing microservices is crucial, as many IT companies are delivering their businesses through microservices. If security “smells” affect microservice-based applications, they can possibly suffer from security leaks and need to be refactored to mitigate the effects of security smells therein. Objective: As the available knowledge on securing microservices is scattered across different pieces of white and grey literature, our objective here is to distill well-known smells for securing microservices, together with the refactorings enabling to mitigate their effects. Method: To capture the state of the art and practice in securing microservices, we conducted a multivocal review of the existing white and grey literature on the topic. We systematically analysed 58 primary studies, selected among those published from 2011 until the end of 2020. Results: Ten bad smells for securing microservices are identified, which we organized in a taxonomy, associating each smell with the security properties it may violate and the refactorings enabling to mitigate its effects. Conclusions: The security smells and the corresponding refactorings have pragmatic value for practitioners, who can exploit them in their daily work on securing microservices. They also serve as a starting point for researchers wishing to establish new research directions on securing microservices. © 2022 Elsevier Inc.",""
"Automated test-based learning and verification of performance models for microservices systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124274286&doi=10.1016%2fj.jss.2022.111225&partnerID=40&md5=5f4a1691c78038edaca0ea24e420f6f2","Effective and automated verification techniques able to provide assurances of performance and scalability are highly demanded in the context of microservices systems. In this paper, we introduce a methodology that applies specification-driven load testing to learn the behavior of the target microservices system under multiple deployment configurations. Testing is driven by realistic workload conditions sampled in production. The sampling produces a formal description of the users’ behavior through a Discrete Time Markov Chain. This model drives multiple load testing sessions that query the system under test and feed a Bayesian inference process which incrementally refines the initial model to obtain a complete specification from run-time evidence as a Continuous Time Markov Chain. The complete specification is then used to conduct automated verification by using probabilistic model checking and to compute a configuration score that evaluates alternative deployment options. This paper introduces the methodology, its theoretical foundation, and the toolchain we developed to automate it. Our empirical evaluation shows its applicability, benefits, and costs on a representative microservices system benchmark. We show that the methodology detects performance issues, traces them back to system-level requirements, and, thanks to the configuration score, provides engineers with insights on deployment options. The comparison between our approach and a selected state-of-the-art baseline shows that we are able to reduce the cost up to 73% in terms of number of tests. The verification stage requires negligible execution time and memory consumption. We observed that the verification of 360 system-level requirements took ∼1 minute by consuming at most 34 KB. The computation of the score involved the verification of ∼7k (automatically generated) properties verified in ∼72 seconds using at most ∼50 KB. © 2022 The Author(s)","Automated verification; Markov models; Microservices; Performance testing; Test-based model learning"
"Selenium-Jupiter: A JUnit 5 extension for Selenium WebDriver","2022","Journal of Systems and Software","10.1016/j.jss.2022.111298","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126631924&doi=10.1016%2fj.jss.2022.111298&partnerID=40&md5=c12a6e93a89f7542efa5767b00d665fc","Selenium WebDriver is a library that allows controlling web browsers (e.g., Chrome, Firefox, etc.) programmatically. It provides a cross-browser programming interface in several languages used primarily to implement end-to-end tests for web applications. JUnit is a popular unit testing framework for Java. Its latest version (i.e., JUnit 5) provides a programming and extension model called Jupiter. This paper presents Selenium-Jupiter, an open-source JUnit 5 extension for Selenium WebDriver. Selenium-Jupiter aims to ease the development of Selenium WebDriver tests thanks to an automated driver management process implemented in conjunction with the Jupiter parameter resolution mechanism. Moreover, Selenium-Jupiter provides seamless integration with Docker, allowing the use of different web browsers in Docker containers out of the box. This feature enables cross-browser testing, load testing, and troubleshooting (e.g., configurable session recordings). This paper presents an example case in which Selenium-Jupiter is used to evaluate the performance of video conferencing systems based on WebRTC. This example case shows that Selenium-Jupiter can build and maintain the required infrastructure for complex tests effortlessly. © 2022 Elsevier Inc.","Automated testing tools; Browser automation; Docker; End-to-end testing; JUnit; Selenium WebDriver"
"ARTe: Providing real-time multitasking to Arduino","2022","Journal of Systems and Software","10.1016/j.jss.2021.111185","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122514814&doi=10.1016%2fj.jss.2021.111185&partnerID=40&md5=7b912da709901e2db20c7f6ffb4f26c6","In the last decade, thanks to its modular hardware and straightforward programming model, the Arduino ecosystem became a reference for learning the development of embedded systems by various users, ranging from amateurs and students to makers. However, while the latest released platforms are equipped with modern microcontrollers, the programming model is still tied to a single-threaded, legacy approach. This limits the exploitation of the underlying hardware platform and poses limitations in new application scenarios, such as IoT and UAVs. This paper presents the Arduino real-time extension (ARTe), which seamlessly extends the Arduino programming model to enable the concurrent execution of multiple loops at different rates configurable by the programmer. This is obtained by embedding a low-footprint, real-time operating system in the Arduino framework. The adherence to the original programming model, together with the hidden support for managing the inherent complexity of concurrent software, allows expanding the applicability of the Arduino framework while ensuring a more efficient usage of the computational resources. Furthermore, the proposed approach allows a finer control of the latencies and the energy consumption. Experimental results show that such advantages are obtained at the cost of a small additional overhead and memory footprint. To highlight the benefits introduced by ARTe, the paper finally presents two case studies, one of such in which ARTe has been leveraged to rapidly prototype a mechanical ventilator for acute COVID-19 cases. We found that ARTe allowed our ventilator design to rapidly adapt to changes in the available components and to the evolving needs of Intensive Care Units (ICU) in the Americas. © 2021 Elsevier Inc.","Arduino; Educational; Multi-tasking; Real-time"
"Understanding security vulnerabilities in student code: A case study in a non-security course","2022","Journal of Systems and Software","10.1016/j.jss.2021.111150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121237420&doi=10.1016%2fj.jss.2021.111150&partnerID=40&md5=587d2b064c8740345d8d2a2644cc29ae","Secure coding education is quite important for students to acquire the skills to quickly adapt to the evolving threats towards the software they are expected to create once they graduate. Educators are also more aware of this situation and incorporate teaching security in their respective fields. An effective application of this is only possible by cultivating the teaching and learning perspectives. Understanding the security awareness and practice of students is useful as an initial step to create a baseline for teaching methods and content. In this paper, we first survey to investigate how students approach security and what could motivate them to learn and apply security practices. Then, we analyze the source code for 6 semesters of coding assignments for 2 tasks using a source code vulnerability analysis tool. In our analysis, we report the types of vulnerabilities and various aspects between them while incorporating the effect of student grades. We then explore the lexical and structural features of security in student code using data analysis and machine learning. For the lexical analysis, we build a classifier to extract informative features and for the structural analysis, we utilize Syntax Trees to represent code and perform clustering in terms of structural features where clusters themselves yield different vulnerability levels. © 2021","Data mining; Secure coding education; Source code analysis; Vulnerability analysis"
"Feature-based software design pattern detection","2022","Journal of Systems and Software","10.1016/j.jss.2021.111179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121212481&doi=10.1016%2fj.jss.2021.111179&partnerID=40&md5=ce9a29a5caa9b86b62190eb575e815d4","Software design patterns are standard solutions to common problems in software design and architecture. Knowing that a particular module implements a design pattern is a shortcut to design comprehension. Manually detecting design patterns is a time consuming and challenging task, therefore, researchers have proposed automatic design pattern detection techniques. However, these techniques show low performance for certain design patterns. In this work, we introduce a design pattern detection approach, DPDF that improves the performance over the state-of-the-art by using code features with machine learning classifiers to automatically train a design pattern detector. DPDF creates a semantic representation of Java source code using the code features and the call graph, and applies the Word2Vec algorithm on the semantic representation to construct the word-space geometric model of the Java source code. DPDF then builds a machine learning classifier trained on a labelled dataset and identifies software design patterns with over 80% Precision and over 79% Recall. Additionally, we have compared DPDF with two existing design pattern detection techniques namely FeatureMaps & MARPLE-DPD. Empirical results demonstrate that our approach outperforms the existing approaches by approximately 35% and 15% respectively in terms of Precision. The run-time performance also supports the practical applicability of our classifier. © 2021 Elsevier Inc.","Code features; Machine learning; Software design patterns; Word-space-model"
"CONSERVE: A framework for the selection of techniques for monitoring containers security","2022","Journal of Systems and Software","10.1016/j.jss.2021.111158","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121691498&doi=10.1016%2fj.jss.2021.111158&partnerID=40&md5=8fe2d286c3677c1ea69d0378826ac6bc","Context: Container-based virtualization is gaining popularity in different domains, as it supports continuous development and improves the efficiency and reliability of run-time environments. Problem: Different techniques are proposed for monitoring the security of containers. However, there are no guidelines supporting the selection of suitable techniques for the tasks at hand. Objective: We aim to support the selection and design of techniques for monitoring container-based virtualization environments. Approach:: First, we review the literature and identify techniques for monitoring containerized environments. Second, we classify these techniques according to a set of categories, such as technical characteristic, applicability, effectiveness, and evaluation. We further detail the pros and cons that are associated with each of the identified techniques. Result: As a result, we present CONSERVE, a multi-dimensional decision support framework for an informed and optimal selection of a suitable set of container monitoring techniques to be implemented in different application domains. Evaluation: A mix of eighteen researchers and practitioners evaluated the ease of use, understandability, usefulness, efficiency, applicability, and completeness of the framework. The evaluation shows a high level of interest, and points out to potential benefits. © 2021 The Authors","Attack analysis; Container monitoring; Intrusion detection; Security; Software and systems engineering; Virtualization"
"Exploring factors and metrics to select open source software components for integration: An empirical study","2022","Journal of Systems and Software","10.1016/j.jss.2022.111255","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125147388&doi=10.1016%2fj.jss.2022.111255&partnerID=40&md5=1819317382aaca977e465c6d5070ae96","Context: Open Source Software (OSS) is nowadays used and integrated in most of the commercial products. However, the selection of OSS projects for integration is not a simple process, mainly due to a of lack of clear selection models and lack of information from the OSS portals. Objective: We investigate the factors and metrics that practitioners currently consider when selecting OSS. We also investigate the source of information and portals that can be used to assess the factors, as well as the possibility to automatically extract such information with APIs. Method: We elicited the factors and the metrics adopted to assess and compare OSS performing a survey among 23 experienced developers who often integrate OSS in the software they develop. Moreover, we investigated the APIs of the portals adopted to assess OSS extracting information for the most starred 100K projects in GitHub. Result: We identified a set consisting of 8 main factors and 74 sub-factors, together with 170 related metrics that companies can use to select OSS to be integrated in their software projects. Unexpectedly, only a small part of the factors can be evaluated automatically, and out of 170 metrics, only 40 are available, of which only 22 returned information for all the 100K projects. Therefore, we recommend project maintainers and project repositories to pay attention to provide information for the project they are hosting, so as to increase the likelihood of being adopted. Conclusion: OSS selection can be partially automated, by extracting the information needed for the selection from portal APIs. OSS producers can benefit from our results by checking if they are providing all the information commonly required by potential adopters. Developers can benefit from our results, using the list of factors we selected as a checklist during the selection of OSS, or using the APIs we developed to automatically extract the data from OSS projects. © 2022 The Author(s)","Open source; Open source adoption; Software selection"
"Quantitative verification with adaptive uncertainty reduction","2022","Journal of Systems and Software","10.1016/j.jss.2022.111275","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125516392&doi=10.1016%2fj.jss.2022.111275&partnerID=40&md5=09e1ce45916e8db809e01f983dda5424","Stochastic models are widely used to verify whether systems satisfy their reliability, performance and other nonfunctional requirements. However, the validity of the verification depends on how accurately the parameters of these models can be estimated using data from component unit testing, monitoring, system logs, etc. When insufficient data are available, the models are affected by epistemic parametric uncertainty, the verification results are inaccurate, and any engineering decisions based on them may be invalid. To address these problems, we introduce VERACITY, a tool-supported iterative approach for the efficient and accurate verification of nonfunctional requirements under epistemic parameter uncertainty. VERACITY integrates confidence-interval quantitative verification with a new adaptive uncertainty reduction heuristic that collects additional data about the parameters of the verified model by unit-testing specific system components over a series of verification iterations. VERACITY supports the quantitative verification of discrete-time Markov chains, deciding which components are to be tested in each iteration based on factors that include the sensitivity of the model to variations in the parameters of different components, and the overheads (e.g., time or cost) of unit-testing each of these components. We show the effectiveness and efficiency of VERACITY by using it for the verification of the nonfunctional requirements of a tele-assistance service-based system and an online shopping web application. © 2022 Elsevier Inc.","Confidence intervals; Nonfunctional requirements; Probabilistic model checking; Quantitative verification; Uncertainty reduction; Unit testing"
"Quality measurement in agile and rapid software development: A systematic mapping","2022","Journal of Systems and Software","10.1016/j.jss.2021.111187","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121962392&doi=10.1016%2fj.jss.2021.111187&partnerID=40&md5=6d6e177215a3aa32fbd26ab7f14ca68f","Context: In despite of agile and rapid software development (ARSD) being researched and applied extensively, managing quality requirements (QRs) are still challenging. As ARSD processes produce a large amount of data, measurement has become a strategy to facilitate QR management. Objective: This study aims to survey the literature related to QR management through metrics in ARSD, focusing on: bibliometrics, QR metrics, and quality-related indicators used in quality management. Methods: The study design includes the definition of research questions, selection criteria, and snowballing as search strategy. Results: We selected 61 primary studies (2001–2019). Despite a large body of knowledge and standards, there is no consensus regarding QR measurement. Terminology is varying as are the measuring models. However, seemingly different measurement models do contain similarities. Conclusion: The industrial relevance of the primary studies shows that practitioners have a need to improve quality measurement. Our collection of measures and data sources can serve as a starting point for practitioners to include quality measurement into their decision-making processes. Researchers could benefit from the identified similarities to start building a common framework for quality measurement. In addition, this could help researchers identify what quality aspects need more focus, e.g., security and usability that have surprisingly few metrics reported. © 2021 The Author(s)","Agile software development; Metrics; Non-functional requirements; Quality indicators; Quality requirements; Rapid software development"
"A stochastic algorithm for scheduling bag-of-tasks applications on hybrid clouds under task duration variations","2022","Journal of Systems and Software","10.1016/j.jss.2021.111123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118511653&doi=10.1016%2fj.jss.2021.111123&partnerID=40&md5=ef6e38c6c5f822d11ca56aadca0af75f","Hybrid cloud computing, which typically involves a hybrid architecture of public and private clouds, is an ideal platform for executing bag-of-tasks (BoT) applications. Most existing BoT scheduling algorithms ignore the uncertainty of task execution times in practical scenarios and schedule tasks by assuming that the task durations can be determined accurately in advance, often leading to the violation of the deadline constraint. In view of this fact, this paper devotes to maximizing the profit of the private cloud provider while guaranteeing the quality-of-service provided by the cloud platform, through designing an effective stochastic BoT scheduling algorithm based on the distribution of task duration variations. With the varying task execution times modeled as random variables, we formulate a stochastic scheduling framework that incorporates a probabilistic constraint upon the makespans of BoT applications. The resultant stochastic optimization model is capable of characterizing the complete distribution information of makespan variations and satisfying the deadline constraint in a probabilistic sense. We further design an immune algorithm-based metaheuristic to solve this stochastic optimization problem. Simulations results justify that our proposed algorithm outperforms several competing algorithms in maximizing the cloud provider's profit while satisfying the user-specified deadline constraint under the impact of uncertain task durations. © 2021 Elsevier Inc.","Bag-of-tasks applications; Hybrid clouds; Probabilistic constraint; Profit maximization; Stochastic task scheduling"
"Architecture evaluation in continuous development","2022","Journal of Systems and Software","10.1016/j.jss.2021.111111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118546234&doi=10.1016%2fj.jss.2021.111111&partnerID=40&md5=3950a226168f88f5f6b312bb14fa53d6","Context: In automotive, stage-gate processes have previously been the norm, with architecture created mainly during an early phase and then used to guide subsequent development phases. Current iterative and Agile development methods, where the implementation evolves continuously, changes the role of architecture. Objective: We investigate how architecture evaluation can provide useful feedback during development of continuously evolving systems. Method: Starting from the Architecture Tradeoff Analysis Method (ATAM), we performed architecture evaluation, both in a national research project led by an automotive Original Equipment Manufacturer (OEM), and at the OEM, in the context of continuous development. This allows us to include the experience of several architects from different organizations over several years. Using data produced during the evaluations we perform a post-hoc analysis to derive initial findings. We then validate and refine these findings through a series of focus groups with architects and industry experts. Findings: We propose principles of continuous evaluation and evolution of architecture, and based on these discuss a roadmap for future research. Conclusion: In iterative development settings, the needs are different from what typical architecture evaluation methods provide. Our principles show the importance of dedicated feedback-loops for continuous evolution of systems and their architecture. © 2021 The Author(s)","Architecture evaluation; Continuous software engineering"
"Automated reverse engineering of role-based access control policies of web applications","2022","Journal of Systems and Software","10.1016/j.jss.2021.111109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118119903&doi=10.1016%2fj.jss.2021.111109&partnerID=40&md5=d096a204167f6e85a4c729af3486e2c8","Access control (AC) is an important security mechanism used in software systems to restrict access to sensitive resources. Therefore, it is essential to validate the correctness of AC implementations with respect to policy specifications or intended access rights. However, in practice, AC policy specifications are often missing or poorly documented; in some cases, AC policies are hard-coded in business logic implementations. This leads to difficulties in validating the correctness of policy implementations and detecting AC defects. In this paper, we present a semi-automated framework for reverse-engineering of AC policies from Web applications. Our goal is to learn and recover role-based access control (RBAC) policies from implementations, which are then used to validate implemented policies and detect AC issues. Our framework, built on top of a suite of security tools, automatically explores a given Web application, mines domain input specifications from access logs, and systematically generates and executes more access requests using combinatorial test generation. To learn policies, we apply machine learning on the obtained data to characterize relevant attributes that influence AC. Finally, the inferred policies are presented to the security engineer, for validation with respect to intended access rights and for detecting AC issues. Inconsistent and insufficient policies are highlighted as potential AC issues, being either vulnerabilities or implementation errors. We evaluated our approach on four Web applications (three open-source and a proprietary one built by our industry partner) in terms of the correctness of inferred policies. We also evaluated the usefulness of our approach by investigating whether it facilitates the detection of AC issues. The results show that 97.8% of the inferred policies are correct with respect to the actual AC implementation; the analysis of these policies led to the discovery of 64 AC issues that were reported to the developers. © 2021 Elsevier Inc.","Access control policies; Access control testing; Machine learning; Reverse engineering"
"Spreadsheet debugging: The perils of tool over-reliance","2022","Journal of Systems and Software","10.1016/j.jss.2021.111119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118112155&doi=10.1016%2fj.jss.2021.111119&partnerID=40&md5=00ee741f44b40f83bd307c7660dce9db","Spreadsheets are widely used in organizations for various purposes such as data aggregation, reporting and decision-making. Since spreadsheets, like other types of software, can contain faulty formulas, it is important to provide developers with appropriate methods to find and fix such faults. Recently, various heuristic and statistics-based fault identification methods were proposed, which point developers to potentially faulty parts of the spreadsheets. Due to their heuristic nature, these methods might, however, miss some faults. As a result, if spreadsheet developers rely too strongly on these methods, they might not pay sufficient attention to problems that are not pinpointed by the methods. In this research, we are the first to study this potential problem of over-reliance in spreadsheet debugging, which may lead to limited debugging effectiveness. We report the outcome of a controlled experiment where 59 participants were tasked to find faulty formulas in a given spreadsheet with and without support of a novel spreadsheet debugging tool. Our results indicate that tool over-reliance can indeed result as a phenomenon of using heuristic debugging techniques. However, the study also provides evidence that making users aware of potential tool limitations within the debugging environment may help to address this problem. Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2021 The Author(s)","Fault identification; Over-reliance; Spreadsheets; User study"
"A case study on the stability of performance tests for serverless applications","2022","Journal of Systems and Software","10.1016/j.jss.2022.111294","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126571137&doi=10.1016%2fj.jss.2022.111294&partnerID=40&md5=2b157a7689a70a90eb5b15ddf86a9036","Context: While in serverless computing, application resource management and operational concerns are generally delegated to the cloud provider, ensuring that serverless applications meet their performance requirements is still a responsibility of the developers. Performance testing is a commonly used performance assessment practice; however, it traditionally requires visibility of the resource environment. Objective: In this study, we investigate whether performance tests of serverless applications are stable, that is, if their results are reproducible, and what implications the serverless paradigm has for performance tests. Method: We conduct a case study where we collect two datasets of performance test results: (a) repetitions of performance tests for varying memory size and load intensities and (b) three repetitions of the same performance test every day for ten months. Results: We find that performance tests of serverless applications are comparatively stable if conducted on the same day. However, we also observe short-term performance variations and frequent long-term performance changes. Conclusion: Performance tests for serverless applications can be stable; however, the serverless model impacts the planning, execution, and analysis of performance tests. © 2022 Elsevier Inc.","FaaS; Function-as-a-Service; Performance; Reproducibility; Serverless; Stability"
"A sampling-based online Co-Location-Resistant Virtual Machine placement strategy","2022","Journal of Systems and Software","10.1016/j.jss.2022.111215","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123863928&doi=10.1016%2fj.jss.2022.111215&partnerID=40&md5=18ab0f73de561d789e7ba53592c24474","In this paper, we discuss the co-location attack problem in the cloud IaaS from the Virtual Machine (VM) placement strategy perspective. We formulate the online secure optimization VM placement problem in a way that guarantees—apriori—a specified level of security while minimizing the number of used physical servers. To solve such a problem, we propose an approximate online secure VM placement algorithm based on sampling. The polynomial-time algorithm is based on a sound security inference procedure based on the confidence interval estimation method. Our empirical results demonstrate the correctness and the effectiveness of our approach in guaranteeing a Co-Location-Resistant (CLR) VM placement with a specific level of confidence and a threshold error as new incoming VM requests are being assigned to servers online. We compared our algorithm to a CLR alternative presented in Azar et al. (2014). © 2022 Elsevier Inc.","Cloud security; Co-location attack; Co-Location-Resistant assignment; Resource optimization; Sampling; Virtual machines placement"
"How are framework code samples maintained and used by developers? The case of Android and Spring Boot","2022","Journal of Systems and Software","10.1016/j.jss.2021.111146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120895129&doi=10.1016%2fj.jss.2021.111146&partnerID=40&md5=63fb68f41e958eef10aebc978d806540","Modern software systems are commonly built on top of frameworks. To accelerate the learning process of features provided by frameworks, code samples are made available to assist developers. However, we know little about how code samples are developed and consumed. In this paper, we aim to fill this gap by assessing the characteristics of framework code samples. We provide insights into how code samples are maintained and used by developers. We analyze over 230 code samples provided by Android and Spring Boot, and assess aspects related to their code, evolution, popularity, and client usage. We find that most code samples are small and simple, provide a working environment for the clients, and rely on automated build tools. They frequently change, for example, to adapt to new framework versions. We also detect that clients commonly fork the code samples, however, they rarely modify them. To further understand the problems faced by developers, we analyze 614 Stack Overflow questions about the code samples and 269 issues from code sample repositories. We find that developers face problems when trying to modify the code samples and the most common issue is related to improvement. Finally, we propose implications to creators and clients of code samples to improve maintenance and usage activities. © 2021 Elsevier Inc.","Android; Code samples; Frameworks; Mining software repositories; Software maintenance; Spring Boot"
"BXtendDSL: A layered framework for bidirectional model transformations combining a declarative and an imperative language","2022","Journal of Systems and Software","10.1016/j.jss.2022.111288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126680668&doi=10.1016%2fj.jss.2022.111288&partnerID=40&md5=e83bc5db00cd99a60bbad11988f60542","Model-driven software development (MDSD) heavily relies on model transformations. While in a strict forward engineering process unidirectional transformations are used, bidirectional transformations are crucial as soon as roundtrip engineering comes into play. In this paper, we present a hybrid language specifically designed to describe bidirectional model transformations. From a declarative transformation specification code is generated which uses our framework for bidirectional and incremental model transformations. A sophisticated code generation mechanism allows for hooking into the generated transformation code at the imperative level to supply behavior that cannot be expressed declaratively. A thorough evaluation demonstrates conciseness, expressiveness, and scalability of our approach. © 2022 Elsevier Inc.","Bidirectional; Declarative; Imperative; Model transformations"
"Cataloging dependency injection anti-patterns in software systems","2022","Journal of Systems and Software","10.1016/j.jss.2021.111125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118586545&doi=10.1016%2fj.jss.2021.111125&partnerID=40&md5=d43e9e59c33852c7c592f7631cc82f54","Context: Dependency Injection (DI) is a commonly applied mechanism to decouple classes from their dependencies in order to provide higher modularization. However, bad DI practices often lead to negative consequences, such as increasing coupling. Although white literature conjectures about the existence of DI anti-patterns, there is no evidence on their practical relevance, usefulness, and generality. Objective: The objective of this study is to propose and evaluate a catalog of DI anti-patterns and associated refactorings. Methodology: We reviewed existing reported DI anti-patterns in order to analyze their completeness. The limitations found in literature motivated proposing a novel catalog of 12 DI anti-patterns. We developed a tool to statically analyze the occurrence level of the candidate DI anti-patterns in both open-source and industry projects. Next, we survey practitioners to assess their perception on the relevance, usefulness, and their willingness on refactoring anti-pattern instances of the catalog. Results: Our static code analyzer tool showed a relative recall of 92.19% and high average precision. It revealed that at least 9 different DI anti-patterns appeared frequently in the analyzed projects. Besides, our survey confirmed the perceived relevance of the catalog and developers expressed their willingness to refactor instances of anti-patterns from source code. Conclusion: The catalog contains DI anti-patterns that occur in practice and that are perceived as useful. Sharing it with practitioners may help them to avoid such anti-patterns, thus improving source-code quality. © 2021 The Authors","Anti-pattern; Dependency injection; Dependency inversion; Inversion of control"
"Can we trust tests to automate dependency updates? A case study of Java Projects","2022","Journal of Systems and Software","10.1016/j.jss.2021.111097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117253900&doi=10.1016%2fj.jss.2021.111097&partnerID=40&md5=019910336213f1ca9674d9c947e5e56c","Developers are increasingly using services such as Dependabot to automate dependency updates. However, recent research has shown that developers perceive such services as unreliable, as they heavily rely on test coverage to detect conflicts in updates. To understand the prevalence of tests exercising dependencies, we calculate the test coverage of direct and indirect uses of dependencies in 521 well-tested Java projects. We find that tests only cover 58% of direct and 21% of transitive dependency calls. By creating 1,122,420 artificial updates with simple faults covering all dependency usages in 262 projects, we measure the effectiveness of test suites in detecting semantic faults in dependencies; we find that tests can only detect 47% of direct and 35% of indirect artificial faults on average. To increase reliability, we investigate the use of change impact analysis as a means of reducing false negatives; on average, our tool can uncover 74% of injected faults in direct dependencies and 64% for transitive dependencies, nearly two times more than test suites. We then apply our tool in 22 real-world dependency updates, where it identifies three semantically conflicting cases and three cases of unused dependencies that tests were unable to detect. Our findings indicate that the combination of static and dynamic analysis should be a requirement for future dependency updating systems. © 2021 The Author(s)","Dependency management; Library updates; Package management; Semantic versioning; Software migration"
"Understanding students’ software development projects: Effort, performance, satisfaction, skills and their relation to the adequacy of outcomes developed","2022","Journal of Systems and Software","10.1016/j.jss.2021.111156","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122470522&doi=10.1016%2fj.jss.2021.111156&partnerID=40&md5=a1e033de9b314d93cb7449ce53545514","Given the inclusion of (often team-based) course projects in tertiary software engineering education, it is necessary to investigate software engineering curricula and students’ experiences while undergoing their software engineering training. Previous research efforts have not sufficiently explored students perceptions around the commitment and adequacy of effort spent on software engineering projects, their project performance and skills that are developed during course projects. This gap in skills awareness includes those that are necessary, anticipated and learned, and the challenges to student project success, which may predict project performance. Such insights could inform curricula design, theory and practice, in terms of improving post-study software development success. We conducted a survey involving undergraduate across four universities in New Zealand and Cyprus to explore these issues, where extensive deductive and inductive analyses were performed. Among our findings we observe that students’ commitment of effort on software engineering project seems appropriate. Students are more satisfied with their team's collaboration performance than technical contributions, but we found that junior students seemed to struggle with teamwork. Further, we observe that the software students developed were of higher quality if they had worked in project teams previously, had stronger technical skills and were involved in timely meetings. This study singles out mechanisms for informing good estimation of effort, mentoring technical competencies and suitable coaching for enhancing project success and student learning. © 2021 Elsevier Inc.","Course project effort; Project challenges; Software engineering skills; Student performance; Student satisfaction"
"The state of research on software engineering competencies: A systematic mapping study","2022","Journal of Systems and Software","10.1016/j.jss.2021.111183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121442183&doi=10.1016%2fj.jss.2021.111183&partnerID=40&md5=72d89db8fbe0056185390ef38bfe01c1","Considering the critical role of software in modern societies, we face an urgent need to educate more competent software professionals. Software engineering competencies (SEC) are considered the backbone of successfully developing software products. Consequently, SEC has become a hotspot for software engineering research and practice. Although scientific literature on SEC is not lacking, to our knowledge, a comprehensive overview of the current state of SEC research is missing. To that end, we conducted an extensive and systematic review of the SEC literature. We provide an overview of the current state of research on SEC, with a particular focus on common SEC research areas. In addition to reporting the available SEC models and frameworks, we compile a list of 49 unique essential competencies of software professionals. Finally, we highlight several gaps in the literature that deserve further research. In particular, we call for a better understanding of how the essential competencies of software professionals change over time, as well as fresh accounts of the essential competencies of software professionals. Additionally, considering recent shifts toward Agile and DevOps methods, future research must explore the competencies required for developing software products in modern development environments. © 2021 The Author(s)","Competence; Competencies; Essential competencies; Mapping study; Software development; Software engineering; Systematic Literature Review"
"E-SC4R: Explaining Software Clustering for Remodularisation","2022","Journal of Systems and Software","10.1016/j.jss.2021.111162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121611444&doi=10.1016%2fj.jss.2021.111162&partnerID=40&md5=3fc21df665dd81fdf14c5d9138a20ca9","Maintenance of existing software requires a large amount of time for comprehending the source code. The architecture of a software, however, may not be clear to maintainers if up-to-date documentations are not available. Software clustering is often used as a remodularisation and architecture recovery technique to help recover a semantic representation of the software design. Due to the diverse domains, structure, and behaviour of software systems, the suitability of different clustering algorithms for different software systems are not investigated thoroughly. Research that introduce new clustering techniques usually validate their approaches on a specific domain, which might limit its generalisability. If the chosen test subjects could only represent a narrow perspective of the whole picture, researchers might risk not being able to address the external validity of their findings. This work aims to fill this gap by introducing a new approach, Explaining Software Clustering for Remodularisation (E-SC4R), to evaluate the effectiveness of different software clustering approaches. This work focuses on hierarchical clustering and Bunch clustering algorithms and provides information about their suitability according to the features of the software, which as a consequence, enables the selection of the most suitable algorithm and configuration that can achieve the best MoJoFM value from our existing pool of choices for a particular software system. The E-SC4R framework is tested on 30 open-source software systems with varying sizes and domains, and demonstrates that it can characterise both the strengths and weaknesses of the analysed software clustering algorithms using software features extracted from the code. The proposed approach also provides a better understanding of the algorithms’ behaviour by showing a 2D representation of the effectiveness of clustering techniques on the feature space generated through the application of dimensionality reduction techniques. © 2021 Elsevier Inc.","Architecture recovery; Feature extraction; Footprint visualisation; Software clustering; Software remodularisation"
"Breaking the vicious circle: A case study on why AI for software analytics and business intelligence does not take off in practice","2022","Journal of Systems and Software","10.1016/j.jss.2021.111135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119991290&doi=10.1016%2fj.jss.2021.111135&partnerID=40&md5=89a7ed2b6b11a2e95db84791e2ba8bdc","In recent years, the application of artificial intelligence (AI) has become an integral part of a wide range of areas, including software engineering. By analyzing various data sources generated in software engineering, it can provide valuable insights into customer behavior, product performance, bugs and errors, and many more. In practice, however, AI for software analytics and business intelligence often remains at a prototypical stage, and the results are rarely used to make decisions based on data. To understand the underlying causes of this phenomenon, we conduct an explanatory case study consisting of and interview study and a survey on the challenges of realizing and utilizing artificial intelligence in the context of software-intensive businesses. As a result, we identify a vicious circle that prevents practitioners from moving from prototypical AI-based analytics to continuous and productively usable software analytics and business intelligence solutions. In order to break the vicious circle in a targeted manner, we identify a set of solutions based on existing literature as well as the previously conducted interviews and survey. Finally, these solutions are validated by a focus group of experts. © 2021 Elsevier Inc.","Artificial intelligence; Business intelligence; Data analytics; Data-driven software engineering; Software analytics"
"SEXTAMT: A systematic map to navigate the wide seas of factors affecting expert judgment software estimates","2022","Journal of Systems and Software","10.1016/j.jss.2021.111148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120717377&doi=10.1016%2fj.jss.2021.111148&partnerID=40&md5=5c13b4073e0fff3deb300663f71f8914","Context: Software projects involve technical and managerial activities, including software estimation. Inaccurate estimates are harmful and improving estimation methods is not enough: we need to understand more of the factors that impact estimates. Objective: Our study aims to identify the existing evidence about the factors that affect estimates in software projects when using expert judgment. Method: We executed a Systematic Literature Mapping (SLM) based on database and snowballing searches, selecting papers by first reading their titles and abstracts and later reading the full text. Results: Researchers investigated a wide range of different factors employing mostly laboratory research strategies and relying primarily on differences of estimates and participants’ perceptions to measure the factors’ effects. Resulting from our analysis, we present the SEXTAMT (Software Estimates of eXperts: A Map of influencing facTors), a map of factors affecting estimates built on three dimensions: project/iteration phase, stakeholders, and type of effect. Conclusion: Over the years, researchers have investigated a varied set of factors. Many of them were explored in different studies, employing diverse research strategies. Such studies provide compelling evidence on the elements that influence expert judgment estimates, which can be used to assess and improve everyday estimation in the software industry. © 2021 Elsevier Inc.","Expert judgment; Software effort estimation"
"Interpretability application of the Just-in-Time software defect prediction model","2022","Journal of Systems and Software","10.1016/j.jss.2022.111245","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125133781&doi=10.1016%2fj.jss.2022.111245&partnerID=40&md5=b7061360cc6cc8668a1ec01746bb2a0e","Software defect prediction is one of the most active fields in software engineering. Recently, some experts have proposed the Just-in-time Defect Prediction Technology. Just-in-time Defect prediction technology has become a hot topic in defect prediction due to its directness and fine granularity. This technique can predict whether a software defect exists in every code change submitted by a developer. In addition, the method has the advantages of high speed and easy tracking. However, the biggest challenge is that the prediction accuracy of Just-in-Time software is affected by the data set category imbalance. In most cases, 20% of defects in software engineering may be in 80% of modules, and code changes that do not cause defects account for a large proportion. Therefore, there is an imbalance in the data set, that is, the imbalance between a few classes and a majority of classes, which will affect the classification prediction effect of the model. Furthermore, because most features do not result in code changes that cause defects, it is not easy to achieve the desired results in practice even though the model is highly predictive. In addition, the features of the data set contain many irrelevant features and redundant features, which are invalid data, which will increase the complexity of the prediction model and reduce the prediction efficiency. To improve the prediction efficiency of Just-in-Time defect prediction technology. We trained a just-in-time defect prediction model using six open source projects from different fields based on random forest classification. LIME Interpretability technique is used to explain the model to a certain extent. By using explicable methods to extract meaningful, relevant features, the experiment can only need 45% of the original work to explain the prediction results of the prediction model and identify critical features through explicable techniques, and only need 96% of the original work to achieve this goal, under the premise of ensuring specific prediction effects. Therefore, the application of interpretable techniques can significantly reduce the workload of developers and improve work efficiency. © 2022 Elsevier Inc.","Interpretability; Interpretation method; Just-in-Time defect prediction; Software metrics"
"Evaluating the performance of clone detection tools in detecting cloned co-change candidates","2022","Journal of Systems and Software","10.1016/j.jss.2022.111229","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123855654&doi=10.1016%2fj.jss.2022.111229&partnerID=40&md5=397ed4c882863976dc40d3315798670a","Co-change candidates are the group of code fragments that require a change if any of these fragments experience a modification in a commit operation during software evolution. The cloned co-change candidates are a subset of the co-change candidates, and the members in this subset are clones of one another. The cloned co-change candidates are usually created by reusing existing code fragments in a software system. Detecting cloned co-change candidates is essential for clone-tracking, and studies have shown that we can use clone detection tools to find cloned co-change candidates. However, although several studies evaluate clone detection tools for their accuracy in detecting cloned fragments, we found no study that evaluates clone detection tools for detecting cloned co-change candidates. In this study, we explore the dimension of code clone research for detecting cloned co-change candidates. We compare the performance of 12 different configurations of nine promising clone detection tools in identifying cloned co-change candidates from eight open-source C and Java-based subject systems of various sizes and application domains. A ranked list and analysis of the results provides valuable insights and guidelines into selecting and configuring a clone detection tool for identifying co-change candidates and leads to a new dimension of code clone research into change impact analysis. © 2022","Clone detection; Cloned co-change candidates; Commit operation; Software maintenance and evolution"
"How has design thinking being used and integrated into software development activities? A systematic mapping","2022","Journal of Systems and Software","10.1016/j.jss.2022.111217","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123700257&doi=10.1016%2fj.jss.2022.111217&partnerID=40&md5=35d34e59111c23f201d09811e19793e5","Software companies have been using Design Thinking (DT) as a user-centered design approach, putting the user at the center of the software development process. In this article, we report a Systematic Mapping Study to investigate the use of DT in software development. We evaluated 127 papers from 2010 to 2021. We analyzed how DT is integrated in software development, what are the models and techniques, what are the criteria used for selecting DT techniques, and what are the key points that DT practitioners should be aware of when using DT. As a result, we identified 3 strategies to integrate DT in software development, 16 models, and 85 techniques. We also found that the selection of techniques is related to the models’ working spaces being performed, and identified 7 criteria used for selecting DT techniques. Furthermore, we summarized 16 key points that DT practitioners should pay attention when using DT, and we proposed 4 takeaways for applying DT in software development Thus, our study contributes to DT practitioners by providing information to be used either as a starting point, or to integrate it into activities already performed by teams, or as a strategy to foster changes in the entire organization's mindset. © 2022 Elsevier Inc.","Design thinking; Literature review; Software development; Systematic mapping study; User-centered design"
"Moderator factors of software security and performance verification","2022","Journal of Systems and Software","10.1016/j.jss.2021.111137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119425405&doi=10.1016%2fj.jss.2021.111137&partnerID=40&md5=632fbd06ad7e6596d7cc0fd0c002c35a","Context: Security and performance are critical software non-functional requirements. Therefore, verification activities should be included in the development process to identify related defects, avoiding failures after deployment. However, there is a lack of understanding on factors moderating the security and performance verification, which jeopardizes organizations to improve their verification activities to assure the releasing of software fulfilling these requirements. Objective: To identify moderator factors influencing security and performance verification and actions to promote them. Methods: Case study to identify security and performance moderators factors. Rapid Literature Reviews with Snowballing to strengthen moderator factors confidence. Practitioners Survey to classify the moderator factors relevance. Results: Identification of eight security and performance moderator factors regarding organizational awareness, cross-functional team, suitable requirements, support tools, verification environment, verification methodology, verification planning, and reuse practices. Rapid Reviews confirmed the moderator factors and revealed actions to promote each. A survey with 37 practitioners allowed us to classify the moderator factors and their actions regarding their relevancy. Conclusions: The moderator factors can be considered key points to software development organizations implement/improve security and performance verification activities in regular software systems. Further investigation is necessary to support the understanding of these moderator factors when building modern software systems. © 2021 Elsevier Inc.","Evidence-based software engineering; Performance; Security; Software testing; Software verification"
"Detecting violations of access control and information flow policies in data flow diagrams","2022","Journal of Systems and Software","10.1016/j.jss.2021.111138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119376943&doi=10.1016%2fj.jss.2021.111138&partnerID=40&md5=002435952487f2d1859afcc67dc40449","The security of software-intensive systems is frequently attacked. High fines or loss in reputation are potential consequences of not maintaining confidentiality, which is an important security objective. Detecting confidentiality issues in early software designs enables cost-efficient fixes. A Data Flow Diagram (DFD) is a modeling notation, which focuses on essential, functional aspects of such early software designs. Existing confidentiality analyses on DFDs support either information flow control or access control, which are the most common confidentiality mechanisms. Combining both mechanisms can be beneficial but existing DFD analyses do not support this. This lack of expressiveness requires designers to switch modeling languages to consider both mechanisms, which can lead to inconsistencies. In this article, we present an extended DFD syntax that supports modeling both, information flow and access control, in the same language. This improves expressiveness compared to related work and avoids inconsistencies. We define the semantics of extended DFDs by clauses in first-order logic. A logic program made of these clauses enables the automated detection of confidentiality violations by querying it. We evaluate the expressiveness of the syntax in a case study. We attempt to model nine information flow cases and six access control cases. We successfully modeled fourteen out of these fifteen cases, which indicates good expressiveness. We evaluate the reusability of models when switching confidentiality mechanisms by comparing the cases that share the same system design, which are three pairs of cases. We successfully show improved reusability compared to the state of the art. We evaluated the accuracy of confidentiality analyses by executing them for the fourteen cases that we could model. We experienced good accuracy. © 2021 The Author(s)","Access control; Data flow diagram; Information flow"
"Distributed runtime verification by past-CTL and the field calculus","2022","Journal of Systems and Software","10.1016/j.jss.2022.111251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124629641&doi=10.1016%2fj.jss.2022.111251&partnerID=40&md5=9e3b87a778a6b1628c7d4f675295d622","Recent trends in the engineering of software-intensive systems increasingly promote the adoption of computation at the edge of the network, in the proximity of where sensing and actuation are performed. Applications are executed directly in IoT devices deployed in the physical environment, possibly with the aid of edge servers: there, interactions are essentially based on physical proximity, and communication with the cloud is sporadic if not absent. The challenge of monitoring the execution of such system, by relying on local interactions only, naturally arises. We address this challenge by proposing a rigorous approach to distributed runtime monitoring for space-based networks of devices. We introduce the past-CTL logic, an extension of past-LTL able to express a variety of properties concerning the knowable past of an event. We formally define a procedure to derive, from a past-CTL formula, monitors that can be distributed on each device and whose collective behaviour verifies the validity of the formula at runtime across space and time. This is achieved by relying on the field calculus, a core programming language used to specify the behaviour of a collection of devices by viewing them as an aggregate computing machine, carrying out altogether a distributed computational process. The field calculus is shown to be a convenient language for our goals, since its functional composition approach provides a natural way of translating in a syntax-directed way properties expressed in a given logic into monitors for such properties. We show that the monitor process executing in each single device runs using local memory, message size, and computation time that are all linear in the size of the formula (1 bit per temporal connective). This matches the efficiency of the best available previous results for (non-distributed) monitors derived from past-LTL formulas. Finally, we empirically evaluate the applicability of the approach to sample problems in distributed computing, through simulated experiments with monitors written through a C++ library implementing the field calculus programming constructs. © 2022 Elsevier Inc.","Distributed systems; Field calculus; Runtime verification; Temporal logic"
"Automatic development of requirement linking matrix based on semantic similarity for robust software development","2022","Journal of Systems and Software","10.1016/j.jss.2021.111211","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122634295&doi=10.1016%2fj.jss.2021.111211&partnerID=40&md5=9f727a56152276ec54686db48522f4d4","With growing complexity of modern software, it is important that the relevant textual requirements are correctly linked into a ‘requirement liking matrix’ during early system development stages. The resulting requirement linking matrix highlights direct and indirect interactions between different requirements, thus facilitating improved design, development, and testing of complex software systems, e.g. automotive software, electrical/electronic architectures. The sheer volume of textual requirements collected in real-life coupled with data noises makes the task of automatic requirement linking a non-trivial exercise. In this paper, we propose a novel semantic similarity model for automatically linking different requirements to organize them into a requirement linking matrix. The model computes the similarity in terms of term-to-term, tuple-to-tuple, and text-to-text scores. The scores are ranked to determine whether the links are having “High”, “Low”, or “No” relationship with each other in a requirement linking matrix. The model is deployed as a prototype tool and its performance is validated by using the real-life data. We also compare our approach with the alternative approaches proposed in literature. The system achieved the average F1 score of 0.93 in correctly linking the heterogeneous requirements. © 2022 Elsevier Inc.","Automotive; Decision support; Requirement engineering; Requirement linking; Semantic similarity"
"Software professionals during the COVID-19 pandemic in Turkey: Factors affecting their mental well-being and work engagement in the home-based work setting","2022","Journal of Systems and Software","10.1016/j.jss.2022.111286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125814724&doi=10.1016%2fj.jss.2022.111286&partnerID=40&md5=bc0d689353cf59f91fad33322c14aa58","With the COVID-19 pandemic, strict measures have been taken to slow down the spread of the virus, and consequently, software professionals have been forced to work from home. However, home-based working entails many challenges, as the home environment is shared by the whole family simultaneously under pandemic conditions. The aim of this study is to explore software professionals’ mental well-being and work engagement and the relationships of these variables with job strain and resource-related factors in the forced home-based work setting during the COVID-19 pandemic. An online cross-sectional survey based on primarily well-known, validated scales was conducted with software professionals in Turkey. The analysis of the results was performed through hierarchical multivariate regression. The results suggest that despite the negative effect of job strain, the resource-related protective factors, namely, sleep quality, decision latitude, work-life balance, exercise predict mental well-being. Additionally, work engagement is predicted by job strain, sleep quality, and decision latitude. The results of the study will provide valuable insights to management of the software companies and professionals about the precautions that can be taken to have a better home-based working experience such as allowing greater autonomy and enhancing the quality of sleep and hence mitigating the negative effects of pandemic emergency situations on software professionals’ mental well-being and work engagement. © 2022 Elsevier Inc.","COVID-19; Cross-sectional survey; Home-based working; Mental well-being; Software professionals; Work engagement"
"A novel load balancing scheme for mobile edge computing","2022","Journal of Systems and Software","10.1016/j.jss.2021.111195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123200338&doi=10.1016%2fj.jss.2021.111195&partnerID=40&md5=35e43df166478fdd7f663a957c0bafec","To overcome long propagation delays for data exchange between the remote cloud data center and end devices in Mobile Cloud Computing (MCC), Mobile Edge Computing (MEC) is emerging to push mobile computing, network control and storage to the network edges. A cloudlet in MEC is a mobility-enhanced small-scale cloud, which contains several MEC servers located in close proximity to mobile devices. The main purpose of a cloudlet is to stably provide services to mobile devices with low latency. When a cloudlet offers hundreds kinds of services to millions of mobile devices, it is critical to balance the loads so as to improve performance. In this paper, we propose a three-layer mobile hybrid hierarchical P2P (MHP2P) model as a cloudlet. MHP2P performs satisfactory service lookup efficiency and system scalability as well as high stability. More importantly, a load balance scheme is provided so that the loads of MHP2P can be well balanced with the increasing of MEC servers and query load. A large number of simulation experiments indicate that the proposed scheme is efficient for enhancing the load balancing performance in MHP2P based MEC systems. © 2021 Elsevier Inc.","Cloudlet; Load balance; MHP2P; Mobile edge computing"
"Clone detection through srcClone: A program slicing based approach","2022","Journal of Systems and Software","10.1016/j.jss.2021.111115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118574720&doi=10.1016%2fj.jss.2021.111115&partnerID=40&md5=60e2ad33125068d3f9054e65ae800179","Software clone detection is an often used approach to understand and maintain software systems. One category of clones that is challenging to detect but very useful is semantic clones, which are similar in semantics but differ in syntax significantly. Semantic clone detectors have trouble scaling to larger systems and sometimes struggle with recall and precision. To address this, we developed a slice-based scalable approach that detects both syntactic and semantic code clones, srcClone. srcClone ascertains code segment similarity by assessing the similarity of their corresponding program slices. We employ a lightweight, publicly-available, scalable program slicer within our clone detection approach. Using dependency analysis to detect and assess cloned components, we discover insights about code components that can be affected by a clone pair or set. These elements are critical in impact analysis. It can also be used by program analysts to run on non-compilable and incomplete source code, which serves comprehension and maintenance tasks very well. We first evaluate srcClone by comparing it to six state-of-the-art tools and two additional semantic clone detectors in performance, recall, and precision. We use the BigCloneBench real clones benchmark to facilitate comparison. We use an additional 16 established benchmark scenarios to perform a qualitative comparison between srcClone and 44 clone detection approaches in their capabilities to detect these scenarios. To further measure scalability, we evaluate srcClone on 191 versions of Linux kernel, containing approximately 87 MLOC. In our evaluations, we illustrate our approach is both relatively scalable and accurate. While its precision is slightly less than some other techniques, it makes up for it in higher recall including semantic clones unable to be found by any existing techniques. We contend our approach is an important advancement in software cloning that it is both demonstrably scalable and can detect more types of clones than existing work, thus providing developers greater information into their software. © 2021 Elsevier Inc.","Clone detection; Code clone; Program slicing; Semantic clones"
"A cyber–physical–social approach for engineering Functional Safety Requirements for automotive systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127141363&doi=10.1016%2fj.jss.2022.111310&partnerID=40&md5=11d9ef7a66c9e80ea917021deb91847c","Several approaches have been developed to assist automotive system manufacturers in designing safer vehicles by facilitating compliance with functional safety standards. However, most of these approaches either mainly focus on the technical aspects of automotive systems and ignore the social ones, or they provide inadequate analysis of such important aspects. To this end, we propose a model-based approach for modeling and analyzing the Functional Safety Requirements (FSR) for automotive systems, which considers both the technical and social aspects of such systems. This approach is based on both the ISO 26262 and ISO/PAS 21448 standards, and it proposes a detailed engineering methodology to assist designers while modeling and analyzing FSR. In particular, this approach proposes a UML profile for modeling the FSR of the automotive system starting from item definition until safety validation, and it offers constraints expressed in Object Constraint Language (OCL) to be used for the verification of FSR models. We demonstrated the applicability and usefulness of the approach relying on a realistic example from the automotive domain, and we also evaluated the usability and utility of the approach with potential end-users. © 2022 Elsevier Inc.","Automotive; Cyber–Physical–Social systems; Functional safety requirements; ISO 26262; ISO/PAS 21448; SOTIF"
"Taxonomy of security weaknesses in Java and Kotlin Android apps","2022","Journal of Systems and Software","10.1016/j.jss.2022.111233","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124315221&doi=10.1016%2fj.jss.2022.111233&partnerID=40&md5=6d285ea9a8c74a8bb65b0a653358aa9b","Android is nowadays the most popular operating system in the world, not only in the realm of mobile devices, but also when considering desktop and laptop computers. Such a popularity makes it an attractive target for security attacks, also due to the sensitive information often manipulated by mobile apps. The latter are going through a transition in which the Android ecosystem is moving from the usage of Java as the official language for developing apps, to the adoption of Kotlin as the first choice supported by Google. While previous studies have partially studied security weaknesses affecting Java Android apps, there is no comprehensive empirical investigation studying software security weaknesses affecting Android apps considering (and comparing) the two main languages used for their development, namely Java and Kotlin. We present an empirical study in which we: (i) manually analyze 681 commits including security weaknesses fixed by developers in Java and Kotlin apps, with the goal of defining a taxonomy highlighting the types of software security weaknesses affecting Java and Kotlin Android apps; (ii) survey 43 Android developers to validate and complement our taxonomy. Based on our findings, we propose a list of future actions that could be performed by researchers and practitioners to improve the security of Android apps. © 2022 The Authors","Android; Security"
"Mining user reviews of COVID contact-tracing apps: An exploratory analysis of nine European apps","2022","Journal of Systems and Software","10.1016/j.jss.2021.111136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118981042&doi=10.1016%2fj.jss.2021.111136&partnerID=40&md5=3cf19a03fd3c6366ab935c810f24ff15","Context: More than 78 countries have developed COVID contact-tracing apps to limit the spread of coronavirus. However, many experts and scientists cast doubt on the effectiveness of those apps. For each app, a large number of reviews have been entered by end-users in app stores. Objective: Our goal is to gain insights into the user reviews of those apps, and to find out the main problems that users have reported. Our focus is to assess the “software in society” aspects of the apps, based on user reviews. Method: We selected nine European national apps for our analysis and used a commercial app-review analytics tool to extract and mine the user reviews. For all the apps combined, our dataset includes 39,425 user reviews. Results: Results show that users are generally dissatisfied with the nine apps under study, except the Scottish (“Protect Scotland”) app. Some of the major issues that users have complained about are high battery drainage and doubts on whether apps are really working. Conclusion: Our results show that more work is needed by the stakeholders behind the apps (e.g., app developers, decision-makers, public health experts) to improve the public adoption, software quality and public perception of these apps. © 2021 Elsevier Inc.","Contact-tracing; COVID; Data mining; Mobile apps; Software engineering; Software in society; User reviews"
"Affective reactions and test-driven development: Results from three experiments and a survey","2022","Journal of Systems and Software","10.1016/j.jss.2021.111154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120941391&doi=10.1016%2fj.jss.2021.111154&partnerID=40&md5=90d06a20f0793fe42744a3b79fe316fd","The research on the claimed effects of Test-Driven Development (TDD) on software quality and developers’ productivity has shown inconclusive results. Some researchers have ascribed such results to the negative affective reactions that TDD would provoke when developers apply it. In this paper, we studied whether and in which phases TDD influences the affective states of developers, who are new to this development approach. To that end, we conducted a baseline experiment and two replications, and analyzed the data from these experiments both individually and jointly. Also, we performed methodological triangulation by means of an explanatory survey, whose respondents were experienced with TDD. The results of the baseline experiment suggested that developers like TDD significantly less, compared to a non-TDD approach. Also, developers who apply TDD like implementing production code significantly less than those who apply a non-TDD approach, while testing production code makes TDD developers significantly less happy. These results were not confirmed in the replicated experiments. We found that the moderator that better explains these differences across experiments is experience (in months) with unit testing, practiced in a test-last manner. The higher the experience with unit testing, the more negative the affective reactions caused by TDD. The results from the survey seem to confirm the role of this moderator. © 2021","Affective state; Experiment; Replication; SAM; Survey; TDD"
"A software reliability growth model for imperfect debugging","2022","Journal of Systems and Software","10.1016/j.jss.2022.111267","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125994060&doi=10.1016%2fj.jss.2022.111267&partnerID=40&md5=600d06246f4f8fd236ce2923dbe08de4","In general, software reliability growth models (SRGMs) are often developed based on the assumptions of perfect debugging, single error type, and consistent testing environment. However, such the assumptions may be unrealistic for testing projects because new errors are always introduced during the debugging process, software errors are not alike, and the testing environment may change as the workforce escalates. Furthermore, the learning effect of the debugging process is taken under advisement, and assumes that it is unstable since the process of the error removal is also imperfect, which may cause a fluctuation of errors in the system. Therefore, the study is based on the Non-Homogeneous Poisson Process with considerations of the phenomenon of imperfect debugging, varieties of errors and change points during the testing period to extend the practicability of SRGMs. Furthermore, the error fluctuation rate is described by a sine cyclical function with a decreasing fluctuation breadth during the detection period, which indicates that the influence of increasing new errors during the testing period will be gradually unremarkable as the testing time elapses. Besides, the expected time of removing simple or complex errors is assumed to be different truncated exponential distributions. Finally, the optimal software release policies are proposed with considerations of the costs which occur in the testing and warranty period under an acceptable threshold of software reliability. © 2022 Elsevier Inc.","Change point; Imperfect debugging; Learning effect; Non-homogeneous Poisson process (NHPP); Software reliability"
"On the evaluation of usability design guidelines for improving network monitoring tools interfaces","2022","Journal of Systems and Software","10.1016/j.jss.2022.111223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123584068&doi=10.1016%2fj.jss.2022.111223&partnerID=40&md5=950f4fb537e3e4b6faa48ff346a8299a","Network monitoring tools are vital to network administrators, helping them make decisions and accomplish their tasks. In general, those tools are developed with a focus on technical aspects not taking into account important usability principles. On the other hand, the Human–Computer Interaction community presents great potential for the improvement of interfaces in network management tools suggesting that usability guidelines can guide software developers during user interface design. The goal of this work is to evaluate how different usability design guidelines can assist software developers in elaborating network monitoring tools interfaces with improved usability, creating a better experience for network administrators. To do that, we engage in an experimental study, where 52 software developers prototyped user interfaces based on different scenarios and applied 12 guidelines for usability design in network monitoring tools. Through the quantitative and qualitative analysis as well as Fisher's Exact Test, we demonstrate that the level of complexity of the scenarios for the creation of the prototypes had no significant effect on the acceptance of the guidelines. We conclude that the guidelines were used by most participants and are relevant to assist the software developers to create interfaces with a focus on usability in network monitoring tools. © 2022 Elsevier Inc.","Design guidelines; Graphical user interface; Network management; Network-monitoring tools; Usability; User-centered design"
"Fault-tolerant scheduling and data placement for scientific workflow processing in geo-distributed clouds","2022","Journal of Systems and Software","10.1016/j.jss.2022.111227","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123862610&doi=10.1016%2fj.jss.2022.111227&partnerID=40&md5=8f994754b429b565647bd836549f02d5","Scientific workflow processing in geo-distributed cloud is crucial for the scheduling of large-scale tasks and the massive data placement among tasks. However, the task execution time and energy consumption of data transmission are two urgent issues when a scientific workflow is processed in the different geo-distributed data centers. Aiming at the data placement problem, this paper proposes a Lagrangian relaxation method. This method considers the workload balance, storage capacity, data dependency, transmission bandwidth, and transmission cost to obtain the minimum time of data transmission time. Aiming at the task scheduling problems, a fault-tolerant scheduling strategy is proposed. The strategy optimizes the task scheduling mechanism by considering the task execution time and energy consumption. Finally, the performance of the proposed methods is evaluated via extensive experiments In terms of the data placement, the experiment results imply that the data transmission time of the proposed relaxation algorithm can averagely achieve up to 14.61%, 38.03%, and 39.57% reduction of over that of ILP-FDP algorithm, GA-DPSO algorithm, and GPDP algorithm, respectively. As for the fault-tolerant scheduling, the energy consumption of the TSPT algorithm is the lowest. Compared with the MTS algorithm, EODS algorithm and EWTS algorithm, the average gains of the proposed algorithm are 15.33%, 16.65%, and 28.96%, respectively. Compared with the benchmark algorithms, the task execution time of the TSPT algorithm can averagely reduce up to 12.78,18.85 and 25.65, respectively. © 2022 Elsevier Inc.","Data placement; Fault-tolerant scheduling; Geo-distributed cloud; QoS"
"Changes in perceived productivity of software engineers during COVID-19 pandemic: The voice of evidence","2022","Journal of Systems and Software","10.1016/j.jss.2021.111197","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123218412&doi=10.1016%2fj.jss.2021.111197&partnerID=40&md5=3005dc80cceb3979d5018271de842bd1","Background: The COVID-19 pandemic triggered a natural experiment of an unprecedented scale as companies closed their offices and sent employees to work from home. Many managers were concerned that their engineers would not be able to work effectively from home, or lack the motivation to do so, and that they would lose control and not even notice when things go wrong. As many companies announced their post-COVID permanent remote-work or hybrid home/office policies, the question of what can be expected from software engineers who work from home becomes more and more relevant. Aims: To understand the nature of home telework we analyze the evidence of perceived changes in productivity comparing office work before the pandemic with the work from home during the pandemic from thirteen empirical surveys of practitioners. Method: We analyzed data from six corporate surveys conducted in four Scandinavian companies combined with the results of seven published surveys studying the perceived changes in productivity in industrial settings. In addition, we sought explanations for the variation in perceived productivity among the engineers from the studied companies through the qualitative analysis of open-ended questions and interviews. Results: Combined results of 7686 data points suggest that though on average perceived productivity has not changed significantly, there are developers who report being more productive, and developers being less productive when working from home. Positively affected individuals in some surveys form large groups of respondents (up to 50%) and mention benefiting from a better organization of work, increased flexibility and focus. Yet, there are equally large groups of negatively affected respondents (up to 51%) who complain about the challenges related to remote teamwork and collaboration, as well as emotional issues, distractions and poor home office environment and equipment. Finally, positive trends are found in longitudinal surveys, i.e., developers’ productivity in the later months of the pandemic show better results than those in the earlier months. Conclusions: We conclude that behind the average “no change” lays a large variation of experiences, which means that the work from home might not be for everyone. Yet, a longitudinal analysis of the surveys is encouraging, as it shows that the more pessimistic results might be influenced by the initial experiences of an unprecedented crisis. At the end, we put forward the lessons learned during the pandemic that can inspire the new post-pandemic work policies. © 2021 The Authors","COVID-19; Empirical study; Perceived productivity; Performance; Surveys; WFH; Work-from-home"
"Test automation maturity improves product quality—Quantitative study of open source projects using continuous integration","2022","Journal of Systems and Software","10.1016/j.jss.2022.111259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124882032&doi=10.1016%2fj.jss.2022.111259&partnerID=40&md5=01ca61abebdc4fa1581ce3f3719687f1","The popularity of continuous integration (CI) is increasing as a result of market pressure to release product features or updates frequently. The ability of CI to deliver quality at speed depends on reliable test automation. In this paper, we present an empirical study to observe the effect of test automation maturity (assessed by standard best practices in the literature) on product quality, test automation effort, and release cycle in the CI context of open source projects. We run our test automation maturity survey and got responses from 37 open source java projects. We also mined software repositories of the same projects. The main results of regression analysis reveal that, higher levels of test automation maturity are positively associated with higher product quality (p-value=0.000624) and shorter release cycle (p-value=0.01891); There is no statistically significant evidence of increased test automation effort due to higher levels of test automation maturity and product quality. Thus, we conclude that, a potential benefit of improving test automation maturity (using standard best practices) is product quality improvement and release cycle acceleration in the CI context of open source projects. We encourage future research to extend our findings by adding more datasets with different programming languages and CI tools, closed source projects, and large-scale industrial projects. Our recommendation to practitioners (in the similar CI context) is to utilize standard best practices to improve test automation maturity. © 2022 The Author(s)","Best practice; Continuous integration; Empirical software engineering; Software repository mining; Software testing; Test automation"
"Considerations and challenges for the adoption of open source components in software-intensive businesses","2022","Journal of Systems and Software","10.1016/j.jss.2021.111152","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122228228&doi=10.1016%2fj.jss.2021.111152&partnerID=40&md5=1a76191d935c3a5d37498d5b169511a9","Component-Based Software Development is a conventional way of working for software-intensive businesses and Open Source Software (OSS) components are frequently considered by businesses for adoption and inclusion in software products. Previous research has found a variety of practices used to support the adoption of OSS components, including formally specified processes and less formal, developer-led approaches, and that the practices used continue to develop. Evolutionary pressures identified include the proliferation of available OSS components and increases in the pace of software development as businesses move towards continuous integration and delivery. We investigate work practices used in six software-intensive businesses in the primary and secondary software sectors to understand current approaches to OSS component adoption and the challenges businesses face establishing effective work practices to evaluate OSS components. We find businesses have established processes for evaluating OSS components and communities that support more complex and nuanced considerations of the cost and risks of component adoption alongside matters such as licence compliance and functional requirements. We also found that the increasing pace and volume of software development within some businesses provides pressure to continue to evolve software evaluation processes. © 2021 The Authors","Component-based software development; Open source software; Software adoption"
"Just-in-time software vulnerability detection: Are we there yet?","2022","Journal of Systems and Software","10.1016/j.jss.2022.111283","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125718261&doi=10.1016%2fj.jss.2022.111283&partnerID=40&md5=341fd720277bb3787f6cddc3cbd38e63","Background: Software vulnerabilities are weaknesses in source code that might be exploited to cause harm or loss. Previous work has proposed a number of automated machine learning approaches to detect them. Most of these techniques work at release-level, meaning that they aim at predicting the files that will potentially be vulnerable in a future release. Yet, researchers have shown that a commit-level identification of source code issues might better fit the developer's needs, speeding up their resolution. Objective: To investigate how currently available machine learning-based vulnerability detection mechanisms can support developers in the detection of vulnerabilities at commit-level. Method: We perform an empirical study where we consider nine projects accounting for 8991 commits and experiment with eight machine learners built using process, product, and textual metrics. Results: We point out three main findings: (1) basic machine learners rarely perform well; (2) the use of ensemble machine learning algorithms based on boosting can substantially improve the performance; and (3) the combination of more metrics does not necessarily improve the classification capabilities. Conclusion: Further research should focus on just-in-time vulnerability detection, especially with respect to the introduction of smart approaches for feature selection and training strategies. © 2022 The Author(s)","Empirical SE; Machine learning; Software vulnerabilities"
"Analysis and assessment of software reliability modeling with preemptive priority queueing policy","2022","Journal of Systems and Software","10.1016/j.jss.2022.111249","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124244112&doi=10.1016%2fj.jss.2022.111249&partnerID=40&md5=88e624037b04ecefbd70c5d43d78b588","Due to the growing complexity and size of software systems in modern society, the quality requirements for computer software have become ever more stringent. In the past, many software reliability growth models (SRGMs) have been proposed to evaluate and assess the reliability and quality of software systems. Some studies have demonstrated that both infinite server queueing (ISQ) and finite server queueing (FSQ) models can be applied to describe the fault detection process (FDP) and fault correction process (FCP) of software systems. However, it has also been noted that most ISQ and FSQ models assumed and obeyed the first come first served (FCFS) rule when removing the detected faults in FDP. In practice, the detected faults generally are classified into different levels of priority and those with higher priority should be fixed earlier. That is, high-priority faults have to be removed quickly to minimize their impact on software systems. Consequently, this assumption should be properly modified or adjusted. In this paper, we proposed a preemptive priority queueing (PPQ) model that considers both a finite number of debuggers and different priority levels. In our proposed PPQ model, faults assigned higher priority would be able to preemptively acquire resources already occupied by lower priority faults. Some numerical examples based on real failure data from different open-source and closed-source software are analyzed and discussed in detail. Experimental results show that the proposed PPQ model can provide more accurate estimation capability for software reliability, compared to traditional SRGMs. © 2022 Elsevier Inc.","Debugging; Non-preemptive priority queue; Open-source software; Preemptive priority queue; Software reliability; Software testing"
"Deep learning application on code clone detection: A review of current knowledge","2022","Journal of Systems and Software","10.1016/j.jss.2021.111141","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118916218&doi=10.1016%2fj.jss.2021.111141&partnerID=40&md5=8773e4b88bf3f2d164833dd5df2fab2e","Bad smells in code are indications of low code quality representing potential threats to the maintainability and reusability of software. Code clone is a type of bad smells caused by code fragments that have the same functional semantics with syntactic variations. In the recent years, the research on duplicate code has been dramatically geared up by deep learning techniques powered by advances in computing power. However, there exists little work studying the current state-of-art and future prospects in the area of applying deep learning to code clone detection. In this paper, we present a systematic review of the literature on the application of deep learning on code clone detection. We aim to find and study the most recent work on the subject, discuss their limitations and challenges, and provide insights on the future work. © 2021 Elsevier Inc.","Code clone; Code smell; Deep learning; Duplicate code; Literature review; Machine learning"
"An analysis of open source software licensing questions in Stack Exchange sites","2022","Journal of Systems and Software","10.1016/j.jss.2021.111113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117943388&doi=10.1016%2fj.jss.2021.111113&partnerID=40&md5=61f51f0ceda8405f8414a67dc4976740","Free and open source software is widely used in the creation of software systems, whereas many organizations choose to provide their systems as open source. Open source software carries licenses that determine the conditions under which the original software can be used. Appropriate use of licenses requires relevant expertise by the practitioners, and has an important legal angle. Educators and employers need to ensure that developers have the necessary training to understand licensing risks and how they can be addressed. At the same time, it is important to understand which issues practitioners face when they are using a specific open source license, when they are developing new open source software products or when they are reusing open source software. In this work, we examine questions posed about open source software licensing using data from the following Stack Exchange sites: Stack Overflow, Software Engineering, Open Source and Law. We analyze the indication of specific licenses and topics in the questions, investigate the attention the posts receive and trends over time, whether appropriate answers are provided and which type of questions are asked. Our results indicate that practitioners need, among other, clarifications about licensing specific software when other licenses are used, and for understanding license content. The results of the study can be useful for educators and employers, organizations that are authoring open source software licenses and developers for understanding the issues faced when using licenses, whereas they are relevant to other software engineering research areas, such as software reusability. © 2021 Elsevier Inc.","Open source software; Software licenses; Stack Exchange; Topic modeling"
"Automatic source code summarization with graph attention networks","2022","Journal of Systems and Software","10.1016/j.jss.2022.111257","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126290388&doi=10.1016%2fj.jss.2022.111257&partnerID=40&md5=e3f7e6aa4ea01ae7593bda0c338965ec","Source code summarization aims to generate concise descriptions for code snippets in a natural language, thereby facilitates program comprehension and software maintenance. In this paper, we propose a novel approach–GSCS–to automatically generate summaries for Java methods, which leverages both semantic and structural information of the code snippets. To this end, GSCS utilizes Graph Attention Networks to process the tokenized abstract syntax tree of the program, which employ a multi-head attention mechanism to learn node features in diverse representation sub-spaces, and aggregate features by assigning different weights to its neighbor nodes. GSCS further harnesses an additional RNN-based sequence model to obtain the semantic features and optimizes the structure by combining its output with a transformed embedding layer. We evaluate our approach on two widely-adopted Java datasets; the experiment results confirm that GSCS outperforms the state-of-the-art baselines. © 2022 Elsevier Inc.","Graph neural network; Recurrent neural network; Source code summarization"
"Enabling automated integration of architectural languages: An experience report from the automotive domain","2022","Journal of Systems and Software","10.1016/j.jss.2021.111106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117890876&doi=10.1016%2fj.jss.2021.111106&partnerID=40&md5=4d525270134bb9d9eebb1af39e6ca782","Modern automotive software systems consist of hundreds of heterogeneous software applications, belonging to separated function domains and often developed within distributed automotive ecosystems consisting of original equipment manufactures, tier-1 and tier-2 companies. Hence, the development of modern automotive software systems is a formidable challenge. A well-known instrument for coping with the tremendous heterogeneity and complexity of modern automotive software systems is the use of architectural languages as a way of enabling different and specific views over these systems. However, the use of different architectural languages might come with the cost of reduced interoperability and automation as different languages might have weak to no integration. In this article, we tackle the challenge of integrating two architectural languages heavily used in the automotive domain for the design and timing analysis of automotive software systems: AMALTHEA and Rubus Component Model. The main contributions of this paper are (i) a mapping scheme for the translation of an AMALTHEA architecture into a Rubus Component Model architecture where high-precision timing analysis can be run, and the back annotation of the analysis results on the starting AMALTHEA architecture; (ii) the implementation of the proposed scheme, which uses the concept of model transformations for enabling a full-fledged automated integration; (iii) the application of such automation on three industrial automotive systems being the brake-by-wire, the full blown engine management system and the engine management system. We discuss and evaluate the proposed contributions using an online, experts survey and the above-mentioned use cases. Based on the evaluation results, we conclude that the proposed automation mechanism is correct and applicable in industrial contexts. Besides, we observe that the performance of the automation mechanism does not degrade when translating large models with several thousands of elements. Eventually, we conclude that experts in this field find the proposed contribution industrially relevant. © 2021 The Author(s)","Architecture languages; Model-based development; Timing verification"
"Software micro-rejuvenation for Android mobile systems","2022","Journal of Systems and Software","10.1016/j.jss.2021.111181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121968695&doi=10.1016%2fj.jss.2021.111181&partnerID=40&md5=c339393f98b49bfff848e7017c52662b","Software aging – the phenomenon affecting many long-running systems, causing performance degradation or an increasing failure rate over mission time, and eventually leading to failure – is known to affect mobile devices and their operating systems, too. Software rejuvenation – the technique typically used to counteract aging – may compromise the user's perception of availability and reliability of the personal device, if applied at a coarse grain, e.g., by restarting applications or, worse, rebooting the entire device. This article proposes a configurable micro-rejuvenation technique to counteract software aging in Android-based mobile devices, acting at a fine-grained level, namely on in-memory system data structures. The technique is engineered in two phases. Before releasing the (customized) Android version, a heap profiling facility is used by the manufacturer's developers to identify potentially bloating data structures in Android services and to instrument their code. After release, an aging detection and rejuvenation service will safely clean up the bloating data structures, with a negligible impact on user perception and device availability, as neither the device nor operating system's processes are restarted. The results of experiments show the ability of the technique to provide significant gains in aging mobile operating system responsiveness and time to failure. © 2021 Elsevier Inc.","Android operating system; Software aging; Software rejuvenation"
"An automated extract method refactoring approach to correct the long method code smell","2022","Journal of Systems and Software","10.1016/j.jss.2022.111221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123722465&doi=10.1016%2fj.jss.2022.111221&partnerID=40&md5=4217939de66ecf1ec4cf865419c65124","Long Method is amongst the most common code smells in software systems. Despite various attempts to detect the long method code smell, few automated approaches are presented to refactor this smell. Extract Method refactoring is mainly applied to eliminate the Long Method smell. However, current approaches still face serious problems such as insufficient accuracy in detecting refactoring opportunities, limitations on correction types, the need for human intervention in the refactoring process, and lack of attention to object-oriented principles, mainly single responsibility and cohesion–coupling principles. This paper aims to automatically identify and refactor the long method smells in Java codes using advanced graph analysis techniques, addressing the aforementioned difficulties. First, a graph representing project entities is created. Then, long method smells are detected, considering the methods’ dependencies and sizes. All possible refactorings are then extracted and ranked by a modularity metric, emphasizing high cohesion and low coupling classes for the detected methods. Finally, a proper name is assigned to the extracted method based on its responsibility. Subsequently, the best destination class is determined such that design modularity is maximized. Experts’ opinion is used to evaluate the proposed approach on five different Java projects. The results show the applicability of the proposed method in establishing the single responsibility principle with a 21% improvement compared to the state-of-the-art extract method refactoring approaches. © 2022 Elsevier Inc.","Code smell; Extract method; Graph analysis; Long method; Software refactoring"
"Classifying crowdsourced mobile test reports with image features: An empirical study","2022","Journal of Systems and Software","10.1016/j.jss.2021.111121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118841323&doi=10.1016%2fj.jss.2021.111121&partnerID=40&md5=1e2ea7d7487cd776a6a5b2bb9b94873f","Crowdsourced testing has become a popular mobile application testing method, and it is capable of simulating real usage scenarios and detecting various bugs with a large workforce. However, inspecting and classifying the overwhelming number of crowdsourced test reports has become a time-consuming yet inevitable task. To alleviate such tasks, in the past decades, software engineering researchers have proposed many automatic test report classification techniques. However, these techniques may become less effective for crowdsourced mobile application testing, where test reports often consist of insufficient text descriptions and rich screenshots and are fundamentally different from those of traditional desktop software. To bridge the gap, we firstly fuse features extracted from text descriptions and screenshots to classify crowdsourced test reports. Then, we empirically investigate the effectiveness of our feature fusion approach under six classification algorithms, namely Naive Bayes (NB), k-Nearest Neighbors (kNN), Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF) and Convolutional Neural Network (CNN). The experimental results on six widely used applications show that (1) SVM with fused features can outperform others in classifying crowdsourced test reports, and (2) image features can improve the test report classification performance. © 2021 Elsevier Inc.","Crowdsourced testing; Image features; Test report classification"
"HUNTER: AI based holistic resource management for sustainable cloud computing","2022","Journal of Systems and Software","10.1016/j.jss.2021.111124","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118576408&doi=10.1016%2fj.jss.2021.111124&partnerID=40&md5=cd1fab94e22f9ed11fa76f49f21453c0","The worldwide adoption of cloud data centers (CDCs) has given rise to the ubiquitous demand for hosting application services on the cloud. Further, contemporary data-intensive industries have seen a sharp upsurge in the resource requirements of modern applications. This has led to the provisioning of an increased number of cloud servers, giving rise to higher energy consumption and, consequently, sustainability concerns. Traditional heuristics and reinforcement learning based algorithms for energy-efficient cloud resource management address the scalability and adaptability related challenges to a limited extent. Existing work often fails to capture dependencies across thermal characteristics of hosts, resource consumption of tasks and the corresponding scheduling decisions. This leads to poor scalability and an increase in the compute resource requirements, particularly in environments with non-stationary resource demands. To address these limitations, we propose an artificial intelligence (AI) based holistic resource management technique for sustainable cloud computing called HUNTER. The proposed model formulates the goal of optimizing energy efficiency in data centers as a multi-objective scheduling problem, considering three important models: energy, thermal and cooling. HUNTER utilizes a Gated Graph Convolution Network as a surrogate model for approximating the Quality of Service (QoS) for a system state and generating optimal scheduling decisions. Experiments on simulated and physical cloud environments using the CloudSim toolkit and the COSCO framework show that HUNTER outperforms state-of-the-art baselines in terms of energy consumption, SLA violation, scheduling time, cost and temperature by up to 12, 35, 43, 54 and 3 percent respectively. © 2021 Elsevier Inc.","Artificial intelligence; Cloud computing; Energy-efficiency; Holistic resource management; Thermal management"
"Path-directed source test case generation and prioritization in metamorphic testing","2022","Journal of Systems and Software","10.1016/j.jss.2021.111091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118708062&doi=10.1016%2fj.jss.2021.111091&partnerID=40&md5=d8b5b3e11e4bb055db5f6efa69ca517b","Metamorphic testing is a technique that makes use of some necessary properties of the software under test, termed as metamorphic relations, to construct new test cases, namely follow-up test cases, based on some existing test cases, namely source test cases. Due to the ability of verifying testing results without the need of test oracles, it has been widely used in many application domains and detected lots of real-life faults. Numerous investigations have been conducted to further improve the effectiveness of metamorphic testing, most of which were focused on the identification and selection of “good” metamorphic relations. Recently, a few studies emerged on the research direction of how to generate and select source test cases that are effective in fault detection. In this paper, we propose a novel approach to generating source test cases based on their associated path constraints, which are obtained through symbolic execution. The path distance among test cases is leveraged to guide the prioritization of source test cases, which further improve the efficiency. A tool has been developed to automate the proposed approach as much as possible. Empirical studies have also been conducted to evaluate the fault-detection effectiveness of the approach. The results show that this approach enhances both the performance and automation of metamorphic testing. It also highlights interesting research directions for further improving metamorphic testing. © 2021 Elsevier Inc.","Metamorphic testing; Path constraint; Source test case; Symbolic execution"
"An integrated tool set for verifying CafeOBJ specifications","2022","Journal of Systems and Software","10.1016/j.jss.2022.111302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127058375&doi=10.1016%2fj.jss.2022.111302&partnerID=40&md5=7bff31f3c9a2de6c10f61f68bde64028","CafeOBJ is a language for specifying and verifying a wide variety of software and/or hardware systems. Traditionally, verification has been carried out via proof scores, which consist of reducing goal-related terms in user-defined modules. Although proof scores are semi-formal (the specifier is partially responsible for soundness), their flexibility makes them a useful approach to verification. For the last years, we have developed different formal tools around the CafeInMaude interpreter, a CafeOBJ interpreter implemented in Maude. Besides supporting proof scores, we implemented a theorem prover, a proof script generator from proof scores, and the first stages of a proof script generator and fixer-upper. In this paper, we present (i) an improved and detailed version of our proof script generator and fixer-upper and (ii) a reimplementation of the CafeInMaude interpreter, which supports, among others, parallel execution, an improved tool integration, and an interactive user interface. The benchmarks used to evaluate the tools confirm the usefulness of the approach. © 2022 The Author(s)","CafeOBJ; Proof scores; Script generation; Script inference; Theorem proving"
"Data Augmentation by Program Transformation","2022","Journal of Systems and Software","10.1016/j.jss.2022.111304","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128304863&doi=10.1016%2fj.jss.2022.111304&partnerID=40&md5=82c4e462b21d5739d74a3fbcdda5730d","Data Augmentation has been recognized as one of the main techniques to improve deep learning models’ generalization ability. However, it has not been widely leveraged in big code tasks due to the essential difficulties of manipulating source code to generate new labeled data of high quality. In this paper, we propose a general data augmentation method based on program transformation. The idea is to extend big code datasets by a set of source-to-source transformation rules that preserve not only the semantics but also the syntactic naturalness of programs. Through controlled experiments, we demonstrated that semantic and syntax-naturalness preserving are the expected properties for a transformation rule to be effective in data augmentation. We designed 18 transformation rules that are proved semantic-preserving and tested syntax-naturalness-preserving. We also implemented and open-sourced a partial program transformation tool for Java based on the rules, named SPAT, whose effectiveness for data augmentation is validated in three big code tasks: method naming, code commenting, and code clone detection. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2022 Elsevier Inc.","Big Code; Data Augmentation; Program Transformation"
"Policy-driven Data Sharing over Attribute-Based Encryption supporting Dual Membership","2022","Journal of Systems and Software","10.1016/j.jss.2022.111271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125459903&doi=10.1016%2fj.jss.2022.111271&partnerID=40&md5=509b921f5357032dbdc815e90eac56e0","Attribute-Based Encryption (ABE) plays an important role in current secure data sharing through fine-grained customizable policies. However, the existing ABE schemes only support simple predicates, = and ≠, but cannot express a more general membership predicates, ∈ and ∉, in policies. The low expressivity of ABE will enlarge the ciphertext storage and reduce the communication efficiency. To overcome this problem, we propose an ABE supporting Dual Membership (DM-ABE). The core problem for implementing this scheme is how to use cryptographic methods to decide the membership between the verified element and the given set. In order to solve this problem, we design a cryptographic algorithm, called Secure Decision of Membership (SDM), based on aggregation functions. In this algorithm, any set can be aggregated into one cryptographic element, and the verified element and the given set can be converted into another cryptographic element in decision process. The membership between them can be decided by the above two cryptographic elements. Furthermore, we construct the DM-ABE by using SDM. Because of the good expressivity of our DM-ABE, we further propose a novel cryptographic data sharing framework by integrating DM-ABE and attribute-based access control to provide fine-grained access control and security protection for private data. In the security proof of DM-ABE, we prove that the DM-ABE satisfies the semantic security against chosen-plaintext attacks under the DBDHE assumption in the standard model through a unified way, considering both two encryption methods for ∈ and ∉ at the same time. Finally, we analyze our scheme in terms of time and space complexity, and compare it with some existing schemes. The results show that our DM-ABE has a better expressive ability on the boolean logic of general membership predicates, ∈ and ∉. © 2022","Attribute-Based Encryption; Dual Membership; Private data sharing; Secure Decision of Membership"
"A domain-specific language for modeling and analyzing solution spaces for technology roadmapping","2022","Journal of Systems and Software","10.1016/j.jss.2021.111094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118703170&doi=10.1016%2fj.jss.2021.111094&partnerID=40&md5=42b37c381553387417395fcdf33aea95","The introduction of major innovations in industry requires a collaboration across the whole value chain. A common way to organize such a collaboration is the use of technology roadmaps, which act as an industry-wide long-term planning tool. Technology roadmaps are used to identify industry needs, estimate the availability of technological solutions, and identify the need for innovation in the future. Roadmaps are inherently both time-dependent and based on uncertain values, i.e., properties and structural components can change over time. Furthermore, roadmaps have to reason about alternative solutions as well as their key performance indicators. Current approaches for model-based engineering do not inherently support these aspects. We present a novel model-based approach treating those aspects as first-class citizens. To address the problem of missing support for time in the context of roadmap modeling, we introduce the concepts of a common global time, time-dependent properties, and time-dependent availability. This includes requirements, properties, and the structure of the model or its components as well. Furthermore, we support the specification and analysis of key performance indicators for alternative solutions. These concepts result in a continuous range of various valid models over time instead of a single valid model at a certain point of time. We present a graphical user interface to enable the user to efficiently create and analyze those models. We further show the semantics of the resulting model by a translation into a set of global constraints as well as how we solve the resulting constraint system. We report on the evaluation of these concepts and the IRIS tool with domain experts from different companies in the automotive value chain based on the industrial case of a smart sensing electrical fuse. © 2021 Elsevier Inc.","Domain-specific languages; Model-based engineering; Modeling; Roadmapping; Technology roadmaps; Time-dependence"
"Estimating the potential of program repair search spaces with commit analysis","2022","Journal of Systems and Software","10.1016/j.jss.2022.111263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124690116&doi=10.1016%2fj.jss.2022.111263&partnerID=40&md5=861a6a1ce0688b37c51999a94160d768","The most natural method for evaluating program repair systems is to run them on bug datasets, such as Defects4J. Yet, using this evaluation technique on arbitrary real-world programs requires heavy configuration. In this paper, we propose a purely static method to evaluate the potential of the search space of repair approaches. This new method enables researchers and practitioners to encode the search spaces of repair approaches and select potentially useful ones without struggling with tool configuration and execution. We encode the search spaces by specifying the repair strategies they employ. Next, we use the specifications to check whether past commits lie in repair search spaces. For a repair approach, including many human-written past commits in its search space indicates its potential to generate useful patches. We implement our evaluation method in LIGHTER. LIGHTER gets a Git repository and outputs a list of commits whose source code changes lie in repair search spaces. We run LIGHTER on 55,309 commits from the history of 72 Github repositories with and show that LIGHTER's precision and recall are 77% and 92%, respectively. Overall, our experiments show that our novel method is both lightweight and effective to study the search space of program repair approaches. © 2022 The Author(s)","Commit analysis; Program repair; Search-space; Static code analysis"
"How secondary school girls perceive Computational Thinking practices through collaborative programming with the micro:bit","2022","Journal of Systems and Software","10.1016/j.jss.2021.111107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117726469&doi=10.1016%2fj.jss.2021.111107&partnerID=40&md5=1a9f1f248621f0efc2ada38bb4f616dc","Computational Thinking (CT) has been investigated from different perspectives. This research aims to investigate how secondary school girls perceive CT practices – the problem-solving practices that students apply while they are engaged in programming – when using the micro:bit device in a collaborative setting. This study also explores the collaborative programming process of secondary school girls with the micro:bit device. We conducted mixed-methods research with 203 secondary school girls (in the state of Victoria, Australia) and 31 mentors attending a girls-only CT program (OzGirlsCT program). The girls were grouped into 52 teams and collaboratively developed computational solutions around realistic, important problems to them and their communities. We distributed two surveys (with 193 responses each) to the girls. Further, we surveyed the mentors (with 31 responses) who monitored the girls, and collected their observation reports on their teams Our study indicates that the girls found “debugging” the most difficult type of CT practice to apply, while collaborative practices of CT were the easiest. We found that prior coding experience significantly reduced the difficulty level of only one CT practice - “debugging”. Our study also identified six challenges the girls faced and six best practices they adopted when working on their computational solutions. © 2021","Computational thinking practices; Education; Girls; K-12; Programming"
"Architectural conformance checking for KDM-represented systems","2022","Journal of Systems and Software","10.1016/j.jss.2021.111116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117948964&doi=10.1016%2fj.jss.2021.111116&partnerID=40&md5=4dd3fe9ca1aed49aa1513beccc91a258","Architecture-Driven Modernization (ADM) is a model-driven reengineering where systems are represented as instances of Knowledge Discovery Metamodel (KDM). KDM is the standard for representing systems in ADM context due to its power for capturing an extensive set of information about software systems. Besides, it is language and platform-independent, so every technique that is able of processing it also present this advantage. A recurrent activity in modernization projects is checking the conformance between the Current Architecture (CA) against the Planned Architecture (PA) in order to identify architectural drifts. The canonical phases of this activity are: (i) specification of the PA with its communication constraints; (ii) ex-traction of the CA, including the relationships among the architectural abstractions; and (iii) comparison between both architectures to identify the drifts. To the best of our knowledge, there is no ACC approach that addresses ACC in ADM context, considering KDM-represented systems. Therefore, we presents an ACC approach to be used in ADM context. We show how KDM can be used in ACC processes for representing the system to be modernized, the PA and the CA. We evaluated Arch-KDM using a small (LabSys-7KLoc) and a medium-size system (FreeMind-84KLoc) and the accuracy of the identification was acceptable. © 2021 Elsevier Inc.","Architectural-conformance checking; Architecture-description language; Architecture-Driven Modernization; Current Architecture; Knowledge Discovery Metamodel; Planned Architecture"
"TOSCAdata: Modeling data pipeline applications in TOSCA","2022","Journal of Systems and Software","10.1016/j.jss.2021.111164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122366695&doi=10.1016%2fj.jss.2021.111164&partnerID=40&md5=c89475e8fe2461a05960789374f37bc4","The serverless platform allows a customer to effectively use cloud resources and pay for the exact amount of used resources. A number of dedicated open source and commercial cloud data management tools are available to handle the massive amount of data. Such modern cloud data management tools are not enough matured to integrate the generic cloud application with the serverless platform due to the lack of mature and stable standards. One of the most popular and mature standards, TOSCA (Topology and Orchestration Specification for Cloud Applications), mainly focuses on application and service portability and automated management of the generic cloud application components. This paper proposes the extension of the TOSCA standard, TOSCAdata, that focuses on the modeling of data pipeline-based cloud applications. Keeping the requirements of modern data pipeline cloud applications, TOSCAdata provides a number of TOSCA models that are independently deployable, schedulable, scalable, and re-usable, while effectively handling the flow and transformation of data in a pipeline manner. We also demonstrate the applicability of proposed TOSCAdata models by taking a web-based cloud application in the context of tourism promotion as a use case scenario. © 2021 Elsevier Inc.","Data flow management; Data migration; Data pipeline; DevOps; Serverless computing; TOSCA"
"Usability inspection: Novice crowd inspectors versus expert","2022","Journal of Systems and Software","10.1016/j.jss.2021.111122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117919161&doi=10.1016%2fj.jss.2021.111122&partnerID=40&md5=540bc30ea029cac22b98b66d11198e13","Objective: This research study aims to investigate the use of novice crowd inspectors for usability inspection with respect to time spent and the cost incurred. This study compares the results of the novice crowd usability inspection guided by a single expert's heuristic usability inspection (novice crowd usability inspection henceforth) with the expert heuristic usability inspection. Background: Traditional usability evaluation methods are time-consuming and expensive. Crowdsourcing has emerged as a cost-effective and quick means of software usability evaluation. Method: In this regard, we designed an experiment to evaluate the usability of two websites and a web dashboard. Results: The results of the experiment show that novice crowd usability inspection guided by a single expert's heuristic usability inspection: a) Finds the same usability issues (w.r.t. content & quantity) as expert heuristic usability inspection. b) Is cost-effective than expert heuristic usability inspection employing less time duration. Conclusion: Based on the findings of this research study, we can conclude that the novice crowd usability inspection guided by a single expert's heuristic usability inspection and expert heuristic usability inspection, on average, gives the same results in terms of issues identified. © 2021","Crowdsourcing; Empirical studies in visualizations; Heuristic evaluations; Usability inspection"
"Towards a logical framework for ideal MBSE tool selection based on discipline specific requirements","2022","Journal of Systems and Software","10.1016/j.jss.2022.111306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127102111&doi=10.1016%2fj.jss.2022.111306&partnerID=40&md5=10302962f463a2206f125a04ceefe74a","Model-Based Systems Engineering (MBSE) has emerged with great potential to fulfill the non-linearly rising demand in interdisciplinary engineering, e.g., product development. However, the variety and complexity of MBSE tools pose difficulties in particular industrial applications. This paper tries to serve as a guideline to find the ideal tool for a specific industrial application as well as to highlight the key criteria that an industry might consider. For this purpose, we propose a logical framework for MBSE tool selection, which is based on market research, the approaches of Quality Function Deployment (QFD), and decision matrix. As customers are at the center of any product, accordingly the needs of MBSE tool users are addressed within this research as the fundamental starting point. Market research and extensive discussions with MBSE tool vendors and academia show the current situation of MBSE tools. To compare the performance of the considered tools, a set of user needs is defined. QFD is performed to analyze the user needs with respect to evaluable technical properties. Subsequently each tool performance is assessed using a decision matrix. Through this process, a well-defined functional structure of MBSE tools is sketched, and in order to identify the properties of an ideal tool, all the attributes of different MBSE tools are mapped to a common platform. For the purpose of evaluation, we apply our proposed logical framework to select an exemplary MBSE tool for interdisciplinary application. © 2022 The Author(s)","Filtration method; MBSE; Software QFD; Tool selection"
"Precise Learning of Source Code Contextual Semantics via Hierarchical Dependence Structure and Graph Attention Networks","2022","Journal of Systems and Software","10.1016/j.jss.2021.111108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118759414&doi=10.1016%2fj.jss.2021.111108&partnerID=40&md5=b2e6deb1d8c0668848a5d5f83bc18516","Deep learning is being used extensively in a variety of software engineering tasks, e.g., program classification and defect prediction. Although the technique eliminates the required process of feature engineering, the construction of source code model significantly affects the performance on those tasks. Most recent works was mainly focused on complementing AST-based source code models by introducing contextual dependencies extracted from CFG. However, all of them pay little attention to the representation of basic blocks, which are the basis of contextual dependencies. In this paper, we integrated AST and CFG and proposed a novel source code model embedded with hierarchical dependencies. Based on that, we also designed a neural network that depends on the graph attention mechanism. Specifically, we introduced the syntactic structural of the basic block, i.e., its corresponding AST, in source code model to provide sufficient information and fill the gap. We have evaluated this model on three practical software engineering tasks and compared it with other state-of-the-art methods. The results show that our model can significantly improve the performance. For example, compared to the best performing baseline, our model reduces the scale of parameters by 50% and achieves 4% improvement on accuracy on program classification task. © 2021 Elsevier Inc.","Abstract syntax Tree; Control flow graph; Deep learning; Graph neural network; Program analysis"
"Random or heuristic? An empirical study on path search strategies for test generation in KLEE","2022","Journal of Systems and Software","10.1016/j.jss.2022.111269","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125223710&doi=10.1016%2fj.jss.2022.111269&partnerID=40&md5=0f7dd83f77db090fe52f28d434b07acc","Randomness may be the most straightforward strategy and has been widely used in software engineering. One criticism for random strategy is its aimless for tasks. Therefore, many heuristic strategies have been proposed to improve the random strategy. However, it is impossible to prove that heuristics are better than randomness in theory, especial for software test generation. This open question is always left to empirical study and researchers expect to conclude some guidelines in practice. This paper studies a key technique, path search strategies, of test generation. We conducted an empirical evaluation and comparison among ten concrete path search approaches provided by KLEE, where two approaches belong to random search strategy and eight belong to heuristic search strategy, with 53 GNU Coreutils applications. We also investigated both cases with and without constraint optimization techniques for KLEE-based test generation. The experimental results show that without optimization, one approach from random strategy – random-path – performs better than other techniques in terms of the number of completed paths, statement coverage and branch coverage. These results indicate that random strategy can be a better choice for test generation in most cases without optimization, and heuristic strategies need further investigation. However, when combined with optimization, the selection of search strategy depends on the specific optimization applied. We further analyze the reasons behind the statistical results and provide guidelines to select the appropriate path search strategy for test generation in practice. © 2022 Elsevier Inc.","Constraint solving; Path search strategies; Software testing; Symbolic execution; Test generation"
"Code-quality evaluation scheme for assessment of student contributions to programming projects","2022","Journal of Systems and Software","10.1016/j.jss.2022.111273","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126357023&doi=10.1016%2fj.jss.2022.111273&partnerID=40&md5=becc03f9d7161829b62a46162b7da409","Project-based learning is the most common approach to software engineering education, due to its emphasis on the teamwork skills essential to real-world collaborations. This study developed an automated programming assessment system (APAS) featuring a code-quality evaluation scheme to overcome difficulties in assessing the contribution of individual team members. Team participation is visualized on a weekly basis to provide insights pertaining to team dynamics, and metrics based on code quality allow the segmentation of students by level of contribution. Insights provided by the proposed system were also shown to facilitate interventions aimed at improving code quality. Empirical analysis based on submission data from 146 students (41 teams) demonstrated the feasibility of the proposed system in monitoring group-based learning projects at the university level, particularly in detecting free-riders. © 2022","Automated programming assessment system; Cooperative/collaborative learning; Programming education; Project-based learning; Quality"
"A novel approach for Software Architecture Product Line Engineering","2022","Journal of Systems and Software","10.1016/j.jss.2021.111191","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121805603&doi=10.1016%2fj.jss.2021.111191&partnerID=40&md5=441846f0816ad9e0b68bd11ef3ea55e2","A large software system exists in different forms, as different variants targeting different business needs and users. This kind of systems is provided as a set of “independent” products and not as a “single-whole”. Developers use ad-hoc mechanisms to manage variability. We defend a vision of software development where we consider an SPL architecture starting from which the architecture of each variant can be derived before its implementation. Indeed, each derived variant can have its own life. In this paper, we propose a novel approach for Software Architecture Product Line (SAPL) Engineering. It consists of, i) a generic process for recovering an SAPL model which is a product line of “software architectures” from large-sized variants. ii) a forward-engineering process that uses the recovered SAPL to derive new customized software architecture variants. The approach is firstly experimented on thirteen Eclipse variants to create a new SAPL. Then, an intensive evaluation is conducted using an existing benchmark which is also based on Eclipse IDE. Our results showed that we can accurately reconstruct such an SAPL and derive effectively pertinent variants. Our study provides insights that recovering SAPL and then deriving software architectures offers good documentation to understand the software before changing it. © 2021 Elsevier Inc.","BUT4Reuse; Component/service-based software; Software architecture; Software Architecture Product Line; Software architecture recovery; SPLE"
"WDBT: Non-volatile memory wear characterization and mitigation for DBT systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111247","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125832620&doi=10.1016%2fj.jss.2022.111247&partnerID=40&md5=a1662b981b599698067d1198ed2f3fc4","Emerging high-capacity and byte-addressable non-volatile memory (NVM) is promising for the next-generation memory system. However, NVM suffers from limited write endurance, as an NVM cell will wear out very soon after a certain number of writes, making NVM undependable. To address this issue, many wear reduction and leveling mechanisms have been proposed. Nevertheless, most of these mechanisms are developed without the knowledge of application semantics and behaviors. In this paper, we advocate application-level wear management, which allows us to create effective and flexible wear reduction and leveling techniques for specific application domains. Particularly, we find that applications running with dynamic binary translation (DBT) exhibit significantly more writes. This is because DBT systems need to handle architectural differences when translating instructions across different architectures. In this paper, we present WDBT, which focuses on wear reduction and leveling for DBT systems on NVM. WDBT is designed based on common practices of DBT systems to reduce the majority of writes introduced by DBT. We also implement a prototype of WDBT using a real-world DBT system, QEMU, for multiple popular instruction sets. Experimental results on SPEC CPU 2017 show that WDBT can effectively reduce writes by 52.09% and 34.48% for x86-64 and RISC-V, respectively. Moreover, the performance overhead of WDBT is negligible. © 2022 Elsevier Inc.","Cross-ISA virtualization; Dynamic binary translation; Non-volatile memory; NVM Wear leveling; NVM Wear reduction; QEMU"
"API beauty is in the eye of the clients: 2.2 million Maven dependencies reveal the spectrum of client–API usages","2022","Journal of Systems and Software","10.1016/j.jss.2021.111134","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119384404&doi=10.1016%2fj.jss.2021.111134&partnerID=40&md5=47a3d9f87a30c8b063bcd50d40765f97","Hyrum's law states a common observation in the software industry: “With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody”. Meanwhile, recent research results seem to contradict this observation when they state that “for most APIs, there is a small number of features that are actually used”. In this work, we perform a large scale empirical study of client–API relationships in the Maven ecosystem, in order to investigate this seeming paradox between the observations in industry and the research literature. We study the 94 most popular libraries in Maven Central, as well as the 829,410 client artifacts that declare a dependency to these libraries and that are available in Maven Central, summing up to 2.2M dependencies. Our analysis indicates the existence of a wide spectrum of API usages, with enough clients, most API types end up being used at least once. Our second key observation is that, for all libraries, there is a small set of API types that are used by the vast majority of its clients. The practical consequences of this study are two-fold: (i) it is possible for API maintainers to find an essential part of their API on which they can focus their efforts; (ii) API developers should limit the public API elements to the set of features for which they are ready to have users. © 2021 The Author(s)","Bytecode analysis; Java; Maven Central Repository; Mining software repositories; Software reuse"
"Exploiting gated graph neural network for detecting and explaining self-admitted technical debts","2022","Journal of Systems and Software","10.1016/j.jss.2022.111219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123200409&doi=10.1016%2fj.jss.2022.111219&partnerID=40&md5=cf75dd4e6f74b76ab89df4be724ddc5c","Self-admitted technical debt (SATD) refers to a specific type of technical debt that is introduced intentionally in the software development and maintenance processes. SATD enables practitioners to take some temporary solutions instead of making comprehensive decisions, which will lead to the high complexity of the software. However, most existing studies relied on manual methods for detecting SATDs. A recent study proposed a method HATD that used a hybrid attention-based method to automatically detect SATDs and it achieved the state-of-the-art performance. However, HATD mainly focused on the locality of the comment instances and lacked of the relationship between long-distance and discontinuous comment instances. To address such an issue, in this work, we propose a novel approach named GGSATD. Specifically, GGSATD first builds the graph for comment instances and then employs the gated graph neural network to iteratively update node representation. The global representation can be obtained by the soft attention mechanism and pooling operation. Experiments on 10 projects show that our GGSATD method obtains promising performance against five baseline methods in both within-project and cross-project scenarios. Extended experiments on seven real-world projects illustrate the effectiveness of our GGSATD method. © 2022 Elsevier Inc.","Attention mechanism; Gated graph neural network; Self-admitted technical debt; Technical debt"
"Preventing accessibility barriers: Guidelines for using user interface design patterns in mobile applications","2022","Journal of Systems and Software","10.1016/j.jss.2021.111213","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122696295&doi=10.1016%2fj.jss.2021.111213&partnerID=40&md5=1da280073185dd8c3a9d7028b75bc0e4","Mobile applications play an important role in many aspects of life. It is essential to be aware of the software development approaches that can support the design of accessible applications. Their main goal is to ensure that the interactive applications are available to everyone, including people with disabilities, reduced skills, or momentarily induced impairments. This paper aims to identify the accessibility barriers that occur when using design patterns for building user interfaces of mobile apps and propose guidelines to prevent the problems most often encountered. We start by conducting a gray literature review in professional forums and blogs to reveal the difficulties developers face when using mobile user interface design patterns. We thus compiled a catalog which contains the descriptions of 9 user interface design patterns, the accessibility barriers linked to the use of each pattern and the guidelines that can be followed to prevent the problem of these barriers. We carried out an evaluation of the use of the catalog with 60 participants. Our results show that in most cases, the guidelines were correctly applied for the prototyping of mobile user interfaces. The findings also revealed the usefulness and ease-of-use of the guidelines from the perspective of the participants. © 2022 Elsevier Inc.","Accessibility; Gray literature review; Mobile application; Software engineering; User interface design pattern"
"Multi-triage: A multi-task learning framework for bug triage","2022","Journal of Systems and Software","10.1016/j.jss.2021.111133","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119324033&doi=10.1016%2fj.jss.2021.111133&partnerID=40&md5=68738277880ffc5c979f5531bf9caa58","Assigning developers and allocating issue types are two important tasks in the bug triage process. Existing approaches tackle these two tasks separately, which is time-consuming due to repetition of effort and negating the values of correlated information between tasks. In this paper, a multi-triage model is proposed that resolves both tasks simultaneously via multi-task learning (MTL). First, both tasks can be regarded as a classification problem, based on historical issue reports. Second, performances on both tasks can be improved by jointly interpreting the representations of the issue report information. To do so, a text encoder and abstract syntax tree (AST) encoder are used to extract the feature representation of bug descriptions and code snippets accordingly. Finally, due to the disproportionate ratio of class labels in training datasets, the contextual data augmentation approach is introduced to generate syntactic issue reports to balance the class labels. Experiments were conducted on eleven open-source projects to demonstrate the effectiveness of this model compared with state-of-the-art methods. © 2021 Elsevier Inc.","Bug triage; Deep learning; Multi-task learning; Recommendation system"
"PRHAN: Automated Pull Request Description Generation Based on Hybrid Attention Network","2022","Journal of Systems and Software","10.1016/j.jss.2021.111160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121254484&doi=10.1016%2fj.jss.2021.111160&partnerID=40&md5=61e11ca00bfc50b74b66c72848d78463","Descriptions of pull requests (PRs) are posted by developers for describing the modifications that they have made and the corresponding reasons in these PRs. Although PRs help developers improve the development efficiency, some developers usually ignore writing the descriptions for PRs. To alleviate the above problem, researchers generally utilize text summarization model to automatically generate descriptions for PRs. However, current RNN-based models still face the challenges such as low efficiency and out-of-vocabulary (OOV), which may influence the further performance improvement to their models. To break this bottleneck, we propose a novel model aiming at the above challenges, named PRHAN (Pull Requests Description Generation Based on Hybrid Attention Network). Specifically, the core of PRHAN is the hybrid attention network, which has faster execution efficiency than RNN-based model. Moreover, we address the OOV problem by the utilizing byte-pair encoding algorithm to build a vocabulary at the sub-word level. Such a vocabulary can represent the OOV words by combining sub-word units. To reduce the sensitivity of the model, we take a simple but effective method into the cross-entropy loss function, named label smoothing. We choose three baseline models, including LeadCM, Transformer and the state-of-the-art model built by Liu et al. and evaluate all the models on the open-source dataset through ROUGE, BLEU, and human evaluation. The experimental results demonstrate that PRHAN is more effective than baselines. Moreover, PRHAN can execute faster than the state-of-the-art model proposed by Liu et al. © 2021 Elsevier Inc.","Byte-pair encoding; Hybrid attention; Label smoothing; PR description"
"GitHub repositories with links to academic papers: Public access, traceability, and evolution","2022","Journal of Systems and Software","10.1016/j.jss.2021.111117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117809505&doi=10.1016%2fj.jss.2021.111117&partnerID=40&md5=600a65e6bcdd64470c00942b59bcf9ab","Traceability between published scientific breakthroughs and their implementation is essential, especially in the case of open-source scientific software which implements bleeding-edge science in its code. However, aligning the link between GitHub repositories and academic papers can prove difficult, and the current practice of establishing and maintaining such links remains unknown. This paper investigates the role of academic paper references contained in these repositories. We conduct a large-scale study of 20 thousand GitHub repositories that make references to academic papers. We use a mixed-methods approach to identify public access, traceability and evolutionary aspects of the links. Although referencing a paper is not typical, we find that a vast majority of referenced academic papers are public access. These repositories tend to be affiliated with academic communities. More than half of the papers do not link back to any repository. We find that academic papers from top-tier SE venues are not likely to reference a repository, but when they do, they usually link to a GitHub software repository. In a network of arXiv papers and referenced repositories, we find that the most referenced papers are (i) highly-cited in academia and (ii) are referenced by repositories written in different programming languages. © 2021 The Authors","Open access; Open science; Software documentation; Traceability"
"Modelling and executing IoT-enhanced business processes through BPMN and microservices","2022","Journal of Systems and Software","10.1016/j.jss.2021.111139","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118878850&doi=10.1016%2fj.jss.2021.111139&partnerID=40&md5=7b52c663bc426679b7d1f0c36037b643","The Internet of Things enables to connect the physical world to digital business processes (BP) and allows a BP to (1) consider real-world data to take more informed business decisions, (2) automate and/or improve BP tasks, and (3) adapt itself to the physical execution environment. We refer to these processes as IoT-enhanced BPs. Although numerous researchers have studied this subject, there are still some challenges to be faced. For instance, the need of a modelling solution that does not increase the notation complexity to facilitate further analysis and engineering decision making, or an execution approach that provides a high degree of independence between the process and the underlying IoT device technology. The objective of this work is defining an approach that (1) considers important intrinsic characteristics of IoT-enhanced BPs at modelling level without growing the complexity of the modelling language, and (2) facilitates the execution of the IoT-enhanced BPs represented in models independently from IoT devices’ technology. To do so, we present a modelling approach that uses standard BPMN concepts to model IoT-enhanced BPs without modifying its metamodel. It applies the Separation of Concern (SoC) design principle: BPMN is used to describe IoT-enhanced BPs while low-level real-world data is captured in an ontology. Finally, a microservice architecture is proposed to execute BPMN models and facilitate its integration with the physical world. This architecture provides high flexibility to support multiples IoT device technologies as well as their evolution and maintenance. The evaluation done allows us to conclude that the application of the SoC principle using BPMN and ontologies facilitates the definition of intrinsic characteristics of IoT-enhanced BPs without increasing the complexity of the BPMN metamodel. Besides, the proposed microservice architecture provides a high degree of decoupling between the created models and the underlying IoT technology. © 2021","BPMN; IoT; Microservices"
"A co-evolutionary genetic algorithms approach to detect video game bugs","2022","Journal of Systems and Software","10.1016/j.jss.2022.111261","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126138181&doi=10.1016%2fj.jss.2022.111261&partnerID=40&md5=af792769e9947acb7beaddc08b9d7b0d","Video games systems are known for their complexity, concurrency and non-determinism, which makes them prone to challenging tacit bugs. Video games development is costly and the corresponding verification process is tiresome. Testing the nondeterministic and concurrent behaviors of video games systems is not only crucial but also challenging, especially when the game state space is huge. Accordingly, typical software testing approaches are neither suitable nor effective to find related bugs. Novel automated approaches to support video game testing are needed. This problem has caught researchers’ attention recently. Approaches found in the literature have tried to address two sub problems: modeling and uncovering bugs. Colored Petri nets is known to support modeling and verifying concurrent and nondeterministic systems. Search approaches have been used in the literature to check the availability of faulty states through exploring state spaces. However, these approaches tend to lack adaptability to test different video games systems due to the limitations of the defined fitness functions, in addition to difficulties in searching huge state spaces due to exhaustive and unguided search. The availability of automated approaches that guide and direct the process of bugs finding is mandatory. Thus, in this study we address this problem as we present a solution for automated software testing using collaborative work of two genetic algorithms (i.e. co-evolutionary) agents, where our approach is applied to colored Petri nets representations of the software workflow. The results of our experiments have shown the potential of the proposed approach in effectively finding bugs automatically. © 2022 Elsevier Inc.","Automated software testing; Colored Petri nets; Genetic algorithms; Reachability analysis; Video game"
"Software product line scoping: A systematic literature review","2022","Journal of Systems and Software","10.1016/j.jss.2021.111189","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121926915&doi=10.1016%2fj.jss.2021.111189&partnerID=40&md5=291459cce1f3a962687f98f10b7de209","Software product line (SPL) scoping aids companies to define the boundaries of their resources such as products, domains, and assets, the target of reuse tasks scoping technical and organizational aspects. As scoping guides the management of the resources in SPL development, it becomes one of the core activities in this process. We can find in the literature several approaches on this topic, proposing techniques and methodologies to be applicable in different organizational scenarios. However, no work comprehensively reviews such approaches and describes the advances in state of the art in the last years. In this context, we look into identifying, analyzing, and extracting detailed characteristics from SPL scoping proposals found in the literature. These characteristics allowed us to compare these approaches, reason about their applicability, and identify existing limitations and research opportunities. Thus, we conducted a systematic literature review alongside snowballing, following a well-defined protocol to retrieve, classify and extract information from the literature. We analyzed a total of 58 studies, identifying 41 different approaches in the field, highlighting their similarities and differences, and establishing a generic scoping process. Furthermore, we discuss research opportunities in the SPL scoping field. © 2021 Elsevier Inc.","Snowballing; Software product line scoping; Software product lines; Systematic literature review"
"Role stereotypes in software designs and their evolution","2022","Journal of Systems and Software","10.1016/j.jss.2022.111296","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126864001&doi=10.1016%2fj.jss.2022.111296&partnerID=40&md5=6fe18d7fec764ce980200c6b5fb72834","Role stereotypes are abstract characterisations of the responsibilities of the building blocks of software applications. The role a class plays within a software system reflects its design intention. Wirfs-Brock introduced the following six role stereotypes: Information Holder, which knows information, Structurer, which maintains object relationships, Service Provider, which offers computing services, Coordinator, which delegates tasks to others, Controller, which directs other's actions, and Interfacer, which transforms information. Knowledge about class role stereotypes can help various software development and maintenance tasks, such as program understanding, program summarisation, and quality assurance. This paper presents an automated machine learning-based approach for classifying the role-stereotype of classes in Java projects. We analyse this approach's performance against a manually labelled ground truth for three open source projects that contain 1,500+ Java classes altogether. The contributions of this paper include: (i) a machine learning (ML) approach to address the problem of automatically inferring role-stereotypes of classes in Object-Oriented Programming Languages, (ii) the manually labelled ground truth, (iii) an evaluation of the performance of the classifier, (iv) an evaluation of the generalisability of the approach, and (v) an illustration of new uses of role-stereotypes. The evaluation shows that the Random Forest algorithm yields the best classification performance. We find, however, that the performance of the ML-classifier varies a lot for different role stereotypes. In particular, its performance degrades when classifying rarer stereotypes. Among the 23 features that we study, features related to the classes’ collaboration characteristics and complexity stand out as the best discriminants of role stereotypes. © 2022 Elsevier Inc.","Class role-stereotypes; Design metrics; Machine learning classification; Program analysis; Software engineering"
"High-availability clusters: A taxonomy, survey, and future directions","2022","Journal of Systems and Software","10.1016/j.jss.2021.111208","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123050942&doi=10.1016%2fj.jss.2021.111208&partnerID=40&md5=a245c8be9065cdb8a6f05b9e28919498","The delivery of key services in domains ranging from finance and manufacturing to healthcare and transportation is underpinned by a rapidly growing number of mission-critical enterprise applications. Ensuring the continuity of these complex applications requires the use of software-managed infrastructures called high-availability clusters (HACs). HACs employ sophisticated techniques to monitor the health of key enterprise application layers and of the resources they use, and to seamlessly restart or relocate application components after failures. In this paper, we first describe the manifold uses of HACs to protect essential layers of a critical application and present the architecture of high availability clusters. We then propose a taxonomy that covers all key aspects of HACs—deployment patterns, application areas, types of cluster, topology, cluster management, failure detection and recovery, consistency and integrity, and data synchronisation; and we use this taxonomy to provide a comprehensive survey of the end-to-end software solutions available for the HAC deployment of enterprise applications. Finally, we discuss the limitations and challenges of existing HAC solutions, and we identify opportunities for future research in the area. © 2021 Elsevier Inc.","Clustering; Dependability; Enterprise system; High availability; High availability clusters; Reliability"
"DigBug—Pre/post-processing operator selection for accurate bug localization","2022","Journal of Systems and Software","10.1016/j.jss.2022.111300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127058392&doi=10.1016%2fj.jss.2022.111300&partnerID=40&md5=62bfc9c89eb2162f7740b4a6932670ef","Bug localization is a recurrent maintenance task in software development. It aims at identifying relevant code locations (e.g., code files) that must be inspected to fix bugs. When such bugs are reported by users, the localization process become often overwhelming as it is mostly a manual task due to incomplete and informal information (written in natural languages) available in bug reports. The research community has then invested in automated approaches, notably using Information Retrieval techniques. Unfortunately, reported performance in the literature is still limited for practical usage. Our key observation, after empirically investigating a large dataset of bug reports as well as workflow and results of state-of-the-art approaches, is that most approaches attempt localization for every bug report without considering the different characteristics of the bug reports. We propose DIGBUG as a straightforward approach to specialized bug localization. This approach selects pre/post-processing operators based on the attributes of bug reports; and the bug localization model is parameterized in accordance as well. Our experiments confirm that departing from “one-size-fits-all” approaches, DIGBUG outperforms the state-of-the-art techniques by 6 and 14 percentage points, respectively in terms of MAP and MRR on average. © 2022","Bug characteristics; Bug localization; Bug report; Fault localization; Information retrieval; Operator combination"
"Profiling gas consumption in solidity smart contracts","2022","Journal of Systems and Software","10.1016/j.jss.2021.111193","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122027163&doi=10.1016%2fj.jss.2021.111193&partnerID=40&md5=155d6be04753e5f79bdb607ced227bfc","Nowadays, more and more applications are developed for running on a distributed ledger technology, namely dApps. The business logic of dApps is usually implemented within smart contracts developed through Solidity, a programming language for writing smart contracts on different blockchain platforms, including the popular Ethereum. In Ethereum, the smart contracts run on the machines of miners and the gas corresponds to the execution fee compensating such computing resources. However, the deployment and execution costs of a smart contract depend on the implementation choices done by developers. Unappropriated design choices could lead to higher gas consumption than necessary. In this paper, we (i) identify a set of 19 Solidity code smells affecting the deployment and transaction costs of a smart contract, and (ii) assess the relevance of such smells through a survey involving 34 participants. On top of these smells, we propose GasMet, a suite of metrics for statically evaluating the code quality of a smart contract from the gas consumption perspective. An experiment involving 2186 smart contracts demonstrates that the proposed metrics have direct associations with deployment costs. The metrics in our suite can be used for more easily identifying source code segments that need optimizations. © 2021 Elsevier Inc.","Code quality; Empirical study; Smart contracts optimization; Software engineering for blockchain technologies; Software metrics"
"Only pay for what you need: Detecting and removing unnecessary TEE-based code","2022","Journal of Systems and Software","10.1016/j.jss.2022.111253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124790549&doi=10.1016%2fj.jss.2022.111253&partnerID=40&md5=a4c103fdeef9bcc05ee18c7d76a371cf","A Trusted Execution Environment (TEE) provides an isolated hardware environment for sensitive code and data to protect a system's integrity and confidentiality. As we discovered, programmers tend to overuse TEE protection. When they place non-sensitive code in TEE, the trusted computing base (TCB) grows unnecessarily, leading to long execution latencies and large attack surfaces. To address this problem, we first study a representative sample of open-source projects to uncover how TEE is utilized in real-world software. To facilitate the process of removing non-sensitive code from TEE, we introduce TEE Insourcing, a new type of software refactoring that identifies and removes the unnecessary program parts out of TEE. We implemented TEE Insourcing as the TEE-DRUP framework, which operates in three phases: (1) a variable sensitivity analysis designates each variable as sensitive or non-sensitive; (2) a TEE-aware taint analysis identifies non-sensitive TEE-based functions; (3) a fully-declarative program transformation automatically moves these functions out of TEE. Our evaluation demonstrates that our approach is correct, effective, and usable. By deploying TEE-DRUP to discover and remove the unnecessary TEE code, programmers can both reduce the TCB's size and improve system performance. © 2022 Elsevier Inc.","Code transformation; Program analysis; TEE"
"Which builds are really safe to skip? Maximizing failure observation for build selection in continuous integration","2022","Journal of Systems and Software","10.1016/j.jss.2022.111292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126629474&doi=10.1016%2fj.jss.2022.111292&partnerID=40&md5=3cfc039a60a0c2daf126f2ad25d5397f","Continuous integration (CI) is a widely used practice in modern software engineering. Unfortunately, it is also an expensive practice. Google and Mozilla estimate their expenses for their CI systems in millions of dollars. To reduce the cost of CI, researchers developed multiple approaches to reduce its computational workload requirements. However, these approaches sometimes make mispredictions and skip failing builds which are not desirable to be skipped. Thus, in this paper, we aim to save computational cost in CI, while also maximizing the observation of failing builds, i.e., to skip builds more safely. First, we perform empirical studies to understand which builds are safe to skip, starting from CI-Skip rules (Abdalkareem et al., 2019) that characterize builds that developers decide to skip. We observe that CI-Skip rules are not so safe as expected. We then develop a collection of CI-Run rules that can complement these rules. Based on our findings, we propose PRECISEBUILDSKIP, a novel approach that maximizes build failure observation and reduces the cost of CI through the strategy of build selection. We evaluate our approach and results show that our approach saved more cost (5.5%) than the safest existing technique but reduced the falsely skipped failing builds from 4.1% to 0% (median value). © 2022 Elsevier Inc.","Build prediction; Continuous integration; Maintenance cost; Safety"
"A user survey on the adoption of crowd-based software engineering instructional screencasts by the new generation of software developers","2022","Journal of Systems and Software","10.1016/j.jss.2021.111144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122530358&doi=10.1016%2fj.jss.2021.111144&partnerID=40&md5=9b159d06ac164e75346ce3848911c383","Context: In recent years, crowd-based content in the form of instructional screencast videos has gained popularity among software engineers. For organizations to remain competitive in attracting and retaining their workforce, they must accommodate the use of such crowd-based documentation content. Objective: We conduct a user survey to gain insights on how a user's background and work tasks influence the use of different documentation media types. In our analysis we focus on how-to tutorial screencasts and how the new generation of software engineers is using such videos as an information resource. Methods: For this research, we report results from our user survey, including benefits and challenges software engineers face in using screencasts as project documentation. We discuss potential avenues on how to improve the usefulness of how-to tutorial screencasts. Results: The level of professional experience, job position or reason for resorting to a documentation, affect the type of resource being used. As most (78%) of our survey participants were junior software engineers, our survey results are in particular applicable to this user group. Conclusion: We conclude our paper with lessons learned and provide some recommendations for screencast creators, such as: building a dedicated platform, making screencasts searchable and link their content to other artifacts. © 2021 Elsevier Inc.","Crowd-based documentation; Software engineering; Tutorial screencast; User survey"
"Understanding Roxygen package documentation in R","2022","Journal of Systems and Software","10.1016/j.jss.2022.111265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125458156&doi=10.1016%2fj.jss.2022.111265&partnerID=40&md5=92a099fb39154a481b7468994bdf1566","R is a package-based programming ecosystem that provides an easy way to install third-party code, datasets, and examples. Thus, R developers rely heavily on the documentation of the packages they import to use them correctly and accurately. This documentation is often written using Roxygen, equivalent to Java's well-known Javadoc. This two-part study provides the first analysis in this area. First, 379 systematically-selected, open-source R packages were mined and analysed to address the quality of their documentation in terms of presence, distribution, and completeness to identify potential sources of documentation debt of technical debt that describes problems in the documentation. Second, a survey addressed how R package developers perceive documentation and face its challenges (with a response rate of 10.04%). Results show that incomplete documentation is the most common smell, with several cases of incorrect use of the Roxygen utilities. Unlike in traditional API documentation, developers do not focus on how behaviour is implemented but on common use cases and parameter documentation. Respondents considered the examples section the most useful, and commonly perceived challenges were unexplained examples, ambiguity, incompleteness and fragmented information. © 2022 Elsevier Inc.","Api documentation; Developers’ survey; Mining software repositories; Roxygen; Software engineering; Technical debt"
"A systematic mapping study addressing the reliability of mobile applications: The need to move beyond testing reliability","2022","Journal of Systems and Software","10.1016/j.jss.2021.111166","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121658690&doi=10.1016%2fj.jss.2021.111166&partnerID=40&md5=3c99c12299b732b281d5eaa22ed6f903","Intense competition in the mobile apps market means it is important to maintain high levels of app reliability to avoid losing users. Yet despite its importance, app reliability is underexplored in the research literature. To address this need, we identify, analyse, and classify the state-of-the-art in the field of mobile apps’ reliability through a systematic mapping study. From the results of such a study, researchers in the field can identify pressing research gaps, and developers can gain knowledge about existing solutions, to potentially leverage them in practice. We found 87 relevant papers which were then analysed and classified based on their research focus, research type, contribution, research method, study settings, data, quality attributes and metrics used. Results indicate that there is a lack of research on understanding reliability with regard to context-awareness, self-healing, ageing and rejuvenation, and runtime event handling. These aspects have rarely been studied, or if studied, there is limited evaluation. We also identified several other research gaps including the need to conduct more research in real-world industrial projects. Furthermore, little attention has been paid towards quality standards while conducting research. Outcomes here show numerous opportunities for greater research depth and breadth on mobile app reliability. © 2021 Elsevier Inc.","Evidence-based software engineering; Mapping study; Mobile app reliability; Software reliability"
"Discovering boundary values of feature-based machine learning classifiers through exploratory datamorphic testing","2022","Journal of Systems and Software","10.1016/j.jss.2022.111231","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124473834&doi=10.1016%2fj.jss.2022.111231&partnerID=40&md5=3b0c2334b3b7fd2a620e68ca8281767f","Testing has been widely recognised as difficult for AI applications. This paper proposes a set of testing strategies for testing machine learning applications in the framework of the datamorphism testing methodology. In these strategies, testing aims at exploring the data space of a classification or clustering application to discover the boundaries between classes that the machine learning application defines. This enables the tester to understand precisely the behaviour and function of the software under test. In the paper, three variants of exploratory strategies are presented with the algorithms implemented in the automated datamorphic testing tool Morphy. The correctness of these algorithms are formally proved. Their capability and cost of discovering borders between classes are evaluated via a set of controlled experiments with manually designed subjects and a set of case studies with real machine learning models. © 2022 Elsevier Inc.","Artificial intelligence; Automation of software test; Datamorphic testing; Exploratory testing; Software testing; Test strategies"
"Towards optimal quality requirement documentation in agile software development: A multiple case study","2022","Journal of Systems and Software","10.1016/j.jss.2021.111112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117699979&doi=10.1016%2fj.jss.2021.111112&partnerID=40&md5=a3e6f55bbba82ad5a36e4ab86f5bf427","Context: Agile software development (ASD) promotes minimal documentation and often prioritizes functional requirements over quality requirements (QRs). The minimal documentation emphasis may be beneficial in reducing time-to-market for software. However, it can also be a concern, especially with QRs, since they are challenging to specify and document and are crucial for software success. Therefore, understanding how practitioners perceive the importance of QR documentation is valuable because it can provide insight into how they approach this task. It also helps in developing models and guidelines that support the documentation of QRs in ASD, which is a research gap. Objective: We aim to understand practitioners’ perceptions of QR documentation and factors influencing this task to derive a model that supports optimal QR documentation in ASD. Method: We conducted a multiple case study involving 12 participants from three cases that apply ASD. Results: Practitioners identify QR documentation as important and perceive it as contributing to ensuring quality, clarifying QRs, and facilitating decision-making. Time constraints, QR awareness, and communication gaps affect QR documentation. Missing and outdated QR documentation may lead to technical debt and a lack of common understanding regarding QRs. We introduce a model to support optimal QR documentation in ASD by focusing on the factors: time constraints, QR awareness, and communication gaps. The model provides a representation and explanation of the factors affecting QR documentation in ASD and identifies mitigation strategies to overcome issues that may occur due to these factors. Conclusion: The study reveals the importance of documenting QRs in ASD. It introduces a model that is based on empirical knowledge of QR documentation practices in ASD. Both practitioners and researchers can potentially benefit from the model. For instance, practitioners can analyze how time constraints or QR awareness affect documentation, see potential issues that may arise from them, and utilize strategies suggested by the model to address these issues. Researchers can learn about QR documentation in ASD and utilize the model to understand the topic. They can also use the study as a baseline to investigate the topic with other cases. © 2021 The Author(s)","Agile software development; Documentation; Non-functional requirements; Quality requirements"
"Multi-paradigm modeling for cyber–physical systems: A systematic mapping review","2022","Journal of Systems and Software","10.1016/j.jss.2021.111081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117711462&doi=10.1016%2fj.jss.2021.111081&partnerID=40&md5=dc1ddd50ca098529b93e97bd7b60c2ae","Cyber–Physical Systems (CPS) are heterogeneous and require cross-domain expertise to model. The complexity of these systems leads to questions about prevalent modeling approaches, their ability to integrate heterogeneous models, and their relevance to the application domains and stakeholders. The methodology for Multi-Paradigm Modeling (MPM) of CPS is not yet fully established and standardized, and researchers apply existing methods for modeling of complex systems and introducing their own. No systematic review has been previously performed to create an overview of the field on the methods used for MPM of CPS. In this paper, we present a systematic mapping study that determines the models, formalisms, and development processes used over the last decade. Additionally, to determine the knowledge necessary for developing CPS, our review studied the background of actors involved in modeling and authors of surveyed studies. The results of the survey show a tendency to reuse multiple existing formalisms and their associated paradigms, in addition to a tendency towards applying transformations between models. These findings suggest that MPM is becoming a essential approach to model CPS, and highlight the importance of future integration of models, standardization of development process and education. © 2021 Elsevier Inc.","Cyber–Physical System; Development process; Formalism; Model; Modeling paradigm; Systematic review"
"An empirical comparison of four Java-based regression test selection techniques","2022","Journal of Systems and Software","10.1016/j.jss.2021.111174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122328878&doi=10.1016%2fj.jss.2021.111174&partnerID=40&md5=8b7ee2be89c00e38436f5c8461816a54","Regression testing is a critical but expensive activity that ensures previously tested functionality is not broken by changes made to the code. Regression test selection (RTS) techniques aim to select and run only those test cases impacted by code changes. The techniques possess different characteristics related to their selection accuracy, test suite size reduction, time to select and run the test cases, and the fault detection ability of the selected test cases. This paper presents an empirical comparison of four Java-based RTS techniques (Ekstazi, HyRTS, OpenClover and STARTS) using multiple revisions from five open source projects. The results show that STARTS selects more test cases than Ekstazi and HyRTS. OpenClover selects the most test cases. Safety and precision violations measure to what extent a technique misses test cases that should be selected and selects only the test cases that are impacted. Using HyRTS as the baseline, OpenClover had significantly worse safety violations compared to STARTS and Ekstazi, and significantly worse precision violations compared to Ekstazi. While STARTS and Ekstazi did not differ on safety violations, Ekstazi had significantly fewer precision violations than STARTS. The average fault detection ability of the RTS techniques was 8.75% lower than the original test suite. © 2021 Elsevier Inc.","Dynamic analysis; Precision; Regression test selection; Safety; Static analysis; Test suite reduction"
"Agile methods used by traditional logistics companies and logistics start-ups: a systematic literature review","2022","Journal of Systems and Software","10.1016/j.jss.2022.111328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129319003&doi=10.1016%2fj.jss.2022.111328&partnerID=40&md5=2e6465f7ef2ae34aa113f82d1a1aa96b","An increasing number of companies and start-ups aims to enhance their agility through the use of agile methods and practices in order to cope with raising product development and project complexity and quickly changing requirements (Laanti et al. (2011); Schön et al. (2017); Könnölä et al. (2016)). Especially in the logistics industry, which is known as a slow adaptor to changes in general but regarding new innovations in particular (Cockburn and Highsmith (2001); Beck (2000); Abbas et al. (2008)), it is relevant to see how these companies cope with change. Opposed to that, logistics start-ups seem to be able to create customer value with disruptive products and services. This paper aims to capture the current state of the literature related to the use of agile methods and practices in established logistics companies and logistics start-ups. Of particular interest will be analyzing which methods and practices are used, what specific challenges established logistics companies and logistics start-ups aim to solve with these agile methods and practices, and the difficulties they face in doing so. A systematic literature review (SLR) with an extensive quality assessment of the included nine studies was conducted. After the analysis, insights on the following points were derived: use of agile methods and practices, the challenges that are solved with these methods and practices, and difficulties in the application of these methods and practices. Future research should deepen these findings with, for instance, qualitative data from real-life cases of logistics companies and start-ups. The originality of the SLR presented lies in its contribution to the largely unexplored field of agility in traditional logistics companies and logistics start-ups, as well as its assessment of the state-of-the-art literature analyzed. © 2022 Elsevier Inc.","Agile methods; Literature review; Logistics; Start-ups; Traditional companies"
"Efficient computation of minimal weak and strong control closure","2022","Journal of Systems and Software","10.1016/j.jss.2021.111140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119421199&doi=10.1016%2fj.jss.2021.111140&partnerID=40&md5=e43b55e0a09369a97f1b2dc6f6d11ad4","Control dependency is a fundamental concept in many program analyses, transformation, parallelization, and compiler optimization techniques. An overwhelming number of definitions of control dependency relations are found in the literature that capture various kinds of program control flow structures. Weak and strong control closure (WCC and SCC) relations capture nontermination insensitive and sensitive control dependencies and subsume all previously defined control dependency relations. In this paper, we have shown that static dependency-based program slicing requires the repeated computation of WCC and SCC. The state-of-the-art WCC and SCC algorithm provided by Danicic et al. has the cubic and the quartic worst-case complexity in terms of the size of the control flow graph and is a major obstacle to be used in static program slicing. We have provided a simple yet efficient method to compute the minimal WCC and SCC which has the quadratic and cubic worst-case complexity and proved the correctness of our algorithms. We implemented ours and the state-of-the-art algorithms in the Clang/LLVM compiler framework and run experiments on a number of SPEC CPU 2017 benchmarks. Our WCC method performs a maximum of 23.8 times and on average 10.6 times faster than the state-of-the-art method to compute WCC. The performance curves of our WCC algorithm for practical applications are closer to the NlogN curve in the microsecond scale. Our SCC method performs a maximum of 226.86 times and on average 67.66 times faster than the state-of-the-art method to compute SCC. Evidently, we improve the practical performance of WCC and SCC computation by an order of magnitude. © 2021 The Author(s)","Control dependency; Nontermination (in)sensitive; Program slicing; Strong control closure; Weak control closure"
"Improving test case selection by handling class and attribute noise","2022","Journal of Systems and Software","10.1016/j.jss.2021.111093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122509530&doi=10.1016%2fj.jss.2021.111093&partnerID=40&md5=dcd9b056a13bedbfed539d9fb158d169","Big data and machine learning models have been increasingly used to support software engineering processes and practices. One example is the use of machine learning models to improve test case selection in continuous integration. However, one of the challenges in building such models is the large volume of noise that comes in data, which impedes their predictive performance. In this paper, we address this issue by studying the effect of two types of noise, called class and attribute, on the predictive performance of a test selection model. For this purpose, we analyze the effect of class noise by using an approach that relies on domain knowledge for relabeling contradictory entries and removing duplicate ones. Thereafter, an existing approach from the literature is used to experimentally study the effect of attribute noise removal on learning. The analysis results show that the best learning is achieved when training a model on class-noise cleaned data only — irrespective of attribute noise. Specifically, the learning performance of the model reported 81% precision, 87% recall, and 84% f-score compared with 44% precision, 17% recall, and 25% f-score for a model built on uncleaned data. Finally, no causality relationship between attribute noise removal and the learning of a model for test case selection was drawn. © 2021 The Author(s)","Attribute noise; Class noise; Test case selection"
"Wayback Machine: A tool to capture the evolutionary behavior of the bug reports and their triage process in open-source software systems","2022","Journal of Systems and Software","10.1016/j.jss.2022.111308","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127143612&doi=10.1016%2fj.jss.2022.111308&partnerID=40&md5=2d31a88ca8253e09fc22bed3277fdfac","The issue tracking system (ITS) is a rich data source for data-driven decision-making. Different characteristics of bugs, such as severity, priority, and time to fix, provide a clear picture of an ITS. Nevertheless, such information may be misleading. For example, the exact time and the effort spent on a bug might be significantly different from the actual reporting time and the fixing time. Similarly, these values may be subjective, e.g., severity and priority values are assigned based on the intuition of a user or a developer rather than a structured and well-defined procedure. Hence, we explore the evolution of the bug dependency graph together with priority and severity levels to explore the actual triage process. Inspired by the idea of the “Wayback Machine” for the World Wide Web, we aim to reconstruct the historical decisions made in the ITS. Therefore, any bug prioritization or bug triage algorithms/scenarios can be applied in the same environment using our proposed ITS Wayback Machine. More importantly, we track the evolutionary metrics in the ITS when a custom triage/prioritization strategy is employed. We test the efficiency of the proposed algorithm using data extracted from three open-source projects. Our empirical study sheds light on the overlooked evolutionary metrics – e.g., overdue bugs and developers’ loads – which are facilitated via our proposed past-event re-generator. © 2022 Elsevier Inc.","Bug dependency graph; Bug prioritization; Defect management; Simulation; Software quality"
"Self-adaptive mobile web service discovery framework for Dynamic Mobile Environment","2022","Journal of Systems and Software","10.1016/j.jss.2021.111120","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118184970&doi=10.1016%2fj.jss.2021.111120&partnerID=40&md5=a2a762fff47495d4a8ed1d8c87a3db71","This paper proposes a self-adaptive mobile web service (MWS) discovery framework for a dynamic mobile environment (DME) to deal with MWS proliferation, dynamic context, and irrelevant MWS discovery challenges. The main contribution of this research includes an improvement of the matchmaking algorithm, enhanced MWS categorization approach, and extensible meta-context ontology that represents the context information in DME. This was achieved by enabling the self-adaptive matchmaker to learn MWS relevance using a Modified-Negative Selection Algorithm (M-NSA) and retrieve the most relevant MWS based on the current context of the discovery. To assess the proposed framework, series of experiments was carried out using publicly-available datasets. The performance of the framework is evaluated against the state-of-the-art frameworks. It was found that the proposed framework is more effective and attained better binary and graded relevance when subjected to context variations which are prevalent in DME. This is useful for service-based application designers and other MWS clients. © 2021 Elsevier Inc.","Dynamic Mobile Environment; Mobile web service; Negative Selection Algorithm; Self-adaptive; Service discovery"
"Developers’ need for the rationale of code commits: An in-breadth and in-depth study","2022","Journal of Systems and Software","10.1016/j.jss.2022.111320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128151737&doi=10.1016%2fj.jss.2022.111320&partnerID=40&md5=6c995674229d738c60c4274c59d7f464","Communicating the rationale behind decisions is essential for the success of software engineering projects. In particular, understanding the rationale of code commits is an important and often difficult task. Although the software engineering community recognizes rationale need and importance, there is a lack of in-depth study of rationale for commits. To bridge this gap, we apply a mixed-methods approach, interviewing software developers and distributing two surveys, to study their perspective of rationale for code commits. We found that software developers need to investigate code commits to understand their rationale when working on diverse tasks. We also found that developers decompose the rationale of code commits into 15 components, each is differently needed, found, and recorded. Furthermore, we explored software developers’ experiences with rationale need, finding, and recording. We discovered factors leading software developers to give up their search for rationale of code commits. Our findings provide a better understanding of the need for rationale of code commits. In light of our findings, we discuss and present our vision about rationale of code commits practitioners’ documentation, tools support, and documentation automation. In addition, we discuss the benefits of analyzes that could arise from good documentation of rationale for code commits. © 2022","Software changes rationale; Software evolution and maintenance"
"Sharing runtime permission issues for developers based on similar-app review mining","2022","Journal of Systems and Software","10.1016/j.jss.2021.111118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118496709&doi=10.1016%2fj.jss.2021.111118&partnerID=40&md5=ccaa53e71999220b75a58b9e7501e7e8","The Android operating system introduces an ask-on-first-use permission policy after 6.0 version to regulate access to user data, which raises Permission-Related Issues (PRIS for short). Relevant research has been conducted to identify the PRIS through investigating users’ opinions towards runtime permissions. These efforts mainly focus on helping users understand and be aware of permissions, but neglect to assist developers in discovering permission requirements. In this paper, we propose a novel framework named PRISharer, which mines potential permission issues from the reviews of similar apps to assist developers in discovering possible permission requirements at runtime. PRISharer first builds a deep fine-grained classifier to identify similar apps, and then employs sentiment analysis based keywords extraction to mine permission-related reviews from similar apps’ reviews. Finally, the <category, permission, issues> mappings based on a multi-label learning method are generated to provide a PRIS profile for developers. The results of comparative experiments on more than 12 million reviews of 17,741 Android apps demonstrate that PRISharer achieves (i) superior performance in terms of F1-score for PRIS analysis, with an average improvement of 24.4%, (ii) the best recall (89.3%) in extracting permission-related reviews and (iii) 82.4% positive responses by expert developers, through which the effectiveness of PRISharer is well verified. © 2021 Elsevier Inc.","Android; Machine learning; Permission-Related Issues (PRIS); Runtime permission; User reviews"
"Prevalence, common causes and effects of technical debt: Results from a family of surveys with the IT industry","2022","Journal of Systems and Software","10.1016/j.jss.2021.111114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118545933&doi=10.1016%2fj.jss.2021.111114&partnerID=40&md5=a52b366394c13e3eb49a6d3efe6efc8c","Context: The technical debt (TD) metaphor describes actions made during various stages of software development that lead to a more costly future regarding system maintenance and evolution. According to recent studies, on average 25% of development effort is spent, i.e. wasted, on TD caused issues in software development organizations. However, further research is needed to investigate the relations between various software development activities and TD. Objective: The objective of this study is twofold. First, to get empirical insight on the understanding and the use of the TD concept in the IT industry. Second, to contribute towards precise conceptualization of the TD concept through analysis of causes and effects. Method: In order to address the research objective a family of surveys was designed as a part of an international initiative that congregates researchers from 12 countries—InsighTD. At country level, national teams ran survey replications with industry practitioners from the respective countries. Results: In total 653 valid responses were collected from 6 countries. Regarding the prevalence of the TD concept 22% of practitioners have only theoretical knowledge about it, and 47% have some practical experiences with TD identification or management. Further analysis indicated that senior practitioners who work in larger organizations, larger teams, and on larger systems are more likely to be experienced with TD management. Time pressure or deadlinewas the single most cited cause of TD. Regarding the effects of TD: delivery delay, low maintainability, and rework were the most cited. Conclusion: InsighTD is the first family of surveys on technical debt in software engineering. It provided a methodological framework that allowed multiple replication teams to conduct research activities and to contribute to a single dataset. Future work will focus on more specific aspects of TD management. © 2021 Elsevier Inc.","Causes of technical debt; Effects of technical debt; InsighTD; Survey; Technical debt"
"HIPPO: A formal-model execution engine to control and verify critical real-time systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.111033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110246445&doi=10.1016%2fj.jss.2021.111033&partnerID=40&md5=2c7345e78de227521c278a9693555b24","The design of embedded real-time systems requires specific toolchains to guarantee time constraints and safe behavior. These tools and their artifacts need to be managed in a coherent way all along the design process and need to address timing constraints and execution semantic in a holistic way during the system's modeling, verification, and implementation phases. However, modeling languages used by these tools do not always share a common semantic. This can introduce a dangerous gap between what designers want to express, what is verified and the behavior of the final executable code. In order to address this problem, we propose a new toolchain, called HIPPO, that integrates tools for design, verification and execution built around a common formalism. Our approach is based on an extension of the FIACRE specification language with runtime features, such as asynchronous function calls and synchronization with events. We formally define the behavior of these additions and describe a compiler to generate both an executable code and a verifiable model from the same high-level specification. The execution of the resulting code is supported by a dedicated execution engine that guarantees real-time behavior and that reduces the semantic gap between high-level models and executable code. We illustrate our approach with a non-trivial use case: the autonomous navigation of a Segway RMP440 robotic platform. We describe how we derive a HIPPO model from an initial specification of the system based on the robotics programming framework [Formula presented]. We also show how to use the HIPPO runtime to control this robot, and how to use formal verification in order to check critical properties on this system. © 2021","Formal toolchain; Robotic case study; Verifiable implementation"
"A model-driven approach for continuous performance engineering in microservice-based systems","2022","Journal of Systems and Software","10.1016/j.jss.2021.111084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117098236&doi=10.1016%2fj.jss.2021.111084&partnerID=40&md5=47e70199c168eed6e6f288b9b64e7894","Microservices are quite widely impacting on the software industry in recent years. Rapid evolution and continuous deployment represent specific benefits of microservice-based systems, but they may have a significant impact on non-functional properties like performance. Despite the obvious relevance of this property, there is still a lack of systematic approaches that explicitly take into account performance issues in the lifecycle of microservice-based systems. In such a context of evolution and re-deployment, Model-Driven Engineering techniques can provide major support to various software engineering activities, and in particular they can allow managing the relationships between a running system and its architectural model. In this paper, we propose a model-driven integrated approach that exploits traceability relationships between the monitored data of a microservice-based running system and its architectural model to derive recommended refactoring actions that lead to performance improvement. The approach has been applied and validated on two microservice-based systems, in the domain of e-commerce and ticket reservation, respectively, whose architectural models have been designed in UML profiled with MARTE. © 2021 The Author(s)","Continuous deployment; Microservices; Model-driven engineering; Performance engineering; Software evolution; Software refactoring"
"Understanding the perceived relevance of capability measures: A survey of Agile Software Development practitioners","2021","Journal of Systems and Software","10.1016/j.jss.2021.111013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107903841&doi=10.1016%2fj.jss.2021.111013&partnerID=40&md5=ee98af1d21b7ee513c74d11005b708f1","Context: In the light of the swift and iterative nature of Agile Software Development (ASD) practices, establishing deeper insights into capability measurement within the context of team formation is crucial, as the capability of individuals and teams can affect team performance and productivity Although a former Systematic Literature Review (SLR) synthesized the state of the art in relation to capability measurement in ASD – with a focus on selecting individuals to agile teams, and capabilities related to team performance, productivity and success determining to what degree the SLR's results apply to practice can provide progressive insights to both research and practice. Objective: Our study investigates how agile practitioners perceive the relevance of individual and team level measures for characterizing the capability of an agile team and its members. Here, the emphasis was also on selecting individuals to agile teams, and capabilities associated with effective teams in terms of their performance, productivity and success. Furthermore, to scrutinize variations in practitioners’ perceptions, our study further analyzes perceptions across stratified demographic groups. Method: We undertook a Web-based survey using a questionnaire built based on the capability measures identified from a previously conducted SLR. Results: Our survey responses (60) indicate that 127 individual and 28 team capability measures were considered as relevant by the majority of practitioners. We also identified seven individual and one team capability measure that have not been previously characterized by our SLR. The surveyed practitioners suggested that an agile team member's responsibility and questioning skills significantly represent the member's capability. Conclusion: Results from our survey align with our SLR's findings. Measures associated with social aspects were observed to be dominant compared to technical and innovative aspects. Our results can support agile practitioners in their team composition decisions. © 2021 Elsevier Inc.","Agile Software Development; Agile team formation; Capability measurement; Individual capability; Survey; Team capability"
"Model-based testing in practice: An experience report from the web applications domain","2021","Journal of Systems and Software","10.1016/j.jss.2021.111032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109076346&doi=10.1016%2fj.jss.2021.111032&partnerID=40&md5=84f7c5b7372922273e3a5ab3a443afcf","In the context of a software testing company, we have deployed the model-based testing (MBT) approach to take the company's test automation practices to higher levels of maturity and capability. We have chosen, from a set of open-source/commercial MBT tools, an open-source tool named GraphWalker, and have pragmatically used MBT for end-to-end test automation of several large web and mobile applications under test. The MBT approach has provided, so far in our project, various tangible and intangible benefits in terms of improved test coverage (number of paths tested), improved test-design practices, and also improved real-fault detection effectiveness. The goal of this experience report (applied research report), done based on “action research”, is to share our experience of applying and evaluating MBT as a software technology (technique and tool) in a real industrial setting. We aim at contributing to the body of empirical evidence in industrial application of MBT by sharing our industry-academia project on applying MBT in practice, the insights that we have gained, and the challenges and questions that we have faced and tackled so far. We discuss an overview of the industrial setting, provide motivation, explain the events leading to the outcomes, discuss the challenges faced, summarize the outcomes, and conclude with lessons learned, take-away messages, and practical advices based on the described experience. By learning from the best practices in this paper, other test engineers could conduct more mature MBT in their test projects. © 2021 Elsevier Inc.","Applied research report; Experience report; Model-based testing; Software testing; Test automation; Web applications"
"A three-step hybrid specification approach to error prevention","2021","Journal of Systems and Software","10.1016/j.jss.2021.110975","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105692548&doi=10.1016%2fj.jss.2021.110975&partnerID=40&md5=743cd911dc0a63f375fc1ebd92c27048","Effectively preventing errors in requirements analysis and design is extremely important for enhancing software productivity and reliability, but how to fulfill this goal remains an open problem. In this paper, we propose a concept of hybrid specification and describe a novel three-step hybrid specification approach to address this problem. We discuss how the three-step approach can be used to effectively prevent errors in the early phases of development. The expected effect of the approach is to strike a good balance between enhancing productivity and ensuring the reliability of the program implemented. We present a controlled experiment to evaluate the effectiveness of the approach. The result of the experiment shows that our method can detect and prevent 28.36% more errors than a comparable traditional requirements analysis method. © 2021 Elsevier Inc.","Formal methods; Hybrid specification; Requirements analysis; Software productivity; Software reliability"
"Application of agile methods in traditional logistics companies and logistics startups: Results from a German Delphi Study","2021","Journal of Systems and Software","10.1016/j.jss.2021.110950","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104974400&doi=10.1016%2fj.jss.2021.110950&partnerID=40&md5=7714a02f37fa7c194312396fd090a6b0","To meet changing requirements and rising product complexity, a growing number of traditional logistics companies and logistics startups are increasing their agility through the use of progressively agile methods. The objective of the Delphi Study is to assess how traditional logistics companies and logistics startups use agile methods in their IT departments, what benefits they realise and what challenges they face introducing and using agile methods. A modified Delphi Study was conducted over three complementary rounds as an iterative expert judgment process. After the analysis of the results, insights were gained on the following points covering traditional logistics companies and logistics startups: (a) used agile methods and practices, (b) perceived benefits that these methods offer and (c) challenges of applying these methods. The results of the Delphi Study show that traditional logistics companies as well as logistics startups chose similar agile methods and practices. Both company types aim to realise mainly the same benefits but face different challenges regarding the introduction of agile methods. The Delphi Study's originality lies in its contribution to the largely unexplored area of agility in the field of logistics. © 2021","Agile methods; Delphi Study; Logistics; Startups; Traditional companies"
"Using metamorphic relations to verify and enhance Artcode classification","2021","Journal of Systems and Software","10.1016/j.jss.2021.111060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114125392&doi=10.1016%2fj.jss.2021.111060&partnerID=40&md5=e414bb31f3d15841477fb733324687ec","Software testing is often hindered where it is impossible or impractical to determine the correctness of the behaviour or output of the software under test (SUT), a situation known as the oracle problem. An example of an area facing the oracle problem is automatic image classification, using machine learning to classify an input image as one of a set of predefined classes. An approach to software testing that alleviates the oracle problem is metamorphic testing (MT). While traditional software testing examines the correctness of individual test cases, MT instead examines the relations amongst multiple executions of test cases and their outputs. These relations are called metamorphic relations (MRs): if an MR is found to be violated, then a fault must exist in the SUT. This paper examines the problem of classifying images containing visually hidden markers called Artcodes, and applies MT to verify and enhance the trained classifiers. This paper further examines two MRs, Separation and Occlusion, and reports on their capability in verifying the image classification using one-way analysis of variance (ANOVA) in conjunction with three other statistical analysis methods: t-test (for unequal variances), Kruskal–Wallis test, and Dunnett's test. In addition to our previously-studied classifier, that used Random Forests, we introduce a new classifier that uses a support vector machine, and present its MR-augmented version. Experimental evaluations across a number of performance metrics show that the augmented classifiers can achieve better performance than non-augmented classifiers. This paper also analyses how the enhanced performance is obtained. © 2021 Elsevier Inc.","Artcode; Classification; Machine learning; Metamorphic relation; Metamorphic testing; Software verification"
"An empirical study of COVID-19 related posts on Stack Overflow: Topics and technologies","2021","Journal of Systems and Software","10.1016/j.jss.2021.111089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116645122&doi=10.1016%2fj.jss.2021.111089&partnerID=40&md5=d2cbe37834f411f338729058ae8a9174","The COVID-19 outbreak, also known as the coronavirus pandemic, has left its mark on every aspect of our lives and at the time of this writing is still an ongoing battle. Beyond the immediate global-wide health response, the pandemic has triggered a significant number of IT initiatives to track, visualize, analyze and potentially mitigate the phenomenon. For individuals or organizations interested in developing COVID-19 related software, knowledge-sharing communities such as Stack Overflow proved to be an effective source of information for tackling commonly encountered problems. As an additional contribution to the investigation of this unprecedented health crisis and to assess how fast and how well the community of developers has responded, we performed a study on COVID-19 related posts in Stack Overflow. In particular, we profiled relevant questions based on key post features and their evolution, identified the most prominent technologies adopted for developing COVID-19 software and their interrelations and focused on the most persevering problems faced by developers. For the analysis of posts we employed descriptive statistics, Association Rule Graphs, Survival Analysis and Latent Dirichlet Allocation. The results reveal that the response of the developers’ community to the pandemic was immediate and that the interest of developers on COVID-19 related challenges was sustained after its initial peak. In terms of the problems addressed, the results show a clear focus on COVID-19 data collection, analysis and visualization from/to the web, in line with the general needs for monitoring the pandemic. © 2021 Elsevier Inc.","COVID-19; Knowledge-sharing; Pandemic; StackOverflow"
"An extensive study on smell-aware bug localization","2021","Journal of Systems and Software","10.1016/j.jss.2021.110986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107653377&doi=10.1016%2fj.jss.2021.110986&partnerID=40&md5=1d4f23a24b8e2a94ef969e4d1bc63859","Bug localization is an important aspect of software maintenance because it can locate modules that should be changed to fix a specific bug. Our previous study showed that the accuracy of the information retrieval (IR)-based bug localization technique improved when used in combination with code smell information. Although this technique showed promise, the study showed limited usefulness because of the small number of: (1) projects in the dataset, (2) types of smell information, and (3) baseline bug localization techniques used for assessment. This paper presents an extension of our previous experiments on Bench4BL, the largest bug localization benchmark dataset available for bug localization. In addition, we generalized the smell-aware bug localization technique to allow different configurations of smell information, which were combined with various bug localization techniques. Our results confirmed that our technique can improve the performance of IR-based bug localization techniques for the class level even when large datasets are processed. Furthermore, because of the optimized configuration of the smell information, our technique can enhance the performance of most state-of-the-art bug localization techniques. © 2021 The Author(s)","Bug localization; Code smell; Information retrieval"
"Runtime verification of train control systems with parameterized modal live sequence charts","2021","Journal of Systems and Software","10.1016/j.jss.2021.110962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103765140&doi=10.1016%2fj.jss.2021.110962&partnerID=40&md5=fc56d266e44ca8781cb6a2ab9d3de69f","With the growing complexity of railway control systems, it is required to preform runtime safety checks of system executions that go beyond conventional runtime monitoring of pre-programmed safety conditions. Runtime verification is a lightweight and rigorous formal method that dynamically analyses execution traces against some formal specifications. A challenge in applying this method in railway systems is defining a suitable monitoring specification language, i.e., a language that is expressive, of reasonable complexity, and easy to understand. In this paper, we propose parameterized modal live sequence charts (PMLSCs) by introducing the alphabet of the specification into charts to distinguish between silent events and unexpected events. We further investigate the expressiveness and complexity theories of the language. In particular, we prove that PMLSCs are closed under negation and the complexity of a subclass of PMLSCs is linear, which allows the language to be used to monitor a system online. Finally, we use PMLSCs to monitor an RBC system in the Chinese high-speed railway and evaluate the performance. The experimental results show that the PMLSC has high monitoring efficiency, and can reduce false alarm rate by introducing alphabets of charts. © 2021 Elsevier Inc.","Live sequence chart; Runtime verification; Train control system"
"Synthesizing researches on Knowledge Management and Agile Software Development using the Meta-ethnography method","2021","Journal of Systems and Software","10.1016/j.jss.2021.110973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104665465&doi=10.1016%2fj.jss.2021.110973&partnerID=40&md5=9253ea36246310c3636bcd183bc38e8f","Context: Software development processes are considered as knowledge intensive and therefore Knowledge Management (KM) can be applied to efficiently manage the knowledge generated. Agile practices can benefit software organizations in terms of KM. Some studies have already presented evidence about this relationship. However, the intersection of these two areas still requires further clarification. Objective: This study synthesizes research on KM and Agile Software Development (ASD) using the meta-ethnography method considering Scrum and XP frameworks. Method: In order to achieve the proposed goal, first, a diagnostic was conducted in different project domains with agile and traditional software development in order to identify the performance of KM activities. Second, the phases of the meta-ethnography analysis method were applied in a set of studies selected from a tertiary review on KM and ASD, as well as classic guides and area references. Finally, the relationships that were identified among the areas investigated were analyzed from interviews with agile development methodology experts. Results: The most common activity investigated between KM and ASD is knowledge sharing. However, in the practical view of software development companies, the attention is on how to use the generated knowledge. Conclusion: The clarification of how KM is present in each agile value, practices, and artifacts allows a reflection on how much knowledge was created, shared, and applied during ASD. Besides, such results presented in this study enable organizations to know each other better and to explore more each KM activity, thus contributing to delivering more value to the customer. © 2021 Elsevier Inc.","Agile software development; Knowledge management; Meta-ethnography; Scrum; XP"
"A parallel worklist algorithm and its exploration heuristics for static modular analyses","2021","Journal of Systems and Software","10.1016/j.jss.2021.111042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111205266&doi=10.1016%2fj.jss.2021.111042&partnerID=40&md5=943fd7d3d1e0cc4f10afd0a520e80c9a","One way to speed up static programme analysis is to make use of today's multi-core CPUs by parallelising the analysis. Existing work on parallel analysis usually targets traditional data-flow analyses for static, first-order languages such as C. Less attention has been given so far to the parallelisation of more general analyses that can also target dynamic, higher-order languages such as JavaScript. These are significantly more challenging to parallelise, as dependencies between analysis results are only discovered during the analysis itself. State-of-the-art parallel analyses for such languages are therefore usually limited, both in their applicability and performance gains. In this work, we propose the parallelisation of modular analyses. Modular analyses compute different parts of the analysis in isolation of one another, and therefore offer inherent opportunities for parallelisation that have not been explored so far. In addition, they can be used to develop a general class of analysers for dynamic, higher-order languages. We present a parallel variant of the worklist algorithm that is used to drive such modular analyses. To further speed up its convergence, we show how this algorithm can exploit the monotonicity of the analysis. Existing modular analyses can be parallelised without additional effort by instead employing this parallel worklist algorithm. We demonstrate this for MODF, an inter-procedural modular analysis, and for MODCONC, an inter-process modular analysis. For MODCONC, we reveal an additional opportunity to exploit even more parallelism in the analysis: analyses of individual MODCONC components can themselves be parallel, resulting in a doubly-parallel exploration. Finally, we present several heuristics for the exploration order of the analysis and discuss how they can impact its performance. The parallel worklist algorithm and the exploration heuristics are implemented for and integrated into MAF, a framework for modular programme analysis. On a set of Scheme benchmarks for MODF, we observe speedups between 3× and 8× when using 4 workers, and speedups between 8× and 32× when using 16 workers, with a maximum speedup of 333× using 128 workers. For MODCONC, we achieve a maximum speedup of 37× with 32 workers. We observe that on a MODF analysis, among 11 exploration heuristics, the heuristics prioritising either components with smaller environments or with less dependencies result in consistent speedups that can reach 20× those of a random exploration strategy. We find a clear correlation between the mean number of dependencies in a programme and the speedup obtained by this heuristic. © 2021 Elsevier Inc.","Concurrency; Dynamic Languages; Modular analysis; Parallelism; Static programme analysis"
"Enhancing the analysis of software failures in cloud computing systems with deep learning","2021","Journal of Systems and Software","10.1016/j.jss.2021.111043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110198322&doi=10.1016%2fj.jss.2021.111043&partnerID=40&md5=04edfb50ec44a893f7220012df7aa6ae","Identifying the failure modes of cloud computing systems is a difficult and time-consuming task, due to the growing complexity of such systems, and the large volume and noisiness of failure data. This paper presents a novel approach for analyzing failure data from cloud systems, in order to relieve human analysts from manually fine-tuning the data for feature engineering. The approach leverages Deep Embedded Clustering (DEC), a family of unsupervised clustering algorithms based on deep learning, which uses an autoencoder to optimize data dimensionality and inter-cluster variance. We applied the approach in the context of the OpenStack cloud computing platform, both on the raw failure data and in combination with an anomaly detection pre-processing algorithm. The results show that the performance of the proposed approach, in terms of purity of clusters, is comparable to, or in some cases even better than manually fine-tuned clustering, thus avoiding the need for deep domain knowledge and reducing the effort to perform the analysis. In all cases, the proposed approach provides better performance than unsupervised clustering when no feature engineering is applied to the data. Moreover, the distribution of failure modes from the proposed approach is closer to the actual frequency of the failure modes. © 2021 Elsevier Inc.","Cloud computing; Deep learning; Failure mode analysis; Fault injection; OpenStack; Software failures"
"A sustainable-development approach for self-adaptive cyber–physical system's life cycle: A systematic mapping study","2021","Journal of Systems and Software","10.1016/j.jss.2021.111010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107766571&doi=10.1016%2fj.jss.2021.111010&partnerID=40&md5=d448645aacc3ff1b088bd1b3a1cb12db","Cyber–Physical Systems (CPS) refer to a new generation of systems where the cyber and physical layers are –strongly– interconnected. The development of these systems requires two fundamental parts. First, the design of sustainable architectures –centered on adaptation, throughout a System-Development Life-Cycle (SDLC)– to develop robust and economically profitable products. Second, the use of self-adaptive techniques to adjust CPSs to the evolving circumstances of their operation context. This work presents a systematic mapping study (SMS) that discusses different approaches used to develop self-adaptive CPSs (SA-CPSs) at each stage of the SDLC, focused on sustainability. The results show trends such as (i) Designs are not limited to particular application domains, (ii) Performance was the most commonly used attribute, and (iii) Monitor–Analyze–Plan–Execute over a shared Knowledge (MAPE-K) is the predominant feedback loop applied in the cyber layer. The results also raise challenges such as (i) How to design and evaluate sustainable SA-CPSs, (ii) How to apply unit and integration testing in the development of SA-CPSs, and (iii) How to develop feedback loops on SA-CPSs with the integration of machine-learning techniques. © 2021 Elsevier Inc.","Cyber–physical systems; Self-adaptive systems; Sustainability; Systems-development life-cycle"
"Android code smells: From introduction to refactoring","2021","Journal of Systems and Software","10.1016/j.jss.2021.110964","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103980784&doi=10.1016%2fj.jss.2021.110964&partnerID=40&md5=b30344c223ab18ca8c1d9eb79139ac32","Object-oriented code smells are well-known concepts in software engineering that refer to bad design and development practices commonly observed in software systems. With the emergence of mobile apps, new classes of code smells have been identified by the research community as mobile-specific code smells. These code smells are presented as symptoms of important performance issues or bottlenecks. Despite the multiple empirical studies about these new code smells, their diffuseness and evolution along change histories remains unclear. We present in this article a large-scale empirical study that inspects the introduction, evolution, and removal of Android code smells. This study relies on data extracted from 324 apps, a manual analysis of 561 smell-removing commits, and discussions with 25 Android developers. Our findings reveal that the high diffuseness of mobile-specific code smells is not a result of releasing pressure. We also found that the removal of these code smells is generally a side effect of maintenance activities as developers do not refactor smell instances even when they are aware of them. © 2021 Elsevier Inc.",""
"Automated defect prioritization based on defects resolved at various project periods","2021","Journal of Systems and Software","10.1016/j.jss.2021.110993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107687766&doi=10.1016%2fj.jss.2021.110993&partnerID=40&md5=5afdecd9713030f71be856b5900f1acb","Defect prioritization is mainly a manual and error-prone task in the current state-of-the-practice. We evaluated the effectiveness of an automated approach that employs supervised machine learning. We used two alternative techniques, namely a Naive Bayes classifier and a Long Short-Term Memory model. We performed an industrial case study with a real project from the consumer electronics domain. We compiled more than 15,000 issues collected over 3 years. We could reach an accuracy level up to 79.36% and we had 3 observations. First, Long Short-Term Memory model has a better accuracy when compared with a Naive Bayes classifier. Second, structured features lead to better accuracy compared to textual descriptions. Third, accuracy is not improved by considering increasingly earlier defects as part of the training data. Increasing the size of the training data even decreases the accuracy compared to the results, when we use data only regarding the recently resolved defects. © 2021 Elsevier Inc.","Defect prioritization; Industrial case study; Machine learning; Process automation; Software maintenance"
"Improving observability in Event Sourcing systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.111015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111319549&doi=10.1016%2fj.jss.2021.111015&partnerID=40&md5=39e78999e0a40d4b3001754faf390b5d","Event Sourcing (ES) systems use an event log with the double purpose of keeping application state and providing decoupled communication. While ES systems keep track of all business events, other untracked events, either from internal components or from external systems may still cause failures. Determining the root cause of such failures usually involves complex procedures based on replaying the event log. Unlike this, in distributed systems, developers often instrument the source code, for the sake of improving observability and perform tracing on workflows and data. Adding tracing to ES thus seems like an unexplored and powerful approach to improve the observability of the system. In this paper, we suggest possible implementations of the idea and discuss their merits. These include the adoption of well-known tracing-related tools and standards in ES systems, with the respective advantages for root-cause analysis, anomaly detection, profiling and others. © 2021","Distributed systems; Event Sourcing; Logging; Microservices; Tracing"
"Adapting Behavior Driven Development (BDD) for large-scale software systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.110944","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103117256&doi=10.1016%2fj.jss.2021.110944&partnerID=40&md5=47ee8b0299705845e8808ea79d5fb0f4","Context: Large-scale software projects require interaction between many stakeholders. Behavior-driven development (BDD) facilitates collaboration between stakeholders, and an adapted BDD process can help improve cooperation in a large-scale project. Objective: The objective of this study is to propose and empirically evaluate a BDD based process adapted for large-scale projects. Method: A technology transfer model was used to propose a BDD based process for large-scale projects. We conducted six workshop sessions to understand the challenges and benefits of BDD. Later, an industrial evaluation was performed for the process with the help of practitioners. Results: From our investigations, understanding of a business aspect of requirements, their improved quality, a guide to system-level use-cases, reuse of artifacts, and help for test organization are found as benefits of BDD. Practitioners identified the following challenges: specification and ownership of behaviors, adoption of new tools, the software projects’ scale, and versioning of behaviors. We proposed a process to address these challenges and evaluated the process with the help of practitioners. Conclusion: The evaluation proved that BDD could be adapted and used to facilitate interaction in large-scale software projects in the software industry. The feedback from the practitioners helped in improving the proposed process. © 2021 The Author(s)","BDD; Behavior-driven; Large-scale; Software processes; System of systems"
"Characterizing top ranked code examples in Google","2021","Journal of Systems and Software","10.1016/j.jss.2021.110971","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104157943&doi=10.1016%2fj.jss.2021.110971&partnerID=40&md5=d8ee20f31d37fc9af7e4bc5ee508afc1","Developers often look for code examples on the web to improve learning and accelerate development. Google indexes millions of pages with code examples: pages with better content are likely to be top ranked. In practice, many factors may influence the rank: page reputation, content quality, etc. Consequently, the most relevant information on the page, i.e., the code example, may be overshadowed by the search engine. Thus, a better understanding of how Google would rank code examples in isolation may provide the basis to detect its strengths and limitations on dealing with such content. In this paper, we assess how the Google search engine ranks code examples. We build a website with 1,000 examples and submit it to Google. After being fully indexed, we query and analyze the returned examples. We find that pages with multiple code examples are more likely to top ranked by Google. Overall, single code examples that are higher ranked are larger, however, they are not necessarily more readable and reusable. We predict top ranked examples with a good level of confidence, but generic factors have more importance than code quality ones. Based on our results, we provide insights for researchers and practitioners. © 2021 Elsevier Inc.","API usage; Code examples; Code search; Google search engine; Software quality"
"User behavior pattern mining and reuse across similar Android apps","2022","Journal of Systems and Software","10.1016/j.jss.2021.111085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115971196&doi=10.1016%2fj.jss.2021.111085&partnerID=40&md5=f20a19d11eef9d90ad1485f95607691d","Nowadays, Android apps have penetrated all aspects of our lives. Despite their popularity, understanding their behaviors is still a challenging task. Considering that many Android apps are in the same category and share similar workflows, in this paper, we propose a user behavior pattern mining and reuse approach across similar Android apps, thereby reducing the cost of understanding new apps. Particularly, for a specific new app, to figure out its typical behaviors, the behavior patterns that refer to the frequently-occurring workflows can be obtained from another similar app and transferred to this app. Moreover, to reuse the behavior patterns on this app, a semantic-based event fuzzy matching strategy and continuous workflow generation strategy are raised to generate workflows for this app. To evaluate our approach's effectiveness and rationality, we conduct a series of experiments on 25 Android apps in five categories. Furthermore, the experimental results show that 88.3% of behavior patterns can be completely reused on similar apps, and the generated workflows cover 89.1% of the top 20% of important states. © 2021 Elsevier Inc.","Android apps; Behavior pattern reuse; GUI model; Semantic-based event fuzzy matching"
"Accessibility in the mobile development industry in Brazil: Awareness, knowledge, adoption, motivations and barriers","2021","Journal of Systems and Software","10.1016/j.jss.2021.110942","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102881390&doi=10.1016%2fj.jss.2021.110942&partnerID=40&md5=c27d586127ac77f38161861915be14c6","Accessibility is a quality sub-characteristic intended to make software products accessible to a broad range of users, regardless of their physical, motor, intellectual or cognitive skills. Mobile accessibility has been on the spotlight recently due to the increasing shift towards mobile platforms. Thus, many advances have been made in this field when it comes to the development of assistive technologies, conception of well-documented accessibility guidelines and supporting tools. However, a general lack of accessibility in mobile applications has been observed in several studies. Researchers have conducted investigations to understand the same phenomenon in web development from the perspective of those involved in the development process, but studies concerning accessibility in the context of mobile applications are still scarce. This paper presents a survey conducted with 872 people involved in mobile application development in the Brazilian industry to gather information on their awareness, adoption, motivations and barriers to ensure digital accessibility. Results show that most participants have moderate accessibility awareness, but low levels of knowledge or adoption in practice. In addition, accessibility is usually not considered in their projects due to lack of requirements, time, training and focus on users with disabilities. © 2021 Elsevier Inc.","Accessibility; Awareness; Barriers; Development; Mobile; Survey"
"On misbehaviour and fault tolerance in machine learning systems","2022","Journal of Systems and Software","10.1016/j.jss.2021.111096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116351574&doi=10.1016%2fj.jss.2021.111096&partnerID=40&md5=8c5aa6e19ea2dc2cba467254b20cb554","Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts. © 2021 The Author(s)","Case study; Fault tolerance; Machine learning; Software architecture; Software engineering"
"On the Understandability of Language Constructs to Structure the State and Behavior in Abstract State Machine Specifications: A Controlled Experiment","2021","Journal of Systems and Software","10.1016/j.jss.2021.110987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107694051&doi=10.1016%2fj.jss.2021.110987&partnerID=40&md5=6700ab0f9177dfd7526a0ae6f79717cd","Abstract State Machine (ASM) theory is a well-known state-based formal method to analyze and specify software and hardware systems. As in other state-based formal methods, the proposed modeling languages for ASMs still lack easy-to-comprehend abstractions to structure state and behavior aspects of specifications. Modern object-oriented languages offer a variety of advanced language constructs, and most of them either offer interfaces, mixins, or traits in addition to classes and inheritance. Our goal is to investigate these language constructs in the context of state-based formal methods using ASMs as a representative of this kind of formal methods. We report on a controlled experiment with 105 participants to study the understandability of the three language constructs in the context of ASMs. Our hypotheses are influenced by the debate of object-oriented communities. We hypothesized that the understandability (measured by correctness and duration variables) shows significantly better understanding for interfaces and traits compared to mixins, as well as at least a similar or better understanding for traits compared to interfaces. The results indicate that understandability of interfaces and traits show a similar good understanding, whereas mixins shows a poorer understanding. We found a significant difference for the correctness of understanding comparing interfaces with mixins. © 2021 The Author(s)","Abstract State Machines; Controlled experiment; Empirical software engineering; Language constructs; Understandability"
"Observation-based approximate dependency modeling and its use for program slicing","2021","Journal of Systems and Software","10.1016/j.jss.2021.110988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107756982&doi=10.1016%2fj.jss.2021.110988&partnerID=40&md5=09c9ccc6680b40e3ffa59f96d9b1b30e","While dependency analysis is foundational to much program analysis, many techniques have limited scalability and handle only monolingual systems. We present a novel dependency analysis technique that aims to approximate program dependency from a relatively small number of perturbed executions. Our technique, MOAD (Modeling Observation-based Approximate Dependency), reformulates program dependency as the likelihood that one program element is dependent on another (instead of a Boolean relationship). MOAD generates program variants by deleting parts of the source code and executing them while observing the impact. MOAD thus infers a model of program dependency that captures the relationship between the modification and observation points. We evaluate MOAD using program slices obtained from the resulting probabilistic dependency models. Compared to the existing observation-based backward slicing technique, ORBS, MOAD requires only 18.6% of the observations, while the resulting slices are only 12% larger on average. Furthermore, we introduce the notion of the observation-based forward slices. Unlike ORBS, which inherently computes backward slices, MOAD's model's dependences can be traversed in either direction allowing us to easily compute forward slices. In comparison to the static forward slice, MOAD only misses deleting 0–6 lines (median 0), while excessively deleting 0–37 lines (median 8) from the slice. © 2021 Elsevier Inc.","Dependency analysis; MOAD; Model learning; Program slicing"
"Systematic literature review of validation methods for AI systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.111050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112415717&doi=10.1016%2fj.jss.2021.111050&partnerID=40&md5=5effdf8b47fc5b128e3b499b5557b7d2","Context: Artificial intelligence (AI) has made its way into everyday activities, particularly through new techniques such as machine learning (ML). These techniques are implementable with little domain knowledge. This, combined with the difficulty of testing AI systems with traditional methods, has made system trustworthiness a pressing issue. Objective: This paper studies the methods used to validate practical AI systems reported in the literature. Our goal is to classify and describe the methods that are used in realistic settings to ensure the dependability of AI systems. Method: A systematic literature review resulted in 90 papers. Systems presented in the papers were analysed based on their domain, task, complexity, and applied validation methods. Results: The validation methods were synthesized into a taxonomy consisting of trial, simulation, model-centred validation, and expert opinion. Failure monitors, safety channels, redundancy, voting, and input and output restrictions are methods used to continuously validate the systems after deployment. Conclusions: Our results clarify existing strategies applied to validation. They form a basis for the synthesization, assessment, and refinement of AI system validation in research and guidelines for validating individual systems in practice. While various validation strategies have all been relatively widely applied, only few studies report on continuous validation. © 2021 The Author(s)","Artificial intelligence; Machine learning; Systematic literature review; Testing; V&V; Validation"
"Automated refactoring of legacy JavaScript code to ES6 modules","2021","Journal of Systems and Software","10.1016/j.jss.2021.111049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112000790&doi=10.1016%2fj.jss.2021.111049&partnerID=40&md5=d4e2f8e0a68267585a4e6f5bdd019115","The JavaScript language did not specify, until ECMAScript 6 (ES6), native features for streamlining encapsulation and modularity. Developer community filled the gap with a proliferation of design patterns and module formats, with impact on code reusability, portability and complexity of build configurations. This work studies the automated refactoring of legacy ES5 code to ES6 modules with fine-grained reuse of module contents through the named import/export language constructs. The focus is on reducing the coupling of refactored modules through destructuring exported module objects to fine-grained module features and enhancing module dependencies by leveraging the ES6 syntax. We employ static analysis to construct a model of a JavaScript project, the Module Dependence Graph (MDG), that represents modules and their dependencies. On the basis of MDG we specify the refactoring procedure for module migration to ES6. A prototype implementation has been empirically evaluated on 19 open source projects. Results highlight the relevance of the refactoring with a developer intent for fine-grained reuse. The analysis of refactored code shows an increase in the number of reusable elements per project and reduction in the coupling of refactored modules. The soundness of the refactoring is empirically validated through code inspection and execution of projects’ test suites. © 2021 Elsevier Inc.","AMD/CommonJS; Code migration; ES6 modules; Module object destructuring; Refactoring"
"An empirical study on the co-occurrence between refactoring actions and Self-Admitted Technical Debt removal","2021","Journal of Systems and Software","10.1016/j.jss.2021.110976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104701235&doi=10.1016%2fj.jss.2021.110976&partnerID=40&md5=465d320d5df377851f8a956da06c8c8f","Technical Debt (TD) concerns the lack of an adequate solution in a software project, from its design to the source code. Its admittance through source code comments, issues, or commit messages is referred to as Self-Admitted Technical Debt (SATD). Previous research has studied SATD from different perspectives, including its distribution, impact on software quality, and removal. In this paper, we investigate the relationship between refactoring and SATD removal. By leveraging a dataset of SATD and their removals in four open-source projects and by using an automated refactoring detection tool, we study the co-occurrence of refactoring and SATD removals. Results of the study indicate that refactoring is more likely to co-occur with SATD removals than with other commits, however, in most cases, they belong to different quality improvement activities performed at the same time. Moreover, if looking closely at refactoring actions co-occurring with SATD removal in the same code entities, a relationship between these activities can be found. Finally, we found how both source code quality metrics and SATD removals play a statistically significant role in the likelihood that the commit applies a refactoring action. © 2021 Elsevier Inc.","Self-Admitted Technical Debt; Software quality; Software refactoring"
"Memory efficient context-sensitive program analysis","2021","Journal of Systems and Software","10.1016/j.jss.2021.110952","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103928999&doi=10.1016%2fj.jss.2021.110952&partnerID=40&md5=da0f6a5e00505e1c9ed6a817e9663d70","Static program analysis is in general more precise if it is sensitive to execution contexts (execution paths). But then it is also more expensive in terms of memory consumption. For languages with conditions and iterations, the number of contexts grows exponentially with the program size. This problem is not just a theoretical issue. Several papers evaluating inter-procedural context-sensitive data-flow analysis report severe memory problems, and the path-explosion problem is a major issue in program verification and model checking. In this paper we propose χ-terms as a means to capture and manipulate context-sensitive program information in a data-flow analysis. χ-terms are implemented as directed acyclic graphs without any redundant subgraphs. To show the efficiency of our approach we run experiments comparing the memory usage of χ-terms with four alternative data structures. Our experiments show that χ-terms clearly outperform all the alternatives in terms of memory efficiency. © 2021 Elsevier Inc.","Context-sensitivity; Data-flow analysis; Static program analysis"
"A novel blockchain protocol for selecting microservices providers and auditing contracts","2021","Journal of Systems and Software","10.1016/j.jss.2021.111030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108401821&doi=10.1016%2fj.jss.2021.111030&partnerID=40&md5=6569cb4a12cedec911e13e0eee5408fb","Software architectures based on containers and microservices are often used to develop and manage large-scale distributed applications. Still, large vertical deployments spanning over multiple cloud and edge infrastructures are cumbersome to negotiate for, as each infrastructure provider is usually unique concerning prices, management strategies and Quality-of-Service (QoS) levels. In this scenario, Service Level Agreement (SLA) contracts are primarily crafted through pre-established templates and clients must trust providers to manage provisioned resources. The present paper proposes Dawn, a novel blockchain protocol for selecting microservice providers and auditing contracts. The protocol exploits the distributed and verifiable storage of a blockchain, as well as its decentralized consensus to enable contracts establishments in unreliable environments. Besides providing a formal definition of the protocol, this work discusses the possible threats to the correct operation of the network, originated by tenants and providers. We show that Dawn is secure under the evaluated terms, that it can efficiently help the contract establishment process as well as it guarantees a functional systematic way of auditing through monitoring. Finally, we studied both best and worst case scenarios regarding the number of issued messages, stored data volume and network traffic to execute Dawn with different numbers of clients and providers. © 2021 Elsevier Inc.","Audit; Blockchain; Microservices; Protocol; Selection; SLA"
"Mining Architecture Tactics and Quality Attributes knowledge in Stack Overflow","2021","Journal of Systems and Software","10.1016/j.jss.2021.111005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108894191&doi=10.1016%2fj.jss.2021.111005&partnerID=40&md5=101ca1327133c7ebdaf0fc00f20eb50a","Context: Architecture Tactics (ATs) are architectural building blocks that provide general architectural solutions for addressing Quality Attributes (QAs) issues. Mining and analysing QA–AT knowledge can help the software architecture community better understand architecture design. However, manually capturing and mining this knowledge is labour-intensive and difficult. Objective: Using Stack Overflow (SO) as our source, our main goals are to effectively mine such knowledge; and to have some sense of how developers use ATs with respect to QA concerns from related discussions. Methods: We applied a semi-automatic dictionary-based mining approach to extract the QA–AT posts in SO. With the mined QA–AT posts, we identified the relationships between ATs and QAs. Results: Our approach allows us to mine QA–AT knowledge accurately with an F-measure of 0.865 and Performance of 82.2%. Using this mining approach, we are able to discover architectural synonyms of QAs and ATs used by designers, from which we discover how developers apply ATs to address quality requirements. Conclusions: We make two contributions in this work: First, we demonstrated a semi-automatic approach to mine ATs and QAs from SO posts; Second, we identified little-known design relationships between QAs and ATs and grouped architectural design considerations to aid architects make architecture tactics design decisions. © 2021","Architecture Tactic; Empirical analysis; Knowledge mining; Quality Attribute; Stack Overflow"
"A hybrid code representation learning approach for predicting method names","2021","Journal of Systems and Software","10.1016/j.jss.2021.111011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107616552&doi=10.1016%2fj.jss.2021.111011&partnerID=40&md5=e96fd839823d7d7ff6490f49b2347f9b","Program semantic properties such as class names, method names, and variable names and types play an important role in software development and maintenance. Method names are of particular importance because they provide the cornerstone of abstraction for developers to communicate with each other for various purposes (e.g., code review and program comprehension). Existing method name prediction approaches often represent code as lexical tokens or syntactical AST (abstract syntax tree) paths, making them difficult to learn code semantics and hindering their effectiveness in predicting method names. Initial attempts have been made to represent code as execution traces to capture code semantics, but suffer scalability in collecting execution traces. In this paper, we propose a hybrid code representation learning approach, named METH2SEQ, to encode a method as a sequence of distributed vectors. METH2SEQ represents a method as (1) a bag of paths on the program dependence graph, (2) a sequence of typed intermediate representation statements and (3) a sentence of natural language comment, to scalably capture code semantics. The learned sequence of vectors of a method is fed to a decoder model to predict method names. Our evaluation with a dataset of 280.5K methods in 67 Java projects has demonstrated that METH2SEQ outperforms the two state-of-the-art code representation learning approaches in F1-score by 92.6% and 36.6%, while also outperforming two state-of-the-art method name prediction approaches in F1-score by 85.6% and 178.1%. © 2021 Elsevier Inc.","Code representation learning; Deep learning; Method name prediction"
"BASBA: A framework for Building Adaptable Service-Based Applications","2021","Journal of Systems and Software","10.1016/j.jss.2021.110989","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105270115&doi=10.1016%2fj.jss.2021.110989&partnerID=40&md5=d4557f97e80d5cd4decac83cc51e70f8","Due to the continuously changing environment of service-based applications (SBAs), the ability to adapt to environmental and contextual changes has become a crucial characteristic of such applications. Providing SBAs with this ability is a complex task, usually carried out in an unsystematic way and interwoven with application logic. As a result, developing and maintaining adaptive SBAs has become a costly and hardly repeatable process. The objective of this paper is to present a model-based approach to developing adaptive SBAs which separates development of adaptation concerns from development of SBAs behaviors. This approach aims to facilitate and improve the development of adaptive behaviors. In this paper, the process of developing an adaptive SBA is defined as specifying adaptive SBA models based on a metamodel and reusable adaptation tactics. These models are then transformed into runtime model artifacts and running system units performing runtime adaptive behaviors. The approach introduces a systematic method to derive adaptation behaviors from adaptation models, which facilitates the development of adaptive behaviors. The empirical evaluations in three studies show that our approach enhances the development of adaptive behaviors in terms of identifying more proper adaptation plans, reducing the development time, and increasing understandability, modifiability, and correctness of code. © 2021 Elsevier Inc.","Models at runtime; Quality of service; Reusability; Self-adaptation; Service-based application; Variability"
"Mining guidelines for architecting robotics software","2021","Journal of Systems and Software","10.1016/j.jss.2021.110969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105348947&doi=10.1016%2fj.jss.2021.110969&partnerID=40&md5=43fe9fdc6a02160d4d5219420efc94df","Context: The Robot Operating System (ROS) is the de-facto standard for robotics software. However, ROS-based systems are getting larger and more complex and could benefit from good software architecture practices. Goal: We aim at (i) unveiling the state-of-the-practice in terms of targeted quality attributes and architecture documentation in ROS-based systems, and (ii) providing empirically-grounded guidance to roboticists about how to properly architect ROS-based systems. Methods: We designed and conducted an observational study where we (i) built a dataset of 335 GitHub repositories containing real open-source ROS-based systems, and (ii) mined the repositories to extract and synthesize quantitative and qualitative findings about how roboticists are architecting ROS-based systems. Results: First, we extracted an empirically-grounded overview of the state of the practice for architecting and documenting ROS-based systems. Second, we synthesized a catalog of 47 architecting guidelines for ROS-based systems. Third, the extracted guidelines were validated by 119 roboticists working on real-world open-source ROS-based systems. Conclusion: Roboticists can use our architecting guidelines for applying good design principles to develop robots that meet quality requirements, and researchers can use our results as evidence-based indications about how real-world ROS systems are architected today, thus inspiring future research contributions. © 2021 The Author(s)","Robotics; ROS; Software architecture"
"The symptoms, causes, and repairs of bugs inside a deep learning library","2021","Journal of Systems and Software","10.1016/j.jss.2021.110935","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103945065&doi=10.1016%2fj.jss.2021.110935&partnerID=40&md5=d05a75e9a1ed4197614d69699e7eafca","In recent years, deep learning has become a hot research topic. Although it achieves incredible positive results in some scenarios, bugs inside deep learning software can introduce disastrous consequences, especially when the software is used in safety-critical applications. To understand the bug characteristic of deep learning software, researchers have conducted several empirical studies on bugs in deep learning applications. Although these studies present useful findings, we notice that none of them analyze the bug characteristic inside a deep learning library like TensorFlow. We argue that some fundamental questions of bugs in deep learning libraries are still open. For example, what are the symptoms and the root causes of bugs inside TensorFlow, and where are they? As the underlying library of many deep learning projects, the answers to these questions are useful and important, since its bugs can have impacts on many deep learning projects. In this paper, we conduct the first empirical study to analyze the bugs inside a typical deep learning library, i.e., TensorFlow. Based on our results, we summarize 8 findings, and present our answers to 4 research questions. For example, we find that the symptoms and root causes of TensorFlow bugs are more like ordinary projects (e.g., Mozilla) than other machine learning libraries (e.g., Lucene). As another example, we find that most TensorFlow bugs reside in its interfaces (26.24%), learning algorithms (11.79%), and how to compile (8.02%), deploy (7.55%), and install (4.72%) TensorFlow across platforms. © 2021 Elsevier Inc.","Bug analysis; Deep learning; Empirical study; TensorFlow"
"The ratio of equivalent mutants: A key to analyzing mutation equivalence","2021","Journal of Systems and Software","10.1016/j.jss.2021.111039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111273855&doi=10.1016%2fj.jss.2021.111039&partnerID=40&md5=6289fabddf1eb4314455db7ba5da86b4","Mutation testing is the art of generating syntactic versions (called mutants) of a base program, and is widely used in software testing, most notably the assessment of test suites. Mutants are useful only to the extent that they are semantically distinct from the base program, but some may well be semantically equivalent to the base program, despite being syntactically distinct. Much research has been devoted to identifying, and weeding out, equivalent mutants, but determining whether two programs are semantically equivalent is a non-trivial, tedious, error-prone task. Yet in practice it is not necessary to identify equivalent mutants individually; for most intents and purposes, it suffices to estimate their number. In this paper, we are interested to estimate, for a given number of mutants generated from a program, the ratio of those that are equivalent to the base program; we refer to this as the Ratio of Equivalent Mutants (REM, for short). We argue, on the basis of analytical grounds, that the REM of a program may be estimated from a static analysis of the program, and that it can be used to analyze many mutation related properties of a program. The purpose/ aspiration of this paper is to draw attention to this potentially cost-effective approach to a longstanding stubborn problem. © 2021 Elsevier Inc.","Mutation equivalence; Mutation testing; Ratio of equivalent mutants; Redundancy"
"Brain and autonomic nervous system activity measurement in software engineering: A systematic literature review","2021","Journal of Systems and Software","10.1016/j.jss.2021.110946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104401249&doi=10.1016%2fj.jss.2021.110946&partnerID=40&md5=5176242dd0c3cd0375748e11d1693b5e","In the past decade, brain and autonomic nervous system activity measurement received increasing attention in the study of software engineering (SE). This paper presents a systematic literature review (SLR) to survey the existing NeuroSE literature. Based on a rigorous search protocol, we identified 89 papers (hereafter denoted as NeuroSE papers). We analyzed these papers to develop a comprehensive understanding of who had published NeuroSE research and classified the contributions according to their type. The 47 articles presenting completed empirical research were analyzed in detail. The SLR revealed that the number of authors publishing NeuroSE research is still relatively small. The thematic focus so far has been on code comprehension, while code inspection, programming, and bug fixing have been less frequently studied. NeuroSE publications primarily used methods related to brain activity measurement (particularly fMRI and EEG), while methods related to the measurement of autonomic nervous system activity (e.g., pupil dilation, heart rate, skin conductance) received less attention. We also present details of how the empirical research was conducted, including stimuli and independent and dependent variables, and discuss implications for future research. The body of NeuroSE literature is still small. Yet, high quality contributions exist constituting a valuable basis for future studies. © 2021 The Author(s)","Brain and autonomic nervous system activity measurements; Electroencephalography (EEG); Functional magnetic resonance imaging (fMRI); Heart- and skin-related measurements; Software engineering; Systematic literature review"
"Tigris: A DSL and framework for monitoring software systems at runtime","2021","Journal of Systems and Software","10.1016/j.jss.2021.110963","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104064076&doi=10.1016%2fj.jss.2021.110963&partnerID=40&md5=0727553d7acee33ec85b0292199f1603","The understanding of the behavioral aspects of a software system is an essential enabler for many software engineering activities, such as adaptation. This involves collecting runtime data from the system so that it is possible to analyze the collected data to guide actions upon the system. Consequently, software monitoring imposes practical challenges because it is often done by intercepting the system execution and recording gathered information. Such monitoring may degrade the performance and disrupt the system execution to unacceptable levels. In this paper, we introduce a two-phase monitoring approach to support the monitoring step in adaptive systems. The first phase collects lightweight coarse-grained information and identifies relevant parts of the software that should be monitored in detail based on a provided domain-specific language. This language is informed by a systematic literature review. The second phase collects relevant and fine-grained information needed for deciding whether and how to adapt the managed system. Our approach is implemented as a framework, called Tigris, that can be seamlessly integrated into existing software systems to support monitoring-based activities. To validate our proposal, we instantiated Tigris to support an application-level caching approach, which adapts caching decisions of a software system at runtime to improve its performance. © 2021 Elsevier Inc.","Caching; Execution trace; Logging; Monitoring; Performance; Sampling"
"Comparison of search strategies for feature location in software models","2021","Journal of Systems and Software","10.1016/j.jss.2021.111037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110520624&doi=10.1016%2fj.jss.2021.111037&partnerID=40&md5=95f3712d12e7bb2c1fa76ae70926662e","Search-based model-driven engineering is the application of search-based techniques to specific problems that are related to software engineering that is driven using software models. In this work, we make use of measures from the literature to report feature location problems in models (size and volume of the model and density, multiplicity, and dispersion of the feature being located) and a set of search strategies (random search, iterated local search, hill climbing, an evolutionary algorithm, and a hybrid between an evolutionary algorithm and hill climbing). The goal is to analyze of the impact of different values that are used to describe the feature location problems and the performance obtained by the different search strategies. We apply the search strategies to 1895 feature location problems that are obtained from 40 industrial software product lines. This work shows that: 1) the best results overall are obtained by a hybrid between evolutionary algorithm and hill climbing; 2) the size of the search space has the greatest impact on the results obtained by the search strategies; and 3) the impact of each of the measures is not the same in the five search strategies. This work highlights the use of the search strategy that produces the best results. In addition, we provide recommendations on when to use each search strategy. © 2021 Elsevier Inc.","Feature location in models; Search strategies"
"Stability evaluation for text localization systems via metamorphic testing","2021","Journal of Systems and Software","10.1016/j.jss.2021.111040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111852151&doi=10.1016%2fj.jss.2021.111040&partnerID=40&md5=514b456958dbeeeff2e35f4ba1563009","The success of learning techniques in solving a variety of hard AI problems promotes the flourish of recognition-based applications. Many state-of-the-art text localization systems, which can detect and report the positions of text segments in an image, are mainly implemented with learning-based techniques. Data-driven learning raises a series of questions on how to verify, validate and evaluate such learning-based systems. In this paper, we propose a methodology to automatically evaluate the stability of text localization systems via metamorphic relations, where a stable system should output consistent results for similar inputs with the same text segments. We introduce six metamorphic relations that should be preserved in a stable text localization system and define the corresponding metrics for stability evaluation. With the defined metamorphic relations, we apply metamorphic testing techniques to compare the inputs and outputs to evaluate system stability, and further diagnose the causes of inconsistency. The extensive experimentation on both academic and commercial text localization systems demonstrates the effectiveness of our method on stability evaluation for such systems. © 2021 Elsevier Inc.","Learning techniques; Metamorphic testing; Stability; Text localization"
"Supporting IoT applications deployment on edge-based infrastructures using multi-layer feature models","2022","Journal of Systems and Software","10.1016/j.jss.2021.111086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115968962&doi=10.1016%2fj.jss.2021.111086&partnerID=40&md5=f776b736335e0a4db66f5f7b7902e6b2","Edge Computing proposes to use the nearby devices in the frontier/Edge of the access network for deploying application tasks of IoT-based systems. However, the functionality of such cyber–physical systems, which is usually distributed in several devices and computers, imposes specific requirements on the infrastructure to run properly. The evolution of an application to meet new user requirements and the high diversity of hardware and software technologies in the IoT/Edge/Cloud can complicate the deployment of continuously evolving applications. The aim of our approach is to apply Multi Layer Feature Models, which capture the variability of applications and the software and hardware infrastructure, to support the deployment in edge-based environments of cyber–physical applications. With this multi-layered approach is possible to support the evolution of application and infrastructure independently. Considering that IoT/Edge/Cloud infrastructures are usually shared by many applications, the deployment process has to assure that there will be enough resources for all of them, informing developers about the feasible alternatives. We provide four modules so that the developer can calculate what is the configuration of minimal set of devices supporting application requirements of the evolved application. In addition, the developer can find what is the application configuration that can be hosted in the current infrastructure. The successive solutions of continuous deployment generated by our approach pursue the reduction of the system energy footprint and/or execution latency. © 2021 The Author(s)","DevOps; Edge Computing; SMT Optimization; Software Product Lines; Task allocation problem; Task Deployment; Variability models"
"Practical heuristics to improve precision for erroneous function argument swapping detection in C and C++","2021","Journal of Systems and Software","10.1016/j.jss.2021.111048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111525472&doi=10.1016%2fj.jss.2021.111048&partnerID=40&md5=57346cd7719c1d75f2b00a422dd3cb7c","Argument selection defects, in which the programmer chooses the wrong argument to pass to a parameter from a potential set of arguments in a function call, is a widely investigated problem. The compiler can detect such misuse of arguments only through the argument and parameter type for statically typed programming languages. When adjacent parameters have the same type or can be converted between one another, a swapped or out of order call will not be diagnosed by compilers. Related research is usually confined to exact type equivalence, often ignoring potential implicit or explicit conversions. However, in current mainstream languages, like C++, built-in conversions between numerics and user-defined conversions may significantly increase the number of mistakes to go unnoticed. We investigated the situation for C and C++ languages where developers can define functions with multiple adjacent parameters that allow arguments to pass in the wrong order. When implicit conversions – such as parameter pairs of types [Formula presented] – are taken into account, the number of mistake-prone functions markedly increases compared to only strict type equivalence. We analysed a sample of projects and categorised the offending parameter types. The empirical results should further encourage the language and library development community to emphasise the importance of strong typing and to restrict the proliferation of implicit conversions. However, the analysis produces a hard to consume amount of diagnostics for existing projects, and there are always cases that match the analysis rule but cannot be “fixed”. As such, further heuristics are needed to allow developers to refactor effectively based on the analysis results. We devised such heuristics, measured their expressive power, and found that several simple heuristics greatly help highlight the more problematic cases. © 2021 The Author(s)","Argument selection defect; Error-prone constructs; Function parameters; Static analysis; Strong typing; Type safety"
"Adopting threat modelling in agile software development projects","2022","Journal of Systems and Software","10.1016/j.jss.2021.111090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116418591&doi=10.1016%2fj.jss.2021.111090&partnerID=40&md5=ad88d48115e38c1202b858dee82f5ac9","The goal of secure software engineering is to create software that keeps performing as intended, even when exposed to attacks. Threat modelling is considered to be a key activity to reach this goal, but has turned out to be challenging to implement in agile teams. This paper presents results from four different studies, in which we have investigated how agile teams do threat modelling today. Study A is based on observations and document analysis from five teams in a single organisation, Study B is based on interviews with eight individuals from four different organisations, Study C is based on a questionnaire survey of 45 students at two different universities, and Study D is based on interviews with seven teams in a single organisation, supplemented with document analysis. Our results include findings, challenges and current good practice related to the use of Data Flow Diagrams, STRIDE and the Microsoft Threat Modelling Tool. We also cross-check our findings with previous relevant work, and provide recommendations for making the threat modelling activities more useful to agile teams. © 2021 Elsevier Inc.","Agile; Data Flow Diagrams; MS-TMT; Software; STRIDE; Threat modelling"
"Why and how is Scrum being adapted in practice: A systematic review","2022","Journal of Systems and Software","10.1016/j.jss.2021.111110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117250893&doi=10.1016%2fj.jss.2021.111110&partnerID=40&md5=679e2472bbc61b779a693b59f1e1db71","Scrum, recognized today as the most popular agile development methodology, has been used in a wide range of settings and for varying purposes, in- and outside of the traditional software development context. The use of Scrum in non-traditional settings and for different needs led to a considerable corpus of academic literature that investigates, presents, and discusses modifications to the original method, aimed to make it fit such novel forms of application. Based on a large-scale review of extant literature, this study systematically analyses why and how Scrum was reportedly modified in different instances and contributes with a synthesis that can serve as a basis for a more systematic approach to future research and practice. We explicate nine common modification objectives for change (e.g., attaining high performance, non-standard contexts, distributed development) mapped against seven generic modification strategies (e.g., method guidance, new procedures, or artifacts). Building on our extensive literature analysis we highlight research gaps and identify promising areas for future research. © 2021 Elsevier Inc.","Agile development; Information systems development; Scrum; Systematic literature review"
"Automated identification of security discussions in microservices systems: Industrial surveys and experiments","2021","Journal of Systems and Software","10.1016/j.jss.2021.111046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111230657&doi=10.1016%2fj.jss.2021.111046&partnerID=40&md5=208ffc6b7b432ceee6d6017e974e86e9","Lack of awareness and knowledge of microservices-specific security challenges and solutions often leads to ill-informed security decisions in microservices system development. We claim that identifying and leveraging security discussions scattered in existing microservices systems can partially close this gap. We define security discussion as “a paragraph from developer discussions that includes design decisions, challenges, or solutions relating to security”. We first surveyed 67 practitioners and found that securing microservices systems is a unique challenge and that having access to security discussions is useful for making security decisions. The survey also confirms the usefulness of potential tools that can automatically identify such security discussions. We developed fifteen machine/deep learning models to automatically identify security discussions. We applied these models on a manually constructed dataset consisting of 4,813 security discussions and 12,464 non-security discussions. We found that all the models can effectively identify security discussions: an average precision of 84.86%, recall of 72.80%, F1-score of 77.89%, AUC of 83.75% and G-mean 82.77%. DeepM1, a deep learning model, performs the best, achieving above 84% in all metrics and significantly outperforms three baselines. Finally, the practitioners’ feedback collected from a validation survey reveals that security discussions identified by DeepM1 have promising applications in practice. © 2021","Automation; Deep learning; Machine learning; Microservices architecture; Security"
"Error messages in relational database management systems: A comparison of effectiveness, usefulness, and user confidence","2021","Journal of Systems and Software","10.1016/j.jss.2021.111034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110531511&doi=10.1016%2fj.jss.2021.111034&partnerID=40&md5=d2eca00715af56e0b123d9717eafc198","The database and the database management system (DBMS) are two of the main components of any information system. Structured Query Language (SQL) is the most popular query language for retrieving data from the database, as well as for many other data management tasks. During system development and maintenance, software developers use a considerable amount of time to interpret compiler error messages. The quality of these error messages has been demonstrated to affect software development effectiveness, and correctly formulating queries and fixing them when needed is an important task for many software developers. In this study, we set out to investigate how participants (N=152) experienced the qualities of error messages of four popular DBMSs in terms of error message effectiveness, perceived usefulness for finding and fixing errors, and error recovery confidence. Our results show differences between the DBMSs by three of the four metrics, and indicate a discrepancy between objective effectiveness and subjective usefulness. The results suggest that although error messages have perceived differences in terms of usefulness for finding and fixing errors, these differences may not necessarily result in differences in query fixing success rates. © 2021 The Author(s)","Compiler; Database management system; Error message; Structured query language (SQL)"
"Investigating the performance of personalized models for software defect prediction","2021","Journal of Systems and Software","10.1016/j.jss.2021.111038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110405837&doi=10.1016%2fj.jss.2021.111038&partnerID=40&md5=650f7d7d862901191d2e8fed333ddfc4","Software defect predictors exploring developer perspective reveal that code changes made by separate developers tend to have different defect patterns. Personalized defect prediction also contributes to this view and gives promising results. We aim to investigate the performance of personalized defect predictors compared to those of traditional models. We conduct an empirical study on six open-source projects for 222 developers. Personalized and traditional defect predictors are built utilizing two algorithms and cross-validation on the historical commit data, and assessed via seven performance measures and statistical tests. Our results show that personalized models (PMs) achieve an increase of up to 24% in recall for 83% of developers, while causing higher false alarm rates for 77% of developers. PMs are better for those developers who contribute to the modules with many prior contributors. Although size metrics contribute to the performance of the majority of the PMs, they significantly differ in terms of information gained from experience, diffusion and history metrics, respectively. The decision of whether a PM should be chosen over a traditional model depends on a set of factors, i.e., selected algorithm, model validation strategy or performance measures, and hence, PM performance significantly differs regarding these factors. © 2021 Elsevier Inc.","Change-level; Defect prediction; Personalized; Software recommendation systems"
"Targeting uncertainty in smart CPS by confidence-based logic","2021","Journal of Systems and Software","10.1016/j.jss.2021.111065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113284116&doi=10.1016%2fj.jss.2021.111065&partnerID=40&md5=dd27bf9da569d0e509eb8e3ab994106e","Since Smart Cyber–Physical Systems (sCPS) are complex and decentralized systems of dynamically cooperating components, architecture-based adaptation is of high importance in their design. In this context, a key challenge is that they typically operate in uncertain environments. Thus, an inherent requirement in sCPS design is the need to deal with the uncertainty of data coming from the environment. Existing approaches often rely on the fact that an adequate model of the environment and/or base probabilities or a prior distribution of data are available. In this paper, we present a specific logic (CB logic), which, based on statistical testing, allows specifying transition guards in architecture-based adaptation without requiring knowledge of the base probabilities or prior knowledge about the data distribution. Applicable in state machines’ transition guards in general, CB logic provides a number of operators over time series that simplify the filtering, resampling, and statistics-backed comparisons of time series, making the application of multiple statistical procedures easy for non-experts. The viability of our approach is illustrated on a running example and a case study demonstrating how CB logic simplifies adaptation triggers. Moreover, a library with a Java and C ++ implementation of CB logic's key operators is available on GitHub. © 2021 Elsevier Inc.","Adaptation; Smart cyber–physical systems; Software architecture; Statistical testing; Uncertainty"
"MontiThings: Model-Driven Development and Deployment of Reliable IoT Applications","2022","Journal of Systems and Software","10.1016/j.jss.2021.111087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116410296&doi=10.1016%2fj.jss.2021.111087&partnerID=40&md5=e2c9daf5a5a44f073d4510683561ec68","Internet of Things (IoT) applications are exposed to harsh conditions due to factors such as device failure, network problems, or implausible sensor values. We investigate how the inherent encapsulation of component and connector (C&C) architectures can be used to develop and deploy reliable IoT applications. Existing C&C languages for the development of IoT applications mainly focus on the description of architectures and the distribution of components to IoT devices. Furthermore, related approaches often pollute the models with low-level implementation details, tying the models to a particular platform and making them harder to understand. In this paper, we introduce MontiThings, a C&C language offering automatic error handling capabilities and a clear separation between business logic and implementation details. The error-handling methods presented in this paper can make C&C-based IoT applications more reliable without cluttering the business logic with error-handling code that is time-consuming to develop and makes the models hard to understand, especially for non-experts. © 2021 Elsevier Inc.","Architecture modeling; Code generation; Deployment; Internet of Things; Model-driven engineering"
"MeTeaM—A method for characterizing mature software metrics teams","2021","Journal of Systems and Software","10.1016/j.jss.2021.111006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107623076&doi=10.1016%2fj.jss.2021.111006&partnerID=40&md5=8134e4088fcd76ea233d60e2dd8d901d","Background: Metrics teams play an increasingly important role in handling data and information in modern software development organizations; they manage their companies’ measurement programs, collect and process data, and develop and distribute information products. Metrics teams can comprise several roles, and their set-up can differ between companies, as can the metrics maturity of host organizations. These differences impact the effectiveness and quality of a team's measurement program. Objective: Our objective was to design and evaluate a model to describe the characteristics of a mature metrics team, which efficiently designs, develops, maintains, and evolves its organization's measurement program. Method: We conducted an action research study on four metrics teams of four distinct companies. We designed and evaluated a domain-specific model for assessing the maturity of metrics teams – MeTeaM – and also assessed the four metrics teams per se. Results: Our results were two-fold: the creation of the metrics team maturity model MeTeaM and a template to assess metrics teams. Our evaluation showed that the model captures the characteristics of successful metrics teams and quantifies the maturity status of both the metrics teams and their host organizations. Conclusions: More mature metrics teams score higher in the MeTeaM model than less mature teams. The assessment provides less mature metrics teams with valuable insights on what factors to improve. Such insights can be shared with and acted upon successfully with their organizations. © 2021 The Author(s)","Measurements; MeTeaM; Metrics; Software; Software engineering; Team maturity"
"An empirical characterization of event sourced systems and their schema evolution — Lessons from industry","2021","Journal of Systems and Software","10.1016/j.jss.2021.110970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104285415&doi=10.1016%2fj.jss.2021.110970&partnerID=40&md5=3b5be998659155c8711c683abd130deb","Event sourced systems are increasing in popularity because they are reliable, flexible, and scalable. In this article, we point a microscope at a software architecture pattern that is rapidly gaining popularity in industry, but has not received as much attention from the scientific community. We do so through constructivist grounded theory, which proves a suitable qualitative method for extracting architectural knowledge from practitioners. Based on the discussion of 19 event sourced systems we explore the rationale for and the context of the event sourcing pattern. A description of the pattern itself and its relation to other patterns as discussed with practitioners is given. The description itself is grounded in the experience of 25 engineers, making it a reliable source for both new practitioners and scientists. We identify five challenges that practitioners experience: event system evolution, the steep learning curve, lack of available technology, rebuilding projections, and data privacy. For the first challenge of event system evolution, we uncover five tactics and solutions that support practitioners in their design choices when developing evolving event sourced systems: versioned events, weak schema, upcasting, in-place transformation, and copy-and-transform. © 2021 The Author(s)","CQRS; Event sourcing; Event-driven architecture; Grounded theory; Schema evolution; Software architecture patterns"
"Evaluating the effectiveness of risk containers to isolate change propagation","2021","Journal of Systems and Software","10.1016/j.jss.2021.110947","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102618546&doi=10.1016%2fj.jss.2021.110947&partnerID=40&md5=982e87402f9ce4292ccda7f667a14f11","Previous studies indicate that error-proneness risks can be isolated into risk containers created from architectural designs, to help detect and mitigate such risks early on. Like error-proneness, change propagation may lead to higher implementation and maintenance costs. We used automated tools to analyse four software development projects using three risk container types, each type based on a different architectural perspective. A strong and significant correlation between design change propagation and implementation co-change was observed for all three container types. We found that Design Rule Containers (DRCs), based on class diagrams, are the most effective for isolating change propagation because they have the least amount of container overlap, highest levels of internal coupling, highest co-change probability between classes that share containers, and the most change sets isolated in containers. Developers from two projects were able to justify why design dependencies had resulted in the top five DRCs being predicted to isolate the most change propagation. This and the previous error-proneness research suggests DRCs are an effective technique to detect and contain code maintainability risks at the design stage. These results provide some evidence that class diagrams are more useful than use case sequence diagrams for analysing maintainability risks in designs. © 2021 Elsevier Inc.","Change propagation; Maintainability; Risk; Software architecture; Technical debt"
"SeCNN: A semantic CNN parser for code comment generation","2021","Journal of Systems and Software","10.1016/j.jss.2021.111036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109986339&doi=10.1016%2fj.jss.2021.111036&partnerID=40&md5=a4ae9930818fec0e7df764501a8c6cf2","A code comment generation system can summarize the semantic information of source code and generate a natural language description, which can help developers comprehend programs and reduce time cost spent during software maintenance. Most of state-of-the-art approaches use RNN (Recurrent Neural Network)-based encoder–decoder neural networks. However, this kind of method may not generate high-quality description when summarizing the information among several code blocks that are far from each other (i.e., the long-dependency problem). In this paper, we propose a novel Semantic CNN parser SeCNN for code comment generation. In particular, we use a CNN (Convolutional Neural Network) to alleviate the long-dependency problem and design several novel components, including source code-based CNN and AST-based CNN, to capture the semantic information of the source code. The evaluation is conducted on a widely-used large-scale dataset of 87,136 Java methods. Experimental results show that SeCNN achieves better performance (i.e., 44.69% in terms of BLEU and 26.88% in terms of METEOR) and has lower execution time cost when compared with five state-of-the-art baselines. © 2021","Code comment generation; Convolutional Neural Network; Long short-term memory network; Program comprehension"
"Machine learning based success prediction for crowdsourcing software projects","2021","Journal of Systems and Software","10.1016/j.jss.2021.110965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104855804&doi=10.1016%2fj.jss.2021.110965&partnerID=40&md5=f45e1fc10566113600c6493c304315c2","Competitive Crowdsourcing Software Development is an online software development paradigm, promises the innovative, cost effective and high quality solutions on time. However, the paradigm is still in infancy and does not address the key challenges such as low rate of submissions and high risk of project failure. A significant number of software projects fail to receive a satisfactory solution and end up wasting the time and efforts of stakeholders. Therefore, the success prediction of a new software project may help stakeholders in the project crowdsourcing decision, saving their time and efforts. To this end, this study proposes a novel approach based on machine learning to predict the success of a software project for crowdsourcing platforms in terms of whether the given project will reach its completion or otherwise. First, the textual description and important attributes of software projects from TopCoder is extracted. Next, the description is preprocessed using natural language processing technologies. Then, keywords are identified using a modified keyword ranking algorithm and each software project is awarded a ranking score. Every software project is modeled as a vector that is based on the extracted attributes, its identified keywords and ranking scores. Using these vectors with their associated solution status, a support vector machine classifier is trained to predict the success of a given software project. Different machine learning classifiers are applied and it turns out that support vector machine yields the highest performance on the given dataset. Finally, the proposed approach is evaluated with history data of real software projects. The results of hold-out validation suggest that the average precision, recall, and f-measure are up to 94.53%, 99.30% and 96.85%, respectively. © 2021 Elsevier Inc.","Classification; Competitive crowdsourcing; Machine learning; Prediction; Risk"
"On the practice of semantic versioning for Ansible galaxy roles: An empirical study and a change classification model","2021","Journal of Systems and Software","10.1016/j.jss.2021.111059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114049265&doi=10.1016%2fj.jss.2021.111059&partnerID=40&md5=a31ed7abb51f8344a26381333925be0a","Ansible, a popular Infrastructure-as-Code platform, provides reusable collections of tasks called roles. Roles are often contributed by third parties, and like general-purpose libraries, they evolve. Therefore, new releases of roles need to be tagged with version numbers, for which Ansible recommends adhering to the semantic versioning format. However, roles significantly differ from general-purpose libraries, and it is not yet known what constitutes a breaking change or the addition of a feature to a role. Consequently, this can cause confusion for clients of a role and new role contributors. To alleviate this issue, we perform an empirical study on semantic versioning in Ansible roles to uncover the types of changes that trigger certain types of version bumps. Our dataset consists of over 81000 version increments spanning upwards of 8500 Ansible roles. We design a novel structural model for these roles, and implement a domain-specific structural change extraction algorithm to calculate structural difference metrics. Afterwards, we quantitatively investigate the state of semantic versioning in Ansible roles and identify the most commonly changed elements. Then, using the structural difference metrics, we train a Random Forest classifier to predict applicable version bumps for Ansible role releases. Finally, we confirm our empirical findings with a developer survey. Our observations show that although most Ansible role developers follow the semantic versioning format, it appears that they do not always consistently follow the same rules when selecting the version bump to apply. Moreover, we find that the distinction between patch and minor increments is often unclear. Therefore, we use the gained insights to formulate a number of guidelines to apply semantic versioning on Ansible roles. These guidelines can be used by role developers to ensure a clear interpretation of the version increments. © 2021 Elsevier Inc.","Ansible; Empirical study; Infrastructure as code; Mining software repositories; Semantic versioning"
"Three decades of software reference architectures: A systematic mapping study","2021","Journal of Systems and Software","10.1016/j.jss.2021.111004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107088983&doi=10.1016%2fj.jss.2021.111004&partnerID=40&md5=9ef67ce307ea298ad2120ca80990b65e","Software reference architectures have played an essential role in software systems development due to the possibility of knowledge reuse. Although increasingly adopted by industry, these architectures are not yet completely understood. This work presents a panorama on existing software reference architectures, characterizing them according to their context, goals, perspectives, application domains, design approaches, and maturity, as well as the industry involvement for their construction. For this, we planned and conducted a systematic mapping study. During last decade, the number of reference architectures in very diverse application domains has increased, resulting from efforts of industry, academia, and through their collaborations. Academic reference architectures are oriented to facilitate the reuse of architectural and domain knowledge. The industry has focused on architectures for standardization with certain maturity level. However, the great amount of architectures studied in this work have been designed without following a systematic process, and they lack the maturity to be used in real software projects. Further investigations can be oriented to gathering empirical evidences, from different sources than academic data libraries, that allow to understand how references architectures have been constructed, utilized, and maintained during the whole software life-cycle. © 2021","Reference architecture; Secondary study; Software architecture; Systematic mapping"
"Learning software configuration spaces: A systematic literature review","2021","Journal of Systems and Software","10.1016/j.jss.2021.111044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114450561&doi=10.1016%2fj.jss.2021.111044&partnerID=40&md5=ecba3100562f3148699814dafb924493","Most modern software systems (operating systems like Linux or Android, Web browsers like Firefox or Chrome, video encoders like ffmpeg, x264 or VLC, mobile and cloud applications, etc.) are highly configurable. Hundreds of configuration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, security, energy consumption, etc. Due to the combinatorial explosion and the cost of executing software, it is quickly impossible to exhaustively explore the whole configuration space. Hence, numerous works have investigated the idea of learning it from a small sample of configurations’ measurements. The pattern “sampling, measuring, learning” has emerged in the literature, with several practical interests for both software developers and end-users of configurable systems. In this systematic literature review, we report on the different application objectives (e.g., performance prediction, configuration optimization, constraint mining), use-cases, targeted software systems, and application domains. We review the various strategies employed to gather a representative and cost-effective sample. We describe automated software techniques used to measure functional and non-functional properties of configurations. We classify machine learning algorithms and how they relate to the pursued application. Finally, we also describe how researchers evaluate the quality of the learning process. The findings from this systematic review show that the potential application objective is important; there are a vast number of case studies reported in the literature related to particular domains or software systems. Yet, the huge variant space of configurable systems is still challenging and calls to further investigate the synergies between artificial intelligence and software engineering. © 2021 Elsevier Inc.","Configurable systems; Machine learning; Software product lines; Systematic literature review"
"Design, monitoring, and testing of microservices systems: The practitioners’ perspective","2021","Journal of Systems and Software","10.1016/j.jss.2021.111061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114322403&doi=10.1016%2fj.jss.2021.111061&partnerID=40&md5=5bf38769ad133522debb4edb71725408","Context: Microservices Architecture (MSA) has received significant attention in the software industry. However, little empirical evidence exists on design, monitoring, and testing of microservices systems. Objective: This research aims to gain a deep understanding of how microservices systems are designed, monitored, and tested in the industry. Methods: A mixed-methods study was conducted with 106 survey responses and 6 interviews from microservices practitioners. Results: The main findings are: (1) a combination of domain-driven design and business capability is the most used strategy to decompose an application into microservices, (2) over half of the participants used architecture evaluation and architecture implementation when designing microservices systems, (3) API gateway and Backend for frontend patterns are the most used MSA patterns, (4) resource usage and load balancing as monitoring metrics, log management and exception tracking as monitoring practices are widely used, (5) unit and end-to-end testing are the most used testing strategies, and (6) the complexity of microservices systems poses challenges for their design, monitoring, and testing, for which there are no dedicated solutions. Conclusions: Our findings reveal that more research is needed to (1) deal with microservices complexity at the design level, (2) handle security in microservices systems, and (3) address the monitoring and testing challenges through dedicated solutions. © 2021 Elsevier Inc.","Design; Industrial survey; Microservices architecture; Monitoring; Testing"
"Practical hybrid confidentiality-based analytics framework with Intel SGX","2021","Journal of Systems and Software","10.1016/j.jss.2021.111045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111696351&doi=10.1016%2fj.jss.2021.111045&partnerID=40&md5=2023a420ac5302dd204cfc32b123a8ac","Massive cloud infrastructure capabilities, including efficient, scalable, and elastic computing resources, have led to a widespread adoption of Internet of Things (IoT) cloud-enabled services. This involves giving complete control to cloud service providers (CSPs) of sensitive IoT data by moving data storage and processing in cloud. An efficient and lightweight advanced encryption standard (AES) cryptosystem can play a major role in protecting IoT data from exposure to CSPs by protecting the privacy of outsourced data. However, AES lacks computation capabilities, which is a critical factor that prevents individuals and organizations from taking full advantage of cloud computing services. When Intel software guard extensions (SGX) is used with AES cryptosystem, the developing framework can provide a practical solution to build a confidentiality-based data analytics framework for IoT-enabled applications in various domains. In this paper, a privacy-preserving data analytics framework is developed that relies on a hybrid-integrated approach, in which both software- and hardware-based solutions are applied to ensure confidentiality and process-sensitive outsourced data in the cloud environment. © 2021 Elsevier Inc.","Cloud computing; Data clustering; Data confidentiality; Intel SGX; Internet of Things"
"A holistic approach for cross-platform software development","2021","Journal of Systems and Software","10.1016/j.jss.2021.110985","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105323265&doi=10.1016%2fj.jss.2021.110985&partnerID=40&md5=b240750c7ed2e90c8de36fdac709f3c5","Cross-platform development solutions can help to make software available on different devices and platforms. But these are normally restricted to preconfigured platforms and consider that each individual solution is equal or similar to each other. As a result, developers have to resort to native development and build individual solutions, one for each device/platform, that cooperate to deliver the desired global functionality. This article presents an approach that takes advantage of existing solutions and have support for extending and including new platforms, and distributing functionality across devices. The approach is based on a general-purpose language that raises the abstraction level in order to keep the software free from platform details. Automatic transformations produce executable code that can be properly divided and deployed separately into different platforms. The proposed approach was evaluated in four ways. In the first evaluation, an existing cross-platform system was recreated using the approach. The second and third evaluations was conducted with expert and novice developers, who tested the approach in practice. The fourth evaluation introduced support for cross-platform testing. Results have brought evidence supporting the following main contributions: use of a single environment, the ability to reuse similar concepts between platforms and the potential to reduce costs. © 2021 Elsevier Inc.","Cross-platform development; General-purpose language; Model-driven development; User studies"
"ARTINALI++: Multi-dimensional Specification Mining for Complex Cyber-Physical System Security","2021","Journal of Systems and Software","10.1016/j.jss.2021.111016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108356860&doi=10.1016%2fj.jss.2021.111016&partnerID=40&md5=50ce2fb8b563c4764b0a38db445b9142","Cyber-Physical Systems (CPSes) have been investigated as a key area of research since they are the core of Internet of Things. CPSs integrate computing and communication with control and monitoring of entities in the physical world. Due to the tight coupling of cyber and physical domains, and to the possible catastrophic consequences of the malicious attacks on critical infrastructures, security is one of the key concerns. However, the exponential growth of IoT has led to deployment of CPSes without support for enforcing important security properties. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing these systems. Mining the specifications of CPSes by experts is a cumbersome and error-prone task. Therefore, it is essential to dynamically monitor the CPS to learn its common behaviors and formulate specifications for detecting malicious bugs and security attacks. Existing solutions for specification mining only combine data and events, but not time. However, time is a semantic property in CPS systems, and hence incorporating time in addition to data and events, is essential for obtaining high accuracy. This paper proposes ARTINALI++, which dynamically mines specifications in CPS systems with arbitrary size and complexity. ARTINALI++ captures the security properties by incorporating time as a substantial property of the system, and generate a multi-dimensional model for the general CPS systems. Moreover, it enhances the model through discovering invariants that represent the physical motions and distinct operational modes in complex CPS systems. We build Intrusion Detection Systems based on ARTINALI++ for three CPSes with various levels of complexity including smart meter, smart artificial pancreas and unmanned aerial vehicle, and measure their detection accuracy. We find that the ARTINALI++ significantly reduces the ratio of false positives and false negatives by 23.45% and 73.6% on average, respectively, over other dynamic specification mining tools on the three CPS platforms. © 2021 Elsevier Inc.","Cyber-Physical Systems; Intrusion Detection Systems; Program analysis; Safety; Security; Specification mining"
"Open Data Ecosystems — An empirical investigation into an emerging industry collaboration concept","2021","Journal of Systems and Software","10.1016/j.jss.2021.111088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115889980&doi=10.1016%2fj.jss.2021.111088&partnerID=40&md5=c6ddc28853e79df7e1af1d2c836e8802","Software systems are increasingly depending on data, particularly with the rising use of machine learning, and developers are looking for new sources of data. Open Data Ecosystems (ODE) is an emerging concept for data sharing under public licenses in software ecosystems, similar to Open Source Software (OSS). It has certain similarities to Open Government Data (OGD), where public agencies share data for innovation and transparency. We aimed to explore open data ecosystems involving commercial actors. Thus, we organized five focus groups with 27 practitioners from 22 companies, public organizations, and research institutes. Based on the outcomes, we surveyed three cases of emerging ODE practice to further understand the concepts and to validate the initial findings. The main outcome is an initial conceptual model of ODEs’ value, intrinsics, governance, and evolution, and propositions for practice and further research. We found that ODE must be value driven. Regarding the intrinsics of data, we found their type, meta-data, and legal frameworks influential for their openness. We also found the characteristics of ecosystem initiation, organization, data acquisition and openness be differentiating, which we advise research and practice to take into consideration. © 2021 The Author(s)","Empirical study; Open data; Open data ecosystem; Open innovation"
"Investigating and recommending co-changed entities for JavaScript programs","2021","Journal of Systems and Software","10.1016/j.jss.2021.111027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109173392&doi=10.1016%2fj.jss.2021.111027&partnerID=40&md5=7e87f129d090436cd782fea592dd3bed","JavaScript (JS) is one of the most popular programming languages due to its flexibility and versatility, but maintaining JS code is tedious and error-prone. In our research, we conducted an empirical study to characterize the relationship between co-changed software entities (e.g., functions and variables), and built a machine learning (ML)-based approach to recommend additional entity to edit given developers’ code changes. Specifically, we first crawled 14,747 commits in 10 open-source projects; for each commit, we created at least one change dependency graph (CDG) to model the referencer–referencee relationship between co-changed entities. Next, we extracted the common subgraphs between CDGs to locate recurring co-change patterns between entities. Finally, based on those patterns, we extracted code features from co-changed entities and trained an ML model that recommends entities-to-change given a program commit. According to our empirical investigation, (1) three recurring patterns commonly exist in all projects; (2) 80%–90% of co-changed function pairs either invoke the same function(s), access the same variable(s), or contain similar statement(s); (3) our ML-based approach CoRec recommended entity changes with high accuracy (73%–78%). CoRec complements prior work because it suggests changes based on program syntax, textual similarity, as well as software history; it achieved higher accuracy than two existing tools in our evaluation. © 2021","Change suggestion; JavaScript; Machine learning; Multi-entity edit"
"Deployment and communication patterns in microservice architectures: A systematic literature review","2021","Journal of Systems and Software","10.1016/j.jss.2021.111014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107980615&doi=10.1016%2fj.jss.2021.111014&partnerID=40&md5=a73a7665ad3e19645a696989928f2fdb","Context: Microservice is an architectural style that separates large systems into small functional units to provide better modularity. A key challenge of microservice architecture design frequently discussed in the literature is the identification and decomposition of the service modules. Besides this, two other key challenges can be identified, including the deployment of the modules on the corresponding execution platform, and adopted communication patterns. Objective: This study aims to identify and describe the reported deployment approaches, and the communication platforms for microservices in the current literature. Furthermore, we aim to describe the identified obstacles of these approaches as well as the corresponding research directions. Method: A systematic literature review (SLR) is conducted using a multiphase study selection process in which we reviewed a total of 239 papers. Among these, we selected 38 of them as primary studies related to the described research questions. Results: Based on our study, we could identify three types of deployment approaches and seven different communication patterns. Moreover, we have identified eight challenges related to the deployment and six challenges related to the communication concerns. Conclusion: Our study shows that in addition to the identification of modules, the deployment and communication approaches are equally crucial for a successful application of the microservice architecture style. Various deployment approaches and communication patterns appear to be useful for different contexts. The identified research directions in the literature study show that still more research is needed to cope with the current challenges. © 2021","Communication concerns; Communication patterns of microservices; Deployment challenges; Microservice architectures; Microservice deployment; Research directions"
"Finding security threats that matter: Two industrial case studies","2021","Journal of Systems and Software","10.1016/j.jss.2021.111003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107630277&doi=10.1016%2fj.jss.2021.111003&partnerID=40&md5=576ed291846a84cd1aabe2eb83429e7a","In the past decade, speed has become an essential trait of software development (e.g., agile, continuous integration, DevOps) and any inefficiency is considered unaffordable time waster. Such a fast pace causes challenges for architectural threat analysis. Leading techniques for threat analysis, like STRIDE, have the advantage of being systematic. However, they are not equipped to discern between important and less critical threats, while the threats are being discovered. Consequently, many threats are discarded at a later time, when their risk value is assessed. An alternative technique, called eSTRIDE, promises to remove these inefficiencies by focusing the analysis on the critical parts of the architecture. Yet, no empirical evidence exists about the actual effect of trading off systematicity, for a more focused attention on high-priority threats. This paper contributes with an empirical study comparing these two approaches in the context of two industrial case studies. We found that the two approaches yield the same number of security threats during a given time frame. However, participants using eSTRIDE found twice as many high-priority threats. The underlying analysis procedures cause similarities and differences in the execution. In addition, security expertise has an effect (albeit small) on the quality of analysis outcomes and execution. © 2021","Case study; Empirical software engineering; Risk; Security deskilling; STRIDE; Threat analysis"
"Facilitating program comprehension with call graph multilevel hierarchical abstractions","2021","Journal of Systems and Software","10.1016/j.jss.2021.110945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102622599&doi=10.1016%2fj.jss.2021.110945&partnerID=40&md5=4523a434a646d128a9cb3a7f93a74351","Program comprehension is a fundamental prerequisite for software maintenance and evolution. In order to understand a software structure, developers often read its codebase or documentation—if available and not outdated. Both approaches are tedious, time-consuming, and inefficient. Recent methods and tools have emerged to facilitate program comprehension, such as static call graphs, which depict the structure of the software system as a directed graph. However, the usage of call graphs still faces two main challenges: (1) large call graphs can be difficult to understand, and (2) they are limited to a single level of granularity, such as function calls. In this paper, we introduce a coarsening technique to create multi-level, hierarchical representations of the call graph. Specifically, we propose a hierarchical clustering approach of the execution paths to visualize the call graph at different granularity levels and for different software units, including packages, classes, and functions. Our overarching goal is to assist software developers in understanding the software system from a high-level of abstraction to the low-level of implementation with the ability to focus on particular parts of the system individually. To validate our approach and tool support, we conducted a user study of 18 software engineers from more than 11 industries who carried out several tasks using our system and then answered a survey. The results demonstrate that our approach is feasible to automatically construct multi-level abstractions of the call graph and hierarchically cluster them into meaningful abstractions. A video demo of the tool is available at https://rakanalanazi.github.io/CodEx/. © 2021 The Author(s)","Hierarchical clustering; Machine learning; Program comprehension; Static analysis; Static call graphs"
"Can we benchmark Code Review studies? A systematic mapping study of methodology, dataset, and metric","2021","Journal of Systems and Software","10.1016/j.jss.2021.111009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107587623&doi=10.1016%2fj.jss.2021.111009&partnerID=40&md5=410e7af91b4396eb78b535706231ea8e","Context: Code Review (CR) is the cornerstone for software quality assurance and a crucial practice for software development. As CR research matures, it can be difficult to keep track of the best practices and state-of-the-art in methodology, dataset, and metric. Objective: This paper investigates the potential of benchmarking by collecting methodology, dataset, and metric of CR studies. Methods: A systematic mapping study was conducted. A total of 112 studies from 19,847 papers published in high-impact venues between the years 2011 and 2019 were selected and analyzed. Results: First, we find that empirical evaluation is the most common methodology (65% of papers), with solution and experience being the least common methodology. Second, we highlight 50% of papers that use the quantitative method or mixed-method have the potential for replicability. Third, we identify 457 metrics that are grouped into sixteen core metric sets, applied to nine Software Engineering topics, showing different research topics tend to use specific metric sets. Conclusion: We conclude that at this stage, we cannot benchmark CR studies. Nevertheless, a common benchmark will facilitate new researchers, including experts from other fields, to innovate new techniques and build on top of already established methodologies. A full replication is available at https://naist-se.github.io/code-review/. © 2021 Elsevier Inc.","Code Review; Mapping study; Mining Software Repositories"
"A functional safety assessment method for cooperative automotive architecture","2021","Journal of Systems and Software","10.1016/j.jss.2021.110991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107825398&doi=10.1016%2fj.jss.2021.110991&partnerID=40&md5=90f239d028b8b4d93ff165a34b9c1cba","The scope of automotive functions has grown from a single vehicle as an entity to multiple vehicles working together as an entity, referred to as cooperative driving. The current automotive safety standard, ISO 26262, is designed for single vehicles. With the increasing number of cooperative driving capable vehicles on the road, it is now imperative to systematically assess the functional safety of architectures of these vehicles. Many methods are proposed to assess architectures with respect to different quality attributes in the software architecture domain, but to the best of our knowledge, functional safety assessment of automotive architectures is not explored in the literature. We present a method, that leverages existing research in software architecture and safety engineering domains, to check whether the functional safety requirements for a cooperative driving scenario are fulfilled in the technical architecture of a vehicle. We apply our method on a real-life academic prototype for a cooperative driving scenario, platooning, and discuss our insights. © 2021 The Authors","Automotive software architecture; Cooperative driving; Functional safety; ISO 26262; Platooning; Safety engineering"
"RepliComment: Identifying clones in code comments","2021","Journal of Systems and Software","10.1016/j.jss.2021.111069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115310486&doi=10.1016%2fj.jss.2021.111069&partnerID=40&md5=17efcd241f88790ff6ced182b04a3a19","Code comments are the primary means to document implementation and facilitate program comprehension. Thus, their quality should be a primary concern to improve program maintenance. While much effort has been dedicated to detecting bad smells, such as clones in code, little work has focused on comments. In this paper we present our solution to detect clones in comments that developers should fix. RepliComment can automatically analyze Java projects and report instances of copy-and-paste errors in comments, and can point developers to which comments should be fixed. Moreover, it can report when clones are signs of poorly written comments. Developers should fix these instances too in order to improve the quality of the code documentation. Our evaluation of 10 well-known open source Java projects identified over 11K instances of comment clones, and over 1,300 of them are potentially critical. We improve on our own previous work Blasi and Gorla (2018), which could only find 36 issues in the same dataset. Our manual inspection of 412 issues reported by RepliComment reveals that it achieves a precision of 79% in reporting critical comment clones. The manual inspection of 200 additional comment clones that RepliComment filters out as being legitimate, could not evince any false negative. © 2021 The Authors","Bad smell; Clones; Code comments; Software quality"
"Identifying architectural technical debt, principal, and interest in microservices: A multiple-case study","2021","Journal of Systems and Software","10.1016/j.jss.2021.110968","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104051358&doi=10.1016%2fj.jss.2021.110968&partnerID=40&md5=bf474414881d6c2a9f9b0844afc822be","Background: Using a microservices architecture is a popular strategy for software organizations to deliver value to their customers fast and continuously. However, scientific knowledge on how to manage architectural debt in microservices is scarce. Objectives: In the context of microservices applications, this paper aims to identify architectural technical debts (ATDs), their costs, and their most common solutions. Method: We conducted an exploratory multiple case study by conducting 25 interviews with practitioners working with microservices in seven large companies. Results: We found 16 ATD issues, their negative impact (interest), and common solutions to repay each debt together with the related costs (principal). Two examples of critical ATD issues found were the use of shared databases that, if not properly planned, leads to potential breaks on services every time the database schema changes and bad API designs, which leads to coupling among teams. We identified ATDs occurring in different domains and stages of development and created a map of the relationships among those debts. Conclusion: The findings may guide organizations in developing microservices systems that better manage and avoid architectural debts. © 2021 The Authors","Cost of software; Cross-company study; Qualitative analysis; Software maintainability; Software quality"
"On researcher bias in Software Engineering experiments","2021","Journal of Systems and Software","10.1016/j.jss.2021.111068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114408763&doi=10.1016%2fj.jss.2021.111068&partnerID=40&md5=82d3d500ce4eeb0c54a8220084d5d718","Researcher bias occurs when researchers influence the results of an empirical study based on their expectations, either consciously or unconsciously. Researcher bias might be due to the use of Questionable Research Practices (QRPs). In research fields like medicine, blinding techniques have been applied to counteract researcher bias. In this paper, we present two studies to increase our body of knowledge on researcher bias in Software Engineering (SE) experiments, including: (i) QRPs potentially leading to researcher bias; (ii) causes behind researcher bias; and (iii) possible actions to counteract researcher bias with a focus on, but not limited to, blinding techniques. The former is an interview study, intended as an exploratory study, with nine experts of the empirical SE community. The latter is a quantitative survey with 51 respondents, who were experts of the above-mentioned community. The findings from the exploratory study represented the starting point to design the survey. In particular, we defined the questionnaire of this survey to support the findings from the exploratory study. From the interview study, it emerged that some QRPs (e.g., post-hoc outlier criteria) are acceptable in certain cases. Also, it appears that researcher bias is perceived in SE and, to counteract researcher bias, a number of solutions have been highlighted. For example, duplicating the data analysis in SE experiments or fostering open data policies in SE conferences/journals. The findings from the interview study are mostly confirmed by those from the survey, and allowed us to delineate recommendations to counteract researcher bias in SE experiments. Some recommendations are intended for SE researchers, while others are purposeful for the boards of SE research venues. © 2021 Elsevier Inc.","Blinding; Experimenter bias; Researcher bias; Survey"
"A bibliometric assessment of software engineering themes, scholars and institutions (2013–2020)","2021","Journal of Systems and Software","10.1016/j.jss.2021.111029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108252663&doi=10.1016%2fj.jss.2021.111029&partnerID=40&md5=dc944031cbeff447a1d32a1944d7584e","This paper is the updated version (2013–2020) of the series of papers on emerging themes, top scholars and institutes on software engineering (SE), published by the Journal of Systems and Software for almost 25 years. The paper reports the findings of a bibliometric study by applying the systematic mapping technique on top-quality software engineering venues (handling a dataset of 11.668 studies). The design of the study remains the same for the complete decade, so that the results are consistent and comparable: As the ranking metric for institutions, we used the count of papers in which authors affiliated with the corresponding institute have been identified in the obtained dataset. Regarding scholars we computed the corresponding rankings based on the number of published papers and the average number of citations. In this version, the analysis of emerging trends and themes has been promoted compared to the previous years to provide more insights on what a newcomer in the software engineering domain should look at, as well as to recap the state-of-research in terms of themes to more experienced SE researchers. © 2021","Bibliometrics; Software engineering research areas; Top-institutions; Top-scholars"
"MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation","2021","Journal of Systems and Software","10.1016/j.jss.2021.111041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111574134&doi=10.1016%2fj.jss.2021.111041&partnerID=40&md5=8f45c141b12cb90b5a0dfa456e256d3e","Software testing depends on effective oracles. Implicit oracles, such as checks for program crashes, are widely applicable but narrow in scope. Oracles based on formal specifications can reveal application-specific failures, but specifications are expensive to obtain and maintain. Metamorphic oracles are somewhere in-between. They test equivalence among different procedures to detect semantic failures. Until now, the identification of metamorphic relations has been a manual and expensive process, except for few specific domains where automation is possible. We present MeMo, a technique and a tool to automatically derive metamorphic equivalence relations from natural language documentation, and we use such metamorphic relations as oracles in automatically generated test cases. Our experimental evaluation demonstrates that 1) MeMo can effectively and precisely infer equivalence metamorphic relations, 2) MeMo complements existing state-of-the-art techniques that are based on dynamic program analysis, and 3) metamorphic relations discovered with MeMo effectively detect defects when used as test oracles in automatically-generated or manually-written test cases. © 2021 The Author(s)","Natural language processing; Software testing; Test oracle generation"
"A scheduling-driven approach to efficiently assign bug fixing tasks to developers","2021","Journal of Systems and Software","10.1016/j.jss.2021.110967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104603597&doi=10.1016%2fj.jss.2021.110967&partnerID=40&md5=e9693858dea5c694ab43fb44e541a36a","The efficient assignment of bug fixing tasks to software developers is of major importance in software maintenance and evolution. When those tasks are not efficiently assigned to developers, the software project might confront extra costs and delays. In this paper, we propose a strategy that minimizes the time and the cost in bug fixing by finding the best feasible developer arrangement to handle bug fixing requests. We enhance therefore a state-of-the-art solution that uses an evolutionary bi-objective algorithm by involving a scheduling-driven approach that explores more parts of the search space. Scheduling is the process of evaluating all possible orders that developers can follow to fix the bugs they have been assigned. Through an empirical study we analyze the performance of the scheduling-driven approach and compare it to state of the art solutions. A non-parametric statistical test with four quality indicator metrics is used to assure its superiority. The experiments using two case-studies (JDT and Platform) showed that the scheduling-driven approach is superior to the state of the art approach in 71% and 74% of cases, respectively. Thus, our approach offers superior performance by assigning more conveniently bug fixing tasks to developers, while still avoiding to overload developers. © 2021 Elsevier Inc.","Bug assignment; Bug fixing; Scheduling-driven; Software maintenance"
"Mob programming: From avant-garde experimentation to established practice","2021","Journal of Systems and Software","10.1016/j.jss.2021.111017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108691765&doi=10.1016%2fj.jss.2021.111017&partnerID=40&md5=e9f1a1f1d5cd9a7bca14a2d3a6696779","Mob programming is an emergent practice in the industry, attracting increasing attention from the research community. Superficially a simple concept – similar to pair programming, but with more people – many of the same concerns arise as with pair programming, only more so: can it truly be efficient, is it for everybody, and what are the benefits and risks involved? In this paper we show that while research interest in this topic is increasing, much of the published literature to date constitutes experience reports, with a number of implicit differences in how mob programming is practiced. To transform mob programming from an area of avant-garde experimentation into a well understood practice in the toolbox of mainstream software engineering, such implicit differences need to be exposed, analyzed and documented, and the contextual enabling factors must be investigated. In this paper we take an important step in that direction by identifying variations in the practice and providing concrete guidance for creating an organizational and social environment where mob programming can be practiced both effectively and safely. © 2021 Elsevier Inc.","Agile; Exploratory testing; Mob programming; Mobbing; Pair programming; Research roadmap"
"A comparative study of test code clones and production code clones","2021","Journal of Systems and Software","10.1016/j.jss.2021.110940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102629039&doi=10.1016%2fj.jss.2021.110940&partnerID=40&md5=7ed9f7eec7e240a1127b2fa5749af772","Clones are one of the most widespread code smells, known to negatively affect the evolution of software systems. While there is a large body of research on detecting, managing, and refactoring clones in production code, clones in test code are often neglected in today's literature. In this paper we provide empirical evidence that further research on clones in test code is warranted. By analysing the clones in five representative open-source systems and comparing production code clones to test code clones, we observe that test code contains twice as many clones as production code. A detailed analysis reveals that most test clones are of Type II and Type III, and that many tests are duplicated multiple times with slight modifications. Moreover, current clone detection tools suffer from false negatives, and that this occurs more frequently in test code than in production code (NiCad = 76%, CPD = 90%, iClones = 12%). So even from a tools perspective, specific fine-tuning for test code is needed. © 2021 Elsevier Inc.","Clone detection; Software clones; Unit-tests"
"To automatically map source code entities to architectural modules with Naive Bayes","2022","Journal of Systems and Software","10.1016/j.jss.2021.111095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117148740&doi=10.1016%2fj.jss.2021.111095&partnerID=40&md5=4dbdac49497c32d13e274103a9bdf815","Background: The process of mapping a source code entity onto an architectural module is to a large degree a manual task. Automating this process could increase the use of static architecture conformance checking methods, such as reflexion modeling, in industry. Current techniques rely on user parameterization and a highly cohesive design. A machine learning approach would potentially require less parameters and better use of the available information to aid in automatic mapping. Aim: We investigate how a classifier can be trained to map from source code to architecture modules automatically. This classifier is trained with semantic and syntactic dependency information extracted from the source code and from architecture descriptions. The classifier is implemented using multinomial naive Bayes and evaluated. Method: We perform experiments and compare the classifier with three state-of-the-art mapping functions in eight open-source Java systems with known ground-truth-mappings. Results: We find that the classifier outperforms the state-of-the-art in all cases and that it provides a useful baseline for further research in the area of semi-automatic incremental clustering. Conclusions: We conclude that machine learning is a useful approach that performs better and with less need for parameterization compared to other approaches. Future work includes investigating problematic mappings and a more diverse set of subject systems. © 2021 The Author(s)","Incremental clustering; Naive Bayes; Orphan adoption; Software architecture"
"Architecture information communication in two OSS projects: The why, who, when, and what","2021","Journal of Systems and Software","10.1016/j.jss.2021.111035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110785141&doi=10.1016%2fj.jss.2021.111035&partnerID=40&md5=0668a812109bba5a04c5ea58e549898e","Architecture information is vital for Open Source Software (OSS) development, and mailing list is one of the widely used channels for developers to share and communicate architecture information. This work investigates the nature of architecture information communication (i.e., why, who, when, and what) by OSS developers via developer mailing lists. We employed a multiple case study approach to extract and analyze the architecture information communication from the developer mailing lists of two OSS projects, ArgoUML and Hibernate, during their development life-cycle of over 18 years. Our main findings are: (a) architecture negotiation and interpretation are the two main reasons (i.e., why) of architecture communication; (b) the amount of architecture information communicated in developer mailing lists decreases after the first stable release (i.e., when); (c) architecture communications centered around a few core developers (i.e., who); (d) and the most frequently communicated architecture elements (i.e., what) are Architecture Rationale and Architecture Model. There are a few similarities of architecture communication between the two OSS projects. Such similarities point to how OSS developers naturally gravitate towards the four aspects of architecture communication in OSS development. © 2021 Elsevier Inc.","Communication; Mailing list; Open Source Software; Software architecture"
"GloBug: Using global data in Fault Localization","2021","Journal of Systems and Software","10.1016/j.jss.2021.110961","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103941334&doi=10.1016%2fj.jss.2021.110961&partnerID=40&md5=26f37892938906153b3c76c8d9914907","Fault Localization (FL) is an important first step in software debugging and is mostly manual in the current practice. Many methods have been proposed over years to automate the FL process, including information retrieval (IR)-based techniques. These methods localize the fault based on the similarity of the reported bug report and the source code. Newer variations of IR-based FL (IRFL) techniques also look into the history of bug reports and leverage them during the localization. However, all existing IRFL techniques limit themselves to the current project's data (local data). In this study, we introduce Globug, which is an IRFL framework consisting of methods that use models pre-trained on the global data (extracted from open-source benchmark projects). In Globug, we investigate two heuristics: (a) the effect of global data on a state-of-the-art IR-FL technique, namely BugLocator, and (b) the application of a Word Embedding technique (Doc2Vec) together with global data. Our large scale experiment on 51 software projects shows that using global data improves BugLocator on average 6.6% and 4.8% in terms of MRR (Mean Reciprocal Rank) and MAP (Mean Average Precision), with over 14% in a majority (64% and 54% in terms of MRR and MAP, respectively) of the cases. This amount of improvement is significant compared to the improvement rates that five other state-of-the-art IRFL tools provide over BugLocator. In addition, training the models globally is a one-time offline task with no overhead on BugLocator's run-time fault localization. Our study, however, shows that a Word Embedding-based global solution did not further improve the results. © 2021 Elsevier Inc.","Automated Fault Localization; Doc2Vec; Global training; Information Retrieval; TF.IDF; Word Embedding"
"A formal specification animation method for operation validation","2021","Journal of Systems and Software","10.1016/j.jss.2021.110948","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104135609&doi=10.1016%2fj.jss.2021.110948&partnerID=40&md5=8174d694284ab2d889c3ec0a3854c73d","Formal specification can benefit software quality by precisely defining the behaviors of operations to prevent primary mistakes in the early phase of software projects, but a remaining challenge is how such a specification can be checked comprehensibly to show whether it satisfies the user's perception of requirements. In this paper, we describe a new technique for animating operation specifications as a means to address this problem. The technique offers new ways to do (1) automatic animation data generation for both input and output of an operation based on pre- and post-conditions, (2) visualized demonstration of the relationships between input and the corresponding output, (3) comprehensible animation of data items, and (4) illustrative animation of logical expressions and the operators used in them. We discuss these issues and present a prototype tool that supports the automation of the proposed technique. We also report an industrial application as a trial experiment to validate the technique. Finally, we conclude the paper and point out future research directions. © 2021 Elsevier Inc.","Formal specification; Specification animation; Verification and validation"
"Towards effective metamorphic testing by algorithm stability for linear classification programs","2021","Journal of Systems and Software","10.1016/j.jss.2021.111012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107698735&doi=10.1016%2fj.jss.2021.111012&partnerID=40&md5=d426f56c00cd19501049d01e7bf145bc","The quality assurance for machine learning systems is becoming increasingly critical nowadays. While many efforts have been paid on trained models from such systems, we focus on the quality of these systems themselves, as the latter essentially decides the quality of numerous models thus trained. In this article, we focus particularly on detecting bugs in implementing one class of model-training systems, namely, linear classification algorithms, which are known to be challenging due to the lack of test oracle. Existing work has attempted to use metamorphic testing to alleviate the oracle problem, but fallen short on overlooking the statistical nature of such learning algorithms, leading to premature metamorphic relations (MRs) suffering efficacy and necessity issues. To address this problem, we first derive MRs from a fundamental property of linear classification algorithms, i.e., algorithm stability, with the soundness guarantee. We then formulate such MRs in a way that is rare in usage but could be more effective according to our field study and analysis, i.e., Past-execution Dependent MR (PD-MR), as contrast to the traditional way, i.e., Past-execution Independent MR (PI-MR), which has been extensively studied. We experimentally evaluated our new MRs upon nine well-known linear classification algorithms. The results reported that the new MRs detected 37.6–329.2% more bugs than existing benchmark MRs. © 2021 Elsevier Inc.","Algorithm stability; Linear classifier; Machine learning program; Metamorphic relation; Metamorphic testing"
"Data correction and evolution analysis of the ProgrammableWeb service ecosystem","2021","Journal of Systems and Software","10.1016/j.jss.2021.111066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114840051&doi=10.1016%2fj.jss.2021.111066&partnerID=40&md5=acac5f4b3237728adbb2c1a4e8497408","The evolution analysis on Web service ecosystems has become a critical problem as the frequency of service changes on the Internet increases rapidly. Developers need to understand these evolution patterns to assist in their decision-making on service selection. ProgrammableWeb is a popular Web service ecosystem on which several evolution analyses have been conducted in the literature. However, the existing studies have ignored the quality issues of the ProgrammableWeb dataset and the issue of service obsolescence. In this study, we first report the quality issues identified in the ProgrammableWeb dataset from our empirical study. Then, we propose a novel method to correct the relevant evolution analysis data by estimating the life cycle of application programming interfaces (APIs) and mashups. We also reveal how to use three different dynamic network models in the service ecosystem evolution analysis based on the corrected ProgrammableWeb dataset. Our experimental experience iterates the quality issues of the original ProgrammableWeb and highlights several research opportunities. © 2021 Elsevier Inc.","APIs; Dynamic network model; Evolution analysis; Mashups; ProgrammableWeb; Service ecosystem"
"ECCOLA — A method for implementing ethically aligned AI systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.111067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114927641&doi=10.1016%2fj.jss.2021.111067&partnerID=40&md5=8014b03c0eb219ac337236bca7a16026","Artificial Intelligence (AI) systems are becoming increasingly widespread and exert a growing influence on society at large. The growing impact of these systems has also highlighted potential issues that may arise from their utilization, such as data privacy issues, resulting in calls for ethical AI systems. Yet, how to develop ethical AI systems remains an important question in the area. How should the principles and values be converted into requirements for these systems, and what should developers and the organizations developing these systems do? To further bridge this gap in the area, in this paper, we present a method for implementing AI ethics: ECCOLA. Following a cyclical action research approach, ECCOLA has been iteratively developed over the course of multiple years, in collaboration with both researchers and practitioners. © 2021 The Author(s)","AI ethics; Artificial intelligence; Ethics; Implementing; Method"
"Improved retrieval of programming solutions with code examples using a multi-featured score","2021","Journal of Systems and Software","10.1016/j.jss.2021.111063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113273878&doi=10.1016%2fj.jss.2021.111063&partnerID=40&md5=e311412560b04b05114b1c40e104f9b5","Developers often depend on code search engines to obtain solutions for their programming tasks. However, finding an expected solution containing code examples along with their explanations is challenging due to several issues. There is a vocabulary mismatch between the search keywords (the query) and the appropriate solutions. Semantic gap may increase for similar bag of words due to antonyms and negation. Moreover, documents retrieved by search engines might not contain solutions containing both code examples and their explanations. So, we propose CRAR (Crowd Answer Recommender) to circumvent those issues aiming at improving retrieval of relevant answers from Stack Overflow containing not only the expected code examples for the given task but also their explanations. Given a programming task, we investigate the effectiveness of combining information retrieval techniques along with a set of features to enhance the ranking of important threads (i.e., the units containing questions along with their answers) for the given task and then selects relevant answers contained in those threads, including semantic features, like word embeddings and sentence embeddings, for instance, a Convolutional Neural Network (CNN). CRAR also leverages social aspects of Stack Overflow discussions like popularity to select relevant answers for the tasks. Our experimental evaluation shows that the combination of the different features performs better than each one individually. We also compare the retrieval performance with the state-of-art CROKAGE (Crowd Knowledge Answer Generator), which is also a system aimed at retrieving relevant answers from Stack Overflow. We show that CRAR outperforms CROKAGE in Mean Reciprocal Rank and Mean Recall with small and medium effect sizes, respectively. © 2021 Elsevier Inc.","Mining crowd knowledge; Stack overflow; Word embedding"
"Exploring the use of static and dynamic analysis to improve the performance of the mining sandbox approach for android malware identification","2022","Journal of Systems and Software","10.1016/j.jss.2021.111092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116719350&doi=10.1016%2fj.jss.2021.111092&partnerID=40&md5=cc1f8436cab2ef6eab3268bfe90a78cf","The popularization of the Android platform and the growing number of Android applications (apps) that manage sensitive data turned the Android ecosystem into an attractive target for malicious software. For this reason, researchers and practitioners have investigated new approaches to address Android's security issues, including techniques that leverage dynamic analysis to mine Android sandboxes. The mining sandbox approach consists in running dynamic analysis tools on a benign version of an Android app. This exploratory phase records all calls to sensitive APIs. Later, we can use this information to (a) prevent calls to other sensitive APIs (those not recorded in the exploratory phase) or (b) run the dynamic analysis tools again in a different version of the app. During this second execution of the fuzzing tools, a warning of possible malicious behavior is raised whenever the new version of the app calls a sensitive API not recorded in the exploratory phase. The use of a mining sandbox approach is an effective technique for Android malware analysis, as previous research works revealed. Particularly, existing reports present an accuracy of almost 70% in the identification of malicious behavior using dynamic analysis tools to mine android sandboxes. However, although the use of dynamic analysis for mining Android sandboxes has been investigated before, little is known about the potential benefits of combining static analysis with a mining sandbox approach for identifying malicious behavior. Accordingly, in this paper we present the results of two studies that investigate the impact of using static analysis to complement the performance of existing dynamic analysis tools tailored for mining Android sandboxes, in the task of identifying malicious behavior. In the first study we conduct a non-exact replication of a previous study (hereafter BLL-Study) that compares the performance of test case generation tools for mining Android sandboxes. Differently from the original work, here we isolate the effect of an independent static analysis component (DroidFax) they used to instrument the Android apps in their experiments. This decision was motivated by the fact that DroidFax could have influenced the efficacy of the dynamic analyses tools positively—through the execution of specific static analysis algorithms DroidFax also implements. In our second study, we carried out a new experiment to investigate the efficacy of taint analysis algorithms to complement the mining sandbox approach previously used to identify malicious behavior. To this end, we executed the FlowDroid tool to mine the source–sink flows from benign/malign pairs of Android apps used in a previous research work. Our study brings several findings. For instance, the first study reveals that DroidFax alone (static analysis) can detect 43.75% of the malwares in the BLL-Study dataset, contributing substantially in the performance of the dynamic analysis tools in the BLL-Study. The results of the second study show that taint analysis is also practical to complement the mining sandboxes approach, with a performance similar to that reached by dynamic analysis tools. © 2021 Elsevier Inc.","Android platform; Malware detection; Mining sandboxes; Static analysis and dynamic analysis"
"A formal approach for the analysis of BPMN collaboration models","2021","Journal of Systems and Software","10.1016/j.jss.2021.111007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110229135&doi=10.1016%2fj.jss.2021.111007&partnerID=40&md5=39d085786cfed3398bc7066909e28b5b","BPMN collaboration models have acquired increasing relevance in software development since they shorten the communication gap between domain experts and IT specialists and permit clarifying the characteristics of software systems needed to provide automatic support for the activities of complex organizations. Nonetheless, the lack of effective formal verification capabilities can hinder the full adoption of the BPMN standard by IT specialists, as it prevents precisely check the satisfaction of behavioral properties, with negative impacts on the quality of the software. To address these issues, this paper proposes BProVe, a novel verification approach for BPMN collaborations. This combines both standard model checking techniques, through the MAUDE's LTL model checker, and statistical model checking techniques, through the statistical analyzer MULTIVESTA. The latter makes BProVe effective also on those scenarios suffering from the state–space explosion problem, made even more acute by the presence of asynchronous message exchanges. To support the adoption of the BProVe approach, we propose a complete web-based tool-chain that allows for BPMN modeling, verification, and result exploration. The feasibility of BProVe has been validated both on synthetically-generated models and on models retrieved from two public repositories. The performed validation highlighted the importance and complementarity of the two supported verification strategies. © 2021 Elsevier Inc.","BPMN; Collaboration; Model checking; Statistical model checking; Verification"
"Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network","2021","Journal of Systems and Software","10.1016/j.jss.2021.111026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108275396&doi=10.1016%2fj.jss.2021.111026&partnerID=40&md5=dc85a8949d2f1e47bc510ac279fc9284","Software defect prediction aims to identify the potential defects of new software modules in advance by constructing an effective prediction model. However, the model performance is susceptible to irrelevant and redundant features. In addition, previous studies mainly use traditional data mining or machine learning techniques for defect prediction, the prediction performance is not superior enough. For the first issue, motivated by the idea of search based software engineering, we leverage the recently proposed whale optimization algorithm (WOA) and another complementary simulated annealing (SA) to construct an enhanced metaheuristic search based feature selection algorithm named EMWS, which can effectively select fewer but closely related representative features. For the second issue, we employ a hybrid deep neural network — convolutional neural network (CNN) and kernel extreme learning machine (KELM) to construct a unified defect prediction predictor called WSHCKE, which can further integrate the selected features into the abstract deep semantic features by CNN and boost the prediction performance by taking full advantage of the strong classification capacity of KELM. We conduct extensive experiments for feature selection or extraction and defect prediction across 20 widely-studied software projects on four evaluation indicators. Experimental results demonstrate the superiority of EMWS and WSHCKE. © 2021","Convolutional neural network; Kernel extreme learning machine; Metaheuristic feature selection; Software defect prediction; Whale optimization algorithm"
"Custom-tailored clone detection for IEC 61131-3 programming languages","2021","Journal of Systems and Software","10.1016/j.jss.2021.111070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114824678&doi=10.1016%2fj.jss.2021.111070&partnerID=40&md5=0e0180186d6e90e946b4c4b7d910d42a","Automated production systems (aPS) are highly customized systems that consist of hardware and software. Such aPS are controlled by a programmable logic controller (PLC), often in accordance with the IEC 61131-3 standard that divides system implementation into so-called program organization units (POUs) as the smallest software unit and is comprised of multiple textual (Structured Text (ST)) and graphical (Function Block Diagram (FBD), Ladder Diagram (LD), and Sequential Function Chart(SFC)) programming languages that can be arbitrarily nested. A common practice during the development of such systems is reusing implementation artifacts by copying, pasting, and then modifying code. This approach is referred to as code cloning. It is used on a fine-granular level where a POU is cloned within a system variant. It is also applied on the coarse-granular system level, where the entire system is cloned and adapted to create a system variant, for example for another customer. This ad hoc practice for the development of variants is commonly referred to as clone-and-own. It allows the fast development of variants to meet varying customer requirements or altered regulatory guidelines. However, clone-and-own is a non-sustainable approach and does not scale with an increasing number of variants. It has a detrimental effect on the overall quality of a software system, such as the propagation of bugs to other variants, which harms maintenance. In order to support the effective development and maintenance of such systems, a detailed code clone analysis is required. On the one hand, an analysis of code clones within a variant (i.e., clone detection in the classical sense) supports experts in refactoring respective code into library components. On the other hand, an analysis of commonalities and differences between cloned variants (i.e., variability analysis) supports the maintenance and further reuse and facilitates the migration of variants into a software productline (SPL). In this paper, we present an approach for the automated detection of code clones within variants (intra variant clone detection) and between variants (inter variant clone detection) of IEC61131-3 control software with arbitrary nesting of both textual and graphical languages. We provide an implementation of the approach in the variability analysis toolkit (VAT) as a freely available prototype for the analysis of IEC 61131-3 programs. For the evaluation, we developed a meta-model-based mutation framework to measure our approach's precision and recall. Besides, we evaluated our approach using the Pick and Place Unit (PPU) and Extended Pick and Place Unit (xPPU) scenarios. Results show the usefulness of intra and inter clone detection in the domain of automated production systems. © 2021 Elsevier Inc.","Clone detection; IEC 61131-3; Reverse engineering; Variability mining"
"How to identify class comment types? A multi-language approach for class comment classification","2021","Journal of Systems and Software","10.1016/j.jss.2021.111047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111034999&doi=10.1016%2fj.jss.2021.111047&partnerID=40&md5=a3a337648ec68671db6599a156ef981b","Most software maintenance and evolution tasks require developers to understand the source code of their software systems. Software developers usually inspect class comments to gain knowledge about program behavior, regardless of the programming language they are using. Unfortunately, (i) different programming languages present language-specific code commenting notations and guidelines; and (ii) the source code of software projects often lacks comments that adequately describe the class behavior, which complicates program comprehension and evolution activities. To handle these challenges, this paper investigates the different language-specific class commenting practices of three programming languages: Python, Java, and Smalltalk. In particular, we systematically analyze the similarities and differences of the information types found in class comments of projects developed in these languages. We propose an approach that leverages two techniques –namely Natural Language Processing and Text Analysis –to automatically identify class comment types, i.e., the specific types of semantic information found in class comments. To the best of our knowledge, no previous work has provided a comprehensive taxonomy of class comment types for these three programming languages with the help of a common automated approach. Our results confirm that our approach can classify frequent class comment information types with high accuracy for the Python, Java, and Smalltalk programming languages. We believe this work can help in monitoring and assessing the quality and evolution of code comments in different programming languages, and thus support maintenance and evolution tasks. © 2021","Code comment analysis; Natural language processing technique; Software documentation"
"Agile elicitation of scalability requirements for open systems: A case study","2021","Journal of Systems and Software","10.1016/j.jss.2021.111064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115009872&doi=10.1016%2fj.jss.2021.111064&partnerID=40&md5=13669f6e8bb898dea40f23303b6c4bd0","Eliciting scalability requirements during agile software development is complicated and poorly described in previous research. This article presents a lightweight artifact for eliciting scalability requirements during agile software development: the ScrumScale model. The ScrumScale model is a simple spreadsheet. The scalability concepts underlying the ScrumScale model are clarified in this design science research, which also utilizes coordination theory. This paper describes the open banking case study, in which a legacy banking system becomes open. This challenges the scalability of this legacy system. The first step in understanding this challenge is to elicit the new scalability requirements. In the open banking case study, key stakeholders from TietoEVRY spent 55 h eliciting the scalability requirements of TietoEVRY's open banking project. According to TietoEVRY, the ScrumScale model provided a systematic way of producing scalability requirements. For TietoEVRY, the scalability concepts behind the ScrumScale model also offered significant advantages in dialogs with other stakeholders. © 2021 The Authors","Agile software development; Design science; Non-functional requirements; Open systems; Performance requirements; Quality requirements; Requirements engineering; Software performance engineering; Workload charaterization"
"Why do builds fail?—A conceptual replication study","2021","Journal of Systems and Software","10.1016/j.jss.2021.110939","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103128877&doi=10.1016%2fj.jss.2021.110939&partnerID=40&md5=6ed68dadf7b14600f001be29c403e444","Previous studies have investigated a wide range of factors potentially explaining software build breakages, focusing primarily on build-triggering code changes or previous CI outcomes. However, code quality factors such as the presence of code/test smells have not been yet evaluated in the context of CI, even though such factors have been linked to problems of comprehension and technical debt, and hence might introduce bugs and build breakages. This paper performs a conceptual replication study on 27,675 Travis CI builds of 15 GitHub projects, considering the features reported by Rausch et al. and Zolfagharinia et al., as well as those related to code/test smells. Using a multivariate model constructed from nine dimensions of features, results indicate a precision (recall) ranging between 58.3% and 79.0% (52.4% and 69.6%) in balanced project datasets, and between 2.5% and 37.5% (2.5% and 12.4%) in imbalanced project datasets. Models trained on our balanced project datasets were later used to perform cross-project prediction on the imbalanced projects, achieving an average improvement of 9.3% (16.2%) in precision (recall). Statistically, the results confirm that features from the build history, author, code complexity, and code/test smell dimensions are the most important predictors of build failures. © 2021 Elsevier Inc.","Build failure; Code smells; Continuous integration; Cross-project prediction; Quantitative analysis; Test smells"
"Testing multiple linear regression systems with metamorphic testing","2021","Journal of Systems and Software","10.1016/j.jss.2021.111062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114122029&doi=10.1016%2fj.jss.2021.111062&partnerID=40&md5=00601662511dbb56b6f37ad87f447c1f","Regression is one of the most commonly used statistical techniques. However, testing regression systems is a great challenge because of the absence of test oracle in general. In this paper, we show that Metamorphic Testing is an effective approach to test multiple linear regression systems. In doing so, we identify intrinsic mathematical properties of linear regression, and then propose 11 Metamorphic Relations to be used for testing. Their effectiveness is examined using mutation analysis with a range of different regression programs. We further look at how the testing could be adopted in a more effective way. Our work is applicable to examine the reliability of predictive systems based on regression that has been widely used in economics, engineering and science, as well as of the regression calculation manipulated by statistical users. © 2021 Elsevier Inc.","Metamorphic relation; Metamorphic testing; Multiple linear regression"
"Tuning configuration of apache spark on public clouds by combining multi-objective optimization and performance prediction model","2021","Journal of Systems and Software","10.1016/j.jss.2021.111028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109572256&doi=10.1016%2fj.jss.2021.111028&partnerID=40&md5=65c0189e7f13ac00f957cb0f351cdda5","Choosing the right configuration for Spark deployed in the public cloud to ensure the efficient running of periodic jobs is hard, because there can be a huge configuration space to explore which is composed of numerous performance-related parameters in different dimensions (e.g., application-level and cloud-level). Choosing poorly will not only significantly degrade performance but may also lead to greater overhead. However, automatically searching for the optimal configuration of various applications to trade-off performance and cost is challenging. To address this issue, we propose a new optimal configuration search algorithm named AB-MOEA/D by combining multi-objective optimization algorithm and performance prediction model. AB-MOEA/D uses a decomposition-based multi-objective optimization algorithm to find the configuration with the objective of minimizing the execution time and cost, where the performance model constructed on the Adaboost algorithm is used to evaluate the fitness of each candidate configuration. Besides, we also present the configuration automatic tuning system with AB-MOEA/D as the optimization engine. The experimental results on six benchmarks with five data sets show that AB-MOEA/D significantly outperforms the previous work in terms of execution time and cost, with average improvements of approximately 35 and 40 percent. © 2021","Configuration tuning; Multi-objective optimization; Spark"
"Multi-factory production planning using edge computing and IIoT platforms","2021","Journal of Systems and Software","10.1016/j.jss.2021.111083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115429169&doi=10.1016%2fj.jss.2021.111083&partnerID=40&md5=41bd515e469b1410dea01331fec2171b","An important prerequisite for determining whether a certain product is producible in any given production facility is an accurate assessment of which production lines and/or the machines are able to execute the necessary production steps. Not only the static information about the capabilities of the machines, but also the conditions of machines and tools are significant for this analysis. Because of the deviation of machine capabilities with increasing deterioration and weary of the equipment, it is also necessary to continuously monitor the status of the machine and analyze the machine conditions. In this paper, we present an approach for generating production plans across multiple factories, considering both static information and dynamic data analysis. Edge devices constantly monitor high frequency machine data and report condensed machine states to an Industrial IoT platform (IIoT). A marketplace within the cloud-application MindSphere enables us to integrate the requirements of the products and the capabilities of the production sites. Customers are be able to evaluate these production plans based on duration, energy consumption, CO2 footprint etc. © 2021 The Authors","Cloud manufacturing; Edge analytics; Factory as a service; Industrie 4.0; Manufacturing ecosystems; Monitoring production"
"A study on correlations between architectural smells and design patterns","2021","Journal of Systems and Software","10.1016/j.jss.2021.110984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104642390&doi=10.1016%2fj.jss.2021.110984&partnerID=40&md5=cc348de9b3e37b1fb41aa770e592db5f","Design patterns are recommended solutions for typical software design problems, with an extensively studied and documented impact on various quality factors. Flaws in design at a higher levels of abstraction are manifested in architectural smells. Some of those smells, similarly to code smells, can reduce the expected advantages of design patterns or even prevent their proper implementation. In this paper we study if and how design patterns and architectural smells are related, and how this knowledge could be exploited in practice. We present an empirical study with an analysis of 16 design patterns and 3 architectural smells in 60 open source Java systems. We analyze their diffuseness and correlation, and we extract association rules that describe their presence and dependencies. We demonstrate that there exist relationships between architectural smells and design patterns, both at the class and package levels. Some smells appear falsely positive, as they result from conscious decisions made by programmers, while the application of some patterns can be a cause of certain smells. Our results provide evidence that design patterns and architectural smells are related and affect each other. With knowledge about the relationships, programmers can avoid the side effects of applying some design patterns. © 2021 Elsevier Inc.","Architectural smells; Architectural smells and design patterns relationships; Design patterns"
"Digging into group establishment: Intervention design and evaluation","2021","Journal of Systems and Software","10.1016/j.jss.2021.110974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105693246&doi=10.1016%2fj.jss.2021.110974&partnerID=40&md5=2992b10c7629112c2537151c31e71678","Previous research has documented challenges in students’ group work. An identifiable segment of the previous research that relates to improving students’ group work conditions is the study of group formation and self- and peer-assessment. Though studies that primarily focus on how to address the conditions of students’ group work and the existing problems can be found, there are not many related to higher education settings. On this ground, the present article advances a qualitative evaluation of the intervention that promotes student groups’ self-awareness and thereby self-regulation toward fair group work during a software engineering project. An inductive thematic analysis was applied to the students’ written reflections on the intervention. To further understand the results, the concept of “group establishment,” referring to destructiveness that complicates individuals’ truthful living at the group level, was employed to reflect on the resulting themes. Hoggett (1998) provided this articulation by synthesizing previous results in psychoanalytic theory. Students’ experiences with the intervention revealed several value gains, including personally identified benefits as well as open group mood, consolidation of grouping, conceptual learning about group work, and regulation for task allocation. Noted challenges included dishonesty and a personal role conflict, and some students reported minor effects on group performance. Students valued safety in the intervention situation and argued that the intervention was needed from outside the group. A summative review of the students’ experiences suggests that the intervention was useful for all groups. The results are discussed from a pedagogic and the aforementioned psychoanalytic perspective, and remarks are made for software engineering education. © 2021","Group establishment; Group work; Intervention; Justice; Software engineering education"
"A systematic literature review and taxonomy of modern code review","2021","Journal of Systems and Software","10.1016/j.jss.2021.110951","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103984601&doi=10.1016%2fj.jss.2021.110951&partnerID=40&md5=c5bcefa45661a1948974711596c4b7c9","Context: Modern Code Review (MCR) is a widely known practice of software quality assurance. However, the existing body of knowledge of MCR is currently not understood as a whole. Objective: Our goal is to identify the state of the art on MCR, providing a structured overview and an in-depth analysis of the research done in this field. Methods: We performed a systematic literature review, selecting publications from four digital libraries. Results: A total of 139 papers were selected and analyzed in three main categories. FOUNDATIONAL STUDIES are those that analyze existing or collected data from the adoption of MCR. PROPOSALS consist of techniques and tools to support MCR, while EVALUATIONS are studies to assess an approach or compare a set of them. Conclusion: The most represented category is FOUNDATIONAL STUDIES, mainly aiming to understand the motivations for adopting MCR, its challenges and benefits, and which influence factors lead to which MCR outcomes. The most common types of PROPOSALS are code reviewer recommender and support to code checking. EVALUATIONS of MCR-supporting approaches have been done mostly offline, without involving human subjects. Five main research gaps have been identified, which point out directions for future work in the area. © 2021 Elsevier Inc.","Modern code review; Software quality; Software verification; Systematic literature review"
"System quality and security certification in seven weeks: A multi-case study in Spanish SMEs","2021","Journal of Systems and Software","10.1016/j.jss.2021.110960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105284323&doi=10.1016%2fj.jss.2021.110960&partnerID=40&md5=a88e916b8affb240e841e5e2f43d056b","Every company wishes to improve its system quality and security, all the more so in these times of digital transformation, since having a good quality and security management system is essential to any company's commercial survival. Such needs are even more pressing for small and medium-sized enterprises (SMEs), given their limited time and resources. To address these needs, a Spanish company, Proceso Social, has developed an innovative method called “SevenWeeks” to allow SMEs to create or improve their quality and security management systems in just seven weeks, with a view to obtaining one or both of the ISO 9001 and ISO/IEC 27001 certifications. We have evaluated the effectiveness and usefulness of SevenWeeks by carrying out a multi-case study of 26 Spanish companies, based on independent sources of evidence. This allowed us to corroborate that SevenWeeks was indeed effective for and perceived as useful by all the companies, as it enabled them to create their own quality and security management systems in only seven weeks and to obtain the necessary ISO certification. The interviewees found SevenWeeks to be an agile and intuitive method, easy to implement, which reduces costs and effort. We also include some recommendations to improve and further develop the method. © 2021 Elsevier Inc.","ISO 9001; ISO/IEC 27001; Multi-case study; Quality management system; Security management system; SME"
"SWFC-ART: A cost-effective approach for Fixed-Size-Candidate-Set Adaptive Random Testing through small world graphs","2021","Journal of Systems and Software","10.1016/j.jss.2021.111008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107686019&doi=10.1016%2fj.jss.2021.111008&partnerID=40&md5=70b2aab1965439a34dae2e10a6eb0100","Adaptive random testing (ART) improves the failure-detection effectiveness of random testing by leveraging properties of the clustering of failure-causing inputs of most faulty programs: ART uses a sampling mechanism that evenly spreads test cases within a software's input domain. The widely-used Fixed-Sized-Candidate-Set ART (FSCS-ART) sampling strategy faces a quadratic time cost, which worsens as the dimensionality of the software input domain increases. In this paper, we propose an approach based on small world graphs that can enhance the computational efficiency of FSCS-ART: SWFC-ART. To efficiently perform nearest neighbor queries for candidate test cases, SWFC-ART incrementally constructs a hierarchical navigable small world graph for previously executed, non-failure-causing test cases. Moreover, SWFC-ART has shown consistency in programs with high dimensional input domains. Our simulation and empirical studies show that SWFC-ART reduces the computational overhead of FSCS-ART from quadratic to log-linear order while maintaining the failure-detection effectiveness of FSCS-ART, and remaining consistent in high dimensional input domains. We recommend using SWFC-ART in practical software testing scenarios, where real-life programs often have high dimensional input domains and low failure rates. © 2021 Elsevier Inc.","Adaptive random testing; Efficiency; Hierarchical Navigable Small World Graphs; Random testing; Software testing"
"Model-based safety engineering for autonomous train map","2022","Journal of Systems and Software","10.1016/j.jss.2021.111082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116015867&doi=10.1016%2fj.jss.2021.111082&partnerID=40&md5=25ff66993237cedad672e5cfca35901a","As a part of the digital revolution of railway systems, an autonomous driving train will use a complete and precise map of railway infrastructure to conduct operational actions. Nevertheless, the full autonomy of trains depends on the safety decisions management capacity both on-board and track-side. These decisions must be refined into safety requirements in order to continuously check the consistency between the perceived infrastructure and safety related properties. However, traditionally, the integration of safety analysis requires the intervention of human agent skills. This may be error-prone and in interference with the embedded aspect of the train map. In this paper, we propose a model-based approach to match between safety concepts expressed as an ontology, a derived safety model and a safety-extended railway infrastructure map model for autonomous trains. This approach is validated by railway safety case studies for autonomous train map. The integration of this model-based safety solution from the early stages of the map system design improves the safety decisions management process. © 2021 Elsevier Inc.","Autonomous train; Model-based safety engineering; Model-driven engineering; Railway infrastructure model; Safety ontology; Safety/assurance case"
"Evaluating T-wise testing strategies in a community-wide dataset of configurable software systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.110990","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106269170&doi=10.1016%2fj.jss.2021.110990&partnerID=40&md5=b588a76ee06011838fe6c1ecc8d7d3c5","Configurable software systems allow developers to maintain a unique platform and address a diversity of deployment contexts and usages. Testing configurable systems is essential because configurations that fail may potentially hurt users and degrade the project reputation. As extensively testing all valid configurations is infeasible in practice, several testing strategies have been proposed to recommend an optimal sample of configurations able to find most existing faults. However, up to now, we could not find studies comparing testing strategies with a community-wide dataset. Aiming at (i) comparing sampling testing strategies and (ii) understanding the location of faults, we use a community-wide dataset from the literature and compare suggested configurations from variations of five t-wise testing strategies (e.g., ICPL-T2, Chvatal-T4, and IncLing-T2). This comparison aims to find which strategies are faster, more comprehensive, effective on identifying faults, time-efficient, and coverage-efficient in this community-wide dataset and the reasons why a strategy fared better in one investigated property. Complementary, we investigate the dispersion of faults over classes and features from the dataset. As a result, we found that the dispersion of faults are usually concentrated in a few classes and features. Furthermore, fault-prone classes and features are distinguishable from classes and features safe of faults. Overall, we believe that with our results practitioners acquire the necessary knowledge to choose a testing strategy that best fits their needs. Moreover, researchers and tool builders are served with a bunch of opportunities to improve existing testing strategies and tools. For instance, they may incorporate information from fault-prone classes and features when selecting configurations to be tested in their testing strategies. © 2021","Feature interactions; Software faults; T-wise sampling strategies; Testing configurable systems"
"A systematic study of reward for reinforcement learning based continuous integration testing","2020","Journal of Systems and Software","10.1016/j.jss.2020.110787","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089907018&doi=10.1016%2fj.jss.2020.110787&partnerID=40&md5=0c6a319a56d70dcdeabc0d40d6f6f597","Continuous integration(CI) testing is characterized by continually changing test cases, limited execution time, and fast feedback, where the classical test prioritization approaches are no longer suitable. Based on the essence of continuous decision mechanism, reinforcement learning(RL) is suggested for prioritizing test cases in CI testing, in which the reward plays a crucial role. In this paper, we conducted a systematic study of the reward function and reward strategy in CI testing. In terms of reward function, the whole historical execution information of test cases is used with the consideration of the failure times and failure distribution. Further considering the validity of historical information, partial historical information is used by proposing a time-window based approach. In terms of reward strategy which means how to reward, three strategies are introduced, i.e., total reward, partial reward, and fuzzy reward. The empirical study is conducted on four industrial-level programs, and the results reveal that using the reward function with historical information improves the Recall by on average 13.21% when compared with existing TF(Test Case Failure) reward function, and the fuzzy reward strategy is more flexible and improve the NAPFD(Normalized Average Percentage of Faults Detected) by on average 3.43% when compared with the other two strategies. © 2020","Continuous integration; Reinforcement learning; Reward policy; Test case prioritization"
"Uncertainty-aware specification and analysis for hardware-in-the-loop testing of cyber-physical systems","2021","Journal of Systems and Software","10.1016/j.jss.2020.110813","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090366447&doi=10.1016%2fj.jss.2020.110813&partnerID=40&md5=a0924ac877b44f81c22cfc12c14a48c9","Hardware-in-the-loop (HiL) testing is important for developing cyber-physical systems (CPS). HiL test cases manipulate hardware, are time-consuming and their behaviors are impacted by the uncertainties in the CPS environment. To mitigate the risks associated with HiL testing, engineers have to ensure that (1) test cases are well-behaved, e.g., they do not damage hardware, and (2) test cases can execute within a time budget. Leveraging the UML profile mechanism, we develop a domain-specific language, HITECS, for HiL test case specification. Using HITECS, we provide uncertainty-aware analysis methods to check the well-behavedness of HiL test cases. In addition, we provide a method to estimate the execution times of HiL test cases before the actual HiL testing. We apply HITECS to an industrial case study from the satellite domain. Our results show that: (1) HITECS helps engineers define more effective assertions to check HiL test cases, compared to the assertions defined without any systematic guidance; (2) HITECS verifies in practical time that HiL test cases are well-behaved; (3) HITECS is able to resolve uncertain parameters of HiL test cases by synthesizing conditions under which test cases are guaranteed to be well-behaved; and (4) HITECS accurately estimates HiL test case execution times. © 2020 The Authors","Cyber-physical systems; Machine learning; Model checking; Simulation; Test case specification and analysis; UML profile"
"Concepts of variation control systems","2021","Journal of Systems and Software","10.1016/j.jss.2020.110796","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090150360&doi=10.1016%2fj.jss.2020.110796&partnerID=40&md5=9d815d534fe05f7df5eb8c13c2b0018e","Version control systems are an integral part of today's software engineering. They facilitate the collaborative management of revisions (sequential versions) and variants (concurrent versions) of software systems under development. Typical version control systems maintain revisions of files and variants of whole software systems. Variants are supported via branching or forking mechanisms that conceptually clone whole systems in a coarse-grained way. Unfortunately, such cloning leads to high maintenance efforts. To avoid these disadvantages and support fine-grained variation, developers need to employ custom configuration mechanisms, which leads to a misappropriation of tools and undesired context switches. Addressing this trade-off, a number of variation control systems has been conceived, providing a richer set of capabilities for handling variants. Variation control systems decompose a software system into finer-grained variable entities and offer high-level metaphors to automatically manage this variability. In this paper, we classify and compare variation control systems and illustrate their core concepts and characteristics. All investigated variation control systems offer an iterative (checkout–modify–commit) workflow, but there are essential differences affecting developers. We highlight challenges and discuss research perspectives for developing the next generation of version and variation control systems. © 2020 Elsevier Inc.","Configuration management; Software product lines; Software repositories; Variability management; Version control"
"Fast and accurate incremental feedback for students’ software tests using selective mutation analysis","2021","Journal of Systems and Software","10.1016/j.jss.2021.110905","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100414509&doi=10.1016%2fj.jss.2021.110905&partnerID=40&md5=fbf232b172675a01e11fa5866f0df72d","As incorporating software testing into programming assignments becomes routine, educators have begun to assess not only the correctness of students’ software, but also the adequacy of their tests. In practice, educators rely on code coverage measures, though its shortcomings are widely known. Mutation analysis is a stronger measure of test adequacy, but it is too costly to be applied beyond the small programs developed in introductory programming courses. We demonstrate how to adapt mutation analysis to provide rapid automated feedback on software tests for complex projects in large programming courses. We study a dataset of 1389 student software projects ranging from trivial to complex. We begin by showing that although the state-of-the-art in mutation analysis is practical for providing rapid feedback on projects in introductory courses, it is prohibitively expensive for the more complex projects in subsequent courses. To reduce this cost, we use a statistical procedure to select a subset of mutation operators that maintains accuracy while minimizing cost. We show that with only 2 operators, costs can be reduced by a factor of 2–3 with negligible loss in accuracy. Finally, we evaluate our approach on open-source software and report that our findings may generalize beyond our educational context. © 2021 The Author(s)","Automated assessment tools; Mutation analysis; Software engineering education; Software testing"
"Secondary studies in the academic context: A systematic mapping and survey","2020","Journal of Systems and Software","10.1016/j.jss.2020.110734","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088403569&doi=10.1016%2fj.jss.2020.110734&partnerID=40&md5=02101d7f95c854d2e273d145575b93ec","Context: Several researchers have reported their experiences in applying secondary studies (Systematic Literature Reviews — SLRs and Systematic Mappings — SMs) in Software Engineering (SE). However, there is still a lack of studies discussing the value of performing secondary studies in an academic context. Goal: The main goal of this study is to provide an overview on the use of secondary studies in an academic context. Method: Two empirical research methods were used. Initially, we conducted a SM to identify the available and relevant studies on the use of secondary studies as a research methodology for conducting SE research projects. Secondly, a survey was performed with 64 SE researchers to identify their perception related to the value of performing secondary studies to support their research projects. Results: Our results show benefits of using secondary studies in the academic context, such as providing an overview of the literature as well as identifying relevant research literature on a research area enabling to find reasons to explain why a research project should be approved for a grant and/or supporting decisions made in a research project. Difficulties faced by SE graduate students with secondary studies are that they tend to be conducted by a team and it demands more effort than a traditional review. Conclusions: Secondary studies are valuable to graduate students. They should consider conducting a secondary study for their research project due to the benefits and contributions provided to develop the overall project. However, the advice of an experienced supervisor is essential to avoid bias. In addition, the acquisition of skills can increase student's motivation to pursue their research projects and prepare them for both academic or industrial careers. © 2020 Elsevier Inc.","Education; Secondary studies; Systematic literature review; Systematic mapping"
"FollowMe@LS: Electricity price and source aware resource management in geographically distributed heterogeneous datacenters","2021","Journal of Systems and Software","10.1016/j.jss.2021.110907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099707129&doi=10.1016%2fj.jss.2021.110907&partnerID=40&md5=48985bcf79d2a7368e09905af738477a","With rapid availability of renewable energy sources and growing interest in their use in the datacenter industry presents opportunities for service providers to reduce their energy related costs, as well as, minimize the ecological impact of their infrastructure. However, renewables are largely intermittent and can, negatively affect users’ applications and their performance, therefore, the profit of the service providers. Furthermore, services could be offered from those geographical locations where electricity is relatively cheaper than other locations; which may degrade the applications’ performance and potentially increase users’ costs. To ensure larger providers’ profits and lower users’ costs, certain non-interactive workloads could be either: moved and executed in geographical locations offering the lowest energy prices; or could be queued and delayed to execute later (in day or night time) when renewables, such as solar and wind energies, are at peak. However, these may have negative impacts on the energy consumption, workloads performance, and users’ costs. Therefore, to ensure energy, performance and cost efficiencies, appropriate workload scheduling, placement, migration, and resource management techniques are required to mange the infrastructure resources, workloads, and energy sources. In this paper, we propose a workload placement and three different migration policies that maximize the providers’ revenues, ensure the workload performance, reduce energy consumption, along with reducing ecological impacts and users’ costs. Using real workload traces and electricity prices for several geographical locations and distributed, heterogeneous, datacenters, our experimental evaluation suggest that the proposed approaches could save significant amount of energy (∼15.26%), reduces service monetary costs (∼0.53% - ∼19.66%), improves (∼1.58%) or, at least, maintains the expected level of applications’ performance, and increases providers’ revenue along with environmental sustainability, against the well-known first fit (FF), best fit (BF) heuristic algorithms, and other closest rivals. © 2021 Elsevier Inc.","Clouds; Datacenters; Energy efficiency; Migrations; Performance"
"Black-box adversarial sample generation based on differential evolution","2020","Journal of Systems and Software","10.1016/j.jss.2020.110767","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088921900&doi=10.1016%2fj.jss.2020.110767&partnerID=40&md5=d4e842cbb388cfd7a724f96da4f8ef4d","Deep Neural Networks (DNNs) are being used in various daily tasks such as object detection, speech processing, and machine translation. However, it is known that DNNs suffer from robustness problems — perturbed inputs called adversarial samples leading to misbehaviors of DNNs. In this paper, we propose a black-box technique called Black-box Momentum Iterative Fast Gradient Sign Method (BMI-FGSM) to test the robustness of DNN models. The technique does not require any knowledge of the structure or weights of the target DNN. Compared to existing white-box testing techniques that require accessing model internal information such as gradients, our technique approximates gradients through Differential Evolution and uses approximated gradients to construct adversarial samples. Experimental results show that our technique can achieve 100% success in generating adversarial samples to trigger misclassification, and over 95% success in generating samples to trigger misclassification to a specific target output label. It also demonstrates better perturbation distance and better transferability. Compared to the state-of-the-art black-box technique, our technique is more efficient. Furthermore, we conduct testing on the commercial Aliyun API and successfully trigger its misbehavior within a limited number of queries, demonstrating the feasibility of real-world black-box attack. © 2020 Elsevier Inc.","Adversarial samples; Black-box testing; Deep Neural Network; Differential evolution"
"Blended graphical and textual modelling for UML profiles: A proof-of-concept implementation and experiment","2021","Journal of Systems and Software","10.1016/j.jss.2021.110912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100104994&doi=10.1016%2fj.jss.2021.110912&partnerID=40&md5=51fc9acc070859640c2d5baa3667250e","Domain-specific modelling languages defined by extending or constraining the Unified Modelling Language (UML) through the profiling mechanism have historically relied on graphical notations to maximise human understanding and facilitate communication among stakeholders. Other notations, such as text-, form-, or table-based are, however, often preferred for specific modelling purposes, due to the nature of a specific domain or the available tooling, or for personal preference. Currently, the state of the art support for UML-based languages provides an almost completely detached, or even entirely mutually exclusive, use of graphical and textual modelling. This becomes inadequate when dealing with the development of modern systems carried out by heterogeneous stakeholders. Our intuition is that a modelling framework based on seamless blended multi-notations can disclose several benefits, among which: flexible separation of concerns, multi-view modelling based on multiple notations, convenient text-based editing operations (inside and outside the modelling environment), and eventually faster modelling activities. In this paper we report on: (i) a proof-of-concept implementation of a framework for UML and profiles modelling using blended textual and graphical notations, and (ii) an experiment on the framework, which eventually shows that blended multi-notation modelling performs better than standard single-notation modelling. © 2021","Blended modelling; MARTE; Multi-view modelling; Papyrus; UML profiles; Xtext"
"A complex network analysis of the Comprehensive R Archive Network (CRAN) package ecosystem","2020","Journal of Systems and Software","10.1016/j.jss.2020.110744","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088231470&doi=10.1016%2fj.jss.2020.110744&partnerID=40&md5=b113cbbf0ed119118355cbf7edd78c4a","Free and open source software package ecosystems have existed for a long time and are among the most sophisticated human-made systems. One of the oldest and most popular software package ecosystems is CRAN, the repository of packages of the statistical language R, which is also one of the most popular environments for statistical computing nowadays. CRAN stores a large number of packages that are updated regularly and depend on a number of other packages in a complex graph of relations; such graph is empirically studied from the perspective of complex network analysis (CNA) in the current article, showing how network theory and measures proposed by previous work can help profiling the ecosystem and detecting strengths, good practices and potential risks in three perspectives: macroscopic properties of the ecosystem (structure and complexity of the network), microscopic properties of individual packages (represented as nodes), and modular properties (community detection). Results show how complex network analysis tools can be used to assess a package ecosystem and, in particular, that of CRAN. © 2020 Elsevier Inc.","Complex network analysis; CRAN; Package ecosystems; R"
"Legacy software migration based on timing contract aware real-time execution environments","2021","Journal of Systems and Software","10.1016/j.jss.2020.110849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094156425&doi=10.1016%2fj.jss.2020.110849&partnerID=40&md5=24a0cb85139917717a5560cb04a1772d","The evolution to next generation embedded systems is shortening the obsolescence period of the underlying hardware. As this happens, software designed for those platforms (a.k.a., legacy code), that might be functionally correct and validated code, may be lost in the architecture and peripheral change unless a retargeting approach is applied. Embedded systems often have real-time computing constraints, therefore, the legacy code retargeting issue directly affects real-time systems. When dealing with real-time legacy code migration, the timing as well as the functional behaviour must be preserved. This article sets the focus on the timing issue, providing a migration path to real-time legacy embedded control applications by integrating a portable timing enforcement mechanism into a machine-adaptable binary translation tool. The proposed timing enforcement solution provides at the same time means for validating the legacy timing behaviour on the new hardware platform using formal timing specifications in the form of contracts. © 2020 Elsevier Inc.","Legacy software; Real-time systems; Retargeting; Time contract"
"An evolutionary approach for generating software models: The case of Kromaia in Game Software Engineering","2021","Journal of Systems and Software","10.1016/j.jss.2020.110804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090025200&doi=10.1016%2fj.jss.2020.110804&partnerID=40&md5=15d8e9a2758ec930eb379d4aa8bc0410","In the context of Model-Driven Engineering applied to video games, software models are high-level abstractions that represent source code implementations of varied content such as the stages of the game, vehicles, or enemy entities (e.g., final bosses). In this work, we present our Evolutionary Model Generation (EMoGen) approach to generate software models that are comparable in quality to the models created by human developers. Our approach is based on an evolution (mutation and crossover) and assessment cycle to generate the software models. We evaluated the software models generated by EMoGen in the Kromaia video game, which is a commercial video game released on Steam and PlayStation 4. Each model generated by EMoGen has more than 1000 model elements. The results, which compare the software models generated by our approach and those generated by the developers, show that our approach achieves results that are comparable to the ones created manually by the developers in the retail and digital versions of the video game case study. However, our approach only takes five hours of unattended time in comparison to ten months of work by the developers. We perform a statistical analysis, and we make an implementation of EMoGen readily available. © 2020 Elsevier Inc.","Game Software Engineering; Model-Driven Engineering; Search-based software engineering"
"Building and evaluating a theory of architectural technical debt in software-intensive systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.110925","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101964849&doi=10.1016%2fj.jss.2021.110925&partnerID=40&md5=56c68cdb45c3ab54d2b00fda88ab9e62","Architectural technical debt in software-intensive systems is a metaphor used to describe the “big” design decisions (e.g., choices regarding structure, frameworks, technologies, languages, etc.) that, while being suitable or even optimal when made, significantly hinder progress in the future. While other types of debt, such as code-level technical debt, can be readily detected by static analyzers, and often be refactored with minimal or only incremental efforts, architectural debt is hard to be identified, of wide-ranging remediation cost, daunting, and often avoided. In this study, we aim at developing a better understanding of how software development organizations conceptualize architectural debt, and how they deal with it. In order to do so, in this investigation we apply a mixed empirical method, constituted by a grounded theory study followed by focus groups. With the grounded theory method we construct a theory on architectural technical debt by eliciting qualitative data from software architects and senior technical staff from a wide range of heterogeneous software development organizations. We applied the focus group method to evaluate the emerging theory and refine it according to the new data collected. The result of the study, i.e., a theory emerging from the gathered data, constitutes an encompassing conceptual model of architectural technical debt, identifying and relating concepts such as its symptoms, causes, consequences, management strategies, and communication problems. From the conducted focus groups, we assessed that the theory adheres to the four evaluation criteria of classic grounded theory, i.e., the theory fits its underlying data, is able to work, has relevance, and is modifiable as new data appears. By grounding the findings in empirical evidence, the theory provides researchers and practitioners with novel knowledge on the crucial factors of architectural technical debt experienced in industrial contexts. © 2021 The Author(s)","Focus group; Grounded theory; Software architecture; Software engineering; Software evolution; Technical debt"
"Product metrics for spreadsheets—A systematic review","2021","Journal of Systems and Software","10.1016/j.jss.2021.110910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100763029&doi=10.1016%2fj.jss.2021.110910&partnerID=40&md5=7cdb2977486e7f6e052943abc814f55c","Software product metrics allow practitioners to improve their products and to optimize development processes based on quantifiable characteristics of source code. To facilitate similar benefits for spreadsheet programs, researchers proposed various product metrics for spreadsheets over the last decades. However, to our knowledge, no comprehensive overview of those efforts is currently available. In this paper, we close this gap by conducting a literature review of research works that either inherently or explicitly define product metrics for spreadsheets. We scanned five major digital libraries for scientific papers that define or use spreadsheet product metrics. Based on the identified 37 papers, we created a novel catalog of product metrics for spreadsheets. The catalog can be used by practitioners and researchers as a central reference for spreadsheet product metrics. In the paper, we (i) describe the proposed metrics in detail, (ii) report how often and for what purposes the metrics are used, (iii) identify significant discrepancies in the naming and definition of the metrics, and (iv) investigate how the appropriateness of the metrics was evaluated. © 2021 The Author(s)","Metrics catalog; Spreadsheet metrics; Spreadsheet metrics survey; Spreadsheet product metrics; Spreadsheet quality assurance"
"Capturing creative requirements via requirements reuse: A machine learning-based approach","2020","Journal of Systems and Software","10.1016/j.jss.2020.110730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088090137&doi=10.1016%2fj.jss.2020.110730&partnerID=40&md5=9d783b29f0dfd2635d86787a6d94f59f","The software industry has become increasingly competitive as we see multiple software serving the same domain and striving for customers. To that end, modern software needs to provide creative features to improve sustainability. To advance software creativity, research has proposed several techniques, including multi-day workshops involving experienced requirements analysts, and semi-automated tools to support creative thinking in a limited scope. Such approaches are either useful only for software with already rich issue tracking systems, or require substantial engagement from analysts with creative minds. In a recent work, we have demonstrated a novel framework that is beneficial for both novel and existing software and allows end-to-end automation promoting creativity. The framework reuses requirements from similar software freely available online, utilizes advanced natural language processing and machine learning techniques, and leverages the concept of requirement boilerplate to generate candidate creative requirements. An application of our framework on software domains: Antivirus, Web Browser, and File Sharing followed by a human subject evaluation have shown promising results. In this invited extension, we present further analysis for our research questions and report an additional evaluation by human subjects. The results exhibit the framework's ability in generating creative features even for a relatively matured application domain, such as Web Browser, and provoking creative thinking among developers irrespective of their experience levels. © 2020 Elsevier Inc.","Boilerplate; Creativity in RE; Machine learning; Natural language processing; Requirements engineering; Requirements reuse"
"Are game engines software frameworks? A three-perspective study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110846","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092504305&doi=10.1016%2fj.jss.2020.110846&partnerID=40&md5=55d7a2871a463cc597b30486fa155cd1","Game engines help developers create video games and avoid duplication of code and effort, like frameworks for traditional software systems. In this paper, we explore open-source game engines along three perspectives: literature, code, and human. First, we explore and summarize the academic literature on game engines. Second, we compare the characteristics of the 282 most popular engines and the 282 most popular frameworks in GitHub. Finally, we survey 124 engine developers about their experience with the development of their engines. We report that: (1) Game engines are not well-studied in software-engineering research with few studies having engines as object of research. (2) Open-source game engines are slightly larger in terms of size and complexity and less popular and engaging than traditional frameworks. Their programming languages differ greatly from frameworks. Engine projects have shorter histories with less releases. (3) Developers perceive game engines as different from traditional frameworks. Generally, they build game engines to (a) better control the environment and source code, (b) learn about game engines, and (c) develop specific games. We conclude that open-source game engines have differences compared to traditional open-source frameworks although this differences do not demand special treatments. © 2020","Framework; Game-engine; Mining; Open-source; Video-game"
"Hansie: Hybrid and consensus regression test prioritization","2021","Journal of Systems and Software","10.1016/j.jss.2020.110850","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094944038&doi=10.1016%2fj.jss.2020.110850&partnerID=40&md5=0bf64cfae3774e64cbd21240dd131e42","Traditionally, given a test-suite and the underlying system-under-test, existing test-case prioritization heuristics report a permutation of the original test-suite that is seemingly best according to their criteria. However, we observe that a single heuristic does not perform optimally in all possible scenarios, given the diverse nature of software and its changes. Hence, multiple individual heuristics exhibit effectiveness differently. Interestingly, together, the heuristics bear the potential of improving the overall regression test selection across scenarios. In this paper, we pose the test-case prioritization as a rank aggregation problem from social choice theory. Our solution approach, named Hansie, is two-flavored: one involving priority-aware hybridization, and the other involving priority-blind computation of a consensus ordering from individual prioritizations. To speed-up test-execution, Hansie executes the aggregated test-case orderings in a parallel multi-processed manner leveraging regular windows in the absence of ties, and irregular windows in the presence of ties. We show the benefit of test-execution after prioritization and introduce a cost-cognizant metric (EPL) for quantifying overall timeline latency due to load-imbalance arising from uniform or non-uniform parallelization windows. We evaluate Hansie on 20 open-source subjects totaling 287,530 lines of source code, 69,305 test-cases, and with parallelization support of up to 40 logical CPUs. © 2020 Elsevier Inc.","Consensus; Hybridization; Permutation distances; Priority awareness; Regression test prioritization"
"Enabling consistency in view-based system development — The VITRUVIUS approach","2021","Journal of Systems and Software","10.1016/j.jss.2020.110815","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092430945&doi=10.1016%2fj.jss.2020.110815&partnerID=40&md5=d28cbaaa6ecfbb3b4477113d7571d1bb","During the development of large software-intensive systems, developers use several modeling languages and tools to describe a system from different viewpoints. Model-driven and view-based technologies have made it easier to define domain-specific languages and transformations. Nevertheless, using several languages leads to fragmentation of information, to redundancies in the system description, and eventually to inconsistencies. Inconsistencies have negative impacts on the system's quality and are costly to fix. Often, there is no support for consistency management across multiple languages. Using a single language is no practicable solution either, as it is overly complex to define, use, and evolve such a language. View-based development is a suitable approach to deal with complex systems, and is widely used in other engineering disciplines. Still, we need to cope with the problems of fragmentation and consistency. In this paper, we present the VITRUVIUS approach for consistency in view-based modeling. We describe the approach by formalizing the notion of consistency, presenting languages for consistency preservation, and defining a model-driven development process. Furthermore, we show how existing models can be integrated. We have evaluated our approach at two case studies from component-based and embedded automotive software development, using our prototypical implementation based on the Eclipse Modeling Framework. © 2020 The Authors","Consistency; Model transformations; Model views; Model-driven software development"
"Scrum versus Rational Unified Process in facing the main challenges of product configuration systems development","2020","Journal of Systems and Software","10.1016/j.jss.2020.110732","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087859453&doi=10.1016%2fj.jss.2020.110732&partnerID=40&md5=203a10bdbc5e76e57dfc867193b2b7ef","Product configuration systems (PCSs) are software applications that enable companies to customise configurable products by facilitating the automation of sales and engineering. Widely used in various industries, PCSs can bring substantial benefits and constitute a fundamental tool for mass customisation. However, serious challenges in PCS development have been reported. Software engineering approaches, such as the rational unified process (RUP) and Scrum, have been adopted to realise high-quality PCSs, but research insights on their use in PCS development are very limited, and their different capabilities to address PCS challenges are almost totally unexplored. This article illustrates the application of RUP and Scrum in PCS development and compares their contributions to addressing PCS development challenges. To perform this comparison, four PCS projects in a company that moved from RUP to Scrum are analysed. The evidence provided suggests that moving from RUP to Scrum has a positive effect in facing organisational,​ IT-related and resource constraint challenges. The results also highlight worsening knowledge management and documentation, product modelling and visualisation. The findings suggest the adaptation of Scrum for PCS development to reinforce Scrum's knowledge-related capabilities. © 2020 Elsevier Inc.","Agile; Product configuration systems (PCSs); Rational unified process (RUP); Scrum"
"On introducing automatic test case generation in practice: A success story and lessons learned","2021","Journal of Systems and Software","10.1016/j.jss.2021.110933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102617817&doi=10.1016%2fj.jss.2021.110933&partnerID=40&md5=b40faf9c91eada3b1199fef556b466f8","The level and quality of automation dramatically affects software testing activities, determines costs and effectiveness of the testing process, and largely impacts on the quality of the final product. While costs and benefits of automating many testing activities in industrial practice (including managing the quality process, executing large test suites, and managing regression test suites) are well understood and documented, the benefits and obstacles of automatically generating system test suites in industrial practice are not well reported yet, despite the recent progresses of automated test case generation tools. Proprietary tools for automatically generating test cases are becoming common practice in large software organizations, and commercial tools are becoming available for some application domains and testing levels. However, generating system test cases in small and medium-size software companies is still largely a manual, inefficient and ad-hoc activity. This paper reports our experience in introducing techniques for automatically generating system test suites in a medium-size company. We describe the technical and organizational obstacles that we faced when introducing automatic test case generation in the development process of the company, and present the solutions that we successfully experienced in that context. In particular, the paper discusses the problems of automating the generation of test cases by referring to a customized ERP application that the medium-size company developed for a third party multinational company, and presents ABT2.0, the test case generator that we developed by tailoring ABT, a research state-of-the-art GUI test generator, to their industrial environment. This paper presents the new features of ABT2.0, and discusses how these new features address the issues that we faced. © 2021 Elsevier Inc.","ABT; Automatic system testing; Automatic test generation; GUI testing; Search based testing; Software testing"
"Do scaling agile frameworks address global software development risks? An empirical study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110823","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091332923&doi=10.1016%2fj.jss.2020.110823&partnerID=40&md5=f8c8370b1596a40798707629a8502ccd","Driven by the need to coordinate activities of multiple agile development teams cooperating to produce a large software product, software-intensive organizations are turning to scaling agile software development frameworks. Despite the growing adoption of various scaling agile frameworks, there is little empirical evidence of how effective their practices are in mitigating risk, especially in global software development (GSD), where project failure is a known problem. In this study, we develop a GSD Risk Catalog of 63 risks to assess the degree to which two scaling agile frameworks–Disciplined Agile Delivery (DAD) and the Scaled Agile Framework (SAFe)–address software project risks in GSD. We examined data from two longitudinal case studies implementing each framework to identify the extent to which the framework practices address GSD risks. Scaling agile frameworks appear to help companies eliminate or mitigate many traditional risks in GSD, especially relating to users and customers. However, several important risks were not eliminated or mitigated. These persistent risks in the main belonged to the Environment quadrant highlighting the inherent risk in developing software across geographic boundaries. Perhaps these frameworks (and arguably any framework), would have difficulty alleviating, issues that appear to be outside the immediate control of the organization. © 2020","Disciplined Agile Delivery (DAD); Empirical study; Global software development (GSD); Risks; Scaled Agile Framework (SAFe); Scaling agile frameworks"
"Assurance and certification of cyber–physical systems: The AMASS open source ecosystem","2021","Journal of Systems and Software","10.1016/j.jss.2020.110812","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090415830&doi=10.1016%2fj.jss.2020.110812&partnerID=40&md5=3b3e549c6d66bc7b632d20bf60cde0b2","Many cyber–physical systems (CPS) are subject to rigorous assurance and certification processes to provide confidence that undue risks are not posed and thus the systems are trustworthy. These processes are complex and time-consuming and tool support can greatly aid in their execution. In line with other trends for systems and software engineering, the need for and interest in open source tools for assurance and certification is growing and different initiatives have been launched. As a concrete example, we report on our experience in developing the AMASS open source ecosystem. This ecosystem includes (1) an open source tool platform that supports the main CPS assurance and certification activities, (2) external tools with added-value features, and (3) an open community of developers and users. The platform integrates existing solutions for system modelling, process engineering, and compliance and argumentation management. We also present the application of the AMASS tool platform in 11 industrial case studies from five different application domains. The results show that the platform is a feasible means for CPS assurance and certification and that practitioners find benefits in assurance-oriented system modelling and in integrated system assurance information, among other areas. Nonetheless, improvement opportunities also exist, most notably regarding tool interoperability and usability. © 2020 Elsevier Inc.","AMASS; Assurance; Certification; Cyber–physical​ system; Ecosystem; Open source"
"Convolutional neural networks for enhanced classification mechanisms of metamodels","2021","Journal of Systems and Software","10.1016/j.jss.2020.110860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096121119&doi=10.1016%2fj.jss.2020.110860&partnerID=40&md5=ca6285b8b8ed222076d39aa568b2626d","Conventional wisdom on Model-Driven Engineering suggests that metamodels are crucial elements for modeling environments consisting of graphical editors, transformations, code generators, and analysis tools. Software repositories are commonly used in practice for locating existing artifacts provided that a classification procedure is available. However, the manual classification of metamodel in repositories produces results that are influenced by the subjectivity of human perception besides being tedious and prone to errors. Therefore, automated techniques for classifying metamodels stored in repositories are highly desirable and stringent. In this work, we propose MEMOCNN as a novel approach to classification of metamodels. In particular, we consider metamodels as data points and classify them using supervised learning techniques. A convolutional neural network has been built to learn from labeled data, and use the trained weights to group unlabeled metamodels. A comprehensive experimental evaluation proves that the proposal effectively categorizes input data and outperforms a state-of-the-art baseline. © 2020 Elsevier Inc.",""
"Crowdsourced Behavior-Driven Development","2021","Journal of Systems and Software","10.1016/j.jss.2020.110840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091623382&doi=10.1016%2fj.jss.2020.110840&partnerID=40&md5=6caa5b38860f11a50ccc3c2106bc6145","Key to the effectiveness of crowdsourcing approaches for software engineering is workflow design, describing how complex work is organized into small, relatively independent microtasks. This paper, we introduce a Behavior-Driven Development (BDD) workflow for accomplishing programming work through self-contained microtasks, implemented as a preconfigured environment called CrowdMicroservices. In our approach, a client, acting on behalf of a software team, describes a microservice as a set of endpoints with paths, requests, and responses. A crowd then implements the endpoints, identifying individual endpoint behaviors that they test, implement, debug, create new functions, and interact with persistence APIs as needed. To evaluate our approach, we conducted a feasibility study in which a small crowd worked to implement a small ToDo microservice. The crowd created an implementation with only four defects, completing 350 microtasks and implementing 13 functions. We discuss the implications of these findings for incorporating crowdsourced programming contributions into traditional software projects. © 2020 Elsevier Inc.","Behavior-Driven Development; Crowdsourcing; Microservices; Microtask programming; Programming environments; Workflow"
"A family of experiments to generate graphical user interfaces from BPMN models with stereotypes","2021","Journal of Systems and Software","10.1016/j.jss.2020.110883","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098461677&doi=10.1016%2fj.jss.2020.110883&partnerID=40&md5=35d3ba8444a53ff8a8909422b292c030","Context: A significant gap separates Business Process Model and Notation (BPMN) models representing processes from the design of Graphical User Interfaces (GUIs). Objective: This paper reports on a family of experiments to validate a method to automatically generate GUIs from BPMN models using stereotypes complemented with UML class primitives, and transformation rules. Method: We conducted two replications (23 and 31 subjects respectively) in which we compared two methods to generate GUIs from BPMN models; one automatic (using Stereotyped BPMN models) and one manual (using Non-stereotyped BPMN models). The study focuses on comparing effort, accuracy, and satisfaction (in terms of perceived ease of use (PEOU), perceived usefulness (PU), and intention to use (ITU)). Results: Results yield significant differences for Effort, Accuracy, and ITU. Effort is lower for the Non-stereotyped method, while accuracy and ITU are higher for the Stereotyped one. If we consider only experimental units whose BPMN models show an accuracy over 75% compared to those of the experimenters’ solution, the difference in accuracy for the designed GUIs is even more significant; in contrast, differences for effort and ITU are reduced. Conclusions: The use of the Stereotyped method reduces the possibility of errors in the process of designing GUIs. © 2020 Elsevier Inc.","Business process model and notation models; Empirical study; Graphical user interfaces design; Stereotypes"
"A systematic literature review of blockchain and smart contract development: Techniques, tools, and open challenges","2021","Journal of Systems and Software","10.1016/j.jss.2020.110891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098979219&doi=10.1016%2fj.jss.2020.110891&partnerID=40&md5=833a98593dfa09a878df12d6643e8dc2","Blockchain platforms and languages for writing smart contracts are becoming increasingly popular. However, smart contracts and blockchain applications are developed through non-standard software life-cycles, in which, for instance, delivered applications can hardly be updated or bugs resolved by releasing a new version of the software. Therefore, this systematic literature review oriented to software engineering aims at highlighting current problems and possible solutions concerning smart contracts and blockchain applications development. In this paper, we analyze 96 articles (written from 2016 to 2020) presenting solutions to tackle software engineering-specific challenges related to the development, test, and security assessment of blockchain-oriented software. In particular, we review papers (that appeared in international journals and conferences) relating to six specific topics: smart contract testing, smart contract code analysis, smart contract metrics, smart contract security, Dapp performance, and blockchain applications. Beyond the systematic review of the techniques, tools, and approaches that have been proposed in the literature to address the issues posed by the development of blockchain-based software, for each of the six aforementioned topics, we identify open challenges that require further research. © 2021 Elsevier Inc.","Empirical study; Ethereum; Smart contract; Software engineering for blockchain technologies; Software metrics; Software quality"
"A Kubernetes controller for managing the availability of elastic microservice based stateful applications","2021","Journal of Systems and Software","10.1016/j.jss.2021.110924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100910850&doi=10.1016%2fj.jss.2021.110924&partnerID=40&md5=b01d6d2e3e057f7b774c14580fc176f6","The architectural style of microservices has been gaining popularity in recent years. In this architectural style, small and loosely coupled modules are deployed and scaled independently to compose cloud-native applications. Carrier-grade service providers are migrating their legacy applications to a microservice based architecture running on Kubernetes which is an open source platform for orchestrating containerized microservice based applications. However, in this migration, service availability remains a concern. Service availability is measured as the percentage of time the service is provisioned. High Availability (HA) is achieved when the service is available at least 99.999% of the time. In this paper, we identify possible architectures for deploying stateful microservice based applications with Kubernetes and evaluate Kubernetes from the perspective of availability it provides for its managed applications. The results of our experiments show that the repair actions of Kubernetes cannot satisfy HA requirements, and in some cases cannot guarantee service recovery. Therefore, we propose an HA State Controller which integrates with Kubernetes and allows for application state replication and automatic service redirection to the healthy microservice instances by enabling service recovery in addition to the repair actions of Kubernetes. Based on experiments we evaluate our solution and compare the different architectures from the perspective of availability and scaling overhead. The results of our investigations show that our solution can improve the recovery time of stateful microservice based applications by 50%. © 2021 Elsevier Inc.","Availability; Containers; Elasticity; Failure; Kubernetes; Microservices"
"A comprehensive comparative study of clustering-based unsupervised defect prediction models","2021","Journal of Systems and Software","10.1016/j.jss.2020.110862","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096687812&doi=10.1016%2fj.jss.2020.110862&partnerID=40&md5=926219c0e671a39dbbdd1870083904d1","Software defect prediction recommends the most defect-prone software modules for optimization of the test resource allocation. The limitation of the extensively-studied supervised defect prediction methods is that they require labeled software modules which are not always available. An alternative solution is to apply clustering-based unsupervised models to the unlabeled defect data, called Clustering-based Unsupervised Defect Prediction (CUDP). However, there are few studies to explore the impacts of clustering-based models on defect prediction performance. In this work, we performed a large-scale empirical study on 40 unsupervised models to fill this gap. We chose an open-source dataset including 27 project versions with 3 types of features. The experimental results show that (1) different clustering-based models have significant performance differences and the performance of models in the instance-violation-score-based clustering family is obviously superior to that of models in hierarchy-based, density-based, grid-based, sequence-based, and hybrid-based clustering families; (2) the models in the instance-violation-score-based clustering family achieves competitive performance compared with typical supervised models; (3) the impacts of feature types on the performance of the models are related to the indicators used; and (4)the clustering-based unsupervised models do not always achieve better performance on defect data with the combination of the 3 types of features. © 2020 Elsevier Inc.","Clustering-based unsupervised models; Data analytics for defect prediction; Empirical study"
"Don't run on fumes—Parametric gas bounds for smart contracts","2021","Journal of Systems and Software","10.1016/j.jss.2021.110923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101900327&doi=10.1016%2fj.jss.2021.110923&partnerID=40&md5=d016918b3a9ab1859618d4b64a67bafe","Gas is a measurement unit of the computational effort that it will take to execute every single replicated operation that takes part in the Ethereum blockchain platform. If a transaction exceeds the amount of gas allotted by the user (known as gas limit), an out-of-gas exception is raised and its execution is interrupted. One of the main open problems in the analysis of Ethereum smart contracts is the inference of sound bounds on their gas consumption. We present, to the best of our knowledge, the first static analysis that is able to infer sound parametric (i.e., non-constant) gas bounds for smart contracts. The inferred bounds can be parametric on the sizes of the input parameters for the functions, but also they can be parametric on the contract state, or blockchain data. Our gas analysis is developed at EVM bytecode level, in which Ethereum gas model is defined. Our analysis is implemented in a tool named GASTAP, Gas-Aware Smart contracT Analysis Platform, which takes as input a smart contract and automatically infers sound gas upper-bounds for its public functions. GASTAP has been applied over 318,093 functions fetched from the Ethereum blockchain, and succeeded to obtain gas bounds for 90.24% of them. © 2021 Elsevier Inc.","Decompilation; Resource analysis; Smart contracts; Static analysis"
"Architectural Design Space for Modelling and Simulation as a Service: A Review","2020","Journal of Systems and Software","10.1016/j.jss.2020.110752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088899853&doi=10.1016%2fj.jss.2020.110752&partnerID=40&md5=5e9372b97fe29bf034c9be9e64148bad","Modelling and Simulation as a Service (MSaaS) is a promising approach to deploy and execute Modelling and Simulation (M&S) applications quickly and on-demand. An appropriate software architecture is essential to deliver quality M&S applications following the MSaaS concept to a wide range of users. This study aims to characterize the state-of-the-art MSaaS architectures by conducting a systematic review of 31 papers published from 2010 to 2018. Our findings reveal that MSaaS applications are mainly designed using layered architecture style, followed by service-oriented architecture, component-based architecture, and pluggable component-based architecture. We also found that interoperability and deployability have the greatest importance in the architecture of MSaaS applications. In addition, our study indicates that the current MSaaS architectures do not meet the critical user requirements of modern M&S applications appropriately. Based on our results, we recommend that there is a need for more effort and research to (1) design the user interfaces that enable users to build and configure simulation models with minimum effort and limited domain knowledge, (2) provide mechanisms to improve the deployability of M&S applications, and (3) gain a deep insight into how M&S applications should be architected to respond to the emerging user requirements in the military domain. © 2020 Elsevier Inc.","Architecture; Modelling and Simulation as a Service; MSaaS; Systematic review"
"Data quality certification using ISO/IEC 25012: Industrial experiences","2021","Journal of Systems and Software","10.1016/j.jss.2021.110938","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102270958&doi=10.1016%2fj.jss.2021.110938&partnerID=40&md5=1aeb145ca4a1649a89066f57156eb756","The most successful organizations in the world are data-driven businesses. Data is at the core of the business of many organizations as one of the most important assets, since the decisions they make cannot be better than the data on which they are based. Due to this reason, organizations need to be able to trust their data. One important activity that helps to achieve data reliability is the evaluation and certification of the quality level of organizational data repositories. This paper describes the results of the application of a data quality evaluation and certification process to the repositories of three European organizations belonging to different sectors. We present findings from the point of view of both the data quality evaluation team and the organizations that underwent the evaluation process. In this respect, several benefits have been explicitly recognized by the involved organizations after achieving the data quality certification for their repositories (e.g., long-term organizational sustainability better internal knowledge of data, and a more efficient management of data quality). As a result of this experience, we have also identified a set of best practices aimed to enhance the data quality evaluation process. © 2021 Elsevier Inc.","Data quality certification; Data quality evaluation process; Data quality management; ISO/IEC 25012; ISO/IEC 25024; ISO/IEC 25040"
"Ensemble Effort Estimation using dynamic selection","2021","Journal of Systems and Software","10.1016/j.jss.2021.110904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100624983&doi=10.1016%2fj.jss.2021.110904&partnerID=40&md5=3b1acf19ac0af039cd87445e810752d2","The Software Effort Estimation (SEE) process has been approached in different ways in the literature, including models built from Machine Learning (ML). The combination of these models (Ensemble) is an important research topic in ML, and has lead to improvements in accuracy compared to individual models. This paper proposes heterogeneous and dynamic ensemble selection (DES) models, composed by a set of regressors dynamically selected by classifiers to estimate software development effort. In the training phase, a pool of regression algorithms is trained using training data and a validation data set to validate the models. Next, some classifiers are trained to identify the best regression model from the pool for each training instance. In the test phase each trained classifier is used to dynamically select a regressor model from the pool for predicting the effort for each test instance. The final prediction is given by the combination of the predictions of the regressors selected by the classifiers. An experimental analysis considering a relevant set of software effort estimation problems is reported. The experiments demonstrate that the proposed method outperforms individual regressors and some state of the art models of the literature. © 2021","Dynamic ensemble selection; Dynamic selection; Ensemble Effort Estimation; Machine Learning; Software Effort Estimation"
"A systematic mapping study of developer social network research","2021","Journal of Systems and Software","10.1016/j.jss.2020.110802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090120296&doi=10.1016%2fj.jss.2020.110802&partnerID=40&md5=896ab5411e11a20486f0c623d5c8d0ef","Developer social networks (DSNs) are a tool for the analysis of community structures and collaborations between developers in software projects and software ecosystems. Within this paper, we present the results of a systematic mapping study on the use of DSNs in software engineering research. We identified 255 primary studies on DSNs. We mapped the primary studies to research directions, collected information about the data sources and the size of the studies, and conducted a bibliometric assessment. We found that nearly half of the research investigates the structure of developer communities. Other frequent topics are prediction systems build using DSNs, collaboration behavior between developers, and the roles of developers. Moreover, we determined that many publications use a small sample size regarding the number of projects, which could be problematic for the external validity of the research. Our study uncovered several open issues in the state of the art, e.g., studying inter-company collaborations, using multiple information sources for DSN research, as well as general lack of reporting guidelines or replication studies. © 2020 Elsevier Inc.","Developer social networks; Literature survey; Mapping study"
"ReSIde: Reusable service identification from software families","2020","Journal of Systems and Software","10.1016/j.jss.2020.110748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088380467&doi=10.1016%2fj.jss.2020.110748&partnerID=40&md5=ebf26f7f0ede11a579ee9ddce5e05f3b","The clone-and-own approach becomes a common practice to quickly develop Software Product Variants (SPVs) that meet variability in user requirements. However, managing the reuse and maintenance of the cloned codes is a very hard task. Therefore, we aim to analyze SPVs to identify cloned codes and package them using a modern systematic reuse approach like Service-Oriented Architecture (SOA). The objective is to benefit from all the advantages of SOA when creating new SPVs. The development based on services in SOA supports the software reuse and maintenance better than the development based on individual classes in monolithic object-oriented software. Existing service identification approaches identify services based on the analysis of a single software product. These approaches are not able to analyze multiple SPVs to identify reusable services of cloned codes. Identifying services by analyzing several SPVs allows to increase the reusability of identified services. In this paper, we propose ReSIde (Reusable Service Identification): an automated approach that identifies reusable services from a set of object-oriented SPVs. This is based on analyzing the commonality and the variability between SPVs to identify the implementation of reusable functionalities corresponding to cloned codes that can be packaged as reusable services. To validate ReSIde, we have applied it on three product families of different sizes. The results show that the services identified based on the analysis of multiple product variants using ReSIde are more reusable than services identified based on the analysis of singular ones. © 2020 Elsevier Inc.","Object-oriented source code; Reverse engineering; Service-oriented reengineering; Software families; Software reuse; Variability"
"Initiatives and challenges of using gamification in software engineering: A Systematic Mapping","2021","Journal of Systems and Software","10.1016/j.jss.2020.110870","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097656638&doi=10.1016%2fj.jss.2020.110870&partnerID=40&md5=7ac05241f188ff21fbc5643c544050f7","Context: Gamification is an emerging subject that has been applied in different areas, bringing contributions to different types of activities. Objective: This paper aims to characterize how gamification has been adopted in non-educational contexts of software engineering (SE) activities. Methods: We performed a Systematic Mapping of the literature obtained from relevant databases of the area. The searches retrieved 2640 studies (published up to January 2020), of which 548 were duplicates, 82 were selected after applying the inclusion and exclusion criteria, and 21 were included via the backward snowballing technique, thus reaching a total of 103 studies to be analyzed. Results: Gamification provided benefits to activities like requirements specification, development, testing, project management, and support process. There is evidence of gamified support to some CMMI 2.0 Practice Areas. The most commonly used gamification elements are points and leaderboards. The main benefit achieved is the increased engagement and motivation to perform tasks. Conclusion: The number of publications and new research initiatives have increased over the years and, from the original authors’ reports, many positive results were achieved in SE activities. Despite this, gamification can still be explored for many SE tasks; for the addressed ones, empirical evidence is very limited. © 2020 Elsevier Inc.","Gamification; Software engineering; Systematic literature mapping"
"Generating summaries for methods of event-driven programs: An Android case study","2020","Journal of Systems and Software","10.1016/j.jss.2020.110800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089832552&doi=10.1016%2fj.jss.2020.110800&partnerID=40&md5=4ec764683d60204b4ae45c885249678b","The lack of proper documentation makes program comprehension a cumbersome process for developers. Source code summarization is one of the existing solutions to this problem. Many approaches have been proposed to summarize source code in recent years. A prevalent weakness of these solutions is that they do not pay much attention to interactions among elements of software. An element is simply a callable code snippet such as a method or even a clickable button. As a result, these approaches cannot be applied to event-driven programs, such as Android applications, because they have specific features such as numerous interactions between their elements. To tackle this problem, we propose a novel approach based on deep neural networks and dynamic call graphs to generate summaries for methods of event-driven programs. First, we collect a set of comment/code pairs from Github and train a deep neural network on the set. Afterward, by exploiting a dynamic call graph, the Pagerank algorithm, and the pre-trained deep neural network, we generate summaries. An empirical evaluation with 14 real-world Android applications and 42 participants indicates 32.3% BLEU4 which is a definite improvement compared to the existing state-of-the-art techniques. We also assessed the informativeness and naturalness of our generated summaries from developers’ perspectives and showed they are sufficiently understandable and informative. © 2020 Elsevier Inc.","Deep learning; Event-driven programs; Neural machine translation; Source code summarization"
"SLA-aware multiple migration planning and scheduling in SDN-NFV-enabled clouds","2021","Journal of Systems and Software","10.1016/j.jss.2021.110943","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102577367&doi=10.1016%2fj.jss.2021.110943&partnerID=40&md5=72d3c02a5e4be6e9f9b560d720ae3ae4","In Software-Defined Networking (SDN)-enabled cloud data centers, live migration is a key approach used for the reallocation of Virtual Machines (VMs) and Virtual Network Functions (VNFs). Using live migration, cloud providers can address their dynamic resource management and fault tolerance objectives without interrupting the service of users. However, performing multiple live migrations in arbitrary order can lead to service degradation. Therefore, efficient migration planning is essential to reduce the impact of live migration overheads. In addition, to prevent Quality of Service (QoS) degradations and Service Level Agreement (SLA) violations, it is necessary to set priorities for different live migration requests with various urgency. In this paper, we propose SLAMIG, a set of algorithms that composes deadline-aware multiple migration grouping algorithm and on-line migration scheduling to determine the sequence of VM/VNF migrations. The experimental results show that our approach with reasonable algorithm runtime can efficiently reduce the number of deadline misses and has a good migration performance compared with the one-by-one scheduling and two state-of-the-art algorithms in terms of total migration time, average execution time, downtime, and transferred data. We also evaluate and analyze the impact of multiple migrations on QoS and energy consumption. © 2021 Elsevier Inc.","Deadline violation; Energy consumption; Live VM migration; Multiple migration performance; Quality of Service; Software-defined networking"
"A Systematic Mapping Study on Microservices Architecture in DevOps","2020","Journal of Systems and Software","10.1016/j.jss.2020.110798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090026555&doi=10.1016%2fj.jss.2020.110798&partnerID=40&md5=6094b5d14e18c776ecdfcdada562f243","Context: Applying Microservices Architecture (MSA) in DevOps has received significant attention in recent years. However, there exists no comprehensive review of the state of research on this topic. Objective: This work aims to systematically identify, analyze, and classify the literature on MSA in DevOps. Methods: A Systematic Mapping Study (SMS) has been conducted on the literature published between January 2009 and July 2018. Results: Forty-seven studies were finally selected and the key results are: (1) Three themes on the research on MSA in DevOps are “microservices development and operations in DevOps”, “approaches and tool support for MSA based systems in DevOps”, and “MSA migration experiences in DevOps”. (2) 24 problems with their solutions regarding implementing MSA in DevOps are identified. (3) MSA is mainly described by using boxes and lines. (4) Most of the quality attributes are positively affected when employing MSA in DevOps. (5) 50 tools that support building MSA based systems in DevOps are collected. (6) The combination of MSA and DevOps has been applied in a wide range of application domains. Conclusion: The results and findings will benefit researchers and practitioners to conduct further research and bring more dedicated solutions for the issues of MSA in DevOps. © 2020 Elsevier Inc.","DevOps; Microservices Architecture; Systematic Mapping Study"
"ProDSPL: Proactive self-adaptation based on Dynamic Software Product Lines","2021","Journal of Systems and Software","10.1016/j.jss.2021.110909","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100049475&doi=10.1016%2fj.jss.2021.110909&partnerID=40&md5=3354f9aa77347ede9e9f1441ed9cc2a2","Dynamic Software Product Lines (DSPLs) are a well-accepted approach to self-adaptation at runtime. In the context of DSPLs, there are plenty of reactive approaches that apply countermeasures as soon as a context change happens. In this paper we propose a proactive approach, PRODSPL, that exploits an automatically learnt model of the system, anticipates future variations of the system and generates the best DSPL configuration that can lessen the negative impact of future events on the quality requirements of the system. Predicting the future fosters adaptations that are good for a longer time and therefore reduces the number of reconfigurations required, making the system more stable. PRODSPL formulates the problem of the generation of dynamic reconfigurations as a proactive controller over a prediction horizon, which includes a mapping of the valid configurations of the DSPL into linear constraints. Our approach is evaluated and compared with a reactive approach, DAGAME, also based on a DSPL, which uses a genetic algorithm to generate quasi-optimal feature model configurations at runtime. PRODSPL has been evaluated using a strategy mobile game and a set of randomly generated feature models. The evaluation shows that PRODSPL gives good results with regard to the quality of the configurations generated when it tries anticipate future events. Moreover, in doing so, PRODSPL enforces the system to make as few reconfigurations as possible. © 2021 Elsevier Inc.","Dynamic Software Product Lines; Linear constraint; Optimization; Proactive control; Self-adaptation"
"Early and quick function points analysis: Evaluations and proposals","2021","Journal of Systems and Software","10.1016/j.jss.2020.110888","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098870522&doi=10.1016%2fj.jss.2020.110888&partnerID=40&md5=8488d44cf0f7970e0279c7086f71c604","Measuring Function Points following the standard process is sometimes long and expensive. To solve this problem, several early estimation methods have been proposed. Among these, the “NESMA Estimated” method is one of the most widely used; it has also been selected by the International Function Point User Group as the official early function point analysis method, under the name of ‘High-level FPA’ method. A large-scale empirical study has shown that the High-level FPA method – although sufficiently accurate – tends to underestimate the size of software. Underestimating the size of the software to be developed can easily lead to wrong decisions, which can even result in project failure. In this paper we investigate the reasons why the High-level FPA method tends to underestimate. We also explore how to improve the method to make it more accurate. Finally, we propose size estimation models built using different criteria and we evaluate the estimation accuracy of these new models. Our results show that it is possible to derive size estimation models from historical data using simple regression techniques: these models are slightly less accurate than those delivered by the High-level FPA method in terms of absolute estimation errors, but can be used earlier than the High-level FPA method, are cheaper, and do not underestimate software size. © 2020 Elsevier Inc.","Early size estimation; Function point analysis; Function Points; Functional Size Measurement; High-level FPA; NESMA Estimated"
"Adaptive distributed monitors of spatial properties for cyber–physical systems","2021","Journal of Systems and Software","10.1016/j.jss.2021.110908","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099783289&doi=10.1016%2fj.jss.2021.110908&partnerID=40&md5=00fcb1f21e35ed4e56355b0e97365eec","Cyber–physical systems increasingly feature highly-distributed and mobile deployments of devices spread over large physical environments: in these contexts, it is generally very difficult to engineer trustworthy critical services, mostly because formal methods generally hardly scale with the number of involved devices, especially when faults, continuous changes, and dynamic topologies are the norm. To start addressing this problem, in this paper we devise a formally correct and self-adaptive implementation of distributed monitors for spatial properties. We start from the Spatial Logic of Closure Spaces, and provide a compositional translation that takes a formula and yields a distributed program that provides runtime verification of its validity. Such programs are expressed in terms of the field calculus, a recently emerged computational model that focusses on global-level outcomes instead of single-device behaviour, and expresses distributed computations by pure functions and the functional composition mechanism. By reusing previous results and tools of the field calculus, we prove correctness of the translation, self-stabilisation of the derived monitors, and empirically evaluate adaptivity of such monitors in a realistic smart city scenario of safe crowd monitoring and control. © 2021 Elsevier Inc.","Field calculus; Runtime verification; Self-adaptive systems; Spatial logics"
"A deductive reasoning approach for database applications using verification conditions","2021","Journal of Systems and Software","10.1016/j.jss.2020.110903","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099715879&doi=10.1016%2fj.jss.2020.110903&partnerID=40&md5=37ff2ab876d16d9146b212fe81d97b25","Deductive verification has gained paramount attention from both academia and industry. Although intensive research in this direction covers almost all mainstream languages, the research community has paid little attention to the verification of database applications. This paper proposes a comprehensive set of Verification Conditions (VCs) generation techniques from database programs, adapting Symbolic Execution, Conditional Normal Form, and Weakest Precondition. The validity checking of the generated VCs for a database program determines its correctness w.r.t. the annotated database properties. The developed prototype DBverify based on our theoretical foundation allows us to instantiate VC generation from PL/SQL codes, yielding to detailed performance analysis of the three approaches under different circumstances. With respect to the literature, the proposed approach shows its competence to support crucial SQL features (aggregate functions, nested queries, NULL values, and set operations) and the embedding of SQL codes within a host imperative language. For the chosen set of benchmark PL/SQL codes annotated with relevant properties of interest, our experiment shows that only 38% of procedures are correct, while 62% violate either all or part of the annotated properties. The primary cause for the latter case is mostly due to the acceptance of runtime inputs in SQL statements without proper checking. © 2021 Elsevier Inc.","Database languages; Deductive reasoning; Formal verification; Verification conditions"
"Signal-Based Properties of Cyber-Physical Systems: Taxonomy and Logic-based Characterization","2021","Journal of Systems and Software","10.1016/j.jss.2020.110881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098205883&doi=10.1016%2fj.jss.2020.110881&partnerID=40&md5=26374033f2392a71c62aafdcb049ec16","The behavior of a cyber-physical system (CPS) is usually defined in terms of the input and output signals processed by sensors and actuators. Requirements specifications of CPSs are typically expressed using signal-based temporal properties. Expressing such requirements is challenging, because of (1) the many features that can be used to characterize a signal behavior; (2) the broad variation in expressiveness of the specification languages (i.e., temporal logics) used for defining signal-based temporal properties. Thus, system and software engineers need effective guidance on selecting appropriate signal behavior types and an adequate specification language, based on the type of requirements they have to define. In this paper, we present a taxonomy of the various types of signal-based properties and provide, for each type, a comprehensive and detailed description as well as a formalization in a temporal logic. Furthermore, we review the expressiveness of state-of-the-art signal-based temporal logics in terms of the property types identified in the taxonomy. Moreover, we report on the application of our taxonomy to classify the requirements specifications of an industrial case study in the aerospace domain, in order to assess the feasibility of using the property types included in our taxonomy and the completeness of the latter. © 2020 Elsevier Inc.","Signal-based properties; Signals; Taxonomy; Temporal logic"
"A Test Restoration Method based on Genetic Algorithm for effective fault localization in multiple-fault programs","2021","Journal of Systems and Software","10.1016/j.jss.2020.110861","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096192360&doi=10.1016%2fj.jss.2020.110861&partnerID=40&md5=8bc06d7445127f51fd89fda85bbd4c8c","Automatic fault localization is essential for software engineering. However, fault localization suffers from the interactions among multiple faults. Our previous research revealed that the fault-coupling effect is responsible for the weakened fault localization performance in multiple-fault programs. On the basis of this finding, we propose a Test Case Restoration Method based on the Genetic Algorithm (TRGA) to search potential coupling test cases and conduct a restoration process for eliminating the coupling effect. The major contributions of the current study are as follows: (1) the construction of a fitness function to measure the possibility of failed test cases becoming coupling test cases; (2) the development of a TRGA that searches potential coupling test cases; (3) and an evaluation of the TRGA efficiency across 14 open-source programs, three spectrum-based fault localizations, and two parallel debugging techniques. The results revealed the TRGA outperformed the original fault localization techniques in 74.28% and 78.57% of the scenarios in the best and worst cases, respectively. On average, the percentage improvement was 4.43% for the best case and 2% for the worst case. A detailed discussion of TRGA parameter settings is also provided. © 2020 Elsevier Inc.","Evolution algorithm; Fault localization; Multiple-faults; Software debugging; Test restoration"
"Studying test-driven development and its retainment over a six-month time span","2021","Journal of Systems and Software","10.1016/j.jss.2021.110937","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102398803&doi=10.1016%2fj.jss.2021.110937&partnerID=40&md5=ee0d29d5ffb46ac8d56ac16ead0332cb","In this paper, we investigate the effect of TDD, as compared to a non-TDD approach, as well as its retainment (or retention) over a time span of (about) six months. To pursue these objectives, we conducted a (quantitative) longitudinal cohort study with 30 novice developers (i.e., third-year undergraduate students in Computer Science). We observed that TDD affects neither the external quality of software products nor developers’ productivity. However, we observed that the participants applying TDD produced significantly more tests, with a higher fault-detection capability, than those using a non-TDD approach. As for the retainment of TDD, we found that TDD is retained by novice developers for at least six months. © 2021 Elsevier Inc.","Longitudinal cohort study; TDD; Test-driven development"
"Reliability analysis of dynamic fault trees with spare gates using conditional binary decision diagrams","2020","Journal of Systems and Software","10.1016/j.jss.2020.110766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090421097&doi=10.1016%2fj.jss.2020.110766&partnerID=40&md5=ace2976c87833a2116a1fca71253a10d","Dynamic fault trees (DFTs) with spare gates have been used extensively in reliability analysis. The traditional approach to DFTs is Markov-based that may suffer from problems like state–space explosion. Algebraic-structure-based methods consume long computation time caused by the inclusive/exclusive formula. Recently, some combinatorial solutions have been applied to DFTs such as sequential binary decision diagrams (SBDD) and algebraic binary decision diagrams (ABDD). They analyze systems by the minimal cut sequence (MCQ) based on sequence-dependence. We propose an analytical method based on conditional binary decision diagrams (CBDD) for combinatorial reliability analysis of non-repairable DFTs with spare gates. A detectable component state is mined to describe the sequence-dependent failure behaviors between components in the spare gate. Minimal cut set (MCS) instead of MCQ is used for qualitative analysis to locate faults via the component state. Compared to Markov-based methods, our method can generate system reliability result with any arbitrary time-to-failure distribution for system components. Different from SBDD and ABDD, specific operation rules are proposed to eliminate inconsistencies and reduce redundancies when building a CBDD. For quantitative analysis, the CBDD simplifies computation via using the sum of disjoint products. Case studies are presented to show the advantage of using our method. © 2020","Conditional binary decision diagram (CBDD); Conditional state transformation; Dependable computing; Dynamic fault tree; System reliability"
"On the impact of release policies on bug handling activity: A case study of Eclipse","2021","Journal of Systems and Software","10.1016/j.jss.2020.110882","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097583445&doi=10.1016%2fj.jss.2020.110882&partnerID=40&md5=8e02b7bea070608baf4aa34fdbbac693","Large software projects follow a continuous development process with regular releases during which bugs are handled. In recent years, many software projects shifted to rapid releases that reduce time-to-market and claim a faster delivery of fixed issues, but also have a shorter period to address bugs. To better understand the impact of rapid releases on bug handling activity, we empirically analyze successive releases of the Eclipse Core projects, focusing on the bug handling rates and durations as well as the feature freeze period. We study the impact of Eclipse's transition from a yearly to quarterly release cycle. We confirm our findings through feedback received from five Eclipse Core maintainers. Among others, our results reveal that Eclipse's bug handling process is becoming more stable over time, with a decreasing number of reported bugs before releases, an increasing bug fixing rate and an increasingly balanced bug handling workload before and after releases. The transition to a quarterly release cycle continued to improve bug handling. In addition, more effort is spent on bug fixing during the feature freeze period, while the bug handling rates do not differ between both periods. © 2020 Elsevier Inc.","Bug handling process; Continuous software development; Empirical software engineering; Feature freeze; Rapid release cycle; Software maintenance"
"Market-oriented online bi-objective service scheduling for pleasingly parallel jobs with variable resources in cloud environments","2021","Journal of Systems and Software","10.1016/j.jss.2021.110934","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102119750&doi=10.1016%2fj.jss.2021.110934&partnerID=40&md5=80185b67e4974a0e0d13918ddf8aca80","In this paper, we study the market-oriented online bi-objective service scheduling problem for pleasingly parallel jobs with variable resources in cloud environments, from the perspective of SaaS (Software-as-as-Service) providers who provide job-execution services. The main process of scheduling SaaS services in clouds is: a SaaS provider purchases cloud instances from IaaS providers to schedule end users’ jobs and charges users accordingly. This problem has several particular features, such as the job-oriented end users, the pleasingly parallel jobs with soft deadline constraints, the online settings, and the variable numbers of resources. For maximizing both the revenue and the user satisfaction rate, we design an online algorithm for SaaS providers to optimally purchase IaaS instances and schedule pleasingly parallel jobs. The proposed algorithm can achieve competitive objectives in polynomial run-time. The theoretical analysis and simulations based on real-world Google job traces as well as synthetic datasets validate the effectiveness and efficiency of our algorithm. © 2021 Elsevier Inc.","Cloud computing; Multi-objective optimization; Online algorithm; Pleasingly parallel jobs; Service scheduling"
"Software reuse cuts both ways: An empirical analysis of its relationship with security vulnerabilities","2021","Journal of Systems and Software","10.1016/j.jss.2020.110653","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086152747&doi=10.1016%2fj.jss.2020.110653&partnerID=40&md5=53dc2f17e80399521ee656e92b24a5d8","Software reuse is a widely adopted practice among both researchers and practitioners. The relation between security and reuse can go both ways: a system can become more secure by relying on mature dependencies, or more insecure by exposing a larger attack surface via exploitable dependencies. To follow up on a previous study and shed more light on this subject, we further examine the association between software reuse and security threats. In particular, we empirically investigate 1244 open-source projects in a multiple-case study to explore and discuss the distribution of security vulnerabilities between the code created by a development team and the code reused through dependencies. For that, we consider both potential vulnerabilities, as assessed through static analysis, and disclosed vulnerabilities, reported in public databases. The results suggest that larger projects in size are associated with an increase on the amount of potential vulnerabilities in both native and reused code. Moreover, we found a strong correlation between a higher number of dependencies and vulnerabilities. Based on our empirical investigation, it appears that source code reuse is neither a silver bullet to combat vulnerabilities nor a frightening werewolf that entail an excessive number of them. © 2020","Case study; Open-source software; Security vulnerabilities; Software reuse"
"Managing latency in edge–cloud environment","2021","Journal of Systems and Software","10.1016/j.jss.2020.110872","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096825954&doi=10.1016%2fj.jss.2020.110872&partnerID=40&md5=6e05d438da33904c54801b5e72da890a","Modern Cyber–physical Systems (CPS) include applications like smart traffic, smart agriculture, smart power grid, etc. Commonly, these systems are distributed and composed of end-user applications and microservices that typically run in the cloud. The connection with the physical world, which is inherent to CPS, brings the need to operate and respond in real-time. As the cloud becomes part of the computation loop, the real-time requirements have to be also reflected by the cloud. In this paper, we present an approach that provides soft real-time guarantees on the response time of services running in cloud and edge–cloud (i.e., cloud geographically close to the end-user), where these services are developed in high-level programming languages. In particular, we elaborate a method that allows us to predict the upper bound of the response time of a service when sharing the same computer with other services. Importantly, as our approach focuses on minimizing the impact on the developer of such services, it does not require any special programming model nor limits usage of common libraries, etc. © 2020 Elsevier Inc.","Cyber–physical systems; edge–cloud; Guaranteed latency"
"Interdisciplinary effects of technical debt in companies with mechatronic products — a qualitative study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110809","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090566692&doi=10.1016%2fj.jss.2020.110809&partnerID=40&md5=726a5e341f7b7d810b22d1c45f9db7cc","Digitalization of products and production systems requires a fusion of mechatronic disciplines, where interfaces between mechanical, electrical, and software engineering are inevitable. The increasingly rapid pace of innovations in mechatronic systems triggers decisions being taken under time and cost pressure. At times, compromises in technical solutions are made, neglecting their long-term damage to the system. Technical debt (TD), a concept from software engineering, refers to short-term benefits that lead to long-term negative consequences, e.g., in the form of more difficult maintainability or evolvability. This also applies to mechatronic systems, yet the knowledge of TD characteristics and correlations in the interdisciplinary life cycle has only received little attention. This first comprehensive survey investigates TD in mechatronics systematically and across sectors. 50 experts, of whom 42% hold positions as department heads, from 21 renowned companies and 10 sectors in the German-speaking region supported this study with real scenarios where TD caused damage to their system. 94 informative TD incidents that were classified into twelve TD types were recorded, of which 2/3 have not yet been eliminated and posed a potential risk to the system. TD emerges most frequently in the first three stages of the life cycle, where the consequences rarely remain isolated at their source but are forwarded to later phases and disciplines in the life cycle. In contrast to the research focus in software engineering, the multi-domain analysis of mechatronic TD issues reveals that software engineers are most burdened by Requirements TD and Infrastructure TD in the interdisciplinary environment. © 2020","Life cycle; Mechatronic product; Mechatronics; TDinMechatronics; Technical debt"
"An empirical study of optimization bugs in GCC and LLVM","2021","Journal of Systems and Software","10.1016/j.jss.2020.110884","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099207145&doi=10.1016%2fj.jss.2020.110884&partnerID=40&md5=f10f0ef2f34271ba5879c849f7a377ac","Optimizations are the fundamental component of compilers. Bugs in optimizations have significant impacts, and can cause unintended application behavior and disasters, especially for safety-critical domains. Thus, an in-depth analysis of optimization bugs should be conducted to help developers understand and test the optimizations in compilers. To this end, we conduct an empirical study to investigate the characteristics of optimization bugs in two mainstream compilers, GCC and LLVM. We collect about 57K and 22K bugs of GCC and LLVM, and then exhaustively examine 8,771 and 1,564 optimization bugs of the two compilers, respectively. The results reveal the following five characteristics of optimization bugs: (1) Optimizations are the buggiest component in both compilers except for the C++ component; (2) the value range propagation optimization and the instruction combine optimization are the buggiest optimizations in GCC and LLVM, respectively; the loop optimizations in both GCC and LLVM are more bug-prone than other optimizations; (3) most of the optimization bugs in both GCC and LLVM are misoptimization bugs, accounting for 57.21% and 61.38% respectively; (4) on average, the optimization bugs live over five months, and developers take 11.16 months for GCC and 13.55 months for LLVM to fix an optimization bug; in both GCC and LLVM, many confirmed optimization bugs have lived for a long time; (5) the bug fixes of optimization bugs involve no more than two files and three functions on average in both compilers, and around 99% of them modify no more than 100 lines of code, while 90% less than 50 lines of code. Our study provides a deep understanding of optimization bugs for developers and researchers. This could provide useful guidance for the developers and researchers to better design the optimizations in compilers. In addition, the analysis results suggest that we need more effective techniques and tools to test compiler optimizations. Moreover, our findings are also useful to the research of automatic debugging techniques for compilers, such as automatic compiler bug isolation techniques. © 2020 Elsevier Inc.","Bug characteristics; Compiler optimization bugs; Compiler reliability; Compiler testing; Empirical study"
"A taxonomy of service identification approaches for legacy software systems modernization","2021","Journal of Systems and Software","10.1016/j.jss.2020.110868","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097329558&doi=10.1016%2fj.jss.2020.110868&partnerID=40&md5=a51ff14a21a115a6125fe1dcfc28f257","The success of modernizing legacy software systems to Service-Oriented Architecture (SOA) depends on Service Identification Approaches (SIAs), which identify reusable functionalities that could become services. The literature describes several SIAs. However, the selection of an identification approach that is suitable for a practitioner is difficult because it depends on several factors, including the goal of modernization, the available legacy artifacts, the organization's development process, the desired output, and the usability of the approach. Accordingly, to select a suitable service identification approach, a practitioner must have a comprehensive view of existing techniques. We report a systematic literature review (SLR) that covers 41 SIAs based on software-systems analyses. Based on this SLR, we create a taxonomy of SIAs and build a multi-layer classification of existing identification approaches. We start from a high-level classification based on the used inputs, the applied processes, the given outputs, and the usability of the SIAs. We then divide each category into a fine-grained taxonomy that helps practitioners in selecting a suitable approach for identifying services in legacy software systems. We build our SLR based on our experience with legacy software modernization, on discussions and experiences working with industrial partners, and analyses of existing SIAs. We validate the correctness and the coverage of our review with industrial experts who modernize(d) legacy software systems to SOA. The results show that our classification conforms to the industrial experts’ experiences. We also show that most of the studied SIAs are still at their infancy. Finally, we identify the main challenges that SIAs need to address, to improve their quality. © 2020 Elsevier Inc.","Legacy system; Microservices; Migration; Service identification; Taxonomy"
"How are issue reports discussed in Gitter chat rooms?","2021","Journal of Systems and Software","10.1016/j.jss.2020.110852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095914658&doi=10.1016%2fj.jss.2020.110852&partnerID=40&md5=97eb6e917653bf96faf53bbc3840dcbc","Informal communication channels like mailing lists, IRC and instant messaging play a vital role in open source software development by facilitating communication within geographically diverse project teams e.g., to discuss issue reports to facilitate the bug-fixing process. More recently, chat systems like Slack and Gitter have gained a lot of popularity and developers are rapidly adopting them. Gitter is a chat system that is specifically designed to address the needs of GitHub users. Gitter hosts project-based asynchronous chats which foster frequent project discussions among participants. Developer discussions contain a wealth of information such as the rationale behind decisions made during the evolution of a project. In this study, we explore 24 open source project chat rooms that are hosted on Gitter, containing a total of 3,133,106 messages and 14,096 issue references. We manually analyze the contents of chat room discussions around 457 issue reports. The results of our study show the prevalence of issue discussions on Gitter, and that the discussed issue reports have a longer resolution time than the issue reports that are never brought on Gitter. © 2020 Elsevier Inc.","Developer discussions; Gitter; Issue reports"
"Dynamic partitioned scheduling of real-time tasks on ARM big.LITTLE architectures","2021","Journal of Systems and Software","10.1016/j.jss.2020.110886","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097883692&doi=10.1016%2fj.jss.2020.110886&partnerID=40&md5=ecca783e025e8ad0bb96e3bd959ebb56","This paper presents Big-LITTLE Constant Bandwidth Server (BL-CBS), a dynamic partitioning approach to schedule real-time task sets in an energy-efficient way on multi-core platforms based on the ARM big.LITTLE architecture. BL-CBS is designed as an on-line and adaptive scheduler, based on a push/pull architecture that is suitable to be incorporated in the current SCHED_DEADLINE code base in the Linux kernel. It employs a greedy heuristic to dynamically partition the real-time tasks among the big and LITTLE cores aiming to minimize the energy consumption and the migrations imposed on the running tasks. The new approach is validated through the open-source RT-Sim simulator, which has been extended integrating an energy model of the ODROID-XU3 board, fitting tightly the power consumption profiles for the big and LITTLE cores of the board. An extensive set of simulations have been run with randomly generated real-time task sets, leading to promising results. © 2020 Elsevier Inc.","ARM big.LITTLE; Energy-efficiency; Heterogeneous multicore processing; Real-time scheduling"
"A comprehensive study of automatic program repair on the QuixBugs benchmark","2021","Journal of Systems and Software","10.1016/j.jss.2020.110825","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091196139&doi=10.1016%2fj.jss.2020.110825&partnerID=40&md5=8e4f1ae937ab07d227ff33d2d7190f86","Automatic program repair papers tend to repeatedly use the same benchmarks. This poses a threat to the external validity of the findings of the program repair research community. In this paper, we perform an empirical study of automatic repair on a benchmark of bugs called QuixBugs, which has been little studied. In this paper, (1) We report on the characteristics of QuixBugs; (2) We study the effectiveness of 10 program repair tools on it; (3) We apply three patch correctness assessment techniques to comprehensively study the presence of overfitting patches in QuixBugs. Our key results are: (1) 16/40 buggy programs in QuixBugs can be repaired with at least a test suite adequate patch; (2) A total of 338 plausible patches are generated on the QuixBugs by the considered tools, and 53.3% of them are overfitting patches according to our manual assessment; (3) The three automated patch correctness assessment techniques, RGTEvosuite, RGTInputSampling and GTInvariants, achieve an accuracy of 98.2%, 80.8% and 58.3% in overfitting detection, respectively. To our knowledge, this is the largest empirical study of automatic repair on QuixBugs, combining both quantitative and qualitative insights. All our empirical results are publicly available on GitHub in order to facilitate future research on automatic program repair. © 2020","Automatic program repair; Bug benchmark; Patch correctness assessment"
"Predicting the emergence of community smells using socio-technical metrics: A machine-learning approach","2021","Journal of Systems and Software","10.1016/j.jss.2020.110847","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093986587&doi=10.1016%2fj.jss.2020.110847&partnerID=40&md5=0a10ae423a46f93ffb1d25251416e0b9","Community smells represent sub-optimal conditions appearing within software development communities (e.g., non-communicating sub-teams, deviant contributors, etc.) that may lead to the emergence of social debt and increase the overall project's cost. Previous work has studied these smells under different perspectives, investigating their nature, diffuseness, and impact on technical aspects of source code. Furthermore, it has been shown that some socio-technical metrics like, for instance, the well-known socio-technical congruence, can potentially be employed to foresee their appearance. Yet, there is still a lack of knowledge of the actual predictive power of such socio-technical metrics. In this paper, we aim at tackling this problem by empirically investigating (i) the potential value of socio-technical metrics as predictors of community smells and (ii) what is the performance of within- and cross-project community smell prediction models based on socio-technical metrics. To this aim, we exploit a dataset composed of 60 open-source projects and consider four community smells such as ORGANIZATIONAL SILO, BLACK CLOUD, LONE WOLF, and BOTTLENECK. The key results of our work report that a within-project solution can reach F-Measure and AUC-ROC of 77% and 78%, respectively, while cross-project models still require improvements, being however able to reach an F-Measure of 62% and overcome a random baseline. Among the metrics investigated, socio-technical congruence, communicability, and turnover-related metrics are the most powerful predictors of the emergence of community smells. © 2020","Community smells; Empirical software engineering; Social debt"
"Transforming abstract to concrete repairs with a generative approach of repair values","2021","Journal of Systems and Software","10.1016/j.jss.2020.110889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100545212&doi=10.1016%2fj.jss.2020.110889&partnerID=40&md5=2135d2c2b5c72f9ae265f97f8386110a","Software models, often comprise of interconnected diagrams, change continuously, and developers often fail in keeping these diagrams consistent. Detecting inconsistencies quickly and efficiently is state of the art. However, repairing them is not trivial, because there are typically multiple model elements that need to be repaired, leading to an exponentially growing space of combinations of repair choices. Despite extensive research on consistency checking, existing approaches either provide abstract repairs only (i.e., identifying the model element but failing to describe the change), which is not satisfactory. This paper presents a novel approach that provides concrete repair choices based on values from the inconsistent models. Thus, our approach first retrieves repair values from the model, turn them to repair choices, and groups them based on their effects. This grouping lets our approach explore the repair space in its entirety, providing quick example-like feedback for all possible repairs. Our approach and its tool implementation have been empirically assessed on 10 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with three versioned models shows that our approach identifies useful repair values that developers have chosen. © 2021 Elsevier Inc.","Abstract repair; Concrete repair; Inconsistency repair; Model repair"
"Change impact analysis: A systematic mapping study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098739462&doi=10.1016%2fj.jss.2020.110892&partnerID=40&md5=1584fd52ae471070cd164765df75c67f","Change Impact Analysis (CIA) is the process of exploring the tentative effects of a change in other parts of a system. CIA is considered beneficial in practice, since it reduces cost of maintenance and the risk of software development failures. In this paper, we present a systematic mapping study that covers a plethora of CIA methods (by exploring 111 papers), putting special emphasis on how the CIA phenomenon can be quantified: to be efficiently managed. The results of our study suggest that: (a) the practical benefits of CIA cover any type of maintenance request (e.g., feature additions, bug fixing) and can help in reducing relevant cost; (b) CIA quantification relies on four parameters (instability, amount of change, change proneness, and changeability), whose assessment is supported by various metrics and predictors; and (c) in this vast research field, there are still some viewpoints that remain unexplored (e.g., the negative consequences of highly change prone artifacts), whereas others are over-researched (e.g., quantification of instability based on metrics). Based on our results, we provide: (a) useful information for practitioners—i.e., the expected benefits of CIA, and a list of CIA-related metrics, emphasizing on the provision of a detailed interpretation of their relation to CIA; and (b) interesting future research directions—i.e., over- and under-researched sub-fields of CIA. © 2020 Elsevier Inc.","Amount of change; Change impact analysis; Change proneness; Changeability; Instability"
"Studying the Relationship Between the Usage of APIs Discussed in the Crowd and Post-Release Defects","2020","Journal of Systems and Software","10.1016/j.jss.2020.110724","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088222392&doi=10.1016%2fj.jss.2020.110724&partnerID=40&md5=b088ab72295646040f6f852bfc25fe14","Software development nowadays is heavily based on libraries, frameworks and their proposed Application Programming Interfaces (APIs). However, due to challenges such as the complexity and the lack of documentation, these APIs may introduce various obstacles for developers and common defects in software systems. To resolve these issues, developers usually utilize Question and Answer (Q&A) websites such as Stack Overflow by asking their questions and finding proper solutions for their problems on APIs. Therefore, these websites have become inevitable sources of knowledge for developers, which is also known as the crowd knowledge. However, the relation of this knowledge to the software quality has never been adequately explored before. In this paper, we study whether using APIs which are challenging according to the discussions of the Stack Overflow is related to code quality defined in terms of post-release defects. To this purpose, we define the concept of challenge of an API, which denotes how much the API is discussed in high-quality posts on Stack Overflow. Then, using this concept, we propose a set of products and process metrics. We empirically study the statistical correlation between our metrics and post-release defects as well as added explanatory and predictive power to traditional models through a case study on five open source projects including Spring, Elastic Search, Jenkins, K-8 Mail Android Client, and OwnCloud Android client. Our findings reveal that our metrics have a positive correlation with post-release defects which is comparable to known high-performance traditional process metrics, such as code churn and number of pre-release defects. Furthermore, our proposed metrics can provide additional explanatory and predictive power for software quality when added to the models based on existing products and process metrics. Our results suggest that software developers should consider allocating more resources on reviewing and improving external API usages to prevent further defects. © 2020 Elsevier Inc.","API; Crowd knowledge; Defect prediction; Stack overflow"
"An automated model-based approach to repair test suites of evolving web applications","2021","Journal of Systems and Software","10.1016/j.jss.2020.110841","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092503781&doi=10.1016%2fj.jss.2020.110841&partnerID=40&md5=f1bd0029fe28f5b05de9fda2d0af392d","Capture–Replay tools are widely used for the automated testing of web applications The scripts written for these Capture–Replay tools are strongly coupled with the web elements of web applications. These test scripts are sensitive to changes in web elements and require repairs as the web pages evolve. In this paper, we propose an automated model-based approach to repair the Capture–Replay test scripts that are broken due to such changes. Our approach repairs the test scripts that may be broken due to the breakages (e.g., broken locators, missing web elements) reported in the existing test breakage taxonomy. Our approach is based on a DOM-based strategy and is independent of the underlying Capture–Replay tool. We developed a tool to demonstrate the applicability of the approach. We perform an empirical study on seven subject applications. The results show that the approach successfully repairs the broken test scripts while maintaining the same DOM coverage and fault-finding capability. We also evaluate the usefulness of the repaired test scripts according to the opinion of professional testers. We conduct an experiment to compare our approach with the state-of-the-art DOM-based test repair approach, WATER. The comparison results show that our approach repairs more test breakages than WATER. © 2020 Elsevier Inc.","Automated test scripts; Model-based; Regression testing; Web test repair; Web testing"
"Imbalanced metric learning for crashing fault residence prediction","2020","Journal of Systems and Software","10.1016/j.jss.2020.110763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089090799&doi=10.1016%2fj.jss.2020.110763&partnerID=40&md5=85f5323bb00e38638bc201012d39504e","As the software crash usually does great harm, locating the fault causing the crash (i.e., the crashing fault) has always been a hot research topic. As the stack trace in the crash reports usually contains abundant information related the crash, it is helpful to find the root cause of the crash. Recently, researchers extracted features of the crash, then constructed the classification model on the features to predict whether the crashing fault resides in the stack trace. This process can accelerate the debugging process and save debugging efforts. In this work, we apply a state-of-the-art metric learning method called IML to crash data for crashing fault residence prediction. This method uses Mahalanobis distance based metric learning to learn high-quality feature representation by reducing the distance between crash instances with the same label and increasing the distance between crash instances with different labels. In addition, this method designs a new loss function that includes four types of losses with different weights to cope with the class imbalanced issue of crash data. The experiments on seven open source software projects show that our IML method performs significantly better than nine sampling based and five ensemble based imbalanced learning methods in terms of three performance indicators. © 2020","Class imbalanced learning; Crashing fault residence prediction; Metric learning; Stack trace"
"Product-line assurance cases from contract-based design","2021","Journal of Systems and Software","10.1016/j.jss.2021.110922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102065578&doi=10.1016%2fj.jss.2021.110922&partnerID=40&md5=8b8f19c9432ac085e9198f0d5ebeed3f","Assurance cases are used to argue in a structured, and evidence-supported way, that a property such as safety or security is satisfied by a system. In some domains however, instead of single systems, product lines with many system-variants are engineered, to satisfy the needs of different customers. In such context, single-system methods for assurance-case creation suffer from scalability issues because the underlying assumption is that the evidence and arguments can be created per system variant. This paper presents a novel method for product-line assurance-case creation where all the arguments and the evidence are created without analyzing each system variant. Consequently, the effort to create an assurance case scales with the complexity of system variants, instead with their number. The method is based on a contract-based design framework for cyber–physical systems, which is extended to define the conditions under which all system variants satisfy a particular property. These conditions are used to define an assurance-case pattern, which can be instantiated for arbitrary product lines. Moreover, the defined pattern is modular to enable step-wise assurance-case creation. Finally, an exploratory case study is performed on a real product-line from the heavy-vehicle manufacturer SCANIA to evaluate the applicability of the presented method. © 2021 Elsevier Inc.","Assurance cases; Contract-based design; Product line engineering"
"Translation from layout-based to visual android test scripts: An empirical evaluation","2021","Journal of Systems and Software","10.1016/j.jss.2020.110845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092452309&doi=10.1016%2fj.jss.2020.110845&partnerID=40&md5=60f3287035ab811acd4dbcfb9819f913","Mobile GUI tests can be classified as layout-based – i.e. using GUI properties as locators – or Visual – i.e. using widgets’ screen captures as locators –. Visual test scripts require significant maintenance efforts to be kept aligned with the tested application as it evolves or it is ported to different devices. This work aims to conceptualize a translation-based approach to automatically derive Visual tests from existing layout-based counterparts or repair them when graphical changes occur, and to develop a tool that implements and validates the approach. We present TOGGLE, a tool that translates Espresso layout-based tests for Android apps to Visual tests that conform to either SikuliX, EyeAutomate, or a combination of the two tools’ syntax. An experiment is conducted to measure the precision of the translation approach, which is evaluated on maintenance tasks triggered by graphical changes due to device diversity. Our results demonstrate the feasibility of a translation-based approach, show that script portability to different devices is improved (from 32% to 93%), and indicate that translation can repair up to 90% of Visual locators in failing tests. GUI test translation mitigates challenges with Visual tests like maintenance effort and portability, enabling their wider use in industrial practice. © 2020 Elsevier Inc.","Empirical software engineering; GUI testing; Mobile testing; Software validation"
"Code smell detection by deep direct-learning and transfer-learning","2021","Journal of Systems and Software","10.1016/j.jss.2021.110936","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102119719&doi=10.1016%2fj.jss.2021.110936&partnerID=40&md5=802a1c364cae9feb102101198284bab7","Context: An excessive number of code smells make a software system hard to evolve and maintain. Machine learning methods, in addition to metric-based and heuristic-based methods, have been recently applied to detect code smells; however, current methods are considered far from mature. Objective: First, explore the feasibility of applying deep learning models to detect smells without extensive feature engineering. Second, investigate the possibility of applying transfer-learning in the context of detecting code smells. Methods: We train smell detection models based on Convolution Neural Networks and Recurrent Neural Networks as their principal hidden layers along with autoencoder models. For the first objective, we perform training and evaluation on C# samples, whereas for the second objective, we train the models from C# code and evaluate the models over Java code samples and vice-versa. Results: We find it feasible to detect smells using deep learning methods though the models’ performance is smell-specific. Our experiments show that transfer-learning is definitely feasible for implementation smells with performance comparable to that of direct-learning. This work opens up a new paradigm to detect code smells by transfer-learning especially for the programming languages where the comprehensive code smell detection tools are not available. © 2021 Elsevier Inc.","Code smells; Deep learning; Smell detection tools; Transfer-learning"
"A Large Scale Analysis of Android — Web Hybridization","2020","Journal of Systems and Software","10.1016/j.jss.2020.110775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089806921&doi=10.1016%2fj.jss.2020.110775&partnerID=40&md5=a6857b5d9f82ce7b17b634b354102a95","Many Android applications embed webpages via WebView components and execute JavaScript code within Android. Hybrid applications leverage dedicated APIs to load a resource and render it in a WebView. Furthermore, Android objects can be shared with the JavaScript world. However, bridging the interfaces of the Android and JavaScript world might also incur severe security threats: Potentially untrusted webpages and their JavaScript might interfere with the Android environment and its access to native features. No general analysis is currently available to assess the implications of such hybrid apps bridging the two worlds. To understand the semantics and effects of hybrid apps, we perform a large-scale study on the usage of the hybridization APIs in the wild. We analyze and categorize the parameters to hybridization APIs for 7,500 randomly selected and the 196 most popular applications from the Google Playstore as well as 1000 malware samples. Our results advance the general understanding of hybrid applications, as well as implications for potential program analyses, and the current security situation: We discovered thousands of flows of sensitive data from Android to JavaScript, the vast majority of which could flow to potentially untrustworthy code. Our analysis identified numerous web pages embedding vulnerabilities, which we exemplarily exploited. Additionally, we discovered a multitude of applications in which potentially untrusted JavaScript code may interfere with (trusted) Android objects, both in benign and malign applications. © 2020 Elsevier Inc.","Android Hybrid Apps; Information Flow Control; Static Analysis"
"Large scale quality transformation in hybrid development organizations – A case study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092034144&doi=10.1016%2fj.jss.2020.110836&partnerID=40&md5=44e0e6c04d328c109864d878d01dbeee","As the software industry transitions to a subscription-based software-as-a-service (SaaS) model, software development companies are transforming to hybrid development organizations with increased adoption of Agile and Continuous Integration/ Continuous Delivery (CI/CD) development practices for newer products while continuing to use Waterfall methods for older products. This transformation is a huge undertaking impacting all aspects of the software development life cycle (SDLC), including the quality management system. This paper presents a case study of a large-scale transformation of a legacy quality management system to a modern system developed and implemented at Cisco Systems. The framework for this transformation is defined by six distinct areas: metrics, process, measurement, reporting, quality analytics, and culture & leadership. Our implementation leveraged recent advances in Machine Learning (ML), Artificial Intelligence (AI), connected data, integrated operations, and big data technologies to solve the challenges created by a hybrid software development organization. We believe this case study will help researchers and industry leaders understand the benefits and potential challenges of such sizeable transformations. © 2020 The Authors","Agile; Hybrid development organization; Quality management system; Quality transformation; Waterfall"
"Security modelling and formal verification of survivability properties: Application to cyber–physical systems","2021","Journal of Systems and Software","10.1016/j.jss.2020.110746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089137320&doi=10.1016%2fj.jss.2020.110746&partnerID=40&md5=a26fd96b3c29619f7fce0fccc8b7e811","The modelling and verification of systems security is an open research topic whose complexity and importance needs, in our view, the use of formal and non-formal methods. This paper addresses the modelling of security using misuse cases and the automatic verification of survivability properties using model checking. The survivability of a system characterises its capacity to fulfil its mission (promptly) in the presence of attacks, failures, or accidents, as defined by Ellison. The original contributions of this paper are a methodology and its tool support, through a framework called surreal. The methodology starts from a misuse case specification enriched with UML profile annotations and obtains, as a by-product, a survivability assessment model (SAM). Using predefined queries the survivability properties are proved in the SAM. A total of fourteen properties have been formulated and also implemented in surreal, which encompasses tools to model the security specification, to create the SAM and to prove the properties. Finally, the paper validates the methodology and the framework using a cyber–physical system (CPS) case study, in the automotive field. © 2020","cyber–physical systems (CPS); Formal verification; Security specification; Survivability properties; UML"
"Function-as-a-Service performance evaluation: A multivocal literature review","2020","Journal of Systems and Software","10.1016/j.jss.2020.110708","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088384495&doi=10.1016%2fj.jss.2020.110708&partnerID=40&md5=f117675ea24528c6c2a76b51a8914637","Function-as-a-Service (FaaS) is one form of the serverless cloud computing paradigm and is defined through FaaS platforms (e.g., AWS Lambda) executing event-triggered code snippets (i.e., functions). Many studies that empirically evaluate the performance of such FaaS platforms have started to appear but we are currently lacking a comprehensive understanding of the overall domain. To address this gap, we conducted a multivocal literature review (MLR) covering 112 studies from academic (51) and grey (61) literature. We find that existing work mainly studies the AWS Lambda platform and focuses on micro-benchmarks using simple functions to measure CPU speed and FaaS platform overhead (i.e., container cold starts). Further, we discover a mismatch between academic and industrial sources on tested platform configurations, find that function triggers remain insufficiently studied, and identify HTTP API gateways and cloud storages as the most used external service integrations. Following existing guidelines on experimentation in cloud systems, we discover many flaws threatening the reproducibility of experiments presented in the surveyed studies. We conclude with a discussion of gaps in literature and highlight methodological suggestions that may serve to improve future FaaS performance evaluation studies. © 2020 The Authors","Benchmarking; Cloud computing; Function-as-a-Service; Multivocal literature review; Performance; Serverless"
"On the diversity and frequency of code related to mathematical formulas in real-world Java projects","2021","Journal of Systems and Software","10.1016/j.jss.2020.110863","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096667195&doi=10.1016%2fj.jss.2020.110863&partnerID=40&md5=cb4be9d7aefc1966da49c4c818cadff0","In this paper, the term formula code refers to fragments of source code that implement a mathematical formula. We present empirical studies that analyze the diversity and frequency of formula code in open-source-software projects. In an exploratory study, we investigated what kinds of formulas are implemented in real-world Java projects and derived syntactical patterns and constraints. We refined these patterns for sum and product formulas to automatically detect formula code in software archives and to reconstruct the implemented formula in mathematical notation. In a quantitative study of a large sample of engineered Java projects on GitHub we analyzed the frequency of formula code and estimated that one of 700 lines of code in this sample implements a sum or product formula. For a sample of scientific-computing projects, we found that one of 100 lines of code implements a sum or product formula. To assess the need for tool support, we investigated the helpfulness of comments for program understanding in a sample of formula-code fragments and performed an online survey. Our findings provide first insights into the characteristics of formula code, that can motivate further studies on the role of formula code in software projects and the design of formula-related tools. © 2020 Elsevier Inc.","Code patterns; Formula code; GitHub; Qualitative study; quantitative study"
"A critical review on the evaluation of automated program repair systems","2021","Journal of Systems and Software","10.1016/j.jss.2020.110817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090822173&doi=10.1016%2fj.jss.2020.110817&partnerID=40&md5=e14dee33eed43f2bfe3f569a5113ce06","Automated Program Repair (APR) has attracted significant attention from software engineering research and practice communities in the last decade. Several teams have recorded promising performance in fixing real bugs and there is a race in the literature to fix as many bugs as possible from established benchmarks. Gradually, repair performance of APR tools in the literature has gone from being evaluated with a metric on the number of generated plausible patches to the number of correct patches. This evolution is necessary after a study highlighting the overfitting issue in test suite-based automatic patch generation. Simultaneously, some researchers are also insisting on providing time cost in the repair scenario as a metric for comparing state-of-the-art systems. In this paper, we discuss how the latest evaluation metrics of APR systems could be biased. Since design decisions (both in approach and evaluation setup) are not always fully disclosed, the impact on repair performance is unknown and computed metrics are often misleading. To reduce notable biases of design decisions in program repair approaches, we conduct a critical review on the evaluation of patch generation systems and propose eight evaluation metrics for fairly assessing the performance of APR tools. Eventually, we show with experimental data on 11 baseline program repair systems that the proposed metrics allow to highlight some caveats in the literature. We expect wide adoption of these metrics in the community to contribute to boosting the development of practical, and reliably performable program repair tools. © 2020 Elsevier Inc.","Assessment; Automated program repair; Evaluation; Metrics"
"Multilayered review of safety approaches for machine learning-based systems in the days of AI","2021","Journal of Systems and Software","10.1016/j.jss.2021.110941","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102578152&doi=10.1016%2fj.jss.2021.110941&partnerID=40&md5=dcbc0dc703f07e37f7f5fecc315e151d","The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety. © 2021 Elsevier Inc.","Autonomous systems; Intelligent software systems; Machine learning; Safety analysis; Software engineering"
"How to kill them all: An exploratory study on the impact of code observability on mutation testing","2021","Journal of Systems and Software","10.1016/j.jss.2020.110864","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097713377&doi=10.1016%2fj.jss.2020.110864&partnerID=40&md5=02048d3581e481714e5b829e0786f6d4","Mutation testing is well-known for its efficacy in assessing test quality, and starting to be applied in the industry. However, what should a developer do when confronted with a low mutation score? Should the test suite be plainly reinforced to increase the mutation score, or should the production code be improved as well, to make the creation of better tests possible? In this paper, we aim to provide a new perspective to developers that enables them to understand and reason about the mutation score in the light of testability and observability. First, we investigate whether testability and observability metrics are correlated with the mutation score on six open-source Java projects. We observe a correlation between observability metrics and the mutation score, e.g., test directness, which measures the extent to which the production code is tested directly, seems to be an essential factor. Based on our insights from the correlation study, we propose a number of ”mutation score anti-patterns”, enabling software engineers to refactor their existing code or add tests to improve the mutation score. In doing so, we observe that relatively simple refactoring operations enable an improvement or increase in the mutation score. © 2020 Elsevier Inc.","Code quality; Code refactoring; Mutation testing; Observability; Testability"
"The role of the project manager in agile software development projects","2021","Journal of Systems and Software","10.1016/j.jss.2020.110871","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097152652&doi=10.1016%2fj.jss.2020.110871&partnerID=40&md5=fa206d972fcf08641f98a55f4d0801f1","Agile teams are not meant to have project managers. Instead, agile methods such as Scrum and XP define roles such as product owner, scrum master, and coach. Studies have uncovered the existence of the project manager in agile projects, pointing to disconnect between theory and practice. To address this gap, a Grounded Theory study with a mixed methods approach was conducted using multiple sources of data including over 45 h of interviews with 39 software practitioners and quantitative data from 57 questionnaire respondents. We present and describe the project manager's role in agile projects in terms of (a) everyday activities: facilitating, mentoring, negotiating, coordinating, and protecting, performed by the project manager using; (b) three management approaches: hard, moderate, and soft; (c) four traditional project management activities continued to be performed by them, including: tracking project progress, reporting on project status, budgeting and forecasting, and managing personnel; and (d) the influence of the presence of the project manager on the frequency with which agile activities are carried out by the teams. Our study highlights the continued presence of the role of the project manager in agile software projects as a part of the transition from traditional to agile ways of working. © 2020 Elsevier Inc.","Agile project management; Agile software development; Project manager; Scrum"
"ACCORDANT: A domain specific-model and DevOps approach for big data analytics architectures","2021","Journal of Systems and Software","10.1016/j.jss.2020.110869","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096655894&doi=10.1016%2fj.jss.2020.110869&partnerID=40&md5=687fc60650be4b4cc588cda440c56270","Big data analytics (BDA) applications use machine learning algorithms to extract valuable insights from large, fast, and heterogeneous data sources. New software engineering challenges for BDA applications include ensuring performance levels of data-driven algorithms even in the presence of large data volume, velocity, and variety (3Vs). BDA software complexity frequently leads to delayed deployments, longer development cycles, and challenging performance assessment. This paper proposes a Domain-Specific Model (DSM), and DevOps practices to design, deploy, and monitor performance metrics in BDA applications. Our proposal includes a design process, and a framework to define architectural inputs, software components, and deployment strategies through integrated high-level abstractions to enable QS monitoring. We evaluate our approach with four use cases from different domains to demonstrate a high level of generalization. Our results show a shorter deployment and monitoring times, and a higher gain factor per iteration compared to similar approaches. © 2020 Elsevier Inc.","Big data analytics deployment; DevOps; Domain-specific model; Performance monitoring; Quality scenarios; Software architecture"
"CommtPst: Deep learning source code for commenting positions prediction","2020","Journal of Systems and Software","10.1016/j.jss.2020.110754","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088924536&doi=10.1016%2fj.jss.2020.110754&partnerID=40&md5=bafd6729f307832361257c34cbdf8685","Existing techniques for automatic code commenting assume that the code snippet to be commented has been identified, thus requiring users to provide the code snippet in advance. A smarter commenting approach is desired to first self-determine where to comment in a given source code and then generate comments for the code snippets that need comments. To achieve the first step of this goal, we propose a novel method, CommtPst, to automatically find the appropriate commenting positions in the source code. Since commenting is closely related to the code syntax and semantics, we adopt neural language model (word embeddings) to capture the code semantic information, and analyze the abstract syntax trees to capture code syntactic information. Then, we employ LSTM (long short term memory) to model the long-term logical dependency of code statements over the fused semantic and syntactic information and learn the commenting patterns on the code sequence. We evaluated CommtPst using large data sets from dozens of open-source software systems in GitHub. The experimental results show that the precision, recall and F-Measure values achieved by CommtPst are 0.792, 0.602 and 0.684, respectively, which outperforms the traditional machine learning method with 11.4% improvement on F-measure. © 2020","Code semantics; Code syntax; Comment generation; Comment position; LSTM"
"Concept drift-aware temporal cloud service APIs recommendation for building composite cloud systems","2021","Journal of Systems and Software","10.1016/j.jss.2020.110902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099198559&doi=10.1016%2fj.jss.2020.110902&partnerID=40&md5=53256e07016c7b3e27f1a78be1e20811","The booming advances of cloud computing promote rapid growth of the number of cloud service Application Program Interfaces (APIs) published at the large-scale software cloud markets. Cloud service APIs recommendation remains a challenging issue for a composite cloud system construction, due to massively available candidate component cloud services with similar (or identical) functionalities in the cloud markets. As for a specific user, the probability distribution of the data indicating his/her preferences to the cloud service APIs may change with time, resulting in concept drifting preferences. To adapt users’ preference drifts and provide effective recommendation results to composite cloud system developers, we propose a concept drift-aware temporal cloud service APIs recommendation approach for composite cloud systems (or CD-APIR) in this paper. First, we track users temporal preferences through users’ behavior-aware information analysis. Second, we utilize Singular Value Decomposition (SVD) method to predict the missing values in the user–service matrices. Third, we identify the degree of users preference drifts by Jensen–Shannon (or JS) divergence. Finally, we recommend cloud service APIs by presenting a piecewise trading-off equation. Experimental evaluations conducted on WS-Dream dataset demonstrate that the CD-APIR approach can effectively improve the accuracy of cloud service APIs recommendation comparing with 7 representative approaches. © 2021 Elsevier Inc.","Application Programming Interfaces (APIs); Cloud service recommendation; Preference drift; Temporal recommendation; User behavior; User preference"
"Toward the automatic classification of Self-Affirmed Refactoring","2021","Journal of Systems and Software","10.1016/j.jss.2020.110821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091343972&doi=10.1016%2fj.jss.2020.110821&partnerID=40&md5=6eefc9d5e9eeaf22a131d74e44fe6740","The concept of Self-Affirmed Refactoring (SAR) was introduced to explore how developers document their refactoring activities in commit messages, i.e., developers explicit documentation of refactoring operations intentionally introduced during a code change. In our previous study, we have manually identified refactoring patterns and defined three main common quality improvement categories including internal quality attributes, external quality attributes, and code smells, by only considering refactoring-related commits. However, this approach heavily depends on the manual inspection of commit messages. In this paper, we propose a two-step approach to first identify whether a commit describes developer-related refactoring events, then to classify it according to the refactoring common quality improvement categories. Specifically, we combine the N-Gram TF–IDF feature selection with binary and multiclass classifiers to build a new model to automate the classification of refactorings based on their quality improvement categories. We challenge our model using a total of 2,867 commit messages extracted from well engineered open-source Java projects. Our findings show that (1) our model is able to accurately classify SAR commits, outperforming the pattern-based and random classifier approaches, and allowing the discovery of 40 more relevant SAR patterns, and (2) our model reaches an F-measure of up to 90% even with a relatively small training dataset. © 2020 Elsevier Inc.","Commit classification; Machine learning; Refactoring; Self-affirmed Refactoring"
"Efficient and effective exploratory testing of large-scale software systems","2021","Journal of Systems and Software","10.1016/j.jss.2020.110890","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099024535&doi=10.1016%2fj.jss.2020.110890&partnerID=40&md5=45657b15e012c33f385912b765258753","Context: Exploratory testing plays an important role in the continuous integration and delivery pipelines of large-scale software systems, but a holistic and structured approach is needed to realize efficient and effective exploratory testing. Objective: This paper seeks to address the need for a structured and reliable approach by providing a tangible model, supporting practitioners in the industry to optimize exploratory testing in each individual case. Method: The reported study includes interviews, group interviews and workshops with representatives from six companies, all multi-national organizations with more than 2,000 employees. Results: The ExET model (Excellence in Exploratory Testing) is presented. It is shown that the ExET model allows companies to identify and visualize strengths and improvement areas. The model is based on a set of key factors that have been shown to enable efficient and effective exploratory testing of large-scale software systems, grouped into four themes: “The testers’ knowledge, experience and personality”, “Purpose and scope”, “Ways of working” and “Recording and reporting”. Conclusions: The validation of the ExET model showed that the model is novel, actionable and useful in practice, showing companies what they should prioritize in order to enable efficient and effective exploratory testing in their organization. © 2021 Elsevier Inc.","Continuous delivery; Continuous integration; Exploratory testing; Large-scale systems; Software testing"
"Implementation relations and testing for cyclic systems with refusals and discrete time","2020","Journal of Systems and Software","10.1016/j.jss.2020.110738","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087940377&doi=10.1016%2fj.jss.2020.110738&partnerID=40&md5=614afca7a9d5cd670708b3d1fe4f90e1","We present a formalism to represent cyclic models and study different semantic frameworks that support testing. These models combine sequences of observable actions and the passing of (discrete) time and can be used to specify a number of classes of reactive systems, an example being robotic systems. We use implementation relations in order to formally define a notion of correctness of a system under test (SUT) with respect to a specification. As usual, the aim is to devise an extension of the classical ioco implementation relation but available timed variants of ioco are not suitable for cyclic models. This paper thus defines new implementation relations that encapsulate the discrete nature of time and take into account not only the actions that models can perform but also the ones that they can refuse. In addition to defining these relations, we study a number of their properties and provide alternative characterisations, showing that the relations are appropriate conservative extensions of trace containment. Finally, we give test derivation algorithms and prove that they are sound and also are complete in the limit. © 2020 Elsevier Inc.","Cyclic systems; Implementation relations; Model-based testing"
"A ground-truth dataset and classification model for detecting bots in GitHub issue and PR comments","2021","Journal of Systems and Software","10.1016/j.jss.2021.110911","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099784243&doi=10.1016%2fj.jss.2021.110911&partnerID=40&md5=15097e5f702a5c909b7e9d08e865259e","Bots are frequently used in Github repositories to automate repetitive activities that are part of the distributed software development process. They communicate with human actors through comments. While detecting their presence is important for many reasons, no large and representative ground-truth dataset is available, nor are classification models to detect and validate bots on the basis of such a dataset. This paper proposes a ground-truth dataset, based on a manual analysis with high interrater agreement, of pull request and issue comments in 5,000 distinct Github accounts of which 527 have been identified as bots. Using this dataset we propose an automated classification model to detect bots, taking as main features the number of empty and non-empty comments of each account, the number of comment patterns, and the inequality between comments within comment patterns. We obtained a very high weighted average precision, recall and F1-score of 0.98 on a test set containing 40% of the data. We integrated the classification model into an open source command-line tool to allow practitioners to detect which accounts in a given Github repository actually correspond to bots. © 2021 Elsevier Inc.","Bot identification; Classification model; Distributed software development; GitHub repositories; Text similarity"
"Software engineering practices for scientific software development: A systematic mapping study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110848","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094318088&doi=10.1016%2fj.jss.2020.110848&partnerID=40&md5=7b30312b4af7a960072fdb8350275947","Background: The development of scientific software applications is far from trivial, due to the constant increase in the necessary complexity of these applications, their increasing size, and their need for intensive maintenance and reuse. Aim: To this end, developers of scientific software (who usually lack a formal computer science background) need to use appropriate software engineering (SE) practices. This paper describes the results of a systematic mapping study on the use of SE for scientific application development and their impact on software quality. Method: To achieve this goal we have performed a systematic mapping study on 359 papers. We first describe a catalog of SE practices used in scientific software development. Then, we discuss the quality attributes of interest that drive the application of these practices, as well as tentative side-effects of applying the practices on qualities. Results: The main findings indicate that scientific software developers are focusing on practices that improve implementation productivity, such as code reuse, use of third-party libraries, and the application of “good” programming techniques. In addition, apart from the finding that performance is a key-driver for many of these applications, scientific software developers also find maintainability and productivity to be important. Conclusions: The results of the study are compared to existing literature, are interpreted under a software engineering prism, and various implications for researchers and practitioners are provided. One of the key findings of the study, which is considered as important for driving future research endeavors is the lack of evidence on the trade-offs that need to be made when applying a software practice, i.e., negative (indirect) effects on other quality attributes. © 2020 Elsevier Inc.","High performance computing; Scientific computing; Software engineering practices"
"GEML: A grammar-based evolutionary machine learning approach for design-pattern detection","2021","Journal of Systems and Software","10.1016/j.jss.2021.110919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100050995&doi=10.1016%2fj.jss.2021.110919&partnerID=40&md5=19d83bf02c1dfee1f3061db0f4aa49f2","Design patterns (DPs) are recognised as a good practice in software development. However, the lack of appropriate documentation often hampers traceability, and their benefits are blurred among thousands of lines of code. Automatic methods for DP detection have become relevant but are usually based on the rigid analysis of either software metrics or specific properties of the source code. We propose GEML, a novel detection approach based on evolutionary machine learning using software properties of diverse nature. Firstly, GEML makes use of an evolutionary algorithm to extract those characteristics that better describe the DP, formulated in terms of human-readable rules, whose syntax is conformant with a context-free grammar. Secondly, a rule-based classifier is built to predict whether new code contains a hidden DP implementation. GEML has been validated over five DPs taken from a public repository recurrently adopted by machine learning studies. Then, we increase this number up to 15 diverse DPs, showing its effectiveness and robustness in terms of detection capability. An initial parameter study served to tune a parameter setup whose performance guarantees the general applicability of this approach without the need to adjust complex parameters to a specific pattern. Finally, a demonstration tool is also provided. © 2021 Elsevier Inc.","Associative classification; Design pattern detection; Grammar-guided genetic programming; Machine learning; Reverse engineering"
"Requirements engineering challenges and practices in large-scale agile system development","2021","Journal of Systems and Software","10.1016/j.jss.2020.110851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096169394&doi=10.1016%2fj.jss.2020.110851&partnerID=40&md5=0f6da7c96c403d6680225aac78da128a","Context: Agile methods have become mainstream even in large-scale systems engineering companies that need to accommodate different development cycles of hardware and software. For such companies, requirements engineering is an essential activity that involves upfront and detailed analysis which can be at odds with agile development methods. Objective: This paper presents a multiple case study with seven large-scale systems companies, reporting their challenges, together with best practices from industry. We also analyze literature about two popular large-scale agile frameworks, SAFe® and LeSS, to derive potential solutions for the challenges. Methods: Our results are based on 20 qualitative interviews, five focus groups, and eight cross-company workshops which we used to both collect and validate our results. Results: We found 24 challenges which we grouped in six themes, then mapped to solutions from SAFe®, LeSS, and our companies, when available. Conclusion: In this way, we contribute a comprehensive overview of RE challenges in relation to large-scale agile system development, evaluate the degree to which they have been addressed, and outline research gaps. We expect these results to be useful for practitioners who are responsible for designing processes, methods, or tools for large scale agile development as well as guidance for researchers. © 2020 The Authors","Large-scale agile; Requirements engineering; Systems engineering"
"Exploring the intersection between software industry and Software Engineering education - A systematic mapping of Software Engineering Trends","2021","Journal of Systems and Software","10.1016/j.jss.2020.110736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089513299&doi=10.1016%2fj.jss.2020.110736&partnerID=40&md5=585de138e3ee7065b0df36e06f9401de","Context: Software has become ubiquitous in every corner of modern societies. During the last five decades, software engineering has also changed significantly to advance the development of various types and scales of software products. In this context, Software Engineering Education plays an important role in keeping students updated with software technologies, processes, and practices that are popular in industries. Objective: We investigate from literature the extent Software Engineering Education addresses major Software Engineering Trends in the academic setting. Method: We conducted a systematic mapping study about teaching major Software Engineering Trends in project courses. We classified 126 papers based on their investigated Software Engineering Trends, specifically Software Engineering processes and practices, teaching approaches, and the evolution of Software Engineering Trends over time. Results: We reveal that Agile Software Development is the major trend. The other Trends, i.e., Software Implementation, Usability and Value, Global Software Engineering, and Lean Software Startup, are relatively small in the academic setting, but continuously growing in the last five years. System of Systems is the least investigated among all Trends. Conclusions: The study points out the possible gaps between Software Industry and Education, which implies actionable insights for researchers, educators, and practitioners. © 2020 The Authors","Industry education intersection; Software Engineering Education; Software Engineering Trends; Software industry; Systematic mapping study"
"A security pattern detection framework for building more secure software","2021","Journal of Systems and Software","10.1016/j.jss.2020.110838","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094313784&doi=10.1016%2fj.jss.2020.110838&partnerID=40&md5=4c0f4e4c9fce4c87031fd88c506247d5","Security patterns are one of the reusable building blocks of a secure software architecture that provide solutions to particular recurring security problems in given contexts. Incomplete or nonstandard implementation of security patterns may produce vulnerabilities and invite attackers. Therefore, the detection of security patterns improves the quality of security features. In this paper, we propose a security pattern detection (SPD) framework and its internal pattern matching techniques. The framework provides a platform for data extraction, pattern matching, and semantic analysis techniques. We implement ordered matrix matching (OMM) and non-uniform distributed matrix matching (NDMM) techniques. The OMM technique detects a security pattern matrix inside the target system matrix (TSM). The NDMM technique determines whether the relationships between all classes of a security pattern are similar to the relationships between some classes of the TSM. The semantic analysis is used to reduce the rate of false positives. We evaluate and compare the performance of the proposed SPD framework using both matching techniques based on four case studies independently. The results show that the NDMM technique provides the location of the security patterns, and it is highly flexible, scalable and has high accuracy with acceptable memory and time consumption for large projects. © 2020","Secure architectural design; Security pattern detection technique; Security patterns; Security quality assurance; Software design component"
"Integrating UX work with agile development through user stories: An action research study in a small software company","2020","Journal of Systems and Software","10.1016/j.jss.2020.110785","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089812983&doi=10.1016%2fj.jss.2020.110785&partnerID=40&md5=4df1f4e80c9c92189ddf642cd78ebc08","The integration of user experience (UX) work with agile software development has been addressed in extensive research of challenges and process models. However, in-depth research of context-specific improvements of this integration with actual UX professionals and agile developers in their actual practice is limited. This study examines how the integration of UX work with agile development can be improved in the context of a small Danish Software-as-a-Service (SaaS) company. We used the problem- and solution-oriented action research method over 12 months in the company. During this period, we initially carried out extensive participant observations, recorded 32 semi-structured interviews, and finally conducted two improvement iterations with evaluations of their effect on agility. We identified user stories as an essential indicator of UX integration. Verbose user stories imply problems in collaboration and trust, while concise user stories and deliberation improve integration of UX work with agile development. The conclusion is that integrating UX work with agile development in practice is complex, contextualized, and difficult even for only a small part of it. We propose that concise user stories and deliberation can be useful and well-defined focuses for integrating UX work with agile software development without sacrificing their agility. © 2020","Action research; Agile software development; Artifact; User experience; User story"
"Technical debt forecasting: An empirical study on open-source repositories","2020","Journal of Systems and Software","10.1016/j.jss.2020.110777","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089668454&doi=10.1016%2fj.jss.2020.110777&partnerID=40&md5=c5865fd97e7d4d15984d4e88ce60aad3","Technical debt (TD) is commonly used to indicate additional costs caused by quality compromises that can yield short-term benefits in the software development process, but may negatively affect the long-term quality of software products. Predicting the future value of TD could facilitate decision-making tasks regarding software maintenance and assist developers and project managers in taking proactive actions regarding TD repayment. However, no notable contributions exist in the field of TD forecasting, indicating that it is a scarcely investigated field. To this end, in the present paper, we empirically evaluate the ability of machine learning (ML) methods to model and predict TD evolution. More specifically, an extensive study is conducted, based on a dataset that we constructed by obtaining weekly snapshots of fifteen open source software projects over three years and using two popular static analysis tools to extract software-related metrics that can act as TD predictors. Subsequently, based on the identified TD predictors, a set of TD forecasting models are produced using popular ML algorithms and validated for various forecasting horizons. The results of our analysis indicate that linear Regularization models are able to fit and provide meaningful forecasts of TD evolution for shorter forecasting horizons, while the non-linear Random Forest regression performs better than the linear models for longer forecasting horizons. In most of the cases, the future TD value is captured with a sufficient level of accuracy. These models can be used to facilitate planning for software evolution budget and time allocation. The approach presented in this paper provides a basis for predictive TD analysis, suitable for projects with a relatively long history. To the best of our knowledge, this is the first study that investigates the feasibility of using ML models for forecasting TD. © 2020 Elsevier Inc.","Empirical study; Machine learning; Technical debt; Technical debt forecasting"
"Neural joint attention code search over structure embeddings for software Q&A sites","2020","Journal of Systems and Software","10.1016/j.jss.2020.110773","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089281710&doi=10.1016%2fj.jss.2020.110773&partnerID=40&md5=c739108203c697be3f5dd9b61402cac9","Code search is frequently needed in software Q&A sites for software development. Over the years, various code search engines and techniques have been explored to support user query. Early approaches often utilize text retrieval models to match textual code fragments for natural query, but fail to build sufficient semantic correlations. Some recent advanced neural methods focus on restructuring bi-modal networks to measure the semantic similarity. However, they ignore potential structure information of source codes and the joint attention information from natural queries. In addition, they mostly focus on specific code structures, rather than general code fragments in software Q&A sites. In this paper, we propose NJACS, a novel two-way attention-based neural network for retrieving code fragments in software Q&A sites, which aligns and focuses the more structure informative parts of source codes to natural query. Instead of directly learning bi-modal unified vector representations, NJACS first embeds the queries and codes using a bidirectional LSTM with pre-trained structure embeddings separately, then learns an aligned joint attention matrix for query-code mappings, and finally derives the pooling-based projection vectors in different directions to guide the attention-based representations. On different benchmark search codebase collected from StackOverflow, NJACS outperforms state-of-art baselines with 7.5% to 6% higher Recall@1 and MRR, respectively. Moreover, our designed structure embeddings can be leveraged for other deep-learning-based software tasks. © 2020","Code search; Joint attention; Software Q&A sites; Structure embeddings"
"On the generation, structure, and semantics of grammar patterns in source code identifiers","2020","Journal of Systems and Software","10.1016/j.jss.2020.110740","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087986700&doi=10.1016%2fj.jss.2020.110740&partnerID=40&md5=59481e15dfbf87ff2162e497c6717a68","Identifiers make up a majority of the text in code. They are one of the most basic mediums through which developers describe the code they create and understand the code that others create. Therefore, understanding the patterns latent in identifier naming practices and how accurately we are able to automatically model these patterns is vital if researchers are to support developers and automated analysis approaches in comprehending and creating identifiers correctly and optimally. This paper investigates identifiers by studying sequences of part-of-speech annotations, referred to as grammar patterns. This work advances our understanding of these patterns and our ability to model them by (1) establishing common naming patterns in different types of identifiers, such as class and attribute names; (2) analyzing how different patterns influence comprehension; and (3) studying the accuracy of state-of-the-art techniques for part-of-speech annotations, which are vital in automatically modeling identifier naming patterns, in order to establish their limits and paths toward improvement. To do this, we manually annotate a dataset of 1,335 identifiers from 20 open-source systems and use this dataset to study naming patterns, semantics, and tagger accuracy. © 2020 Elsevier Inc.","Identifier naming; Part-of-speech tagging; Program comprehension; Software maintenance; Source code analysis"
"A systematic mapping study on architectural smells detection","2021","Journal of Systems and Software","10.1016/j.jss.2020.110885","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098698807&doi=10.1016%2fj.jss.2020.110885&partnerID=40&md5=79bcce91325598c5ba71753a900109f3","The recognition of the need for high-quality software architecture is evident from the increasing trend in investigating architectural smells. Detection of architectural smells is paramount because they can seep through to design and implementation stages if left unidentified. Many architectural smells detection techniques and tools are proposed in the literature. The diversity in the detection techniques and tools suggests the need for their collective analysis to identify interesting aspects for practice and open research areas. To fulfill this, in this paper, we unify the knowledge about the detection of architectural smells through a systematic mapping study. We report on the existing detection techniques and tools for architectural smells to identify their limitations. We find there has been limited investigation of some architectural smells (e.g., micro-service smells); many architectural smells are not detected by tools yet; and there are limited empirical validations of techniques and tools. Based on our findings, we suggest several open research problems, including the need to (1) investigate undetected architectural smells (e.g., Java package smells), (2) improve the coverage of architectural smell detection across architecture styles (e.g., service-oriented and cloud), and (3) perform empirical validations of techniques and tools in industry across different languages and project domains. © 2020 Elsevier Inc.","Antipatterns; Architectural debt; Architectural smells; Smell detection techniques; Systematic mapping study"
"A systematic literature review on Technical Debt prioritization: Strategies, processes, factors, and tools","2021","Journal of Systems and Software","10.1016/j.jss.2020.110827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092459560&doi=10.1016%2fj.jss.2020.110827&partnerID=40&md5=a59e030ac2d0a88e9c0468720ad2ecec","Background: Software companies need to manage and refactor Technical Debt issues. Therefore, it is necessary to understand if and when refactoring of Technical Debt should be prioritized with respect to developing features or fixing bugs. Objective: The goal of this study is to investigate the existing body of knowledge in software engineering to understand what Technical Debt prioritization approaches have been proposed in research and industry. Method: We conducted a Systematic Literature Review of 557 unique papers published until 2020, following a consolidated methodology applied in software engineering. We included 44 primary studies. Results: Different approaches have been proposed for Technical Debt prioritization, all having different goals and proposing optimization regarding different criteria. The proposed measures capture only a small part of the plethora of factors used to prioritize Technical Debt qualitatively in practice. We present an impact map of such factors. However, there is a lack of empirical and validated set of tools. Conclusion: We observed that Technical Debt prioritization research is preliminary and there is no consensus on what the important factors are and how to measure them. Consequently, we cannot consider current research conclusive. In this paper, we therefore outline different directions for necessary future investigations. © 2020 Elsevier Inc.","Systematic Literature Review; Technical Debt; Technical Debt prioritization"
"FaaSten your decisions: A classification framework and technology review of function-as-a-Service platforms","2021","Journal of Systems and Software","10.1016/j.jss.2021.110906","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099635714&doi=10.1016%2fj.jss.2021.110906&partnerID=40&md5=b41e03029ee170cb515f45dec3388ca4","Function-as-a-Service (FaaS) is a cloud service model enabling developers to offload event-driven executable snippets of code. The execution and management of such functions becomes a FaaS provider's responsibility, therein included their on-demand provisioning and automatic scaling. Key enablers for this cloud service model are FaaS platforms, e.g., AWS Lambda, Microsoft Azure Functions, or OpenFaaS. At the same time, the choice of the most appropriate FaaS platform for deploying and running a serverless application is not trivial, as various organizational and technical aspects have to be taken into account. In this work, we present (i) a FaaS platform classification framework derived using a multivocal review and (ii) a technology review of the ten most prominent FaaS platforms, based on the proposed classification framework. We also present a FaaS platform selection support system, called FAASTENER, which can help researchers and practitioners to choose the FaaS platform most suited for their requirements. © 2021 The Authors","Classification framework; FaaS; Function-as-a-Service; Platform; Serverless; Technology review"
"TagDC: A tag recommendation method for software information sites with a combination of deep learning and collaborative filtering","2020","Journal of Systems and Software","10.1016/j.jss.2020.110783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089462386&doi=10.1016%2fj.jss.2020.110783&partnerID=40&md5=f178326dcf9c1f5c8a43aaf5ad47d3c5","Software information sites (e.g., StackOverflow, Freecode, etc.) are increasingly essential for software developers to share knowledge, communicate new techniques, and collaborate. With the rapid growth of software objects, tags are widely applied to aid developers’ various operations on software information sites. Since tags are freely and optionally selected by developers, the differences in background, expression habits, and understanding of software objects among developers may cause inconsistent or inappropriate tags. To alleviate the problems of tag synonyms and tag explosion, we propose TagDC, i.e., a composite Tag recommendation method with Deep learning and Collaborative filtering. TagDC consists of two complementary modules: the word learning enhanced CNN capsule module (TagDC-DL) and the collaborative filtering module (TagDC-CF). It can improve the understanding of software objects from different perspectives. Given a new software object, TagDC can calculate a list of the combined confidence probabilities of tags and then recommend TOP-K tags by ranking the probabilities in the list. We evaluated our TagDC on nine datasets with different scales. The experimental results show that TagDC achieves a better effectiveness against two state-of-the-art baseline methods (i.e., TagCNN and FastTagRec) with a substantial improvement. © 2020 Elsevier Inc.","Collaborative filtering; Deep learning; Software information site; Tag recommendation"
"Some SonarQube issues have a significant but small effect on faults and changes. A large-scale empirical study","2020","Journal of Systems and Software","10.1016/j.jss.2020.110750","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087390369&doi=10.1016%2fj.jss.2020.110750&partnerID=40&md5=7d907accd59bcbf3f76304179772122c","Context: Companies frequently invest effort to remove technical issues believed to impact software qualities, such as removing anti-patterns or coding styles violations. Objective: We aim to analyze the diffuseness of SonarQube issues in software systems and to assess their impact on code changes and fault-proneness, considering also their different types and severities. Methods: We conducted a case study among 33 Java projects from the Apache Software Foundation repository. Results: We analyzed 726 commits containing 27K faults and 12M changes in Java files. The projects violated 173 SonarQube rules generating more than 95K SonarQube issues in more than 200K classes. Classes not affected by SonarQube issues are less change-prone than affected ones, but the difference between the groups is small. Non-affected classes are slightly more change-prone than classes affected by SonarQube issues of type Code Smell or Security Vulnerability. As for fault-proneness, there is no difference between non-affected and affected classes. Moreover, we found incongruities in the type and severity assigned by SonarQube. Conclusion: Our result can be useful for practitioners to understand which SonarQube issues should be refactored and for researchers to bridge the missing gaps. Moreover, results can also support companies and tool vendors in identifying SonarQube issues as accurately as possible. © 2020","Change-proneness; Empirical study; Fault-proneness; SonarQube"
"Continuous experimentation and the cyber–physical systems challenge: An overview of the literature and the industrial perspective","2020","Journal of Systems and Software","10.1016/j.jss.2020.110781","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089796894&doi=10.1016%2fj.jss.2020.110781&partnerID=40&md5=f423c74d4f907014462127376e0f902c","Context: New software development patterns are emerging aiming at accelerating the process of delivering value. One is Continuous Experimentation, which allows to systematically deploy and run instrumented software variants during development phase in order to collect data from the field of application. While currently this practice is used on a daily basis on web-based systems, technical difficulties challenge its adoption in fields where computational resources are constrained, e.g., cyber–physical systems and the automotive industry. Objective: This paper aims at providing an overview of the engagement on the Continuous Experimentation practice in the context of cyber–physical systems. Method: A systematic literature review has been conducted to investigate the link between the practice and the field of application. Additionally, an industrial multiple case study is reported. Results: The study presents the current state-of-the-art regarding Continuous Experimentation in the field of cyber–physical systems. The current perspective of Continuous Experimentation in industry is also reported. Conclusions: The field has not reached maturity yet. More conceptual analyses are found than solution proposals and the state-of-practice is yet to be achieved. However it is expected that in time an increasing number of solutions will be proposed and validated. © 2020","Continuous Experimentation; Cyber–physical systems; Software engineering"
"Early validation of cyber–physical space systems via multi-concerns integration","2020","Journal of Systems and Software","10.1016/j.jss.2020.110742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088829475&doi=10.1016%2fj.jss.2020.110742&partnerID=40&md5=9040473a156e4eb27823b3ecfe9e5c7d","Cyber–physical space systems are engineered systems operating within physical space with design requirements that depend on space, e.g., regarding location or movement behavior. They are built from and depend upon the seamless integration of computation and physical components. Typical examples include systems where software-driven agents such as mobile robots explore space and perform actions to complete particular missions. Design of such a system often depends on multiple concerns expressed by different stakeholders, capturing different aspects of the system. We propose a model-driven approach supporting (a) separation of concerns during design, (b) systematic and semi-automatic integration of separately modeled concerns, and finally (c) early validation via statistical model checking. We evaluate our approach over two different case studies of cyber–physical space systems. © 2020",""
"Constrained locating arrays for combinatorial interaction testing","2020","Journal of Systems and Software","10.1016/j.jss.2020.110771","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089285223&doi=10.1016%2fj.jss.2020.110771&partnerID=40&md5=11f78dfa303b6868231c06cc3bfeb089","This paper introduces the notion of Constrained Locating Arrays (CLAs), mathematical objects which can be used for fault localization in software testing. CLAs extend ordinary locating arrays to make them applicable to testing of systems that have constraints on test parameters. Such constraints are common in real-world systems; thus CLA enhances the applicability of locating arrays to practical testing problems. The paper also proposes an algorithm for constructing CLAs. Experimental results show that the proposed algorithm scales to problems of practical sizes. © 2020","Combinatorial interaction testing; Covering arrays; Locating arrays; Software testing"
"Accessibility and Software Engineering Processes: A Systematic Literature Review","2021","Journal of Systems and Software","10.1016/j.jss.2020.110819","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090877307&doi=10.1016%2fj.jss.2020.110819&partnerID=40&md5=281b42d6d9f8f3a0f8ac7fe67113058d","Guidelines, techniques, and methods have been presented in the literature in recent years to contribute to the development of accessible software and to promote digital inclusion. Considering that software product quality depends on the quality of the development process, researchers have investigated how to include accessibility during the software development process in order to obtain accessible software. Two Systematic Literature Reviews (SLR) have been conducted in the past to identify such research initiatives. This paper presents a new SLR, considering the period from 2011 to 2019. The review of 94 primary studies showed the distribution of publications on different phases of the software life cycle, mainly the design and testing phases. The study also identified, for the first time, papers about accessibility and software process establishment. This result reinforces that, in fact, accessibility is not characterized as a property of the final software only. Instead, it evolves over the software life cycle. Besides, this study aims to provide designers and developers with an updated view of methods, tools, and other assets that contribute to process enrichment, valuing accessibility, as well as shows the gaps and challenges which deserve to be investigated. © 2020 Elsevier Inc.","Accessibility; Design for disabilities; Methods for accessibility; Software Engineering; Systematic Literature Review"
"Can this fault be detected: A study on fault detection via automated test generation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089805027&doi=10.1016%2fj.jss.2020.110769&partnerID=40&md5=2ddf6ff76637c00444e77b284efe1666","Automated test generation can reduce the manual effort in improving software quality. A test generation method employs code coverage, such as the widely-used branch coverage, to guide the inference of tests. These tests can be used to detect hidden faults. An automatic tool takes a specific type of code coverage as a configurable parameter. Given an automated tool of test generation, a fault may be detected by one type of code coverage, but omitted by another. In frequently released software projects, the time budget of testing is limited. Configuring code coverage for a testing tool can effectively improve the quality of projects. In this paper, we conduct a study on whether a fault can be detected by specific code coverage in automated test generation. We build predictive models with 60 metrics of faulty source code to identify detectable faults under eight types of code coverage. In the experiment, an off-the-shelf tool, EvoSuite is used to generate test data. Experimental results based on four research questions show that different types of code coverage result in the detection of different faults; a code coverage can be used as a supplement to increase the number of detected faults if another coverage is applied first; for each coverage, the number of detected faults increases with its cutoff time in test generation. Our result shows that the choice of code coverage can be learned via multi-objective optimization from sampled faults and directly applied to new faults. This study can be viewed as a preliminary result to support the configuration of code coverage in the application of automated test generation. © 2020 Elsevier Inc.","Code coverage; Code metrics; Predictive models; Test generation"
"Does code quality affect pull request acceptance? An empirical study","2021","Journal of Systems and Software","10.1016/j.jss.2020.110806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090024069&doi=10.1016%2fj.jss.2020.110806&partnerID=40&md5=3fc8497ac55b764c00a741b82889bf76","Background: Pull requests are a common practice for making contributions and reviewing them in both open-source and industrial contexts. Objective: Our goal is to understand whether quality flaws such as code smells, anti-patterns, security vulnerabilities, and coding style violations in a pull request's code affect the chance of its acceptance when reviewed by a maintainer of the project. Method: We conducted a case study among 28 Java open-source projects, analyzing the presence of 4.7 M code quality flaws in 36 K pull requests. We analyzed further correlations by applying logistic regression and six machine learning techniques. Moreover, we manually validated 10% of the pull requests to get further qualitative insights on the importance of quality issues in cases of acceptance and rejection. Results: Unexpectedly, quality flaws measured by PMD turned out not to affect the acceptance of a pull request at all. As suggested by other works, other factors such as the reputation of the maintainer and the importance of the delivered feature might be more important than other qualities in terms of pull request acceptance. Conclusions:. Researchers have already investigated the influence of the developers’ reputation and the pull request acceptance. This is the first work investigating code style violations and specifically PMD rules. We recommend that researchers further investigate this topic to understand if different measures or different tools could provide some useful measures. © 2020 The Authors","Machine learning; PMD rules; Pull requests"
"Understanding and recommending security requirements from problem domain ontology: A cognitive three-layered approach","2020","Journal of Systems and Software","10.1016/j.jss.2020.110695","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086587944&doi=10.1016%2fj.jss.2020.110695&partnerID=40&md5=533147a9762e34955b4c6dbbf582d9a5","Socio-technical systems (STS) are inherently complex due to the heterogeneity of its intertwined components. Therefore, ensuring STS security continues to pose significant challenges. Persistent security issues in STS are extremely critical to address as threats to security can affect entire enterprises, resulting in significant recovery costs. A profound understanding of the problems across multiple dimensions of STS is the key in addressing such security issues. However, we lack a systematic acquisition of the scattered knowledge related to design, development, and execution of STS. In this work, we methodologically analyze security issues from a requirements engineering perspective. We propose a cognitive three-layered framework integrating various modeling methodologies and knowledge sources related to security. This framework helps in understanding essential components of security and making recommendations of security requirements regarding threat analyses and risk assessments using Problem Domain Ontology (PDO) knowledge base. We also provide tool support for our framework. With the goal-oriented security reference model, we demonstrate how security requirements are recommended based on PDO, with the help of the tool. The organized acquisition of knowledge from SME groups and the domain working group provides rich context of security requirements, and also enhances the re-usability of the knowledge set. © 2020 Elsevier Inc.","Ontology; Requirements engineering; Security"
"HMER: A Hybrid Mutation Execution Reduction approach for Mutation-based Fault Localization","2020","Journal of Systems and Software","10.1016/j.jss.2020.110661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085263349&doi=10.1016%2fj.jss.2020.110661&partnerID=40&md5=e7fa20c371433e0197381bbae2d5d07a","Identifying the location of faults in programs has been recognized as one of the most manually and time cost activities during software debugging process. Fault localization techniques, which seek to identify faulty program statements as quickly as possible, can assist developers in alleviating the time and manual cost of software debugging. Mutation-based fault localization(MBFL) has a promising fault localization accuracy, but suffered from huge mutation execution cost. To reduce the cost of MBFL, we propose a Hybrid Mutation Execution Reduction(HMER) approach in this paper. HMER consists of two steps: Weighted Statement-Oriented Mutant Sampling(WSOME) and Dynamic Mutation Execution Strategy(DMES). In the first step, we employ Spectrum-Based Fault Localization(SBFL) techniques to calculate the suspiciousness value of statements, and guarantee that the mutants generated from statements with higher suspiciousness value will have more chance to be remained in the sampling process. Next, a dynamic mutation execution strategy is used to execute the reduced mutant set on test suite to avoid worthless execution. Empirical results on 130 versions from 9 subject programs show that HMER can reduce 74.5%-93.4% mutation execution cost while keeping almost the same fault localization accuracy with the original MBFL. A further Wilcoxonsigned−ranktest indicates that when employing HMER strategy in MBFL, the fault localization accuracy has no statistically significant difference in most cases compared with the original MBFL without any reduction techniques. © 2020 Elsevier Inc.","Cost reduction; Mutant sampling; Mutation-based Fault Localization; Spectrum-based Fault Localization"
"Vendor Switching: Factors that matter when engineers onboard their own replacement","2020","Journal of Systems and Software","10.1016/j.jss.2020.110719","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087390070&doi=10.1016%2fj.jss.2020.110719&partnerID=40&md5=77deec455190b3982e9f7c45656b81b0","Offshore outsourcing is a common way of working, but sourcing collaborations do not always last, and sometimes vendors are switched. Vendor switching results in a complex form of relationship, in which the competing outgoing and incoming vendors are expected to cooperate. The success of such transitions highly depends on successful knowledge transfer and thus the willingness of the outgoing vendor to train their own replacement. While switching decisions have gained attention, the role of the vendor relationship is still relatively unexplored. In this paper, we report findings from a multi-case study of vendor switching in three projects based on 22 interviews with 27 interviewees. We developed a theoretical model explaining the complex interplay between the factors affecting such transitions. Our results confirm that opportunistic behavior of outgoing vendors is a probable threat. We found that the vendor relationship moderates the link between initial negative emotions and the opportunistic behavior of the outgoing vendor. Other important factors affecting the success of the transition include the relationship with the client, outgoing vendor's management engagement, and the cultural and organizational fit between vendors. We conclude with recommendations for companies switching vendors. © 2020 Elsevier Inc.","Empirical; Global software engineering; Knowledge transfer; Offshore outsourcing; Opportunism; Vendor switching"
"Formal analysis and verification of the PSTM architecture using CSP","2020","Journal of Systems and Software","10.1016/j.jss.2020.110559","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082114573&doi=10.1016%2fj.jss.2020.110559&partnerID=40&md5=192d76b43fd864f518ad2153f3c84543","Starting with the analysis of the source codes of the Python Software Transactional Memory (PSTM) architecture, this paper applies process algebra CSP to formally verify the architecture at a fine-grained level. We analyze the communication process and components of the architecture from multiple perspectives and establish models describing the communication behaviors of the PSTM architecture. We use model checker PAT to automatically simulate and verify the established model. After adapting the traditional transactional properties to the PSTM architecture, we analyze and verify five properties for the PSTM architecture, including deadlock freeness, atomicity, isolation, consistency and optimism. The verification results indicate that all the properties are valid. Based on the judgement of the execution logic of the communication procedure in the PSTM architecture, we can conclude that the architecture can have a proper communication and can guarantee atomicity, isolation, consistency and optimism. Besides, we also provide a case study with an application scenario and propose a corollary that the value of the shared counter is equal to the number of parallel processes. We verify whether the case study system can satisfy all the conditions of corollary from both positive and negative perspectives. The results show that the corollary is tenable. © 2020","Formal analysis and verification; Shared counter; The PSTM arhcitecture; Transactional memory"
"An automatic software vulnerability classification framework using term frequency-inverse gravity moment and feature selection","2020","Journal of Systems and Software","10.1016/j.jss.2020.110616","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085160465&doi=10.1016%2fj.jss.2020.110616&partnerID=40&md5=ec6889e866047fa61c70546a282eb115","Vulnerability classification is an important activity in software development and software quality maintenance. A typical vulnerability classification model usually involves a stage of term selection, in which the relevant terms are identified via feature selection. It also involves a stage of term-weighting, in which the document weights for the selected terms are computed, and a stage for classifier learning. Generally, the term frequency-inverse document frequency (TF-IDF) model is the most widely used term-weighting metric for vulnerability classification. However, several issues hinder the effectiveness of the TF-IDF model for document classification. To address this problem, we propose and evaluate a general framework for vulnerability severity classification using the term frequency-inverse gravity moment (TF-IGM). Specifically, we extensively compare the term frequency-inverse gravity moment, term frequency-inverse document frequency, and information gain feature selection using five machine learning algorithms on ten vulnerable software applications containing a total number of 27,248 security vulnerabilities. The experimental result shows that: (i) the TF-IGM model is a promising term weighting metric for vulnerability classification compared to the classical term-weighting metric, (ii) the effectiveness of feature selection on vulnerability classification varies significantly across the studied datasets and (iii) feature selection improves vulnerability classification. © 2020 Elsevier Inc.","Classification; Feature selection; Machine learning algorithms; Severity; Software vulnerability; Term-weighting"
"Kulla, a container-centric construction model for building infrastructure-agnostic distributed and parallel applications","2020","Journal of Systems and Software","10.1016/j.jss.2020.110665","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085526224&doi=10.1016%2fj.jss.2020.110665&partnerID=40&md5=7cd10080cf629280f443808a144b8b41","This paper presents the design, development, and implementation of Kulla, a virtual container-centric construction model that mixes loosely coupled structures with a parallel programming model for building infrastructure-agnostic distributed and parallel applications. In Kulla, applications, dependencies and environment settings, are mapped with construction units called Kulla-Blocks. A parallel programming model enables developers to couple those interoperable structures for creating constructive structures named Kulla-Bricks. In these structures, continuous dataflow and parallel patterns can be created without modifying the code of applications. Methods such as Divide&Containerize (data parallelism), Pipe&Blocks (streaming), and Manager/Block (task parallelism) were developed to create Kulla-Bricks. Recursive combinations of Kulla instances can be grouped in deployment structures called Kulla-Boxes, which are encapsulated into VCs to create infrastructure-agnostic parallel and/or distributed applications. Deployment strategies were created for Kulla-Boxes to improve the IT resource profitability. To show the feasibility and flexibility of this model, solutions combining real-world applications were implemented by using Kulla instances to compose parallel and/or distributed system deployed on different IT infrastructures. An experimental evaluation based on use cases solving satellite and medical image processing problems revealed the efficiency of Kulla model in comparison with some traditional state-of-the-art solutions. © 2020","Construction model; Infrastructure-agnostic applications; Parallel patterns; Pipelines; Virtual containers"
"An Android application risk evaluation framework based on minimum permission set identification","2020","Journal of Systems and Software","10.1016/j.jss.2020.110533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079002933&doi=10.1016%2fj.jss.2020.110533&partnerID=40&md5=4abcbc6d6111c55efe5608f854f009a5","Android utilizes a security mechanism that requires apps to request permission for accessing sensitive user data, e.g., contacts and SMSs, or certain system features, e.g., camera and Internet access. However, Android apps tend to be overprivileged, i.e., they often request more permissions than necessary. This raises the security problem of overprivilege. To alleviate the overprivilege problem, this paper proposes MPDroid, an approach that combines static analysis and collaborative filtering to identify the minimum permissions for an Android app based on its app description and API usage. Given an app, MPDroid first employs collaborative filtering to identify the initial minimum permissions for the app. Then, through static analysis, the final minimum permissions that an app really needs are identified. Finally, it evaluates the overprivilege risk by inspecting the app's extra privileges, i.e., the unnecessary permissions requested by the app. Experiments are conducted on 16,343 popular apps collected from Google Play. The results show that MPDroid outperforms the state-of-the-art approach significantly. © 2020","App risk evaluation; Collaborative filtering; Minimum permissions; Permission overprivilege; Static analysis"
"Towards the adoption of OMG standards in the development of SOA-based IoT systems","2020","Journal of Systems and Software","10.1016/j.jss.2020.110720","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087512124&doi=10.1016%2fj.jss.2020.110720&partnerID=40&md5=832d035201580a2e01a19aa690cd5bb4","A common feature of the Internet of Things (IoT) is the high heterogeneity, regarding network protocols, data formats, hardware and software platforms. Aiming to deal with such a degree of heterogeneity, several frameworks have applied the Model-Driven Development (MDD) to build IoT applications. On the software architecture viewpoint, the literature has shown that the Service-Oriented Architecture (SOA) is a promising style to address the interoperability of entities composing these solutions. Some features of IoT make it challenging to analyze the impact of design decisions on the SOA-based IoT applications behavior. Thus, it is a key requirement to simulate the model to verify whether the system performs as expected before its implementation. Although the literature has identified that the SOA style is suitable for addressing the interoperability, existing modeling languages do not consider SOA elements as first-class citizens when designing IoT applications. Furthermore, although existing MDD frameworks provide modeling languages comprising well-defined syntax, they lack execution semantics, thus, are not suitable for model execution and analysis. This work aims at addressing these issues by introducing IoTDraw. The framework provides a fully OMG-compliant executable modeling language for SOA-based IoT systems; thus, its specifications can be implemented by any tool implementing OMG standards. © 2020 Elsevier Inc.","Application; Internet of Things; Model-Driven Development; Service-Oriented Architecture"
"A comparison of quality flaws and technical debt in model transformation specifications","2020","Journal of Systems and Software","10.1016/j.jss.2020.110684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086381665&doi=10.1016%2fj.jss.2020.110684&partnerID=40&md5=992a7eee002a59b3a9df05cff256dfc8","The quality of model transformations (MT) has high impact on model-driven engineering (MDE) software development approaches, because of the central role played by transformations in MDE for refining, migrating, refactoring and other operations on models. For programming languages, a popular paradigm for code quality is the concept of technical debt (TD), which uses the analogy that quality flaws in code are a debt burden carried by the software, which must either be ‘redeemed’ by expending specific effort to remove its flaws, or be tolerated, with ongoing additional costs to maintenance due to the flaws. Whilst the analysis and management of quality flaws and TD in programming languages has been investigated in depth over several years, less research on the topic has been carried out for model transformations. In this paper we investigate the characteristics of quality flaws and technical debt in model transformation languages, based upon systematic analysis of over 100 transformation cases in four leading MT languages. Based on quality flaw indicators for TD, we identify significant differences in the level and kinds of technical debt in different MT languages, and we propose ways in which TD in MT can be reduced and managed. © 2020 Elsevier Inc.","Model transformations; Software quality; Technical debt"
"Evaluating and strategizing the onboarding of software developers in large-scale globally distributed projects","2020","Journal of Systems and Software","10.1016/j.jss.2020.110699","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086890507&doi=10.1016%2fj.jss.2020.110699&partnerID=40&md5=8c6d19c3d72d9ce809748ed7fefc048c","The combination of scale and distribution in software projects makes the onboarding of new developers problematic. To the best of our knowledge, there is no research on the relationship between onboarding strategies and the performance evolution of newcomers in large-scale, globally distributed projects. Furthermore, there are no approaches to support the development of strategies to systematically onboard developers. In this paper, we address these gaps by means of an industrial case study. We identified that the following aspects seem to be related to the observed onboarding results: the distance to mentors, the formal training approach used, the allocation of large and distributed tasks in the early stages of the onboarding process, and team instability. We conclude that onboarding must be planned well ahead and should consider avoiding the aspects mentioned above. Based on the results of this investigation, we propose a process to strategize and evaluate onboarding. To develop the process, we used business process modeling. We conducted a static validation of the proposed process utilizing interviews with experts. The static validation of the process indicates that it can help companies to deal with the challenges associated with the onboarding of newcomers through more systematic, effective, and repeatable onboarding strategies. © 2020 Elsevier Inc.","Global Software Engineering; Large-scale software development; Onboarding"
"A refinement checking based strategy for component-based systems evolution","2020","Journal of Systems and Software","10.1016/j.jss.2020.110598","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084650891&doi=10.1016%2fj.jss.2020.110598&partnerID=40&md5=d12f865a0953aea22323c3bb29cfa304","We propose inheritance and refinement relations for a CSP-based component model (BRIC), which supports a constructive design based on composition rules that preserve classical concurrency properties such as deadlock freedom. The proposed relations allow extension of functionality, whilst preserving behavioural properties. A notion of extensibility is defined on top of a behavioural relation called convergence, which distinguishes inputs from outputs and the context where they are communicated, allowing extensions to reuse existing events with different purposes. We mechanise the strategy for extensibility verification using the FDR4 tool, and illustrate our results with an autonomous healthcare robot case study. © 2020 Elsevier Inc.","Behavioural specification; Component extensibility; Correctness by construction; CSP; FDR4"
"Satisfaction and its correlates in agile software development","2020","Journal of Systems and Software","10.1016/j.jss.2020.110544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079738556&doi=10.1016%2fj.jss.2020.110544&partnerID=40&md5=b6e5ab655bb13d9e23b283977d87e7d0","In this paper we address the topic of software development team members satisfaction with their development process. We present an in-depth analysis of the results of a nationwide survey about software development in Switzerland. We wanted to find out if satisfaction relates to the applied development method, and to the use of various practices, and impacts on business, team and software issues. We found that higher satisfaction is reported more by those using Agile development than with plan-driven processes. We explored the different perspectives of developers and those with a management role and found a high consistency of satisfaction between Agile developers and Agile management, and differences with those using working plan-driven methods. We found that certain practices and impacts have high correlations to satisfaction, and that collaborative processes are closely related to satisfaction. We then explored the relationship between satisfaction and various other perspectives. Our results in this analysis are principally descriptive, but we think they can be a relevant contribution to understand the challenges for everyone involved in Agile development. © 2020",""
"On testing machine learning programs","2020","Journal of Systems and Software","10.1016/j.jss.2020.110542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081030171&doi=10.1016%2fj.jss.2020.110542&partnerID=40&md5=0019441a753439c6d2a0513d3a102029","Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs. © 2020","Data cleaning; Feature engineering testing; Implementation testing; Machine learning; Model testing"
"More precise construction of static single assignment programs using reaching definitions","2020","Journal of Systems and Software","10.1016/j.jss.2020.110590","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083322697&doi=10.1016%2fj.jss.2020.110590&partnerID=40&md5=a015e4113794c21c5fa9dd29ef02380a","The Static Single Assignment (SSA) form is an intermediate representation used for the analysis and optimization of programs in modern compilers. The ϕ-function placement is the most computationally expensive part of converting any program into its SSA form. The most widely-used ϕ-function placement algorithms are based on computing dominance frontiers (DF). However, this kind of algorithms works under the limiting assumption that all variables are defined at the beginning of the program, which is not the case for local variables. In this paper, we introduce an innovative ϕ-placement algorithm based on computing reaching definitions (RD), which generates a precise number of ϕ-functions. We provided theorems and proofs showing the correctness and the theoretical computational complexity of our algorithms. We implemented our approach and a well-known DF-based algorithm in the Clang/LLVM compiler framework, and performed experiments on a number of benchmarks. The results show that the limiting assumption of the DF-based algorithm when compared with the more accurate results of our RD-based approach leads to generating up to 87% (69% on average) superfluous ϕ-functions on all benchmarks, and thus brings about a significant precision loss. Moreover, even though our approach computes more information to generate precise results, it is able to analyze up to 92.96% procedures (65.63% on average) of all benchmarks with execution time within twice the execution time of the reference DF-based approach. © 2020 Elsevier Inc.","Dataflow analysis; Program optimization; Program transformation; Reaching definition; Static single assignment"
"Finding faults: A scoping study of fault diagnostics for Industrial Cyber–Physical Systems","2020","Journal of Systems and Software","10.1016/j.jss.2020.110638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084943715&doi=10.1016%2fj.jss.2020.110638&partnerID=40&md5=b077bf5f4aae340fb9e379de4a5b086e","Context: As Industrial Cyber–Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. Each of these sectors has adopted particular methods to meet their differing diagnostic needs. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. We created categories for the different diagnostic approaches via a pilot study and present an analysis of the trends that emerged. We then compared the maturity of these approaches by adapting and using the NASA Technology Readiness Level (TRL) scale. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Hybrid techniques that blend aspects of these three broad categories were also encountered. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems. Connecting ICPS together to share sufficient telemetry to diagnose and manage faults is difficult when the physical environment places demands on ICPS. Despite these challenges, the most mature papers present robust fault diagnosis and analysis techniques which have moved beyond the laboratory and are proving valuable in real-world environments. © 2020 Elsevier Inc.","Aerospace; Automotive; Avionics; Faults; Industrial control; Industrial cyber–physical systems"
"From API to NLI: A new interface for library reuse","2020","Journal of Systems and Software","10.1016/j.jss.2020.110728","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087591981&doi=10.1016%2fj.jss.2020.110728&partnerID=40&md5=f1c9f2f53a7b7bfc8b709df61a8ffb2f","Developers frequently reuse APIs from existing libraries to implement certain functionality. However, learning APIs is difficult due to their large scale and complexity. In this paper, we design an abstract framework NLI2CODE to ease the reuse process. Under the framework, users can reuse library functionalities with a high-level, automatically-generated NLI (Natural Language Interface) instead of the detailed API elements. The framework consists of three components: a functional feature extractor to summarize the frequently-used library functions in natural language form, a code pattern miner to give a code template for each functional feature, and a synthesizer to complete code patterns into well-typed snippets. From the perspective of a user, a reuse task under NLI2CODE starts from choosing a functional feature and our framework will guide the user to synthesize the desired solution. We instantiated the framework as a tool to reuse Java libraries. The evaluation shows our tool can generate a high-quality natural language interface and save half of the coding time for newcomers to solve real-world programming tasks. © 2020 Elsevier Inc.","Code pattern; Library reuse; Program synthesis"
"A systematic literature review of model-driven security engineering for cyber–physical systems","2020","Journal of Systems and Software","10.1016/j.jss.2020.110697","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086721259&doi=10.1016%2fj.jss.2020.110697&partnerID=40&md5=cc3ed0fa936ddb0759f4f98a5486c183","The last years have elevated the importance of cyber–physical systems like IoT applications, smart cars, or industrial control systems, and, therefore, these systems have also come into the focus of attackers. In contrast to software products running on PCs or smartphones, updating and maintaining cyber–physical systems presents a major challenge. This challenge, combined with the often decades-long lifetime of cyber–physical systems, and with their deployment in often safety-critical contexts, makes it particularly important to consider their security already at design time. When aiming to obtain a provably secure design, model-driven security approaches are key, as they allow to identify and mitigate threats in early phases of the development. As attacks may exploit both code-level as well as physical vulnerabilities, such approaches must consider not just the cyber layer but the physical layer as well. To find out which model-driven security approaches for cyber–physical systems exist considering both layers, we conducted a systematic literature review. From a set of 1160 initial papers, we extracted 69 relevant publications describing 17 candidate approaches. We found seven approaches specifically developed for cyber–physical systems. We provide a comprehensive description of these approaches, discuss them in particular detail, and determine their limitations. We found out that model-driven security is a relevant research area but most approaches focus only on specific security properties and even for CPS-specific approaches the platform is only rarely taken into account. © 2020 Elsevier Inc.","Cyber–physical systems; Literature survey; Model-driven security; Platform-specific; Security modeling; Systematic literature review"
"Integrating GitLab metrics into coursework consultation sessions in a software engineering course","2020","Journal of Systems and Software","10.1016/j.jss.2020.110613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084171536&doi=10.1016%2fj.jss.2020.110613&partnerID=40&md5=2e071c4a7ed30bc6ef99a7caf87b4453","Software developers use version control systems for collaborative coding. These systems are integrated into several software development platforms (including GitLab and GitHub) which support additional software engineering functionalities. Using these platforms in an educational context allows students to gain skills relevant to industry, whilst providing a means of keeping track of their activities. In this paper, we investigate the effect of presenting teams of students with GitLab metrics about their performance at coursework consultation sessions (checkpoint sessions), with a particular focus on the number of issues assigned and completed, and the number of commits made to the repository. A comparative analysis of project marks in two consecutive academic years indicates that these checkpoint sessions may lead to better student outcomes. An interview study with students and teaching assistants identified viewing the GitLab metrics in the checkpoints as an opportunity to see the relative contributions of team members and address resulting issues, and as a catalyst for improving engagement with the team project. The study also identified drawbacks of using the metrics too simplistically, and suggested that it was important to consider the quality and amount of written code, as well as the number of times someone committed to the repository. © 2020 Elsevier Inc.","Collaborative software development; Git; Software engineering education; Undergraduate education; Version control system"
"Improving software bug-specific named entity recognition with deep neural network","2020","Journal of Systems and Software","10.1016/j.jss.2020.110572","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082623231&doi=10.1016%2fj.jss.2020.110572&partnerID=40&md5=13ace9290b80aa235079b3d172cd329a","There is a large volume of bug data in the bug repository, which contains rich bug information. Existing studies on bug data mining mainly rely on using information retrieval (IR) technology to search relevant historical bug reports. These studies basically treat a bug report as a closed unit, ignoring the semantic and structural information within it. Named-entity recognition (NER) is an important task of information extraction (IE) technology. Based on NER, fine-grained factual information could be comprehensively extracted to further form structured data, which provides a new way to improve the accessibility of bug information. However, bug NER is different from general NER tasks. Bug reports are free-form text, which include a mixed language environment studded with code, abbreviations and software-specific vocabularies. In this paper, we propose a deep neural network approach for bug-specific entity recognition called DBNER using bidirectional long short-term memory (LSTM) with Conditional Random Fields decoding model (CRF). DBNER extracts multiple features from the massive bug data and uses attention mechanism to improve the consistency of entity tags in the bug reports. Experiment results show that the F1-score reaches an average of 91.19%. In addition, in cross-project experiments, the DBNER's F1-score reaches an average of 84%. © 2020 Elsevier Inc.","LSTM-CRF; Named entity recognition; Software bug analysis; Software bug corpus"
"Requirements for adopting software process lines","2020","Journal of Systems and Software","10.1016/j.jss.2020.110546","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080044458&doi=10.1016%2fj.jss.2020.110546&partnerID=40&md5=43b8efccf2ce38c410e6152abeb32f9e","A Software Process Line (SPrL) is potentially suitable for constructing software development methodologies by reusing core assets. However, adopting this approach without prior assessment of its suitability can lead to failure. The aim of this paper is to identify a set of requirements that can be used for deciding whether to adopt the SPrL approach in an organization. Identification of the requirements was accomplished in two stages: the characteristics important in method tailoring were first identified via a Systematic Mapping Study (SMS) that focused on analyzing 43 primary studies; the degree of importance of the identified characteristics was then determined using a questionnaire survey in which 31 experts participated. By analyzing the results of the SMS and the survey, we have identified 12 product-related, 22 project-related, and 10 organization-related requirements. In addition to these requirements, we have also identified two relevant requirements by studying previous research on Software Product Lines (SPL) and Business Process Lines (BPL). The requirements thus identified can help organizations decide on whether to adopt the SPrL approach: the more an organization satisfies the requirements, the more frequently method tailoring occurs in that organization, and hence, the more justified it is to adopt the SPrL approach. © 2020","Empirical Study; Method Tailoring; Software Process Lines; Systematic Mapping Study"
"Automated defect identification via path analysis-based features with transfer learning","2020","Journal of Systems and Software","10.1016/j.jss.2020.110585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083420285&doi=10.1016%2fj.jss.2020.110585&partnerID=40&md5=62cdd1a1ae83db9589e284fbae3c80e6","Recently, artificial intelligence techniques have been widely applied to address various specialized tasks in software engineering, such as code generation, defect identification, and bug repair. Despite the diffuse usage of static analysis tools in automatically detecting potential software defects, developers consider the large number of reported alarms and the expensive cost of manual inspection to be a key barrier to using them in practice. To automate the process of defect identification, researchers utilize machine learning algorithms with a set of hand-engineered features to build classification models for identifying alarms as actionable or unactionable. However, traditional features often fail to represent the deep syntactic structure of alarms. To bridge the gap between programs’ syntactic structure and defect identification features, this paper first extracts a set of novel fine-grained features at variable-level, called path-variable characteristic, by applying path analysis techniques in the feature extraction process. We then raise a two-stage transfer learning approach based on our proposed features, called feature ranking-matching based transfer learning, to increase the performance of cross-project defect identification. Our experimental results for eight open-source projects show that the proposed features at variable-level are promising and can yield significant improvement on both within-project and cross-project defect identification. © 2020 The Author(s)","Automated defect identification; Machine learning; Model evaluation; Path analysis; Transfer learning"
"A novel Security-by-Design methodology: Modeling and assessing security by SLAs with a quantitative approach","2020","Journal of Systems and Software","10.1016/j.jss.2020.110537","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079004147&doi=10.1016%2fj.jss.2020.110537&partnerID=40&md5=f5928b336718e374a42157b1caf9167d","Recent software development methodologies, as DevOps or Agile, are very popular and widely used, especially for the development of cloud services and applications. They dramatically reduce the time-to-market of developed software but, at the same time, they can be hardly integrated with security design and risk management methodologies. These cannot be easily automated and require big economic investments, due to the necessity of security experts in the development team and to the lack of automatic tools to evaluate risk and to assess security in the design and operation phases. This paper presents a novel Security-by-Design methodology based on Security Service Level Agreements (SLAs), which can be integrated within modern development processes and that is able to support the risk management life-cycle in an almost-completely automated way. In particular, it relies upon a guided risk analysis process and a completely automated security assessment phase, which enable to assess the security properties granted by a cloud application and to report them in a Security SLA. We validated the proposed methodology with respect to a real case study, which showed its effectiveness in improving the awareness of designer and developer teams on security aspects and in reducing the secure design process time. © 2020 Elsevier Inc.","Secure Cloud Application Development; Security Assessment; Security metrics; Security models; Security Service Level Agreement; Security-by-Design methodologies"
"An automatically created novel bug dataset and its validation in bug prediction","2020","Journal of Systems and Software","10.1016/j.jss.2020.110691","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086642369&doi=10.1016%2fj.jss.2020.110691&partnerID=40&md5=6e52c4da63beda2bf2629ecc658adb6c","Bugs are inescapable during software development due to frequent code changes, tight deadlines, etc.; therefore, it is important to have tools to find these errors. One way of performing bug identification is to analyze the characteristics of buggy source code elements from the past and predict the present ones based on the same characteristics, using e.g. machine learning models. To support model building tasks, code elements and their characteristics are collected in so-called bug datasets which serve as the input for learning. We present the BugHunter Dataset: a novel kind of automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information. Other available bug datasets follow the traditional approach of gathering the characteristics of all source code elements (buggy and non-buggy) at only one or more pre-selected release versions of the code. Our approach, on the other hand, captures the buggy and the fixed states of the same source code elements from the narrowest timeframe we can identify for a bug's presence, regardless of release versions. To show the usefulness of the new dataset, we built and evaluated bug prediction models and achieved F-measure values over 0.74. © 2020 The Authors","Bug dataset; Bug prediction; Code metrics; GitHub; Machine learning; Static code analysis"
"Descriptions of issues and comments for predicting issue success in software projects","2020","Journal of Systems and Software","10.1016/j.jss.2020.110663","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085566995&doi=10.1016%2fj.jss.2020.110663&partnerID=40&md5=a435e374cfe286459ac27eba3647e1db","Software development tasks must be performed successfully to achieve software quality and customer satisfaction. Knowing whether software tasks are likely to fail is essential to ensure the success of software projects. Issue Tracking Systems store information of software tasks (issues) and comments, which can be useful to predict issue success; however; almost no research on this topic exists. This work studies the usefulness of textual descriptions of issues and comments for predicting whether issues will be resolved successfully or not. Issues and comments of 588 software projects were extracted from four popular Issue Tracking Systems. Seven machine learning classifiers were trained on 30k issues and more than 120k comments, and more than 6000 experiments were performed to predict the success of three types of issues: bugs, improvements and new features. The results provided evidence that descriptions of issues and comments are useful for predicting issue success with more than 85% of accuracy and precision, and that the predictions of issue success vary over time. Words related to software development were particularly relevant for predicting issue success. Other communication aspects and their relationship to the success of software projects must be researched in detail using data from software tools. © 2020 Elsevier Inc.","Issue success; Issue Tracking System; Machine learning; Software development; Software project"
"An effective formulation of the multi-criteria test suite minimization problem","2020","Journal of Systems and Software","10.1016/j.jss.2020.110632","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084937967&doi=10.1016%2fj.jss.2020.110632&partnerID=40&md5=314383c87aa5e5d825c79ec49173cc63","Test suite minimization problem has been mainly addressed by employing heuristic techniques or integer linear programming focusing on a specific criterion or bi-criteria. These approaches fall short to compute optimal solutions especially when there exists overlap among test cases in terms of various criteria such as code coverage and the set of detected faults. Nonlinear formulations have also been proposed recently to address such cases. However, these formulations require significantly more computational resources compared to linear ones. Moreover, they are also subject to shortcomings that might still lead to sub-optimal solutions. In this paper, we identify such shortcomings and we propose an alternative formulation of the problem. We have empirically evaluated the effectiveness of our approach based on a publicly available dataset and compared it with respect to the state-of-the-art based on the same objective function and the same set of criteria including statement coverage, fault-revealing capability, and test execution time. Results show that our formulation leads to either better results or the same results, when the previously obtained results were already the optimal ones. In addition, our formulation is a linear formulation, which can be solved much more efficiently compared to non-linear formulations. © 2020 Elsevier Inc.","Integer programming; Multi-objective optimization; Regression testing; Software testing; Test suite minimization"
"The effects of database complexity on SQL query formulation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082870101&doi=10.1016%2fj.jss.2020.110576&partnerID=40&md5=b380935152f9c9a471082c4af7ad4bdb","In Structured Query Language (SQL) education, students often execute queries against a simple exercise database. Recently, databases that are more realistic have been utilized to the effect that students find exercises more interesting and useful, as these databases more accurately mimic databases students are likely to encounter in their future work environments. However, using even the most engaging database can be counterproductive to learning, if a student is not able to formulate correct queries due to the complexity of the database schema. Scientific evidence on the effects of database complexity on student's query formulation is limited, and with queries from 744 students against three databases of varying logical complexity, we set out to study how database complexity affects the success rates in query formulation. The success rates against a simple database were significantly higher than against a semi-complex and a complex database, which indicates that it is easier for students to write SQL queries against simpler databases. This suggests, at least in the scale of our exercise databases, that educators should also consider the negative effects of more realistic databases, even though they have been shown to increase student engagement. © 2020 Elsevier Inc.","Database; Database complexity; Education; Structured query language (SQL); Student learning"
"An ontology-based learning approach for automatically classifying security requirements","2020","Journal of Systems and Software","10.1016/j.jss.2020.110566","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081157908&doi=10.1016%2fj.jss.2020.110566&partnerID=40&md5=816b60db31640a3b8165a4ac22bd00d5","Although academia has recognized the importance of explicitly specifying security requirements in early stages of system developments for years, in reality, many projects mix security requirements with other types of requirements. Thus, there is a strong need for precisely and efficiently classifying such security requirements from other requirements in requirement specifications. Existing studies leverage lexical evidence to build probabilistic classifiers, which are domain-dependent by design and cannot effectively classify security requirements from different application domains. In this paper, we propose an ontology-driven learning approach to automatically classify security requirements. Our approach consists of a conceptual layer and a linguistic layer, which understands security requirements based on not only lexical evidence but also conceptual domain knowledge. In particular, we apply a systematic approach to identify linguistic features of security requirements based on an extended security requirements ontology and linguistic knowledge, connecting the conceptual layer with the linguistic layer. Such linguistic features are then used to train domain-independent security requirements classifiers by using machine learning techniques. We have carried out a series of experiments to evaluate the performance and generalization ability of our proposal against existing approaches. The results of the experiments show that the proposed approach outperforms existing approaches with a significant increase of F1 score (0.63 VS. 0.44) when the training dataset and the testing dataset come from different application domains, i.e., the classifiers trained by our approach can be generalized to classify security requirements from different domains. © 2020 Elsevier Inc.","linguistic pattern; machine learning; natural language processing; security requirements classification; security requirements ontology"
"Optimal test activity allocation for covariate software reliability and security models","2020","Journal of Systems and Software","10.1016/j.jss.2020.110643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084804341&doi=10.1016%2fj.jss.2020.110643&partnerID=40&md5=f67d542c5a4ce6ac93766203b1240344","Traditional software reliability growth models enable quantitative assessment of the software testing process by characterizing the fault detection in terms of testing time or effort. However, the majority of these models do not identify specific testing activities underlying fault discovery and thus can only provide limited guidance on how to incrementally allocate effort. Although there are several novel studies focused on covariate software reliability growth models, they are limited to model development, application, and assessment. This paper presents a non-homogeneous Poisson process software reliability growth model incorporating covariates based on the discrete Cox proportional hazards model. An efficient and stable expectation conditional maximization algorithm is applied to identify the model parameters. An optimal test activity allocation problem is formulated to maximize fault discovery. The proposed method is illustrated through numerical examples on two data sets. © 2020 Elsevier Inc.","Covariates; Non-homogeneous Poisson process; Proportional hazards model; Software reliability growth model; Software security; Test activity allocation"
"GAP: Forecasting commit activity in git projects","2020","Journal of Systems and Software","10.1016/j.jss.2020.110573","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082019288&doi=10.1016%2fj.jss.2020.110573&partnerID=40&md5=e98fd68894be58e1beb603302b28a430","Abandonment of active developers poses a significant risk for many open source software projects. This risk can be reduced by forecasting the future activity of contributors involved in such projects. Focusing on the commit activity of individuals involved in git repositories, this paper proposes a practicable probabilistic forecasting model based on the statistical technique of survival analysis. The model is empirically validated on a wide variety of projects accounting for 7528 git repositories and 5947 active contributors. We found that a model based on the last 20 observed days of commit activity per contributor provides the best concordance. We also found that the predictions provided by the model are generally close to actual observations, with slight underestimations for low probability predictions and slight overestimations for higher probability predictions. This model is implemented as part of an open source tool, called GAP, that predicts future commit activity. © 2020 Elsevier Inc.","Commit activity; Developer abandonment; Distributed software development; Git; Prediction model"
"Enhancing example-based code search with functional semantics","2020","Journal of Systems and Software","10.1016/j.jss.2020.110568","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081654312&doi=10.1016%2fj.jss.2020.110568&partnerID=40&md5=56932d995e27fc82bb3c51b6498423cf","As the quality and quantity of open source code increase, effective and efficient search for code implementing certain semantics, or semantics-based code search, has become an emerging need for software developers to retrieve and reuse existing source code. Previous techniques in semantics-based code search encode the semantics of loop-free Java code snippets as constraints and utilize an SMT solver to find encoded snippets that match an input/output (IO) query. We present in this article the Quebio approach to semantics-based search for Java methods. Quebio advances the state-of-the-art by supporting important language features like invocation to library APIs and enabling the search to handle more data types like array/List, Set, and Map. Compared with existing approaches, Quebio also integrates a customized keyword-based search that uses as the input a textual, behavioral summary of the desired methods to quickly prune the methods to be checked against the IO examples. To evaluate the effectiveness and efficiency of Quebio, we constructed a repository of 14,792 methods from 723 open source Java projects hosted on GitHub and applied the approach to resolve 47 queries extracted from StackOverflow. Quebio was able to find methods correctly implementing the specified IO behaviors for 43 of the queries, significantly outperforming the existing semantics-based code search techniques. The average search time with Quebio was 213.2 seconds for each query. © 2020 Elsevier Inc.","Semantics-based code search; SMT solver; Symbolic analysis"
"In-the-field monitoring of functional calls: Is it feasible?","2020","Journal of Systems and Software","10.1016/j.jss.2020.110523","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078115145&doi=10.1016%2fj.jss.2020.110523&partnerID=40&md5=1917cd14ed8370a3a3ac9bc3b818ddc4","Collecting data about the sequences of function calls executed by an application while running in the field can be useful to a number of applications, including failure reproduction, profiling, and debugging. Unfortunately, collecting data from the field may introduce annoying slowdowns that negatively affect the quality of the user experience. So far, the impact of monitoring has been mainly studied in terms of the overhead that it may introduce in the monitored applications, rather than considering if the introduced overhead can be really recognized by users. In this paper we take a different perspective studying to what extent collecting data about sequences of function calls may impact the quality of the user experience, producing recognizable effects. Interestingly we found that, depending on the nature of the executed operation and its execution context, users may tolerate a non-trivial overhead. This information can be potentially exploited to collect significant amount of data without annoying users. © 2020","Dynamic analysis; Monitoring; User experience"
"A federated society of bots for smart contract testing","2020","Journal of Systems and Software","10.1016/j.jss.2020.110647","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095476326&doi=10.1016%2fj.jss.2020.110647&partnerID=40&md5=c1a9920aba51185bb746f6f625869c69","Smart contracts are a new type of software that allows its users to perform irreversible transactions on a distributed persistent data storage called the blockchain. The nature of such contracts and the technical details of the blockchain architecture give raise to new kinds of faults, which require specific test behaviours to be exposed. In this paper we present SOCRATES, a generic and extensible framework to test smart contracts running in a blockchain. The key properties of SOCRATES are: (1) it comprises bots that interact with the blockchain according to a set of composable behaviours; (2) it can instantiate a society of bots, which can trigger faults due to multi-user interactions that are impossible to expose with a single bot. Our experimental results show that SOCRATES can expose known faults and detect previously unknown faults in contracts currently published in the Ethereum blockchain. They also show that a society of bots is often more effective than a single bot in fault exposure. © 2020 Elsevier Inc.","Blockchain; Smart contracts; Software testing"
"The impact factors on the performance of machine learning-based vulnerability detection: A comparative study","2020","Journal of Systems and Software","10.1016/j.jss.2020.110659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086430077&doi=10.1016%2fj.jss.2020.110659&partnerID=40&md5=fee3f88f5fae1d3bc3b2ab6bb98620fd","Machine learning-based Vulnerability detection is an active research topic in software security. Different traditional machine learning-based and deep learning-based vulnerability detection methods have been proposed. To our best knowledge, we are the first to identify four impact factors and conduct a comparative study to investigate the performance influence of these factors. In particular, the quality of datasets, classification models and vectorization methods can directly affect the detection performance, in contrast function/variable name replacement can affect the features of vulnerability detection and indirectly affect the performance. We collect three different vulnerability code datasets from two various sources (i.e., NVD and SARD). These datasets can correspond to different types of vulnerabilities. Moreover, we extract and analyze the features of vulnerability code datasets to explain some experimental results. Our findings based on the experimental results can be summarized as follows: (1) Deep learning models can achieve better performance than traditional machine learning models. Of all the models, BLSTM can achieve the best performance. (2) CountVectorizer can significantly improve the performance of traditional machine learning models. (3) Features generated by the random forest algorithm include system-related functions, syntax keywords, and user-defined names. Different vulnerability types and code sources will generate different features. (4) Datasets with user-defined variable and function name replacement will decrease the performance of vulnerability detection. (5) As the proportion of code from SARD increases, the performance of vulnerability detection will increase. © 2020 Elsevier Inc.","Comparative study; Deep learning; Feature extraction; Machine learning; Vulnerability detection"
"Transformed k-nearest neighborhood output distance minimization for predicting the defect density of software projects","2020","Journal of Systems and Software","10.1016/j.jss.2020.110592","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084173623&doi=10.1016%2fj.jss.2020.110592&partnerID=40&md5=c73ca715e2124781f8a410c73385362c","Background: Software defect prediction is one of the most important research topics in software engineering. An important product measure to determine the effectiveness of software processes is the defect density (DD). Cased-based reasoning (CBR) has been the prediction technique most widely applied in the software prediction field. The CBR involves k-nearest neighborhood for finding the number (k) of similar software projects selected to be involved in the prediction process. Objective: To propose the application of a transformed k-nearest neighborhood output distance minimization (TkDM) algorithm to predict the DD of software projects to compare its prediction accuracy with those obtained from statistical regression, support vector regression, and neural networks. Method: Data sets were obtained from the ISBSG release 2018. A leave-one-out cross validation method was performed. Absolute residual was used as the prediction accuracy criterion for models. Results: Statistical significance tests among models showed that the TkDM had the best prediction accuracy than those ones from statistical regression, support vector regression, and neural networks. Conclusions: A TkDM can be used for predicting the DD of new and enhanced software projects developed and coded in specific platforms and programming languages types. © 2020 Elsevier Inc.","Case-based reasoning; ISBSG; Neural networks; Software defect density prediction; Support vector regression; Transformed k-nearest neighborhood output distance minimization"
"REPD: Source code defect prediction as anomaly detection","2020","Journal of Systems and Software","10.1016/j.jss.2020.110641","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084807737&doi=10.1016%2fj.jss.2020.110641&partnerID=40&md5=f5b0e3fbe91d227e4e89102433cab376","In this paper, we present a novel approach for within-project source code defect prediction. Since defect prediction datasets are typically imbalanced, and there are few defective examples, we treat defect prediction as anomaly detection. We present our Reconstruction Error Probability Distribution (REPD) model which can handle point and collective anomalies. We compare it on five different traditional code feature datasets against five models: Gaussian Naive Bayes, logistic regression, k-nearest-neighbors, decision tree, and Hybrid SMOTE-Ensemble. In addition, REPD is compared on 24 semantic features datasets against previously mentioned models. In order to compare the performance of competing models, we utilize F1-score measure. By using statistical means, we show that our model produces significantly better results, improving F1-score up to 7.12%. Additionally, REPD's robustness to dataset imbalance is analyzed by creating defect undersampled and non-defect oversampled datasets. © 2020 Elsevier Inc.","Anomaly detection; Defect prediction; Program analysis; REPD"
"FCCI: A fuzzy expert system for identifying coincidental correct test cases","2020","Journal of Systems and Software","10.1016/j.jss.2020.110635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084802206&doi=10.1016%2fj.jss.2020.110635&partnerID=40&md5=dbbc9ad501d60c89bb29ebf90f60575d","Spectrum-based fault localization (SBFL) is a promising approach to reduce the cost of program debugging and there has been a large body of research on introducing effective SBFL techniques. However, performance of these techniques can be adversely affected by the existence of coincidental correct (CC) test cases in the test suites. Such test cases execute the faulty statement but do not cause failures. Given that coincidental correctness is prevalent, it is necessary to precisely identify CC test cases and eliminate their effects from test suites. To do so, in this paper, we propose several important factors to identify CC test cases and model the CC identification process as a decision making system by constructing a fuzzy expert system and proposing a novel fuzzy CC identification method, namely FCCI. FCCI estimates the CC likelihood of passed test cases using the designed fuzzy rules, which effectively correlate the proposed CC identification factors. We evaluated FCCI by conducting extensive experiments on 17 popular and open source subject programs ranging from small- to large-scale containing both artificial and real faults. The experimental results indicate that FCCI successfully improves the accuracy of the CC identification as well as the accuracy of the representative SBFL techniques. © 2020 Elsevier Inc.","Coincidentally correct test cases; Fuzzy expert system; Software debugging; Spectrum-based fault localization"
"Correctness checking for BPMN collaborations with sub-processes","2020","Journal of Systems and Software","10.1016/j.jss.2020.110594","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083821136&doi=10.1016%2fj.jss.2020.110594&partnerID=40&md5=aae281d81008b9abe295bdccbf0f0692","BPMN collaboration models are commonly used to describe the behaviour and interactions of processes in an inter-organisational context. An important role in this kind of models is played both by the message flow, and by sub-processes. The interplay between these features of BPMN models can conceal subtle or unexpected effects, which makes the design activity error-prone, thus leading to the possible inclusion of incorrect behaviour. In this paper, we face this problem by providing a framework for checking the correctness of BPMN models. In particular we are interested on collaboration models that include message exchange and/or sub-processes, and with a special focus on properties well-established in the business process domain, namely safeness and soundness. To enable such a verification, we have (i) defined an operational semantics for BPMN collaborations, (ii) formalised safeness and soundness properties, and a new relaxed version of soundness for detecting situations where asynchronous messages are not handled correctly by the receiver, (iii) applied the related checks on state-space representations (i.e., labelled transition systems) of collaborations, and (iv) implemented the overall formal framework that has been also integrated in the Camunda modelling environment. The resulting verification framework and tool, named S3, have been validated in relation to its effectiveness, efficiency and usability, both by using models available on a publicly accessible repository, and by carrying out experiments with a group of designers. © 2020 Elsevier Inc.","BPMN 2.0; Collaborations; Formal verification; Message flow; Sub-Processes"
"ASPLe: A methodology to develop self-adaptive software systems with systematic reuse","2020","Journal of Systems and Software","10.1016/j.jss.2020.110626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084955201&doi=10.1016%2fj.jss.2020.110626&partnerID=40&md5=26781baf75679b07712b33cfab41a69e","More than two decades of research have demonstrated an increasing need for software systems to be self-adaptive. Self-adaptation manages runtime dynamics, which are difficult to predict before deployment. A vast body of knowledge to develop Self-Adaptive Software Systems (SASS) has been established. However, we discovered a lack of process support to develop self-adaptive systems with reuse. The lack of process support may hinder knowledge transfer and quality design. To that end, we propose a domain-engineering based methodology, Autonomic Software Product Lines engineering (ASPLe), which provides step-by-step guidelines for developing families of SASS with systematic reuse. The evaluation results from a case study show positive effects on quality and reuse for self-adaptive systems designed using the ASPLe compared to state-of-the-art engineering practices. © 2020 The Author(s)","Domain engineering; Self-Adaptation; Software design; Software reuse; Uncertainty; Variability"
"When to update systematic literature reviews in software engineering","2020","Journal of Systems and Software","10.1016/j.jss.2020.110607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084732557&doi=10.1016%2fj.jss.2020.110607&partnerID=40&md5=a86cb9f26f52c3c7da16d6ede96ca73d","[Context] Systematic Literature Reviews (SLRs) have been adopted by the Software Engineering (SE) community for approximately 15 years to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially outdated, and there are no systematic proposals on when to update SLRs in SE. [Objective] The goal of this paper is to provide recommendations on when to update SLRs in SE. [Method] We evaluated, using a three-step approach, a third-party decision framework (3PDF) employed in other fields, to decide whether SLRs need updating. First, we conducted a literature review of SLR updates in SE and contacted the authors to obtain their feedback relating to the usefulness of the 3PDF within the context of SLR updates in SE. Second, we used these authors’ feedback to see whether the framework needed any adaptation; none was suggested. Third, we applied the 3PDF to the SLR updates identified in our literature review. [Results] The 3PDF showed that 14 of the 20 SLRs did not need updating. This supports the use of a decision support mechanism (such as the 3PDF) to help the SE community decide when to update SLRs. [Conclusions] We put forward that the 3PDF should be adopted by the SE community to keep relevant evidence up to date and to avoid wasting effort with unnecessary updates. © 2020","Software engineering; Systematic literature review update; Systematic literature reviews"
"Architecture design evaluation of PaaS cloud applications using generated prototypes: PaaSArch Cloud Prototyper tool","2020","Journal of Systems and Software","10.1016/j.jss.2020.110701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087327505&doi=10.1016%2fj.jss.2020.110701&partnerID=40&md5=c7526948e987d1ad7ec137cfb96df1e2","Platform as a Service (PaaS) cloud domain brings great benefits of an elastic platform with many prefabricated services, but at the same time challenges software architects who need to navigate a rich set of services, variability of PaaS cloud environment and quality conflicts in existing design tactics, which makes it almost impossible to foresee the impact of architectural design decisions on the overall application quality without time-consuming implementation of application prototypes. To ease the architecture design of PaaS cloud applications, this paper proposes a design-time quality evaluation approach for PaaS cloud applications based on automatically generated prototypes, which are deployed to the cloud and repeatedly evaluated in the context of multiple quality attributes and environment configurations. In this paper, all steps of the approach are described and demonstrated on an example of a real-world complex IoT system for collection and processing of Smart Home sensor data. The approach has been implemented and the automated prototype generation and evaluation tool, referred to as PaaSArch Cloud Prototyper, is presented together with the approach. © 2020 Elsevier Inc.","Cloud computing; Internet of things; Performance; Prototype generation; Quality evaluation; Software architecture design"
"Using source code density to improve the accuracy of automatic commit classification into maintenance activities","2020","Journal of Systems and Software","10.1016/j.jss.2020.110673","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085726544&doi=10.1016%2fj.jss.2020.110673&partnerID=40&md5=cd86319a997ed9d56444c550e38bf011","Source code is changed for a reason, e.g., to adapt, correct, or adapt it. This reason can provide valuable insight into the development process but is rarely explicitly documented when the change is committed to a source code repository. Automatic commit classification uses features extracted from commits to estimate this reason. We introduce source code density, a measure of the net size of a commit, and show how it improves the accuracy of automatic commit classification compared to previous size-based classifications. We also investigate how preceding generations of commits affect the class of a commit, and whether taking the code density of previous commits into account can improve the accuracy further. We achieve up to 89% accuracy and a Kappa of 0.82 for the cross-project commit classification where the model is trained on one project and applied to other projects. Models trained on single projects yield accuracies of up to 93% with a Kappa approaching 0.90. The accuracy of the automatic commit classification has a direct impact on software (process) quality analyses that exploit the classification, so our improvements to the accuracy will also improve the confidence in such analyses. © 2020 Elsevier Inc.","Commit classification; Maintenance activities; Software evolution; Software quality; Source code density"
"Job-work fit as a determinant of the acceptance of large-scale agile methodology","2020","Journal of Systems and Software","10.1016/j.jss.2020.110577","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085922375&doi=10.1016%2fj.jss.2020.110577&partnerID=40&md5=58cdc964aa34dd024d5a2f5eb61cf22f","The declaration of the Agile Manifesto in 2001 was hailed as a paradigmatic shift in the development of software systems. Initially, agile development was typically deployed in projects with a single team with fewer than ten members. Recently, a new kind of agile development has emerged for larger projects that may be referred to as large-scale agile methodology (LSAM). The taxonomy of LSAM is more enhanced and subsumes the Agile Manifesto. Based on the software development literature, the study investigates five important antecedents of methodology acceptance: perceived usefulness, compatibility, subjective norm, mandatoriness, and external support. As a result of the initial PLS analysis, the study introduces the notion of Job-Work Fit as a second-order construct composed of perceived usefulness and compatibility. Based on a survey of 123 respondents, the study finds that the construct job-work fit significantly explains the LSAM acceptance for large projects. Job-work fit also mediates the relationships between subjective norm and external support with LSAM acceptance. This research suggests that as the stakes become higher because of project size and agility, job-work fit emerges as the central determinant of LSAM acceptance. © 2020 Elsevier Inc.","Compatibility; External support; Large-scale agile methodology; Mandatoriness; Perceived usefulness; Subjective norm"
"Cloud reliability and efficiency improvement via failure risk based proactive actions","2020","Journal of Systems and Software","10.1016/j.jss.2020.110524","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078492488&doi=10.1016%2fj.jss.2020.110524&partnerID=40&md5=5d2cfea6e53e6822f3e37d4170f5e910","Due to the huge magnitude and complexity of cloud computing systems (CCS), failures are inevitable, which lead to reliability and efficiency losses. Failure mitigation, fault tolerance, and recovery actions can be performed to improve CCS reliability and efficiency. Using data collected during CCS operation, failure prediction and risk identification techniques could anticipate such failure occurrences. In this paper, we develop a framework to combine risk identification with follow-up proactive actions for CCS reliability and efficiency improvement. We start by analyzing cloud failures and the related operational data. Then a tree based predictive model is trained to diagnose high risk cloud tasks. By proactively terminating these high risk tasks, both the number of CCS failures and the resource consumption could be significantly reduced. The impact of these proactive actions can be simulated to quantify the improvement to both system reliability and efficiency. The new approach has been applied on the Google cluster dataset, covering approximately 400GB of operational data over 29 consecutive days, to demonstrate its viability and effectiveness. © 2020","Cloud computing system; Efficiency; Failure mitigation and fault tolerance; Reliability; Risk identification"
"PRISE: A process to support iStar extensions","2020","Journal of Systems and Software","10.1016/j.jss.2020.110649","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083758805&doi=10.1016%2fj.jss.2020.110649&partnerID=40&md5=0fe0875ecd3851ccf444aae0885347dd","iStar is a goal-based requirement modelling language, being used both in industrial and academic projects of different domains. Often the language is extended to incorporate new constructs related to an application domain or to adjust it to practical situations during requirements modelling. These iStar extensions have been proposed in an ad hoc way resulting in many problems of incompleteness, inconsistency and conflicts. Recently, the language was standardised, but it continues being extended. Thus, we consider that this is an adequate moment to study how to support the proposals of the next iStar extensions. In this paper, we define PRISE, a process to support the creation of iStar extensions which is driven by model-based development concepts, reuse of existing iStar extensions and guidelines of experts. This process can be customised. We illustrate the usage of PRISE by recreating five existing iStar extensions. Finally, we evaluated PRISE with interviews and a survey with experts; and, we performed an interview to analyse the opinion about the usage of the PRISE to create a new iStar extension by a novice. The evaluation and validation indicate good results to avoid problems and increase the quality of the proposals and well receptivity by the experts and novice. © 2020 Elsevier Inc.","Extension; Goal modelling; iStar; Modelling language; Process"
"Capturing software architecture knowledge for pattern-driven design","2020","Journal of Systems and Software","10.1016/j.jss.2020.110714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087589493&doi=10.1016%2fj.jss.2020.110714&partnerID=40&md5=c855eff3f9c3be34a90363fb1676a854","Context: Software architecture is a knowledge-intensive field. One mechanism for storing architecture knowledge is the recognition and description of architectural patterns. Selecting architectural patterns is a challenging task for software architects, as knowledge about these patterns is scattered among a wide range of literature. Method: We report on a systematic literature review, intending to build a decision model for the architectural pattern selection problem. Moreover, twelve experienced practitioners at software-producing organizations evaluated the usability and usefulness of the extracted knowledge. Results: An overview is provided of 29 patterns and their effects on 40 quality attributes. Furthermore, we report in which systems the 29 patterns are applied and in which combinations. The practitioners confirmed that architectural knowledge supports software architects with their decision-making process to select a set of patterns for a new problem. We investigate the potential trends among architects to select patterns. Conclusion: With the knowledge available, architects can more rapidly select and eliminate combinations of patterns to design solutions. Having this knowledge readily available supports software architects in making more efficient and effective design decisions that meet their quality concerns. © 2020 Elsevier Inc.","Architectural patterns; Architectural styles; Design decisions; Knowledge acquisition; Quality attributes"
"Identifying vulnerabilities of SSL/TLS certificate verification in Android apps with static and dynamic analysis","2020","Journal of Systems and Software","10.1016/j.jss.2020.110609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083770002&doi=10.1016%2fj.jss.2020.110609&partnerID=40&md5=b6fc0dc4743fa0c1a34c012c53e3dc2c","Many Android developers fail to properly implement SSL/TLS during the development of an app, which may result in Man-In-The-Middle (MITM) attacks or phishing attacks. In this work, we design and implement a tool called DCDroid to detect these vulnerabilities with the combination of static and dynamic analysis. In static analysis, we focus on four types of vulnerable schema and locate the potential vulnerable code snippets in apps. In dynamic analysis, we prioritize the triggering of User Interface (UI) components based on the results obtained with static analysis to confirm the misuse of SSL/TLS. With DCDroid we analyze 2213 apps from Google Play and 360app. The experimental results show that 457 (20.65%) apps contain potential vulnerable code. We run apps with DCDroid on two Android smart phones and confirm that 245 (11.07%) of 2213 apps are truly vulnerable to MITM and phishing attacks. We propose several strategies to reduce the number of crashes and shorten the execution time in dynamic analysis. Comparing with our previous work, DCDroid decreases 57.18% of the number of apps’ crash and 32.47% of the execution time on average. It also outperforms other three tools, namely, AndroBugs, kingkong and appscan, in terms of detection accuracy. © 2020","Android security; Dynamic analysis; MITM; SSL/TLS; Static analysis; Vulnerability detection"
"ARC: Anomaly-aware Robust Cloud-integrated IoT service composition based on uncertainty in advertised quality of service values","2020","Journal of Systems and Software","10.1016/j.jss.2020.110557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079843005&doi=10.1016%2fj.jss.2020.110557&partnerID=40&md5=8743726a797ee731d4e409fc55eb2f95","From the IoT perspective, each intelligent device can be considered as a potential source of service. Since several services perform the same function, albeit with different quality of service (QoS) parameters, service composition becomes a crucial problem to find an optimal set of services to automate a typical business process. The majority of prior research has investigated the service composition problem with the assumption that advertised QoS values are deterministic and do not change over time. However, factors like sensors failure and network topology changes cause uncertainty in the advertised QoS values. To address this challenge, we propose a novel Anomaly-aware Robust service Composition (ARC) to deal with the problem of uncertainty of QoS values in a dynamic environment of Cloud and IoT. The proposed approach uses Bertsimas and Sim mathematical robust optimization method, which is independent of the statistical distribution of QoS values, to compose services. Moreover, our approach exploits a machine learning-based anomaly detection technique to improve the stability of the solution with a fine-grained identification of abnormal QoS records. The results demonstrate that our approach achieves 14.55% of the average improvement in finding optimal solutions compared to the previous works, such as information theory-based and clustering-based methods. © 2020","Anomaly detection; Cloud Computing; IoT; Robust optimization; Service composition; Uncertainty"
"CODE reuse in practice: Benefiting or harming technical debt","2020","Journal of Systems and Software","10.1016/j.jss.2020.110618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085600238&doi=10.1016%2fj.jss.2020.110618&partnerID=40&md5=d1aeb8bef87fa072c6e747cdc8606cbf","During the last years the TD community is striving to offer methods and tools for reducing the amount of TD, but also understand the underlying concepts. One popular practice that still has not been investigated in the context of TD, is software reuse. The aim of this paper is to investigate the relation between white-box code reuse and TD principal and interest. In particular, we target at unveiling if the reuse of code can lead to software with better levels of TD. To achieve this goal, we performed a case study on approximately 400 OSS systems, comprised of 897 thousand classes, and compare the levels of TD for reused and natively-written classes. The results of the study suggest that reused code usually has less TD interest; however, the amount of principal in them is higher. A synthesized view of the aforementioned results suggest that software engineers shall opt to reuse code when necessary, since apart from the established reuse benefits (i.e., cost savings, increased productivity, etc.) are also getting benefits in terms of maintenance. Apart from understanding the phenomenon per se, the results of this study provide various implications to research and practice. © 2020 Elsevier Inc.","Case study; Reuse; Technical debt"
"Adaptive metamorphic testing with contextual bandits","2020","Journal of Systems and Software","10.1016/j.jss.2020.110574","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082018209&doi=10.1016%2fj.jss.2020.110574&partnerID=40&md5=1bba19d8ad9ed4a856cca7eb1be80621","Metamorphic Testing is a software testing paradigm which aims at using necessary properties of a system under test, called metamorphic relations, to either check its expected outputs, or to generate new test cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or to test programs for which there are uncertainties on expected outputs such as learning systems. In this article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic relations are likely to transform a source test case, such that it has higher chance to discover faults. We present experimental results over two major case studies in machine learning, namely image classification and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing efficiently identifies weaknesses of the tested systems in context of the source test case. © 2020 Elsevier Inc.","Contextual bandits; Machine learning; Metamorphic testing; Software testing"
"Examining the reuse potentials of IoT application frameworks","2020","Journal of Systems and Software","10.1016/j.jss.2020.110706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087202176&doi=10.1016%2fj.jss.2020.110706&partnerID=40&md5=1ff619129eeb34044e8c0f7464da0e1a","The major challenge that a developer confronts when building IoT systems is the management of a plethora of technologies implemented with various constraints, from different manufacturers, that at the end need to cooperate. In this paper we argue that developers can benefit from IoT frameworks by reusing their components so as to build in less time and effort IoT systems that can easily integrate new technologies. In order to explore the reuse opportunities offered by IoT frameworks we have performed a case study and analyzed 503 components reused by 35 IoT projects. We examined (a) the types of functionality that are most facilitated for reuse (b) the reuse strategy that is most adopted (c) thequality of the reused components. The results of the case study suggest that the main functionality reused is the one related to the Device Management layer and that Black-box reuse is the main type. Moreover, the quality of the reused components is improved compared to the rest of the components built from scratch. © 2020","Black-box reuse; IoT applications; Reusability; White-box reuse effort estimation"
"A large empirical assessment of the role of data balancing in machine-learning-based code smell detection","2020","Journal of Systems and Software","10.1016/j.jss.2020.110693","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086465273&doi=10.1016%2fj.jss.2020.110693&partnerID=40&md5=c52d1386d8458354f2e436c3ae9006dc","Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics is used to detect smelly code components. However, these techniques suffer from subjective interpretations, a low agreement between detectors, and threshold dependability. To overcome these limitations, previous work applied Machine-Learning that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine-Learning is not always suitable for code smell detection due to the highly imbalanced nature of the problem. In this study, we investigate five approaches to mitigate data imbalance issues to understand their impact on Machine Learning-based approaches for code smell detection in Object-Oriented systems and those implementing the Model-View-Controller pattern. Our findings show that avoiding balancing does not dramatically impact accuracy. Existing data balancing techniques are inadequate for code smell detection leading to poor accuracy for Machine-Learning-based approaches. Therefore, new metrics to exploit different software characteristics and new techniques to effectively combine them are needed. © 2020 Elsevier Inc.","Code smells; Data balancing; Machine learning; Model view controller; Object oriented"
"Substructure similarity search for engineering service-based systems","2020","Journal of Systems and Software","10.1016/j.jss.2020.110569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081051973&doi=10.1016%2fj.jss.2020.110569&partnerID=40&md5=e5dff790654042d24d02ad75c4f73137","With the broad application of service-oriented architecture in service-oriented software engineering, service-based systems (SBSs) are becoming ever more widely used. As a result, the selection of appropriate component services destined to fulfill the functional requirements becomes a critical challenge for successfully building SBSs, especially when the pre-specified SBS plan involves a complicated structure. Because building an exact SBS is often too restrictive, a similarity search for complex functional requirements becomes an essential operation that must be efficiently supported. We thus investigate in this work the substructure similarity search problem of building a SBS. To solve this new research problem, we propose a feature-based method, called the substructure similarity search for service-based systems (5S), to help users find similar SBS solutions by progressively relaxing a SBS plan. The 5S approach models each SBS as a set of features and transforms the task of relaxation of a SBS into the maximum allowed missing features, which can filter many SBSs directly without costly structure comparisons. 5S thus opens a new paradigm for efficient SBS engineering that shortens the build cycle. Finally, we discuss a series of experiments using real-world Web service datasets that demonstrate the effectiveness and efficiency of the proposed approach. © 2020","Graph matching; Service composition; Service-based system; Substructure similarity search; Web service"
"General framework, opportunities and challenges for crowdsourcing techniques: A Comprehensive survey","2020","Journal of Systems and Software","10.1016/j.jss.2020.110611","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084086399&doi=10.1016%2fj.jss.2020.110611&partnerID=40&md5=48d9f72eb604170131db51a6cceea57b","Crowdsourcing, a distributed human problem-solving paradigm is an active research area which has attracted significant attention in the fields of computer science, business, and information systems. Crowdsourcing holds novelty with advantages like open innovation, scalability, and cost-efficiency. Although considerable research work is performed, however, a survey on the crowdsourcing process-technology has not been divulged yet. In this paper, we present a systematic survey of crowdsourcing in focussing emerging techniques and approaches for improving conventional and developing future crowdsourcing systems. We first present a simplified definition of crowdsourcing. Then, we propose a framework based on three major components, synthesize a wide spectrum of existing studies for various dimensions of the framework. According to the framework, we first introduce the initialization step, including task design, task settings, and incentive mechanisms. Next, in the implementation step, we look into task decomposition, crowd and platform selection, and task assignment. In the last step, we discuss different answer aggregation techniques, validation methods and reward tactics, and reputation management. Finally, we identify open issues and suggest possible research directions for the future. © 2020 Elsevier Inc.","Aggregation; Crowdsourcing; Decomposition; Framework; Incentives; Reputation"
"Regression test case prioritization by code combinations coverage","2020","Journal of Systems and Software","10.1016/j.jss.2020.110712","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087333633&doi=10.1016%2fj.jss.2020.110712&partnerID=40&md5=a2ee8f272a9cd8f5362798a847109569","Regression test case prioritization (RTCP) aims to improve the rate of fault detection by executing more important test cases as early as possible. Various RTCP techniques have been proposed based on different coverage criteria. Among them, a majority of techniques leverage code coverage information to guide the prioritization process, with code units being considered individually, and in isolation. In this paper, we propose a new coverage criterion, code combinations coverage, that combines the concepts of code coverage and combination coverage. We apply this coverage criterion to RTCP, as a new prioritization technique, code combinations coverage based prioritization (CCCP). We report on empirical studies conducted to compare the testing effectiveness and efficiency of CCCP with four popular RTCP techniques: total, additional, adaptive random, and search-based test prioritization. The experimental results show that even when the lowest combination strength is assigned, overall, the CCCP fault detection rates are greater than those of the other four prioritization techniques. The CCCP prioritization costs are also found to be comparable to the additional test prioritization technique. Moreover, our results also show that when the combination strength is increased, CCCP provides higher fault detection rates than the state-of-the-art, regardless of the levels of code coverage. © 2020 Elsevier Inc.","Code combinations coverage; Regression testing; Software testing; Test case prioritization"
"A pattern-based approach to detect and improve non-descriptive test names","2020","Journal of Systems and Software","10.1016/j.jss.2020.110639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084941234&doi=10.1016%2fj.jss.2020.110639&partnerID=40&md5=1f9052ff00a9ca84c603f1bd2bc1795f","Unit tests are an important artifact that supports the software development process in several ways. For example, when a test fails, its name can provide the first step towards understanding the purpose of the test. Unfortunately, unit tests often lack descriptive names. In this paper, we propose a new, pattern-based approach that can help developers improve the quality of test names of JUnit tests by making them more descriptive. It does this by detecting non-descriptive test names and in some cases, providing additional information about how the name can be improved. Our approach was assessed using an empirical evaluation on 34352 JUnit tests. The results of the evaluation show that the approach is feasible, accurate, and useful at discriminating descriptive and non-descriptive names with a 95% true-positive rate. © 2020 Elsevier Inc.","Documentation; Software quality; Software testing"
"Representing software project vision by means of video: A quality model for vision videos","2020","Journal of Systems and Software","10.1016/j.jss.2019.110479","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077532554&doi=10.1016%2fj.jss.2019.110479&partnerID=40&md5=bf6f83aa1aa2935c39a007a8a1313bd2","Establishing a shared software project vision is a key challenge in Requirements Engineering (RE). Several approaches use videos to represent visions. However, these approaches omit how to produce a good video. This missing guidance is one crucial reason why videos are not established in RE. We propose a quality model for videos representing a vision, so-called vision videos. Based on two literature reviews, we elaborate ten quality characteristics of videos and five quality characteristics of visions which together form a quality model for vision videos that includes all 15 quality characteristics. We provide two representations of the quality model: (a) a hierarchical decomposition of vision video quality into the quality characteristics and (b) a mapping of these characteristics to the video production and use process. While the hierarchical decomposition supports the evaluation of vision videos, the mapping provides guidance for video production. In an evaluation with 139 students, we investigated whether the 15 characteristics are related to the overall quality of vision videos perceived by the subjects from a developer's the point of view. Six characteristics (video length, focus, prior knowledge, clarity, pleasure, and stability) correlated significantly with the likelihood that the subjects perceived a vision video as good. These relationships substantiate a fundamental relevance of the proposed quality model. Therefore, we conclude that the quality model is a sound basis for future refinements and extensions. © 2019 Elsevier Inc.","Production; Quality characteristic; Quality model; Video; Vision; Vision video"
"Contextualizing rename decisions using refactorings, commit messages, and data types","2020","Journal of Systems and Software","10.1016/j.jss.2020.110704","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087427538&doi=10.1016%2fj.jss.2020.110704&partnerID=40&md5=8a8bbef13aba5b49ba839997ebf2c067","Identifier names are the atoms of program comprehension. Weak identifier names decrease developer productivity and degrade the performance of automated approaches that leverage identifier names in source code analysis; threatening many of the advantages which stand to be gained from advances in artificial intelligence and machine learning. Therefore, it is vital to support developers in naming and renaming identifiers. In this paper, we extend our prior work, which studies the primary method through which names evolve: rename refactorings. In our prior work, we contextualize rename changes by examining commit messages and other refactorings. In this extension, we further consider data type changes which co-occur with these renames, with a goal of understanding how data type changes influence the structure and semantics of renames. In the long term, the outcomes of this study will be used to support research into: (1) recommending when a rename should be applied, (2) recommending how to rename an identifier, and (3) developing a model that describes how developers mentally synergize names using domain and project knowledge. We provide insights into how our data can support rename recommendation and analysis in the future, and reflect on the significant challenges, highlighted by our study, for future research in recommending renames. © 2020 Elsevier Inc.","Data types; Identifier names; Program comprehension; Refactoring; Rename refactoring"
"An extensive study of class-level and method-level test case selection for continuous integration","2020","Journal of Systems and Software","10.1016/j.jss.2020.110614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085140525&doi=10.1016%2fj.jss.2020.110614&partnerID=40&md5=6c1de8f9869b9bf3b2c5af7ca6c37a7e","Continuous Integration (CI) is an important practice in agile development. With the growth of integration system, running all tests to verify the quality of submitted code, is clearly uneconomical. This paper aims at selecting a proper test subset for continuous integration so as to reduce test cost as much as possible without sacrificing quality. We first propose a static test case selection framework Sapient which aims at selecting a precise test subset towards fully covering all changed code and affected code. There are four major steps: locate changes, build dependency graphs, identify related tests by searching dependency graphs, and extend tests. Based on Sapient, we then develop a class-level test case selection approach FEST and a method-level approach MEST. FEST captures the class-level dependencies, especially the hidden references, which facilitates the modeling of full dependency relations at the class level. MEST combines two dynamic execution rules (i.e., dynamic invocation in reflection and dynamic binding in inheritance) with static dependencies to support building the precise method-level dependencies. Evaluation is conducted on 18 open source projects with 261 continuous integration versions from Eclipse and Apache communities. Results show that both FEST and MEST can detect almost all faults of two baselines (i.e., actual CI testing and ClassSRTS), and find new faults compared with actual CI testing (in 25% and 26% versions) and ClassSRTS (in 18% and 27% versions) respectively. Furthermore, MEST outperforms FEST in reduced test size, fault detection efficiency and test cost, indicating method-level test case selection can be more effective than class-level selection. This study can further speed up the feedback and improve the fault detection efficiency of CI testing, and has good application prospects for the CI testing in large-scale and complex systems. © 2020","Class-level test case selection; Continuous integration; Dynamic execution rules; Method-level test case selection; Program dependencies; Test case selection"
"SEET: Symbolic Execution of ETL Transformations","2020","Journal of Systems and Software","10.1016/j.jss.2020.110675","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086448523&doi=10.1016%2fj.jss.2020.110675&partnerID=40&md5=534236f86ab64dc6207862b69d750cc1","Model transformations are known as the main pillar of model-driven approaches. A model transformation is a program, written in a transformation language, to convert a model into another model or code. Similar to any other program, model transformations need to be verified. The problem is that some transformation errors, e.g., logical errors, can only be detected via execution. Our focus in this research is on the Epsilon Transformation Language (ETL), one of the most extensively used model transformation languages. Lack of approaches to detecting logical errors in ETL transformations is a gap which needs to be addressed. In this paper, we present an approach to symbolic execution of ETL transformations and detecting logical errors. The approach uses a constraint solver to assess the satisfiability of a path condition and generates a symbolic metamodel footprint which can be used to detect errors. The approach is corroborated by a tool that is integrated with Eclipse. To evaluate the approach, the precision and recall are calculated for two well-known case studies. The scalability is evaluated via nine experiments. The usefulness and usability aspects are evaluated in a subjective manner. The results show the improvement in the field of verifying ETL transformations. © 2020 Elsevier Inc.","Epsilon Transformation Language (ETL); Metamodel footprint; Model-Driven Engineering (MDE); Symbolic execution; Verification of model transformations"
"Similarity-based analyses on software applications: A systematic literature review","2020","Journal of Systems and Software","10.1016/j.jss.2020.110669","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086137745&doi=10.1016%2fj.jss.2020.110669&partnerID=40&md5=1998b34bc2c21777901a5735c9247122","In empirical studies on processes, practices, and techniques of software engineering, automation and machine learning are gaining popularity. In order to extract knowledge from existing software projects, a sort of similarity analysis is often performed using different methodologies, data and metadata. This systematic literature review focuses therefore on existing approaches of similarity-, categorization- and relevance-based analysis on software applications. In total, 136 relevant publications and patents were identified between 2002 and 2019 according to the established inclusion and exclusion criteria, which perform a calculation of software similarity in general or to support certain software engineering phases. © 2020 Elsevier Inc.","Machine learning; Secondary study; Software similarity"
"SpongeBugs: Automatically generating fix suggestions in response to static code analysis warnings","2020","Journal of Systems and Software","10.1016/j.jss.2020.110671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085943235&doi=10.1016%2fj.jss.2020.110671&partnerID=40&md5=2ccaf1da93c43d927035cea150335f52","Static code analysis tools such as FindBugs and SonarQube are widely used on open-source and industrial projects to detect a variety of issues that may negatively affect the quality of software. Despite these tools’ popularity and high level of automation, several empirical studies report that developers normally fix only a small fraction (typically, less than 10% (Marcilio et al., 2019) of the reported issues—so-called “warnings”. If these analysis tools could also automatically provide suggestions on how to fix the issues that trigger some of the warnings, their feedback would become more actionable and more directly useful to developers. In this work, we investigate whether it is feasible to automatically generate fix suggestions for common warnings issued by static code analysis tools, and to what extent developers are willing to accept such suggestions into the codebases they are maintaining. To this end, we implemented SpongeBugs, a Java program transformation technique that fixes 11 distinct rules checked by two well-known static code analysis tools (SonarQube and SpotBugs). Fix suggestions are generated automatically based on templates, which are instantiated in a way that removes the source of the warnings; templates for some rules are even capable of producing multi-line patches. Based on the suggestions provided by SpongeBugs, we submitted 38 pull requests, including 946 fixes generated automatically by our technique for various open-source Java projects, including Eclipse UI – a core component of the Eclipse IDE – and both SonarQube and SpotBugs. Project maintainers accepted 87% of our fix suggestions (97% of them without any modifications). We further evaluated the applicability of our technique on software written by students and on a curated collection of bugs. All results indicate that our approach to generating fix suggestions is feasible, flexible, and can help increase the applicability of static code analysis tools. © 2020 Elsevier Inc.","Automatic fix suggestion; Static code analysis"
"Uncertainty modeling and runtime verification for autonomous vehicles driving control: A machine learning-based approach","2020","Journal of Systems and Software","10.1016/j.jss.2020.110617","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084319998&doi=10.1016%2fj.jss.2020.110617&partnerID=40&md5=256802bad9e645660b2b1f31db43a283","Intelligent Transportation Systems (ITS) are attracting much attention from the industry, academia, and government in staging the new generation of transportation. In the coming years, the human-driven vehicles and autonomous vehicles would co-exist for a long time in uncertain environments. How to efficiently control the autonomous vehicle and improve the interaction accuracy as well as the human drivers’ safety is a hot topic for the autonomous industry. The safety-critical nature of the ITSs demands the system designers to provide provably correct guarantees about the actions, models, control, and performance. To model and recognize the drivers’ behavior, we use machine learning classification algorithms based on the data we get from the uncertain environments. We define a parameterized modeling language stohChart(p) (parameterized stochastic hybrid statecharts) to describe the interactions of agents in ITSs. The learning result of the driver behavior classification is transferred to stohChart(p) as the parameters timely. Then we propose a mapping algorithm to transform stohChart(p) to NPTA (Networks of Probabilistic Timed Automata) and use the statistical model checker UPPAAL-SMC to verify the quantitative properties. So the run-time verification method can help autonomous vehicles make “more intelligent” decisions at run-time. We illustrate our approach by modeling and analyzing a scenario of the autonomous vehicle try to change to a lane occupied by a human-driven car. © 2020 Elsevier Inc.","Autonomous driving control; Driving style classification; Intelligent decision & control; Machine learning; Runtime verification; Statistical model checking; Uncertainty modeling"
"A classification framework for automated control code generation in industrial automation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083001647&doi=10.1016%2fj.jss.2020.110575&partnerID=40&md5=b7a8f91c6b81c019db981c72072df986","Software development for the automation of industrial facilities (e.g., oil platforms, chemical plants, power plants, etc.) involves implementing control logic, often in IEC 61131-3 programming languages. Developing safe and efficient program code is expensive and today still requires substantial manual effort. Researchers have thus proposed numerous approaches for automatic control logic generation in the last two decades, but a systematic, in-depth analysis of their capabilities and assumptions is missing. This paper proposes a novel classification framework for control logic generation approaches defining criteria derived from industry best practices. The framework is applied to compare and analyze 13 different control logic generation approaches. Prominent findings include different categories of control logic generation approaches, the challenge of dealing with iterative engineering processes, and the need for more experimental validations in larger case studies. © 2020 Elsevier Inc.","Code generation; Control engineering; Industrial automation; Model-driven development; Software design and implementation; UML / SysML"
"Architectural runtime models for integrating runtime observations and component-based models","2020","Journal of Systems and Software","10.1016/j.jss.2020.110722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087587294&doi=10.1016%2fj.jss.2020.110722&partnerID=40&md5=66f348e62a9978c6b9325971534ee34f","Keeping track of modern software applications while dynamically changing requires strong interaction of evolution activities on development level and adaptation activities on operation level. Knowledge about software architecture is key for both, developers while evolving the system and operators while adapting the system. Existing architectural models used in development differ from those used in operation in terms of purpose, abstraction and content. Consequences are limited reuse of development models during operation, lost architectural knowledge and limited phase-spanning consideration of software architecture. In this paper, we propose modeling concepts of the iObserve approach to align architectural models used in development and operation. We present a correspondence model to bridge the divergent levels of abstraction between implementation artifacts and component-based architectural models. A transformation pipeline uses the information stored in the correspondence model to update architectural models based on changes during operation. Moreover, we discuss the modeling of complex workload based on observations during operation. In a case study-based evaluation, we examine the accuracy of our models to reflect observations during operation and the scalability of the transformation pipeline. Evaluation results show the accuracy of iObserve. Furthermore, evaluation results indicate iObserve adequately scales for some cases but shows scalability limits for others. © 2020 Elsevier Inc.","Palladio Component Model; Performance model; Runtime model; Software architecture; Workload"
"Cyber-physical modelling in Modelica with model-reduction techniques","2020","Journal of Systems and Software","10.1016/j.jss.2019.110517","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078731691&doi=10.1016%2fj.jss.2019.110517&partnerID=40&md5=3a236aff81b2835f757b0caff13c4577","Object-oriented modelling of cyber-physical systems with Modelica and similar environments has brought many advantages, especially the efficient re-use of models and thus the possibility of creating powerful multi-domain libraries. Unfortunately, the models have become highly complex, which causes serious problems during processing and execution. Consequently, verification and debugging is becoming an increasingly challenging task. The continuous investigation of simplifications and reductions in all phases of model developments is thus urgent. The present paper deals with reduction methods based on metric ranking and preserve realisation, which means that the structure and the parameters of the model remain physically interpretable. Two model-reduction methods are described and implemented in Open Modelica. The first operates on a set of differential-algebraic equations, and the second is based on modified bond-graphs-reduction techniques. The latter approach is suitable for component-based models in Modelica that are usually represented graphically with object diagrams. The paper briefly describes the research area, the problems of the adoption of the developed model reduction techniques to the Modelica environments, and the final implementation. Both proposed approaches are tested on the model of a car suspension system and briefly discussed. © 2020 Elsevier Inc.","Continuous systems modelling; Model reduction; Modelica; Object oriented modelling; Ranking metrics; Realisation preserving modelling"
"Inner source software development: Current thinking and an agenda for future research","2020","Journal of Systems and Software","10.1016/j.jss.2020.110520","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078200229&doi=10.1016%2fj.jss.2020.110520&partnerID=40&md5=b4f872acd43f9ce5ef96a35f355d4dfa","Context: Inner source software development (ISSD) has been viewed as an alternative approach in which organisations adopt open source software development (OSSD) practices and exploit its benefits internally. Objective: In this paper, we aim to provide an extensive review of current research on ISSD and to establish a research agenda on this domain. Method: The review is primarily performed using a systematic literature review protocol. Results: We identified, critically evaluated and integrated the findings of 37 primary studies, describing 25 empirical research papers, 10 frameworks/methods, models and tools to support the implementation of inner source, as well as a set of benefits and challenges associated with ISSD. Conclusion: This study presents four main contributions. First, the study provides an in-depth review of ISSD to date, i.e. the evolution of research across inner source, contributions of existing research developments, and theories, models and frameworks used to study inner source. Second, our review applies the OSSD approach framework as the lens to analyse ISSD. Third, the review updates the key challenges associated with ISSD from a management perspective. The final contribution is the establishment of a research agenda to advance knowledge on ISSD. © 2020 Elsevier Inc.","Inner source; Inner source software development; Research agenda; Systematic literature review"
"Java decompiler diversity and its application to meta-decompilation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085736599&doi=10.1016%2fj.jss.2020.110645&partnerID=40&md5=73fd4a8d3c360e0f2ba496a106fbeaf3","During compilation from Java source code to bytecode, some information is irreversibly lost. In other words, compilation and decompilation of Java code is not symmetric. Consequently, decompilation, which aims at producing source code from bytecode, relies on strategies to reconstruct the information that has been lost. Different Java decompilers use distinct strategies to achieve proper decompilation. In this work, we hypothesize that the diverse ways in which bytecode can be decompiled has a direct impact on the quality of the source code produced by decompilers. In this paper, we assess the strategies of eight Java decompilers with respect to three quality indicators: syntactic correctness, syntactic distortion and semantic equivalence modulo inputs. Our results show that no single modern decompiler is able to correctly handle the variety of bytecode structures coming from real-world programs. The highest ranking decompiler in this study produces syntactically correct, and semantically equivalent code output for 84%, respectively 78%, of the classes in our dataset. Our results demonstrate that each decompiler correctly handles a different set of bytecode classes. We propose a new decompiler called Arlecchino that leverages the diversity of existing decompilers. To do so, we merge partial decompilation into a new one based on compilation errors. Arlecchino handles 37.6% of bytecode classes that were previously handled by no decompiler. We publish the sources of this new bytecode decompiler. © 2020","Decompilation; Java bytecode; Reverse engineering; Source code analysis"
"Examining the effects of developer familiarity on bug fixing","2020","Journal of Systems and Software","10.1016/j.jss.2020.110667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086628677&doi=10.1016%2fj.jss.2020.110667&partnerID=40&md5=cf77863a868475aa13874d491dc90971","Background: In modern software systems’ maintenance and evolution, how to fix software bugs efficiently and effectively becomes increasingly more essential. A deep understanding of developers’/assignees’ familiarity with bugs could help project managers make a proper allotment of maintenance resources. However, to our knowledge, the effects of developer familiarity on bug fixing have not been studied. Aims: Inspired by the understanding of developers’/assignees’ familiarity with bugs, we aim to investigate the effects of familiarity on efficiency and effectiveness of bug fixing. Method: Based on evolution history of buggy code lines, we propose three metrics to evaluate the developers’/assignees’ familiarity with bugs. Additionally, we conduct an empirical study on 6 well-known Apache Software Foundation projects with more than 9000 confirmed bugs. Results: We observe that (a) familiarity is one of the common factors in cases of bug fixing: the developers are more likely to be assigned to fix the bugs introduced by themselves; (b) familiarity has complex effects on bug fixing: although the developers fix the bugs introduced by themselves more quickly (with high efficiency), they are more likely to introduce future bugs when fixing the current bugs (with worse effectiveness). Conclusion: We put forward the following suggestions: (a) managers should assign some “outsiders” to participate in bug fixing. (b) when developers deal with his own code, managers should assign more maintenance resource (e.g., more inspection) to developers. © 2020 Elsevier Inc.","Bug fixing; Efficiency and effectiveness; Familiarity; Software maintenance and evolution; Software metrics"
"On tracking Java methods with Git mechanisms","2020","Journal of Systems and Software","10.1016/j.jss.2020.110571","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082575814&doi=10.1016%2fj.jss.2020.110571&partnerID=40&md5=eee7a0963d6d21a5f15df546ca9e0c1d","Method-level historical information is useful in various research on mining software repositories such as fault-prone module detection or evolutionary coupling identification. An existing technique named Historage converts a Git repository of a Java project to a finer-grained one. In a finer-grained repository, each Java method exists as a single file. Treating Java methods as files has an advantage, which is that Java methods can be tracked with Git mechanisms. The biggest benefit of tracking methods with Git mechanisms is that it can easily connect with any other tools and techniques build on Git infrastructure. However, Historage's tracking has an issue of accuracy, especially on small methods. More concretely, in the case that a small method is renamed or moved to another class, Historage has a limited capability to track the method. In this paper, we propose a new technique, FinerGit, to improve the trackability of Java methods with Git mechanisms. We implement FinerGit as a system and apply it to 182 open source software projects, which include 1,768K methods in total. The experimental results show that our tool has a higher capability of tracking methods in the case that methods are renamed or moved to other classes. © 2020 The Author(s)","Mining software repositories; Source code analysis; Tracking Java methods"
"Partially observable Markov decision process to generate policies in software defect management","2020","Journal of Systems and Software","10.1016/j.jss.2020.110518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078088367&doi=10.1016%2fj.jss.2020.110518&partnerID=40&md5=b7477b3a04912b944344f8b0ad4febee","Bug repositories are dynamic in nature and as new bugs arrive, the old ones are closed. In a typical software project, bugs and their dependencies are reported manually and gradually using a issue tracking system. Thus, not all of the bugs in the system are available at any time, creating uncertainty in the dependency structure of the bugs. In this research, we propose to construct a dependency graph based on the reported dependency-blocking information in a issue tracking system. We use two graph metrics, depth and degree, to measure the extent of blocking bugs. Due to the uncertainty in the dependency structure, simply ordering bugs in the descending order of depth and/or degree may not be the best policy to prioritize bugs. Instead, we propose a Partially Observable Markov Decision Process model for sequential decision making and Partially Observable Monte Carlo Planning to identify the best policy for this sequential decision-making process. We validated our proposed approach by mining the data from two open source projects, and a commercial project. We compared our proposed framework with three baseline policies. The results on all datasets show that our proposed model significantly outperforms the other policies with respect to average discounted return. © 2020","Defect management; Partially observable Markov decision Process; Partially observable Monte Carlo Planning; Policy; Reinforcement learning"
"A machine learning based framework for code clone validation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087971737&doi=10.1016%2fj.jss.2020.110686&partnerID=40&md5=fae6e9e8eef5743ebfee334cfe263cbf","A code clone is a pair of code fragments, within or between software systems that are similar. Since code clones often negatively impact the maintainability of a software system, several code clone detection techniques and tools have been proposed and studied over the last decade. However, the clone detection tools are not always perfect and their clone detection reports often contain a number of false positives or irrelevant clones from specific project management or user perspective. To detect all possible similar source code patterns in general, the clone detection tools work on the syntax level while lacking user-specific preferences. This often means the clones must be manually inspected before analysis in order to remove those false positives from consideration. This manual clone validation effort is very time-consuming and often error-prone, in particular for large-scale clone detection. In this paper, we propose a machine learning approach for automating the validation process. First, a training dataset is built by taking code clones from several clone detection tools for different subject systems and then manually validating those clones. Second, several features are extracted from those clones to train the machine learning model by the proposed approach. The trained algorithm is then used to automatically validate clones without human inspection. Thus the proposed approach can be used to remove the false positive clones from the detection results, automatically evaluate the precision of any clone detectors for any given set of datasets, evaluate existing clone benchmark datasets, or even be used to build new clone benchmarks and datasets with minimum effort. In an experiment with clones detected by several clone detectors in several different software systems, we found our approach has an accuracy of up to 87.4% when compared against the manual validation by multiple expert judges. The proposed method also shows better results in several comparative studies with the existing related approaches for clone classification. © 2020 Elsevier Inc.","Clone management; Code clones; Machine learning; Validation"
"The influence of Technical Debt on software developer morale","2020","Journal of Systems and Software","10.1016/j.jss.2020.110586","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084567139&doi=10.1016%2fj.jss.2020.110586&partnerID=40&md5=711f33c489b63a15403c895bbcea06f5","Context: Previous research in the Technical Debt (TD) field has mainly focused on the technical and economic aspects, while its human aspect has received minimal attention. Objective: This paper aims to understand how software developers’ morale is influenced by TD and how their morale is influenced by TD management activities. Furthermore, this study correlates the morale with the amount of wastage of time due to TD. Method: Firstly, we conducted 15 interviews with professionals, and, secondly, these data were complemented with a survey. Thirdly, we collected 473 data points from 43 developers reporting their amount of wasted time. The collected data were analyzed using both quantitative and qualitative techniques, including thematic and statistical analysis. Results: Our results show that the occurrence of TD is associated with a lack of progress and waste of time. This might have a negative influence on developers’ morale. Further, management of TD seems to have a positive influence on developers’ morale. Conclusions: The results highlight the effects TD has on practitioners’ software work. This study presents results indicating that software suffering from TD reduces developers’ morale and thereby also their productivity. However, our results also indicate that TD management increases developers’ morale and developer productivity. © 2020","Developer morale; Human factors; Software development; Software productivity; Technical debt; Technical debt management"
"ThermoSim: Deep learning based framework for modeling and simulation of thermal-aware resource management for cloud computing environments","2020","Journal of Systems and Software","10.1016/j.jss.2020.110596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083500386&doi=10.1016%2fj.jss.2020.110596&partnerID=40&md5=82dc7876067c850e7f73f5d982eb7a57","Current cloud computing frameworks host millions of physical servers that utilize cloud computing resources in the form of different virtual machines. Cloud Data Center (CDC) infrastructures require significant amounts of energy to deliver large scale computational services. Moreover, computing nodes generate large volumes of heat, requiring cooling units in turn to eliminate the effect of this heat. Thus, overall energy consumption of the CDC increases tremendously for servers as well as for cooling units. However, current workload allocation policies do not take into account effect on temperature and it is challenging to simulate the thermal behavior of CDCs. There is a need for a thermal-aware framework to simulate and model the behavior of nodes and measure the important performance parameters which can be affected by its temperature. In this paper, we propose a lightweight framework, ThermoSim, for modeling and simulation of thermal-aware resource management for cloud computing environments. This work presents a Recurrent Neural Network based deep learning temperature predictor for CDCs which is utilized by ThermoSim for lightweight resource management in constrained cloud environments. ThermoSim extends the CloudSim toolkit helping to analyze the performance of various key parameters such as energy consumption, service level agreement violation rate, number of virtual machine migrations and temperature during the management of cloud resources for execution of workloads. Further, different energy-aware and thermal-aware resource management techniques are tested using the proposed ThermoSim framework in order to validate it against the existing framework (Thas). The experimental results demonstrate the proposed framework is capable of modeling and simulating the thermal behavior of a CDC and ThermoSim framework is better than Thas in terms of energy consumption, cost, time, memory usage and prediction accuracy. © 2020 Elsevier Inc.","Cloud computing; Deep learning; Energy; Resource management; Simulation; Thermal-aware"
"Systematic literature reviews in software engineering—enhancement of the study selection process using Cohen's Kappa statistic","2020","Journal of Systems and Software","10.1016/j.jss.2020.110657","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085270179&doi=10.1016%2fj.jss.2020.110657&partnerID=40&md5=79abfe45a51df2426282a8474ab1c90e","Context: Systematic literature reviews (SLRs) rely on a rigorous and auditable methodology for minimizing biases and ensuring reliability. A common kind of bias arises when selecting studies using a set of inclusion/exclusion criteria. This bias can be decreased through dual revision, which makes the selection process more time-consuming and remains prone to generating bias depending on how each researcher interprets the inclusion/exclusion criteria. Objective: To reduce the bias and time spent in the study selection process, this paper presents a process for selecting studies based on the use of Cohen's Kappa statistic. We have defined an iterative process based on the use of this statistic during which the criteria are refined until obtain almost perfect agreement (k>0.8). At this point, the two researchers interpret the selection criteria in the same way, and thus, the bias is reduced. Starting from this agreement, dual review can be eliminated; consequently, the time spent is drastically shortened. Method: The feasibility of this iterative process for selecting studies is demonstrated through a tertiary study in the area of software engineering on works that were published from 2005 to 2018. Results: The time saved in the study selection process was 28% (for 152 studies) and if the number of studies is sufficiently large, the time saved tend asymptotically to 50%. Conclusions: Researchers and students may take advantage of this iterative process for selecting studies when conducting SLRs to reduce bias in the interpretation of inclusion and exclusion criteria. It is especially useful for research with few resources. © 2020 Elsevier Inc.","Cohen's kappa; Evidence-based practice; Systematic review"
"Using likely invariants for test data generation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079882290&doi=10.1016%2fj.jss.2020.110549&partnerID=40&md5=96ffed8d7d6f9555682592e7f9813c4e","Various approaches have been developed to tackle the problem of automatic test data generation. Among them search-based methods use metaheuristic algorithms to guide search in the input space of the program under test. This paper presents a new approach for improving search-based test data generation methods. This approach is based on learning the relationships between program input values and program parts covered by those values. The learned relationships are used to accelerate achieving test coverage goals. We introduce the concepts of branch likely invariant and path likely invariant as the basis for the learning method. In addition, we utilize simple predicates (based on some predefined templates) over program input variables to generate better initial candidate solutions, and use the mutation of the mentioned predicates to cover unexplored program parts. The current version of the proposed approach only considers numeric and string input parameters. To evaluate the performance of the proposed approach, a series of experiments have been carried out on a number of different benchmark programs. Through experiments and analysis, we show that the proposed approach enhances the effectiveness of common search-based test data generation methods, in terms of the coverage percentage. © 2020","Branch likely invariants; Metaheuristic algorithms; Path likely invariants; Search-based test data generation"
"Multilevel analysis of the java virtual machine based on kernel and userspace traces","2020","Journal of Systems and Software","10.1016/j.jss.2020.110589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084193075&doi=10.1016%2fj.jss.2020.110589&partnerID=40&md5=68a71176f277cb021c76f2f678056a56","Performance analysis of Java applications requires a deep understanding of the Java virtual machine and the system on which it is running. An unexpected latency can be caused by a bug in the source code, a misconfiguration or an external factor like CPU or disk contention. Existing tools have difficulties finding the root cause of some latencies because they do not efficiently collect performance data from the different layers of the system. In this paper, we propose a multilevel analysis framework that uses Kernel and userspace tracing to help developers understand and evaluate the performance of their applications. Kernel tracing is used to gather information about thread scheduling, system calls, I/O operations, etc. and userspace tracing is used to monitor the internal components of the JVM such as the garbage collectors and the JIT compilers. By bridging the gap between kernel and userspace traces, our tool provides full visibility to developers and helps them diagnose difficult performance issues. We show the usefulness of our approach by using it to detect problems in different Java applications. © 2020 Elsevier Inc.","Java; JVM; Performance; Profiling; Tracing"
"MT-EA4Cloud: A Methodology For testing and optimising energy-aware cloud systems","2020","Journal of Systems and Software","10.1016/j.jss.2020.110522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078176143&doi=10.1016%2fj.jss.2020.110522&partnerID=40&md5=dd7379aa97200e7c9d61277707d98b2e","Currently, using conventional techniques for checking and optimising the energy consumption in cloud systems is unpractical, due to the massive computational resources required. An appropriate test suite focusing on the parts of the cloud to be tested must be efficiently synthesised and executed, while the correctness of the test results must be checked. Additionally, alternative cloud configurations that optimise the energetic consumption of the cloud must be generated and analysed accordingly, which is challenging. To solve these issues we present MT-EA4Cloud, a formal approach to check the correctness – from an energy-aware point of view – of cloud systems and optimise their energy consumption. To make the checking of energy consumption practical, MT-EA4Cloud combines metamorphic testing, evolutionary algorithms and simulation. Metamorphic testing allows to formally model the underlying cloud infrastructure in the form of metamorphic relations. We use metamorphic testing to alleviate both the reliable test set problem, generating appropriate test suites focused on the features reflected in the metamorphic relations, and the oracle problem, using the metamorphic relations to check the generated results automatically. MT-EA4Cloud uses evolutionary algorithms to efficiently guide the search for optimising the energetic consumption of cloud systems, which can be calculated using different cloud simulators. © 2020 Elsevier Inc.","Cloud modelling; Energy-aware systems; Evolutionary algorithms; Metamorphic testing; Simulation"
"Uncertainty in information system development: Causes, effects, and coping mechanisms","2020","Journal of Systems and Software","10.1016/j.jss.2020.110655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085194972&doi=10.1016%2fj.jss.2020.110655&partnerID=40&md5=4bafa91ea77475e750024a44717b55f6","Information system development (ISD) projects are an ever-growing field of project management (PM) with their unique features, and project failures in ISD are relatively common. In the broader context of PM, uncertainty is a studied, yet mercurial phenomenon. By contrast, uncertainty in ISD projects has received relatively little attention from scholars, and PM literature has not systematically focused on uncertainty in ISD from a viewpoint other than that of project managers. In order to understand uncertainties in ISD projects, we need to first understand the causes behind them, their effects on everyday ISD work, and share coping mechanisms utilized among industry professionals. In the context of ISD projects, we set out to explore what causes uncertainty, what are the effects of uncertainty, and how software industry professionals cope with uncertainty. We conducted eleven semi-structured interviews with a diverse range of ISD professionals, and analyzed the interviews using conventional content analysis. Our results extend and complement current knowledge on the causes, effects, and coping mechanisms of uncertainty, especially in the context of ISD. Additionally, we present practical considerations on how to implement our findings into ISD industry and education. © 2020 Elsevier Inc.","Cause; Coping mechanism; Effect; Information system development; Risk; Uncertainty"
"Lexical content as a cooperation aide: A study based on Java software","2020","Journal of Systems and Software","10.1016/j.jss.2020.110543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079839348&doi=10.1016%2fj.jss.2020.110543&partnerID=40&md5=2780b507cdfd4b65899d3b8d7b9da73d","Collaborative development is a paradigm shift in software development. Loosely coupled developers coordinate their work via distributed versioning systems (SVN, Git, and others), code reviews and priority-led bug tracking systems. This development approach allows many different developers to input additional source code to the same source artifact. This article focuses on the lexical content of the source code produced in a collaborative environment. The lexical content is described as the ‘dictionary’ of the key terms contained within a source artifact. We posit that the lexical content of a Java class will increase as long as more developers add more content to the same class. We analyse the 100 top-ranked GitHub applications (at the time of the sampling) written in Java. Each of their classes is reduced to its lexical content, its size (in LOCs) recorded, as well as the number of different developers who contributed to its source code. Our results show that (i) the lexical content of Java classes is bounded in size, (ii) more developers make the size of the lexical content larger, and (iii) the lexical content of a system's classes might increase with more developers, but depending on its application domain. The implications for practitioners are two-fold: (i) classes with a large set of lexical content should be split in multiple classes, to minimize the need for further maintenance; and (ii) classes developed by many developers should adhere to specific guidelines so that its lexical content does not increase boundlessly. We tested our results in a tailored case study and we confirmed our findings: larger-than-threshold class corpora tend to deteriorate the class cohesion. © 2020","Clustering; Distributed development; Information retrieval (IR); Lexical content; Object-oriented (OO); Open-source software"
"A proposal of architecture for integration and uniform use of hybrid SQL/NoSQL database components","2020","Journal of Systems and Software","10.1016/j.jss.2020.110633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085258645&doi=10.1016%2fj.jss.2020.110633&partnerID=40&md5=24fffea7c6c5df41891f05d0c591355b","The popularity of social networks and the expansion of various second-generation Internet services have contributed to the increase of data, of different structuredness levels, in use. Relational databases (frequently called SQL databases) pose themselves as a logical choice for the management of data containing fixed or rarely changeable structure. The need for fast processing of vast quantities of unstructured data has opened the door for the rise of NoSQL databases popularity. The business of modern organizations often faces the challenge of parallel use of different database types. In recent years, hybrid SQL/NoSQL databases, which contain SQL and NoSQL databases as its components, become a popular solution for the issue above. This paper identifies and describes a possible way of integration and uniform use (as two significant non-functional requirements) of hybrid database components, as well as introduce the architecture for this purpose. The presented architecture, with its specially developed components, provides as simple usage as a single database does, with advantages of parallel use of databases of different types. The functioning principle of the new architecture is elaborated on a series of practical use cases of various complexities, which were tested against a hybrid database, and Oracle and MongoDB as well. © 2020 Elsevier Inc.","Architecture proposal; Database integration; Hybrid database; NoSQL; SQL; Uniform use"
"Software-testing education: A systematic literature mapping","2020","Journal of Systems and Software","10.1016/j.jss.2020.110570","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082879194&doi=10.1016%2fj.jss.2020.110570&partnerID=40&md5=d2ea0d3167a2817188efc650fc3548d3","Context: With the rising complexity and scale of software systems, there is an ever-increasing demand for sophisticated and cost-effective software testing. To meet such a demand, there is a need for a highly-skilled software testing work-force (test engineers) in the industry. To address that need, many university educators worldwide have included software-testing education in their software engineering (SE) or computer science (CS) programs. Many papers have been published in the last three decades (as early as 1992) to share experience from such undertakings. Objective: Our objective in this paper is to summarize the body of experience and knowledge in the area of software-testing education to benefit the readers (both educators and researchers) in designing and delivering software testing courses in university settings, and to also conduct further education research in this area. Method: To address the above need, we conducted a systematic literature mapping (SLM) to synthesize what the community of educators have published on this topic. After compiling a candidate pool of 307 papers, and applying a set of inclusion/exclusion criteria, our final pool included 204 papers published between 1992 and 2019. Results: The topic of software-testing education is becoming more active, as we can see by the increasing number of papers. Many pedagogical approaches (how to best teach testing), course-ware, and specific tools for testing education have been proposed. Many challenges in testing education and insights on how to overcome those challenges have been proposed. Conclusion: This paper provides educators and researchers with a classification of existing studies within software-testing education. We further synthesize challenges and insights reported when teaching software testing. The paper also provides a reference (“index”) to the vast body of knowledge and experience on teaching software testing. Our mapping study aims to help educators and researchers to identify the best practices in this area to effectively plan and deliver their software testing courses, or to conduct further education-research in this important area. © 2020 Elsevier Inc.","Education research; Software testing; Software-engineering education; Software-testing education; Systematic literature mapping; Systematic literature review"
"Model-based testing of software product lines: Mapping study and research roadmap","2020","Journal of Systems and Software","10.1016/j.jss.2020.110608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084755958&doi=10.1016%2fj.jss.2020.110608&partnerID=40&md5=dcaf99d702a5fb59f9415d8bae02e050","Model-Based Testing (MBT) has been successfully applied to Software Product Lines (SPL). This paper provides a panorama of state-of-the-art on MBT of SPLs. We performed a systematic mapping for answering questions related with domains, approaches, solution types, variability, test case automation, artifacts, and evaluation. We built a roadmap from 44 selected studies. Main obtained results are: Software and Automotive domains are most considered; Black-box testing is widely performed; most studies have fully-automated support; variability is considered in most studies; Finite State Machines is the most used model to test SPLs; Behavioral-based and Scenario-based are the most used models; Case Studies and Experiments are used to evaluate MBT solutions and the majority is performed in industrial environments; traceability is not widely explored for MBT solutions. Furthermore, we provide a roadmap synthesizing studies based on used models, more formal artifacts, supporting tools, variability management, (semi-)automation, and traceability. The roadmap contributes to identify related primary studies based on given artifacts, variability management, tools, automation, and traceability techniques and to identify, from a given primary study, which artifacts, tools, variability management, automation and traceability techniques are related. Therefore, the roadmap serves as a guide to researchers and practitioners on how to model-based test SPLs. © 2020 Elsevier Inc.","MBT Roadmap; Model-b-ased testing; Reuse of test cases; Software product line; Systematic mapping study; Variability"
"WARDER: Towards effective spreadsheet defect detection by validity-based cell cluster refinements","2020","Journal of Systems and Software","10.1016/j.jss.2020.110615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084065558&doi=10.1016%2fj.jss.2020.110615&partnerID=40&md5=cd9df1fea9f78a9c34ad88165b651d75","Nowadays spreadsheets are very popular and being widely used. However, they can be prone to various defects and cause severe consequences when end users poorly maintain them. Our research communities have proposed various techniques for automated detection of spreadsheet defects, but they commonly fall short of effectiveness, either due to their limited scope or relying on strict patterns. In this article, we discuss and improve one state-of-the-art technique, CUSTODES, which exploits spreadsheet cell clustering and defect detection to extend its scope and make its detection patterns adaptive to varying spreadsheet styles. Still, CUSTODES can be prone to problematic clustering when accidentally involving irrelevant cells, leading to a largely reduced detection precision. Regarding this, we present WARDER to refine CUSTODES's spreadsheet cell clustering based on three extensible validity-based properties. Experimental results show that WARDER could improve the precision by 19.1% on spreadsheet cell clustering, which contributed to a precision improvement of 23.3 ~ 24.3% for spreadsheet defect detection, as compared to CUSTODES (F-measure increased from 0.71 to 0.79 ~ 0.82). WARDER also exhibited satisfactory results on another practical large-scale spreadsheet corpus VEnron2, improving the defect detection precision by 10.7 ~ 21.2% over CUSTODES. © 2020","Cell clustering; Defect detection; Validity property"
"Scalability Assessment of Microservice Architecture Deployment Configurations: A Domain-based Approach Leveraging Operational Profiles and Load Tests","2020","Journal of Systems and Software","10.1016/j.jss.2020.110564","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080916968&doi=10.1016%2fj.jss.2020.110564&partnerID=40&md5=c88d1e94be06f1c2496ff9e6c0f0d2c0","Microservices have emerged as an architectural style for developing distributed applications. Assessing the performance of architecture deployment configurations — e.g., with respect to deployment alternatives — is challenging and must be aligned with the system usage in the production environment. In this paper, we introduce an approach for using operational profiles to generate load tests to automatically assess scalability pass/fail criteria of microservice configuration alternatives. The approach provides a Domain-based metric for each alternative that can, for instance, be applied to make informed decisions about the selection of alternatives and to conduct production monitoring regarding performance-related system properties, e.g., anomaly detection. We have evaluated our approach using extensive experiments in a large bare metal host environment and a virtualized environment. First, the data presented in this paper supports the need to carefully evaluate the impact of increasing the level of computing resources on performance. Specifically, for the experiments presented in this paper, we observed that the evaluated Domain-based metric is a non-increasing function of the number of CPU resources for one of the environments under study. In a subsequent series of experiments, we investigate the application of the approach to assess the impact of security attacks on the performance of architecture deployment configurations. © 2020",""
"QMM-VANET: An efficient clustering algorithm based on QoS and monitoring of malicious vehicles in vehicular ad hoc networks","2020","Journal of Systems and Software","10.1016/j.jss.2020.110561","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081114220&doi=10.1016%2fj.jss.2020.110561&partnerID=40&md5=6c217dffabaf92def20216b3069f8cfd","Vehicular ad hoc networks (VANETs) are considered as a subset of mobile ad hoc networks (MANETs) that can be used in the transportation field. These networks considerably improve the traffic safety and accident prevention. Because of the characteristics of VANETs such as self-organization, frequent link disconnections and rapid topology changes, developing efficient routing protocols is a challenging task. To address this issue, clustering is an appropriate approach in a mobile environment. Clustering aims to partition the vehicles into a number of clusters based on some predefined metrics such as velocity, distance and location. In this paper, a clustering routing protocol, named QMM-VANET, which considers Quality of Service (QoS) requirements, the distrust value parameters and mobility constraints, is proposed. This protocol specifies a reliable and stable cluster and increases the stability and connectivity during communications. This protocol is composed of three parts: (1) computing the QoS of vehicles and electing a trustier vehicle as a cluster-head, (2) selecting a set of proper neighboring nodes as gateways for retransmitting the packets and (3) using gateway recovery algorithm to choose another gateway in case of failure of the link. NS-2 simulator is utilized to illustrate the performance of our proposed protocol in a highway scenario. The performance analyses display that the QMM-VANET protocol can achieve low end-to-end delay and high packet delivery ratio and maintain the network stability. © 2020 Elsevier Inc.","Clustering; Quality of Service (QoS); Stability; Vehicular ad hoc networks (VANETs)"
"Code smells and refactoring: A tertiary systematic review of challenges and observations","2020","Journal of Systems and Software","10.1016/j.jss.2020.110610","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084281534&doi=10.1016%2fj.jss.2020.110610&partnerID=40&md5=5535225b27a76be0585a492854c82fa2","Refactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support. In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018. We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work. Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring. © 2020","Code smells; Refactoring; Tertiary systematic review"
"Achieving agility and quality in product development - an empirical study of hardware startups","2020","Journal of Systems and Software","10.1016/j.jss.2020.110599","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083814774&doi=10.1016%2fj.jss.2020.110599&partnerID=40&md5=1698508c1aeacbb7addce46dc26bf6a7","Context: Startups aim at scaling their business, often by developing innovative products with limited human and financial resources. The development of software products in the startup context is known as opportunistic, agility-driven, and with high tolerance for technical debt. The special context of hardware startups calls for a better understanding of state-of-the-practice of hardware startups’ activities. Objective: This study aimed to identify whether and how startups can achieve product quality while maintaining focus on agility. Method: We conducted an exploratory study with 13 hardware startups, collecting data through semi-structured interviews and analysis of documentation. We proposed an integrative model of agility and quality in hardware startups. Results: Agility in hardware startups is complex and not achieved through adoption of fast-paced development practices alone. Hardware startups follow a quality-driven approach for development of core components, where frequent user testing is a measure for early debt management. Hardware startups often lack mindset and strategies for achieving long-term quality in early stages. Conclusions: Hardware startups need attention to hardware quality to allow for evolutionary prototyping and speed. Future research should focus on defining quality-driven practices that contribute to agility, and strategies and mindsets to support long-term quality in the hardware startup context. © 2020 The Author(s)","Empirical research; Hardware startup; Product development; Software engineering; Startup"
"Leading successful government-academia collaborations using FLOSS and agile values","2020","Journal of Systems and Software","10.1016/j.jss.2020.110548","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079654155&doi=10.1016%2fj.jss.2020.110548&partnerID=40&md5=f5abcdcb46821ef0c2da7fa0bff65230","Government and academia share concerns for efficiently and effectively servicing societal demands, which includes the development of e-government software. Government-academia partnerships can be a valuable approach for improving productivity in achieving these goals. However, governmental and academic institutions tend to have very different agendas and organizational and managerial structures, which can hinder the success of such collaborative projects. In order to identify effective approaches to overcome collaboration barriers, we systematically studied the case of the Brazilian Public Software portal project, a 30-month government-academia collaboration that, using Free/Libre/Open Source Software practices and agile methods for project management, developed an unprecedented platform in the context of the Brazilian government. We gathered information from experience reports and data collection from repositories and interviews to derive a collection of practices that contributed to the success of the collaboration. In this paper, we describe how the data analysis led to the identification of a set of three high-level decisions supported by the adoption of nine best practices that improved the project performance and enabled professional training of the whole team. © 2020 Elsevier Inc.","Agile methodologies; e-Government; Free software; Government-Academia collaboration; Open source software; Project management"
"Diversified keyword search based web service composition","2020","Journal of Systems and Software","10.1016/j.jss.2020.110540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079012133&doi=10.1016%2fj.jss.2020.110540&partnerID=40&md5=eb61c62d7851431c4599e55a03893f22","To assist system engineers in efficiently building service-based software systems, the keyword search based service composition approach on service connection graphs (scgraphs) has been proposed recently. However, due to the ambiguity of keywords, a keyword query may represent a bunch of different user requirements. Thus the current approach that only returns the composition with the optimal Quality of Service (QoS) cannot guarantee to hit the spot. In this paper, in order to satisfy the various possible requirements underlying a given keyword query, we formally introduce the top-k diverse service composition problem, and present a novel diversified keyword search approach on scgraphs to address it. Specifically, we firstly propose an All-Then-Diversify (ATD) algorithm that enumerates all potential compositions by searching a scgraph and then derives the top-k diverse subsets by deriving the maximal independent sets of a similarity graph. Then, due to the possibly large number of potential compositions, we present a Pop-And-Diversify (PAD) algorithm that only maintains a similarity graph of the top compositions that have been found so far during the search and computes its maximal independent sets incrementally until convergence, thereby reducing unnecessary computation overheads. Moreover, we propose two composition similarity measurements w.r.t. the categories or descriptions of services respectively. Lastly, the experimental results on ProgrammableWeb.com demonstrate that, our approach outperforms another state-of-the-art composition diversification approach on both metrics of density and redundancy, and meanwhile, improves the efficiency of diversification significantly. © 2020 Elsevier Inc.","Diversification; Keyword search; Quality of service; Service composition; Web service"
"On Adaptive Change Recommendation","2020","Journal of Systems and Software","10.1016/j.jss.2020.110550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081969715&doi=10.1016%2fj.jss.2020.110550&partnerID=40&md5=42c569dfb684cfe7e5b70b1ef4bed35c","As the complexity of a software system grows, it becomes harder for developers to be aware of all the dependencies between its artifacts (e.g., files or methods). Change impact analysis helps to overcome this challenge, by recommending relevant source-code artifacts related to a developer's current changes. Association rule mining has shown promise in determining change impact by uncovering relevant patterns in the system's change history. State-of-the-art change impact mining typically uses a change history of tens of thousands of transactions. For efficiency, targeted association rule mining constrains the transactions used to those potentially relevant to answering a particular query. However, it still considers all the relevant transactions in the history. This paper presents ATARI, a new adaptive approach that further constrains targeted association rule mining by considering a dynamic selection of the relevant transactions. Our investigation of adaptive change impact mining empirically studies fourteen algorithm variants. We show that adaptive algorithms are viable, can be just as applicable as the start-of-the-art complete-history algorithms, and even outperform them for certain queries. However, more important than this direct comparison, our investigation motivates and lays the groundwork for the future study of adaptive techniques, and their application to challenges such as on-the-fly impact analysis at GitHub-scale. © 2020 Elsevier Inc.","Association rule mining; Change impact analysis; Change recommendation; Evolutionary coupling"
"Modeling programs hierarchically with stack-augmented LSTM","2020","Journal of Systems and Software","10.1016/j.jss.2020.110547","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079383470&doi=10.1016%2fj.jss.2020.110547&partnerID=40&md5=a11b5671bf053ee7fa64e6db4a7296ae","Programming language modeling has attracted extensive attention in recent years, and it plays an essential role in program processing fields. Statistical language models, which are initially designed for natural languages, have been generally used for modeling programming languages. However, different from natural languages, programming languages contain explicit and hierarchical structure that is hard to learn by traditional statistical language models. To address this challenge, we propose a novel Stack-Augmented LSTM neural network for programming language modeling. Adding a stack memory component into the LSTM network enables our model to capture the hierarchical information of programs through the PUSH and POP operations, which further allows our model capturing the long-term dependency in the programs. We evaluate the proposed model on three program analysis tasks, i.e., code completion, program classification, and code summarization. Evaluation results show that our proposed model outperforms baseline models in all the three tasks, indicating that by capturing the structural information of programs with a stack, our proposed model can represent programs more precisely. © 2020 Elsevier Inc.","Deep learning; Hierarchical structure; Programming language modeling; Software engineering"
"The effect of multiple developers on structural attributes: A Study based on java software","2020","Journal of Systems and Software","10.1016/j.jss.2020.110593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083715526&doi=10.1016%2fj.jss.2020.110593&partnerID=40&md5=0f09fda7dc90cd292096beae75a2598c","Context: Long-term software projects employ different software developers who collaborate on shared artifacts. The accumulation of changes pushed by different developers leave traces on the underlying code, that have an effect on its future maintainability, and even reuse. Objective: This study focuses on the how the changes by different developers might have an impact on the code: we investigate whether the work of multiple developers, and their experience, have a visible effect on the structural metrics of the underlying code. Method: We consider nine object-oriented (OO) attributes and we measure them in a GitHub sample containing the top 200 ‘forked’ projects. For each of their classes, we evaluated the number of distinct developers contributing to its source code, and their experience in the project. Results: We show that the presence of multiple developers working on the same class has a visible effect on the chosen OO metrics, and often in the opposite direction to what the guidelines for each attribute suggest. We also show how the relative experience of developers in a project plays an important role in the distribution of those metrics, and the future maintenance of the Java classes. Conclusions: Our results show how distributed development has an effect on the structural attributes of a software system and how the experience of developers plays a fundamental role in that effect. We also discover workarounds and best practices in 4 applied case studies. © 2020","Collaborative development; Metrics; Object oriented; Open source; Software structure"
"The effect of transactive memory systems on process tailoring in software projects: The moderating role of task conflict and shared temporal cognitions","2020","Journal of Systems and Software","10.1016/j.jss.2020.110545","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079903683&doi=10.1016%2fj.jss.2020.110545&partnerID=40&md5=aac378c48ae3c1294a189003c7a82c68","Contemporary software projects are unique and volatile, leading development teams to modify standard development processes and continue to make adjustments as needed. Adjusting software project development to accommodate the variance and dynamics is called software process tailoring (SPT). Because SPT critically determines how projects are conducted, its performance merits investigation. However, the extant literature lacks empirical evidence of the underlying effects that operate and influence the performance of SPT. Specifically, SPT is a team-based activity that requires the exchange of knowledge and opinions among members to yield an integrative tailoring solution; SPT is also a highly conflicting process involving task and temporal conflicts. Given these characteristics, teams’ operational mechanisms that increase SPT performance remain unknown. To address the aforementioned gaps, this study adopts the transactive memory systems (TMS) theory to develop a research model to explore how a team's TMS affects SPT performance with task conflict and shared temporal cognitions (STC) acting as moderators. By examining 102 software project teams, we found that TMS has a positive impact on SPT performance. Surprisingly, task conflict reduces the effect of TMS on SPT performance, whereas STC amplifies the influence of TMS-SPT performance. © 2020","Shared temporal cognitions (STC); Software process tailoring (SPT); SPT effectiveness; SPT efficiency; Task conflict; Transactive memory system (TMS)"
"Relation-based test case prioritization for regression testing","2020","Journal of Systems and Software","10.1016/j.jss.2020.110539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079013222&doi=10.1016%2fj.jss.2020.110539&partnerID=40&md5=8ffb1b95a9a8623726177cfe118e28e0","Test case prioritization (TCP), which aims at detecting faults as early as possible is broadly used in program regression testing. Most existing TCP techniques exploit coverage information with the hypothesis that higher coverage has more chance to catch bugs. Static structure information such as function and statement are frequently employed as coverage granularity. However, the former consumes less costs but presents lower capability to detect faults, the latter typically incurs more overhead. In this paper, dynamic function call sequences are argued that can guide TCP effectively. Same set of functions/statements can exhibit very different execution behaviors. Therefore, mapping program behaviors to unit-based (function/statement) coverage may not be enough to predict fault detection capability. We propose a new approach AGC (Additional Greedy method Call sequence). Our approach leverages dynamic relation-based coverage as measurement to extend the original additional greedy coverage algorithm in TCP techniques. We conduct our experiments on eight real-world java open source projects and systematically compare AGC against 22 state-of-the-art TCP techniques with different granularities. Results show that AGC outperforms existing techniques on large programs in terms of bug detection capability, and also achieves the highest mean APFD value. The performance demonstrates a growth trend as the size of the program increases. © 2020","Dynamic call sequence; Software testing; Test case prioritization"
"Bug severity prediction using question-and-answer pairs from Stack Overflow","2020","Journal of Systems and Software","10.1016/j.jss.2020.110567","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081122470&doi=10.1016%2fj.jss.2020.110567&partnerID=40&md5=e8f8e489ce5e1bc6fc7f2c1a48f01b72","Nowadays, bugs have been common in most software systems. For large-scale software projects, developers usually conduct software maintenance tasks by utilizing software artifacts (e.g., bug reports). The severity of bug reports describes the impact of the bugs and determines how quickly it needs to be fixed. Bug triagers often pay close attention to some features such as severity to determine the importance of bug reports and assign them to the correct developers. However, a large number of bug reports submitted every day increase the workload of developers who have to spend more time on fixing bugs. In this paper, we collect question-and-answer pairs from Stack Overflow and use logical regression to predict the severity of bug reports. In detail, we extract all the posts related to bug repositories from Stack Overflow and combine them with bug reports to obtain enhanced versions of bug reports. We achieve severity prediction on three popular open source projects (e,g., Mozilla, Ecplise, and GCC) with Naïve Bayesian, k-Nearest Neighbor algorithm (KNN), and Long Short-Term Memory (LSTM). The results of our experiments show that our model is more accurate than the previous studies for predicting the severity. Our approach improves by 23.03%, 21.86%, and 20.59% of the average F-measure for Mozilla, Eclipse, and GCC by comparing with the Naïve Bayesian based approach which performs the best among all baseline approaches. © 2020 Elsevier Inc.","Bug reports; Logistic regression; Severity prediction; Stack overflow"
"Systematic literature review of empirical studies on mental representations of programs","2020","Journal of Systems and Software","10.1016/j.jss.2020.110565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081591086&doi=10.1016%2fj.jss.2020.110565&partnerID=40&md5=94b8aef93f2ec25c527c8e979034621b","Programmers are frequently tasked with modifying, enhancing, and extending applications. To perform these tasks, programmers must understand existing code by forming mental representations. Empirical research is required to determine the mental representations constructed during program comprehension to inform the development of programming languages, instructional practices, and tools. To make recommendations for future work a systematic literature review was conducted that summarizes the empirical research on mental representations formed during program comprehension, how the methods and tasks have changed over time, and the research contributions. The data items included in the systematic review are empirical studies of programmers that investigated the comprehension and internal representation of code written in a formal programming language. The eligibility criteria used in the review were meant to extract studies with a focus on knowledge representation as opposed to knowledge utilization. The results revealed a lack of incremental research and a dramatic decline in the research meaning that newly developed or popularized languages and paradigms have not been a part of the research reviewed. Accordingly, we argue that there needs to be a resurgence of empirical research on the psychology of programming to inform the design of tools and languages, especially in new and emerging paradigms. © 2020 Elsevier Inc.","Mental representations; Program comprehension; Systematic literature review"
"Traceability Link Recovery between Requirements and Models using an Evolutionary Algorithm Guided by a Learning to Rank Algorithm: Train control and management case","2020","Journal of Systems and Software","10.1016/j.jss.2020.110519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078162306&doi=10.1016%2fj.jss.2020.110519&partnerID=40&md5=d435e141ef375949a8c6f017fa0654b1","Traceability Link Recovery (TLR) has been a topic of interest for many years within the software engineering community. In recent years, TLR has been attracting more attention, becoming the subject of both fundamental and applied research. However, there still exists a large gap between the actual needs of industry on one hand and the solutions published through academic research on the other. In this work, we propose a novel approach, named Evolutionary Learning to Rank for Traceability Link Recovery (TLR-ELtoR). TLR-ELtoR recovers traceability links between a requirement and a model through the combination of evolutionary computation and machine learning techniques, generating as a result a ranking of model fragments that can realize the requirement. TLR-ELtoR was evaluated in a real-world case study in the railway domain, comparing its outcomes with five TLR approaches (Information Retrieval, Linguistic Rule-based, Feedforward Neural Network, Recurrent Neural Network, and Learning to Rank). The results show that TLR-ELtoR achieved the best results for most performance indicators, providing a mean precision value of 59.91%, a recall value of 78.95%, a combined F-measure of 62.50%, and a MCC value of 0.64. The statistical analysis of the results assesses the magnitude of the improvement, and the discussion presents why TLR-ELtoR achieves better results than the baselines. © 2020","Evolutionary Algorithm; Learning to Rank; Models; Requirements Engineering; Traceability Link Recovery"
"Exploring software reusability metrics with Q&A forum data","2020","Journal of Systems and Software","10.1016/j.jss.2020.110652","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085207531&doi=10.1016%2fj.jss.2020.110652&partnerID=40&md5=035cce48e5e8a125d9ae7da0dd84568c","Question and answer (Q&A) forums contain valuable information regarding software reuse, but they can be challenging to analyze due to their unstructured free text. Here we introduce a new approach (LANLAN), using word embeddings and machine learning, to harness information available in StackOverflow. Specifically, we consider two different kinds of user communication describing difficulties encountered in software reuse: ‘problem reports’ point to potential defects, while ‘support requests’ ask for clarification on software usage. Word embeddings were trained on 1.6 billion tokens from StackOverflow and applied to identify which Q&A forum messages (from two large open source projects: Eclipse and Bioconductor) correspond to problem reports or support requests. LANLAN achieved an area under the receiver operator curve (AUROC) of over 0.9; it can be used to explore the relationship between software reusability metrics and difficulties encountered by users, as well as predict the number of difficulties users will face in the future. Q&A forum data can help improve understanding of software reuse, and may be harnessed as an additional resource to evaluate software reusability metrics. © 2020 Elsevier Inc.","Machine learning; Reusability; Software reuse; StackOverflow; Text mining"
"MSL: A pattern language for engineering self-adaptive systems","2020","Journal of Systems and Software","10.1016/j.jss.2020.110558","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080064786&doi=10.1016%2fj.jss.2020.110558&partnerID=40&md5=e5cb4e7f501ac7d6c2d275af0d9b8715","In architecture-based self-adaptation of decentralized systems, design patterns have been introduced to ease the design of complex adaptation solutions that usually require the interaction of different MAPE-K (Monitor-Analyze-Plan-Execute over a shared Knowledge) control loops, each dealing with an adaptation concern of the managed system. Such MAPE patterns have been proposed by means of a graphical notation, but without a well-defined way to document them and to express the semantics of components interactions. In this paper, we propose an approach to overcome these limitations. We present a domain-specific language, called MSL for MAPE Specification Language, to define and instantiate MAPE patterns and to give semantics to some semantic variation points of the equivalent graphical notation for MAPE pattern. We also provide a formal semantics of the language by means of self-adaptive Abstract State Machines, an extension of the Abstract State Machines (ASMs) formalism to model self-adaptation. Such semantics definition comes with an automatic transformation of MSL models into formal executable models, and opens to the possibility of performing rigorous analysis (validation w.r.t. the adaptation requirements and verification of adaptation properties) of MSL models. Moreover, we present our current results toward a (long-term) realization of an MSL-centric framework, where MSL is the notation of a modeling front-end, on top of richer and more specific modeling, analysis, and implementation back-end frameworks. As proof of concept of our approach, we show the application of MSL and its formal support to a running case study in the field of home automation, by modeling an adaptive control of a virtual smart home developed with the OpenHAB runtime platform. © 2020 Elsevier Inc.","Adaptive smart home systems; Architecture-based self-adaptation; MAPE-K pattern loops; Pattern-oriented modeling; Self-adaptive ASMs"
"Analyzing bug fix for automatic bug cause classification","2020","Journal of Systems and Software","10.1016/j.jss.2020.110538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078897679&doi=10.1016%2fj.jss.2020.110538&partnerID=40&md5=191649ff5658da96b1af0ce31e45dfab","During the bug fixing process, developers usually need to analyze the source code to induce the bug cause, which is useful for bug understanding and localization. The bug fixes of historical bugs usually reflects the bug causes when fixing them. This paper aims at exploiting the corresponding relationship between bug causes and bug fixes to automatically classify bugs into their cause categories. First, we define the code-related bug classification criterion from the perspective of the cause of bugs. Then, we propose a new model to exploit the knowledge in the bug fix by constructing fix trees from the diff source code at Abstract Syntax Tree (AST) level, and representing each fix tree based on the encoding method of Tree-based Convolutional Neural Network (TBCNN). Finally, the corresponding relationship between bug causes and bug fixes is analyzed by automatically classifying bugs into their cause categories. We collected 2000 real-world bugs from two open source projects Mozilla and Radare2 to evaluate our approach. The experimental results show the existence of observational correlation between the bug fix and the cause of the historical bugs, and the proposed fix tree can effectively express the characteristics of the historical bugs for bug cause classification. © 2020","Bug analysis; Bug cause classification; Fix tree; TBCNN"
"Foundations for measuring IT-outsourcing success and failure","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067963453&doi=10.1016%2fj.jss.2019.06.074&partnerID=40&md5=c53e3ced45d2a0fe2c4395a7fc72cbd1","We implemented five easy-to-complete questionnaires in Excel, which could serve as early warning signals for practitioners interested in the odds of their IT-outsourcing deals and could serve to redirect their course when still possible. The questionnaires are based on our earlier published longitudinal, observational study on 30 representative ITO-deals in the Netherlands, of which we know whether they failed or not. Our questionnaires predicted their outcome correctly. To help redirect the course of a dubious deal, we developed a questionnaire estimating the odds in relation to boosting strongly significant critical success determinants. Another questionnaire guides practitioners how to further improve on less critical factors. There are no specific reasons that limit our results to the Dutch situation, which makes it promising, therefore, to apply the Excel as an aid in improving ITO deals in other contexts. © 2019 Elsevier Inc.","Failure determinants; IT-outsourcing; Odds-improvement; Outcome-prediction; Success determinants"
"LoopFix: an approach to automatic repair of buggy loops","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067959590&doi=10.1016%2fj.jss.2019.06.076&partnerID=40&md5=472c6cd439d28958416c9da0fae87856","Bugs may be present in various places in a program. Bugs contained in loops (hereafter buggy loops) are challenging to fix using existing bug fixing techniques. In this work, we propose LoopFix, an approach that aims to repair buggy loops automatically. LoopFix takes a program and a test suite including at least one failed test case as inputs, and generates a patch to fix the bug. LoopFix first leverages on existing bug localization techniques to obtain ranked buggy statements. After that, LoopFix exploits a component based program synthesis approach to synthesize a patch based on the runtime information obtained through symbolic execution. Finally, LoopFix validates the patch with the given test suite. We have implemented LoopFix in a prototype tool and compared the performance of LoopFix with Angelix, S3 and JFix on three widely adopted datasets, i.e., IntroClass, Defects4J and Loops. Our experiments show that LoopFix fixes about 30% of buggy loops and performs more effective than the other tools on buggy loops. © 2019","Automatic repair; Buggy loop; Program synthesis; Symbolic execution; Test suite based"
"A universal cross language software similarity detector for open source software categorization","2020","Journal of Systems and Software","10.1016/j.jss.2019.110491","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077321850&doi=10.1016%2fj.jss.2019.110491&partnerID=40&md5=9030c98112098112ff917a8e2dcdb613","While there are novel approaches for detecting and categorizing similar software applications, previous research focused on detecting similarity in applications written in the same programming language and not on detecting similarity in applications written in different programming languages. Cross-language software similarity detection is inherently more challenging due to variations in language, application structures, support libraries used, and naming conventions. In this paper we propose a novel model, CroLSim, to detect similar software applications across different programming languages. We define a semantic relationship among cross-language libraries and API methods (both local and third party) using functional descriptions and a word-vector learning model. Our experiments show that CroLSim can successfully detect cross-language similar software applications, which outperforms all existing approaches (mean average precision rate of 0.65, confidence rate of 3.6, and 75% highly rated successful queries). Furthermore, we applied CroLSim to a source code repository to see whether our model can recommend cross-language source code fragments if queried directly with source code. From our experiments we found that CroLSim can recommend cross-language functional similar source code when source code is directly used as a query (average precision=0.28, recall=0.85, and F-Measure=0.40). © 2019","API Calls; Cross-Language software similarity detection; Doc2Vec; Singular value decomposition"
"On the performance of method-level bug prediction: A negative result","2020","Journal of Systems and Software","10.1016/j.jss.2019.110493","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076861613&doi=10.1016%2fj.jss.2019.110493&partnerID=40&md5=a8825cf1e785f9d6e400c13b79406180","Bug prediction is aimed at identifying software artifacts that are more likely to be defective in the future. Most approaches defined so far target the prediction of bugs at class/file level. Nevertheless, past research has provided evidence that this granularity is too coarse-grained for its use in practice. As a consequence, researchers have started proposing defect prediction models targeting a finer granularity (particularly method-level granularity), providing promising evidence that it is possible to operate at this level. Particularly, models mixing product and process metrics provided the best results. We present a study in which we first replicate previous research on method-level bug-prediction, by using different systems and timespans. Afterwards, based on the limitations of existing research, we (1) re-evaluate method-level bug prediction models more realistically and (2) analyze whether alternative features based on textual aspects, code smells, and developer-related factors can be exploited to improve method-level bug prediction abilities. Key results of our study include that (1) the performance of the previously proposed models, tested using the same strategy but on different systems/timespans, is confirmed; but, (2) when evaluated with a more practical strategy, all the models show a dramatic drop in performance, with results close to that of a random classifier. Finally, we find that (3) the contribution of alternative features within such models is limited and unable to improve the prediction capabilities significantly. As a consequence, our replication and negative results indicate that method-level bug prediction is still an open challenge. © 2019","Defect prediction; Empirical software engineering; Mining software repositories"
"Modeling stack overflow tags and topics as a hierarchy of concepts","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069201861&doi=10.1016%2fj.jss.2019.07.033&partnerID=40&md5=07231cddfc5e5bbc1f2746a8fa6873a0","Developers rely on online Q&A forums to look up technical solutions, to pose questions on implementation problems, and to enhance their community profile by contributing answers. Many popular developer communication platforms, such as the Stack Overflow Q&A forum, require threads of discussion to be tagged by their contributors for easier lookup in both asking and answering questions. In this paper, we propose to leverage Stack Overflow's tags to create a hierarchical organization of concepts discussed on this platform. The resulting concept hierarchy couples tags with a model of their relevancy to prospective questions and answers. For this purpose, we configure and apply a supervised multi-label hierarchical topic model to Stack Overflow questions and demonstrate the quality of the model in several ways: by identifying tag synonyms, by tagging previously unseen Stack Overflow posts, and by exploring how the hierarchy could aid exploratory searches of the corpus. The results suggest that when traversing the inferred hierarchical concept model of Stack Overflow the questions become more specific as one explores down the hierarchy and more diverse as one jumps to different branches. The results also indicate that the model is an improvement over the baseline for the detection of tag synonyms and that the model could enhance existing ensemble methods for suggesting tags for new questions. The paper indicates that the concept hierarchy as a modeling imperative can create a useful representation of the Stack Overflow corpus. This hierarchy can be in turn integrated into development tools which rely on information retrieval and natural language processing, and thereby help developers more efficiently navigate crowd-sourced online documentation. © 2019 Elsevier Inc.","Concept hierarchy; Entropy-based search evaluation; Hierarchical topic model; Stack overflow; Tag prediction; Tag synonym identification"
"HSP: A hybrid selection and prioritisation of regression test cases based on information retrieval and code coverage applied on an industrial case study","2020","Journal of Systems and Software","10.1016/j.jss.2019.110430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072983204&doi=10.1016%2fj.jss.2019.110430&partnerID=40&md5=8beb16e9c6171b91d5742b4c511bc298","The usual way to guarantee quality of software products is via testing. This paper presents a novel strategy for selection and prioritisation of Test Cases (TC) for Regression testing. In the lack of code artifacts from where to derive Test Plans, this work uses information conveyed by textual documents maintained by Industry, such as Change Requests. The proposed process is based on Information Retrieval techniques combined with indirect code coverage measures to select and prioritise TCs. The aim is to provide a high coverage Test Plan which would maximise the number of bugs found. This process was implemented as a prototype tool which was used in a case study with our industrial partner (Motorola Mobility). Experiments results revealed that the combined strategy provides better results than the use of information retrieval and code coverage independently. Yet, it is worth mentioning that any of these automated options performed better than the previous manual process deployed by our industrial partner to create test plans. © 2019","Code coverage; Information retrieval; Regression testing; Static analysis; Test cases selection and prioritisation"
"Using Orthogonal Defect Classification to characterize NoSQL database defects","2020","Journal of Systems and Software","10.1016/j.jss.2019.110451","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074356605&doi=10.1016%2fj.jss.2019.110451&partnerID=40&md5=ded7f995d86d9b215b3b34a1b965f916","NoSQL databases are increasingly used for storing and managing data in business-critical Big Data systems. The presence of software defects (i.e., bugs) in these databases can bring in severe consequences to the NoSQL services being offered, such as data loss or service unavailability. Thus, it is essential to understand the types of defects that frequently affect these databases, allowing developers take action in an informed manner (e.g., redirect testing efforts). In this paper, we use Orthogonal Defect Classification (ODC) to classify a total of 4096 software defects from three of the most popular NoSQL databases: MongoDB, Cassandra, and HBase. The results show great similarity for the defects across the three different NoSQL systems and, at the same time, show the differences and heterogeneity regarding research carried out in other domains and types of applications, emphasizing the need for possessing such information. Our results expose the defect distributions in NoSQL databases, provide a foundation for selecting representative defects for NoSQL systems, and, overall, can be useful for developers for verifying and building more reliable NoSQL database systems. © 2019","Defect analysis; NoSQL; Orthogonal Defect Classification; Software defect; Software fault"
"Augmenting Java method comments generation with context information based on neural networks","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069831870&doi=10.1016%2fj.jss.2019.07.087&partnerID=40&md5=2e5749fa6255e6ce260a5fcde09b39ef","Code comments are crucial to program comprehension. In this paper, we propose a novel approach ContextCC to automatically generate concise comments for Java methods based on neural networks, leveraging techniques of program analysis and natural language processing. Firstly, ContextCC employs program analysis techniques, especially abstract syntax tree parsing, to extract context information including methods and their dependency. Secondly, it filters code and comments out of the context information to build up a high-quality data set based on a set of pre-defined templates and rules. Finally, ContextCC trains a code comment generation model based on recurrent neural networks. Experiments are conducted on Java projects crawled from GitHub. We show empirically that the performance of ContextCC is superior to state-of-the-art baseline methods. © 2019 Elsevier Inc.","Comment generation; Natural language processing; Neural networks"
"A dataflow-driven approach to identifying microservices from monolithic applications","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070497510&doi=10.1016%2fj.jss.2019.07.008&partnerID=40&md5=1e643829307c4767556056c389f8371f","Microservices architecture emphasizes employing multiple small-scale and independently deployable microservices, rather than encapsulating all function capabilities into one monolith. Correspondingly, microservice-oriented decomposition, which has been identified to be an extremely challenging task, plays a crucial and prerequisite role in developing microservice-based systems. To address the challenges in such a task, we propose a dataflow-driven semi-automatic decomposition approach. In particular, a four-step decomposition procedure is defined: (1) conduct the business requirement analysis to generate use case and business logic specification; (2) construct the fine-grained Data Flow Diagrams (DFD) and the process-datastore version of DFD (DFDPS) representing the business logics; (3) extract the dependencies between processes and datastores into decomposable sentence sets; and (4) identify candidate microservices by clustering processes and their closely related datastores into individual modules from the decomposable sentence sets. To validate this microservice-oriented decomposition approach, we performed a case study on Cargo Tracking System that is a typical case decomposed by other microservices identification methods (Service Cutter and API Analysis), and made comparisons in terms of specific coupling and cohesion metrics. The results show that the proposed dataflow-driven decomposition approach can recommend microservice candidates with sound coupling and cohesion through a rigorous and easy-to-operate implementation with semi-automatic support. © 2019 Elsevier Inc.","Business logic(s); Data flow; Decomposition; Microservices; Monolith; Software engineering"
"Component-based development of embedded systems with GPUs","2020","Journal of Systems and Software","10.1016/j.jss.2019.110488","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076520844&doi=10.1016%2fj.jss.2019.110488&partnerID=40&md5=f486b714fcf438971bf39e84e2e34aa2","One pressing challenge of many modern embedded systems is to successfully deal with the considerable amount of data that originates from the interaction with the environment. A recent solution comes from the use of GPUs, providing a significantly improved performance for data-parallel applications. Another trend in the embedded systems domain is component-based development. However, existing component-based approaches lack specific support to develop embedded systems with GPUs. As a result, components with GPU capability need to encapsulate all the required GPU information, leading to component specialization to specific platforms, hence drastically impeding component reusability. To facilitate component-based development of embedded systems with GPUs, we introduce the concept of flexible components. This increases the design flexibility by allowing the system developer to decide component allocation (i.e., either the CPU or GPU) at a later stage of the system development, with no change to the component implementation. Furthermore, we provide means to automatically generate code for adapting flexible components corresponding to their hardware placement, as well as code for component communication. Through the introduced support, components with GPU capability are platform-independent, and can be executed, without manual adjustment, on a large variety of hardware (i.e., platforms with different GPU characteristics). © 2019","CBD; Component-based development; Embedded systems; GPU; Graphics processing units; Software components"
"Software architecture design in global software development: An empirical study","2019","Journal of Systems and Software","10.1016/j.jss.2019.110400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072283236&doi=10.1016%2fj.jss.2019.110400&partnerID=40&md5=1202309972af2b0455a19f61bd395c3a","In Global Software Development (GSD), the additional complexity caused by global distance requires processes to ease collaboration difficulties, reduce communication overhead, and improve control. How development tasks are broken down, shared and prioritized is key to project success. While the related literature provides some support for architects involved in GSD, guidelines are far from complete. This paper presents a GSD Architectural Practice Framework reflecting the views of software architects, all of whom are working in a distributed setting. In-depth interviews with architects from seven different GSD organizations revealed a complex set of challenges and practices. We found that designing software for distributed teams requires careful selection of practices that support understanding and adherence to defined architectural plans across sites. Teams used Scrum which aided communication, and Continuous Integration which helped solve synchronization issues. However, teams deviated from the design, causing conflicts. Furthermore, there needs to be a balance between the self-organizing Scrum team methodology and the need to impose architectural design decisions across distributed sites. The research presented provides an enhanced understanding of architectural practices in GSD companies. Our GSD Architectural Practice Framework gives practitioners a cohesive set of warnings, which for the most part, are matched by recommendations. © 2019 Elsevier Inc.","Empirical study; Global software development; GSD; GSE; Scrum; Software architecture"
"Cost-efficient dynamic scheduling of big data applications in apache spark on cloud","2020","Journal of Systems and Software","10.1016/j.jss.2019.110515","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077491846&doi=10.1016%2fj.jss.2019.110515&partnerID=40&md5=4c51d81d8321d5f7b1de391b123e1a55","Job scheduling is one of the most crucial components in managing resources, and efficient execution of big data applications. Specifically, scheduling jobs in a cloud-deployed cluster are challenging as the cloud offers different types of Virtual Machines (VMs) and jobs can be heterogeneous. The default big data processing framework schedulers fail to reduce the cost of VM usages in the cloud environment while satisfying the performance constraints of each job. The existing works in cluster scheduling mainly focus on improving job performance and do not leverage from VM types on the cloud to reduce cost. In this paper, we propose efficient scheduling algorithms that reduce the cost of resource usage in a cloud-deployed Apache Spark cluster. In addition, the proposed algorithms can also prioritise jobs based on their given deadlines. Besides, the proposed scheduling algorithms are online and adaptive to cluster changes. We have also implemented the proposed algorithms on top of Apache Mesos. Furthermore, we have performed extensive experiments on real datasets and compared to the existing schedulers to showcase the superiority of our proposed algorithms. The results indicate that our algorithms can reduce resource usage cost up to 34% under different workloads and improve job performance. © 2019","Apache spark; Cloud; Cost-efficiency; Scheduling"
"Evaluating lexical approximation of program dependence","2020","Journal of Systems and Software","10.1016/j.jss.2019.110459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074660962&doi=10.1016%2fj.jss.2019.110459&partnerID=40&md5=fb7ea186f072ed13977d3bcc1552f8dc","Complex dependence analysis typically provides an underpinning approximation of true program dependence. We investigate the effectiveness of using lexical information to approximate such dependence, introducing two new deletion operators to Observation-Based Slicing (ORBS). ORBS provides direct observation of program dependence, computing a slice using iterative, speculative deletion of program parts. Deletions become permanent if they do not affect the slicing criterion. The original ORBS uses a bounded deletion window operator that attempts to delete consecutive lines together. Our new deletion operators attempt to delete multiple, non-contiguous lines that are lexically similar to each other. We evaluate the lexical dependence approximation by exploring the trade-off between the precision and the speed of dependence analysis performed with new deletion operators. The deletion operators are evaluated independently, as well as collectively via a novel generalization of ORBS that exploits multiple deletion operators: Multi-operator Observation-Based Slicing (MOBS). An empirical evaluation using three Java projects, six C projects, and one multi-lingual project written in Python and C finds that the lexical information provides a useful approximation to the underlying dependence. On average, MOBS can delete 69% of lines deleted by the original ORBS, while taking only 36% of the wall clock time required by ORBS. © 2019","Lexical analysis; ORBS; Program slicing"
"LDFR: Learning deep feature representation for software defect prediction","2019","Journal of Systems and Software","10.1016/j.jss.2019.110402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072198290&doi=10.1016%2fj.jss.2019.110402&partnerID=40&md5=abda901529ce30099075791267ebd65a","Software Defect Prediction (SDP) aims to detect defective modules to enable the reasonable allocation of testing resources, which is an economically critical activity in software quality assurance. Learning effective feature representation and addressing class imbalance are two main challenges in SDP. Ideally, the more discriminative the features learned from the modules and the better the rescue performed on the imbalance issue, the more effective it should be in detecting defective modules. In this study, to solve these two challenges, we propose a novel framework named LDFR by Learning Deep Feature Representation from the defect data for SDP. Specifically, we use a deep neural network with a new hybrid loss function that consists of a triplet loss to learn a more discriminative feature representation of the defect data and a weighted cross-entropy loss to remedy the imbalance issue. To evaluate the effectiveness of the proposed LDFR framework, we conduct extensive experiments on a benchmark dataset with 27 defect data (each with three types of features), using three traditional and three effort-aware indicators. Overall, the experimental results demonstrate the superiority of our LDFR framework in detecting defective modules when compared with 27 baseline methods, except in terms of the indicator of Precision. © 2019","Deep feature representation; Deep neural network; Software defect prediction; Triplet loss; Weighted cross-entropy loss"
"Finding needles in a haystack: Leveraging co-change dependencies to recommend refactorings","2019","Journal of Systems and Software","10.1016/j.jss.2019.110420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072512740&doi=10.1016%2fj.jss.2019.110420&partnerID=40&md5=c973e93613b038a3816c0580fcb40729","A fine-grained co-change dependency arises when two fine-grained source-code entities, e.g., a method, change frequently together. This kind of dependency is relevant when considering remodularization efforts (e.g., to keep methods that change together in the same class). However, existing approaches for recommending refactorings that change software decomposition (such as a move method) do not explore the use of fine-grained co-change dependencies. In this paper we present a novel approach for recommending move method and move field refactorings, which removes co-change dependencies and evolutionary smells, a particular type of dependency that arise when fine-grained entities that belong to different classes frequently change together. First we evaluate our approach using 49 open-source Java projects, finding 610 evolutionary smells. Our approach automatically computes 56 refactoring recommendations that remove these evolutionary smells, without introducing new static dependencies. We also evaluate our approach by submitting pull-requests with the recommendations of our technique, in the context of one large and two medium size proprietary Java systems. Quantitative results show that our approach outperforms existing approaches for recommending refactorings when dealing with co-change dependencies. Qualitative results show that our approach is promising, not only for recommending refactorings but also to reveal opportunities of design improvements. © 2019 Elsevier Inc.","Co-change dependencies; Design quality; Refactoring; Remodularization; Software clustering"
"Mahtab: Phase-wise acceleration of regression testing for C","2019","Journal of Systems and Software","10.1016/j.jss.2019.110403","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071749412&doi=10.1016%2fj.jss.2019.110403&partnerID=40&md5=ca61aea482a461c1601325e6c9255ae8","Software regression testing consists of offline, online, and execution phases which are executed sequentially. The offline phase involves code instrumentation and test-coverage collection. Subsequently, the online phase performs program differencing, test-suite selection and prioritization. Finally, the selected test-cases are executed against the new version of software for its re-validation. Regression testing is a time-consuming process and is often on the critical path of the project. To improve the turn-around time of software development cycle, our goal is to reduce regression testing time across all phases using multi-core parallelization. This poses several challenges that stem from I/O, dependence on third-party libraries, and inherently sequential components in the overall testing process. We propose parallelization test-windows to effectively partition test-cases across threads. To measure the benefit of prioritization coupled with multi-threaded execution, we propose a new metric, EPSilon, for rewarding failure observation frequency in the timeline of test-execution. To measure the rate of code-change coverage due to regression test prioritization, we introduce ECC, a variant of the widely used APFD metric. We illustrate the effectiveness of our approach using the popular Software-artifact Infrastructure Repository (SIR) and five real-world projects from GitHub. © 2019 Elsevier Inc.","Parallelization window; Regression test selection; Relevance-and-confinedness; Test-prioritization"
"Model checking of in-vehicle networking systems with CAN and FlexRay","2020","Journal of Systems and Software","10.1016/j.jss.2019.110461","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075921003&doi=10.1016%2fj.jss.2019.110461&partnerID=40&md5=5f6e041dd020fe99b135b735d3da51bd","An in-vehicle networking (IVN) system consists of electronic components that are connected by buses and communicate through multiple protocols according to their requirements. In practice, intelligent vehicles need to exchange data between subsystems that use various protocols, such as the controller area network (CAN) and FlexRay. Such systems are more likely to encounter delays and message loss during transmission, presenting serious safety issues. Moreover, IVN systems are extremely complicated because of their large number of nodes, multiple communication protocols, and diverse topologies. As a result, it is difficult to check the timed properties of the system directly and accurately. In this paper, we present an appropriate abstraction for modeling IVN systems that utilize CAN and FlexRay during the design phase. The timed properties of communication are analyzed using the UPPAAL platform. As there are numerous IVN system structures, a framework is developed to build a model for an IVN system design. The model is to verify the transmission of messages between different protocols. We evaluate the validity, applicability, and reusability of the framework and show the performance of the framework for verifying IVN system models. © 2019","CAN; FlexRay; In-vehicle networking systems; Model checking; UPPAAL"
"Aligning software engineering education with industrial needs: A meta-analysis","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067367468&doi=10.1016%2fj.jss.2019.06.044&partnerID=40&md5=d0208c7c6372af683601072ca290e671","Context: According to various reports, many software engineering (SE) graduates often face difficulties when beginning their careers, which is mainly due to misalignment of the skills learned in university education with what is needed in the software industry. Objective: Our objective is to perform a meta-analysis to aggregate the results of the studies published in this area to provide a consolidated view on how to align SE education with industry needs, to identify the most important skills and also existing knowledge gaps. Method: To synthesize the body of knowledge, we performed a systematic literature review (SLR), in which we systematically selected a pool of 35 studies and then conducted a meta-analysis using data extracted from those studies. Results: Via a meta-analysis and using data from 13 countries and over 4,000 data points, highlights of the SLR include: (1) software requirements, design, and testing are the most important skills; and (2) the greatest knowledge gaps are in configuration management, SE models and methods, SE process, design (and architecture), as well as in testing. Conclusion: This paper provides implications for both educators and hiring managers by listing the most important SE skills and the knowledge gaps in the industry. © 2019 Elsevier Inc.","Important skills; Industry needs; Knowledge gap; Meta-analysis; Software engineering education; Systematic literature review (SLR)"
"A model-driven approach for the development of native mobile applications focusing on the data layer","2020","Journal of Systems and Software","10.1016/j.jss.2019.110489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076522972&doi=10.1016%2fj.jss.2019.110489&partnerID=40&md5=bfbb8a4d4438f9db98a6adc0ac5f5a5c","The data layer access design is a critical task for mobile applications which need constant access to remote data, making them available offline in case of network connectivity problems. Moreover, the variety of mobile operating systems and platforms (fragmentation phenomenon) that handles data storage differently affects the portability of mobile applications. This issue suggests the use of Model-Driven Development (MDD) approach that may ease the smartphone application development for different platforms. Therefore, we propose an extension of the Model-Oriented Web Approach (MoWebA) for the development of native mobile applications focusing on the data layer. MoWebA Mobile (as we name the proposal) covers data persistence concepts to achieve offline applications in case of network connectivity problems. It defines meta-models and the Architecture-Specific Model (ASM) for data persistence and data provider to design the data sources of mobile applications. As well, we propose transformation rules for the generation of Android and Windows Phone applications. The MoWebA Mobile process was illustrated by means of modeling, design, and development of a typical mobile application. Despite the learning curve of the approach, the first evaluation suggests that MoWebA Mobile has the potential for supporting the mobile design applications considering the persistence of data. © 2019 Elsevier Inc.","Data layer; Data persistence; Data provider; Mobile applications; Model-Driven Development; Model-Driven Mobile Development"
"CrossRec: Supporting software developers by recommending third-party libraries","2020","Journal of Systems and Software","10.1016/j.jss.2019.110460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075896516&doi=10.1016%2fj.jss.2019.110460&partnerID=40&md5=bc912b1263dc2544c5044f82e1c25aeb","When creating a new software system, or when evolving an existing one, developers do not reinvent the wheel but, rather, seek available libraries that suit their purpose. In such a context, open source software repositories contain rich resources that can provide developers with helpful advice to support their tasks. However, the heterogeneity of resources and the dependencies among them are the main obstacles to the effective mining and exploitation of the available data. In this sense, advanced techniques and tools are needed to mine the metadata to bring in meaningful recommendations. In this paper, we present CrossRec, a recommender system to assist open source software developers in selecting suitable third-party libraries. CrossRec exploits a collaborative filtering technique to recommend libraries to developers by relying on the set of dependencies, which are currently included in the project being developed. We perform an empirical evaluation to compare the proposed approach with three state-of-the-art baselines, i.e., LibRec, LibFinder, and LibCUP on three considerably large datasets. The experimental results show that CrossRec overcomes the limitation of the baselines by recommending also libraries with a specific version. More importantly, it outperforms LibRec and LibCUP with respect to various quality metrics. © 2019 Elsevier Inc.","Mining software repositories; Open Source software; Recommender systems"
"An automated change impact analysis approach for User Requirements Notation models","2019","Journal of Systems and Software","10.1016/j.jss.2019.110397","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070923402&doi=10.1016%2fj.jss.2019.110397&partnerID=40&md5=d063a07e82b71d6dfb071b321b643228","Requirements and their models often evolve to reflect changing needs, technologies, and regulations. The decision to implement proposed changes to requirements models relies on means to capture and analyze the potential impact of such changes. The User Requirements Notation (URN) is a standardized requirements modeling language that incorporates two complementary views based on Use Case Maps (UCM), for expressing scenarios and processes bound to architectural components, and on the Goal-oriented Requirement Language (GRL), for capturing the goals of actors and their relationships. This paper presents a new Change Impact Analysis (CIA) approach for URN models. Following a proposed change, this approach helps identify potentially impacted URN constructs within the selected GRL/UCM view, as well as throughout other view elements connected with URN links. This URN-oriented CIA approach is implemented as an extension of the jUCMNav modeling environment, and its applicability is demonstrated using an illustrative URN specification and three real and publicly available specifications. Furthermore, an empirical study involving 10 participants is used to assess the accuracy of this approach in identifying impacted URN elements upon specification changes. Results indicate excellent accuracy and a significant reduction in user-perceived difficulty when estimating the impact of changes in URN specifications. © 2019 Elsevier Inc.","Change impact analysis; Goal-Oriented Requirement Language (GRL); jUCMNav; Requirements; Use Case Map (UCM); User Requirements Notation (URN)"
"Predicting failures in multi-tier distributed systems","2020","Journal of Systems and Software","10.1016/j.jss.2019.110464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076597363&doi=10.1016%2fj.jss.2019.110464&partnerID=40&md5=94247d4584f8b4f0072716e8f5e8aabe","Many applications are implemented as multi-tier software systems, and are executed on distributed infrastructures, like cloud infrastructures, to benefit from the cost reduction that derives from dynamically allocating resources on-demand. In these systems, failures are becoming the norm rather than the exception, and predicting their occurrence, as well as locating the responsible faults, are essential enablers of preventive and corrective actions that can mitigate the impact of failures, and significantly improve the dependability of the systems. Current failure prediction approaches suffer either from false positives or limited accuracy, and do not produce enough information to effectively locate the responsible faults. In this paper, we present PreMiSE, a lightweight and precise approach to predict failures and locate the corresponding faults in multi-tier distributed systems. PreMiSE blends anomaly-based and signature-based techniques to identify multi-tier failures that impact on performance indicators, with high precision and low false positive rate. The experimental results that we obtained on a Cloud-based IP Multimedia Subsystem indicate that PreMiSE can indeed predict and locate possible failure occurrences with high precision and low overhead. © 2019 Elsevier Inc.","Cloud computing; Data analytics; Failure prediction; Machine learning; Multi-tier distributed systems; Self-healing systems"
"StaDART: Addressing the problem of dynamic code updates in the security analysis of android applications","2020","Journal of Systems and Software","10.1016/j.jss.2019.07.088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073942145&doi=10.1016%2fj.jss.2019.07.088&partnerID=40&md5=b374e8583ebf85a0697bfeed908a6ae8","Dynamic code update techniques (Android Studio – support for dynamic delivery), such as dynamic class loading and reflection, enable Android apps to extend their functionality at runtime. At the same time, these techniques are misused by malware developers to transform a seemingly benign app into a malware, once installed on a real device. Among the corpus of evasive techniques used in modern real-world malware, evasive usage of dynamic code updates plays a key role. First, we demonstrate the ineffectiveness of existing tools to analyze apps in the presence of dynamic code updates using our test apps, i.e., Reflection-Bench and InboxArchiver. Second, we present StaDART, combining static and dynamic analysis of Android apps to reveal the concealed behavior of malware. StaDART performs dynamic code interposition using a vtable tampering technique for API hooking to avoid modifications to the Android framework. Furthermore, we integrate it with a triggering solution, DroidBot, to make it more scalable and fully automated. We present our evaluation results with a dataset of 2000 real world apps; containing 1000 legitimate apps and 1000 malware samples. The evaluation results with this dataset and Reflection-Bench show that StaDART reveals suspicious behavior that is otherwise hidden to static analysis tools. © 2019","Android; Dynamic class loading; Dynamic code updates; Reflection; Security analysis"
"hW-inference: A heuristic approach to retrieve models through black box testing","2020","Journal of Systems and Software","10.1016/j.jss.2019.110426","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074001539&doi=10.1016%2fj.jss.2019.110426&partnerID=40&md5=a9d074796ef42604fcca019fe021a3ab","We present an efficient approach to retrieve behavioural models from reactive software systems in the form of Finite State Machines by testing them. The system is accessed in black box mode; thus, no source or binary code is needed. The novelty of the approach is that it does not require to reset the system between tests (queries) and does not require any knowledge of the system apart from its input domain. Experiments have shown that it can scale up to systems that may have thousands of states. © 2019 Elsevier Inc.","FSM; Model based testing; Model inference; Reverse engineering"
"ANDROIDOFF:Offloading android application based on cost estimation","2019","Journal of Systems and Software","10.1016/j.jss.2019.110418","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072523999&doi=10.1016%2fj.jss.2019.110418&partnerID=40&md5=1f1dec8e07e13ff526be60e0b7b6c013","Computation offloading is a promising way of improving the performance and reducing the battery power consumption, since it moves some time-consuming computation activities to nearby servers. Although various approaches have been proposed to support computation offloading, we argue that there is still sufficient space for improvements, since existing approaches cannot accurately estimate the execution costs. As a result, we find that their offloading plans are less optimized. To handle the problem, in this paper, given an Android application, we propose a novel approach, called ANDROIDOFF, that supports offloading at the granularity of objects. Supporting such capability is challenging due to the two reasons: (1) through dynamic execution, it is feasible to collect the execution costs of only partial methods, and (2) it is difficult to accurately estimate the execution costs of the remaining methods. To overcome the challenges, given an Android application, ANDROIDOFF first combines static and dynamic analysis to predict the execution costs of all its methods. After all the costs are estimated, ANDROIDOFF synthesizes an offloading plan, in which determines the offloading details. We evaluate ANDROIDOFF on a real-world application, with two mobile devices. Our results show that, compared with other approaches, ANDROIDOFF saves the response time by 8%–49% and reduces the energy consumption by 12%–49% on average for computation-intensive applications. © 2019 Elsevier Inc.","Code analysis; Computation offloading; Mobile edge computing"
"Scented since the beginning: On the diffuseness of test smells in automatically generated test code","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069618644&doi=10.1016%2fj.jss.2019.07.016&partnerID=40&md5=c680d364ff5958e74087b43b7c8794b3","Software testing represents a key software engineering practice to ensure source code quality and reliability. To support developers in this activity and reduce testing effort, several automated unit test generation tools have been proposed. Most of these approaches have the main goal of covering as more branches as possible. While these approaches have good performance, little is still known on the maintainability of the test code they produce, i.e.,whether the generated tests have a good code quality and if they do not possibly introduce issues threatening their effectiveness. To bridge this gap, in this paper we study to what extent existing automated test case generation tools produce potentially problematic test code. We consider seven test smells, i.e.,suboptimal design choices applied by programmers during the development of test cases, as measure of code quality of the generated tests, and evaluate their diffuseness in the unit test classes automatically generated by three state-of-the-art tools such as RANDOOP, JTEXPERT, and EVOSUITE. Moreover, we investigate whether there are characteristics of test and production code influencing the generation of smelly tests. Our study shows that all the considered tools tend to generate a high quantity of two specific test smell types, i.e.,Assertion Roulette and Eager Test, which are those that previous studies showed to negatively impact the reliability of production code. We also discover that test size is correlated with the generation of smelly tests. Based on our findings, we argue that more effective automated generation algorithms that explicitly take into account test code quality should be further investigated and devised. © 2019 Elsevier Inc.","Empirical studies; Software quality; Test case generation; Test smells"
"A machine-learning based ensemble method for anti-patterns detection","2020","Journal of Systems and Software","10.1016/j.jss.2019.110486","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076559059&doi=10.1016%2fj.jss.2019.110486&partnerID=40&md5=ec8a0ff172eff58af69f033014cacec9","Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods. © 2019","Anti-patterns; Ensemble methods; Machine learning; Software quality"
"Feature dependencies in automotive software systems: Extent, awareness, and refactoring","2020","Journal of Systems and Software","10.1016/j.jss.2019.110458","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074794408&doi=10.1016%2fj.jss.2019.110458&partnerID=40&md5=29777002ff6c8242319f689b450f5150","Many automotive companies consider their software development process to be feature-oriented. In the past, features were regarded as isolated system parts developed and tested by developers from different departments. However, in modern vehicles, features are more and more connected and their behavior depends on each other in many situations. In this article, we describe how feature-oriented software development is conducted in automotive companies and which challenges arise from that. We present an empirical analysis of feature dependencies in three real-world automotive systems. The analysis shows that features in modern vehicles are highly interdependent. Furthermore, the study reveals that developers are not aware of these dependencies in most cases. For the three examined cases, we show that less than 12% of the components in the system architecture are responsible for more than 90% of the feature dependencies. Finally, we propose a refactoring approach for implicit communal components, which makes them explicit by moving them to a dedicated platform component layer. © 2019","Automotive; Empirical study; Feature interaction; Requirements engineering; Software architecture; Technical debt"
"ECOS: An efficient task-clustering based cost-effective aware scheduling algorithm for scientific workflows execution on heterogeneous cloud systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.110405","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071725077&doi=10.1016%2fj.jss.2019.110405&partnerID=40&md5=a9e63acb66976c76f1f11975551f6d55","Cloud Computing provides an attractive execution environment for scientific workflow execution. However, due to the increasingly high charge cost of using cloud service, cost minimization for workflows execution on cloud systems has become a crucial issue. Traditional work are adopting the sophisticated scheduling techniques to address such issue. Differently, this paper has proposed an efficient task-clustering based cost-effective aware scheduling algorithm (ECOS) to minimize the cost without comprising the deadline constraint. First, with respect to the characteristics of multi-type workflows, cloud heterogeneity and cost model, we have formulated the problem of task-clustering to simplify the structure of workflows and workflow scheduling to minimize cost within the deadline constraint. Then, we have devised ECOS with two key steps: (1) vertical clustering is with the time consideration that selectively merges the sequential tasks to reduce the transferring time within the workflow; (2) horizontal clustering and greedy allocation is to aggregate the parallel tasks and greedily allocate resources to that tasks with the aim of minimizing cost within deadline. Last, we have conducted the experiment that compare with well-known task-clustering based algorithms via WorkflowSim platform. The results have demonstrated that ECOS can efficiently merge tasks and minimize the total cost without comprising the deadline constraint both in small and large datasets. Moreover, we have discussed the ECOS in terms of various schedulers and number of tasks to validate the performance of ECOS. © 2019 Elsevier Inc.","Cloud computing; Cost minimization; Greedy allocation; Heterogeneous cloud systems; Scientific workflows scheduling; Task clustering"
"Leveraging creativity in requirements elicitation within agile software development: A systematic literature review","2019","Journal of Systems and Software","10.1016/j.jss.2019.110396","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071319044&doi=10.1016%2fj.jss.2019.110396&partnerID=40&md5=7b4ac4bad6ec3d40390d28b28222f2eb","Agile approaches tend to focus solely on scoping and simplicity rather than on problem solving and discovery. This hampers the development of innovative solutions. Additionally, little has been said about how to capture and represent the real user needs. To fill this gap, some authors argue in favor of the application of “Creative thinking” for requirements elicitation within agile software development. This synergy between creativeness and agility has arisen as a new means of bringing innovation and flexibility to increasingly demanding software. The aim of the present study is therefore to employ a systematic review to investigate the state-of-the-art of those approaches that leverage creativity in requirements elicitation within Agile Software Development, as well as the benefits, limitations and strength of evidence of these approaches. The review was carried out by following the guidelines proposed by Dr. Kitchenham. The search strategy identified 1451 studies, 17 of which were eventually classified as primary studies. The selected studies contained 13 different and unique proposals. These approaches provide evidence that enhanced creativity in requirements elicitation can be successfully implemented in real software projects. We specifically observed that projects related to user interface development, such as those for mobile or web applications, are good candidates for the use of these approaches. We have also found that agile methodologies such as Scrum, Extreme Programming or methodologies based on rapid modelling are preferred when introducing creativity into requirements elicitation. Despite this being a new research field, there is a mixture of techniques, tools and processes that have already been and are currently being successfully tested in industry. Finally, we have found that, although creativity is an important ingredient with which to bring about innovation, it is not always sufficient to generate new requirements because this needs to be followed by user engagement and a specific context in which proper conditions, such as flexibility, time or resources, have to be met. © 2019","Agile methodologies; Creative thinking; Requirements elicitation; Software development; Software project management; Systematic review"
"Teamwork behaviors in implementing enterprise systems with multiple projects: Results from Chinese firms","2019","Journal of Systems and Software","10.1016/j.jss.2019.110392","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070648011&doi=10.1016%2fj.jss.2019.110392&partnerID=40&md5=ef8ac0a8b5630d031b6dd8741900197e","Enterprise system (ES) implementations are unique in their scope to encompass information transfer and standardization across an entire organization. The ES has become a ubiquitous software product essential for many organizations. A successful ES implementation requires integrating multiple projects, over a wide time span, led by managers across diverse functions and interests. The single project principles proven useful for the development of less complex systems require an upgrade to account for the need to integrate the activities and outputs of the multiple projects in an ES implementation. In particular, the set of goals must move all projects to deliver the ES scope while expending only the allocated resources available to an organization. Based on the literature of social interdependence theory, we develop a model to examine teamwork behaviors in the context of an ES implementation. Specifically, goal interdependence among the multiple ES projects fosters promotive interteam behaviors of monitoring across ES project teams and ES project team adaptability, which, in turn, lead to higher levels of implementation performance. The model holds true according to data from a sample of key participants in recent ES implementations in small to medium Chinese enterprises. Interdependent goals allow for a degree of functional independence while pursuing goals important to the organization as a whole. © 2019","Enterprise systems; ES integration team; Goal interdependence; Inter-teamwork; Multiple projects"
"A topological analysis of communication channels for knowledge sharing in contemporary GitHub projects","2019","Journal of Systems and Software","10.1016/j.jss.2019.110416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072155257&doi=10.1016%2fj.jss.2019.110416&partnerID=40&md5=822ea7796cc3f6f71e7396b860d65708","With over 28 million developers, success of the GitHub collaborative platform is highlighted through an abundance of communication channels among contemporary software projects. Knowledge is broken into two forms and its sharing (through communication channels) can be described as externalization or combination by the SECI model. Such platforms have revolutionized the way developers work, introducing new channels to share knowledge in the form of pull requests, issues and wikis. It is unclear how these channels capture and share knowledge. In this research, our goal is to analyze these communication channels in GitHub. First, using the SECI model, we are able to map how knowledge is shared through the communication channels. Then in a large-scale topology analysis of seven library package projects (i.e., involving over 70 thousand projects), we extracted insights of the different communication channels within GitHub. Using two research questions, we explored the evolution of the channels and adoption of channels by both popular and unpopular library package projects. Results show that (i) contemporary GitHub Projects tend to adopt multiple communication channels, (ii) communication channels change over time and (iii) communication channels are used to both capture new knowledge (i.e., externalization) and updating existing knowledge (i.e., combination). © 2019 Elsevier Inc.",""
"Energy-Delay investigation of Remote Inter-Process communication technologies","2020","Journal of Systems and Software","10.1016/j.jss.2019.110506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077509321&doi=10.1016%2fj.jss.2019.110506&partnerID=40&md5=38c1298c8c6fe6311432a11e589c3414","Most modern information technology devices use the Internet for creating, reading, updating, and deleting shared data through remote inter-process communication (IPC). To evaluate the energy consumption of IPC technologies and the corresponding run-time performance implications, we performed an empirical study on popular IPC systems implemented in Go, Java, JavaScript, Python, PHP, Ruby, and C#. We performed our experiments on computer platforms equipped with Intel and ARM processors. We observed that JavaScript and Go implementations of gRPCoffer the lowest energy consumption and execution time. Furthermore, by analysing their system call traces, we found that inefficient use of system calls can contribute to increased energy consumption and poor execution time. © 2019 Elsevier Inc.","Energy Efficiency; Programming Languages; Remote Inter-Process Communication; System Calls"
"Why and how to balance alignment and diversity of requirements engineering practices in automotive","2020","Journal of Systems and Software","10.1016/j.jss.2019.110516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077505760&doi=10.1016%2fj.jss.2019.110516&partnerID=40&md5=fb156c861016408b6c1186ac3c420998","In large-scale automotive companies, various requirements engineering (RE) practices are used across teams. RE practices manifest in Requirements Information Models (RIM) that define what concepts and information should be captured for requirements. Collaboration of practitioners from different parts of an organization is required to define a suitable RIM that balances support for diverse practices in individual teams with the alignment needed for a shared view and team support on system level. There exists no guidance for this challenging task. This paper presents a mixed methods study to examine the role of RIMs in balancing alignment and diversity of RE practices in four automotive companies. Our analysis is based on data from systems engineering tools, 11 semi-structured interviews, and a survey to validate findings and suggestions. We found that balancing alignment and diversity of RE practices is important to consider when defining RIMs. We further investigated enablers for this balance and actions that practitioners take to achieve it. From these factors, we derived and evaluated recommendations for managing RIMs in practice that take into account the lifecycle of requirements and allow for diverse practices across sub-disciplines in early development, while enforcing alignment of requirements that are close to release. © 2019","Aligning software engineering practices; Automotive software engineering; Large-scale software development; Mixed methods research; Requirements information models"
"Automated code-based test selection for software product line regression testing","2019","Journal of Systems and Software","10.1016/j.jss.2019.110419","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072228048&doi=10.1016%2fj.jss.2019.110419&partnerID=40&md5=2f0f8c13098a8fe14791766e26574e66","Regression testing for software product lines (SPLs) is challenging and can be expensive because it must ensure that all the products of a product family are correct whenever changes are made. SPL regression testing can be made efficient through a test case selection method that selects only the test cases relevant to the changes. Some approaches for SPL test case selection have been proposed but either they were not efficient by requiring intervention from human experts or they cannot be used if requirements specifications, architecture and/or traceabilities for test cases are not available or partially eroded. To address these limitations, we propose an automated method of source code-based regression test selection for SPLs. Our method reduces the repetition of the selection procedure and minimizes the in-depth analysis effort for source code and test cases based on the commonality and variability of a product family. Evaluation results of our method using six product lines show that our method reduces the overall time to perform regression testing by 14.8% ∼ 49.1% on average compared to an approach of repetitively applying Ekstazi, which is the state-of-the-art regression test selection method for a single product, to each product of a product family. © 2019","Product lines testing; Regression test selection; Software evolution; Software maintenance"
"SPELLing out energy leaks: Aiding developers locate energy inefficient code","2020","Journal of Systems and Software","10.1016/j.jss.2019.110463","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076571333&doi=10.1016%2fj.jss.2019.110463&partnerID=40&md5=bc76964406171ec89848c40317bef37d","Although hardware is generally seen as the main culprit for a computer's energy usage, software too has a tremendous impact on the energy spent. Unfortunately, there is still not enough support for software developers so they can make their code more energy-aware. This paper proposes a technique to detect energy inefficient fragments in the source code of a software system. Test cases are executed to obtain energy consumption measurements, and a statistical method, based on spectrum-based fault localization, is introduced to relate energy consumption to the source code. The result of our technique is an energy ranking of source code fragments pointing developers to possible energy leaks in their code. This technique was implemented in the SPELL toolkit. Finally, in order to evaluate our technique, we conducted an empirical study where we asked participants to optimize the energy efficiency of a software system using our tool, while also having two other groups using no tool assistance and a profiler, respectively. We showed statistical evidence that developers using our technique were able to improve the energy efficiency by 43% on average, and even out performing a profiler for energy optimization. © 2019 Elsevier Inc.","Fault Localization; Green Computing; Green Software; Program Analysis; Program Optimization"
"Sequence effects in the estimation of software development effort","2020","Journal of Systems and Software","10.1016/j.jss.2019.110448","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073923576&doi=10.1016%2fj.jss.2019.110448&partnerID=40&md5=4c9eb5e9c0cc02133ae7e19c4b58392c","Currently, little is known about how much the sequence in which software development tasks or projects are estimated affects judgment-based effort estimates. To gain more knowledge, we examined estimation sequence effects in two experiments. In the first experiment, 362 software professionals estimated the effort of three large tasks of similar sizes, whereas in the second experiment 104 software professionals estimated the effort of four large and five small tasks. The sequence of the tasks was randomised in both experiments. The first experiment, with tasks of similar size, showed a mean increase of 10% from the first to the second and a 3% increase from the second to the third estimate. The second experiment showed that estimating a larger task after a smaller one led to a mean decrease in the estimate of 24%, and that estimating a smaller task after a larger one led to a mean increase of 25%. There was no statistically significant reduction in the sequence effect with higher competence. We conclude that more awareness about how the estimation sequence affects the estimates may reduce potentially harmful estimation biases. In particular, it may reduce the likelihood of a bias towards too low effort estimates. © 2019","Effort estimation; Human judgment; Sequence effect; Software development"
"Effective testing of Android apps using extended IFML models","2020","Journal of Systems and Software","10.1016/j.jss.2019.110433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073748144&doi=10.1016%2fj.jss.2019.110433&partnerID=40&md5=34f4f5d2021204875285fb770283dd74","The last decade has seen a vast proliferation of mobile apps. To improve the reliability of such apps, various techniques have been developed to automatically generate tests for them. While such techniques have been proven to be useful in producing test suites that achieve significant levels of code coverage, there is still enormous demand for techniques that effectively generate tests to exercise more code and detect more bugs of apps. We propose in this paper the ADAMANT approach to automated Android app testing. ADAMANT utilizes models that incorporate valuable human knowledge about the behaviours of the app under consideration to guide effective test generation, and the models are encoded in an extended version of the Interaction Flow Modeling Language (IFML). In an experimental evaluation on 10 open source Android apps, ADAMANT generated over 130 test actions per minute, achieved around 68% code coverage, and exposed 8 real bugs, significantly outperforming other test generation tools like MONKEY, ANDROIDRIPPER, and GATOR in terms of code covered and bugs detected. © 2019","Android apps; Interaction Flow Modeling Language; Model-based testing"
"An empirical study of security warnings from static application security testing tools","2019","Journal of Systems and Software","10.1016/j.jss.2019.110427","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072696763&doi=10.1016%2fj.jss.2019.110427&partnerID=40&md5=4653173a73b896b12be4772c8968a1e1","The Open Web Application Security Project (OWASP) defines Static Application Security Testing (SAST) tools as those that can help find security vulnerabilities in the source code or compiled code of software. Such tools detect and classify the vulnerability warnings into one of many types (e.g., input validation and representation). It is well known that these tools produce high numbers of false positive warnings. However, what is not known is if specific types of warnings have a higher predisposition to be false positives or not. Therefore, our goal is to investigate the different types of SAST-produced warnings and their evolution over time to determine if one type of warning is more likely to have false positives than others. To achieve our goal, we carry out a large empirical study where we examine 116 large and popular C++ projects using six different state-of-the-art open source and commercial SAST tools that detect security vulnerabilities. In order to track a piece of code that has been tagged with a warning, we use a new state of the art framework called cregit+ that traces source code lines across different commits. The results demonstrate the potential of using SAST tools as an assessment tool to measure the quality of a product and the possible risks without manually reviewing the warnings. In addition, this work shows that pattern-matching static analysis technique is a very powerful method when combined with other advanced analysis methods. © 2019","False positives; Security warnings; Software vulnerability; Static application security testing tools"
"A data replication strategy with tenant performance and provider economic profit guarantees in Cloud data centers","2020","Journal of Systems and Software","10.1016/j.jss.2019.110447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073951997&doi=10.1016%2fj.jss.2019.110447&partnerID=40&md5=28554622421052ceab99acdf527208d9","Meeting tenant performance requirements through data replication while ensuring an economic profit is very challenging for cloud providers. For this purpose, we propose a data Replication Strategy that satisfies Performance tenant objective and provider profit in Cloud data centers (RSPC). Before the execution of each tenant query Q, data replication is considered only if: (i) the estimated Response Time of Q (RTQ) exceeds a critical RT threshold (per-query replication), or (ii) more often, if RTQ exceeds another (lower) RT threshold for a given number of times (replication per set of queries). Then, a new replica is really created only if a suitable replica placement is heuristically found so that the RT requirement is satisfied again while ensuring an economic profit for the provider. Both the provider's revenues and expenditures are also estimated while penalties and replication costs are taken into account. Furthermore, the replica factor is dynamically adjusted in order to reduce the resource consumption. Compared to four other strategies, RSPC best satisfies the RT requirement under high loads, complex queries and strict RT thresholds. Moreover, penalty and data transfer costs are significantly reduced, which impacts the provider profit. © 2019","Cloud systems; Cost model; Data replication; Databases; Economic profit; Performance"
"A survey on the use of access permission-based specifications for program verification","2020","Journal of Systems and Software","10.1016/j.jss.2019.110450","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074537486&doi=10.1016%2fj.jss.2019.110450&partnerID=40&md5=1e007a3296a81146055cb355d4843c80","Verifying the correctness and reliability of imperative and object-oriented programs is one of the grand challenges in computer science. In imperative programming models, programmers introduce concurrency manually by using explicit concurrency constructs such as multi-threading. Multi-threaded programs are prone to synchronization problems such as data races and dead-locks, and verifying API protocols in object-oriented programs is a non-trivial task due to improper and unexpected state transition at run time. This is in part due to the unexpected sharing of program states in such programs. With these considerations in mind, access permissions have been investigated as a means to reasoning about the correctness of such programs. Access permissions are abstract capabilities that characterize the way a shared resource can be accessed by multiple references. This paper provides a comprehensive survey of existing access permission-based verification approaches. We describe different categories of permissions and permission-based contracts. We elaborate how permission-based specifications have been used to ensure compliance of API protocols and to avoid synchronization problems in concurrent programs. We compare existing approaches based on permission usage, analysis performed, language and/or tool supported, and properties being verified. Finally, we provide insight into the research challenges posed by existing approaches and suggest future directions. © 2019 Elsevier Inc.","Access permissions; Concurrency; Permission inference; Program verification; Protocol verification; Survey"
"Action-Oriented Programming Model: Collective Executions and Interactions in the Fog","2019","Journal of Systems and Software","10.1016/j.jss.2019.110391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070882337&doi=10.1016%2fj.jss.2019.110391&partnerID=40&md5=5fe7ac6015639b4c082f2f979f72bffb","Today's dominant design for the Internet of Things (IoT) is a Cloud-based system, where devices transfer their data to a back-end and in return receive instructions on how to act. This view is challenged when delays caused by communication with the back-end become an obstacle for IoT applications with, for example, stringent timing constraints. In contrast, Fog Computing approaches, where devices communicate and orchestrate their operations collectively and closer to the origin of data, lack adequate tools for programming secure interactions between humans and their proximate devices at the network edge. This paper fills the gap by applying Action-Oriented Programming (AcOP) model for this task. While originally the AcOP model was proposed for Cloud-based infrastructures, presently it is re-designed around the notion of coalescence and disintegration, which enable the devices to collectively and autonomously execute their operations in the Fog by serving humans in a peer-to-peer fashion. The Cloud's role has been minimized—it is being leveraged as a development and deployment platform. © 2019 The Authors","Edge computing; Fog Computing; Programming model; Proximity-based computing; Socio-technical systems"
"Graph-based root cause analysis for service-oriented and microservice architectures","2020","Journal of Systems and Software","10.1016/j.jss.2019.110432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073941287&doi=10.1016%2fj.jss.2019.110432&partnerID=40&md5=49d3be6d8238989f81fbc76eda13a4bc","Service-oriented architectures and microservices define two ways of designing software with the aim of dividing an application into loosely-coupled services that communicate among each other. This translates into rapid development, where each service is developed and deployed by small teams, enabling continuous shipping of new features and fast-evolving applications. However, the underlying complexity of this type of architecture can hinder observability and maintenance by the user. In particular, identifying the root cause of an anomaly detected in the application can be a difficult and time-consuming task, considering the numerous services and connections to be examined. In this work, we present a root cause analysis framework, based on graph representations of these architectures. The graphs can be used to compare any anomalous situation that happens in the system with a library of anomalous graphs that serves as a knowledge base for the user troubleshooting those anomalies. We use the Grid’5000 testbed to deploy three different architectures and inject a set of anomalies. The results show how our graph-based approach is 19.41% more effective than a machine learning method that does not take into account the relationship between elements. © 2019","Containers; Graphs; Microservices; Root Cause Analysis; SOA"
"Adopting DevOps in the real world: A theory, a model, and a case study","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070228584&doi=10.1016%2fj.jss.2019.07.083&partnerID=40&md5=b470067a5546ef570fda4d6c8fbd40ef","DevOps is a set of practices and cultural values that aims to reduce the barriers between development and operations teams. Due to its increasing interest and imprecise definitions, existing research works have tried to characterize DevOps. Nevertheless, little is known about the practitioners’ understandingabout successful paths for DevOps adoption. Therefore, our goal is to detail real scenarios of DevOps adoption, presenting a theory, a model, and a case study. We used classic Grounded Theory to build a theory about 15 scenarios of successful DevOps adoption in companies from different domains and countries. We proposed a model (i.e., a workflow for DevOps adoption) and evaluated it through a case study at a Brazilian Government institution. We used a focus group to collect the company perceptions about DevOps adoption. This paper increments the existing view of DevOps by detailing real scenarios and explaining the role of each category during DevOps adoption. We provide evidence that collaboration is the core DevOps concern, contrasting with an existing wisdom that automation and tooling can be enough to achieve DevOps. Altogether, our results contribute to: generating an adequate understanding of DevOps, from the practitioners’ perspective; and assisting other institutions in the path towards DevOps adoption. © 2019","DevOps; Focus group; Grounded theory; Software development; Software operations"
"An approach to solving non-linear real constraints for symbolic execution","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073705853&doi=10.1016%2fj.jss.2019.07.045&partnerID=40&md5=75481c511f24b4991e094712dd038583","Constraint solvers are well-known tools for solving many real-world problems such as theorem proving and real-time scheduling. One of the domains that strongly relies on constraint solvers is the technique of symbolic execution for automatic test data generation. Many researchers have tried to alleviate the shortcomings of the available constraint solvers to improve their applications in symbolic execution for test data generation. Despite many recent improvements, constraint solvers are still unable to efficiently deal with certain types of constraints. In particular, constraints that include non-linear real arithmetic are among the most challenging ones. In this paper, we propose a new approach to solving non-linear real constraints for symbolic execution. This approach emphasizes transforming constraints into functions with specific properties, which are named Satisfaction Functions. A satisfaction function is generated in a way that by maximizing it, values that satisfy the corresponding constraint are obtained. We compared the performance of our technique with three constraint solvers that were known to be able to solve non-linear real constraints. The comparison was made regarding the speed and correctness criteria. The results showed that our technique was comparable with other methods regarding the speed criterion and outperformed these methods regarding the correctness criterion. © 2019","Constraint solving; Decision procedure; Non-linear arithmetic; Software testing; Symbolic execution; Test data generation"
"Modeling, analyzing and predicting security cascading attacks in smart buildings systems-of-systems","2020","Journal of Systems and Software","10.1016/j.jss.2019.110484","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077182748&doi=10.1016%2fj.jss.2019.110484&partnerID=40&md5=a79379998a11364f95f571cbc67e446d","Software systems intelligence and complexity have been continuously increasing to deliver more and more features to support business critical and mission critical processes in numerous domains such as defense, health-care, and smart cities. Contemporary software-based solutions are composed of several software systems, that form System-of-Systems (SoS). SoS differentiating characteristics, such as emergent behavior, introduce specific issues that render their security modeling, simulation and analysis a critical challenge. The aim of this work is to investigate how Software Engineering (SE) approaches can be leveraged to model and analyze secure SoS solutions for predicting high impact (cascading) attacks at the architecture stage. In order to achieve this objective, we propose a Model Driven Engineering method, Systems-of-Systems Security (SoSSec), that comprises: (1) a modeling language (SoSSecML) for secure SoS modeling and (2) Multi-Agent Systems (MAS) for security analysis of SoS architectures. To illustrate our proposed approach in terms of modeling, simulating, and discovering attacks, we have conducted a case study on a real-life smart building SoS, the Adelaide University Health and Medical School (AHMS). The results from this case study demonstrate that our proposed method discovers cascading attacks comprising of a number of individual attacks, such as a Denial of Service, that arise from a succession of exploited vulnerabilities through interactions among the constituent systems of SoS. In future work, we intend to extend SoSSec to address diverse unknown emergent behaviors and non-functional properties such as safety and trust. © 2019 Elsevier Inc.","Model driven engineering; Multi-agent systems simulation; Security modeling and analysis; Smart buildings; Software architecture; Systems-of-systems"
"Enhancing C/C++ based OSS development and discoverability with CBRJS: A Rust/Node.js/WebAssembly framework for repackaging legacy codebases","2019","Journal of Systems and Software","10.1016/j.jss.2019.110395","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070883143&doi=10.1016%2fj.jss.2019.110395&partnerID=40&md5=08f12957b98a5ec8bb188c18405e8041","Since the appearance of the C programming language and later C++, a plethora of libraries have been developed in both languages. Unfortunately, discovering such Open Source Software (OSS) components efficiently is not always an easy task. Nonetheless, recent advancements in OSS technologies present an opportunity to improve the status quo. In this paper, we introduce a prototype framework, which utilizes the Rust and JavaScript programming languages, as well as their respective ecosystems, alongside the WebAssembly state-of-the-art Web standard, for achieving boosted exposure for hard-to-find C/C++ OSS components, by taking advantage of their package discovery and delivery channels. By demonstrating how this system works, we show that this methodology is capable of increasing the exposure of such libraries, and providing a modernized stage for further development and maintenance. Provided metrics exhibit a more than twofold increase in downloads for a re-packaged library, superior discoverability compared to standard public OSS code repositories, as well as evidence that Web browser vendors invest heavily in optimizing the underlying runtime. © 2019","C/C++; JavaScript; Open source software; Rust; Software discoverability; WebAssembly"
"Exploring the gap between the student expectations and the reality of teamwork in undergraduate software engineering group projects","2019","Journal of Systems and Software","10.1016/j.jss.2019.110393","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070667200&doi=10.1016%2fj.jss.2019.110393&partnerID=40&md5=f9134c968f5710004cc304c7be2cebac","Software engineering group projects aim to provide a nurturing environment for learning about teamwork in software engineering. Since social and teamwork issues have been consistently identified as serious problems in such projects, we aim to better understand the breakdown between the expectations teams have at the start of a group project and their experiences at the end of the project. In this paper, we investigate how 35 teams of undergraduate students approach software engineering group project courses, and how their previous experience with collaborative software development matches their expectations for group work. We then analyse the retrospective documents delivered by the same teams at the end of a 27-week software engineering group project course, mirroring the expectations at the start of the project with the realities described by the end of it. © 2019","Engineering education; Group project; Project; Software engineering"
"Evolution of statistical analysis in empirical software engineering research: Current state and steps forward","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068745690&doi=10.1016%2fj.jss.2019.07.002&partnerID=40&md5=92d1671e33881237c2b9c197245bd92a","Software engineering research is evolving and papers are increasingly based on empirical data from a multitude of sources, using statistical tests to determine if and to what degree empirical evidence supports their hypotheses. To investigate the practices and trends of statistical analysis in empirical software engineering (ESE), this paper presents a review of a large pool of papers from top-ranked software engineering journals. First, we manually reviewed 161 papers and in the second phase of our method, we conducted a more extensive semi-automatic classification of papers spanning the years 2001–2015 and 5196 papers. Results from both review steps was used to: i) identify and analyse the predominant practices in ESE (e.g., using t-test or ANOVA), as well as relevant trends in usage of specific statistical methods (e.g., nonparametric tests and effect size measures) and, ii) develop a conceptual model for a statistical analysis workflow with suggestions on how to apply different statistical methods as well as guidelines to avoid pitfalls. Lastly, we confirm existing claims that current ESE practices lack a standard to report practical significance of results. We illustrate how practical significance can be discussed in terms of both the statistical analysis and in the practitioner's context. © 2019 Elsevier Inc.","Empirical software engineering; Practical significance; Semi-automated literature review; Statistical methods"
"A systematic literature review of techniques and metrics to reduce the cost of mutation testing","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070519994&doi=10.1016%2fj.jss.2019.07.100&partnerID=40&md5=16c5597d0bcf90620a695227f5273989","Historically, researchers have proposed and applied many techniques to reduce the cost of mutation testing. It has become difficult to find all techniques and to understand the cost-benefit tradeoffs among them, which is critical to transitioning this technology to practice. This paper extends a prior workshop paper to summarize and analyze the current knowledge about reducing the cost of mutation testing through a systematic literature review. We selected 175 peer-reviewed studies, from which 153 present either original or updated contributions. Our analysis resulted in six main goals for cost reduction and 21 techniques. In the last decade, a growing number of studies explored techniques such as selective mutation, evolutionary algorithms, control-flow analysis, and higher-order mutation. Furthermore, we characterized 18 metrics, with particular interest in the number of mutants to be executed, test cases required, equivalent mutants generated and detected, and mutant execution speedup. We found that cost reduction for mutation is increasingly becoming interdisciplinary, often combining multiple techniques. Additionally, measurements vary even for studies that use the same techniques. Researchers can use our results to find more detailed information about particular techniques, and to design comparable and reproducible experiments. © 2019 Elsevier Inc.","Cost reduction; Mutation analysis; Mutation testing; Systematic review"
"A survey on clone refactoring and tracking","2020","Journal of Systems and Software","10.1016/j.jss.2019.110429","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073972279&doi=10.1016%2fj.jss.2019.110429&partnerID=40&md5=44bd7677992170b1285f96deac5f559d","Code clones, identical or nearly similar code fragments in a software system's code-base, have mixed impacts on software evolution and maintenance. Focusing on the issues of clones researchers suggest managing them through refactoring, and tracking. In this paper we present a survey on the state-of-the-art of clone refactoring and tracking techniques, and identify future research possibilities in these areas. We define the quality assessment features for the clone refactoring and tracking tools, and make a comparison among these tools considering these features. To the best of our knowledge, our survey is the first comprehensive study on clone refactoring and tracking. According to our survey on clone refactoring we realize that automatic refactoring cannot eradicate the necessity of manual effort regarding finding refactoring opportunities, and post refactoring testing of system behaviour. Post refactoring testing can require a significant amount of time and effort from the quality assurance engineers. There is a marked lack of research on the effect of clone refactoring on system performance. Future investigations in this direction will add much value to clone refactoring research. We also feel the necessity of future research towards real-time detection, and tracking of code clones in a big-data environment. © 2019","Clone refactoring; Clone tracking; Clone-types; Code clones"
"Augmenting ant colony optimization with adaptive random testing to cover prime paths","2020","Journal of Systems and Software","10.1016/j.jss.2019.110495","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076836482&doi=10.1016%2fj.jss.2019.110495&partnerID=40&md5=17f2295b34dedee10d310ddad27f574d","Test data generation has a notable impact on the performance of software testing. A well-known approach to automate this activity is search-based test data generation. Most studies in this area use branch coverage as the test criterion. Since the prime path coverage criterion includes branch coverage, it has higher probability to detect software failures than the branch coverage criterion. This paper customizes and improves ant colony optimization (ACO) to provide a test data generation approach for covering prime paths. The proposed approach incorporates the notion of input space partitioning to maintain pheromone values in the search space. In addition, it employs the idea of adaptive random testing in the local search. At last, it uses the information of program predicates in order to make a relation between the logic of the program and pheromone values in the search space. The experimental results confirm the positive effects of the mentioned contributions, especially for programs with complex predicates. Furthermore, they represent that, on average, test suites generated by the proposed approach has 9% better mutation score in comparison to test suites produced by EvoSuite, a well-known test data generation tool. © 2019 Elsevier Inc.","Adaptive random testing; Ant colony optimization; Mutation analysis; Prime path coverage; Search-based test data generation"
"Exploring onboarding success, organizational fit, and turnover intention of software professionals","2020","Journal of Systems and Software","10.1016/j.jss.2019.110442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073542032&doi=10.1016%2fj.jss.2019.110442&partnerID=40&md5=b0cd58a2c78ebaa6f39a0f11bf285207","The IT sector struggles with talent acquisition and low retention rates. While several field studies have explored onboarding of software developers, the software engineering literature lacks studies that develop and evaluate theoretical models. This study seeks to explore the link between onboarding of new hires and turnover intention of these professionals. In particular, we develop a theoretical model that identifies a number of onboarding activities, and link these to onboarding success. We then look at what we have termed “organizational fit,” which we define as two aspects of software professionals, namely job satisfaction and the quality of their relationships on the workfloor, and investigate how these mediate the relation between short-term onboarding success and a longer-term intention to leave (or stay with) an organization. We test our model with a sample of 102 software professionals using PLS-SEM. The findings suggest that providing support to new hires plays a major role in onboarding success, but that training is less important. Further, we found that job satisfaction mediates the relationship between onboarding success and turnover intention, but workplace relationship quality does not. Based on the findings, we discuss a number of implications for practice and suggestions for future research. © 2019 Elsevier Inc.","Job satisfaction; Onboarding; PLS; Survey; Turnover intention"
"On the testing resource allocation problem: Research trends and perspectives","2020","Journal of Systems and Software","10.1016/j.jss.2019.110462","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075504562&doi=10.1016%2fj.jss.2019.110462&partnerID=40&md5=7863049fd4730fbe83797ab303a2334a","In testing a software application, a primary concern is how to effectively plan the assignment of resources available for testing to the software components so as to achieve a target goal under given constraints. In the literature, this is known as testing resources allocation problem (TRAP). Researchers spent a lot of effort to propose models for supporting test engineers in this task, and a variety of solutions exist to assess the best trade-off between testing time, cost and quality of delivered products. This article presents a systematic mapping study aimed at systematically exploring the TRAP research area in order to provide an overview on the type of research performed and on results currently available. A sample of 68 selected studies has been classified and analyzed according to defined dimensions. Results give an overview of the state of the art, provide guidance to improve practicability and allow outlining a set of directions for future research and applications of TRAP solutions. © 2019 Elsevier Inc.","Literature review; Reliability allocation; Resource allocation; Survey; Test planning; Testing"
"Detection of intermittent faults in software programs through identification of suspicious shared variable access patterns","2020","Journal of Systems and Software","10.1016/j.jss.2019.110455","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074455468&doi=10.1016%2fj.jss.2019.110455&partnerID=40&md5=b26545c51f9778e1d012091c6fc2f517","Intermittent faults are a very common problem in the software world, while difficult to be debugged. Most of the existing approaches though assume that suitable instrumentation has been provided in the program, typically in the form of assertions that dictate which program states are considered to be erroneous. In this paper we propose a method that can be used to detect probable sources of intermittent faults within a program. Our method proposes certain points in the code, whose data interdependencies combined with their execution interweaving indicate that they could be the cause of intermittent faults. It is the responsibility of the user to accept or reject these proposals. An advantage of this method is that it removes the need for having predefined assertion points in the code, being able to detect potential sources of intermittent faults in the whole bulk of the code, with no instrumentation requirements on the side of the programmer. The proposed approach exploits information from the dynamic behavior of the program. In comparison with parser-based approaches which analyze only the program structure, our approach is immutable to language term changes and in general is not depending on any user-provided assertions or configuration. © 2019 Elsevier Inc.","Fault detection; Intermittent faults; Model-based checking; Shared variables"
"A framework for pervasive computing applications based on smart objects and end user development","2020","Journal of Systems and Software","10.1016/j.jss.2019.110496","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076831348&doi=10.1016%2fj.jss.2019.110496&partnerID=40&md5=ed09e6637954ee1ab0610ed7de76c6d5","Pervasive Computing (PerComp) research remains to this date technology-centric, requiring more focus on utilizing human and societal intelligence. To bridge this gap we motivate the need for a conceptual framework that provides a vision for enabling end-users to participate actively in the design of PerComp applications. The framework in particular provides guidance for the development of tools that allow end-users to configure their own smart environments. It emphasizes the benefits of using Smart Objects (SOs) as components of PerComp applications under a system engineering perspective and the benefits of using affordances as dynamic connectable capabilities under a user experience perspective. A generic application model is developed within the framework that manifests the concepts governing the structure and operation of applications composed by connecting SOs together and adhering to a rule-based behavior. A multi-layered software mediator is defined to provide a platform for the runtime support of such PerComp applications. Theoretical foundations for the key concepts adopted regarding the interaction design and system engineering perspectives are discussed and the results of a user evaluation study focusing on the perceived usability of the high-level conceptual models and the developed tools are reported. © 2019","Component-based system engineering; End User Development; Framework; Pervasive computing; Smart objects; User evaluation"
"Summarizing vulnerabilities’ descriptions to support experts during vulnerability assessment activities","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067484623&doi=10.1016%2fj.jss.2019.06.001&partnerID=40&md5=e0579693c8b71c5713018f40eb9f7116","Vulnerabilities affecting software and systems have to be promptly fixed, to prevent violations to integrity, availability and confidentiality policies of targeted organizations. Once a vulnerability is discovered, it is published on the Common Vulnerabilities and Exposures (CVE) database, freely available on the web. However, vulnerabilities are described using natural language, which makes them hard to be automatically interpreted by machines. As a consequence, vulnerability assessment activities tend to be time-consuming and imprecise, as the assessors must manually read the majority of the vulnerabilities concerning the perimeter to be protected, to make a decision on which vulnerabilities have the highest priority for patching. In this paper we present CVErizer, an approach able to automatically generate summaries of daily posted vulnerabilities and categorize them according to a taxonomy modeled for industry. We empirically assess the classification capabilities of the approach on a set of 3369 pre-labeled CVE records and perform an end-to-end evaluation of CVErizer summaries involving 15 cybersecurity master students and 4 professional security experts. Our study demonstrates the high performance of the proposed approach in correctly extracting and classifying information from CVE descriptions. Summaries are also considered highly useful for helping analysts during the vulnerability assessment processes. © 2019 Elsevier Inc.","Natural language processing; Software maintenance; Software security; Summarization"
"Experimental assessment of XOR-Masking data obfuscation based on K-Clique opaque constants","2020","Journal of Systems and Software","10.1016/j.jss.2019.110492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076567594&doi=10.1016%2fj.jss.2019.110492&partnerID=40&md5=6381d779626f2d53819fead6feb1003a","Data obfuscations are program transformations used to complicate program understanding and conceal actual values of program variables. The possibility to hide constant values is a basic building block of several obfuscation techniques. In XOR-Masking, a constant mask is used to obfuscate data, but this mask must be hidden too, in order to keep the obfuscation resilient to attacks. In this paper, we present a novel extension of XOR-Masking where the mask is an opaque constant, i.e. a value that is difficult to guess by static analysis. In fact, opaque constants are constructed such that static analysis should solve the k-clique problem, which is known to be NP-complete, to identify the mask value. In our experimental assessment we apply obfuscation to 12 real Java applications. We observe that obfuscation does not alter the program correctness and we record performance overhead due to obfuscation, in terms of execution time and memory consumption. © 2019","Data obfuscation; Obfuscation overhead; Program transformation"
"An empirical study of configuration changes and adoption in Android apps","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068077797&doi=10.1016%2fj.jss.2019.06.095&partnerID=40&md5=c528cf7ad6caecc6e334c7a732020aed","Android platform is evolving rapidly. Therefore, evolution and maintenance of Android apps are major concerns among developers. One of the essential components of each app is an Android manifest file, which is a configuration file used to declare various key attributes of apps. This paper presents an empirical study to understand app evolution through configuration changes. The results of this study will help developers in identifying change-proneness attributes, including change patterns and the reason behind the change, understanding the adoption of different attributes introduced in different versions of the Android platform, and understanding effort distribution pattern in configuration changes and taking proactive measures to reduce the effort. In this paper, we use a data mining approach. We analyze commit histories of Android manifest files of 908 apps to understand the app evolution. The results of this study show that most of the apps extend core functionalities and improve user interface over time, configuration changes are mostly influenced by functionalities extension, platform evolution, and bug reports, very few numbers of existing apps adopt new attributes introduced by the platform, apps are generally slow in adopting new attributes, and significant effort is wasted in changing configuration and then reverting back the change. © 2019","Android app evolution; App maintenance; Configuration changes; Effort estimation; Permission evolution"
"The long and winding road: MBSE adoption for functional avionics of spacecraft","2020","Journal of Systems and Software","10.1016/j.jss.2019.110453","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075454887&doi=10.1016%2fj.jss.2019.110453&partnerID=40&md5=1c790c134cc79ec676832772f80e008e","Model-Based Systems Engineering (MBSE) represents a move away from the traditional approach of Document-Based Systems Engineering (DBSE). It is claimed that MBSE promotes consistency, communication, clarity and maintainability within systems engineering projects and addresses issues associated with cost, complexity and safety. While these potential benefits of MBSE are generally agreed upon by would-be practitioners, its implementation is challenging and many organisations struggle to overcome the cultural and technical hurdles along the long and winding road to MBSE adoption. In this paper, we aim to ease the process of implementation by investigating where the current issues with the existing systems engineering processes lie, and where a model-based approach may be able to help, from the perspective of engineers working on spacecraft functional avionics in Airbus. A repeatable process has been developed to elicit this information. Semi-structured interviews have been conducted with 25 Airbus engineers working in Operations, Software and Failure, Detection, Isolation and Recovery. The acquired data has been thematically analysed to extract common themes from the responses. The results presented in this paper have yielded four recommended application areas to consider when applying MBSE to Functional Avionics: organisation modelling; early functional validation; communication and consistency; template model framework development. © 2019","Functional avionics; MBSE; Modelling; Systems engineering; Thematic analysis"
"Do concern mining tools really help requirements analysts? An empirical study of the vetting process","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068222830&doi=10.1016%2fj.jss.2019.06.073&partnerID=40&md5=6aa222cef9a1118bc9c68673298bcf3c","Software requirements are often described in natural language because they are useful to communicate and validate. Due to their focus on particular facets of a system, this kind of specifications tends to keep relevant concerns (also known as early aspects) from the analysts’ view. These concerns are known as crosscutting concerns because they appear scattered among documents. Concern mining tools can help analysts to uncover concerns latent in the text and bring them to their attention. Nonetheless, analysts are responsible for vetting tool-generated solutions, because the detection of concerns is currently far from perfect. In this article, we empirically investigate the role of analysts in the concern vetting process, which has been little studied in the literature. In particular, we report on the behavior and performance of 55 subjects in three case-studies working with solutions produced by two different tools, assessed in terms of binary classification measures. We discovered that analysts can improve “bad” solutions to a great extent, but performed significantly better with “good” solutions. We also noticed that the vetting time is not a decisive factor to their final accuracy. Finally, we observed that subjects working with solutions substantially different from those of existing tools (better recall) can also achieve a good performance. © 2019 Elsevier Inc.","Crosscutting concern; Empirical study; Human behavior; Requirements engineering; Tool support; Use case specifications"
"A snowballing literature study on test amplification","2019","Journal of Systems and Software","10.1016/j.jss.2019.110398","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070967135&doi=10.1016%2fj.jss.2019.110398&partnerID=40&md5=c0df463a716f8c4817e9ebb40511d5ba","The adoption of agile approaches has put an increased emphasis on testing, resulting in extensive test suites. These suites include a large number of tests, in which developers embed knowledge about meaningful input data and expected properties as oracles. This article surveys works that exploit this knowledge to enhance manually written tests with respect to an engineering goal (e.g., improve coverage or refine fault localization). While these works rely on various techniques and address various goals, we believe they form an emerging and coherent field of research, which we coin “test amplification”. We devised a first set of papers from DBLP, searching for all papers containing “test” and “amplification” in their title. We reviewed the 70 papers in this set and selected the 4 papers that fit the definition of test amplification. We use them as the seeds for our snowballing study, and systematically followed the citation graph. This study is the first that draws a comprehensive picture of the different engineering goals proposed in the literature for test amplification. We believe that this survey will help researchers and practitioners entering this new field to understand more quickly and more deeply the intuitions, concepts and techniques used for test amplification. © 2019 Elsevier Inc.","Automatic testing; Test amplification; Test augmentation; Test optimization; Test regeneration"
"On the challenges novice programmers experience in developing IoT systems: A Survey","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070199153&doi=10.1016%2fj.jss.2019.07.101&partnerID=40&md5=9dcc522c4694f79fde4494294752fbf2","The co-existence of various kinds of devices, protocols, architectures, and applications make Internet of Things (IoT) systems complex to develop, even for experienced programmers. When novice programmers are learning to implement these systems, they are required to deal with areas in which they do not have a deep knowledge. Furthermore, besides becoming proficient in these areas separately, they should integrate them and build a system whose components are heterogeneous from both software and hardware perspectives. The accurate understanding of the most challenging issues that novices face is fundamental to envision strategies aimed at easing the development of IoT systems. This paper focuses on identifying such issues in terms of software development tasks that novice programmers encounter when working on IoT systems. To this end, a survey was conducted among 40 novice developers that worked in groups developing IoT systems during several years of a university course. Based on their own experiences, individually and as a group, the most challenging development tasks were identified and prioritized over a common architecture, in terms of difficulty level and efforts. In addition, qualitative data about the causes of these issues was collected and analyzed. Finally, the paper offers critical insights and points out possible future work. © 2019 Elsevier Inc.","Internet of Things; Novice programmers; Software engineering; Survey"
"DEEPLINK: Recovering issue-commit links based on deep learning","2019","Journal of Systems and Software","10.1016/j.jss.2019.110406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071724162&doi=10.1016%2fj.jss.2019.110406&partnerID=40&md5=bfeed1062b2e4c3731bc17ea07aa9a2d","The links between issues in an issue-tracking system and commits resolving the issues in a version control system are important for a variety of software engineering tasks (e.g., bug prediction, bug localization and feature location). However, only a small portion of such links are established by manually including issue identifiers in commit logs, leaving a large portion of them lost in the evolution history. To recover issue-commit links, heuristic-based and learning-based techniques leverage the metadata and text/code similarity in issues and commits; however, they fail to capture the embedded semantics in issues and commits and the hidden semantic correlations between issues and commits. As a result, this semantic gap inhibits the accuracy of link recovery. To bridge this gap, we propose a semantically-enhanced link recovery approach, named DEEPLINK, which is built on top of deep learning techniques. Specifically, we develop a neural network architecture, using word embedding and recurrent neural network, to learn the semantic representation of natural language descriptions and code in issues and commits as well as the semantic correlation between issues and commits. In experiments, to quantify the prevalence of missing issue-commit links, we analyzed 1078 highly-starred GitHub Java projects (i.e., 583,795 closed issues) and found that only 42.2% of issues were linked to corresponding commits. To evaluate the effectiveness of DEEPLINK, we compared DEEPLINK with a state-of-the-art link recovery approach FRLink using ten GitHub Java projects and demonstrated that DEEPLINK can outperform FRLink in terms of F-measure. © 2019 Elsevier Inc.","Deep learning; Issue-commit links; Semantic understanding"
"Performance evaluation of web service response time probability distribution models for business process cycle time simulation","2020","Journal of Systems and Software","10.1016/j.jss.2019.110480","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076588834&doi=10.1016%2fj.jss.2019.110480&partnerID=40&md5=801e6b84374e470269485e457e8090bb","Context: The adoption of Business Process Management (BPM) is enabling companies to improve the pace of building new capabilities, enhancing existing ones, and measuring process performance to identify bottlenecks. It is essential to compute the cycle time of the process to assess the performance of a business process. The cycle time typically forms part of service level agreements (SLAs) and is a crucial contributor to the overall user experience and productivity. The simulation technique is versatile and has broad applicability for determining realistic cycle time using historical data of web service response time. BPM tools offer inadequate support for modeling input data used in simulation in the form of descriptive statistics or standard probability distributions like normal, lognormal, which results in inaccurate simulation results. Objective: We evaluate the effectiveness of different parametric and non-parametric probability distributions for modeling data of web service response time. We further assess how the choice of probability distribution impacts the accuracy of the simulated cycle time of a business process. The work is the first of such a study using real-world data for encouraging Business Process Simulation Specification (BPSim) standard setters and BPM tools to enhance their support for such distributions in their simulation engine. Method: We consider several parametric and non-parametric distributions and explore how well these distributions fit web service response time from extensive public and a real-world dataset. The cycle time of the business process of a real-world system is simulated using the identified distributions to model the underlying web service data. Results: Our results show that kernel distribution is the most suitable choice, followed by Burr. Kernel outperforms Burr by 86.63% for the public and 84.21% for the real-world dataset. The choice of distribution affects the percentile ranks like 90 and above than the median. The use of single-point values underestimates cycle time values at higher percentiles. Conclusion: Based on our empirical results, we recommend the addition of kernel and Burr to the current list of distributions supported by BPSim and BPM tools. © 2019","Cycle time; Non-parametric distributions; Parametric distributions; Performance evaluation; Simulation input modeling; Web service response time"
"A systematic literature review on semantic web enabled software testing","2020","Journal of Systems and Software","10.1016/j.jss.2019.110485","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076846933&doi=10.1016%2fj.jss.2019.110485&partnerID=40&md5=78783098c5be3378958095aa6cdf5cff","Software testing, as a major verification and validation activity which revolves around quality tests, is a knowledge-intensive activity. Hence, it is reasonable to expect that it can be improved by effective application of semantic web technologies, e.g., ontologies, which have been frequently used in knowledge engineering activities. The objective of this work is to investigate and provide a better understanding of how semantic web enabled techniques, i.e., the techniques that are based on the effective application of the semantic web technologies, have been used to support software testing activities. For this purpose, a Systematic Literature Review based on a predefined procedure is conducted. A total of 52 primary studies were identified as relevant, which have undergone a thorough meta-analysis with regards to our posed research questions. This study indicates the benefits of semantic web enabled software testing in both industry and academia. It also identifies main software testing activities that can benefit from the semantic web enabled techniques. Furthermore, contributions of such techniques to the testing process are thoroughly examined. Finally, potentials and difficulties of applying these techniques to software testing, along with the promising research directions are discussed. © 2019 Elsevier Inc.","Ontology; Semantic web; Software testing; Systematic literature review; Test generation"
"Automatic method change suggestion to complement multi-entity edits","2020","Journal of Systems and Software","10.1016/j.jss.2019.110441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073505497&doi=10.1016%2fj.jss.2019.110441&partnerID=40&md5=64787f3217f858a1c953f0184629af65","When maintaining software, developers sometimes change multiple program entities (i.e., classes, methods, and fields) to fulfill one maintenance task. We call such complex changes multi-entity edits. Consistently and completely applying multi-entity edits can be challenging, because (1) the changes scatter in different entities and (2) the incorrectly edited code may not trigger any compilation or runtime error. This paper introduces CMSuggester, an approach to suggest complementary changes for multi-entity edits. Given a multi-entity edit that (i) adds a new field or method and (ii) modifies one or more methods to access the field or invoke the method, CMSuggester suggests other methods to co-change for the new field access or method invocation. The design of CMSuggester is motivated by our preliminary study, which reveals that co-changed methods usually access existing fields or invoke existing methods in common. Our evaluation shows that based on common field accesses, CMSuggester recommended method changes in 463 of 685 tasks with 70% suggestion accuracy; based on common method invocations, CMSuggester handled 557 of 692 tasks with 70% accuracy. Compared with prior work ROSE, TARMAQ, and Transitive Association Rules (TAR), CMSuggester recommended more method changes with higher accuracy. Our research can help developers correctly apply multi-entity edits. © 2019","Change suggestion; Common field access; Common method invocation; Multi-entity edit"
"Efficient anytime algorithms to solve the bi-objective Next Release Problem","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068417412&doi=10.1016%2fj.jss.2019.06.097&partnerID=40&md5=baeace423ec0fd8ebe4cd5d40dd5a351","The Next Release Problem consists in selecting a subset of requirements to develop in the next release of a software product. The selection should be done in a way that maximizes the satisfaction of the stakeholders while the development cost is minimized and the constraints of the requirements are fulfilled. Recent works have solved the problem using exact methods based on Integer Linear Programming. In practice, there is no need to compute all the efficient solutions of the problem; a well-spread set in the objective space is more convenient for the decision maker. The exact methods used in the past to find the complete Pareto front explore the objective space in a lexicographic order or use a weighted sum of the objectives to solve a single-objective problem, finding only supported solutions. In this work, we propose five new methods that maintain a well-spread set of solutions at any time during the search, so that the decision maker can stop the algorithm when a large enough set of solutions is found. The methods are called anytime due to this feature. They find both supported and non-supported solutions, and can complete the whole Pareto front if the time provided is long enough. © 2019 Elsevier Inc.","Anytime algorithm; Multi-objective optimization; Next release problem; Pareto front; Search-based software engineering"
"Toward collisions produced in requirements rankings: A qualitative approach and experimental study","2019","Journal of Systems and Software","10.1016/j.jss.2019.110417","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072193237&doi=10.1016%2fj.jss.2019.110417&partnerID=40&md5=0623452cd48c5a7b3e021f070e5d7dca","Requirements prioritization is an important issue that determines the way requirements are selected and processed in software projects. There already exist specific methods to classify and prioritize requirements, most of them based on quantitative measures. However, most of existing approaches do not consider collisions, which are an important concern in large-scale requirements sets and, more specifically, in agile development processes where requirements have to be uniquely selected for each software increment. In this paper, we propose QMPSR (Qualitative Method for Prioritizing Software Requirements), an approach that features the prioritization of requirements by considering qualitative elements that are related to the project's priorities. Our approach highlights a prioritization method that has proven to reduce collisions in software requirements rankings. Furthermore, QMPSR improves accuracy in classification when facing large-scale requirements sets, featuring no scalability problems as the number of requirements increases. We formally introduce QMPSR and then define prioritization effort and collision metrics to carry out comprehensive experiments involving different sets of requirements, comparing our approach with well-known existing prioritization methods. The experiments have provided satisfactory results, overcoming existing approaches and ensuring scalability. © 2019","Qualitative prioritization method; Requirement collision; Requirement prioritization"
"Google summer of code: Student motivations and contributions","2020","Journal of Systems and Software","10.1016/j.jss.2019.110487","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076507705&doi=10.1016%2fj.jss.2019.110487&partnerID=40&md5=443cf2f0faec4519100fe0ef6346365b","Several open source software (OSS) projects participate in engagement programs like Summers of Code expecting to foster newcomers’ onboarding and receive contributions. However, scant empirical evidence identifies why students join such programs. In this paper, we study the well-established Google Summer of Code (GSoC), which is a 3-month OSS engagement program that offers stipends and mentorship to students willing to contribute to OSS projects. We combined a survey (of students and mentors) and interviews (of students) to understand what motivates students to enter GSoC. Our results show that students enter GSoC for an enriching experience, and not necessarily to become frequent contributors. Our data suggest that, while stipends are an important motivator, students participate for work experience and the ability to enhance their resumés. We also discuss practical implications for students, mentors, OSS projects, and Summer of Code programs. © 2019","Google summer of code; Motivation; Newcomers; Open source software"
"Combining data analytics and developers feedback for identifying reasons of inaccurate estimations in agile software development","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067949072&doi=10.1016%2fj.jss.2019.06.075&partnerID=40&md5=2a76ab2061c78a8115ece5d1248314f4","Background: Effort estimations are critical tasks greatly influencing the accomplishment of software projects. Despite their recognized relevance, little is yet known what indicators for inaccurate estimations exist, and which are the reasons of inaccurate estimations. Aims: In this manuscript, we aim at contributing to this existing gap. To this end, we implemented a tool that combines data analytics and developers’ feedback, and we employed that tool in a study. In that study, we explored the most common reasons of inaccurate user story estimations and the possible indicators of inaccurate estimations. Method: We relied on a mixed method approach used to study reasons and indicators for the identification and prediction of inaccurate estimations in practical agile software development contexts. Results: Our results add to the existing body of knowledge in multiple ways. We elaborate causes for inaccurate estimations going beyond the borders of existing literature; for instance, we show that lack of developers’ experience is the most common reason of inaccurate estimations. Further, our results suggest, for example, that the higher the complexity, the higher the uncertainty in the estimation. Conclusions: Overall, our results strengthen our confidence in the usefulness of using data analytics with human-in-the-loop mechanisms to improve effort estimations. © 2019 Elsevier Inc.","Agile methods; Data analytics; Empirical software engineering; Estimations; Mixed methods"
"Software developer productivity loss due to technical debt—A replication and extension study examining developers’ development work","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067077032&doi=10.1016%2fj.jss.2019.06.004&partnerID=40&md5=88236d00c048ff16347b58588a39b258","Software companies need to deliver customer value continuously, both from a short- and long-term perspective. However, software development can be impeded by technical debt (TD). Although significant theoretical work has been undertaken to describe the negative effects of TD, little empirical evidence exists on how much wasted time and additional activities TD causes. The study aims to explore the consequences of TD in terms of wastage of development time. This study investigates on which activities this wasted time is spent and whether different TD types impact the wasted time differently. This study reports the results of a longitudinal study surveying 43 developers and including16 interviews followed by validation by an additional study using a different and independent dataset and focused on replicating the findings addressing the findings. The analysis of the reported wasted time revealed that developers waste, on average, 23% of their time due to TD and that developers are frequently forced to introduce new TD. The most common activity on which additional time is spent is performing additional testing. The study provides evidence that TD hinders developers by causing an excessive waste of working time, where the wasted time negatively affects productivity. © 2019","Software development; Software productivity; Technical debt; Wasted development time"
"Online cost optimization algorithms for tiered cloud storage services","2020","Journal of Systems and Software","10.1016/j.jss.2019.110457","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074935750&doi=10.1016%2fj.jss.2019.110457&partnerID=40&md5=651b75f4e35360c60f48f00398bba521","The new generation multi-tiered cloud storage services offer various tiers, such as hot and cool tiers, which are characterized by differentiated Quality of Service (QoS) (i.e., access latency, availability and throughput) and the corresponding storage and access costs. However, selecting among these storage tiers to efficiently manage data and improve performance at reduced cost is still a core and difficult problem. In this paper, we address this problem by developing and evaluating algorithms for automated data placement and movement between hot and cool storage tiers. We propose two practical online object placement algorithms that assume no knowledge of future data access. The first online cost optimization algorithm uses no replication (NR) and initially places the object in the hot tier. Then, based on read/write access pattern following a long tail distribution, it may decide to move the object to the cool tier to optimize the storage service cost. The second algorithm with replication (WR) initially places the object in the cool tier, and then replicates it in the hot tier upon receiving read/write requests to it. Additionally, we analytically demonstrate that the online algorithms incur less than twice the cost in comparison to the optimal offline algorithm that assumes the knowledge of exact future workload on the objects. The experimental results using a Twitter Workload and the CloudSim simulator confirm that the proposed algorithms yield significant cost savings (5%–55%) compared to the no-migration policy which permanently stores data in the hot tier. © 2019","Access cost; Competitive ratio; Cost optimization; Online algorithms; Storage cost; Tiered cloud storage"
"Run-time evaluation of architectures: A case study of diversification in IoT","2020","Journal of Systems and Software","10.1016/j.jss.2019.110428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073076930&doi=10.1016%2fj.jss.2019.110428&partnerID=40&md5=41b170dc2ef251d4fcf5459237c1af79","Run-time properties of modern software system environments, such as Internet of Things (IoT), are a challenge for existing software architecture evaluation methods. Such systems are largely data-driven, characterized by their dynamism, unpredictability in operation, hyper-connectivity, and scale. Properties, such as performance, delayed delivery, and scalability, are acknowledged to pose great risk and are difficult to evaluate at design-time. Run-time evaluation could potentially be used to complement design-time evaluation, enabling significant deviations from the expected performance values to be captured. However, there are no systematic software architecture evaluation methods that intertwine and interleave design-time and run-time evaluation. This paper addresses this gap by proposing a novel run-time architecture evaluation method suited for systems that exhibit uncertainty and dynamism in their operation. Our method uses machine learning and cost-benefit analysis at run-time to continuously profile the architecture decisions made, to assess their added value. We demonstrate the applicability and effectiveness of this approach in the context of an IoT system architecture, where some architecture design decisions were diversified to meet Quality of Service (QoS) requirements. Our approach provides run-time assessment for these decisions which can inform deployment, refinement, and/or phasing-out decisions. © 2019","Design diversity; Internet of things; IoT; Run-time architecture evaluation; Runtime architecture evaluation; Software architectures for dynamic environments"
"Expression caching for runtime verification based on parameterized probabilistic models","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069194136&doi=10.1016%2fj.jss.2019.07.007&partnerID=40&md5=73d386ac2558e70c1cf423c38b345016","Self-adaptive software systems change their behaviors to adapt to their environmental changes at runtime. Runtime verification, which checks the correctness of behaviors after adaptation, sometimes uses probabilistic model checking, because the verification has to deal with uncertainty. However, since probabilistic model checking is usually computation intensive and time consuming, a more efficient verification mechanism is desired. A possible approach is to pre-generate some expressions for model checking at design time and execute model checking simply by evaluating the expressions at runtime. A problem with this approach is that when environmental changes require changes of the system model, these expressions need to be re-generated at runtime. In order to cope with such significant changes, we develop a caching mechanism that reduces computational time at runtime. We also introduce a parameterization technique in order to improve the efficiency of caching. The experimental results show that our new implementation of the caching mechanism greatly improves the computational time of runtime verification. © 2019 The Author(s)","Caching; Discrete time Markov chain; Probabilistic model checking; Runtime verification; Self-adaptive systems"
"Towards complex product line variability modelling: Mining relationships from non-boolean descriptions","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067061361&doi=10.1016%2fj.jss.2019.06.002&partnerID=40&md5=902e0fc6fb00e71592623805678dcb59","Software product line engineering relies on systematic reuse and mass customisation to reduce the development time and cost of a software system family. The extractive adoption of a product line requires to extract variability information from the description of a collection of existing software systems to model their variability. With the increasing complexity of software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of existing boolean variability models, such as multi-valued attributes or UML-like cardinalities, were proposed to enhance their expressiveness and support variability modelling in complex product lines. In this paper, we propose an approach to extract complex variability information, i.e., involving features as well as multi-valued attributes and cardinalities, in the form of logical relationships. This approach is based on Formal Concept Analysis and Pattern Structures, two mathematical frameworks for knowledge discovery that bring theoretical foundations to complex variability extraction algorithms. We present an application on product comparison matrices representing complex descriptions of software system families. We show that our method does not suffer from scalability issues and extracts all pertinent relationships, but that it also extracts numerous accidental relationships that need to be filtered. © 2019 Elsevier Inc.","Complex software product line; Extended feature models; Formal concept analysis; Pattern structures; Reverse engineering; Variability modelling"
"Are unit and integration test definitions still valid for modern Java projects? An empirical study on open-source projects","2020","Journal of Systems and Software","10.1016/j.jss.2019.110421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072858282&doi=10.1016%2fj.jss.2019.110421&partnerID=40&md5=df29532744a79362813a000b30f12a0b","Context: Unit and integration testing are popular testing techniques. However, while the software development context evolved over time, the definitions remained unchanged. There is no empirical evidence, if these commonly used definitions still fit to modern software development. Objective: We analyze, if the existing standard definitions of unit and integration tests are still valid in modern software development contexts. Hence, we analyze if unit and integration tests detect different types of defects, as expected from the standard literature. Method: We classify 38,782 test cases into unit and integration tests according to the definition of the IEEE and use mutation testing to assess their defect detection capabilities. All integrated mutations are classified into five different defect types. Afterwards, we evaluate if there are any statistically significant differences in the results between unit and integration tests. Results: We could not find any evidence that one test type is more capable of detecting certain defect types than the other one. Our results suggest that the currently used definitions do not fit modern software development contexts. Conclusions: This finding implies that we need to reconsider the definitions of unit and integration tests and suggest that the current property-based definitions may be exchanged with usage-based definitions. © 2019","Empirical software engineering; Integration testing; Software testing; Unit testing"
"Automatic retrieval and analysis of high availability scenarios from system execution traces: A case study on hot standby router protocol","2020","Journal of Systems and Software","10.1016/j.jss.2019.110490","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076544196&doi=10.1016%2fj.jss.2019.110490&partnerID=40&md5=b83dba60cfb5b0d9559f6e95b0142cb6","High availability (HA) is becoming an increasingly important requirement in a growing number of domains. It is even mandatory for critical systems, such as networking and communications, that cannot afford downtime. Such systems often monitor the state of crucial services and produce huge amounts of execution trace data, where functional and non-functional log entries are intertwined; hence they are hard to dissociate and analyze. Dynamic analysis aims at capturing and analyzing run-time behavior of a system based on its execution traces. In this paper, we apply dynamic analysis to retrieve and analyze HA scenarios from system execution traces. Our proposed approach aims to help analysts understand and report on how a highly available system detects and recovers from failures. As a proof of concept, we have selected the Hot Standby Router Protocol (HSRP) in order to demonstrate the applicability of our approach. We have evaluated empirically the effectiveness of our technique using four real-world case studies of IP networks running HSRP. Results have shown that high availability scenarios were successfully retrieved and analyzed. Moreover, results have shown that our prototype tool HAAnalyzer was able to effectively unveil high availability behavioral and temporal errors, that were seeded in the execution traces. © 2019","Dynamic analysis; Error detection and diagnosis; High availability; Hot standby router protocol; Non-functional; Trace segmentation"
"Characterization of implied scenarios as families of common behavior","2019","Journal of Systems and Software","10.1016/j.jss.2019.110425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072605849&doi=10.1016%2fj.jss.2019.110425&partnerID=40&md5=3644483a3dc1375eb43c20c2830440d2","Concurrent systems face a threat to their reliability in emergent behaviors, which are not included in the specification but can happen during runtime. When concurrent systems are modeled in a scenario-based manner, it is possible to detect emergent behaviors as implied scenarios (ISs) which, analogously, are unexpected scenarios that can happen due to the concurrent nature of the system. Until now, the process of dealing with ISs can demand significant time and effort from the user, as they are detected and dealt with in a one by one basis. In this paper, a new methodology is proposed to deal with various ISs at a time, by finding Common Behaviors (CBs) among them. Additionally, we propose a novel way to group CBs into families utilizing a clustering technique using the Smith-Waterman algorithm as a similarity measure. Thus allowing the removal of multiple ISs with a single fix, decreasing the time and effort required to achieve higher system reliability. A total of 1798 ISs were collected across seven case studies, from which 14 families of CBs were defined. Consequently, only 14 constraints were needed to resolve all collected ISs, applying our approach. These results support the validity and effectiveness of our methodology. © 2019 Elsevier Inc.","Concurrent systems; Dependability; Hierarchical clustering; Implied scenarios; Smith-Waterman algorithm"
"An empirical study on bug propagation through code cloning","2019","Journal of Systems and Software","10.1016/j.jss.2019.110407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072265282&doi=10.1016%2fj.jss.2019.110407&partnerID=40&md5=1eb3e7acce7ac9ed3cdee68d4896edf6","Code clones are identical or nearly similar code fragments in a code-base. According to the existing studies, code clones are directly related to bugs. Code cloning, creating code clones, is suspected to propagate temporarily hidden bugs from one code fragment to another. However, there is no study on the intensity of bug-propagation through code cloning. In this paper, we define two clone evolutionary patterns that reasonably indicate bug propagation through code cloning. By analyzing software evolution history, we identify those code clones that evolved following the bug propagation patterns. According to our study on thousands of commits of seven subject systems, overall 18.42% of the clone fragments that experience bug-fixes contain propagated bugs. Type-3 clones are primarily involved with bug-propagation. Bug propagation is more likely to occur in the clone fragments that are created in the same commit rather than in different commits. Moreover, code clones residing in the same file have a higher possibility of containing propagated bugs compared to those residing in different files. Severe bugs can sometimes get propagated through code cloning. Automatic support for immediately identifying occurrences of bug-propagation can be beneficial for software maintenance. Our findings are important for prioritizing code clones for management. © 2019","Bug propagation; Clone-types; Code clones; Software maintenance"
"Finding help with programming errors: An exploratory study of novice software engineers’ focus in stack overflow posts","2020","Journal of Systems and Software","10.1016/j.jss.2019.110454","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074504797&doi=10.1016%2fj.jss.2019.110454&partnerID=40&md5=19894eab979b998ae2f4ee3adc7409c7","Monthly, 50 million users visit Stack Overflow, a popular Q&A forum used by software developers, to share and gather knowledge and help with coding problems. Although Q&A forums serve as a good resource for seeking help from developers beyond the local team, the abundance of information can cause developers, especially novice software engineers, to spend considerable time in identifying relevant answers and suitable suggested fixes. This exploratory study aims to understand how novice software engineers direct their efforts and what kinds of information they focus on within a post selected from the results returned in response to a search query on Stack Overflow. The results can be leveraged to improve the Q&A forum interface, guide tools for mining forums, and potentially improve granularity of traceability mappings involving forum posts. We qualitatively analyze the novice software engineers’ perceptions from a survey as well as their annotations of a set of Stack Overflow posts. Our results indicate that novice software engineers pay attention to only 27% of code and 15–21% of text in a Stack Overflow post to understand and determine how to apply the relevant information to their context. Our results also discern the kinds of information prominent in that focus. © 2019","Empirical study; Mining software repositories; Software developer behavior; Stack overflow"
"Affect recognition in code review: An in-situ biometric study of reviewer's affect","2020","Journal of Systems and Software","10.1016/j.jss.2019.110434","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073994062&doi=10.1016%2fj.jss.2019.110434&partnerID=40&md5=e72b1daead622e4a7162e10c677d11ff","Code review in software development is an important practice that increases team productivity and improves product quality. Code review is also an example of remote, computer-mediated asynchronous communication prone to the loss of affective information. Since positive affect has been linked to productivity in software development, prior research has focused on sentiment analysis in source codes. Although the methods of sentiment analysis have advanced, there are remaining challenges due to numerous domain oriented expressions, subtle nuances, and indications of sentiment. Here we explore the potentials of (1) nonverbal behavioral signals such as conventional typing, and (2) indirect physiology (eye gaze, GSR, touch pressure) that reflect genuine affective states in the in-situ code review in a large software company. Nonverbal behavioral signals of 33 professional software developers were unobtrusively recorded while they worked on their daily code review. Using Linear Mixed Effect Models, we observed that affect presented in the written comments was associated with prolonged typing duration. Using physiological features, a trained Random Forest classifier could predict post-task valence with 90.0% accuracy (F1-score = 0.937) and arousal with 83.9% accuracy (F1-score = 0.856). The results presents potentials for intelligent affect-aware interfaces for code review in-situ. © 2019 Elsevier Inc.","Affective computing; Code review; CSCW; Physiological signals"
"CVE-assisted large-scale security bug report dataset construction method","2020","Journal of Systems and Software","10.1016/j.jss.2019.110456","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074980613&doi=10.1016%2fj.jss.2019.110456&partnerID=40&md5=23e143e2efb99dffae256ae090910f7c","Identifying SBRs (security bug reports) is crucial for eliminating security issues during software development. Machine learning are promising ways for SBR prediction. However, the effectiveness of the state-of-the-art machine learning models depend on high-quality datasets, while gathering large-scale datasets are expensive and tedious. To solve this issue, we propose an automated data labeling approach based on iterative voting classification. It starts with a small group of ground-truth traing samples, which can be labeled with the help of authoritative vulnerability records hosted in CVE (Common Vulnerabilities and Exposures). The accuracy of the prediction model is improved with an iterative voting strategy. By using this approach, we label over 80k bug reports from OpenStack and 40k bug reports from Chromium. The correctness of these labels are then manually reviewed by three experienced security testing members. Finally, we construct a large-scale SBR dataset with 191 SBRs and 88,472 NSBRs (non-security bug reports) from OpenStack; and improve the quality of existing SBR dataset Chromium by identifying 64 new SBRs from previously labeled NSBRs and filtering out 173 noise bug reports from this dataset. These share datasets as well as the proposed dataset construction method help to promote research progress in SBR prediction research domain. © 2019 Elsevier Inc.","Common vulnerabilities and exposures; Dataset construction; Security bug report prediction; Voting classification"
"Providentia: Using search-based heuristics to optimize satisficement and competing concerns between functional and non-functional objectives in self-adaptive systems","2020","Journal of Systems and Software","10.1016/j.jss.2019.110497","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077509640&doi=10.1016%2fj.jss.2019.110497&partnerID=40&md5=e780500e2884caa8d4ac382ae4243cc9","In general, a system may be subject to a combination of functional requirements (FRs) that dictate behavior and non-functional requirements (NFRs) that characterize how FRs are to be satisfied. NFRs also introduce cross-cutting concerns that may be difficult to predict, where the degree of satisfaction (i.e., satisficement) of one NFR may be impacted by the satisficement of one or more FRs/NFRs. In particular, self-adaptive systems (SASs) can modify system configurations or behaviors at run time to continuously satisfy FRs and NFRs. This paper presents Providentia, a search-based technique to optimize the satisficement of NFRs in an SAS experiencing various sources of uncertainty. Providentia explores different combinations of weighted FRs to maximize NFR/FR satisficement. Experimental results suggest that Providentia-optimized goal models significantly improve the satisficement of an SAS when compared with manually- and randomly-generated weights and subgoals. Additionally, we apply a hyper-heuristic (Providentia-SAW) to balance the contribution of NFRs, FRs, and the number of adaptations and further improve the Providentia technique. We apply Providentia and Providentia-SAW to two case studies in different application domains involving a remote data mirroring network and a robotic vacuum controller, respectively. © 2019 Elsevier Inc.","Evolutionary computation; Non-functional requirements; Optimization; Search-based software engineering; Self-adaptive systems"
"Effects of contextual information on maintenance effort: A controlled experiment","2020","Journal of Systems and Software","10.1016/j.jss.2019.110443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073552709&doi=10.1016%2fj.jss.2019.110443&partnerID=40&md5=aead9e8642058675ccf007d2c98d5a65","There has been an increased focus on context-aware tools in software engineering. Within this area, an important challenge is to define and model the context for software-development projects and software development in general. This article reports a controlled experiment that compares the effort to implement changes, the correctness and the maintainability of an existing application between two projects; one that uses qualitative dashboards depicting contextual information, and one that does not. The results of this controlled experiment suggest that the usage of qualitative dashboards improves the correctness during the software maintenance activities and reduces the effort to implement these activities. © 2019 Elsevier Inc.","Contextual information; Empirical study; Maintenance effort; Qualitative dashboard"
"SCC++: Predicting the programming language of questions and snippets of Stack Overflow","2020","Journal of Systems and Software","10.1016/j.jss.2019.110505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077056512&doi=10.1016%2fj.jss.2019.110505&partnerID=40&md5=4e1f80985638dac3cbd34d77e7d98257","Stack Overflow is the most popular Q&A website among software developers. As a platform for knowledge sharing and acquisition, the questions posted on Stack Overflow usually contain a code snippet. Determining the programming language of a source code file has been considered in the research community; it has been shown that Machine Learning (ML) and Natural Language Processing (NLP) algorithms can be effective in identifying the programming language of source code files. However, determining the programming language of a code snippet or a few lines of source code is still a challenging task. Online forums such as Stack Overflow and code repositories such as GitHub contain a large number of code snippets. In this paper, we design and evaluate Source Code Classification (SCC++), a classifier that can identify the programming language of a question posted on Stack Overflow. The classifier achieves an accuracy of 88.9% in classifying programming languages by combining features from the title, body and the code snippets of the question. We also propose a classifier that only uses the title and body of the question and has an accuracy of 78.9%. Finally, we propose a classifier of code snippets only that achieves an accuracy of 78.1%. These results show that deploying Machine Learning techniques on the combination of text and code snippets of a question provides the best performance. In addition, the classifier can distinguish between code snippets from a family of programming languages such as C, C++ and C#, and can also identify the programming language version such as C# 3.0, C# 4.0 and C# 5.0. © 2019 Elsevier Inc.","Classification; Machine learning; Natural language processing; Programming languages"
"Industry requirements for FLOSS governance tools to facilitate the use of open source software in commercial products","2019","Journal of Systems and Software","10.1016/j.jss.2019.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072534077&doi=10.1016%2fj.jss.2019.08.001&partnerID=40&md5=fbbc9ac3473e48e133d34e38d8e28cad","Virtually all software products incorporate free/libre and open source software (FLOSS) components. However, ungoverned use of FLOSS components can result in legal and financial risks, and risks to a firm's intellectual property. To avoid these risks, companies must govern their FLOSS use through open source governance processes and by following industry best practices. A particular challenge is license compliance. To manage the complexity of governance and compliance, companies should use tools and well-defined processes. This paper investigates and presents industry requirements for FLOSS governance tools, followed by an evaluation of the suggested requirements. We chose eleven companies with an advanced understanding of open source governance and interviewed their FLOSS governance experts to derive a theory of industry requirements for tooling. We extended our previous work adding the requirement category on the architecture model for software products. We then analyzed the features of leading governance tools and used this analysis to evaluate two categories of our theory: FLOSS license scanning and FLOSS components in product bills of materials. The result is a list of FLOSS governance requirements. For practical relevance, we cast our theory as a requirements specification for FLOSS governance tools. © 2019","Company requirements for FLOSS tools; FLOSS; FLOSS governance tools; FOSS; Open source governance; Open source software"
"An OSLC-based environment for system-level functional testing of ERTMS/ETCS controllers","2020","Journal of Systems and Software","10.1016/j.jss.2019.110478","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075887207&doi=10.1016%2fj.jss.2019.110478&partnerID=40&md5=a23afbba0357282043d461d5c782d4cf","Product and application life-cycle management (PLM/ALM) are the processes that govern a product and a software system, respectively, encompassing the creation, deployment and operation of a system from the beginning to the end of its life. As both PLM and ALM require cross-discipline collaboration and cooperation, tools integration and inter-operation are necessary to enable the efficient and effective usage of tool suites supporting the management of the entire system life-cycle and overcome the limitations of all-in-one solutions from one tool vendor. In this context, the Open Services for Life-cycle Collaboration (OSLC) initiative proposes a set of specifications to allow a seamless integration based on linked data. This paper describes the work performed within the ARTEMIS JU project CRYSTAL to develop an environment for the functional system-level testing of railway controllers, relying on OSLC to enable inter-operation with existing PLM/ALM tools. A concrete realization of the proposed architecture is described also discussing some design and implementation choices. A real industrial case study is used to exemplify the features and the usage of the environment in testing one of the functionalities of the Radio Block Centre, the vital core of the European Rail Traffic Management System/European Train Control System (ERTMS/ETCS) Control System. © 2019","Critical systems; Life-cycle collaboration; Model based testing; OSLC; Testing automation"
"M3 - A hybrid measurement-modeling approach for CPU-bound applications on cross-platform architectures","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068484825&doi=10.1016%2fj.jss.2019.07.001&partnerID=40&md5=005fbe5efaf9bd80cffb7eb366d1b34c","Predicting performance of CPU intensive applications for a target platform is of significant importance to IT industries. However, the target hardware platform for which we want to predict the performance is often different from the testbed on which the application performance measurements are done, and may not be unavailable for deployment for various practical reasons. This paper presents M3, a Measure-Measure-Model method, which uses a pipeline of three steps to address this problem. The methodology starts with measuring CPU service demands of the application on the testbed. Then, it builds clones that mimic the application code in terms of the type of operations, the number and size of network calls with external servers and API calls made at different layers of the technology stack. The clones are simple and easy to deploy, yet demonstrate the same speedup factor between the source and the target as the original application. The clones are then deployed on the testbed and on the target to estimate the application CPU service demand under light load generation. In the final step, this estimated CPU service demand is fed into specific performance modeling tools that are capable of predicting application performance on the target under arbitrary higher workload. Predictions made using this approach are validated against direct measurements made on five applications in Java and PHP, and on a number of combinations of testbed and target platforms (Intel and AMD servers) and the prediction error is always less than 20%. © 2019 Elsevier Inc.","Benchmark; Cross-platform; Modeling; Multi-tier; Performance; Prediction"
"LAURA architecture: Towards a simpler way of building situation-aware and business-aware IoT applications","2020","Journal of Systems and Software","10.1016/j.jss.2019.110494","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076549690&doi=10.1016%2fj.jss.2019.110494&partnerID=40&md5=5534d08c73f459d4d8dac7dcc9c83e95","The explosion of smart objects made companies rethink their Business Model (BM) using Wireless Sensor Networks (WSN) and the Internet of Things (IoT) aiming to improve their Business Processes (BP) to achieve competitiveness. Business environments are complex due to the wide variety of technologies, hardware and software solutions that compose heterogeneous enterprise environments. On the other hand, putting real-world IoT scenarios into practice is still a challenge for even experienced developers, because it requires low-level programming skills and, at the same time, specific domain knowledge of a company‘s BM. This research paper proposes LAURA – Lean AUtomatic code generation for situation-aware and business-awaRe Applications, a flexible, service-oriented and general open-source conceptual architecture, designed to support the deployment of decoupled IoT applications. Empirical evaluation has shown that LAURA simplifies the development of final Situation-Aware or Business-Aware applications, reducing the need for specialized IoT low-level knowledge, while showing an acceptable performance. LAURA also provides the freedom and independence to modify, adapt or integrate its architecture according to specific needs of the stakeholders. © 2019 Elsevier Inc.","Business Process; Internet of things; IoT Architecture; Situation-Awareness; Wireless sensor networks"
"Measuring the reusability of software components using static analysis metrics and reuse rate information","2019","Journal of Systems and Software","10.1016/j.jss.2019.110423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072530259&doi=10.1016%2fj.jss.2019.110423&partnerID=40&md5=df151165979c491b88f99bb788984a25","Nowadays, the continuously evolving open-source community and the increasing demands of end users are forming a new software development paradigm; developers rely more on reusing components from online sources to minimize the time and cost of software development. An important challenge in this context is to evaluate the degree to which a software component is suitable for reuse, i.e. its reusability. Contemporary approaches assess reusability using static analysis metrics by relying on the help of experts, who usually set metric thresholds or provide ground truth values so that estimation models are built. However, even when expert help is available, it may still be subjective or case-specific. In this work, we refrain from expert-based solutions and employ the actual reuse rate of source code components as ground truth for building a reusability estimation model. We initially build a benchmark dataset, harnessing the power of online repositories to determine the number of reuse occurrences for each component in the dataset. Subsequently, we build a model based on static analysis metrics to assess reusability from five different properties: complexity, cohesion, coupling, inheritance, documentation and size. The evaluation of our methodology indicates that our system can effectively assess reusability as perceived by developers. © 2019","Code reuse; Developer-perceived reusability; Reusability estimation; Static analysis metrics"
"A complete run-time overhead-aware schedulability analysis for MrsP under nested resources","2020","Journal of Systems and Software","10.1016/j.jss.2019.110449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074376544&doi=10.1016%2fj.jss.2019.110449&partnerID=40&md5=75181957f8b2c96faa7d394194dbe592","Multiprocessor Resource Sharing Protocol (MrsP) is a hard real-time multiprocessor resource sharing protocol for fully partitioned fixed-priority systems, and adopts a novel helping mechanism to allow task migrations during resource accessing. Previous research focusing on analysing MrsP systems have delivered two forms of timing analysis which effectively bound response time and migration cost of tasks under MrsP, and have demonstrated advantages of this protocol. An adjustable non-preemptive section is also introduced that effectively reduces the number of migrations needed during each resource access. However, these analysis methods are only applicable if a non-nested resource accessing model is assumed. In addition, there is no clear approach towards the configuration of the non-preemptive section length, and the computation cost for applying the analysis remains unknown. In this paper, we extend the MrsP analysis for systems with nested resources. Major run-time costs incurred by MrsP tasks are also taken into account to form a complete run-time cost-aware schedulability analysis. In addition, recommendations towards non-preemptive section configuration are given from both analytic and empiric perspectives. Finally, a set of evaluations are conducted to investigate schedulability of MrsP under nested resources and the cost for applying the proposed analysis. As a result of this paper, the schedulability test for MrsP is complete and the computation costs of its use are now understood. © 2019","Hard real-time system; Multiprocessor resource sharing protocol; Nested resources; Run-time overhead; Schedulability test"
"Evaluating and empirically improving the visual syntax of use case diagrams","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067991014&doi=10.1016%2fj.jss.2019.06.096&partnerID=40&md5=0639480bf535d27aba0bb2ccd5fd5410","Use case modeling is a forefront technique to specify functional requirements of a system. Many research works related to use case modeling have been devoted to improving various aspects of use case modeling and its utilization in software development processes. One key aspect of use case models that has thus far been overlooked by the research community is the visual perception of use case diagrams by its readers. Any model is used transfer a mental idea by a modeler to a model reader. Even if a use case diagram is constructed flawlessly, if it is misread or misinterpreted by its reader then the intrinsic purpose of modeling has failed. This paper provides a two-fold contribution. Firstly, this paper presents an evaluation of the cognitive effectiveness of use case diagrams notation. The evaluation is based on theory principles and empirical evidence mainly from the cognitive science field. Secondly, it provides empirically validated improvements to the use case diagram notation that enhances its cognitive effectiveness. Empirical validation of the improvements is drawn by conducting an industrial survey using business analyst professionals. Empirical validation is also drawn by conducting an experiment using software engineering professionals as subjects. © 2019","Cognitive effectiveness; UML; Use case notation; Visual syntax"
"Cooperative Thinking: Analyzing a new framework for software engineering education","2019","Journal of Systems and Software","10.1016/j.jss.2019.110401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071636994&doi=10.1016%2fj.jss.2019.110401&partnerID=40&md5=0c8fde655b18387f170171442eb8dbaa","Computational Thinking (CT) and Agile Values (AV) focus respectively on the individual capability to think algorithmically, and on the principles of collaborative software development. Although these two dimensions of software engineering education complement each other, very few studies explored their interaction. In this paper we use an exploratory Structural Equation Modeling technique to introduce and analyze Cooperative Thinking (CooT), a model of team-based computational problem solving. We ground our model on the existing literature and validate it through Partial Least Square modeling. Cooperative Thinking is new competence which aim is to support cooperative problem solving of technical contents suitable to deal with complex software engineering problems. This article suggests to tackle the CooT construct as an education goal, to train students of software development to improve both their individual and teaming performances. © 2019 Elsevier Inc.","Agile; Computational Thinking; Computer Science Education; Cooperative Thinking; Empirical Software Engineering; High School; K-12; Latent Variable Analysis; Multivariate Analysis; Partial Least Squares; Software Engineering Education; Structural Equation Modelling"
"How does object-oriented code refactoring influence software quality? Research landscape and challenges","2019","Journal of Systems and Software","10.1016/j.jss.2019.110394","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071483107&doi=10.1016%2fj.jss.2019.110394&partnerID=40&md5=d59edcfb8685632e0dcd8d2ce99b7a0f","Context: Software refactoring aims to improve software quality and developer productivity. Numerous empirical studies investigating the impact of refactoring activities on software quality have been conducted over the last two decades. Objective: This study aims to perform a comprehensive systematic mapping study of existing empirical studies on evaluation of the effect of object-oriented code refactoring activities on software quality attributes. Method: We followed a multi-stage scrutinizing process to select 142 primary studies published till December 2017. The selected primary studies were further classified based on several aspects to answer the research questions defined for this work. In addition, we applied vote-counting approach to combine the empirical results and their analysis reported in primary studies. Results: The findings indicate that studies conducted in academic settings found more positive impact of refactoring on software quality than studies performed in industries. In general, refactoring activities caused all quality attributes to improve or degrade except for cohesion, complexity, inheritance, fault-proneness and power consumption attributes. Furthermore, individual refactoring activities have variable effects on most quality attributes explored in primary studies, indicating that refactoring does not always improve all quality attributes. Conclusions: This study points out several open issues which require further investigation, e.g., lack of industrial validation, lesser coverage of refactoring activities, limited tool support, etc. © 2019","Object-oriented software; Quality measures; Refactoring activity; Software quality; Systematic mapping study"
"Collaborative configuration approaches in software product lines engineering: A systematic mapping study","2019","Journal of Systems and Software","10.1016/j.jss.2019.110422","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072521286&doi=10.1016%2fj.jss.2019.110422&partnerID=40&md5=e5a157a93bfb0d90f2f1991725a95868","In the context of software product line engineering, collaborative configuration is a decision-making process where multiple stakeholders contribute in building a single product specification. Several approaches addressing collaboration during configuration have already been proposed, but we still have little hard evidence about their effectiveness and little understanding about how collaborative configuration process should be carried out. This paper presents a classification framework to help understand existing collaborative configuration approaches. To elaborate it, a systematic mapping study was conducted guided by three research questions and 41 primary studies was selected out of 238 identified ones. The proposed framework is composed of four dimensions capturing main aspects related to configuration approaches: purpose, collaboration, process and tool. Each dimension is itself multi-faceted and a set of attributes is associated to each facet. Using this framework, we position and classify existing approaches, structure the representation of each approach characteristics, highlight their strengths and weaknesses, compare them to each other, and identify open issues. This study gives a solid foundation for classifying existing and future approaches for product lines collaborative configuration. Researchers and practitioners can use our framework for identifying existing research/technical gaps to attack, better scoping their own contributions, or understanding existing ones. © 2019","Collaborative configuration; Framework; Product lines; Systematic mapping study"
"The Moitree middleware for distributed mobile-cloud computing","2019","Journal of Systems and Software","10.1016/j.jss.2019.07.089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070236311&doi=10.1016%2fj.jss.2019.07.089&partnerID=40&md5=657890f207b49df44550ba9985bc502a","Commonly, mobile cloud computing assumes that each mobile device of a user is paired with a user-controlled surrogate in the cloud to overcome resource limitations on mobiles. Our Avatar platform leverages this model to support efficient distributed computing over mobile devices. An avatar is a per-user, always-on software entity that resides in the cloud and acts as the surrogate of the mobile. Mobile-avatar pairs participate in distributed computing as a unified computing entity in such a way that the workload and the demand for resources on the mobiles remain low. This paper presents Moitree, the middleware of the Avatar platform, which provides a common programming and execution framework for mobile distributed apps. Moitree allows the components of a distributed app to execute seamlessly over a set of mobile-avatar pairs, with the provision of offloading computation and communication to the cloud. The programming framework has two key features: user collaborations are modeled using context-aware group semantics - groups are created dynamically based on context; data communication among group members is offloaded to the cloud through high-level communication channels. A prototype of Moitree, along with several apps, has been implemented and evaluated on Android devices and on a cloud running Android x86 avatars. © 2019","Distributed systems; Middleware; Mobile apps; Mobile cloud computings"
"Survey-based investigation, feature extraction and classification of Greek municipalities maturity for open source adoption and migration prospects","2019","Journal of Systems and Software","10.1016/j.jss.2019.110431","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072785004&doi=10.1016%2fj.jss.2019.110431&partnerID=40&md5=cca537c4a58be9a2bda5ab66a1ae4b2f","While FOSS solutions have attracted a significant amount of attention, alongside with necessary and growing IT related expenditure for the digitalization of public administration, authorities face the question of whether migration to such solutions is feasible and/or worthwhile. The purpose of the presented work is to analyze existing domain research and extract key features, grouped in three major areas (namely Readiness, Ease of Use and Gain from migration), that should be investigated prior to a migration attempt. Following and building upon an extensive survey on Greek municipalities, the latter are categorized via the k-means non-supervised machine learning method to 3 levels of maturity regarding candidate participation in relevant projects. A combined scoring approach, based on similar features of the target FOSS solution as well as the aforementioned municipality categorization, is presented in order to detect a priori good candidate combinations (municipality and software) for minimizing a migration project risk. The method is validated through the aforementioned survey on municipalities with confirmed FOSS usage, indicating that selection in the proposed organized manner can aid in harvesting FOSS benefits. Furthermore, it is compared against a popular maturity model method (BPMMM) in order to comment on the applicability and classification process. © 2019","Classification; FOSS; Maturity models; Migration; Risk analysis; Survey"
"Reproducing performance bug reports in server applications: The researchers’ experiences","2019","Journal of Systems and Software","10.1016/j.jss.2019.06.100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069005716&doi=10.1016%2fj.jss.2019.06.100&partnerID=40&md5=5e40f1fff045088999bb8190579f9925","Performance is one of the key aspects of non-functional qualities as performance bugs can cause significant performance degradation and lead to poor user experiences. While bug reports are intended to help developers to understand and fix bugs, they are also extensively used by researchers for finding benchmarks to evaluate their testing and debugging approaches. Although researchers spend a considerable amount of time and effort in finding usable performance bugs from bug repositories, they often get only a few. Reproducing performance bugs is difficult even for performance bugs that are confirmed by developers with domain knowledge. The amount of information disclosed in a bug report may not always be sufficient to reproduce the performance bug for researchers, and thus hinders the usability of bug repository as the resource for finding benchmarks. In this paper, we study the characteristics of confirmed performance bugs by reproducing them using only informations available from the bug report to examine the challenges of bug reproduction from the perspective of researchers. We spent more than 800 h over the course of six months to study and to try to reproduce 93 confirmed performance bugs, which are randomly sampled from two large-scale open-source server applications. We (1) studied the characteristics of the reproduced performance bug reports; (2) summarized the causes of failed-to-reproduce performance bug reports from the perspective of researchers by reproducing bugs that have been solved in bug reports; (3) shared our experience on suggesting workarounds to improve the bug reproduction success rate; (4) delivered a virtual machine image that contains a set of 17 ready-to-execute performance bug benchmarks. The findings of our study provide guidance and a set of suggestions to help researchers to understand, evaluate, and successfully replicate performance bugs. © 2019 Elsevier Inc.","Bug characteristics study; Experience report; Performance bug reproduction"
"Maintaining interoperability in open source software: A case study of the Apache PDFBox project","2020","Journal of Systems and Software","10.1016/j.jss.2019.110452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074353150&doi=10.1016%2fj.jss.2019.110452&partnerID=40&md5=40242d86ca012a52e20f012ec3a44064","Software interoperability is commonly achieved through the implementation of standards for communication protocols or data representation formats. Standards documents are often complex, difficult to interpret, and may contain errors and inconsistencies, which can lead to differing interpretations and implementations that inhibit interoperability. Through a case study of two years of activity in the Apache PDFBox project we examine day-to-day decisions made concerning implementation of the PDF specifications and standards in a community open source software (OSS) project. Thematic analysis is used to identify semantic themes describing the context of observed decisions concerning interoperability. Fundamental decision types are identified including emulation of the behaviour of dominant implementations and the extent to which to implement the PDF standards. Many factors influencing the decisions are related to the sustainability of the project itself, while other influences result from decisions made by external actors, including the developers of dependencies of PDFBox. This article contributes a fine grained perspective of decision-making about software interoperability by contributors to a community OSS project. The study identifies how decisions made support the continuing technical relevance of the software, and factors that motivate and constrain project activity. © 2019 The Authors","Community open source software; Portable document format; Software implementation; Software interoperability; Standards"
"A routing algorithm for wireless sensor networks based on clustering and an fpt-approximation algorithm","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066094284&doi=10.1016%2fj.jss.2019.05.032&partnerID=40&md5=35aa849dad147d26583832f914caab4d","Clustering sensor nodes is an effective method for routing in Wireless Sensor Networks (WSNs), which maximizes the network lifetime and reduces the energy consumption. However, in a clustered-WSN, the Cluster Heads (CHs) bear a higher load compared to the other nodes, which leads to their earlier death. Therefore, minimizing the maximum load of the CHs is an important problem, which is called the Load-Balanced Clustering Problem (LBCP). LBCP is an NP-hard problem and the best-known approximation factor for this problem is 1.5. Moreover, it has been shown that there is no polynomial-time approximation algorithm that solves this problem with a better approximation factor. In this paper, we propose a Fixed Parameter Tractable (FPT) approximation algorithm with an approximation factor of 1.2 for LBCP. We also propose an energy-efficient and energy-balanced routing algorithm for routing between the CHs and the sink. The simulation results show that the proposed algorithm is practical for large-scale WSNs and performs better than the other similar algorithms. © 2019 Elsevier Inc.","Energy-aware; Load balancing; Multi-hop routing; Wireless sensor network"
"A motifs-based Maximum Entropy Markov Model for realtime reliability prediction in System of Systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061640286&doi=10.1016%2fj.jss.2019.02.023&partnerID=40&md5=37441cc14b37d0f0bff8349713e820f5","System of Systems (SoS) based on service composition is considered as an effective way to build large-scale complex software systems. It regards the system as a service and integrates multiple component systems into a new system. The performance of the component system may fluctuate at any time because of the complex and changeable running state and external environment of the component system, which will affect the running of the SoS. The online reliability prediction technology is used to predict the reliability of the component system of an SoS in the near future. It aims to find errors and correct them in time so as to ensure that the SoS can run continuously and smoothly. To tackle the reliability prediction problem of component system in a dynamic and uncertain environment, the paper integrates Maximum Entropy Markov Model (MEMM) with time series motifs to achieve a new prediction model (m_MEMM), which is referred to as motifs-based MEMM. Extensive experiments are conducted to demonstrate the effectiveness and accuracy of the proposed approach. © 2019 Elsevier Inc.","Maximum Entropy Markov Model; Reliability prediction; Service composition; System of Systems; Time series"
"Pro-IDTV: A sociotechnical process model for designing IDTV applications","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065543752&doi=10.1016%2fj.jss.2019.04.078&partnerID=40&md5=fe9ed7b602de4daa501592f55a7f5edc","TV is considered a pervasive social medium; with the arrival of digital technology, interactive Digital TV (iDTV) has come into existence, and TV usage has changed. On the one hand, these changes increase TV's potential in terms of its processing capacity, its potential to connect with other devices, and its ability to connect people to broadcasting companies in a two-way trade. On the other hand, these changes also increase the complexity of user interaction, as well as the social and technical challenges for designing solutions. Broadcasting companies may not be prepared to include iDTV application design in their production chain, and there are few theoretical references that can help them. This paper proposes a process model for iDTV named Pro-iDTV: a process for designing iDTV applications grounded on a sociotechnical approach that uses principles of Participatory Design and Organizational Semiotics. This process model was created and applied in a TV company, providing a practical understanding of the real forces that govern the organization, for, in addition, the process has the ability to tailor solutions to fit to this context. This study's results reflect the viability and benefits of the Pro-iDTV for supporting the design of iDTV applications in a practical setting. © 2019","Human-computer interaction; Interactive digital TV; Organizational semiotics; Participatory design; Process model; Socially aware computing"
"Static correction of Maude programs with assertions","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063666978&doi=10.1016%2fj.jss.2019.03.061&partnerID=40&md5=3925b4fd87e2f111934a28aff2e0ce59","                             In this paper, we present a novel transformation method for Maude programs featuring both automatic program diagnosis and correction. The input of our method is a reference specification A of the program behavior that is given in the form of assertions together with an overly general program R whose execution might violate the assertions. Our correction technique translates R into a refined program R                             ′                              in which every computation is also a computation in R that satisfies the assertions of A. The technique is first formalized for topmost rewrite theories, and then we generalize it to larger classes of rewrite theories that support nested structured configurations. Our technique copes with infinite space states and does not require the knowledge of any failing run. We report experiments that assess the effectiveness of assertion-driven correction.                          © 2019 Elsevier Inc.","Assertion checking; Equational rewriting; Maude; Program repair; Program transformation; Rewriting logic"
"A meta-model for software protections and reverse engineering attacks","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059877518&doi=10.1016%2fj.jss.2018.12.025&partnerID=40&md5=ab32fb28ecd97abcc47f48a5f85cdea0","Software protection techniques are used to protect valuable software assets against man-at-the-end attacks. Those attacks include reverse engineering to steal confidential assets, and tampering to break the software's integrity in unauthorized ways. While their ultimate aims are the original assets, attackers also target the protections along their attack path. To allow both humans and tools to reason about the strength of available protections (and combinations thereof) against potential attacks on concrete applications and their assets, i.e., to assess the true strength of layered protections, all relevant and available knowledge on the relations between the relevant aspects of protections, attacks, applications, and assets need to be collected, structured, and formalized. This paper presents a software protection meta-model that can be instantiated to construct a formal knowledge base that holds precisely that information. The presented meta-model is validated against existing models and taxonomies in the domain of software protection, and by means of prototype tools that we developed to help non-modelling-expert software defenders with populating a knowledge base and with extracting and inferring practically useful information from it. All discussed tools are available as open source, and we evaluate their use as part of a software protection work flow on an open source application and industrial use cases. © 2019 Elsevier Inc.","Attack modelling; Decision support; Meta-model; Reverse engineering; Security knowledge base; Software protection"
"Disaster recovery solutions for IT systems: A Systematic mapping study","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059349647&doi=10.1016%2fj.jss.2018.12.023&partnerID=40&md5=39ac84f4aad0dfaba52817b566e2e9cc","Context: Organizations are spending an unprecedented amount of money towards the cost of keeping Information Technology (IT) systems operational. Hence, these systems need to be designed using effective fault-tolerant techniques like Disaster Recovery (DR) solutions. Even though research has been done in the DR field, it is necessary to assess the current state of research and practice, to provide practitioners with evidence that enables foster its further development. Objective: This paper has the following goals: to investigate state-of-the-art solutions for DR, as well as to systematically analyze the current published research and identify different strategies available in the literature. Method: A systematic mapping study was conducted, in which 49 studies, dated from 2007 to 2017, were evaluated. Results: Various DR practices are being investigated. The results identified a number of relevant issues, including reasons to adopt DR solutions, strategies used to implement DR solutions, approaches employed to analyze DR solutions, and metrics considered during the analyses of DR solutions. Conclusion: The number of strategies and reasons for adopting DR solutions is overwhelming. Hence, there was a need to provide a consolidated view of the field. Also, the results can help to direct future research efforts in this critical area. © 2018 Elsevier Inc.","Disaster recovery; Information technology; Systematic mapping"
"End-user development, end-user programming and end-user software engineering: A systematic mapping study","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057320807&doi=10.1016%2fj.jss.2018.11.041&partnerID=40&md5=03d342429f8a6b7731eb70356de92ad6","End-User Development (EUD), End-Programming (EUP) and End-User Software Engineering (EUSE) are three related research fields that study methods and techniques for empowering end users to modify and create digital artifacts. This paper presents a systematic mapping study aimed at identifying and classifying scientific literature about EUD, EUP and EUSE in the time range January 2000–May 2017. We selected 165 papers found through a manual selection of papers from specific conferences, journal special issues, and books, integrated with an automatic search on the most important digital libraries. The answer to our research question was built through a classification of the selected papers on seven dimensions: type of approach, interaction technique, phase in which the approach is adopted, application domain, target use, class of users, and type of evaluation. Our findings suggest that EUD, EUP and EUSE are active research topics not only in Human–Computer Interaction, but also in other research communities. However, little cross-fertilization exists among the three themes, as well as unifying frameworks and approaches for guiding novice designers and practitioners. Other findings highlight trends and gaps related to the analysis’ dimensions, which have implications on the design of future tools and suggest open issues for further investigations. © 2018 Elsevier Inc.","End-user development; End-user programming; End-user software engineering; Systematic mapping study"
"Systematic composition of independent language features","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062348521&doi=10.1016%2fj.jss.2019.02.026&partnerID=40&md5=23bd944fdd445057fe6eddd70976328d","Systematic reuse is crucial to efficiently engineer and deploy software languages to software experts and domain experts alike. But “software languages are software too” and hence their engineering, customization, and reuse are subject to similar challenges. To this effect, we propose an approach for composing independent, grammar-based language syntax modules in a structured way that realizes a separation of concerns among the participants in the life cycle of the languages. We present a refined concept of systematic and controlled syntactic variability of extensible software language product lines through identification of syntax variation points and derivation of variants from independently developed features. This facilitates reuse of software languages and reduces the efforts of engineering and customizing languages for specific domains. We realized our concept with the MontiCore language workbench and assessed it through a case study on architecture description languages. Ultimately, systematic and controlled software language reuse reduces the effort of software language engineering and fosters the applicability of software languages. © 2019 Elsevier Inc.",""
"An empirical study of tactical vulnerabilities","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058386691&doi=10.1016%2fj.jss.2018.10.030&partnerID=40&md5=53c7207521eb45d11f99a14916b2f9b6","Architectural security tactics (e.g., authorization, authentication) are used to achieve stakeholders’ security requirements. Security tactics allow the system to react, resist, detect and recover from attacks. Flaws in the adoption of these tactics into the system's architecture, an incorrect implementation of security tactics, or deterioration of tactic implementations over time can introduce severe vulnerabilities that are exploitable by attackers. Therefore, in this work, we present the Common Architectural Weakness Enumeration (CAWE), a catalog of known weaknesses rooted in the design or implementation of security tactics which can result in tactical vulnerabilities. We categorized all known software weaknesses as tactic-related and non-tactic related. This way, our CAWE catalog enumerates common weaknesses in a security architecture that can lead to tactical vulnerabilities. From our CAWE catalog, we found 223 different types of tactical vulnerabilities. In this work, we also used this catalog to study tactical vulnerabilities in three large-scale open source projects: Chromium, PHP, and Thunderbird. In a detailed analysis, we identified the most occurring vulnerability types on these projects. From this study we observed that (i) Improper Input Validation and Improper Access Control were the most occurring vulnerability types in Chromium, PHP and Thunderbird and (ii) “Validate Inputs” and “Authorize Actors” were the security tactics mostly affected by these tactical vulnerabilities. Moreover, in a qualitative analysis of 632 tactical vulnerabilities and their fixes in these systems, we characterized their root causes and investigated the way the original developers of each system fixed these vulnerabilities. From this qualitative analysis, we found 44 distinct root causes that lead to these tactical vulnerabilities. The results of this study not only show how architectural weaknesses in systems have created severe vulnerabilities, but also provide recommendations driven by empirical data for addressing such security problems. © 2018 Elsevier Inc.","Architectural weaknesses; Security tactics; Software security architecture; Tactical vulnerabilities"
"BrownoutCon: A software system based on brownout and containers for energy-efficient cloud computing","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065894559&doi=10.1016%2fj.jss.2019.05.031&partnerID=40&md5=c35adecba4fa8eadc6b68d173fb3f2ef","VM consolidation and Dynamic Voltage Frequency Scaling approaches have been proved to be efficient to reduce energy consumption in cloud data centers. However, the existing approaches cannot function efficiently when the whole data center is overloaded. An approach called brownout has been proposed to solve the limitation, which dynamically deactivates or activates optional microservices or containers. In this paper, we propose a brownout-based software system for container-based clouds to handle overloads and reduce power consumption. We present its design and implementation based on Docker Swarm containers. The proposed system is integrated with existing Docker Swarm without the modification of their configurations. To demonstrate the potential of BrownoutCon software in offering energy-efficient services in brownout situation, we implemented several policies to manage containers and conducted experiments on French Grid’5000 cloud infrastructure. The results show the currently implemented policies in our software system can save about 10%–40% energy than the existing baselines while ensuring quality of services. © 2019 Elsevier Inc.","Brownout; Cloud data centers; Containers; Energy efficiency; Microservices; Quality of service"
"Managing inter-model inconsistencies in model-based systems engineering: Application in automated production systems engineering","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063760990&doi=10.1016%2fj.jss.2019.03.060&partnerID=40&md5=9d60825acf070ab3267104105c6da7a8","To cope with the challenge of managing the complexity of automated production systems, model-based approaches are applied increasingly. However, due to the multitude of different disciplines involved in automated production systems engineering, e.g., mechanical, electrical, and software engineering, several modeling languages are used within a project to describe the system from different perspectives. To ensure that the resulting system models are not contradictory, the necessity to continuously diagnose and handle inconsistencies within and in between models arises. This article proposes a comprehensive approach that allows stakeholders to specify, diagnose, and handle inconsistencies in model-based systems engineering. In particular, to explicitly capture the dependencies and consistency rules that must hold between the disparate engineering models, a dedicated graphical modeling language is proposed. By means of this language, stakeholders can specify, diagnose, and handle inconsistencies in the accompanying inconsistency management framework. The approach is implemented based on the Eclipse Modeling Framework (EMF) and evaluated based on a demonstrator project as well as a small user experiment. First findings indicate that the approach is expressive enough to capture typical dependencies and consistency rules in the automated production system domain and that it requires less effort compared to manually developing inter-model inconsistency management solutions. © 2019 Elsevier Inc.","Automated production systems; Inconsistency management; Model-based systems engineering"
"Where is my feature and what is it about? A case study on recovering feature facets","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063202330&doi=10.1016%2fj.jss.2019.01.057&partnerID=40&md5=1ff059865351bc087fea2fb55947a2fa","Developers commonly use features to define, manage, and communicate functionalities of a system. Unfortunately, the locations of features in code and other characteristics (feature facets), relevant for evolution and maintenance, are often poorly documented. Since developers change, and knowledge fades with time, such information often needs to be recovered. Modern projects boast a richness of information sources, such as pull requests, release logs, and otherwise specified domain knowledge. However, it is largely unknown from what sources the features, their locations, and their facets can be recovered. We present an exploratory study on identifying such information in two popular, variant-rich, and long-living systems: The 3D-printer firmware Marlin and the Android application Bitcoin-wallet. Besides the available information sources, we also investigated the projects’ communities, communications, and development cultures. Our results show that a multitude of information sources (e.g., commit messages and pull requests) is helpful to recover features, locations, and facets to different extents. Pull requests were the most valuable source to recover facets, followed by commit messages and the issue tracker. As many of the studied information sources are, so far, rarely exploited in techniques for recovering features and their facets, we hope to inspire researchers and tool builders with our results. © 2019 Elsevier Inc.","Bitcoin-wallet; Case study; Feature facets; Feature location; Marlin; Software product line"
"Understanding the order of agile practice introduction: Comparing agile maturity models and practitioners’ experience","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066489426&doi=10.1016%2fj.jss.2019.05.035&partnerID=40&md5=22006193082de64d9377f9fde753d8fd","Context: Agile maturity models (AMMs) suggest that agile practices are introduced in a certain order. However, whether the order of agile practice introduction as suggested in the AMMs is relevant in industry has not been evaluated in an empirical study. Objectives: In this study, we want to investigate: (1) order of agile practice introduction mentioned in AMMs, (2) order of introducing agile practices in industry, and (3) similarities and differences between (1) and (2). Methods: We conducted a literature survey to identify strategies proposed by the AMMs. We then compared the AMMs’ suggestions to the strategies used by practitioners, which we elicited from a survey and a series of interviews from an earlier study. Results: The literature survey revealed 12 AMMs which provide explicit mappings of agile practices to maturity levels. These mappings showed little agreement on when practices should be introduced. Comparison of the AMMs’ suggestions and the empirical study revealed that the guidance suggested by AMMs are not aligned with industry practice. Conclusion: Currently, AMMs do not provide sufficient information to guide agile adoption in industry. Our results suggest that there might be no universal strategy for agile adoption that works better than others. © 2019 Elsevier Inc.","Agile maturity model; Agile practice; Introduction strategies"
"Using acceptance tests to predict files changed by programming tasks","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065200156&doi=10.1016%2fj.jss.2019.04.060&partnerID=40&md5=4aad0eb0577429d14673863d77ae000b","In a collaborative development context, conflicting code changes might compromise software quality and developers productivity. To reduce conflicts, one could avoid the parallel execution of potentially conflicting tasks. Although hopeful, this strategy is challenging because it relies on the prediction of the required file changes to complete a task. As predicting such file changes is hard, we investigate its feasibility for BDD (Behaviour-Driven Development) projects, which write automated acceptance tests before implementing features. We develop a tool that, for a given task, statically analyzes Cucumber tests and infers test-based interfaces (files that could be executed by the tests), approximating files that would be changed by the task. To assess the accuracy of this approximation, we measure precision and recall of test-based interfaces of 513 tasks from 18 Rails projects on GitHub. We also compare such interfaces with randomly defined interfaces, interfaces obtained by textual similarity of test specifications with past tasks, and interfaces computed by executing tests. Our results give evidence that, in the specific context of BDD, Cucumber tests might help to predict files changed by tasks. We find that the better the test coverage, the better the predictive power. A hybrid approach for computing test-based interfaces is promising. © 2019 Elsevier Inc.","Behaviour-driven development; Collaborative development; File change prediction; Task scheduling"
"A model-driven IT governance process based on the strategic impact evaluation of services","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059127055&doi=10.1016%2fj.jss.2018.12.024&partnerID=40&md5=24841bdb851f1fac327cb9d548c48e1a","Model-driven engineering is used for managing software systems development. In most cases, requirements representations are transformed into design diagrams themselves transformed into code, ensuring traceability. Such an approach can nevertheless take roots on a more abstract level and be used for IT governance. Goal-oriented requirements engineering can be adapted to model strategic objectives aimed to lead the organization to an enhanced competitive position in the long-term. Most IT governance and management frameworks are driven by the concept of service. The latter allows to package the work offer of an IT provider. Within their realization, such services align or misalign with strategic objectives. This paper proposes a model-driven IT governance process allowing to evaluate the alignment of business IT services to strategic objectives; it follows the 3 stages of IT governance: evaluate, direct and monitor. The approach allows to integrate the governance level as a (graphical) strategic layer made of long-term objectives that business IT services potentially contribute or hamper to attain. The strategic layer is custom developed for each organization and linked with organizational representations in a model-driven fashion to study business and IT alignment. The framework is called MoDrIGo; it is applied onto a case study in a hospital. © 2018 Elsevier Inc.","Business strategy; I*; IT Governance; IT Strategy; Model-Driven engineering; Service-based modeling"
"Preference based multi-objective algorithms applied to the variability testing of software product lines","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061664081&doi=10.1016%2fj.jss.2019.02.028&partnerID=40&md5=0612a36fee4f5e275e1d01b21726d15e","Evolutionary Multi-Objective Algorithms (EMOAs) have been applied to derive products for the variability testing of Software Product Lines (SPLs), which is a complex task impacted by many factors, such as the number of products to be tested, coverage criteria, and efficacy to reveal faults. But such algorithms generally produce a lot of solutions that are uninteresting to the tester. This happens because traditional search algorithms do not take into consideration the user preferences. To ease the selection of the best solutions and avoid effort generating uninteresting solutions, this work introduces an approach that applies Preference-Based Evolutionary Multi-objective Algorithms (PEMOAs) to solve the problem. The approach is multi-objective, working with the number of products to be tested, pairwise coverage and mutation score. It incorporates the preferences before the evolution process and uses the Reference Point (RP) method. Two PEMOAs are evaluated: R-NSGA-II and r-NSGA-II, using two different formulations of objectives, and three kinds of RPs. PEMOAs outperform the traditional NSGA-II by generating a greater number of solutions in the Region of Interest (ROI) associated to the RPs. The use of PEMOAs can reduce the tester's burden in the task of selecting a better and reduced set of products for SPL testing. © 2019 Elsevier Inc.","Preference-Based algorithms; Search-Based software engineering; Software product line testing"
"Synthesizing tradeoff spaces with quantitative guarantees for families of software systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062227387&doi=10.1016%2fj.jss.2019.02.055&partnerID=40&md5=0028038de7ef0eb658b992c70721da16","Designing software in a way that guarantees run-time behavior while achieving an acceptable balance among multiple quality attributes is an open problem. Providing guarantees about the satisfaction of the same requirements under uncertain environments is even more challenging. Tools and techniques to inform engineers about poorly-understood design spaces in the presence of uncertainty are needed, so that engineers can explore the design space, especially when tradeoffs are crucial. To tackle this problem, we describe an approach that combines synthesis of spaces of system design alternatives from formal specifications of architectural styles with probabilistic formal verification. The main contribution of this paper is a formal framework for specification-driven synthesis and analysis of design spaces that provides formal guarantees about the correctness of system behaviors and satisfies quantitative properties (e.g., defined over system qualities) subject to uncertainty, which is treated as a first-class entity. We illustrate our approach in two case studies: a service-based adaptive system and a mobile robotics architecture. Our results show how the framework can provide useful insights into how average case probabilistic guarantees can differ from worst case guarantees, emphasizing the relevance of combining quantitative formal verification methods with structural synthesis, in contrast with techniques based on simulation and dynamic analysis that can only provide estimates about average case probabilistic properties. © 2019","Architectural style; Architecture synthesis; Probabilistic model checking; Quantitative guarantees; Tradeoff analysis; Uncertainty"
"How enterprise architecture improves the quality of IT investment decisions","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062838184&doi=10.1016%2fj.jss.2019.02.053&partnerID=40&md5=2283ea9d4a6a71464f421922bac78c0b","According to literature, enterprise architecture (EA) is supposed to support IT investment decision-making. However, it is not yet clear how EA can do that. The objective of this study is to explore how EA can support IT investment decisions. A quantitative research approach was chosen, in which data were collected from a survey of 142 participants. These data were used to perform a comparative analysis between top and bottom quartile organizations on 1) the EA maturity, 2) the use of EA artifacts in the preparation of IT investments, and 3) the key insights that EA provides in preparation of IT investments. We found that top quartile organizations are more mature in all EA maturity areas. They also make more extensive use of different types of EA artifacts in the preparation of IT investment decisions, especially diagnostic and actionable artifacts. Finally, EA provides top quartile organizations with more key insights in the preparation of IT investment decisions, and in particular, strategic insights. As a result of our research we created a conceptual model that integrates seven propositions. Further research is required to test these propositions and develop instruments to aid enterprise architects to effectively support IT investment decisions. © 2019 Elsevier Inc.","Enterprise architecture; Enterprise architecture artifacts; Enterprise architecture insights; Enterprise architecture maturity; IT investments; Quality of IT investment decision outcomes"
"Are architectural smells independent from code smells? An empirical study","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064869442&doi=10.1016%2fj.jss.2019.04.066&partnerID=40&md5=8334052a5794276a00b59c0c89ab5a37","Background. Architectural smells and code smells are symptoms of bad code or design that can cause different quality problems, such as faults, technical debt, or difficulties with maintenance and evolution. Some studies show that code smells and architectural smells often appear together in the same file. The correlation between code smells and architectural smells, however, is not clear yet; some studies on a limited set of projects have claimed that architectural smells can be derived from code smells, while other studies claim the opposite. Objective. The goal of this work is to understand whether architectural smells are independent from code smells or can be derived from a code smell or from one category of them. Method. We conducted a case study analyzing the correlations among 19 code smells, six categories of code smells, and four architectural smells. Results. The results show that architectural smells are correlated with code smells only in a very low number of occurrences and therefore cannot be derived from code smells. Conclusion. Architectural smells are independent from code smells, and therefore deserve special attention by researchers, who should investigate their actual harmfulness, and practitioners, who should consider whether and when to remove them. © 2019 Elsevier Inc.","Architectural smells; Code smells; Empirical analysis; Technical debt"
"Evaluating the extension mechanisms of the knowledge discovery metamodel for aspect-oriented modernizations","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058487323&doi=10.1016%2fj.jss.2018.12.011&partnerID=40&md5=955c55cbbb3786bc89ae6962a3794f8c","Crosscutting concerns are an intrinsic problem of legacy systems, hindering their maintenance and evolution. A possible solution is to modernize these systems employing aspect-orientation, which provides suitable abstractions for modularizing these kind of concerns. Architecture-Driven Modernization is a more specific kind of software reengineering focused on employing standard metamodels along the whole process, promoting interoperability and reusability across different tools/vendors. Its main metamodel is the Knowledge Discovery Metamodel (KDM), which is able to represent a significant amount of system details. However, up to this moment, there is no extension of this metamodel for aspect-orientation, preventing software engineers from conducting Aspect-Oriented Modernizations. Therefore, in this paper we present our experience on creating a heavyweight and a lightweight extension of KDM for aspect-orientation. We conducted two evaluations. The first one showed all aspect-oriented concepts were represented in both extensions. The second one was a experiment, in which we have analyzed the productivity of software engineers using both extensions. The results showed that the heavyweight extension propitiate a more productive environment in terms of time and number of errors when compared to the lightweight one. © 2018","Aspect-oriented modernization; Heavyweight extension; Knowledge discovery metamodel; Legacy systems; Lightweight extension; OMG"
"Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065830870&doi=10.1016%2fj.jss.2019.05.001&partnerID=40&md5=af8ac66aae7d2814e3a310ab244a971a","In industrial practice the clone-and-own strategy is often applied when in the pressure of high demand of customized features. The adoption of software product line (SPL) architecture is a large one time investment that affects both technical and organizational issues. The analysis of the feature structure is a crucial point in the SPL adoption process involving domain experts working at a higher level of abstraction and developers working directly on the program code. We propose automatic methods to extract feature-to-program links starting from very high level set of features provided by domain experts. For this purpose we combine call graph information with textual similarity between code and high level features. In addition, in depth understanding of the feature structure is supported by finding communities between programs and relating them to features. As features are originated from domain experts, community analysis reveals discrepancies between expert view and internal code structure. We found that communities correspond well to the high level features, with usually more than half of feature code located in specialized communities. We report experiments at two levels of features and more than 2000 Magic 4GL programs in an industrial SPL adoption project. © 2019 The Authors","Community detection; Feature extraction; Information retrieval; Software product line"
"Landscaping systematic mapping studies in software engineering: A tertiary study","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058845930&doi=10.1016%2fj.jss.2018.12.018&partnerID=40&md5=851b1a8deb4cb5c7dc758e7e85014310","Context: A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area. Objectives: We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs. Methodology: We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK). Results: We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies. Conclusion: There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score. © 2018","Secondary study; Software engineering; Survey; Systematic mapping study; Tertiary study"
"Minimum/maximum delay testing of product lines with unbounded parametric real-time constraints","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059456237&doi=10.1016%2fj.jss.2018.12.028&partnerID=40&md5=31f341b25ccf2ccd3361100fffb5f3f9","Non-functional requirements like real-time behaviors are of ever-growing interest in many application domains of software product lines. Consequently, existing modeling formalisms and analysis techniques for reasoning about time-critical behaviors have to be adapted to product-line engineering, too. Featured timed automata (FTA) extend timed automata (TA) by feature constraints to enable efficient family-based verification of real-time properties. Here, we present configurable parametric timed automata (CoPTA) to further extend expressiveness of FTA by freely configurable and a-priori unbounded timing intervals of real-time constraints. Hence, CoPTA models impose infinite configuration spaces which makes variant-by-variant analysis practically infeasible. Instead, we present a family-based test-suite generation methodology for CoPTA models ensuring symbolical location coverage for every model configuration. Furthermore, we define a novel coverage criterion, called Minimum/Maximum Delay (M/MD) coverage, requiring every location in a CoPTA model to be reached by test cases with minimum/maximum possible durations, for systematically investigating best-case/worst-case execution times. We extend our family-based test-suite generation methodology to also achieve M/MD coverage on CoPTA models. Our evaluation results, obtained from applying our CoPTA tool to a collection of subject systems, reveal efficiency improvements of family-based test-suite generation, as compared to a variant-by-variant strategy in case of finite configuration spaces. © 2018 Elsevier Inc.","Best-case/Worst-case execution time; Model-based testing; Real-time systems; Software product lines; Timed automata"
"Partially safe evolution of software product lines","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065535714&doi=10.1016%2fj.jss.2019.04.051&partnerID=40&md5=934055ff3b6b84f546018ac92c054289","Software Product Lines allow the automatic generation of related products built with reusable artefacts. In this context, developers may need to perform changes and check whether products are affected. A strategy to perform such analysis is verifying behaviour preservation through the use of formal theories. The product line refinement notion requires behaviour preservation for all existing products. Nevertheless, in evolution scenarios like bug fixes, some products intentionally have their behaviour changed. To support developers in these and other unsafe scenarios, we define a theory of partial product line refinement that helps to precisely understand which products are affected by a change. This provides a kind of impact analysis that could, for example, reduce test effort, since only affected products need to be tested. We provide properties such as compositionality, which deals with changes to a specific product line element, and general properties to support developers when safe and partially safe scenarios are combined. We also define a set of transformation templates, which are classified according to their compatibility to specific types of product lines. To evaluate our work, we analyse two product lines: Linux and Soletta, to discover if our templates could be helpful in evolving these systems. © 2019 Elsevier Inc.","Product line evolution; Product line maintenance; Product line refinement"
"Energy-efficient low-latency audio on android","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063064929&doi=10.1016%2fj.jss.2019.03.013&partnerID=40&md5=666a3bc6dffb813eae037c6ba3e48d05","Counting more than two billion devices, Android is nowadays one of the most popular open-source general-purpose operating systems, based on Linux. Because of the diversity of applications that can be installed, it manages a number of different workloads, many of them requiring performance/QoS guarantees. When running audio processing applications, the user would like an uninterrupted, glitch-free, output stream that reacts to the user input, typically with a delay not bigger than 4−10 ms, while keeping the energy consumption of the mobile device as low as possible. This work focuses on improvements to the real-time audio processing performance on Android. Such improvements are achieved by using a deadline based scheduler and an adaptive scheduling strategy that dynamically and proactively modulates the allocated runtime. The proposed strategy is evaluated through an extensive set of experiments, showing that (1) compared to the existing way to ensure low-latency audio processing, the proposed mechanism provides an energy saving of almost 40%, and (2) compared to the existing way to achieve a good balance between power consumption and latency in a glitch-free audio processing experience, the proposed solution reduces audio latency from 26.67 ms to 2.67 ms, at the expense of a limited power consumption increase of 6.25%. © 2019 Elsevier Inc.","Adaptive reservations; Android; Energy efficiency; Linux kernel; Low-latency audio; Real-time scheduling"
"A systematic literature review on crowdsourcing in software engineering","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064228488&doi=10.1016%2fj.jss.2019.04.027&partnerID=40&md5=a6fb649f582f4d2df767908d01305406","Background: Crowdsourcing outsources a task to large groups of people by open call format, and it recently plays significant role for software practitioners. Aim: The purpose of this study is to conduct a comprehensive overview on crowdsourcing in software engineering (CSE), concerning business models, tools, platforms, software development processes, and software economics. Method: We conducted a systematic literature review on CSE. We identified 158 relevant studies and 6 secondary studies. We further reviewed 67 primary studies that passed our quality assessment criteria. We defined 10 research questions and synthesized different approaches used in primary studies regarding each question. Results: Majority of studies report the application of crowdsourcing for coding and testing tasks. Crowdsourcing follows a unique methodology in which project planning, task specification and deployment have more emphasis. There is not enough literature on effort estimation approaches in CSE and associated cost factors. Complexity of the task and its expected duration play significant role in estimation. Conclusions: Future studies should focus more on economic models, experience reports, specific software development methodologies, and strategic pricing mechanism for CSE. © 2019 Elsevier Inc.","Crowdsourcing; Crowdsourcing in software engineering; Empirical software engineering; Systematic literature review"
"PARS: A parallel model for scaled processing of complex events","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065492406&doi=10.1016%2fj.jss.2019.05.014&partnerID=40&md5=89440cd338b332a4b436eb2565e444f4","The need for fast processing of high volume of event streams has triggered the deployment of parallel processing models and techniques for complex event processing. It is however hard to parallelize stateful operators, making the implementations of distributed complex event processing systems very challenging. A well-defined parallel processing model is a pre-requisite to the implementation of any high performance distributed complex event processing system. Most recent works use partition key or shared memory for parallelizing stateful operators but suffer from imbalanced distribution of keys and slow access time to shared memory. This paper proposes a new parallel model called PARS using three partitioning techniques to allow scalable processing of complex events without using partition key or shared memory. We define PARS formally and use this formalism to prove its soundness and completeness. To present a proof-of-concept and evaluate PARS, we use an event generator to simulate event sources and show significant improvement in system scalability. Compared to other works, we experience 10%–20% higher throughput in our experimental results. Our experiments demonstrate processing speeds of up to 5,200,000 complex event detection per second on a multi-machine cluster. © 2019 Elsevier Inc.","Complex event processing; Data stream processing; Distributed processing; Parallel model; Scalability; Stateful operators"
"Tracking runtime concurrent dependences in java threads using thread control profiling","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056586550&doi=10.1016%2fj.jss.2018.11.003&partnerID=40&md5=8e2fc19e917d25c722d48872b7696b26","More than 75% of recent Java projects include some form of concurrent programming. Due to complex interactions between multi-threads, concurrent programs are often harder to understand and test than single threaded programs. To facilitate understanding and testing of concurrent programs, we developed a new profiling method called TCP (Thread Control Profiling). Outputs of TCP presents frequencies of control dependence, which includes thread creation, thread synchronization, interruption, and so on, of the executed thread. TCP first performs static analysis of detailed concurrency syntax and semantics of Java to construct the profiling graph model TCDG (Thread Control Dependence Graph). TCDG is then used for instrumentation and for generating profiles. We have evaluated TCP using a case study and a few experiments. The case study shows that TCP method can effectively prioritize test cases for testing concurrent programs. One experiment shows that outputs from TCP facilitate developers’ understanding of concurrent code. Other experiments evaluate various possible overheads introduced by the TCP method. Results show that TCP can provide rich and useful information with reasonable costs. © 2018 Elsevier Inc.","Concurrent; Dynamic analysis; Java thread; Profiling; Synchronization"
"The role of Sprint planning and feedback in game development projects: Implications for game quality","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064683740&doi=10.1016%2fj.jss.2019.04.057&partnerID=40&md5=b7981c2171a9207fdf441d23c7ec1ebe","Game development projects adopt Scrum to leverage their flexibility, as game concepts and the customer preferences are highly abstract and unpredictable. The most desirable features in an original game will not be easily identified during the first phase of development but will emerge later in a clear pattern as developers and testers continuously playtest the game. Thus, game development projects use feedback from game testers to understand what they think of various features and concepts, to obtain a better understanding of problem spaces. This study proposes that game tester feedback moderates the effect of Sprint planning on game quality. A field study was conducted using a pair-matched questionnaire in which 102 game development projects participated. Results showed that Sprint planning has a positive effect on game quality. The results also revealed that iterative feedback has a moderating effect on the relationship between Sprint planning and game quality. Theoretical and practical implications are discussed. © 2019 Elsevier Inc.","Feedback; Game development project; Game quality; Game tester; Sprint planning"
"Modelling equivalence classes of feature models with concept lattices to assist their extraction from product descriptions","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062039403&doi=10.1016%2fj.jss.2019.02.027&partnerID=40&md5=1e2cd2f59cc13d6e3cda01a31519be86","Software product line engineering gathers a set of methods to help create, manage and maintain a collection of similar software systems. Variability modelling is a focal point of this paradigm, where feature models (FMs) are the prevalent notation. Migration from single system development to software product lines is a spreading topic in software engineering. To ease the migration, research has been done to automatically extract FMs from software descriptions, but most of these approaches are defined in a functional manner based on an ad-hoc variability analysis. In this paper, we propose a theoretical view on FM extraction from software descriptions based on Formal Concept Analysis (FCA). It is a structural framework for variability representation which allows to lay down theoretical foundation to variability extraction. We propose an original mapping between relationships expressed in FMs and the ones emphasised in FCA conceptual structures. We show that conceptual structures represent equivalence classes of FMs that steer the user choices during their synthesis, and propose a reverse engineering method based on them. We discuss its applicability and show that the combinatorial explosion of concept lattices can be avoided by the use of two sub-orders embodying the necessary information concerning variability. © 2019 Elsevier Inc.","Feature models; Formal concept analysis; Reverse engineering; Software product lines; Variability modelling"
"Does the fault reside in a stack trace? Assisting crash localization by predicting crashing fault residence","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056287541&doi=10.1016%2fj.jss.2018.11.004&partnerID=40&md5=c8accb92a9dbd293eafc62c056b9ac81","Given a stack trace reported at the time of software crash, crash localization aims to pinpoint the root cause of the crash. Crash localization is known as a time-consuming and labor-intensive task. Without tool support, developers have to spend tedious manual effort examining a large amount of source code based on their experience. In this paper, we propose an automatic approach, namely CraTer, which predicts whether a crashing fault resides in stack traces or not (referred to as predicting crashing fault residence). We extract 89 features from stack traces and source code to train a predictive model based on known crashes. We then use the model to predict the residence of newly-submitted crashes. CraTer can reduce the search space for crashing faults and help prioritize crash localization efforts. Experimental results on crashes of seven real-world projects demonstrate that CraTer can achieve an average accuracy of over 92%. © 2018","Crash localization; Crashing fault residence; Predictive model; Stack trace"
"Adopting integrated application lifecycle management within a large-scale software company: An action research approach","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057340795&doi=10.1016%2fj.jss.2018.11.021&partnerID=40&md5=006c7da7b6e3cd30619af2a67facd395","Context: Application Lifecycle Management (ALM) is a paradigm for integrating and managing the various activities related to the governance, development and maintenance of software products. In the last decade, several ALM tools have been proposed to support this process, and an increasing number of companies have started to adopt ALM. Objective: We aim to investigate the impact of adopting ALM in a real industrial context to understand and justify both the benefits and obstacles of applying integrated ALM. Method: As a research methodology, we apply action research that we have carried out within HAVELSAN, a large-scale IT company. The research was carried out over a period of seven years starting in 2010 when the ALM initiative has been started in the company to increase productivity and decrease maintenance costs. Results: The paper presents the results of the action research that includes the application of ALM practices. The transitions among the different steps are discussed in detail, together with the identified obstacles, benefits and lessons learned. Conclusions: Our seven-year study shows that the adoption of ALM processes is not trivial and its success is related to many factors. An important conclusion is that a piecemeal solution as provided by ALM 1.0 is not feasible for the complex process and tool integration problems of large enterprises. Hence the transition to ALM 2.0 was found necessary to cope with the organizational and business needs. Although ALM 2.0 appeared to be a more mature ALM approach, there are still obstacles that need attention from both researchers and practitioners. © 2018","Action research; Application lifecycle management; Application lifecycle management transformation; Change management; DevOps"
"Astor: Exploring the design space of generate-and-validate program repair beyond GenProg","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061161484&doi=10.1016%2fj.jss.2019.01.069&partnerID=40&md5=66de7e1d6487cb091fe7e54b47c7e1a4","This article contributes to defining the design space of program repair. Repair approaches can be loosely characterized according to the main design philosophy, in particular “generate- and-validate” and synthesis-based approaches. Each of those repair approaches is a point in the design space of program repair. Our goal is to facilitate the design, development and evaluation of repair approaches by providing a framework that: a) contains components commonly present in most approaches, b) provides built-in implementations of existing repair approaches. This paper presents a Java framework named Astor that focuses on the design space of generate-and-validate repair approaches. The key novelty of Astor is to provides explicit extension points to explore the design space of program repair. Thanks to those extension points, researchers can both reuse existing program repair components and implement new ones. Astor includes 6 unique implementations of repair approaches in Java, including GenProg for Java called jGenProg. Researchers have already defined new approaches over Astor. The implementations of program repair approaches built already available in Astor are capable of repairing, in total, 98 real bugs from 5 large Java programs. Astor code is publicly available on Github: https://github.com/SpoonLabs/astor. © 2019","Automated Program Repair; Defects; Evaluation Frameworks; Software Bugs; Software Maintenance; Software Testing"
"Semantics-based platform for context-aware and personalized robot interaction in the internet of robotic things","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057508312&doi=10.1016%2fj.jss.2018.11.022&partnerID=40&md5=c8be9790daeb085756c611ac68f7f955","Robots are moving from well-controlled lab environments to the real world, where an increasing number of environments has been transformed into smart sensorized IoT spaces. Users will expect these robots to adapt to their preferences and needs, and even more so for social robots that engage in personal interactions. In this paper, we present declarative ontological models and a middleware platform for building services that generate interaction tasks for social robots in smart IoT environments. The platform implements a modular, data-driven workflow that allows developers of interaction services to determine the appropriate time, content and style of human-robot interaction tasks by reasoning on semantically enriched IoT sensor data. The platform also abstracts the complexities of scheduling, planning and execution of these tasks, and can automatically adjust parameters to the personal profile and current context. We present motivational scenarios in three environments: a smart home, a smart office and a smart nursing home, detail the interfaces and executional paths in our platform and present a proof-of-concept implementation. © 2018 Elsevier Inc.","Context-aware systems; Internet of things; Ontology; Personalization; Semantics; Social robots"
"Towards the definitive evaluation framework for cross-platform app development approaches","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064165528&doi=10.1016%2fj.jss.2019.04.001&partnerID=40&md5=ba4ff468ddfc141a023496faaf0c94be","Mobile app development is hindered by device fragmentation and vendor-specific modifications. Boundaries between devices blur with PC-tablet hybrids on the one side and wearables on the other. Future apps need to support a host of app-enabled devices with differing capabilities, along with their software ecosystems. Prior work on cross-platform app development concerned concepts and prototypes, and compared approaches that target smartphones. To aid choosing an appropriate framework and to support the scientific assessment of approaches, an up-to-date comparison framework is needed. Extending work on a holistic, weighted set of assessment criteria, we propose what could become the definitive framework for evaluating cross-platform approaches. We have based it on sound abstract concepts that allow extensions. The weighting capabilities offer customisation to avoid the proverbial comparison of apples and oranges lurking in the variety of available frameworks. Moreover, it advises on multiple development situations based on a single assessment. In this article, we motivate and describe our evaluation criteria. We then present a study that assesses several frameworks and compares them to Web Apps and native development. Our findings suggest that cross-platform development has seen much progress but the challenges are ever growing. Therefore, additional support for app developers is warranted. © 2019 The Authors","Cross-platform; Development framework; Mobile app; Mobile computing; Multi-platform"
"An auction-based incentive mechanism for heterogeneous mobile clouds","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062826619&doi=10.1016%2fj.jss.2019.03.003&partnerID=40&md5=67a36207c23fd84f1a13da1dde457dd5","Traditional mobile cloud computing (MCC) adopts a onefold mobile device and public cloud paradigm. As the mobile device capabilities continue to develop, their opportunistic utilization in MCC has recently gained popularity. The new paradigm is a hybrid/heterogeneous mobile cloud (HMC) where mobile devices, cloudlets, and private/public cloud form a shared resource network for task offloading. However, mobile device users are discouraged from sharing their devices for running foreign tasks due to the battery life and privacy concerns. To incentivize mobile device users to utilize and participate in the HMC offloading service, we designed a task offloading market for the HMC service, where a mobile user can compete as a seller with others by bidding its redundant computing resources, and another mobile user as a buyer can pay the bidding price and offload the task to the winning user. To enable an incentive and fair competition of the mobile cloud offloading market, we propose a reverse auction-based incentive mechanism, mCloudAuc, to provide real-time auctions. The proposed auction algorithm demonstrates computation efficiency, truthfulness, and individual rationality for the participants through proof and multiple simulations. Our prototype implementation of mCloudAuc on Android platform has also shown its feasibility in practice. © 2019","Code offloading; Incentive mechanism; Mobile cloud computing; Reverse auction"
"A mixed-method empirical study of Function-as-a-Service software development in industrial practice","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058711426&doi=10.1016%2fj.jss.2018.12.013&partnerID=40&md5=45d29e054337fa5497a92cd1cb99e12a","Function-as-a-Service (FaaS) describes cloud computing services that make infrastructure components transparent to application developers, thus falling in the larger group of “serverless” computing models. When using FaaS offerings, such as AWS Lambda, developers provide atomic and short-running code for their functions, and FaaS providers execute and horizontally scale them on-demand. Currently, there is no systematic research on how developers use serverless, what types of applications lend themselves to this model, or what architectural styles and practices FaaS-based applications are based on. We present results from a mixed-method study, combining interviews with practitioners who develop applications and systems that use FaaS, a systematic analysis of grey literature, and a Web-based survey. We find that successfully adopting FaaS requires a different mental model, where systems are primarily constructed by composing pre-existing services, with FaaS often acting as the “glue” that brings these services together. Tooling availability and maturity, especially related to testing and deployment, remains a major difficulty. Further, we find that current FaaS systems lack systematic support for function reuse, and abstractions and programming models for building non-trivial FaaS applications are limited. We conclude with a discussion of implications for FaaS providers, software developers, and researchers. © 2018 Elsevier Inc.","Cloud computing; Empirical research; Function-as-a-Service; Serverless"
"Software product lines and variability modeling: A tertiary study","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059325213&doi=10.1016%2fj.jss.2018.12.027&partnerID=40&md5=d993c7b96dbf5e2de7d1cc3beaa44f60","Context: A software product line is a means to develop a set of products in which variability is a central phenomenon captured in variability models. The field of SPLs and variability have been topics of extensive research over the few past decades. Objective: This research characterizes systematic reviews (SRs) in the field, studies how SRs analyze and use evidence-based results, and identifies how variability is modeled. Method: We conducted a tertiary study as a form of systematic review. Results: 86 SRs were included. SRs have become a widely adopted methodology covering the field broadly otherwise except for variability realization. Numerous variability models exist that cover different development artifacts, but the evidence is insufficient in quantity and immature, and we argue for better evidence. SRs perform well in searching and selecting studies and presenting data. However, their analysis and use of the quality of and evidence in the primary studies often remains shallow, merely presenting of what kinds of evidence exist. Conclusions: There is a need for actionable, context-sensitive, and evaluated solutions rather than novel ones. Different kinds of SRs (SLRs and Maps) need to be better distinguished, and evidence and quality need to be better used in the resulting syntheses. © 2018","Mapping study; Software product line; Systematic literature review; Tertiary study; Variability; Variability modeling"
"Re-implementing a legacy system","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066271675&doi=10.1016%2fj.jss.2019.05.012&partnerID=40&md5=161a5c24c9502c2cdc0b8e48490d5701","Re-implementation is one of the alternatives to migrate a legacy software system next to conversion, wrapping and redevelopment. It is a compromise solution between automated conversion and complete redevelopment. The technical architecture can be revised and the code replaced, but the functional architecture – the use cases remains as it was. The challenge of this approach is to preserve the functionality while changing the technical implementation. This approach is taken when conversion is not feasible and redevelopment is too expensive or too great a risk. It entails more than a 1:1 transformation but less than a total rewrite. The same components remain with different contents. In this paper the case for reimplementation is presented and the process described. The tools required to support the process are identified and their use illustrated. Finally, two industrial case studies are presented, one with a VisualAge/ PL/I-DB2 system and one with a COBOL-IMS application. © 2019 Elsevier Inc.","Code refactoring; Code rewriting; Data renaming; Reverse engineering; Software migration; Software re-implementation"
"Swarm debugging: The collective intelligence on interactive debugging","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064200182&doi=10.1016%2fj.jss.2019.04.028&partnerID=40&md5=f084643c3376200d0f75ebb0cf4431b8","One of the most important tasks in software maintenance is debugging. To start an interactive debugging session, developers usually set breakpoints in an integrated development environment and navigate through different paths in their debuggers. We started our work by asking what debugging information is useful to share among developers and study two pieces of information: breakpoints (and their locations) and sessions (debugging paths). To answer our question, we introduce the Swarm Debugging concept to frame the sharing of debugging information, the Swarm Debugging Infrastructure (SDI) with which practitioners and researchers can collect and share data about developers’ interactive debugging sessions, and the Swarm Debugging Global View (GV) to display debugging paths. Using the SDI, we conducted a large study with professional developers to understand how developers set breakpoints. Using the GV, we also analyzed professional developers in two studies and collected data about their debugging sessions. Our observations and the answers to our research questions suggest that sharing and visualizing debugging data can support debugging activities. © 2019 Elsevier Inc.","Debugging; Distributed systems; Empirical studies; Information foraging; Software visualization; Swarm debugging"
"Revisiting the impact of common libraries for android-related investigations","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064915523&doi=10.1016%2fj.jss.2019.04.065&partnerID=40&md5=b17bd5ba66ba1208b524d3756afbd6b1","The packaging model of Android apps requires the entire code to be shipped into a single APK file in order to be installed and executed on a device. This model introduces noises to Android app analyses, e.g., detection of repackaged applications, malware classification, as not only the core developer code but also the other assistant code will be visited. Such assistant code is often contributed by common libraries that are used pervasively by all apps. Despite much effort has been put in our community to investigate Android libraries, the momentum of Android research has not yet produced a complete and reliable set of common libraries for supporting thorough analyses of Android apps. In this work, we hence leverage a dataset of about 1.5 million apps from Google Play to identify potential common libraries, including advertisement libraries, and their abstract representations. With several steps of refinements, we finally collect 1113 libraries supporting common functions and 240 libraries for advertisement. For each library, we also collected its various abstract representations that could be leveraged to find new usages, including obfuscated cases. Based on these datasets, we further empirically revisit three popular Android app analyses, namely (1) repackaged app detection, (2) machine learning-based malware detection, and (3) static code analysis, aiming at measuring the impact of common libraries on their analysing performance. Our experimental results demonstrate that common library can indeed impact the performance of Android app analysis approaches. Indeed, common libraries can introduce both false positive and false negative results to repackaged app detection approaches. The existence of common libraries in Android apps may also impact the performance of machine learning-based classifications as well as that of static code analysers. All in all, the aforementioned results suggest that it is essential to harvest a reliable list of common libraries and also important to pay special attention to them when conducting Android-related investigations. © 2019",""
"Impact of usability on process lead-time in information systems: A case study","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056457773&doi=10.1016%2fj.jss.2018.11.002&partnerID=40&md5=c91b8f7b567207b3c1837a6107d8c29f","Technological advancements have started to demand the need for information systems to survive in the business world. There is an inherent need to be fast and efficient to conquer the market. In order to reduce time to market there is a need to shorten the lead-time in design and development process of products. Hence, lead-time is a significant performance metric for a product development organizations having information systems. It is important to explore the factors which affect the process lead-time in information systems. The objective of this study is to explore the impact that usability (one of the factors that may affect the process lead-time) has on process lead-time. This paper presents a case study involving qualitative and quantitate data from Volvo Group, which uses an information system called KOLA for their design and development process. Data was collected through 29 interviews, 17 think aloud sessions, 5 document studies, and 73 responses from a survey. This study identifies which usability factors and heuristics that have an impact on process lead-time. Moreover, the results show that the users level of experience determines the effective utilization of accelerators and thereby affecting the process lead-time. Finally, usability plays an important role in shortening the process lead-time in information systems. © 2018 Elsevier Inc.","Case study; Information systems; Process lead-time; Usability"
"Data prefetching and file synchronizing for performance optimization in Hadoop-based hybrid cloud","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061453975&doi=10.1016%2fj.jss.2019.02.007&partnerID=40&md5=5f86420894c114fe7803d6ac68a1356b","Driven by the technical factors such as system reliability, bandwidth constraints, data confidentiality and security, as well as the economic factors such as initial capital expenditure and re-occurring operating expenditure, today's cloud computing tends to adopt hybrid cloud model. However, because hybrid clouds scale both numerically and geographically, the network delay becomes the main constraint in remote file system access. To hide network latency and reduce job completion time in Hadoop-based hybrid cloud data access, a scheduling-aware data prefetching scheme to enhance non-local map task's data locality in Hadoop-based centralized hybrid cloud (CHCDLOS-Prefetch) and a file synchronizing method to decrease job execution delay in Hadoop-based distributed hybrid cloud (DHCDLO-Sync) are proposed. In the former, input data for non-local map tasks are fetched ahead of time to target compute nodes by making use of idle network bandwidth. In the latter, considered from job level scheduling, data files with high popularity are proactively synchronized beforehand among sub-clouds to strength intra sub-cloud data locality in distributed hybrid cloud. Extensive experimental results illustrate that compared to the Capacity, the Fair and the DARE algorithms, our proposed algorithms improve hybrid cloud performance more significantly in data locality and job completion time. © 2019 Elsevier Inc.","Data prefetching; File synchronizing; Hybrid cloud"
"Efficient accelerator sharing in virtualized environments: A Xeon Phi use-case","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060044163&doi=10.1016%2fj.jss.2018.12.029&partnerID=40&md5=40ce8c52117ffa3a19515058e3273299","High performance computing experts and scientists increasingly pay attention to heterogeneous processing and most systems today are built with some form of heterogeneity. At the same time, a large amount of HPC workloads is executed in cloud computing environments, due to the benefits that cloud offers both for the users and the infrastructure providers. In this context, there is a growing need to bridge the gap between the accelerator ecosystem and the world of virtualization, which is the building block of cloud computing environments. Hence, future accelerator platforms need to be designed in a virtualization-friendly approach, both in the software and in the hardware level. In this paper we extend our previous work and introduce vPHI, a transparent framework that enables sharing of Xeon Phi devices between multiple virtual machines. We design vPHI based on the paravirtualization technique and provide compatibility with the accelerator's native transport layer to the upper level software stack components. In this way, with vPHI existing applications can be used without any source code or binary modifications. The evaluation of our framework in the popular offload mode of execution shows promising results with vPHI exhibiting minimal overhead for the computational phases in a deployment with multiple (up to 8) VMs. vPHI can enable better accelerator utilization, increasing up to 3.4x the total throughput versus a single host application. We also discuss the applicability of our approach to future accelerator technologies and in this way form a path to bring the accelerator use cases closer to the cloud computing world. © 2019 Elsevier Inc.","Accelerator sharing; Accelerator virtualization; HPC; Virtualization; vPHI; Xeon Phi virtualization"
"Fine-grained just-in-time defect prediction","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059617149&doi=10.1016%2fj.jss.2018.12.001&partnerID=40&md5=e18332213b4ec63d273dcd1be2e9bb2e","Defect prediction models focus on identifying defect-prone code elements, for example to allow practitioners to allocate testing resources on specific subsystems and to provide assistance during code reviews. While the research community has been highly active in proposing metrics and methods to predict defects on long-term periods (i.e.,at release time), a recent trend is represented by the so-called short-term defect prediction (i.e.,at commit-level). Indeed, this strategy represents an effective alternative in terms of effort required to inspect files likely affected by defects. Nevertheless, the granularity considered by such models might be still too coarse. Indeed, existing commit-level models highlight an entire commit as defective even in cases where only specific files actually contain defects. In this paper, we first investigate to what extent commits are partially defective; then, we propose a novel fine-grained just-in-time defect prediction model to predict the specific files, contained in a commit, that are defective. Finally, we evaluate our model in terms of (i) performance and (ii) the extent to which it decreases the effort required to diagnose a defect. Our study highlights that: (1) defective commits are frequently composed of a mixture of defective and non-defective files, (2) our fine-grained model can accurately predict defective files with an AUC-ROC up to 82% and (3) our model would allow practitioners to save inspection efforts with respect to standard just-in-time techniques. © 2018","Empirical Software Engineering; Just-in-time defect prediction; Mining software repositories"
"Co-change patterns: A large scale empirical study","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063057395&doi=10.1016%2fj.jss.2019.03.014&partnerID=40&md5=00946ed81e94e315057b1c6edf8848d7","Co-Change Clustering is a modularity assessment technique that reveals how often changes are localized in modules and whether a change propagation represents design problems. This technique is centered on co-change clusters, which are highly inter-related source code files considering co-change relations. In this paper, we conduct a series of empirical analysis in a large corpus of 133 popular software projects on GitHub. We describe six co-change patterns by projecting them over the directory structure. We mine 1802 co-change clusters and 1719 co-change clusters (95%) are covered by the six co-change patterns. In this study, we aim to answer two central questions: (i) Are co-change patterns detected in different programming languages? (ii) How do different co-change patterns relate to rippling, activity density, ownership, and team diversity on clusters? We conclude that Encapsulated and Well-Confined clusters (Wrapped) implement well-defined and confined concerns. Octopus clusters are proportionally numerous regarding to other patterns. They relate significantly with ripple effect, activity, ownership, and diversity in development teams. Although Crosscutting are scattered over directories, they implement well-defined concerns. Despite they present higher activity compared to Wrapped clusters, it is not necessarily easy to get rid of them, suggesting that support tools may play a crucial role. © 2019","Co-change clusters; Co-change patterns; Modularity"
"FM-CF: A framework for classifying feature model building approaches","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064193087&doi=10.1016%2fj.jss.2019.04.026&partnerID=40&md5=a62ab1a6a851a93263ca06bd355f7073","Software product line engineering has emerged as a prominent software engineering paradigm, as it comprises a set of core assets sharing functionality and quality attributes. Feature modelling is one of the most frequently used techniques for modelling the variability within a software product line. There are several proposals for building Feature Models which rely on semi-automated or fully automated means. Unfortunately, automatic feature model construction has been addressed from different viewpoints, so it is not easy to know which is the best approach for automating the building of variability models. In fact, there is no clarity regarding common elements, and the main differences that characterise such approaches. Additionally, the wide variety of terms used to refer to the process of building a Feature Model (e.g. synthesis, location, re-engineering, and weaving) means that approaches are varied and very heterogeneous, making them complex to understand and classify. This paper introduces FM-CF, which is a Conceptual experience-based Framework for classifying approaches for the automatic building of Feature Models. The framework considers a set of categories mainly focused on characterising some aspects, such as input sources, methods and techniques, results, and types of evaluation. A literature review of (semi-) automated Feature Model construction was performed to identify approaches for building Feature Models by (semi-)automatic means, and the main terms used by those approaches. Then the completeness of the framework was evaluated by mapping the set of dimensions and their items, and the terms extracted from the literature. The conceptual framework provides guidance to researchers for choosing the appropriate aspects with which to build Feature Models, and helps in the understanding and clarification of the proposed approaches. © 2019","Classification; Feature model; Framework; Models; Software product lines"
"Real-time control architecture based on Xenomai using ROS packages for a service robot","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060513751&doi=10.1016%2fj.jss.2019.01.052&partnerID=40&md5=abafa41310fd956c63af5eeafeffac38","This paper proposes a real-time (RT) control architecture based on Xenomai, an RT embedded Linux, to control a service robot along with non-real-time (NRT) robot operating system (ROS) packages. Most software, including device drivers and ROS, are developed to operate under the standard Linux kernel that does not provide RT guarantees. Standard Linux system calls in an RT context stimulates mode switching, resulting in non-deterministic responses and stability problems such as priority inversion and kernel panic. This paper overcomes such issues through a communication interface between RT and NRT tasks, termed cross-domain datagram protocol. The proposed architecture supports priority-based scheduling of multiple tasks while exposing an interface compatible with the original ROS packages. Moreover, it enables standard device driver operation inside RT tasks without developing RT device drivers that requires significant amount of development time. Feasibility is proven by implementation on a Raspberry Pi 3, a low-cost open embedded hardware platform, and conducted various experiments to analyze its performance and applied it to a service robot using ROS navigation packages. The results indicate that the proposed architecture can effectively provide an RT environment without stability issues when utilizing ROS packages and standard device drivers. © 2019","Cross-domain datagram protocol; Real-time control architecture; Robot operating system; Service mobile robots; Xenomai"
"Employing rule mining and multi-objective search for dynamic test case prioritization","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063729476&doi=10.1016%2fj.jss.2019.03.064&partnerID=40&md5=2f45d142fe8c545b1730fd76fb05fe11","                             Test case prioritization (TP) is widely used in regression testing for optimal reordering of test cases to achieve specific criteria (e.g., higher fault detection capability) as early as possible. In our earlier work, we proposed an approach for black-box dynamic TP using rule mining and multi-objective search (named as REMAP) by defining two objectives (fault detection capability and test case reliance score) and considering test case execution results at runtime. In this paper, we conduct an extensive empirical evaluation of REMAP by employing three different rule mining algorithms and three different multi-objective search algorithms, and we also evaluate REMAP with one additional objective (estimated execution time) for a total of 18 different configurations (i.e., 3 rule mining algorithms × 3 search algorithms × 2 different set of objectives) of REMAP. Specifically, we empirically evaluated the 18 variants of REMAP with 1) two variants of random search while using two objectives and three objectives, 2) three variants of greedy algorithm based on one objective, two objectives, and three objectives, 3) 18 variants of static search-based prioritization approaches, and 4) six variants of rule-based prioritization approaches using two industrial and three open source case studies. Results showed that the two best variants of REMAP with two objectives and three objectives significantly outperformed the best variants of competing approaches by 84.4% and 88.9%, and managed to achieve on average 14.2% and 18.8% higher Average Percentage of Faults Detected per Cost (APFD                             c                             ) scores.                          © 2019 Elsevier Inc.","Black-box regression testing; Dynamic test case prioritization; Multi-objective optimization; Rule mining; Search"
"Software project management in high maturity: A systematic literature mapping","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056281325&doi=10.1016%2fj.jss.2018.10.002&partnerID=40&md5=78e04d7b8f13dc9979b6ce59e5fc1243","High maturity in software development involves statistically controlling the performance of critical subprocesses and using the predictability thus gained to manage projects with better planning precision and monitoring control. Maturity models such as CMMI mention statistical and other quantitative methods, techniques, and tools supporting high-maturity project management, but do not provide details about them, their use or their available types. Thus, knowledge is lacking on how to support software process improvement initiatives to select and apply statistical and other quantitative methods, techniques and tools in this context. The goal of this study is to identify various methods, techniques, and tools which can assist in high-maturity software project management. By conducting a systematic literature mapping, we identified 108 papers describing 153 contributions. We describe the contributions identified, classifying them by their type, their software technology maturation phase, the method by which they were evaluated, the development methods and characteristics which they support, and the process/indicator areas to which they were applied. We hope this work can help fill the knowledge gap on the statistical and other quantitative methods, techniques and tools actually being proposed, evaluated, experimented with and adopted by organizations to support quantitative high-maturity software project management. © 2018","High maturity project management; Maturity models; Quantitative project management"
"Verifying fragility in digital systems with uncertainties using DSVerifier v2.0","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063263200&doi=10.1016%2fj.jss.2019.03.015&partnerID=40&md5=09a3c48b4e12f6b725ae7728ba63a0f0","Control-system robustness verification with respect to implementation aspects lacks automated verification approaches for checking stability and performance of uncertain control systems, when considering finite word-length (FWL) effects. Here we describe and evaluate novel verification procedures for digital systems with uncertainties, based on software model checking and satisfiability modulo theories, named as DSVerifier v2.0, which is able to check robust stability of closed-loop control systems with respect to FWL effects. In particular, we describe our verification algorithms to check for limit-cycle oscillations (LCOs), output quantization error, and robust non-fragile stability on common closed-loop associations of digital control systems (i.e., series and feedback). DSVerifier v2.0 model checks new properties of closed-loop systems (e.g., LCO), including stability and output quantization error for uncertain plant models, and considers unknown parameters and FWL effects. Experimental results over a large set of benchmarks show that 35%, 34%, and 41% of success can be reached for stability, LCO, and output quantization error verification procedures, respectively, for a set of 396 closed-loop control system implementations and realizations. © 2019 Elsevier Inc.","Bounded model checking; Fixed-point digital controllers; Formal methods; System reliability; Uncertainty"
"Developing a model-driven reengineering approach for migrating PL/SQL triggers to Java: A practical experience","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061007557&doi=10.1016%2fj.jss.2019.01.068&partnerID=40&md5=b25973164a23921bb0248647029349b2","Model-driven software engineering (MDE) techniques are not only useful in forward engineering scenarios, but can also be successfully applied to evolve existing systems. RAD (Rapid Application Development) platforms emerged in the nineties, but the success of modern software technologies motivated that a large number of enterprises tackled the migration of their RAD applications, such as Oracle Forms. Our research group has collaborated with a software company in developing a solution to migrate PL/SQL monolithic code on Forms triggers and program units to Java code separated in several tiers. Our research focused on the model-driven reengineering process applied to develop the migration tool for the conversion of PL/SQL code to Java. Legacy code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In this paper, we propose a software process to implement a model-driven re-engineering. This process integrates a TDD-like approach to incrementally develop model transformations with three kinds of validations for the generated code. The implementation and validation of the re-engineering approach are explained in detail, as well as the evaluation of some issues related with the application of MDE. © 2019","KDM; Model-driven development; Model-driven software modernization; Oracle forms; Reengineering; Software modernization"
"Who should make decision on this pull request? Analyzing time-decaying relationships and file similarities for integrator prediction","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065147140&doi=10.1016%2fj.jss.2019.04.055&partnerID=40&md5=6cc09f33dd10328570df8560dfc15627","In pull-based development model, integrators are responsible for making decisions about whether to accept pull requests and integrate code contributions. Ideally, pull requests are assigned to integrators and evaluated within a short time after their submissions. However, the volume of incoming pull requests is large in popular projects, and integrators often encounter difficulties in processing pull requests in a timely fashion. Therefore, an automatic integrator prediction approach is required to assign appropriate pull requests to integrators. In this paper, we propose an approach TRFPre which analyzes Time-decaying Relationships and File similarities to predict integrators. We evaluate the effectiveness of TRFPre on 24 projects containing 138,373 pull requests. Experimental results show that TRFPre makes accurate integrator predictions in terms of accuracies and Mean Reciprocal Rank. Less than 2 predictions are needed to find correct integrator in 91.67% of projects. In comparison with state-of-the-art approaches cHRev, WRC, TIE, CoreDevRec and ACRec, TRFPre improves top-1 accuracy by 68.2%, 73.9%, 49.3%, 14.3% and 46.4% on average across 24 projects. © 2019 Elsevier Inc.","Code review; Github; Integrator prediction; Open source"
"Search-Based test case prioritization for simulation-Based testing of cyber-Physical system product lines","2019","Journal of Systems and Software","10.1016/j.jss.2018.09.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057869201&doi=10.1016%2fj.jss.2018.09.055&partnerID=40&md5=023adbc51c9d46d3d0e6dfa732df128b","Cyber-Physical Systems (CPSs) integrate computation with physical processes. These systems are usually highly configurable to address different customer needs and are evolving to be CPS product lines. The variability of CPS product lines is large, which implies that they can be set into millions of configurations. As a result, different cost-effective methods are needed to optimize the test process of these systems. We propose a search-based approach that aims to cost-effectively optimize the test process of CPS product lines by prioritizing the test cases that are executed in specific products at different test levels. The prioritized test suite aims at reducing the fault detection time, the simulation time and the time required to cover functional and non-functional requirements. We compared our approach by integrating five search algorithms as well as Random Search (RS) using four case studies. As compared with RS, the search algorithms managed to reduce fault detection time by 47%, the simulation time by 23%, the functional requirements covering time by 22% and the non-functional requirements covering time by 47%. Moreover, we observed that the performance of search algorithms varied for different case studies but the local search algorithms were more effective than the global search algorithms. © 2018 Elsevier Inc.","Cyber-Physical system product lines; Product lines; Search algorithms; Test case prioritization; Variability"
"State of the art of cyber-physical systems security: An automatic control perspective","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058033875&doi=10.1016%2fj.jss.2018.12.006&partnerID=40&md5=30541230bb21209a8e5ec87df56ba5f0","Cyber-physical systems are integrations of computation, networking, and physical processes. Due to the tight cyber-physical coupling and to the potentially disrupting consequences of failures, security here is one of the primary concerns. Our systematic mapping study sheds light on how security is actually addressed when dealing with cyber-physical systems from an automatic control perspective. The provided map of 138 selected studies is defined empirically and is based on, for instance, application fields, various system components, related algorithms and models, attacks characteristics and defense strategies. It presents a powerful comparison framework for existing and future research on this hot topic, important for both industry and academia. © 2018 Elsevier Inc.","Cyber-physical systems; Security; Systematic mapping study"
"Latency-aware Virtualized Network Function provisioning for distributed edge clouds","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062071319&doi=10.1016%2fj.jss.2019.02.030&partnerID=40&md5=fb8d16e47bbe1b8299f5704c1e3c640f","The emergence of Network Function Virtualization (NFV) enabled decoupling network functionality from dedicated hardware and placing them upon generic computing resources. Moreover, the introduction of edge computing paradigm which utilized the resources at the network edges brings reduced end-to-end latency. With these technologies, Virtualized Network Functions (VNFs) can be placed in anywhere either in the central clouds to utilize more resources or in the network edges to reduce the end-to-end latency. In this work, we propose a dynamic resource provisioning algorithm for VNFs to utilize both edge and cloud resources. Adapting to dynamically changing network volumes, the algorithm automatically allocates resources in both the edge and the cloud for VNFs. The algorithm considers the latency requirement of different applications in the service function chain, which allows the latency-sensitive applications to reduce the end-to-end network delay by utilizing edge resources over the cloud. We evaluate our algorithm in the simulation environment with large-scale web application workloads and compare with the state-of-the-art baseline algorithm. The result shows that the proposed algorithm reduces the end-to-end response time by processing 77.9% more packets in the edge nodes compared to the application non-aware algorithm. © 2019 Elsevier Inc.","Cloud computing; Edge computing; Network Function Virtualization (NFV); Service Function Chaining; Software-defined clouds; Software-Defined Networking (SDN)"
"A distributed data management system to support large-scale data analysis","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056241321&doi=10.1016%2fj.jss.2018.11.007&partnerID=40&md5=ca193dcca95e88b489c90179f070f4bb","Distributed data management is a key technology to enable efficient massive data processing and analysis in cluster-computing environments. Specifically, in environments where the data volumes are beyond the system capabilities, big data files are required to be summarized by representative samples with the same statistical properties as the whole dataset. This paper proposes a big data management system (BDMS) based on distributed random sample data blocks. It presents a high-level architecture design of the BDMS which extends the current distributed file systems. This system offers certain functionalities for block-level management such as statistically-aware data partitioning, data blocks organization, and data blocks selection. This paper also presents a round-random partitioning scheme to represent a big dataset as a set of non-overlapping data blocks; each block is a random sample of the whole dataset. Based on the presented scheme, two algorithms are introduced as an implementation strategy to convert the HDFS blocks of a big file into a set of random sample data blocks which is also stored in HDFS. The experimental results show that the execution time of partitioning operation is acceptable in the real applications because this operation is only performed once on each input data file. © 2018 Elsevier Inc.","Big data; Data management; Distributed and parallel processing; Random sample partition; Randomness"
"Genetic algorithm based test data generation for MPI parallel programs with blocking communication","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065889055&doi=10.1016%2fj.jss.2019.04.049&partnerID=40&md5=80c275109ce7738602df9329c22dd1b6","Parallel computing is one of mainstream techniques for high-performance computation in which MPI parallel programs have gained more and more attention. Genetic algorithms (GAs)have been widely employed in automated test data generation, leading to a major family of search-based software testing techniques. However, previous GA-based methods have limitations when testing MPI parallel programs with blocking communication. In this paper, we focus on the path coverage problem for MPI parallel programs with blocking communication, and formulate the problem as an optimization problem with its decision variable being the program input and the execution order of sending nodes. In addition, we develop target amending strategies for candidates when solving the problem using genetic algorithms. The proposed method is evaluated and compared with several state-of-the-art methods through a series of controlled experiments on five typical programs. The experimental results show that the proposed method can effectively and efficiently generate test data for path coverage. © 2019 Elsevier Inc.","Blocking communication; Genetic algorithm; Parallel program; Path coverage; Test data"
"Uncertainty-wise test case generation and minimization for Cyber-Physical Systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063204143&doi=10.1016%2fj.jss.2019.03.011&partnerID=40&md5=ef92dc0188ac1419922794fa1b2be37e","Cyber-Physical Systems (CPSs) typically operate in highly indeterminate environmental conditions, which require the development of testing methods that must explicitly consider uncertainty in test design, test generation, and test optimization. Towards this direction, we propose a set of uncertainty-wise test case generation and test case minimization strategies that rely on test ready models explicitly specifying subjective uncertainty. We propose two test case generation strategies and four test case minimization strategies based on the Uncertainty Theory and multi-objective search. These strategies include a novel methodology for designing and introducing indeterminacy sources in the environment during test execution and a novel set of uncertainty-wise test verdicts. We performed an extensive empirical study to select the best algorithm out of eight commonly used multi-objective search algorithms, for each of the four minimization strategies, with five use cases of two industrial CPS case studies. The minimized set of test cases obtained with the best algorithm for each minimization strategy were executed on the two real CPSs. The results showed that our best test strategy managed to observe 51% more uncertainties due to unknown indeterminate behaviors of the physical environments of the CPSs as compared to the other test strategies. Also, the same test strategy managed to observe 118% more unknown uncertainties as compared to the unique number of known uncertainties. © 2019 Elsevier Inc.","Cyber-Physical System; Multi-objective search; Test case generation and minimization; Uncertainty"
"Software project scheduling problem in the context of search-based software engineering: A systematic review","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065781168&doi=10.1016%2fj.jss.2019.05.024&partnerID=40&md5=2b45831474f938f0d09efbd4323ffd36","This work provides a systematic literature review of the software project scheduling problem, in the context of search-based software engineering, and summarizes the main models, techniques, search algorithms and evaluation criteria applied to solve this problem. We also discuss trends and research opportunities. Our keyword search found 438 papers, published in the last 20 years. After considering the inclusion and exclusion criteria and performing the snowballing procedure, we have analyzed 37 primary studies. The results show the predominance of the use of evolutionary algorithms. The static model, in which the scheduling is performed once during the project, is considered in the majority of the papers. Synthetic instances are commonly used to validate the heuristic and hypervolume and execution time are the mostly applied evaluating criteria. © 2019 Elsevier Inc.","Search-based software engineering; Software project scheduling problem; Systematic review"
"Execution allowance based fixed priority scheduling for probabilistic real-time systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062808574&doi=10.1016%2fj.jss.2019.03.001&partnerID=40&md5=7f2e58abcc0e787c141ac3ddb9a266e0","Real-time systems tend to be probabilistic in nature because of the performance variations of complex chips. We present an execution allowance based fixed priority scheduling scheme for probabilistic real-time systems. This scheme consists of a probabilistic Worst Case Execution Time reshaping algorithm and a fixed priority scheduling strategy. It assigns a specific execution allowance to each task and schedules tasks under the Rate Monotonic policy. We present a schedulability analysis and show how to determine an appropriate execution allowance for each task. Evaluation shows that our proposed scheme can significantly outperform the existing approaches. © 2019 Elsevier Inc.","Fixed priority scheduling; Probabilistic; Real-time systems"
"TSTSS: A two-stage training subset selection framework for cross version defect prediction","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064625005&doi=10.1016%2fj.jss.2019.03.027&partnerID=40&md5=97b8cc1da74232c5f5b4cb4008f434db","Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules in the current version. Unfortunately, the differences of data distribution across versions may hinder the effectiveness of the trained CVDP model. Thus, it is not trivial to select a suitable training subset from the prior version to promote the CVDP performance. In this paper, we propose a novel method, called Two-Stage Training Subset Selection (TSTSS), to address this challenging issue. In the first stage, TSTSS utilizes a sparse modeling representative selection method to select an initial module subset from the prior version which can well reconstruct the data of the prior version. In the second stage, TSTSS leverages a dissimilarity-based sparse subset selection method to further refine the selected module subset, which enables the selected modules to well represent the modules of the current version. Finally, we use a novel weighted extreme learning machine classifier to construct the CVDP model. We evaluate the CVDP performance of TSTSS on 50 cross-version pairs using 6 indicators. The experiments show that TSTSS can efficiently improve the CVDP performance compared with 11 baseline methods. © 2019","Cross version defect prediction; Spare modeling; Training subset selection; Weighted extreme learning machine"
"Scalable complex event processing using adaptive load balancing","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058516203&doi=10.1016%2fj.jss.2018.12.012&partnerID=40&md5=8a4c69df1abd95ede08636f6b4ef3067","An essential requirement of large-scale event-driven systems is the real-time detection of complex patterns of events from a large number of basic events and derivation of higher-level events using complex event processing (CEP) mechanisms. Centralized CEP mechanisms are however not scalable and thus inappropriate for large-scale domains with many input events and complex patterns, rendering the horizontal scaling of CEP mechanisms a necessity. In this paper, we propose CCEP as a mechanism for clustering of heterogeneous CEP engines to provide horizontal scalability using adaptive load balancing. We experimentally compare the performance of CCEP with the performances of three CEP clustering mechanisms, namely VISIRI, SCTXPF, and RR. The results of experiments show that CCEP increases throughput by 40 percent and thus it is more scalable than the other three chosen mechanisms when the input event rate changes at runtime. Although CCEP increases the network utilization by about 40 percent, it keeps the load of the system two times more balanced and reduces the input event loss three times. © 2018","Adaptive load balancing; CEP; Complex event processing; Horizontal scaling; Scalability"
"A semi-partitioned model for mixed criticality systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060102640&doi=10.1016%2fj.jss.2019.01.015&partnerID=40&md5=5d297cdb5b5421e4bf82ebbc3670a3e2","Many Mixed Criticality algorithms have been developed with an assumption that lower criticality-level tasks may be abandoned in order to guarantee the schedulability of higher-criticality tasks when the criticality level of the system changes. But it is valuable to explore means by which all of the tasks remain schedulable through these criticality level changes. This paper introduces a semi-partitioned model for a multi-core platform that allows all of the tasks to remain schedulable if only a bounded number of cores increase their criticality level. In such a model, some lower-criticality tasks are allowed to migrate instead of being abandoned. Detailed response time analysis for this model is derived. This paper also introduces possible approaches for establishing migration routes. Together with related previous work, an appropriate semi-partitioned model for mixed criticality systems hosted on multi-core platforms is recommended. © 2019","Mixed criticality system; Multi-core platform; Semi-partitioned approach"
"Towards understanding bugs in an open source cloud management stack: An empirical study of OpenStack software bugs","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061825667&doi=10.1016%2fj.jss.2019.02.025&partnerID=40&md5=421d848d70842c04db7c2654a3a682d7","Cloud management stack (CMS) provides convenience for organizations in managing their cloud platforms. CMS software is complex and bugs in it can cause serious damage to the cloud environment. Therefore, an in-depth understanding of CMS bugs can help developers detect and fix them. In this paper, we conduct a thorough empirical study of several key characteristics of bugs in OpenStack — the most popular open source CMS. Our study computes general statistics for about 50K OpenStack bugs, including the evolution of bugs, the distribution of bugs, and the duration of bugs. We then selected 579 bugs for an in-depth study. In particular, we study the input factors for triggering the bugs, the consequences of the bugs, and how the bugs are fixed. The findings of this study provide a set of lessons learned and guidance to aid practitioners and researchers to better handle bugs in CMS software. © 2019 Elsevier Inc.","Bug reports; Cloud management stack; Empirical study; Openstack"
"Empirical software engineering: From discipline to interdiscipline","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056722950&doi=10.1016%2fj.jss.2018.11.019&partnerID=40&md5=3e4c8523beddc25cdab39c87eeeb276f","Empirical software engineering has received much attention in recent years and coined the shift from a more design-science-driven engineering discipline to an insight-oriented, and theory-centric one. Yet, we still face many challenges, among which some increase the need for interdisciplinary research. This is especially true for the investigation of social, cultural and human-centric aspects of software engineering. Although we can already observe an increased recognition of the need for more interdisciplinary research in (empirical) software engineering, such research configurations come with challenges barely discussed from a scientific point of view. In this position paper, we critically reflect upon the epistemological setting of empirical software engineering and elaborate its configuration as an Interdiscipline. In particular, we (1) elaborate a pragmatic view on empirical research for software engineering reflecting a cyclic process for knowledge creation, (2) motivate a path towards symmetrical interdisciplinary research, and (3) adopt five rules of thumb from other interdisciplinary collaborations in our field before concluding with new emerging challenges. This supports to elevate empirical software engineering from a developing discipline moving towards a paradigmatic stage of normal science to one that configures interdisciplinary teams and research methods symmetrically. © 2018 Elsevier Inc.","Empirical software engineering; Interdisciplinary research; Science & technology studies; Symmetrical collaboration"
"Feature-oriented contract composition","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062387758&doi=10.1016%2fj.jss.2019.01.044&partnerID=40&md5=1f9f7efbfc45bc0df3673e185666d1fb","A software product line comprises a set of products that share a common code base, but vary in specific characteristics called features. Ideally, features of a product line are developed in isolation and composed subsequently. Product lines are increasingly used for safety–critical software, for which quality assurance becomes indispensable. While the verification of product lines gained considerable interest in research over the last decade, the subject of how to specify product lines is only covered rudimentarily. A challenge to overcome is composition; similar to inheritance in object-oriented programming, features of a product line may refine other features along with their specifications. To investigate how refinement and composition of specifications can be established, we derive a notion of feature-oriented contracts comprising preconditions, postconditions, and framing conditions of a method. We discuss six mechanisms to perform contract composition between original and refining contracts. Moreover, we identify and discuss desired properties for contract composition and evaluate which properties are established by which mechanism. Our three main insights are that (a) contract refinement is seldom but crucial, (b) the Liskov principle does not apply to features, and (c) it is sufficient to accommodate techniques from object-orientation in the contract-composition mechanisms for handling frame refinements. © 2019 Elsevier Inc.","Deductive verification; Design by contract; Feature-oriented programming; Formal methods; Software product lines"
"Architecting with microservices: A systematic mapping study","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060521992&doi=10.1016%2fj.jss.2019.01.001&partnerID=40&md5=95d7beacebfc79566f4a506b7a7180ba","Context: A microservice architecture is composed of a set of small services, each running in its own process and communicating with lightweight mechanisms. Many aspects on architecting with microservices are still unexplored and existing research is still far from being crispy clear. Objective: We aim at identifying, classifying, and evaluating the state of the art on architecting with microservices from the following perspectives: publication trends, focus of research, and potential for industrial adoption. Method: We apply the systematic mapping methodology. We rigorously selected 103 primary studies and we defined and applied a classification framework to them for extracting key information for subsequent analysis. We synthesized the obtained data and produced a clear overview of the state of the art. Results: This work contributes with (i) a classification framework for research studies on architecting with microservices, (ii) a systematic map of current research of the field, (iii) an evaluation of the potential for industrial adoption of research results, and (iv) a discussion of emerging findings and implications for future research. Conclusion: This study provides a solid, rigorous, and replicable picture of the state of the art on architecting with microservices. Its results can benefit both researchers and practitioners of the field. © 2019 Elsevier Inc.","Microservices; Software architecture; Systematic mapping study"
"Black-Box model-Based regression testing of fail-Safe behavior in web applications","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058956928&doi=10.1016%2fj.jss.2018.11.020&partnerID=40&md5=e73766d0f1fe5857e19469a3bfd1f008","This paper provides an approach for selective black-box model-based regression testing for web applications, emphasizing testing proper mitigation of external failures in web applications. This approach uses an existing model of web applications, FSMWeb, and extends its test generation capabilities to include selective regression testing of fail-safe behavior. It classifies existing tests as reusable, retestable, and obsolete. The approach reduces the number of tests compared to a full retest between 3% to 81% depending on the type of changes in the example and by over 99% in the case study. Removing reusable requirements reduced test requirements between 49% to 65% in the case study. The approach also uses partial regeneration for new tests wherever possible. © 2018","Black-Box testing; External failure; Model-Based testing; Regression testing; Web application"
"Empirical research for software architecture decision making: An analysis","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058851581&doi=10.1016%2fj.jss.2018.12.003&partnerID=40&md5=becf2ca54cb9aa6f5c5cea0ffaa0069d","Context: Despite past empirical research in software architecture decision making, we have not yet systematically studied how to perform such empirical research. Software architecture decision making involves humans, their behavioral issues and practice. As such, research on decision making needs to involve not only engineering but also social science research methods. Objective: This paper studies empirical research on software architecture decision making. We want to understand what research methods have been used to study human decision making in software architecture. Further, we want to provide guidance for future studies. Method: We analyzed research papers on software architecture decision making. We classified the papers according to different sub-dimensions of empirical research design like research logic, research purpose, research methodology and process. We introduce the study focus matrix and the research cycle to capture the focus and the goals of a software architecture decision making study. We identify gaps in current software architecture decision making research according to the classification and discuss open research issues inspired by social science research. Conclusion: We show the variety of research designs and identify gaps with respect to focus and goals. Few papers study decision making behavior in software architecture design. Also these researchers study mostly the process and much less the outcome and the factors influencing decision making. Furthermore, there is a lack of improvements for software architecture decision making and in particular insights into behavior have not led to new practices. The study focus matrix and the research cycle are two new instruments for researchers to position their research clearly. This paper provides a retrospective for the community and an entry point for new researchers to design empirical studies that embrace the human role in software architecture decision making. © 2018 Elsevier Inc.","Decision making; Empirical research; Human aspects; Software architecture"
"DELDROID: An automated approach for determination and enforcement of least-privilege architecture in android","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057263674&doi=10.1016%2fj.jss.2018.11.049&partnerID=40&md5=81ece8e23dbc3ed0d1ea7fe0f4dcd475","Android is widely used for the development and deployment of autonomous and smart systems, including software targeted for IoT and mobile devices. Security of such systems is an increasingly important concern. Android relies on a permission model to secure the system's resources and apps. In Android, since the permissions are granted at the granularity of apps, and all components in an app inherit those permissions, an app's components are over-privileged, i.e., components are granted more privileges than they actually need. Systematic violation of least-privilege principle in Android is the root cause of many security vulnerabilities. To mitigate this issue, we have developed DELDROID, an automated system for determination of least privilege architecture in Android and its enforcement at runtime. A key contribution of DELDROID is the ability to limit the privileges granted to apps without modifying them. DELDROID utilizes static analysis techniques to extract the exact privileges each component needs. A Multiple-Domain Matrix representation of the system's architecture is then used to automatically analyze the security posture of the system and derive its least-privilege architecture. Our experiments on hundreds of real-world apps corroborate DELDROID's ability in effectively establishing the least-privilege architecture and its benefits in alleviating the security threats. © 2018","Android security; Multiple-Domain-Matrix (MDM); Software architecture"
"A comparison and evaluation of variants in the coupling between objects metric","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061400563&doi=10.1016%2fj.jss.2019.02.020&partnerID=40&md5=0d405c6a63e6f2d0398b03973731c036","The Coupling Between Objects metric (CBO) is a widely-used metric but, in practice, ambiguities in its correct implementation have led to different values being computed by different metric tools and studies. CBO has often been shown to correlate with defect occurrence in software systems, but the use of different calculations is commonly overlooked. This paper investigates the varying interpretations of CBO used by those metrics tools and researchers and defines a set of metrics representing the different computational approaches used. These metrics are calculated for a large-scale Java system and logistic regression used to correlate them with defect data obtained by analysing the system's version tracking records. The different variations of CBO are shown to have significantly different correlations to defects. Regarding results, a clear binary divide was found between CBO values which, on the one hand, predicted a defect and, on the other, those that did not. The results, therefore, show that a clarification or unambiguous re-definition of CBO is both desirable and essential for a general consensus on its use. Moreover, applications of the metric must pay close attention to the actual method of calculation being used and, conclusions and comparisons made as a result. © 2019","Comparison; Coupling; Empirical; Software metrics"
"On Haskell and energy efficiency","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059519783&doi=10.1016%2fj.jss.2018.12.014&partnerID=40&md5=491529a780ff5950f343649eebd50878","Background: Recent work has studied diverse affecting factors on software energy efficiency. Objective: This paper attempts to shed light on the energy behavior of programs written in a lazy, purely functional programming language, Haskell. Methodology: We conducted two in-depth and complementary studies to analyze the energy efficiency of programs from two different perspectives: strictness and concurrency. Results: We found that small changes can make a big difference. In one benchmark, under a specific configuration, choosing the MVar data sharing primitive over TMVar can yield 60% energy savings. In another benchmark, TMVar can yield up to 30% savings over MVar. Thus, tools that support developers in refactoring a program to switch between primitives can be very useful. In addition, the relationship between energy consumption and performance is not always clear. In sequential benchmarks, high performance is an accurate proxy for low energy consumption. However, for one of our concurrent benchmarks, the variants with the best performance also exhibited the worst energy consumption. We report on deviating cases. Conclusions: To support developers, we have extended existing performance analysis tools to also gather and present data about energy consumption. Furthermore, we provide a set of guidelines to help Haskell developers save energy. © 2018 Elsevier Inc.","Energy efficiency; Functional programming; Haskell"
"The Robot Operating System: Package reuse and community dynamics","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061837357&doi=10.1016%2fj.jss.2019.02.024&partnerID=40&md5=08eb31290f04c42cebe9d5455a21822d","ROS, the Robot Operating System, offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to write robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. Like any software ecosystem, ROS must evolve in order to keep meeting the requirements of its users. In practice, packages may end up being abandoned between releases: no one may be available to update a package, or newer packages offer similar functionality. As such, we wanted to identify and understand the evolution challenges faced by the ROS ecosystem. In this article, we report our findings after interviewing 19 ROS developers in depth, followed by a focus group (4 participants) and an online survey of 119 ROS community members. We specifically focused on the issues surrounding package reuse and how to contribute to existing packages. To conclude, we discuss the implications of our findings, and propose five recommendations for overcoming the identified issues, with the goal of improving the health of the ROS ecosystem. © 2019 Elsevier Inc.","Package management; Robot Operating System; Software ecosystems"
"Alone or Together? Inter-organizational affiliations of open source communities","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058217527&doi=10.1016%2fj.jss.2018.12.007&partnerID=40&md5=2493ccb1189a4abc53a479240f2ca534","Many of today's open source software (OSS) communities operate beneath an umbrella organization, while others are organized entirely independently, and yet others follow a strategy somewhere in between, sharing certain resources and services. In our paper, we analyze four mature OSS communities (GENIVI, PolarSys, LibreOffice and PostgreSQL) representing different organizational forms. Our qualitative case studies illustrate that OSS communities preferring to control all of their resources are organized autonomously, while those focused mainly on software development are integrated into an umbrella organization. An interjacent strategy is pursued by OSS communities affiliated with an intermediary form of organization that takes care of legal and financial issues, without prescribing organizational structures or a specific license. The findings of our case studies show that there is no one-size-fits-all approach for OSS communities and each strategy has specific advantages and disadvantages. Arguing with the theoretical concepts of Resource Dependence Theory (RDT) and Transaction Cost Economics (TCE), we are able to relate the findings of our qualitative empirical study to theoretical concepts explaining different organizational behavior. Therefore, this study contributes new insights concerning the inter-organizational affiliations of OSS communities thus responding to the question why different forms of OSS community governance exist. © 2018 Elsevier Inc.","Collaborative software development; Open source software; OSS foundations; OSS governance"
"Efficient runtime metaprogramming services for Java","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064150258&doi=10.1016%2fj.jss.2019.04.030&partnerID=40&md5=6062a871cf80ffd75c4c39e5e54f5848","The Java programming language and platform provide many optimizations to execute statically typed code efficiently. Although Java has gradually incorporated more dynamic features across its versions, it does not provide several metaprogramming features supported by most dynamic languages, such as structural intercession (the ability to dynamically modify the structure of classes) and dynamic code generation. Therefore, we propose a method to add those metaprogramming features to Java in order to increase its runtime adaptiveness, while taking advantage of the robustness of its static type system and the performance of its virtual machine. We support the dynamic addition, deletion and replacement of class methods and fields, and dynamic code generation. The metaprogramming services are provided as a library, so neither the Java language nor its virtual machine are modified. We evaluate our system, called JMPLib, and compare it with the existing metaprogramming systems for the Java platform and other highly optimized dynamic languages. JMPLib obtains similar runtime performance to the existing fastest system that modifies the implementation of the Java virtual machine. Moreover, our system introduces no performance penalty when metaprogramming is not used, and consumes fewer memory resources than the rest of implementations for the Java platform. © 2019 Elsevier Inc.","Code instrumentation; Dynamic code evaluation; Metaprogramming; Runtime performance; Static type checking; Structural intercession"
"Hierarchical scheduling of real-time tasks over Linux-based virtual machines","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058182399&doi=10.1016%2fj.jss.2018.12.008&partnerID=40&md5=3ba13e6b5493dad60cea753011b83bc5","Virtualization has made feasible the full isolation of virtual machines (VMs) among each other. When applications running within VMs have real-time constraints, threads implementing the virtual cores must be scheduled in a predictable manner over the physical cores. In this paper, we propose a possible implementation of such a predictable VM scheduling based on Linux and kvm (a hosted hypervisor). The proposed implementation is based on vanilla Linux kernels and standard qemu/kvm, and does not require to apply any patch or to use custom software. We also show that previous work makes some assumptions that are unrealistic in practical situations. Motivated by these considerations, we finally propose a principled methodology to practically implement hierarchical scheduling with Linux. Finally, an extensive set of experiments based on Linux and kvm illustrates how the VMs and host scheduler can be set-up to match theoretical results with experiments. © 2018 Elsevier Inc.","Hierarchical scheduling; Kvm; Linux; Real-time; Virtual machines"
"Integrating UX principles and practices into software development organizations: A case study of influencing events","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064314342&doi=10.1016%2fj.jss.2019.03.066&partnerID=40&md5=98e2aca40b9972108f35a8b25f1e9506","Current studies on User eXperience (UX) integration often do not investigate or reflect on the transition companies go through from only developing Graphical User Interfaces (GUI) to also considering usability and more recently UX. Understanding this transition provides a more holistic and realistic picture of integration and can be a rich source of knowledge for improving UX integration in the software industry. Applying case study and grounded theory research we show that UX integration, like other organizational changes, can include a mixture of planned and emergent initiatives, and is influenced by various intertwined events; not only those that reside inside an organization but also those external to it. We also show that different decisions that are made outside the authority of UX practitioners have an inevitable impact on enabling or prohibiting UX integration. In addition, we found that for a successful integration, practitioners need to explicitly consider and address the characteristics of UX, otherwise, the integration efforts may have a lopsided focus on the pragmatic aspect of UX, consequently, leave the hedonic aspect unaddressed. Based on our findings, we present four lessons learned and five pitfalls companies should consider to go beyond GUI design and usability to also address UX. © 2019","Case study; Grounded theory; Organizational change; Software quality; Usability; User eXperience (UX)"
"Touch gesture-based authentication on mobile devices: The effects of user posture, device size, configuration, and inter-session variability","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057590159&doi=10.1016%2fj.jss.2018.11.017&partnerID=40&md5=c08a05c47a5d43a33f5e3f3d9d0ada32","Touch dynamics is a behavioral biometric that authenticates users by analyzing the characteristics of the touch gestures executed on mobile devices. Current research in this field has mostly focused on identifying the best algorithms and attributes to improve authentication performance. However, such systems must also be resilient against environmental variables. In this paper, we demonstrate that the user's posture, device size and configuration have a significant impact on the performance of touch-based authentication systems. Our results indicate that authentication accuracy increases with the device size. Furthermore, we conclude that using a device's 3-D orientation is necessary to attain better authentication performance. Our findings indicate that the features used in state-of-the-art touch-based authentication systems are insufficient to provide constant, reliable performance when any of the studied environmental variables change. With this paper, we release a new data set. Unlike the currently publicly available touch-based authentication datasets, our collection protocols control for all the studied variables. Our research study demonstrates threats to validity that noisy environmental conditions introduce to these currently available public datasets. This work is an extension of a previous publication. Presented user authentication approaches are unique and may have immediate benefits to the development of better touch-based authentication systems. © 2018","Behavioral biometrics; Continual authentication; Gesture based authentication; Mobile security; Soft biometrics; Touch based authentication"
"A systematic mapping study on higher order mutation testing","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067995509&doi=10.1016%2fj.jss.2019.04.031&partnerID=40&md5=ac00782b83cbec90e8f28ab45b034398","Context: Higher Order Mutants (HOMs) present some advantages concerning the First-Order Mutants (FOMs). HOMs can better simulate real and subtle faults, reduce the number of generated mutants and test cases, and so on. Objective: In order to characterize the Higher Order Mutation Testing (HOMT) field, this paper presents results of a mapping study, by synthesizing characteristics of the HOMT approaches, HOM generation strategies, evaluation aspects, trends and research opportunities. Method: We followed a research plan to locate, assess, extract and group the outcomes from relevant studies. We found 69 primary studies, which were classified based on dimensions related to aspects of the conducted evaluation, purpose and use of HOMs. Results: Java is the preferred language. Most approaches use Second-Order Mutants (SOMs). We found 50 different techniques used to generate/select HOMs. We observed that from 39 primary studies which apply a strategy, ≈49% use search-based techniques. Conclusions: HOMT has been arising interest in the last years. The results herein presented provide researchers the start-of-the-art on HOMT, allowing them to understand existing approaches, and how the HOMs have been used and evaluated. Furthermore, this paper points out open issues and not addressed topics, which require more investigation, discussing trends and research opportunities in the field. © 2019 Elsevier Inc.","Higher order mutation; Mutation testing; Systematic mapping"
"Not all bugs are the same: Understanding, characterizing, and classifying bug types","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063036021&doi=10.1016%2fj.jss.2019.03.002&partnerID=40&md5=bf0a4ddf12a99d772131db3f486211b4","Modern version control systems, e.g., GitHub, include bug tracking mechanisms that developers can use to highlight the presence of bugs. This is done by means of bug reports, i.e., textual descriptions reporting the problem and the steps that led to a failure. In past and recent years, the research community deeply investigated methods for easing bug triage, that is, the process of assigning the fixing of a reported bug to the most qualified developer. Nevertheless, only a few studies have reported on how to support developers in the process of understanding the type of a reported bug, which is the first and most time-consuming step to perform before assigning a bug-fix operation. In this paper, we target this problem in two ways: first, we analyze 1280 bug reports of 119 popular projects belonging to three ecosystems such as MOZILLA, APACHE, and ECLIPSE, with the aim of building a taxonomy of the types of reported bugs; then, we devise and evaluate an automated classification model able to classify reported bugs according to the defined taxonomy. As a result, we found nine main common bug types over the considered systems. Moreover, our model achieves high F-Measure and AUC-ROC (64% and 74% on overall, respectively). © 2019 Elsevier Inc.","Bug classification; Empirical study; Taxonomy"
"Unsupervised learning approach for web application auto-decomposition into microservices","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061817610&doi=10.1016%2fj.jss.2019.02.031&partnerID=40&md5=57f8990e608a80f88f109ad1fe841ce1","Nowadays, large monolithic web applications are manually decomposed into microservices for many reasons including adopting a modern architecture to ease maintenance and increase reusability. However, the existing approaches to refactor a monolithic application do not inherently consider the application scalability and performance. We devise a novel method to automatically decompose a monolithic application into microservices to improve the application scalability and performance. Our proposed decomposition method is based on a black-box approach that uses the application access logs and an unsupervised machine-learning method to auto-decompose the application into microservices mapped to URL partitions having similar performance and resource requirements. In particular, we propose a complete automated system to decompose an application into microservices, deploy the microservices using appropriate resources, and auto-scale the microservices to maintain the desired response time. We evaluate the proposed system using real web applications on a public cloud infrastructure. The experimental evaluation shows an improved performance of the auto-created microservices compared with the monolithic version of the application and the manually created microservices. © 2019","Application decomposition; Cloud computing; Microservices; Scalability; Web applications"
"Model transformation for analyzing dependability of AADL model by using HiP-HOPS","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061978067&doi=10.1016%2fj.jss.2019.02.019&partnerID=40&md5=6ca5c6a8159f4df5ae4cde66eda47ba0","The Architecture Analysis and Design Language (AADL) has emerged as a potential future standard in aerospace, automobile and avionics industries for model-based development of dependability-critical systems. As AADL is relatively new, some existing analysis methods and tools are not able to accept AADL models. In this paper we show that, by using model transformation techniques, we can automatically transform AADL models into a form that is directly executable by fault-tree-based dependability analysis and optimisation tools. This model transformation opens a path by which AADL models may benefit from automatic synthesis and analysis of fault trees, temporal fault tree analysis, multiple failure mode and effects analysis and model architecture optimisation. In this paper, we present a new model transformation framework. The core of the framework is a novel transformation from a state machine-based error model to a fault-tree model. The framework has been implemented as a plug-in (AADL2HiP-HOPS) for the AADL model development tool OSATE. The plug-in may be used to transform AADL models into a state-of-the-art dependability analysis and optimisation tool: HiP-HOPS. To illustrate the transformation and subsequent HiP-HOPS analysis, an example AADL model is transformed. © 2019 Elsevier Inc.","AADL; Dependability analysis; Dependability modeling; HiP-HOPS; Model transformation"
"Sentiment based approval prediction for enhancement reports","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065795226&doi=10.1016%2fj.jss.2019.05.026&partnerID=40&md5=5b01d3e5c431de06f8fcfa641ea30158","The maintenance and evolution of the software application is a continuous phase in the industry. Users are frequently proposing enhancement requests for further functionalities. However, although only a small part of these requests are finally adopted, developers have to go through all of such requests manually, which is tedious and time consuming. To this end, in this paper we propose a sentiment based approach to predict how likely enhancement reports would be approved or rejected so that developers can first handle likely-to-be-approved requests. This could help the software applications to compete in the industry by upgrading their features in time as per user's requirements. First, we preprocess enhancement reports using natural language preprocessing techniques. Second, we identify the words having positive and negative sentiments in the summary attribute of the enhancements reports and calculate the sentiment of each enhancement report. Finally, with the history data of real software application, we train a machine learning based classifier to predict whether a given enhancement report would be approved. The proposed approach has been evaluated with the history data from real software applications. The cross-application validation suggests that the proposed approach outperforms the state-of-the-art. The evaluation results suggest that the proposed approach increases the accuracy from 70.94% to 77.90% and improves the F-measure significantly from 48.50% to 74.53%. © 2019","Classification; Enhancement reports; Machine learning algorithms"
"Communication channels in safety analysis: An industrial exploratory case study","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063969297&doi=10.1016%2fj.jss.2019.04.004&partnerID=40&md5=94ce205483363cba490e8da1e98ff2c9","Context: Safety analysis is a predominant activity in developing safety-critical systems. It is a highly cooperative task among multiple functional departments due to increasingly sophisticated safety-critical systems and close-knit development processes. Communication occurs pervasively. Motivation: Effective communication channels among multiple functional departments influence safety analysis quality as well as a safe product delivery. However, the use of communication channels during safety analysis is sometimes arbitrary and poses challenges. Objective: In this article, we aim to investigate the existing communication channels, their usage frequencies, their purposes and challenges during safety analysis in industry. Method: We conducted a multiple case study by surveying 39 experts and interviewing 21 experts in safety-critical companies including software developers, quality engineers and functional safety managers. Direct observations and documentation review were also conducted. Results: Popular communication channels during safety analysis include formal meetings, project coordination tools, documentation and telephone. Email, personal discussion, training, internal communication software and boards are also in use. Training involving safety analysis happens 1-4 times per year, while other aforementioned communication channels happen ranging from 1-4 times per day to 1-4 times per month. We summarise 28 purposes of using these aforementioned communication channels. Communication happens mostly for the purpose of clarifying safety requirements, fixing temporary problems, conflicts and obstacles and sharing safety knowledge. The top 10 challenges are: (1) sensitiveness and confidentiality of safety analysis information; (2) fragmented safety analysis information; (3) inconsistent safety analysis information; (4) asynchronous channels; (5) a lack of tool support; (6) misunderstanding between developers and safety analysts; (7) language, geographic and culture limitations; (8) unwillingness to communicate (groupthink); (9) storage, authority, regulation and monitoring of safety analysis information; (10) a lack of documentation concerning safety analysis to support communication. Conclusion: During safety analysis, to use communication channels effectively and avoid challenges, a clear purpose of communication during safety analysis should be established at the beginning. We have limitations primarily on the research context namely the scope of domains, participants and countries. To derive countermeasures of fixing the top 10 challenges are potential next steps. © 2019 Elsevier Inc.","Case study; Challenges; Communication; Purposes; Safety analysis; Safety-critical systems"
"Automatically detecting the scopes of source code comments","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063458121&doi=10.1016%2fj.jss.2019.03.010&partnerID=40&md5=3b43cc844f31b4717ec2be757349a1b3","Comments convey useful information about the system functionalities and many methods for software engineering tasks take comments as an important source for many software engineering tasks such as code semantic analysis, code reuse and so on. However, unlike structural doc comments, it is challenging to identify the relationship between the functional semantics of the code and its corresponding textual descriptions nested inside the code and apply it to automatic analyzing and mining approaches in software engineering tasks efficiently. In this paper, we propose a general method for the detection of source code comment scopes. Based on machine learning, our method utilized features of code snippets and comments to detect the scopes of source code comments automatically in Java programs. On the dataset of comment-statement pairs from 4 popular open source projects, our method achieved a high accuracy of 81.45% in detecting the scopes of comments. Furthermore, the results demonstrated the feasibility and effectiveness of our comment scope detection method on new projects. Moreover, our method was applied to two specific software engineering tasks in our studies: analyzing software repositories for outdated comment detection and mining software repositories for comment generation. As a general approach, our method provided a solution to comment-code mapping. It improved the performance of baseline methods in both tasks, which demonstrated that our method is conducive to automatic analyzing and mining approaches on software repositories. © 2019 The Authors","Comment scope detection; Machine learning; Software repositories"
"Safety for mobile robotic system: A systematic mapping study from a software engineering perspective","2019","Journal of Systems and Software","10.1016/j.jss.2019.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061605341&doi=10.1016%2fj.jss.2019.02.021&partnerID=40&md5=829d5ad328d7a436167b42c4db5f9661","Robotic research is making huge progress. However, existing solutions are facing a number of challenges preventing them from being used in our everyday tasks: (i) robots operate in unknown environments, (ii) robots collaborate with each other and even with humans, and (iii) robots shall never injure people or create damages. Researchers are targeting those challenges from various perspectives, producing a fragmented research landscape. We aim at providing a comprehensive and replicable picture of the state of the art from a software engineering perspective on existing solutions aiming at managing safety for mobile robotic systems. We apply the systematic mapping methodology on an initial set of 1274 potentially relevant research papers, we selected 58 primary studies and analyzed them according to a systematically-defined classification framework. This work contributes with (i) a classification framework for methods or techniques for managing safety when dealing with the software of mobile robotic systems (MSRs), (ii) a map of current software methods or techniques for software safety for MRSs, (iii) an elaboration on emerging challenges and implications for future research, and (iv) a replication package for independent replication and verification of this study. Our results confirm that generally existing solutions are not yet ready to be used in everyday life. There is the need of turn-key solutions ready to deal with all the challenges mentioned above. © 2019 Elsevier Inc.","Safety for mobile robots; Software; Systematic mapping study"
"File-level socio-technical congruence and its relationship with bug proneness in OSS projects","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066732591&doi=10.1016%2fj.jss.2019.05.030&partnerID=40&md5=8220227e37332672cd5e08a9c90f02b4","Coordination is important in software development. Socio-Technical Congruence (STC) is proposed to measure the match between coordination requirements and actual coordination activities. The previous work of Cataldo et al. computes STC in commercial projects and finds it related to software failures. In this paper, we study the relationship between file-level STC and bug proneness in Open Source Software (OSS) projects. We apply the fundamental STC framework to the OSS data setting and present a method of computing file-level STC based on our available data. We also propose a derivative STC metric called Missing Developer Links (MDL), which is to measure the amount of coordination breakdowns. In our empirical analysis on five OSS projects, we find that MDL is more related to bug proneness than STC. Furthermore, STC or MDL can be computed based on different types of file networks and developer networks, and we find out the best file network and the best developer network via an empirical study. We also evaluate the usefulness of STC or MDL metrics in bug prediction. This work is promising to help detect coordination issues in OSS projects. © 2019 Elsevier Inc.","Coordination breakdown; Developer network; Open source software; Socio-technical congruence; Software quality"
"Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061379394&doi=10.1016%2fj.jss.2019.01.051&partnerID=40&md5=4622948460196b32b981ac9ca67c43a7","Context: Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks. Objective: We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems. Method: We used Systematic Literature Review (SLR) method for reviewing 74 papers. Result: Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each. Conclusion: Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML. © 2019 Elsevier Inc.","Architectural tactic; Big data; Cybersecurity; Quality attribute"
"Model based system assurance using the structured assurance case metamodel","2019","Journal of Systems and Software","10.1016/j.jss.2019.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065495606&doi=10.1016%2fj.jss.2019.05.013&partnerID=40&md5=ed7496d2ecfa475dc9892588ff9db017","Assurance cases are used to demonstrate confidence in system properties of interest (e.g. safety and/or security). A number of system assurance approaches are adopted by industries in the safety-critical domain. However, the task of constructing assurance cases remains a manual, lenghty and informal process. The Structured Assurance Case Metamodel (SACM)is a standard specified by the Object Management Group (OMG). SACM provides a richer set of features than existing system assurance languages/approaches. SACM provides a foundation for model-based system assurance, which bears great application potentials in growing technology domains such as Open Adaptive Systems. However, the intended usage of SACM has not been sufficiently explained. In addition, there has not been support to interoperate between existing assurance case (models)and SACM models. In this article, we explain the intended usage of SACM based on our involvement in the OMG specification process of SACM. In addition, to promote a model-based approach, we provide SACM compliant metamodels for existing system assurance approaches (the Goal Structuring Notation and Claims-Arguments-Evidence), and the transformations from these models to SACM. We also briefly discuss the tool support for model-based system assurance which helps practitioners make the transition from existing system assurance approaches to model-based system assurance using SACM. © 2019 Elsevier Inc.","Claims-Arguments-Evidence; Goal structuring notation; Model based system assurance; Model driven engineering; Structured assurance case metamodel"
"Continuously analyzing finite, message-driven, time-synchronous component & connector systems during architecture evolution","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058783827&doi=10.1016%2fj.jss.2018.12.016&partnerID=40&md5=1c17c794269d94f847e0e6d407d85fcb","Understanding the semantic differences of continuously evolving system architectures by semantic analyses facilitates engineers during evolution analysis in understanding the impact of the syntactical changes between two architecture versions. To enable effective semantic differencing usable in practice, this requires means to fully automatically check whether one version of a system admits behaviors that are not possible in another version. Previous work produced very general system models for message-driven time-synchronous (MDTS) systems that impede fully automated semantic differencing but very adequately describe such systems from a black-box viewpoint abstracting from hidden internal component behavior. This paper presents a system model for MDTS systems from a white-box viewpoint (assuming component implementation availability) and presents a sound and complete method for semantic differencing of finite MDTS system architectures. This method relies on representing (sub-)architectures as channel automata and a reduction from the semantic differencing problem for such automata to the language inclusion problem for Büchi automata. The system model perfectly captures the logical basics of MDTS systems from a white-box viewpoint and the method enables to fully automatically calculate semantic differences between two finite MDTS systems on push-button basis, yields witnesses, and ultimately facilitates semantic evolution analysis of such systems. © 2018 Elsevier Inc.","Automata; Component Software Engineering; Evolution Analysis; Refinement; Semantic Differencing; Semantics"
"A weighted fuzzy classification approach to identify and manipulate coincidental correct test cases for fault localization","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060565751&doi=10.1016%2fj.jss.2019.01.056&partnerID=40&md5=4a8a00ee204e7b7bd0578ee8431f0dac","Identifying the location of faults effectively and accurately is highly important in the debugging process of software engineering. Coverage-based Fault Localization (CBFL) has been widely studied that can alleviate the effort of developers to find the faults position using the execution information of test cases. Coincidental Correct (CC) test cases are the specific test cases that execute the faulty statements but with a correct output, which have been illustrated with a negative effect on the accuracy of CBFL. In this paper, we propose a weighted fuzzy classification approach to identify CC test cases and three fuzzy strategies are suggested to manipulate CC test cases for CBFL. Firstly, we present a simple but efficient approach to identify some CC test cases for single fault programs, which provide labeled samples that enable the application of supervised classification algorithms for CC identification. Then, a Fuzzy Weighted K-Nearest Neighbor (FW-KNN) algorithm is proposed to classify potential CC from the passed test cases, in which a ‘weighted’ similarity measure and a “weighted” CC probability computation are presented. Finally, three fuzzy CC test cases manipulation strategies are presented to mitigate the impact of CC test cases in CBFL. Various empirical studies are conducted on 190 faulty versions of 12 programs to investigate the impact of “weighted” and “fuzzy” methods for CC identification by the comparison of the effectiveness and efficiency between FW-KNN and three popular cluster and classification techniques. The results indicate that the proposed FW-KNN has higher accuracy and lower time cost. The Precision, Recall and False Positive Rate of FW-KNN is 96.47%, 83.40% and 2.85%, respectively. Besides, by utilizing code block coverage, the time cost can be reduced by 72.97% in average compared to statement coverage. The experimental results also indicate that the fault localization accuracy of CBFL can be improved by the proposed CC test cases manipulation strategies. © 2019 Elsevier Inc.","Coincidental correct test cases; Coverage-Based fault localization; Fuzzy classification; K-Nearest Neighbor; Software debugging"
"Peak resource analysis of concurrent distributed systems","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057086619&doi=10.1016%2fj.jss.2018.11.018&partnerID=40&md5=47a57fd769fb8e9d8df4c6b0ee601fa0","Traditional cost analysis frameworks have been defined for cumulative resources which keep on increasing along the computation. Examples of cumulative resources are execution time, number of executed steps, and energy consumption. Non-cumulative resources are acquired and (possibly) released along the execution. Examples of non-cumulative cost are number of connections established that are later closed, or resources requested to a virtual host which are released after using them. We present a static analysis framework to infer the peak cost for non-cumulative types of resources in concurrent distributed systems. Our framework is generic w.r.t. the type of resource and can be instantiated with any of the above mentioned resources as well as with any other resource that is measurable by inspecting the instructions of the program. The concurrent distributed language that we consider allows creating distributed locations dynamically within the program and spawning tasks that execute concurrently at any of the existing locations. Our analysis infers, for the different distributed locations, the maximum (or peak) amount of resources that they may require along any execution. This information can be used, among other applications, to detect bottlenecks in the system and to efficiently dimension the processing capacity and storage that the locations of the concurrent distributed system require. © 2018 Elsevier Inc.","Distributed systems; Resource analysis; Static analysis"
"An empirical study on pareto based multi-objective feature selection for software defect prediction","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063027166&doi=10.1016%2fj.jss.2019.03.012&partnerID=40&md5=5c01431e5c1513393fc635993a5c8750","The performance of software defect prediction (SDP) models depend on the quality of considered software features. Redundant features and irrelevant features may reduce the performance of the constructed models, which require feature selection methods to identify and remove them. Previous studies mostly treat feature selection as a single objective optimization problem, and multi-objective feature selection for SDP has not been thoroughly investigated. In this paper, we propose a novel method MOFES (Multi-Objective FEature Selection), which takes two optimization objectives into account. One optimization objective is to minimize the number of selected features, this objective is related to the cost analysis of this problem. Another objective is to maximize the performance of the constructed SDP models, this objective is related to the benefit analysis of this problem. MOFES utilizes Pareto based multi-objective optimization algorithms (PMAs) to solve this problem. In our empirical study, we design and conduct experiments on RELINK and PROMISE datasets, which are gathered from real open source projects. Firstly, we analyze the influence of different PMAs on MOFES and find that NSGA-II can achieve the best performance on both datasets. Then, we compare MOFES method with 22 state-of-the-art filter based and wrapper based feature selection methods, and find that MOFES can effectively select fewer but closely related features to construct high-quality models. Moreover, we also analyze the frequently selected features by MOFES, and these findings can be used to provide guidelines on gathering high-quality SDP datasets. Finally, we analyze the computational cost of MOFES and find that MOFES only needs 107 seconds on average. © 2019 Elsevier Inc.","Empirical study; Feature selection; Multi-Objective optimization; Search based software engineering; Software defect prediction"
"Mobile user behavior based topology formation and optimization in ad hoc mobile cloud","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056594140&doi=10.1016%2fj.jss.2018.11.005&partnerID=40&md5=9013dd4350d88fa015dbf2d9946df9e9","As more and more mobile users use social networks to interact with other people currently, the resource limitation of mobile devices becomes an urgent problem to be solved. However, being a novel type of mobile cloud, the ad hoc mobile cloud allows participating mobile devices to share resources with other neighboring mobile devices, which can help the mobile device process a large amount of compute-intensive applications and conserve energy. In this paper, a mobile user behavior based topology formation and optimization in ad hoc mobile cloud is proposed. The mobile device nodes having similar behavioral features are grouped and formed as an ad hoc mobile cloud, which can reduce network delay and improve efficiency of node interaction. Subsequently, a flower pollination based offloading strategy is presented to reduce response time and save energy consumption. The experiment results show that our proposed ad hoc mobile cloud topology outperforms in scalability, while the offloading algorithm can reduce approximately 25%–50% response time and energy consumption as compared with that of other benchmark algorithms. © 2018 Elsevier Inc.","Ad hoc mobile cloud; Mobile user behavior; Topology formation"
"Achieving change requirements of feature models by an evolutionary approach","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060250135&doi=10.1016%2fj.jss.2019.01.045&partnerID=40&md5=752b8443cb575254c5c404702e32bd22","Feature models are a widely used modeling notation for variability and commonality management in software product line (SPL) engineering. In order to keep an SPL and its feature model aligned, feature models must be changed by including/excluding new features and products, either because faults in the model are found or to reflect the normal evolution of the SPL. The modification of the feature model to be made to satisfy these change requirements can be complex and error-prone. In this paper, we present a method that is able to automatically update a feature model in order to satisfy a given update request. The method is based on an evolutionary algorithm that iteratively applies structure-preserving mutations to the original model, until the model is completely updated or some other termination condition occurs. Among all the possible models achieving the update request, the method privileges those structurally simpler. We evaluate the approach on real-world feature models; although it does not guarantee to completely update all the possible feature models, empirical analysis shows that, on average, around 89% of requested changes are applied. © 2019 Elsevier Inc.","Evolutionary approach; Feature model; Mutation; Software product line; Update request"
"What are the factors affecting the handover process in open source development?","2019","Journal of Systems and Software","10.1016/j.jss.2019.03.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064261296&doi=10.1016%2fj.jss.2019.03.063&partnerID=40&md5=c93c895c5012e6a11a0242dcff3c4f7d","                             Background: Handover is common in open source software (OSS) development, which could have a negative impact on software quality and progress. Objective: We aim to identify factors associated with the handover process for future improvements. Method: We first propose a metric, i.e. Active Days Coverage (abbr. ADC) together with an algorithm, i.e., Handover Duration Identification (abbr. HDI) to identify the handover processes in open software projects automatically. To evaluate our method, we selected two sample sets (i.e. sample set A and sample set B) from Github. With the sample set A, an automatic identification (to identify possible handover processes) together with an email inquiry (to identify actual handover processes) have been conducted. With the sample set B, we analyze fourteen potential factors impacting a handover process using the stepwise regression method. Results: The precision, recall and accuracy of our identification method reach 0.67, 0.73 and 0.65 respectively. The rate of correct identification of HDI algorithm is over 0.5 on average. Six factors were identified as the major factors impacting a handover process as well as seven combinations of these six factors were tested by stepwise regression method to explore possible correlation with the corresponding handover duration, among which five combinations show R                             2                              greater than 0.4 with one reaches 0.493. Conclusion: This study implies that handover can be identified automatically. Developers usually follow a common handover process under various context. Moreover, although a significant correlation between the duration of a handover process and the combination of certain factors could be observed, there is no single factor that has a significant correlation with the duration of a handover process.                          © 2019 Elsevier Inc.","Handover; Open source software development; Repository mining; Software process"
"An empirical study on decision making for quality requirements","2019","Journal of Systems and Software","10.1016/j.jss.2018.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058167239&doi=10.1016%2fj.jss.2018.12.002&partnerID=40&md5=02fae7383f85258ab071122b42a2774f","Context: Quality requirements are important for product success yet often handled poorly. The problems with scope decision lead to delayed handling and an unbalanced scope. Objective: This study characterizes the scope decision process to understand influencing factors and properties affecting the scope decision of quality requirements. Method: We studied one company's scope decision process over a period of five years. We analyzed the decisions artifacts and interviewed experienced engineers involved in the scope decision process. Results: Features addressing quality aspects explicitly are a minor part (4.41%) of all features handled. The phase of the product line seems to influence the prevalence and acceptance rate of quality features. Lastly, relying on external stakeholders and upfront analysis seems to lead to long lead-times and an insufficient quality requirements scope. Conclusions: There is a need to make quality mode explicit in the scope decision process. We propose a scope decision process at a strategic level and a tactical level. The former to address long-term planning and the latter to cater for a speedy process. Furthermore, we believe it is key to balance the stakeholder input with feedback from usage and market in a more direct way than through a long plan-driven process. © 2018 Elsevier Inc.","Non-functional requirements; Product management; Quality requirements; Requirements engineering; Requirements scope decision"
"Improved schedulability analysis of the contention-free policy for real-time systems","2019","Journal of Systems and Software","10.1016/j.jss.2019.04.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064882934&doi=10.1016%2fj.jss.2019.04.067&partnerID=40&md5=91d528980db9fd5771d07e151af49ff2","Real-time scheduling is the primary research for designing real-time systems whose correctness is determined by not only logical correctness but also timely execution. Real-time scheduling involves two fundamental issues: scheduling algorithm design and schedulability analysis development, which aim at developing a prioritization policy for real-time tasks and offering their timing guarantees at design time, respectively. Among the numerous scheduling algorithms and schedulability analysis for a multiprocessor platform, the contention-free (CF)policy and response-time analysis (RTA)have received considerable attention owing to their wide applicability and high analytical performance, respectively. Notwithstanding their effectiveness, it has been conjectured that it is not feasible to exploit the two techniques together. In this study, we propose a new schedulability analysis for the CF policy, referred to as pseudo-response time analysis (PRTA), which exploits a new notion of pseudo-response time effectively capturing the time instant at which the schedulability of a task is guaranteed under the CF policy. To demonstrate the effectiveness of PRTA, we apply PRTA to the existing earliest deadline first and rate monotonic scheduling algorithms employing the CF policy, and show that up to 46.4% and 18.3% schedulability performance improvement can be achieved, respectively, compared to those applying the existing schedulability analysis. © 2019 Elsevier Inc.","CF policy; Real-time scheduling; Response-time analysis; Schedulability analysis"
"Performance analysis of radio spectrum for cognitive radio wireless networks using discrete time Markov chain","2019","Journal of Systems and Software","10.1016/j.jss.2019.01.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060547538&doi=10.1016%2fj.jss.2019.01.053&partnerID=40&md5=1fa647eb6df41dfd3792219c72224a86","Cognitive radio (CR) arises as an important technique for wireless network with the aim to provide better utilization of radio spectrum, which is being utilized sporadically in some areas. IEEE 802.22 wireless regional area network (WRAN) is the first standard for CR technology designed to opportunistically utilize the unused or under-utilized TV bands. A WRAN cell normally consists of a number of customer premises equipments (CPEs)/CR users and a base station (BS) having master/slave architecture. When a CPE is powered on, it first attempts to associate with the BS. In this paper, a discrete time Markov chain model (DTMC) is presented to show the association process of CPEs with the BS. To the best of our knowledge, this paper is the first in which DTMC Model is analyzed and investigated for WRAN. Using this model, various parameters such as association time, expected return time to the respective backoff stage, first passage time, etc., are derived. Finally, the evaluation results are provided to analyze the system's performance. © 2019 Elsevier Inc.","Base station; Cognitive radio; Customer premises equipment; Discrete time Markov chain model"
"Developing and using checklists to improve software effort estimation: A multi-case study","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054338217&doi=10.1016%2fj.jss.2018.09.054&partnerID=40&md5=7fc66958e0bfd3e598a4d0b730493570","Expert judgment based effort estimation techniques are widely used for estimating software effort. In the absence of process support, experts may overlook important factors during estimation, leading to inconsistent estimates. This might cause underestimation, which is a common problem in software projects. This multi-case study aims to improve expert estimation of software development effort. Our goal is two-fold: 1) to propose a process to develop and evolve estimation checklists for agile teams, and 2) to evaluate the usefulness of the checklists in improving expert estimation processes. The use of checklists improved the accuracy of the estimates in two case companies. In particular, the underestimation bias was reduced to a large extent. For the third case, we could not perform a similar analysis, due to the unavailability of historical data. However, when checklist was used in two sprints, the estimates were quite accurate (median Balanced Relative Error (BRE) bias of -0.05). The study participants from the case companies observed several benefits of using the checklists during estimation, such as increased confidence in estimates, improved consistency due to help in recalling relevant factors, more objectivity in the process, improved understanding of the tasks being estimated, and reduced chances of missing tasks. © 2018 Elsevier Inc.","Agile software development; Case study; Checklist; Expert judgment based effort estimation"
"Performance and programming effort trade-offs of android persistence frameworks","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053795102&doi=10.1016%2fj.jss.2018.08.038&partnerID=40&md5=271c023f1215df1c5059ee7663630872","A fundamental building block of a mobile application is the ability to persist program data between different invocations. Referred to as persistence, this functionality is commonly implemented by means of persistence frameworks. Without a clear understanding of the energy consumption, execution time, and programming effort of popular Android persistence frameworks, mobile developers lack guidelines for selecting frameworks for their applications. To bridge this knowledge gap, we report on the results of a systematic study of the performance and programming effort trade-offs of eight Android persistence frameworks, and provide practical recommendations for mobile application developers. © 2018 Elsevier Inc.","Mobile application; Performance; Persistence framework; Programming effort"
"On a pursuit for perfecting an undergraduate requirements engineering course","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050152264&doi=10.1016%2fj.jss.2018.07.008&partnerID=40&md5=85102f0c3210dcb12699a21f938712ea","Requirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a “soft” skill by students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where “soft” skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, several project-based approaches have been experimented with in teaching RE, and these have evolved over time. In this paper, the progress of teaching methodologies is documented to capture the pros and cons of these varied approaches, and to reflect on what worked and what did not in teaching RE to undergraduate engineering students. © 2018 Elsevier Inc.","Course evolution; Project-Based learning; Requirements engineering"
"Improving software design reasoning–A reminder card approach","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048198393&doi=10.1016%2fj.jss.2018.05.019&partnerID=40&md5=1de4f5d4fc1d558fcab12ee21a2c328c","Software designers have been known to think naturalistically. This means that there may be inadequate rational thinking during software design. In the past two decades, many research works suggested that designers need to produce design rationale. However, design rationale can be produced to retrofit naturalistic decisions, which means that design decisions may still not be well reasoned. Through a controlled experiment, we studied design reasoning and design rationale by asking participants to carry out a group design. As treatment, we provided 6 out of 12 student teams with a set of reasoning reminder cards to see how they compare with teams without the reminder cards. Additionally, we performed the same experiment with 2 teams of professionals who used the reminder cards, and compared the results with 3 teams of professionals. The experimental results show that both professionals and students who were equipped with the reasoning reminder cards reasoned more with their design. Second, the more a team discusses design reasoning, the more design rationale they find. © 2018 Elsevier Inc.",""
"A systematic review on the code smell effect","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050081131&doi=10.1016%2fj.jss.2018.07.035&partnerID=40&md5=b882822659f4a20d49e3fd2c3be4b4c5","Context: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics. © 2018 Elsevier Inc.","Code smell; Systematic review; Thematic synthesis"
"Runtime management and quantitative evaluation of changing system goals in complex autonomous systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049352939&doi=10.1016%2fj.jss.2018.06.076&partnerID=40&md5=d88d9f9c5bec60ac7b4f02060f84315f","A key challenge in cyber-physical systems (CPS) design is their highly dynamic nature including runtime changes of system goals. Additional safety regulations or changed priorities may apply, e.g. (temporarily) focusing on safety goals after some incident occurred. Goal-aware CPS continuously evaluate goal achievement and autonomously perform adaptations for re-achievement at runtime. For complex system goals capturing dependencies, priorities, and conflicts, efficient goal evaluation techniques are required. To enable a fine-grained balancing of the cost-benefit ratio of autonomous decisions at runtime, a qualitative evaluation of goals is not sufficient. We provide an algorithm that efficiently calculates the quantitative “distance” between a system state and the system goals. We organise various goal types, their parent-children-relationships, context-dependent importances, and dependency relations in a hierarchical goal model. Due to its modular structure, goals can easily be added, removed, and changed at runtime. We illustrate our approach with an exemplary autonomous air drone delivery system and discuss it based on illustrative example scenarios. We argue that our approach enables a) the design of complex context-dependent quantitative goal models for autonomous goal-aware systems, b) the measurement of the impact of autonomous decisions at runtime, and c) the efficient runtime management of changing system goals. © 2018 Elsevier Inc.","Autonomous goal-aware systems; Hierarchical goal model; Quantitative goal evaluation; Runtime changes"
"Characterizing and predicting blocking bugs in open source projects","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047060997&doi=10.1016%2fj.jss.2018.03.053&partnerID=40&md5=f93c74686464d53b3b3c2cb0733313ea","Software engineering researchers have studied specific types of issues such reopened bugs, performance bugs, dormant bugs, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems. In this paper, we study blocking bugs in eight open source projects and propose a model to predict them early on. We extract 14 different factors (from the bug repositories) that are made available within 24 hours after the initial submission of the bug reports. Then, we build decision trees to predict whether a bug will be a blocking bugs or not. Our results show that our prediction models achieve F-measures of 21%–54%, which is a two-fold improvement over the baseline predictors. We also analyze the fixes of these blocking bugs to understand their negative impact. We find that fixing blocking bugs requires more lines of code to be touched compared to non-blocking bugs. In addition, our file-level analysis shows that files affected by blocking bugs are more negatively impacted in terms of cohesion, coupling complexity and size than files affected by non-blocking bugs. © 2018","Code metrics; Post-release defects; Process metrics"
"Estimating the reputation of newcomer web services using a regression-Based method","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051659107&doi=10.1016%2fj.jss.2018.08.026&partnerID=40&md5=aec7af85ffe01b7ac5ebb755ec1f0f46","In this paper, we propose a novel method to estimate the initial reputation values of newcomer web services. In fact, the reputation of web services is one of the criteria used for recommending services in service-oriented computing environments. The lack of evaluating the initial reputation values can subvert the performance of a service recommendation system making it vulnerable to different threats like whitewashing and Sybil attacks, which negatively affect its quality of recommendation. The proposed method uses Quality of Service (QoS) attributes from a side, and reputation values of similar services from the second side, to estimate the reputation values of newcomer services. Basically, it employs regression models, including Support Vector Regression, in the estimation process of the unknown reputation values of newcomers from their known QoS values. We demonstrate the efficiency of the method in estimating the reputation of newcomer services through statistical evidences gathered from experimentations conducted on a set of real-world web services. © 2018 Elsevier Inc.","Feedback rating; Honest and malicious service raters; Reputation measurement; Support vector regression; Web services recommendation"
"The next 700 CPU power models","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050111040&doi=10.1016%2fj.jss.2018.07.001&partnerID=40&md5=d3e8b3f8e71a683a91b62a9e93d9de02","Software power estimation of CPUs is a central concern for energy efficiency and resource management in data centers. Over the last few years, a dozen of ad hoc power models have been proposed to cope with the wide diversity and the growing complexity of modern CPU architectures. However, most of these CPU power models rely on a thorough expertise of the targeted architectures, thus leading to the design of hardware-specific solutions that can hardly be ported beyond the initial settings. In this article, we rather propose a novel toolkit that uses a configurable/interchangeable learning technique to automatically learn the power model of a CPU, independently of the features and the complexity it exhibits. In particular, our learning approach automatically explores the space of hardware performance counters made available by a given CPU to isolate the ones that are best correlated to the power consumption of the host, and then infers a power model from the selected counters. Based on a middleware toolkit devoted to the implementation of software-defined power meters, we implement the proposed approach to generate CPU power models for a wide diversity of CPU architectures (including Intel, ARM, and AMD processors), and using a large variety of both CPU and memory-intensive workloads. We show that the CPU power models generated by our middleware toolkit estimate the power consumption of the whole CPU or individual processes with an accuracy of 98.5% on average, thus competing with the state-of-the-art power models. © 2018 Elsevier Inc.","Energy monitoring; Open testbed; Power models; Software toolkit; Software-defined power meters"
"DiVM: Model checking with LLVM and graph memory","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046825489&doi=10.1016%2fj.jss.2018.04.026&partnerID=40&md5=76096239d3ea18e3ff95368acbb481b6","In this paper, we introduce the concept of a virtual machine with graph-organised memory as a versatile backend for both explicit-state and abstraction-driven verification of software. Our virtual machine uses the LLVM IR as its instruction set, enriched with a small set of hypercalls. We show that the provided hypercalls are sufficient to implement a small operating system, which can then be linked with applications to provide a POSIX-compatible verification environment. Finally, we demonstrate the viability of the approach through a comparison with a more traditionally-designed LLVM model checker. © 2018 Elsevier Inc.","C++; Model checking; Verification; Virtual machine"
"Combining malleability and I/O control mechanisms to enhance the execution of multiple applications","2019","Journal of Systems and Software","10.1016/j.jss.2018.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056212990&doi=10.1016%2fj.jss.2018.11.006&partnerID=40&md5=746e3da520fdd98e627603964cb80526","This work presents a common framework that integrates CLARISSE, a cross-layer runtime for the I/O software stack, and FlexMPI, a runtime that provides dynamic load balancing and malleability capabilities for MPI applications. This integration is performed both at application level, as libraries executed within the application, as well as at central-controller level, as external components that manage the execution of different applications. We show that a cooperation between both runtimes provides important benefits for overall system performance: first, by means of monitoring, the CPU, communication and I/O performances of all executing applications are collected, providing a holistic view of the complete platform utilization. Secondly, we introduce a coordinated way of using CLARISSE and FlexMPI control mechanisms, based on two different optimization strategies, with the aim of improving both the application I/O and overall system performance. Finally, we present a detailed description of this proposal, as well as an empirical evaluation of the framework on a cluster showing significant performance improvements at both application and wide-platform levels. We demonstrate that with this proposal the overall I/O time of an application can be reduced by up to 49% and the aggregated FLOPS of all running applications can be increased by 10% with respect to the baseline case. © 2018 Elsevier Inc.","Cross-layer optimizations; I/O scheduling; Malleability; Middleware; MPI high-performance computing; Parallel I/O"
"Pragmatic cyber physical systems design based on parametric models","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050800511&doi=10.1016%2fj.jss.2018.06.044&partnerID=40&md5=3b519fcf8fdac0b52429a68685e99f0d","The adaptive nature of cyber physical systems (CPS) comes from the fact that they are deeply immersed in the physical environments that are inherently dynamic. CPS also have stringent requirements on real-time operation and safety that are fulfilled by rigorous model design and verification. In the real-time literature, adaptation is mostly limited to off-line modeling of well known and predicted transitions; but this is not appropriate for cyber physical systems as each transition can have unique and unknown characteristics. In the adaptive systems literature, adaptation solutions are silent about timely execution and about the underlying hardware possibilities that can potentially speed up execution. This paper presents a solution for designing adaptive cyber physical systems by using parametric models that are verified during the system execution (i.e., online), so that adaptation decisions are made based on the timing requirements of each particular adaptation event. Our approach allows the system to undergo timely adaptations that exploit the potential parallelism of the software and its execution over multicore processors. We exemplify the approach on a specific use case with autonomous vehicles communication, showing its applicability for situations that require time-bounded online adaptations. © 2018 Elsevier Inc.","Adaptive systems; Autonomous systems; CPS; Verification"
"Analytical metadata modeling for next generation BI systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049309062&doi=10.1016%2fj.jss.2018.06.039&partnerID=40&md5=c5611883330ef7d2c8af5d3303377b35","Business Intelligence (BI) systems are extensively used as in-house solutions to support decision-making in organizations. Next generation BI 2.0 systems claim for expanding the use of BI solutions to external data sources and assisting the user in conducting data analysis. In this context, the Analytical Metadata (AM) framework defines the metadata artifacts (e.g., schema and queries) that are exploited for user assistance purposes. As such artifacts are typically handled in ad-hoc and system specific manners, BI 2.0 argues for a flexible solution supporting metadata exploration across different systems. In this paper, we focus on the AM modeling. We propose SM4AM, an RDF-based Semantic Metamodel for AM. On the one hand, we claim for ontological metamodeling as the proper solution, instead of a fixed universal model, due to (meta)data models heterogeneity in BI 2.0. On the other hand, RDF provides means for facilitating defining and sharing flexible metadata representations. Furthermore, we provide a method to instantiate our metamodel. Finally, we present a real-world case study and discuss how SM4AM, specially the schema and query artifacts, can help traversing different models instantiating our metamodel and enabling innovative means to explore external repositories in what we call metamodel-driven (meta)data exploration. © 2018","Business intelligence; Metadata; Ontological metamodeling"
"A systematic identification of consistency rules for UML diagrams","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048551081&doi=10.1016%2fj.jss.2018.06.029&partnerID=40&md5=d5d2f7c8ba37850eb621057a0a89f0d0","UML diagrams describe different views of one software. These diagrams strongly depend on each other and must therefore be consistent with one another, since inconsistencies between diagrams may be a source of faults during software development activities that rely on these diagrams. It is therefore paramount that consistency rules be defined and that inconsistencies be detected, analyzed and fixed. The relevant literature shows that authors typically define their own consistency rules, sometimes defining the same rules and sometimes defining rules that are already in the UML standard. The reason might be that no consolidated set of rules that are relevant by authors can be found to date. The aim of our research is to provide an up to date, consolidated set of UML consistency rules and obtain a detailed overview of the current research in this area. We therefore followed a systematic procedure in order to collect from literature up to March 2017 and analyze UML consistency rules. We then consolidated a set of 119 UML consistency rules (avoiding redundant definitions or definitions already in the UML standard), which can be used as an important reference for UML-based software development activities, for teaching UML-based software development, and for further research. © 2018","Model checking and verification; Systematic Mapping Study; UML consistency rules"
"Agile values or plan-driven aspects: Which factor contributes more toward the success of data warehousing, business intelligence, and analytics project development?","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054196676&doi=10.1016%2fj.jss.2018.09.081&partnerID=40&md5=4eb033ddbf5e8718f7290d6d912cca59","Practically all organizations are developing data warehousing, business intelligence, and analytics (DW/BIA) projects for achieving customer value. A DW/BIA development project may be characterized by both agile and plan-driven aspects. The reported study investigated two research questions: (1) Which factor, agile values or plan-driven aspects, contributes more toward the success of DW/BIA? (2) What are the significant antecedents of agile values and plan-driven aspects? 124 respondents engaged in DW/BIA development filled a 30-item questionnaire on seven constructs. The partial least squares structural equation modeling (PLS-SEM) method was used to determine the strength of the relationships among the following factors: technological capability, shared understanding, top management commitment, and complexity as antecedents; agile values and plan-driven aspects as mediating; and project success as the dependent construct. Based on a prediction-oriented segmentation (PLS-POS) analysis, the findings indicate that there are two groups, agile-plan balanced and agile-heavy, which represent different approaches to DW/BIA development. Top management commitment and shared understanding emerge as strong antecedents to agile values and plan-driven aspects. Overall, the factor agile values contributes more toward the success of DW/BIA development. © 2018","Agile values; Analytics; Business intelligence; Plan-driven; Shared understanding; Top management commitment"
"A Systematic Mapping Study driven by the margin of error","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050157441&doi=10.1016%2fj.jss.2018.06.078&partnerID=40&md5=2c7cd80c90c4d185a555dea46b0689f6","Until recently, many Systematic Literature Reviews (SLRs) and Systematic Mapping Studies (SMSs) have been proposed. However, when SMS is performed on a broad topic with a large amount of primary studies, the cost of assessment of all primary studies requires unjustified resources. In this paper, a new approach is introduced for performing SMSs, called SMS driven by the margin of error. The main objective of the described work was to decrease the assessment cost of primary studies by stopping the process of classification of primary studies when enough evidence has been collected. We introduced a statistical approach with random sampling and a margin of error into the design of SMSs when a topic under discussion is broad with a large number of primary studies. In this paper, SMS driven by the margin of error was applied on three different use cases: SMS on Domain-Specific Languages, SMS on Template-based Code Generation, and SMS on Software Reliability Modeling, where it was shown that the proposed approach reduced the cost of assessing primary studies and quantified the reliability of SMS. © 2018 Elsevier Inc.","Margin of error; Reliability; Software engineering; Systematic Mapping Study; Systematic review"
"Unusual events in GitHub repositories","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046668982&doi=10.1016%2fj.jss.2018.04.063&partnerID=40&md5=875e59d3f45d982836c6d486ddbb491b","In large and active software projects, it becomes impractical for a developer to stay aware of all project activity. While it might not be necessary to know about each commit or issue, it is arguably important to know about the ones that are unusual. To investigate this hypothesis, we identified unusual events in 200 GitHub projects using a comprehensive list of ways in which an artifact can be unusual and asked 140 developers responsible for or affected by these events to comment on the usefulness of the corresponding information. Based on 2,096 answers, we identify the subset of unusual events that developers consider particularly useful, including large code modifications and unusual amounts of reviewing activity, along with qualitative evidence on the reasons behind these answers. Our findings provide a means for reducing the amount of information that developers need to parse in order to stay up to date with development activity in their projects. © 2018 Elsevier Inc.","Awareness; GitHub; Unusual events"
"Strategies for managing power relationships in software ecosystems","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050251226&doi=10.1016%2fj.jss.2018.07.036&partnerID=40&md5=9fe6cc9a320fa135f781b62d6b496c02","Building a software ecosystem provides companies with business benefits as well as share risks and costs with a network of partners. The ability to establish successful partnerships with other companies can influence the success or failure of the ecosystem. Companies use power to build alliances and strengthen their position in the ecosystem. However, the inappropriate use of power may create tensions that threaten partnerships. To explore the dynamics of power and dependence in software ecosystems, we conducted three case studies of ecosystems formed by small-to-medium enterprises. As a result, we present a set of hypotheses that explain the effects of power on software ecosystems. As theoretical contribution, we present a meta-model that integrates concepts from software ecosystems literature with constructs from classical power theories. Our practical contribution is a set of strategies that companies can employ to manage power relationships with partners, so that their ecosystems can evolve in a healthy and prosperous manner. By obtaining an understanding of the occurrence of power and dependence, companies can recognise how to exercise power and deal with the power from partners in order to leverage their relationships. © 2018","Partnerships; Power; Small-to-medium enterprises; Software ecosystem; Strategies"
"SentiStrength-SE: Exploiting domain specificity for improved sentiment analysis in software engineering text","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051778155&doi=10.1016%2fj.jss.2018.08.030&partnerID=40&md5=3d9f1f0e20a353a1502ee58c9aa0dd33","Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed through building a domain dictionary and appropriate heuristics. These domain-specific techniques are then realized in SentiStrength-SE, a tool we have developed for improved sentiment analysis in text especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both qualitative and quantitative evaluations of our tool. We also separately evaluate the contributions of individual major components (i.e., domain dictionary and heuristics) of SentiStrength-SE. The empirical evaluations confirm that the domain specificity exploited in our SentiStrength-SE enables it to substantially outperform the existing domain-independent tools/toolkits (SentiStrength, NLTK, and Stanford NLP) in detecting sentiments in software engineering text. © 2018 Elsevier Inc.","Automation; Domain dictionary; Emotions; Empirical study; Sentiments; Software engineering"
"Investigating faults missed by test suites achieving high code coverage","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048492247&doi=10.1016%2fj.jss.2018.06.024&partnerID=40&md5=41a2c8f104f02fa8f3982650ba4b7ed2","Code coverage criteria are commonly used to determine the adequacy of a test suite. However, studies investigating code coverage and fault-finding capabilities have mixed results. Some studies have shown that creating test suites to satisfy coverage criteria has a positive effect on finding faults, while other studies do not. In order to improve the fault-finding capabilities of test suites, it is essential to understand what is causing these mixed results. In this study, we investigated one possible source of variation in the results observed: fault type. Specifically, we studied 45 different types of faults and evaluated how effectively human-created test suites with high coverage percentages were able to detect each type of fault. Our results showed, with statistical significance, there were specific types of faults found less frequently than others. However, improvements in the formulation and selection of test oracles could overcome these weaknesses. Based on our results and the types of faults that were missed, we suggest focusing on the strength of test oracles along with code coverage to improve the effectiveness of test suites. © 2018","Automated testing; Code coverage; Software testing; Test suite effectiveness"
"Bug-proneness and late propagation tendency of code clones: A Comparative study on different clone types","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048546570&doi=10.1016%2fj.jss.2018.05.028&partnerID=40&md5=0136aba4db6079ab3bb39c50b752dd65","Code clones are defined to be the exactly or nearly similar code fragments in a software system's code-base. The existing clone related studies reveal that code clones are likely to introduce bugs and inconsistencies in the code-base. However, although there are different types of clones, it is still unknown which types of clones have a higher likeliness of introducing bugs to the software systems and so, should be considered more important for managing with techniques such as refactoring or tracking. With this focus, we performed an empirical study that compared the bug-proneness of the major clone-types: Type 1, Type 2, and Type 3. According to our experimental results on thousands of revisions of nine diverse subject systems, Type 3 clones exhibit the highest bug-proneness among the three clone-types. The bug-proneness of Type 1 clones is the lowest. Also, Type 3 clones have the highest likeliness of being co-changed consistently while experiencing bug-fixing changes. Moreover, the Type 3 clones that experience bug-fixes have a higher possibility of evolving following a Similarity Preserving Change Pattern (SPCP) compared to the bug-fix clones of the other two clone-types. From the experimental results it is clear that Type 3 clones should be given a higher priority than the other two clone-types when making clone management decisions. Our investigation on the relatedness between bug-proneness and late propagation in code clones implies that bug-proneness of code clones is not primarily related with late propagation. The possibility that a bug-fix experienced by a clone fragment will be related with late propagation is only 1.4%. Moreover, for only 10.76% of the cases, a late propagation experienced by clone fragments can be related with a bug. Thus, late propagation contributes to a very little proportion of the bugs in code clones. We believe that our study provides useful implications for ranking clones for management such as refactoring and tracking. © 2018","Bug-proneness; Clone-types; Code clones; Late propagation"
"Efficient refactoring scheduling based on partial order reduction","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051119424&doi=10.1016%2fj.jss.2018.07.076&partnerID=40&md5=90037fc0eb9668a9a3cd1871f4c4f9c2","Anti-patterns are poor solutions to design problems that make software systems hard to understand and to extend. Components involved in anti-patterns are reported to be consistently related to high changes and faults rates. Developers are advised to perform refactoring to remove anti-patterns, and consequently improve software design quality and reliability. However, since the number of anti-patterns in a system can be very large, the process of manual refactoring can be overwhelming. To assist a software engineer who has to perform this task, we propose a novel approach RePOR (Refactoring approach based on Partial Order Reduction). We perform a case study with five open source systems to assess the performance of RePOR against two well-known metaheuristics (Genetic Algorithm, and Ant Colony Optimization), one conflict-aware refactoring approach and, a new approach based on sampling (SWAY). Results show that RePOR can correct a median of 73% of anti-patterns (10% more than existing approaches) with a significant reduction in effort (measured by the number of refactorings applied) ranging from 69% to 85%, and a reduction of execution time ranging between 50% and 87%, in comparison to existing approaches. © 2018 Elsevier Inc.","Ant colony optimization; Anti-patterns; Design quality; Genetic algorithm; Refactoring schedule; Software refactoring"
"TDQN: Trace-driven analytic queuing network modeling of computer systems","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055748953&doi=10.1016%2fj.jss.2018.10.036&partnerID=40&md5=7c2675e6a30e85c5ef1e9939025ceef5","Many current performance studies of computer systems are evaluated by subjecting the system under evaluation to a workload obtained from publicly available traces. Many current performance studies of computer systems are evaluated by using publicly available traces as inputs. This approach provides credibility to performance studies but generally precludes the use of computationally efficient analytic models that rely on strict stochastic assumptions. Simulation or prototype implementations, less general and more time consuming methods, are used when the use of traces limits the applicability of analytic performance models. Analytic models (e.g., queuing theory for single queues or queuing networks) have extensively been used to estimate job execution times under steady state conditions. This paper discusses the use of closed Queuing Network (QN) models during finite time intervals to estimate the execution time of jobs submitted to a computer system. The paper presents the Trace-Driven Queuing Network (TDQN) algorithm that allows job traces to be used as input to analytic models. Validations against experimental results showed that the absolute relative error between measurements and execution time predictions obtained with the TDQN algorithm is below 10% in most cases. Additionally, the paper shows how the TDQN algorithm can be used to estimate the makespan of a stream of jobs submitted to a scheduler, which decides which computer of a cluster will process the job. © 2018 Elsevier Inc.","Computer performance; Mean value analysis; Queuing networks; Scheduling for computer clusters; Trace-driven queuing networks"
"Energy-aware virtual machine allocation for cloud with resource reservation","2019","Journal of Systems and Software","10.1016/j.jss.2018.09.084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055471140&doi=10.1016%2fj.jss.2018.09.084&partnerID=40&md5=f41af1fcaf594b077fdf98132020044d","To reduce the price of pay-as-you-go style cloud applications, an increasing number of cloud service providers offer resource reservation-based services that allow tenants to customize their virtual machines (VMs) with specific time windows and physical resources. However, due to the lack of efficient management of reserved services, the energy efficiency of host physical machines cannot be guaranteed. In today's highly competitive cloud computing market, such low energy efficiency will significantly reduce the profit margin of cloud service providers. Therefore, how to explore energy efficient VM allocation solutions for reserved services to achieve maximum profit is becoming a key issue for the operation and maintenance of cloud computing. To address this problem, this paper proposes a novel and effective evolutionary approach for VM allocation that can maximize the energy efficiency of a cloud data center while incorporating more reserved VMs. Aiming at accurate energy consumption estimation, our approach needs to simulate all the VM allocation updates, which is time-consuming using traditional cloud simulators. To overcome this, we have designed a simplified simulation engine for CloudSim that can accelerate the process of our evolutionary approach. Comprehensive experimental results obtained from both simulation on CloudSim and real cloud environments show that our approach not only can quickly achieve an optimized allocation solution for a batch of reserved VMs, but also can consolidate more VMs with fewer physical machines to achieve better energy efficiency than existing methods. To be specific, the overall profit improvement and energy savings achieved by our approach can be up to 24% and 41% as compared to state-of-the-art methods, respectively. Moreover, our approach could enable the cloud data center to serve more tenant requests. © 2018 Elsevier Inc.","Cloud computing; Energy efficiency; Evolutionary algorithm; Virtual machine allocation; VM acceptance ratio"
"Determining relevant training data for effort estimation using Window-based COCOMO calibration","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055346555&doi=10.1016%2fj.jss.2018.10.019&partnerID=40&md5=e87e294090b213d620a750df36780a78","Context: A software estimation model is often built using historical project data. As software development practices change over time, however, a model based on past data may not make accurate predictions for a new project. Objectives: We investigate the use of moving windows to determine relevant training data for COCOMO calibration. Method: We present a windowing calibration approach to calibrating COCOMO and assess performance of effort estimation models calibrated using windows and all data. Results: Our results show that calibrating COCOMO using small windows of the most recently completed projects generates superior estimates than using all available historical projects. Large windows tend to produce worse estimates. Conclusions: This study provides empirical evidence to support the use of small windows of projects completed so far to calibrate models when COCOMO-like data is available. Additionally, when the change in software development over time is rapid, the use of windows is more justifiable for improving estimation accuracy. © 2018","COCOMO; Model calibration; Moving windows; Project management; Software estimation; Window-based calibration"
"Feedback-based integrated prediction: Defect prediction based on feedback from software testing process","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047826271&doi=10.1016%2fj.jss.2018.05.029&partnerID=40&md5=9b30455308731581a7c56555b4136cd1","Test resource constraints is a common phenomenon in software testing. Using defect prediction to guide the resource allocation can significantly improve the efficiency and effectiveness of available test resources. However, traditional defect prediction (t-DP) is a static strategy, where the predictor cannot be dynamically adjusted during the software testing process (STP). This paper combines defect prediction with feedback control in STP and proposes a feedback-based defect prediction model, where the test results generated during STP is used as feedback information for on-line adjustment of predictor to optimize the prediction result. In addition, a novel approach called feedback-based integrated prediction (FIP) is proposed to improve the prediction accuracy, where a global predictor and a local predictor are employed to make an integrated prediction using the weight to adjust the effects of predictors at different test stages. A systematic experiment is conducted to investigate the performance of the FIP over 10 public data sets. Results show that FIP has better prediction efficiency and better robustness for external data than the t-DP, especially when the percentage of the test modules is 40%. © 2018 Elsevier Inc.","Defect prediction; Feedback control; Integrated prediction; Software testing; Test resource constraints"
"MULAPI: Improving API method recommendation with API usage location","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046644944&doi=10.1016%2fj.jss.2018.04.060&partnerID=40&md5=09a2f8cb32f98c384043967abde94288","During the evolution of a software system, a large number of feature requests are continuously proposed by users. To implement these feature requests, developers often utilize existing third-party libraries and make use of Application Programming Interfaces (APIs) to accelerate the feature implementation process. However, it is not always obvious which API methods are suitable and where these API methods can be used in the target program. In this paper, we propose an approach, MULAPI (Method Usage and Location for API), to recommend API methods and figure out the API usage location where these API methods would be used. MULAPI employs feature location to identify feature related files as API usage location. Further, these feature related files are taken into account to recommend API methods by exploring the source code repository and API libraries as well. We evaluate MULAPI on more than 1000 feature requests of eight Java projects (Axis/Java, CXF, Hadoop Common, Hbase, Struts2, Hadoop HDFS, Hive and Hadoop Map/Reduce), and recommend API methods from ten third-party libraries. The empirical results show that MULAPI can accurately recommend API methods and usage location, and moreover, MULAPI improves the effectiveness of API method recommendation, compared with the state-of-the-art approach. © 2018 Elsevier Inc.","API method recommendation; API usage location; Feature location; Feature request"
"Threat analysis of software systems: A systematic literature review","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049335581&doi=10.1016%2fj.jss.2018.06.073&partnerID=40&md5=1c0a9b6943b5f2da98ea881d918617f7","Architectural threat analysis has become an important cornerstone for organizations concerned with developing secure software. Due to the large number of existing techniques it is becoming more challenging for practitioners to select an appropriate threat analysis technique. Therefore, we conducted a systematic literature review (SLR) of the existing techniques for threat analysis. In our study we compare 26 methodologies for what concerns their applicability, characteristics of the required input for analysis, characteristics of analysis procedure, characteristics of analysis outcomes and ease of adoption. We also provide insight into the obstacles for adopting the existing approaches and discuss the current state of their adoption in software engineering trends (e.g. Agile, DevOps, etc.). As a summary of our findings we have observed that: the analysis procedure is not precisely defined, there is a lack of quality assurance of analysis outcomes and tool support and validation are limited. © 2018 Elsevier Inc.","Risk assessment; Security-by-design; Software systems; Systematic literature review (SLR); Threat analysis (modeling)"
"Tuning self-adaptation in cyber-physical systems through architectural homeostasis","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056237002&doi=10.1016%2fj.jss.2018.10.051&partnerID=40&md5=25d686eab674f7e84f8a9f5908967d7b","Self-adaptive software-intensive cyber-physical systems (sasiCPS) encounter a high level of run-time uncertainty. State-of-the-art architecture-based self-adaptation approaches assume designing against a fixed set of situations that warrant self-adaptation. As a result, failures may appear when sasiCPS operate in environment conditions they are not specifically designed for. In response, we propose to increase the homeostasis of sasiCPS, i.e., the capacity to maintain an operational state despite run-time uncertainty, by introducing run-time changes to the architecture-based self-adaptation strategies according to environment stimuli. In addition to articulating the main idea of architectural homeostasis, we introduce four mechanisms that reify the idea: (i) collaborative sensing, (ii) faulty component isolation from adaptation, (iii) enhancing mode switching, and (iv) adjusting guards in mode switching. Moreover, our experimental evaluation of the four mechanisms in two different case studies confirms that allowing a complex system to change its self-adaptation strategies helps the system recover from run-time errors and abnormalities and keep it in an operational state. © 2018 Elsevier Inc.","Architecture homeostasis; Cyber-physical systems; Run-time uncertainty; Self-adaptation strategies; Software architecture"
"Automated composition and optimization of services for variability-intensive domains","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053721692&doi=10.1016%2fj.jss.2018.07.039&partnerID=40&md5=370f807187350ac09090dafa1a54b88c","The growth in the number of publicly available services on the Web has encouraged developers to rely more heavily on such services to deliver products in a faster, cheaper and more reliable fashion. Many developers are now using a collection of these services in tandem to build their applications. While there has been much attention to the area of service composition, there are few works that examine the possibility of automatically generating service compositions for variability-intensive application domains. High variability in a domain is often captured through an organized feature space, which has the potential for developing many different application instantiations. The focus of our work is to develop an end-to-end technique that would enable the automatic generation of composite services based on a specific configuration of the feature space that would be directly executable and presented in WS-BPEL format. To this end, we adopt concepts from software product line engineering and AI planning to deliver the automated composition of online services. We will further benefit from such notions as safeness and threat from AI planning to optimize the generated service compositions by introducing parallelism where possible. Furthermore, we show how the specification of the generated service composition can be translated into executable WS-BPEL code. More specifically, the core contributions of our work are: (1) we show how AI planning techniques can be used to generate a workflow based on a feature model configuration; (2) we propose a method for optimizing a workflow generated based on AI planning techniques; and (3) we demonstrate that the optimized workflow can be directly translated into WS-BPEL code. We evaluate our work from two perspectives: (i) we will first formally prove that the methods that we have proposed are sound and complete from a theoretical perspective, and (ii) we will show through experimentation that our proposed work is usable from a practical point of view. © 2018 Elsevier Inc.","Automated composition; BPEL; Feature models; Planning; Software product lines; Workflow optimization"
"Collaborative and teamwork software development in an undergraduate software engineering course","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049922420&doi=10.1016%2fj.jss.2018.07.010&partnerID=40&md5=0dc5efc11db4722003475830541d308b","Two key elements of modern software development are collaboration and teamwork. Current methodologies (e.g., agile) and platforms are based on these key elements. This paper describes our experience in stimulating collaboration and teamwork activities of students in the context of a software engineering course at the third year of an undergraduate program in computer science at the University of Milano-Bicocca in Italy. The students were asked to develop a software project in teams of 3 to 5 students for the final exam of the course. The students used GitHub as a collaborative software development platform. In addition, they analyzed the quality of the developed software through SonarQube. The students were also asked to perform project management tasks (e.g., the Gantt) using the Microsoft Project tool. At the end of the course, we gathered the student feedback through a questionnaire on their collaboration and teamwork experience (through GitHub and Microsoft Project tools) and on the use of a software analysis assessment tool, i.e., SonarQube. From their feedback, the students were enthusiastic about working in teams for their project development and about learning how to use tools which are exploited not only in the academic world but also in industry. © 2018","Collaborative software development; GitHub; Microsoft project; Software engineering course; SonarQube; Teamwork"
"Architecture enforcement concerns and activities - An expert study","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051505412&doi=10.1016%2fj.jss.2018.08.025&partnerID=40&md5=d631ea2210fb2d7555e0d61b1397536a","Architecture enforcement is concerned with the correct and seamless implementation of architecture design decisions in order to ensure software quality. In a previous study, we conducted an empirical study in order to gain insight into the industrial practice of architecture enforcement. There, we asked 12 software architects from industry about their experience with the architecture enforcement process. As a result, we identified architecture enforcement concerns and activities. In this paper, we extend our contributions of the existing study. Firstly, we conducted five additional interviews with software architects from two different domains, namely the enterprise application and the automotive domain. This adds new architecture concerns and activities to the existing list. Secondly, we conducted a literature review. We compared our findings from the interviews with the results from the literature review and evaluated how architects’ enforcement concerns and activities are discussed in literature. We found that several concerns and activities are already known from literature, but are not viewed in the context of architecture enforcement by the software architecture community. Lastly, we connected the discovered architecture concerns and activities with each other. Those relationships determine the reason why a specific architecture enforcement activity is conducted. © 2018 Elsevier Inc.","Architecture enforcement; Architecture erosion; Empirical study; Software architecture; Software architecture in industry"
"Test prioritization in continuous integration environments","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053760573&doi=10.1016%2fj.jss.2018.08.061&partnerID=40&md5=b3c576e72ea0009187cb61d61f65ba5c","Two heuristics namely diversity-based (DBTP) and history-based test prioritization (HBTP) have been separately proposed in the literature. Yet, their combination has not been widely studied in continuous integration (CI) environments. The objective of this study is to catch regression faults earlier, allowing developers to integrate and verify their changes more frequently and continuously. To achieve this, we investigated six open-source projects, each of which included several builds over a large time period. Findings indicate that previous failure knowledge seems to have strong predictive power in CI environments and can be used to effectively prioritize tests. HBTP does not necessarily need to have large data, and its effectiveness improves to a certain degree with larger history interval. DBTP can be used effectively during the early stages, when no historical data is available, and also combined with HBTP to improve its effectiveness. Among the investigated techniques, we found that history-based diversity using NCD Multiset is superior in terms of effectiveness but comes with relatively higher overhead in terms of method execution time. Test prioritization in CI environments can be effectively performed with negligible investment using previous failure knowledge, and its effectiveness can be further improved by considering dissimilarities among the tests. © 2018 Elsevier Inc.","Build history; Continuous integration; Regression testing; Test case prioritization; Test diversity"
"Early software defect prediction: A systematic map and review","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049343803&doi=10.1016%2fj.jss.2018.06.025&partnerID=40&md5=d6d0b13384cebbf73458571a60712e3d","Context: Software defect prediction is a trending research topic, and a wide variety of the published papers focus on coding phase or after. A limited number of papers, however, includes the prior (early) phases of the software development lifecycle (SDLC). Objective: The goal of this study is to obtain a general view of the characteristics and usefulness of Early Software Defect Prediction (ESDP) models reported in scientific literature. Method: A systematic mapping and systematic literature review study has been conducted. We searched for the studies reported between 2000 and 2016. We reviewed 52 studies and analyzed the trend and demographics, maturity of state-of-research, in-depth characteristics, success and benefits of ESDP models. Results: We found that categorical models that rely on requirement and design phase metrics, and few continuous models including metrics from requirements phase are very successful. We also found that most studies reported qualitative benefits of using ESDP models. Conclusion: We have highlighted the most preferred prediction methods, metrics, datasets and performance evaluation methods, as well as the addressed SDLC phases. We expect the results will be useful for software teams by guiding them to use early predictors effectively in practice, and for researchers in directing their future efforts. © 2018 Elsevier Inc.","Early defect prediction; Prediction model; Software defect; Software quality; Systematic literature review; Systematic mapping"
"A likelihood-free Bayesian derivation method for service variants","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047626396&doi=10.1016%2fj.jss.2018.05.011&partnerID=40&md5=bdd58e86af0c410f5910cfb4b495cf7a","Application programming interfaces (API), allowing systems to be accessed by the services they expose, have proliferated on the Internet and gained strategic interest in the IT industry. However, integration opportunities for larger, enterprise systems are hampered by complex and overloaded operations of their interfaces, having hundreds of parameters and multiple levels of nesting, corresponding to multiple business entities. Static (code) analysis techniques have been proposed to analyse service interfaces of enterprise systems. They support the derivation of business entities and relationships from the parameters of interface operations, allowing the restructure of operations, based on individual entities. In this paper, we extend the repertoire of static interface analysis to derive service variants, whereby subsets of operation parameters correspond to multiple nested business entity subtypes of variants. Specifically, we apply a Monte Carlo sampling method, based on likelihood-free Bayesian sampling, to traverse large parameter spaces, based on higher probabilistic tree search, to efficiently find subsets of parameters related to prospective subtypes. The results demonstrate a method with significant success rates in massive search spaces, as applied to the FedEx Shipment interface whose operations have in excess of 1000 parameters. © 2018 Elsevier Inc.","Likelihood-free Bayesian methods; Service synthesis; Variant derivation"
"Software engineering process models for mobile app development: A systematic literature review","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051666660&doi=10.1016%2fj.jss.2018.08.028&partnerID=40&md5=3302698fae5f1de6eb91ae6dcf2542bc","Context: An effective development model can help improve competitive advantage and shorten release cycles, which is vital in the fast paced environment of mobile app development. Objective: The aim with this paper is to provide an extensive review of existing mobile app development models. Method: The review is done by following a systematic literature review process. Also presented is an assessment of the usefulness and relevance to industry of the models based on a rigor and relevance framework. Results: 20 primary studies were identified, each with distinct models. Agile methods or state-based principles are commonly adopted across the models. Relatively little effort focuses on deployment, maintenance, project evaluation activities. Conclusion: The review reveals that the contexts in which the identified models are intended to be used vary. This benefits practitioners as they are able to select a model that suits their contexts. However, the usefulness in industry of most of the models, based on the contexts in which the models were evaluated, is questionable. There is a need for evaluating mobile app models in contexts that resemble realistic contexts. The review also calls for further research addressing special constraints of mobile apps, e.g., testing apps on multiple-platforms, user involvement in release planning and continuous deployment. © 2018 Elsevier Inc.","Hybrid apps; Mobile application development; Mobile apps; Native apps; Software engineering process models; Systematic literature review"
"An exploratory case study on reusing architecture decisions in software-intensive system projects","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048152167&doi=10.1016%2fj.jss.2018.05.064&partnerID=40&md5=de73e0bb816ce9cf2b6138096686902f","Reusing architecture decisions from previous projects promises to support architects when taking decisions. However, little is known about the state of art of decision-reuse and the benefits and challenges associated with reusing decisions. Therefore, we study how software architects reuse architecture decisions, the stakeholders and their concerns related to decision-reuse, and how architects perceive the ideal future state of decision-reuse. We conducted a qualitative explorative case study in the software-intensive systems industry. The study has shown that architects frequently reuse decisions but are confined to decisions they already know or have heard about. The results also suggest that architects reuse decisions in an ad-hoc manner. Moreover this study presents a conceptual model of decision-reuse and lists stakeholder concerns with regards to decision-reuse. The results of this study indicate that improving the documentation and discoverability of decisions holds a large potential to increase reuse of decisions and that decision documentation is not only important for system understanding or in the context of architecture reviews but also to support architects in upcoming projects. © 2018",""
"On the analysis of spectrum based fault localization using hitting sets","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054846389&doi=10.1016%2fj.jss.2018.10.013&partnerID=40&md5=379e248835fa6ff88bffaf98d341d4c7","Combining spectrum-based fault localization (SBFL) with other techniques is generally regarded as a feasible approach as advantages from both techniques would be preserved. SENDYS which combines SBFL with slicing-hitting-set-computation is one of the promising techniques. However, all current evaluations on SENDYS were obtained via empirical studies, which have inevitable threats to validity. Besides, purely empirical studies cannot reveal the essential reason that why SENDYS performs well or badly, and whether all the complicated computations are necessary. Therefore, in this paper, we provide an in-depth theoretical analysis on SENDYS, which can give definite and convincing conclusions. We generalize our previous theoretical framework on SBFL, to make it applicable to combined techniques like SENDYS. We first provide a variant of current SENDYS by patching its loophole of ignoring “zero or negative risk values” in normalization. This variant plays as a substitution of the original SENDYS, as well as one of the baselines in our analysis. Then, by modifying a few steps of this variant, we propose an enhanced SENDYS and theoretically prove its superiority over several other methods in single-fault scenario. Moreover, we provide a short-cut reformulation of the enhanced SENDYS by preserving its performance, but only requiring very simple computations. And it is proved to be even better than traditional SBFL maximal formulas. As a complementary, our empirical studies with 13 subject programs demonstrate the obvious superiority of the enhanced SENDYS, as well as its stability across different formulas in single-fault scenario. For multiple-fault cases, the variant of SENDYS is observed to have the best performance. Besides, this variant has shown great helpfulness in improving the bad performance of the original SENDYS when encountering the NOR problem. © 2018 Elsevier Inc.","Dynamic slicing; Hitting set; Risk evaluation formulas; Spectrum based fault localization; Theoretical analysis"
"ESPRET: A tool for execution time estimation of manual test cases","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053193472&doi=10.1016%2fj.jss.2018.09.003&partnerID=40&md5=bb19b632d239f3c2396a411b664c9e3a","Manual testing is still a predominant and an important approach for validation of computer systems, particularly in certain domains such as safety-critical systems. Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. In this work, we present, apply and evaluate ESPRET (EStimation and PRediction of Execution Time) as our tool for estimating and predicting the execution time of manual test cases based on their test specifications. Our approach works by extracting timing information for various steps in manual test specification. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test steps is already available or not. Since executing test cases on the several machines may take different time, we predict the actual execution time for test cases by a set of regression models. Finally, an empirical evaluation of the approach and tool has been performed on a railway use case at Bombardier Transportation (BT) in Sweden. © 2018 Elsevier Inc.","Execution time; Manual testing; Optimization; Regression analysis; Software testing; Test specification"
"Automatic assignment of integrators to pull requests: The importance of selecting appropriate attributes","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048779347&doi=10.1016%2fj.jss.2018.05.065&partnerID=40&md5=feff6a7605d08ea7da138bb13f35a396","In open-source projects that adopt the pull-based development workflow, a core developer needs to analyze the contribution received via pull requests and decide on integrating it or not in the repository. However, this process is time-consuming, leading to an increasing number of pull requests left to be analyzed. Consequently, the assignment of suitable integrators to pull requests becomes an important step in the pull-based development workflow. Classification methods have already been used to recommend integrators, based on different sets of predictive attributes. The main contribution of this paper is to identify a set of attributes that can improve the performance of the integrator prediction task reported in the literature. To do so, we first evaluate different sets of attributes used by previous studies with different classification algorithms. Besides, we explore attribute selection strategies on an extended set of attributes composed not only by the attributes already used in the literature but also new attributes we consider relevant to the problem. Experiments with 32 open-source projects evidenced that after applying attribute selection strategies and, consequently, identifying a more suitable set of attributes, the recommendation has achieved normalized improvements 54% higher than the state-of-the-art. © 2018 Elsevier Inc.","Distributed software development; Integrator assignment; Pull-based software development"
"An aggregated coupling measure for the analysis of object-oriented software systems","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056183401&doi=10.1016%2fj.jss.2018.10.052&partnerID=40&md5=67e126f648590c47ba3cdf4fa1a26da1","Coupling is a fundamental property of software systems which is strongly connected with the quality of software design and has high impact on program understanding. The coupling between software components influences software maintenance and evolution as well. In order to ease the maintenance and evolution processes it is essential to estimate the impact of changes made in the software system, coupling indicating such a possible impact. This paper introduces a new aggregated coupling measurement which captures both the structural and the conceptual characteristics of coupling between the software components. The proposed measure combines the textual information contained in the source code with the structural relationships between software components. We conduct several experiments which underline that the proposed aggregated coupling measure reveals new characteristics of coupling and is also effective for change impact analysis. © 2018 Elsevier Inc.","Conceptual coupling; Coupling measure; Structural coupling; Unsupervised learning"
"Business process model refactoring applying IBUPROFEN. An industrial evaluation","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054565285&doi=10.1016%2fj.jss.2018.10.012&partnerID=40&md5=79d19b9bf803f3e26ca622fcdd69e001","Business process models are recognized as being important assets for companies, since appropriate management of them provides companies with a competitive advantage. Quality assurance of business process models has become a critical issue, especially when companies carry out reverse engineering techniques to retrieve their business process models. Thus, companies have to deal with several quality faults, such as unmeaningful elements, fine-grained granularity or incompleteness, which seriously affect understandability and modifiability of business process models. The most widely-used method to reduce these faults is refactoring. Although several refactoring operators exist in the literature, there are no refactoring techniques specially developed for business process models obtained by process mining and other reverse engineering techniques. Therefore, this paper presents the use of IBUPROFEN, a business process model refactoring technique for those models obtained by reverse engineering. IBUPROFEN is applied in an in-depth case study with a real-life information system belonging to a European bank company. The goal of this industrial evaluation is to prove that the refactoring operators improve the understandability and modifiability of the business process model after being refactored. In addition, the scalability of the technique is assessed to demonstrate the feasibility of its application. © 2018","Business process model; Case study; Modifiability; Refactoring; Understandability"
"Coordinated actor model of self-adaptive track-based traffic control systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048497434&doi=10.1016%2fj.jss.2018.05.034&partnerID=40&md5=1e412e29795eadc1c466151ca34fece7","Self-adaptation is a well-known technique to handle growing complexities of software systems, where a system autonomously adapts itself in response to changes in a dynamic and unpredictable environment. With the increasing need for developing self-adaptive systems, providing a model and an implementation platform to facilitate integration of adaptation mechanisms into the systems and assuring their safety and quality is crucial. In this paper, we target Track-based Traffic Control Systems (TTCSs) in which the traffic flows through pre-specified sub-tracks and is coordinated by a traffic controller. We introduce a coordinated actor model to design self-adaptive TTCSs and provide a general mapping between various TTCSs and the coordinated actor model. The coordinated actor model is extended to build large-scale self-adaptive TTCSs in a decentralized setting. We also discuss the benefits of using Ptolemy II as a framework for model-based development of large-scale self-adaptive systems that supports designing multiple hierarchical MAPE-K feedback loops interacting with each other. We propose a template based on the coordinated actor model to design a self-adaptive TTCS in Ptolemy II that can be instantiated for various TTCSs. We enhance the proposed template with a predictive adaptation feature. We illustrate applicability of the coordinated actor model and consequently the proposed template by designing two real-life case studies in the domains of air traffic control systems and railway traffic control systems in Ptolemy II. © 2018 Elsevier Inc.","MAPE-K feedback loop; Model@Runtime; Ptolemy II framework; Self-adaptive systems; Track-based traffic control systems"
"Efficient synthesis of robust models for stochastic systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047800364&doi=10.1016%2fj.jss.2018.05.013&partnerID=40&md5=99788a400e60316779ca62de300b7d38","We describe a tool-supported method for the efficient synthesis of parametric continuous-time Markov chains (pCTMC) that correspond to robust designs of a system under development. The pCTMCs generated by our RObust DEsign Synthesis (RODES) method are resilient to changes in the system's operational profile, satisfy strict reliability, performance and other quality constraints, and are Pareto-optimal or nearly Pareto-optimal with respect to a set of quality optimisation criteria. By integrating sensitivity analysis at designer-specified tolerance levels and Pareto optimality, RODES produces designs that are potentially slightly suboptimal in return for less sensitivity—an acceptable trade-off in engineering practice. We demonstrate the effectiveness of our method and the efficiency of its GPU-accelerated tool support across multiple application domains by using RODES to design a producer-consumer system, a replicated file system and a workstation cluster system. © 2018","Multi-objective optimisation; Probabilistic model synthesis; Robust design; Software performance and reliability engineering"
"Robustness of spectrum-based fault localisation in environments with labelling perturbations","2019","Journal of Systems and Software","10.1016/j.jss.2018.09.091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055746671&doi=10.1016%2fj.jss.2018.09.091&partnerID=40&md5=a70618b9f501abf17ef874c65447fb20","Most fault localisation techniques take as inputs a faulty program and a test suite, and produce as output a ranked list of suspicious code locations at which the program may be defective. If only a small portion of the executions are labelled erroneously, we expect a fault localisation technique to be robust to these errors. However, it is not known which fault localisation techniques with high accuracy are robust and which techniques are best at finding faults under the trade-off between accuracy and robustness. In this paper, a theoretical analysis of the impacts of labelling perturbations on spectrum-based fault localisation techniques (SBFL) is presented from different aspects first. We theoretically analyse the influence of labelling perturbations on three relations among risk evaluation formulas and the effect of mislabelling cases on the ranking of faulty statements. Then, we conduct controlled experiments on 18 programs with 3079 faulty versions from different domains to compare the robustness of 23 classes of risk evaluation formulas. Besides, experiments are conducted for evaluating the robustness of two neural network-based techniques. The impacts of perturbation degrees, number of faults and types of labelling perturbation on the robustness of formulas are empirically studied, and several interesting findings are obtained. © 2018 Elsevier Inc.","Labelling perturbations; Risk evaluation formulas; Robustness; Software fault localisation"
"Enable more frequent integration of software in industry projects","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046647997&doi=10.1016%2fj.jss.2018.05.002&partnerID=40&md5=73948cf60ead9391294092bc7b35fba1","Based on interviews with 20 developers from two case study companies that develop large-scale software-intensive embedded systems, this paper presents twelve factors that affect how often developers commit software to the mainline. The twelve factors are grouped into four themes: “Activity planning and execution”, “System thinking”, “Speed” and “Confidence through test activities”. Based on the interview results and a literature study we present the EMFIS model, which allows companies to explicate a representation of the organization's current situation regarding continuous integration impediments, and visualizes what the organization must focus on in order to enable more frequent integration of software. The model is used to perform an assessment of the twelve factors, where the ratings from participants representing the developers are summarized separately from ratings from participants representing the enablers (responsible for processes, development tools, test environments etc.). The EMFIS model has been validated in workshops and interviews, which in total included 46 individuals in five case study companies. The model was well received during the validation, and was appreciated for its simplicity and its ability to show differences in rating between developers and enablers. © 2018","Continuous delivery; Continuous integration; Embedded systems; Large-scale; Software integration"
"A bibliometric assessment of software engineering scholars and institutions (2010–2017)","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055899894&doi=10.1016%2fj.jss.2018.10.029&partnerID=40&md5=5e64adcac6d527f42433ffeac1286552","This paper presents the findings of a bibliometric study, targeting an eight-year period (2010–2017), with the aim of identifying: (a) emerging research directions, (b) the top-20 institutions, and (c) top-20 early stage, consolidated, and experienced scholars in the field of software engineering. To perform this goal, we performed a bibliometric study, by applying the mapping study technique on top-quality software engineering venues, and developed a dataset of 14,456 primary studies. As the ranking metric for institutions, we used the count of papers in which authors affiliated with this institute have been identified in the obtained dataset, whereas regarding scholars we computed the corresponding rankings based on the number of published papers and the average number of citations. Finally, we identified the top-20 rising scholars in the SE research community, based on their recent publication record (between 2015 and 2017) and their research age. © 2018 Elsevier Inc.","Publications; Software engineering; Top institutions; Top scholars"
"Increasing test efficiency by risk-driven model-based testing","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049896037&doi=10.1016%2fj.jss.2018.06.080&partnerID=40&md5=567c644a8f3bd43b31b2d7ebd637c12f","We introduce an approach and a tool, RIMA, for adapting test models used for model-based testing to augment information regarding failure risk. We represent test models in the form of Markov chains. These models comprise a set of states and a set of state transitions that are annotated with probability values. These values steer the test case generation process, which aims at covering the most probable paths. RIMA refines these models in 3 steps. First, it updates transition probabilities based on a collected usage profile. Second, it updates the resulting models based on fault likelihood at each state, which is estimated based on static code analysis. Third, it performs updates based on error likelihood at each state, which is estimated with dynamic analysis. The approach is evaluated with two industrial case studies for testing digital TVs and smart phones. Results show that the approach increases test efficiency by revealing more faults in less testing time. © 2018","Industrial case study; Model refinement; Model-based testing; Risk-based testing; Software test automation; Statistical usage testing"
"Exoneration-based fault localization for SQL predicates","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055738898&doi=10.1016%2fj.jss.2018.10.037&partnerID=40&md5=de92685fe6947ff23cf168e1a678bc3d","Spectrum-based fault localization (SFL) techniques automatically localize faults in program entities (statements, predicates, SQL clauses, etc.) by analyzing information collected from test executions. One application of SFL techniques is to find faulty SQL statements in database applications. However, prior techniques treated each SQL statement as one program entity, thus they could not find faulty elements inside SQL statements. Since SQL statements can be complex, identifying the faulty element within a faulty SQL statement is still time-consuming. In our previous paper, we developed a novel fault localization method based on row-based dynamic slicing and delta debugging techniques that can localize faults in individual clauses within SQL predicates. We call this technique exoneration-based fault localization because it can exonerate “innocent” elements and precisely identify the faulty element, whereas previous SFL techniques simply ranked all the elements in an SQL statement based on suspiciousness. This paper improves the exoneration-based fault localization technique with a new algorithm that considerably reduces the execution time. We also conducted an empirical study that compared nine existing SFL techniques with the exoneration-based technique in localizing faulty clauses in SQL predicates. Results indicate that the new exoneration-based technique surpasses the other techniques both in terms of effectiveness and efficiency. © 2018","Exoneration-based fault localization; Fault localization; Spectrum-based fault localization; SQL; WHERE clause"
"Hybrid charging scheduling schemes for three-dimensional underwater wireless rechargeable sensor networks","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053388548&doi=10.1016%2fj.jss.2018.09.002&partnerID=40&md5=6b3af41ac9baeb400f5732ea59d280e1","Recent breakthrough in wireless power transfer provides a new paradigm for enabling wireless energy replenishment for wireless rechargeable sensor networks, especially in the underwater environment. In this paper, we first propose the concept of UWRSNs (underwater wireless rechargeable sensor networks) and then develop a series of 3D charging schemes for enhancing charging efficiency, using underwater charging robot mules in three-dimensional charging scenarios. Through constructing the architecture of UWRSNs, we develop a basic charging scheme SCS (Shortest-path Charging Scheme), which minimizes the traveling cost for the charging mules in the 3D underwater environment. Then, ECS (Emergency Charging Scheme) is proposed, which concentrates on serving emergency nodes. After that, a charging algorithm that combines ECS and SCS to collaboratively solve the charging problem, namely, HOCS (Hybrid Optimal Charging Scheme) is developed. At last, experimental simulations are conducted to show the outperformed merits of the proposed scheme. Experimental results demonstrate that our schemes not only save energy and time, but also ensure effective utilization of resources. © 2018 Elsevier Inc.","Average waiting time; Charging efficiency; Charging scheduling; Underwater wireless rechargeable sensor networks"
"Self-adaptation of service compositions through product line reconfiguration","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048324338&doi=10.1016%2fj.jss.2018.05.069&partnerID=40&md5=9584a7108c868875e51bc43f459f8a7c","The large number of published services has motivated the development of tools for creating customized composite services known as service compositions. While service compositions provide high agility and development flexibility, they can also pose challenges when it comes to delivering guaranteed functional and non-functional requirements. This is primarily due to the highly dynamic environment in which services operate. In this paper, we propose adaptation mechanisms that are able to effectively maintain functional and non-functional quality requirements in service compositions derived from software product lines. Unlike many existing work, the proposed adaptation mechanism does not require explicit user-defined adaptation strategies. We adopt concepts from the software product line engineering paradigm where service compositions are viewed as a collection of features and adaptation happens through product line reconfiguration. We have practically implemented the proposed mechanism in ourMagus tool suite and performed extensive experiments, which show that our work is both practical and efficient for automatically adapting service compositions once violations of functional or non-functional requirements are observed. © 2018 Elsevier Inc.","Feature model; Self adaptation; Service composition; Software product lines"
"Supporting semi-automatic co-evolution of architecture and fault tree models","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046787518&doi=10.1016%2fj.jss.2018.04.001&partnerID=40&md5=f3b0c51da3455af339b2ec7c48b620e9","During the whole life-cycle of software-intensive systems in safety-critical domains, system models must consistently co-evolve with quality evaluation models like fault trees. However, performing these co-evolution steps is a cumbersome and often manual task. To understand this problem in detail, we have analyzed the evolution and mined common changes of architecture and fault tree models for a set of evolution scenarios of a part of a factory automation system called Pick and Place Unit. On the other hand, we designed a set of intra- and inter-model transformation rules which fully cover the evolution scenarios of the case study and which offer the potential to semi-automate the co-evolution process. In particular, we validated these rules with respect to completeness and evaluated them by a comparison to typical visual editor operations. Our results show a significant reduction of the amount of required user interactions in order to realize the co-evolution. © 2018 Elsevier Inc.","Fault trees; Model co-evolution; Model transformation; Safety; System architecture"
"Supporting end users to control their smart home: design implications from a literature review and an empirical investigation","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049315712&doi=10.1016%2fj.jss.2018.06.035&partnerID=40&md5=8d2f1042cd1b53e692ad4e596b85c6ea","Designing tools that allow end users to easily control and manage a smart home is a critical issue that researchers in Ambient Intelligence and Internet of Things have to address. Because of the variety of available solutions, with their advantages and limitations, it is not straightforward to understand which are the requirements that must be satisfied to effectively support end users. This paper aims to contribute to this topic through a systematic and rigorous activity based on two main pillars of the empirical research in software engineering: i) a literature review addressing design and evaluation of tools for smart home control oriented to end users, and ii) an experimental study in which three tools, that emerged from the literature review as the most suitable and widespread, were compared in order to identify the interaction mechanisms that end users appreciate most. On the basis of the obtained results, a set of design implications that may drive the development of future tools for smart home control and management are presented. © 2018","Ambient intelligence; End-user development; Internet of things; Rule-based paradigm; Smart home"
"Architecture-based change impact analysis in cross-disciplinary automated production systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054007690&doi=10.1016%2fj.jss.2018.08.058&partnerID=40&md5=8f5d5be1dd08f743f585a29da2d9b528","Maintaining an automated production system is a challenging task as it comprises artifacts from multiple disciplines – namely mechanical, electrical, and software engineering. As the artifacts mutually affect each other, even small modifications may cause extensive side effects. Consequently, estimating the maintenance effort for modifications in an automated production system precisely is time consuming and often nearly as complicated as implementing the modifications. In this paper, we present the KAMP4aPS approach for architecture-based change impact analysis in production automation. We propose metamodels to specify the various artifacts of the system and modifications to them, as well as algorithms and rules for change propagation analysis based on the models. We evaluate KAMP4aPS for three different change scenarios based on the established xPPU community case study on production automation. In the case study, we investigate different configurations of metamodels and change propagation rules. Evaluation results indicate the accuracy of change propagation for applying KAMP4aPS to the specific metamodel and rules. © 2018 Elsevier Inc.","Change impact analysis; Maintenance cost estimation; Manufacturing system; Metamodeling; Production automation; Programmable logic controller"
"Code smells and their collocations: A large-scale experiment on open-source systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047970347&doi=10.1016%2fj.jss.2018.05.057&partnerID=40&md5=13247a0e7927890a2a6e02889e25dbc8","Code smells indicate possible flaws in software design, that could negatively affect system's maintainability. Interactions among smells located in the same classes (i.e., collocated smells) have even more detrimental effect on quality. Extracted frequent patterns of collocated smells could help to understand practical consequences of collocations. In this paper we identify and empirically validate frequent collocations of 14 code smells detected in 92 Java systems, using three approaches: pairwise correlation analysis, PCA and associative rules. To cross-validate the results, we used up to 6 detectors for each smell. Additionally, we examine and compare techniques used to extract the relationships. The contribution is three-fold: (1) we identify and empirically validate relationships among the examined code smells on a large dataset that we made publicly available, (2) we discuss how the choice of code smell detectors affects results, and (3) we analyze the impact of software domain on existence of the smell collocations. Additionally, we found that analytical methods we used to discover collocations, are complementary. Smells collocations display recurring patterns that could help prioritizing the classes affected by code smells to be refactored and developing or enhancing detectors exploiting information about collocations. They can also help the developers focusing on classes deserving more maintenance effort. © 2018","Code smell detectors; Code smells; Collocated smells; Inter-smell relationships; Smell interaction; Source code quality"
"On the placement of controllers in software-Defined-WAN using meta-heuristic approach","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048535804&doi=10.1016%2fj.jss.2018.05.032&partnerID=40&md5=c1820d4483f52a0f0f1416787e65f10d","Software Defined Networks (SDN) is a popular modern network technology that decouples the control logic from the underlying hardware devices. The control logic has implemented as a software entity that resides in a server called controller. In a Software-Defined Wide Area Network (SDWAN) with n nodes; deploying k number of controllers (k < n) is one of the challenging issue. Due to some internal or external factors, when the primary path between switch to controller fails, it severely interrupt the networks’ availability. In this regard, the proposed approach provides a seamless backup mechanism against single link failure with minimum communication delay based on the survivability model. In order to obtain an efficient solution, we have considered controller placement problem (CPP) as a multi-objective combinatorial optimization problem and solve it using two population-based meta-heuristic techniques such as: Particle Swarm Optimization (PSO) and FireFly Algorithm (FFA). For CPP, three metrics have been considered: (a) controller to switch latency, (b) inter-controller latency and (c) multi-path connectivity between the switch and controller. The performance of the algorithms is evaluated on a set of publicly available network topologies in order to obtain the optimum number of controllers, and controller positions. Then we present Average Delay Rise (ADR) metric to measure the increased delay due to the failure of the primary path. By comparing the performance of our scheme to competing scheme, it was found that our proposed scheme effectively improves the survivability of the control path and the performance of the network as well. © 2018 Elsevier Inc.","Backup path; Controller; CPP; Meta-Heuristic approach; PSO; SDN"
"Understanding the impact of emotions on software: A case study in requirements gathering and evaluation","2019","Journal of Systems and Software","10.1016/j.jss.2018.06.077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055743572&doi=10.1016%2fj.jss.2018.06.077&partnerID=40&md5=bf4bf8781076b1de269ecc40cba349a5","Smart home technology has received growing interest in recent years with a focus on automation and assistance, for example, Alexa, Google Home, Apple HomePod, and many cheap IoT devices. Better supporting elderly people to continue live in their home using smart home technology is a key target application. However, most of the existing smart home solutions for the elderly are not designed with people's emotional goals in mind, leading to lack of adoption, lack of engagement, and failure of the technology. In this paper, we introduce an emotion-oriented requirements engineering approach to help identifying, modeling and evaluating emotional goals. We also explain how we used this technique to help us develop SofiHub - a new smart home platform for elderly people. SofiHub comprises a range of devices and software for sensing, interaction, passive monitoring, and emergency assistance. We have conducted multiple trials including initial field trials for elderly people in real houses. We have used our emotion-oriented requirements techniques to evaluate the participants’ emotional reactions before, during, and after trials to understand the impact of such technology on elderly people's emotions to the SofiHub solution. Our analysis shows that SofiHub successfully alleviates their loneliness, makes them feel safer and cared about. We also found that the trial participants developed a strong relation with the system and hence, felt frustrated when SofiHub did not respond in ways expected or desired. We reflect on the lessons learned from the trials related to our emotion-oriented design and evaluation experimental approach, including refining our set of evaluation tools. © 2018","Elderly; Emotion-oriented development approach; Emotions; Independent living; Loneliness; Smart home"
"A systematic literature review of software visualization evaluation","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048819348&doi=10.1016%2fj.jss.2018.06.027&partnerID=40&md5=12c991ee713cc8a9c4f4bca5e4224e5c","Context:Software visualizations can help developers to analyze multiple aspects of complex software systems, but their effectiveness is often uncertain due to the lack of evaluation guidelines. Objective: We identify common problems in the evaluation of software visualizations with the goal of formulating guidelines to improve future evaluations. Method:We review the complete literature body of 387 full papers published in the SOFTVIS/VISSOFT conferences, and study 181 of those from which we could extract evaluation strategies, data collection methods, and other aspects of the evaluation. Results:Of the proposed software visualization approaches, 62% lack a strong evaluation. We argue that an effective software visualization should not only boost time and correctness but also recollection, usability, engagement, and other emotions. Conclusion:We call on researchers proposing new software visualizations to provide evidence of their effectiveness by conducting thorough (i) case studies for approaches that must be studied in situ, and when variables can be controlled, (ii) experiments with randomly selected participants of the target audience and real-world open source software systems to promote reproducibility and replicability. We present guidelines to increase the evidence of the effectiveness of software visualization approaches, thus improving their adoption rate. © 2018","Evaluation; Literature review; Software visualisation"
"Efficient cloud service discovery approach based on LDA topic modeling","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054155937&doi=10.1016%2fj.jss.2018.09.069&partnerID=40&md5=b7bf0a30715e2678eed82afb74f632fa","With the rapid development of Cloud-based services, the necessity of a Cloud service discovery engine becomes a fundamental requirement. A semantic focused crawler is one of the most key components of Cloud service discovery engines. However, the huge size and varied functionalities of Cloud services on the Web have a great effect on crawlers to provide effective Cloud services. It is a challenge for semantic crawlers to search only for URLs that offer Cloud services from this explosion of information. To solve these issues, this paper proposes a self-adaptive semantic focused crawler based on Latent Dirichlet Allocation (LDA) for efficient Cloud service discovery. In this paper, we present a Cloud Service Ontology (CSOnt) that defines Cloud service categories. CSOnt contains a set of concepts, allowing the crawler to automatically collect and categorize Cloud services. Moreover, our proposed crawler adopts URLs priority techniques to maintain the order of URLs to be parsed for efficient retrieval of the relevant Cloud services. Additionally, we create a self-adaptive semantic focused crawler, which has an ontology-learning function to automatically improve the proposed Cloud Service Ontology and maintain the crawler's performance. © 2018 Elsevier Inc.","Cloud service ontology; LDA Model; Self-adaptive ontology; Semantic focused crawler; TF-IDF; URLs priority"
"Bridging the gap between awareness and trust in globally distributed software teams","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049453311&doi=10.1016%2fj.jss.2018.06.028&partnerID=40&md5=fd6968a905c265064db476da15d19e1b","Trust remains a key challenge for globally distributed teams despite decades of research. Awareness, a key component of collaboration, has even more research around it. However, detailed accounts of the interrelationship of awareness and trust are still lacking in the literature, particularly in the setting of software teams. The gap we seek to fill with this article is to examine how software tool support for awareness can engender trust among globally distributed software developers. We highlight qualitative results from a previous and extensive field study that shows how trust is still a problem in contemporary teams. These results motivate a specific examination of how developers form attributions of one another. We describe a collection of visualization widgets designed to address the specific issues found in the field. To evaluate their effectiveness, we performed a controlled laboratory study with 28 students and 12 professional software developers who used these visualizations collected into a tool environment called Theseus. The results show that in general, participants using the visualizations make more accurate attributions, and their perceived trustworthiness of their remote teammates more accurately reflects actual circumstances. We conclude with a discussion of the implications of our results for theory and practice. © 2018","Attribution error; Awareness; Empirical studies; Global software engineering; Perceived trustworthiness; Trust"
"Requirements traceability technologies and technology transfer decision support: A systematic review","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053763040&doi=10.1016%2fj.jss.2018.09.001&partnerID=40&md5=aea377c24bd6ad0c6611eea9d3ad370f","Requirements traceability (RT) is a core activity in Requirements Engineering. Various types of RT technologies have been extensively studied for decades. In this paper, we present a systematic literature review from 114 papers between 2006 and 2016 on RT techniques. We summarized 10 major challenges in current RT activities, and categorized existing RT techniques into 6 groups and 25 sub-groups. Moreover, we built mapping relations between these challenges and techniques, and identified 7 potential future research directions. Based on 83 empirical studies, the evaluations for technology transfer are conducted. The main conclusions are: (1) The “trustworthy” and “automated” challenges are the most widely investigated ones, while “scalable” “coordinated” “dynamic” and “lightweight” challenges receive much less attention; (2) “Trace link generation” especially information retrieval-based (IR-based) methods, are the most studied techniques; (3) IR-based methods have the most potential to be adopted by industry, as they have been validated from multiple viewpoints; (4) Seven promising future research directions are identified, which include developing scalable, dynamic and lightweight tracing techniques, introducing new approaches in other disciplines to meet the RT challenges, improving the express ability of trace links, promoting the industry adoption of RT technologies and developing new techniques to support developers’ coordination. © 2018","Quality assessment; Requirements traceability challenges; Requirements traceability technology; Systematic literature review; Technology transfer"
"Parallel construction of interprocedural memory SSA form","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054028683&doi=10.1016%2fj.jss.2018.09.038&partnerID=40&md5=eff822bd0163ddbe3dace881420786ae","Interprocedural memory SSA form, which provides a sparse data-flow representation for indirect memory operations, paves the way for many advanced program analyses. Any performance improvement for memory SSA construction benefits for a wide range of clients (e.g., bug detection and compiler optimisations). However, its construction is much more expensive than that for scalar-based SSA form. The memory objects distinguished at a pointer dereference significantly increases the number of variables that need to be put on SSA form, resulting in considerable analysis overhead when analyzing large programs (e.g., millions of lines of code). This paper presents PARSSA, a fully parameterised approach for parallel construction of interprocedural memory SSA form by utilising multi-core computing resources. PARSSA partitions whole-program memory objects into uniquely identified memory regions. The indirect memory accesses in a function are fully parameterised using partitioned memory regions, so that the memory SSA construction of a parameterised function is readily parallelised. We implemented PARSSA in LLVM using Intel Threading Building Block (TBB) for creating parallel tasks. We evaluated PARSSA using 15 large applications. PARSSA achieves up to 6.9 × speedup against the sequential version on an 8-core machine. © 2018 Elsevier Inc.",""
"Designing and implementing an environment for software start-up education: Patterns and anti-patterns","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052893296&doi=10.1016%2fj.jss.2018.08.060&partnerID=40&md5=e5d9a84360d53764eac6a14fa99cc40d","Today's students are prospective entrepreneurs, as well as potential employees in modern, start-up-like intrapreneurship environments within established companies. In these settings, software development projects face extreme requirements in terms of innovation and attractiveness of the end-product. They also suffer severe consequences of failure such as termination of the development effort and bankruptcy. As the abilities needed in start-ups are not among those traditionally taught in universities, new knowledge and skills are required to prepare students for the volatile environment that new market entrants face. This article reports experiences gained during seven years of teaching start-up knowledge and skills in a higher-education institution. Using a design-based research approach, we have developed the Software Factory, an educational environment for experiential, project-based learning. We offer a collection of patterns and anti-patterns that help educational institutions to design, implement and operate physical environments, curricula and teaching materials, and to plan interventions that may be required for project-based start-up education. © 2018 The Authors","Computer science; Curriculum; Experiential learning; Project-based learning; Software engineering; Start-up education"
"A comparative study of workflow customization strategies: Quality implications for multi-tenant SaaS","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050105246&doi=10.1016%2fj.jss.2018.07.014&partnerID=40&md5=c4021b9ea78fbebfef478b4a5fcebc15","Multi-tenant Software-as-a-Service (SaaS) applications share a single runtime instance among multiple customer organizations (tenants). To account for differences in tenant requirements, they have to support run-time customization. Run-time customization involves a wide range of software artifacts such as user interfaces, databases, web-services and business process or workflow definitions. This paper analyzes and compares the quality implications of different business process customization strategies for multi-tenant SaaS applications. The customization strategies are selected from an existing survey and the comparison criteria are derived from two essential characteristics of SaaS: it is (i) a business model aiming at “economies of scale” and (ii) a software delivery model with specific automation requirements. The comparative study shows that there is no single best strategy, and provides SaaS architects with support for making appropriate trade-off decisions when adopting a workflow customization strategy. As a by-product of this study, a number of points for future improvement and innovations in existing workflow technology are identified, which are exclusively relevant for multi-tenant SaaS applications. © 2018 Elsevier Inc.","Functional customization; Multi-tenancy; Software quality; Software-as-a-Service; Workflow automation"
"A systematic mapping study on text analysis techniques in software architecture","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050799006&doi=10.1016%2fj.jss.2018.07.055&partnerID=40&md5=a57aa7272d46e146eaf79ed3686bff2b","Context: Information from artifacts in each phase of the software development life cycle can potentially be mined to enhance architectural knowledge. Many text analysis techniques have been proposed for mining such artifacts. However, there is no comprehensive understanding of what artifacts these text analysis techniques analyze, what information they are able to extract or how they enhance architecting activities. Objective: This systematic mapping study aims to study text analysis techniques for mining architecture-related artifacts and how these techniques have been used, and to identify the benefits and limitations of these techniques and tools with respect to enhancing architecting activities. Method: We conducted a systematic mapping study and defined five research questions. We analyzed the results using descriptive statistics and qualitative analysis methods. Results: Fifty-five studies were finally selected with the following results: (1) Current text analysis research emphasizes on architectural understanding and recovery. (2) A spectrum of text analysis techniques have been used in textual architecture information analysis. (3) Five categories of benefits and three categories of limitations were identified. Conclusions: This study shows a steady interest in textual architecture information analysis. The results give clues for future research directions on improving architecture practice through using these text analysis techniques. © 2018 Elsevier Inc.","Software architecture; Systematic mapping study; Text analysis technique"
"On the refinement of spreadsheet smells by means of structure information","2019","Journal of Systems and Software","10.1016/j.jss.2018.09.092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054566503&doi=10.1016%2fj.jss.2018.09.092&partnerID=40&md5=2e30bad732af459cb5f60ce2e9b003d7","Spreadsheet users are often unaware of the risks imposed by poorly designed spreadsheets. One way to assess spreadsheet quality is to detect smells which attempt to identify parts of spreadsheets that are hard to comprehend or maintain and which are more likely to be the root source of bugs. Unfortunately, current spreadsheet smell detection techniques suffer from a number of drawbacks that lead to incorrect or redundant smell reports. For example, the same quality issue is often reported for every copy of a cell, which may overwhelm users. To deal with these issues, we propose to refine spreadsheet smells by exploiting inferred structural information for smell detection. We therefore first provide a detailed description of our static analysis approach to infer clusters and blocks of related cells. We then elaborate on how to improve existing smells by providing three example refinements of existing smells that incorporate information about cell groups and computation blocks. Furthermore, we propose three novel smell detection techniques that make use of the inferred spreadsheet structures. Empirical evaluation of the proposed techniques suggests that the refinements successfully reduce the number of incorrectly and redundantly reported smells, and novel deficits are revealed by the newly introduced smells. © 2018 Elsevier Inc.","Code smells; Spreadsheets; Static analysis"
"Mutomvo: Mutation testing framework for simulated cloud and HPC environments","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047830844&doi=10.1016%2fj.jss.2018.05.010&partnerID=40&md5=76c080ee7bf289d2a27d8fc808e95672","Many current applications provide high performance to process large volumes of data. These applications usually run in highly distributed environments, like cloud and HPC systems. Nevertheless, the large and complex architectures required for deploying these applications may not be available during the development phase. This limitation can be overcome by using simulation platforms to model a wide range of distributed system configurations and execute these applications in the modeled system. Usually, these applications are tested against a small number of test cases that are manually designed by the testers. It is desirable to have effective test suites in order to detect failures in the application models. In this paper we propose a mutation testing framework for detecting errors in distributed applications executed in simulated environments. The execution of a test suite against the set of mutated models allows to determine its effectiveness for detecting different errors. The proposal has been implemented in a tool called MuTomVo. In order to support the feasibility of the proposal, we have carried out a case study over three applications running in different distributed systems: a client/server model, intensive computation and scientific pipeline. © 2018 Elsevier Inc.","Distributed systems; Mutation testing; Simulation; Software testing"
"A generic approach to model generation operations","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046452973&doi=10.1016%2fj.jss.2018.04.053&partnerID=40&md5=6c3d299f1504947075a80f44cbfbc3dd","Model generation operations are important artifacts in MDE applications. These approaches can be used for model verification, model finding, and others. In many scenarios, model transformations can as well be represented by a model generation operation. This often comes with the advantage of being bidirectional and supporting increments. However, most part of model generation approaches do not target several operation kinds, but narrower scenarios by mapping the generation problem into solver specific problems. They are efficient, but often don't have a supporting framework. In this paper, we present an approach and framework that allows to specify and to execute model operations that can be represented in terms of model generation operations. We first introduce a model search layer that can be used with different solvers. We illustrate this layer with a driving example implemented using Alloy/SAT solver. On top of this, we introduce a transformation layer, which specification are translated into the model search layer, independently from any solver. The solution is natively bidirectional, incremental and it is not restricted to one-and-one scenarios. The approach is illustrated by two use cases and with 3 different scenarios, backed by a full, extensible and free implementation. © 2018 Elsevier Inc.","Alloy; Model search; Model transformations"
"A systematic literature review on the semi-automatic configuration of extended product lines","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050750643&doi=10.1016%2fj.jss.2018.07.054&partnerID=40&md5=1696f5875fdc07475a8226022ed5d091","Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of techniques used by each work, how these techniques are evaluated and their main shortcomings. As main results, our review identified (i) the need to improve the quality of the evaluation of existing approaches, (ii) a lack of hybrid solutions to support multiple configuration constraints, and (iii) a need to improve scalability and performance conditions. © 2018 Elsevier Inc.","Extended product line; Product configuration; Systematic literature review"
"Execution anomaly detection in large-scale systems through console log analysis","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047805628&doi=10.1016%2fj.jss.2018.05.016&partnerID=40&md5=68f744117133ba6c3e9adef1f108b7ff","Execution anomaly detection is important for development, maintenance and performance tuning in large-scale systems. System console logs are the significant source of troubleshooting and problem diagnosis. However, manually inspecting logs to detect anomalies is unfeasible due to the increasing volume and complexity of log files. Therefore, this is a substantial demand for automatic anomaly detection based on log analysis. In this paper, we propose a general method to mine console logs to detect system problems. We first give some formal definitions of the problem, and then extract the set of log statements in the source code and generate the reachability graph to reveal the reachable relations of log statements. After that, we parse the log files to create log messages by combining information about log statements with information retrieval techniques. These messages are grouped into execution traces according to their execution units. We propose a novel anomaly detection algorithm that considers traces as sequence data and uses a probabilistic suffix tree based method to organize and differentiate significant statistical properties possessed by the sequences. Experiments on a CloudStack testbed and a Hadoop production system show that our method can effectively detect running anomalies in comparison with existing four detection algorithms. © 2018","Control flow analysis; Execution anomaly detection; Log analysis; Trace anomaly index"
"Online model learning for self-aware computing infrastructures","2019","Journal of Systems and Software","10.1016/j.jss.2018.09.089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054354986&doi=10.1016%2fj.jss.2018.09.089&partnerID=40&md5=cef58003202b20e72c1bc1728ed2c301","Performance models are valuable and powerful tools for performance prediction. However, the creation of performance models usually requires significant manual effort. Furthermore, as the modeled structures are subject to frequent change in modern infrastructures, such performance models need to be adapted as well. We therefore propose a reference architecture for online model learning in virtualized environments, which enables the automatic extraction of the aforementioned performance models. We follow an agent-based approach, which enables us to incorporate the extraction of information about the application structure as well as the virtualization structures present in modern computing centers. Our evaluation shows that our collaborating agents are able to reduce the manual effort of performance model extraction by 85.4%. The resulting performance model is able to predict the system utilization with an absolute error of less than 4% and the end-to-end response time with a relative error of less than 21%. © 2018 Elsevier Inc.","Model extraction; Model learning; Performance model; Self-aware computing"
"The exception handling riddle: An empirical study on the Android API","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047071722&doi=10.1016%2fj.jss.2018.04.034&partnerID=40&md5=a82392373e54b591e725cb343a07073c","We examine the use of the Java exception types in the Android platform's Application Programming Interface (API) reference documentation and their impact on the stability of Android applications. We develop a method that automatically assesses an API's quality regarding the exceptions listed in the API's documentation. We statically analyze ten versions of the Android platform's API (14–23) and 3539 Android applications to determine inconsistencies between exceptions that analysis can find in the source code and exceptions that are documented. We cross-check the analysis of the Android platform's API and applications with crash data from 901,274 application execution failures (crashes). We discover that almost 10% of the undocumented exceptions that static analysis can find in the Android platform's API source code manifest themselves in crashes. Additionally, we observe that 38% of the undocumented exceptions that developers use in their client applications to handle API methods also manifest themselves in crashes. These findings argue for documenting known might-thrown exceptions that lead to execution failures. However, a randomized controlled trial we run shows that relevant documentation improvements are ineffective and that making such exceptions checked is a more effective way for improving applications’ stability. © 2018 Elsevier Inc.","Application programming interfaces; Documentation; Exceptions"
"Adapting agile practices in university contexts","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050741829&doi=10.1016%2fj.jss.2018.07.011&partnerID=40&md5=548f645ad8ab3a0e0d5b11e45c5dee6a","Teaching agile practices has found its place in software engineering curricula in many universities across the globe. As a result, educators and students have embraced different ways to apply agile practices during their courses through lectures, games, projects, workshops and more for effective theoretical and practical learning. Practicing agile in university contexts comes with challenges for students and to counter these challenges, they perform some adaptations to standard agile practices making them effective and easier to use in university contexts. This study describes the constraints the students faced while applying agile practices in a university course taught at the University of Auckland, including difficulty in setting up common time for all team members to work together, limited availability of customer due to busy schedule and the modifications the students introduced to adapt agile practices to suit the university context, such as daily stand-ups with reduced frequency, combining sprint meetings, and rotating scrum master from team. In addition, it summarizes the effectiveness of these modifications based on reflection of the students. Recommendations for educators and students are also provided. Our findings and recommendations will help educators and students better coordinate and apply agile practices on industry-based projects in university contexts. © 2018","Adapting; Agile practices; Agile software development; Contextualization; Teaching; University"
"Specifying uncertainty in use case models","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050809144&doi=10.1016%2fj.jss.2018.06.075&partnerID=40&md5=50f56e02a4fdfaa20f30962df6aeebd7","Context: Latent uncertainty in the context of software-intensive systems (e.g., Cyber-Physical Systems (CPSs)) demands explicit attention right from the start of development. Use case modeling—a commonly used method for specifying requirements in practice, should also be extended for explicitly specifying uncertainty. Objective: Since uncertainty is a common phenomenon in requirements engineering, it is best to address it explicitly by identifying, qualifying, and, where possible, quantifying uncertainty at the beginning stage. The ultimate aim, though not within the scope of this paper, was to use these use cases as the starting point to create test-ready models to support automated testing of CPSs under uncertainty. Method: We extend the Restricted Use Case Modeling (RUCM) methodology and its supporting tool to specify uncertainty as part of system requirements. Such uncertainties include those caused by insufficient domain expertise of stakeholders, disagreements among them, and known uncertainties about assumptions about the environment of the system. The extended RUCM, called U-RUCM, inherits the features of RUCM, such as automated analyses and generation of models, to mention but a few. Consequently, U-RUCM provides all the key benefits offered by RUCM (i.e., reducing ambiguities in requirements), but also, it allows specification of uncertainties with the possibilities of reasoning and refining existing ones and even uncovering unknown ones. Results: We evaluated U-RUCM with two industrial CPS case studies. After refining RUCM models (specifying initial requirements), by applying the U-RUCM methodology, we successfully identified and specified additional 306% and 512% (previously unknown) uncertainty requirements, as compared to the initial requirements specified in RUCM. This showed that, with U-RUCM, we were able to get a significantly better and more precise characterization of uncertainties in requirement engineering. Conclusion: Evaluation results show that U-RUCM is an effective methodology (with tool support) for dealing with uncertainty in requirements engineering. We present our experience, lessons learned, and future challenges, based on the two industrial case studies. © 2018 Elsevier Inc.","Belief; Uncertainty; Use case modeling"
"Enhancing change prediction models using developer-related factors","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047066686&doi=10.1016%2fj.jss.2018.05.003&partnerID=40&md5=6e8b7b0cd196beaff8a211c160329761","Continuous changes applied during software maintenance risk to deteriorate the structure of a system and are a threat to its maintainability. In this context, predicting the portions of source code where specific maintenance operations should be focused on may be crucial for developers to prevent maintainability issues. Previous work proposed change prediction models relying on product and process metrics as predictors of change-prone source code classes. However, we believe that existing approaches still miss an important piece of information, i.e., developer-related factors that are able to capture the complexity of the development process under different perspectives. In this paper, we firstly investigate three change prediction models that exploit developer-related factors (e.g., number of developers working on a class) as predictors of change-proneness of classes and then we compare them with existing models. Our findings reveal that these factors improve the capabilities of change prediction models. Moreover, we observed interesting complementarities among the prediction models. For this reason, we devised a novel change prediction model exploiting the combination of developer-related factors and product and evolution metrics. The results show that such a combined model is up to 22% more effective than the single models in the identification of change-prone classes. © 2018 Elsevier Inc.","Change prediction; Empirical study; Mining software repositories"
"Improving reusability of software libraries through usage pattern mining","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052331660&doi=10.1016%2fj.jss.2018.08.032&partnerID=40&md5=7c0289c5437875dae5fce4b41e59df2a","Modern software systems are increasingly dependent on third-party libraries. It is widely recognized that using mature and well-tested third-party libraries can improve developers’ productivity, reduce time-to-market, and produce more reliable software. Today's open-source repositories provide a wide range of libraries that can be freely downloaded and used. However, as software libraries are documented separately but intended to be used together, developers are unlikely to fully take advantage of these reuse opportunities. In this paper, we present a novel approach to automatically identify third-party library usage patterns, i.e., collections of libraries that are commonly used together by developers. Our approach employs a hierarchical clustering technique to group together software libraries based on external client usage. To evaluate our approach, we mined a large set of over 6000 popular libraries from Maven Central Repository and investigated their usage by over 38,000 client systems from the Github repository. Our experiments show that our technique is able to detect the majority (77%) of highly consistent and cohesive library usage patterns across a considerable number of client systems. © 2018","Clustering; Software libraries; Software reuse; Usage patterns"
"Open meta-modelling frameworks via meta-object protocols","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051012076&doi=10.1016%2fj.jss.2018.07.023&partnerID=40&md5=96b9110adfdf8ea776cf438643fe1478","Meta-modelling is central to Model-Driven Engineering. Many meta-modelling notations, approaches and tools have been proposed along the years, which widely vary regarding their supported modelling features. However, current approaches tend to be closed and rigid with respect to the supported concepts and semantics. Moreover, extending the environment with features beyond those natively supported requires highly technical knowledge. This situation hampers flexibility and interoperability of meta-modelling environments. In order to alleviate this situation, we propose open meta-modelling frameworks, which can be extended and configured via meta-object protocols (MOPs). Such environments offer extension points on events like element instantiation, model loading or property access, and enable selecting particular model elements over which the extensions are to be executed. We show how MOP-based mechanisms permit extending meta-modelling frameworks in a flexible way, and allow describing a wide range of meta-modelling concepts. As a proof of concept, we show and compare an implementation in the METADEPTH tool and an aspect-based implementation atop the Eclipse Modelling Framework (EMF). We have evaluated our approach by extending EMF and METADEPTH with modelling services not foreseen initially when they were created. The evaluation shows that MOP-based mechanisms permit extending meta-modelling frameworks in a flexible way, and are powerful enough to support the specification of a broad variety of meta-modelling features. © 2018 Elsevier Inc.","Aspect orientation; Extensibility; Flexible meta-modelling; Meta-object protocol; Model-Driven Engineering; Multi-level modelling"
"How does docker affect energy consumption? Evaluating workloads in and out of Docker containers","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053352568&doi=10.1016%2fj.jss.2018.07.077&partnerID=40&md5=a866674254e6fe1a8729a6ef3fd1a1fc","Context: Virtual machines provide isolation of services at the cost of hypervisors and more resource usage. This spurred the growth of systems like Docker that enable single hosts to isolate several applications, similar to VMs, within a low-overhead abstraction called containers. Motivation: Although containers tout low overhead performance, how much do they increase energy use? Methodology: This work statistically compares the energy consumption of three application workloads in Docker and on bare-metal Linux. Results: In all cases, there was a statistically significant (t-test and Wilcoxon p <.05) increase in energy consumption when running tests in Docker, mostly due to the performance of I/O system calls. Developers worried about I/O overhead could consider baremetal deployments over Docker container deployments. © 2018","Cloud computing; Containerization; Docker; Energy consumption; Microservice; Virtualization"
"Perceived importance of agile requirements engineering practices – A survey","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047088466&doi=10.1016%2fj.jss.2018.05.012&partnerID=40&md5=50a5a3afeafd7d32476db742ced2c1ec","Context:Requirements Engineering (RE) is one of the key processes in software development. Since Agile software development advocates continuous improvement, the question arises which Agile RE practices are the most essential and shall be adopted/improved at first? Objective:Investigate and rank the Agile RE practices based on how practitioners perceive their importance for a software project. Method:We conducted a survey asking 136 Agile software development practitioners how they perceive the importance of the 31 Agile RE practices that we had identified in a literature study. We used a ranking method based on the PROMETHEE family methods to create the ranking of relative importance of the practices. Results:The opinions of respondents from a wide range of countries around the globe allowed us to determine the perceived importance of the Agile RE practices and create a seven-tier ranking of the practices. Moreover, the analysis concerning demographic data let us identify some relationships between the experience of the respondents and their view on the importance of the Agile RE practices. Conclusions:Our findings suggest the most critical Agile RE practices are those supporting iterative development with emergent requirements and short feedback loop. Moreover, in many cases, the perceived importance of practices seems to depend on the context of the project (e.g., methodology, domain). We also learned that the popularity of the practices is highly correlated with their perceived importance. © 2018","Agile; Importance; Practices; Requirements engineering; Survey study"
"Early validation of system requirements and design through correctness-by-construction","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051117215&doi=10.1016%2fj.jss.2018.07.053&partnerID=40&md5=0aad1023612f203e6a19662c15eceeb1","Early validation of requirements aims to reduce the need for the high-cost validation testing and corrective measures at late development stages. This work introduces a systematic process for the unambiguous specification of system requirements and the guided derivation of formal properties, which should be implied by the system ’s structure and behavior in conjunction with its external stimuli. This rigorous design takes place through the incremental construction of a model using the BIP (Behavior-Interaction-Priorities) component framework. It allows building complex designs by composing simpler reusable designs enforcing given properties. If some properties are neither enforced nor verified, the model is refined or certain requirements are revised. A validated model provides evidence of requirements’ consistency and design correctness. The process is semi-automated through a new tool and existing verification tools. Its effectiveness was evaluated on a set of requirements for the control software of the CubETH nanosatellite and an extract of software requirements for a Low Earth Orbit observation satellite. Our experience and obtained results helped in identifying open challenges for applying the method in industrial context. These challenges concern with the domain knowledge representation, the expressiveness of used specification languages, the library of reusable designs and scalability. © 2018","Correctness-by-construction; Model-based design; Requirements formalization; Rigorous system design"
"A general method for rendering static analyses for diverse concurrency models modular","2019","Journal of Systems and Software","10.1016/j.jss.2018.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054446475&doi=10.1016%2fj.jss.2018.10.001&partnerID=40&md5=f8f15eb58800d538d6ba25efedf5755f","Shared-memory multi-threading and the actor model both share the notion of processes featuring communication, respectively by modifying shared state and by sending messages. Existing static analyses for concurrent programs either model every possible process interleavings and therefore suffer from the state explosion problem, or feature modularity but lack in precision or in their support for dynamic processes. In this paper we present a general method for obtaining a scalable analysis of concurrent programs featuring dynamic process creation. Our MODCONC method transforms an abstract concurrent semantics modeling processes and communication into a modular static analysis treating the behavior of processes separately from their communication. We present MODCONC in a generic way and demonstrate its applicability by instantiating it for multi-threaded and actor-based programs. The resulting analyses are evaluated in terms of precision, performance, scalability, and soundness. While a typical non-modular static analysis time out on half of our 56 benchmarks with a 30 min timeout, MODCONC analyses successfully analyze all of them in less than 30 s, while remaining on par in terms of precision. Analyzing concurrent processes in isolation while modeling their communications is the key ingredient in supporting scalable analysis of concurrent programs featuring dynamic process communication. © 2018","Actors; Concurrency; Modular analysis; Static analysis; Threads"
"A prediction-Based VM consolidation approach in IaaS Cloud Data Centers","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054305744&doi=10.1016%2fj.jss.2018.09.083&partnerID=40&md5=17345af110f848f291192c1fd20a5d2b","Recent years have witnessed a rapid growth in exploiting Cloud environments to host and deliver various types of virtualized resources as on-demand services. In order to optimally use Cloud resources, the arrangement of virtual machines (VMs) in physical machines (PMs) must be performed strategically, because the placement of VMs in accordance with the available resources can reduce energy consumption, improve resource utilization and, consequently, can increase companies benefits. However, VMs could have time varying workloads, which leads to degradation of performance and power consumption. Thus, re-configuring the VMs placement is essential. Virtual machine consolidation aims to optimally use the available resources by allocating several virtual machines on a set of physical ones (PMs). To determine the PMs capacities to reallocate VMs, it is important to predict their states based on resource utilization history within each VM, and the past VMs migration traffic. However, a common limitation between existing VM consolidation approaches is the lack of information about the history of (and the future) VM migration traffic. Through this paper, we aim to propose a virtual machine consolidation approach based on the estimation of requested resources and the future VM migration traffic. We exploit the strength of Kernel Density Estimation technique (KDE) as a powerful mean to forecast the future resource usage of each VM, and AKKA toolkit as an actor-based model that allows exchanging useful information about the host's states. We adopt a weighted-graph representation to model the history of migration traffic between PMs and to design the actor-based topology of the data center. The obtained results show the effectiveness of our approach in terms of total number of migrations and energy consumption. © 2018","AKKA Toolkit; Cloud computing; Infrastructure as a service; Kernel density estimation; Prediction; Virtual machine consolidation"
"On some end-user programming constructs and their understandability","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046686715&doi=10.1016%2fj.jss.2018.03.064&partnerID=40&md5=daaf950ee624712bf2aa3413e4977cdb","Context: End-user programming is becoming more and more important. However, existing programming paradigms and the languages based on them seem far-removed from what end-user programmers would need, especially in the area of Management Information. Objective: To evaluate the understandability of a set of programming constructs based on the spreadsheet metaphor from the point of view of end-user programmers in the context of Management Information. The examined set comprises single assignment with exemplary computations, data-driven iterations, selection by colours, and read-write heads (we refer to this as Board Programming). Method: A series of experiments was performed with students of Management Engineering, split into an experimental group and a control group. Each participant was given a piece of code expressed either with the proposed programming construct (experimental group) or its classical counterpart (control group). Their task was to predict the results. For the purpose of evaluation, the FACT indicators of understandability (First attempt failure rate, Attempt number, Cancellation ratio, prediction Time) were proposed and measured. Results: Three of the four examined features, i.e. single assignment with exemplary computations, data-driven iterations, and read-write heads, proved to increase understandability of the chosen programs with regard to three out of the four FACT indicators, and these results were statistically significant. Selection by colours was not as effective as expected: the FACT indicator values were improved by that feature, but the difference was not statistically significant. Conclusions: The described programming constructs appear to be an interesting option when designing an end-user programming language for domain experts in the field of Management Information. © 2018 Elsevier Inc.","Code understanding; End-user programming; Spreadsheets"
"Past and future of software architectures for context-aware systems: A systematic mapping study","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055048145&doi=10.1016%2fj.jss.2018.09.074&partnerID=40&md5=7f749b224d731c3071fcf725dd4b73da","There is a growing interest on context-aware systems in recent years. Context-aware systems are able to change their behaviour depending on new conditions regarding the user, the platform and the environment. These systems are evolving towards interacting with the user in a transparent and ubiquitous manner, especially by means of different types of sensors, which can gather a wide range of data from the user, the platform the user is interacting with, and the environment where such interaction is taking place. It is worth noting that the software architecture of a system is a key artefact during its development and its adaptation process. Hence, the definition of the software architecture becomes essential while developing context-aware systems since it should reflect how the context is tackled for adaptation purposes. With the aim of studying this issue, we have designed and conducted a systematic mapping study to provide an overview about the different architectural approaches used in context-aware systems. One of the main findings of this study is that there are not many software architecture proposals that deal with context-awareness in an explicit way during the adaptation process. It was also detected that there are Human Computer Interaction (HCI) works that focus on context-aware adaptations but neglect partially or completely any possible change in the system architecture during the adaptation process. Due to this, we perceived a need to analyse what research works highlight the use of context and its relationship to the software architecture in existing context-aware systems. Therefore, this mapping study attempts to bridge the gap between Software Architecture and HCI in order to align the adaptation at the architectural level (changes in the configuration of architectural components) and at the HCI level (changes in the interaction modality or the user interface in general). © 2018","Environment; Human-computer interaction; Platform; Quality of adaptation; Quasi-gold standard; User"
"A multi-target approach to estimate software vulnerability characteristics and severity scores","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053813600&doi=10.1016%2fj.jss.2018.09.039&partnerID=40&md5=25c4d80c01b268048ce816968ed79a04","Software vulnerabilities constitute a great risk for the IT community. The specification of the vulnerability characteristics is a crucial procedure, since the characteristics are used as input for a plethora of vulnerability scoring systems. Currently, the determination of the specific characteristics -that represent each vulnerability- is a process that is performed manually by the IT security experts. However, the vulnerability description can be very informative and useful to predict vulnerability characteristics. The primary goal of this research is the enhancement, the acceleration and the support of the manual procedure of the vulnerability characteristic assignment. To achieve this goal, a model, which combines texts analysis and multi-target classification techniques was developed. This model estimates the vulnerability characteristics and subsequently, calculates the vulnerability severity scores from the predicted characteristics. To perform the present research, a dataset that contains 99,091 records from a large -publicly available- vulnerability database was used. The results are encouraging, since they show accuracy in the prediction of the vulnerability characteristics and scores. © 2018 Elsevier Inc.","Information security; Multi-target classification; Software vulnerability; Text analysis"
"An anatomy of requirements engineering in software startups using multi-vocal literature and case survey","2018","Journal of Systems and Software","10.1016/j.jss.2018.08.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053782208&doi=10.1016%2fj.jss.2018.08.059&partnerID=40&md5=ae174250fce698866a8c8cc47ab8053a","Context: Software startups aim to develop innovative products, grow rapidly, and thus become important in the development of economy and jobs. Requirements engineering (RE) is a key process area in software development, but its effects on software startups are unclear. Objective: The main objective of this study was to explore how RE (elicitation, documentation, prioritization and validation) is used in software startups. Method: A multi-vocal literature review (MLR) was used to find scientific and gray literature. In addition, a case survey was employed to gather empirical data to reach this study's objective. Results: In the MLR, 36 primary articles were selected out of 28,643 articles. In the case survey, 80 respondents provided information about software startup cases across the globe. Data analysis revealed that during RE processes, internal sources (e.g., for source), analyses of similar products (e.g., elicitation), uses of informal notes (e.g., for documentation), values to customers, products and stakeholders (e.g., for prioritization) and internal reviews/prototypes (e.g., for validation) were the most used techniques. Conclusion: After an analysis of primary literature, it was concluded that research on this topic is still in early stages and more systematic research is needed. Furthermore, few topics were suggested for future research. © 2018 Elsevier Inc.","Case survey; Multi-vocal literature review; Requirements engineering; Software startups"
"A novel TOPSIS evaluation scheme for cloud service trustworthiness combining objective and subjective aspects","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047091723&doi=10.1016%2fj.jss.2018.05.004&partnerID=40&md5=5b6cc32229c4febd5f9b316dfd1f127c","Cloud computing has been paid more attention due to its outstanding advantages. However, trust issue greatly affects the adoption of cloud services. Selecting trustworthy cloud service from those with same functionality but different qualities has become a significant challenge. Since trustworthiness evaluation is multi-dimensional, how to assign weight for each influence factor is a non-trivial problem. In this paper, we put forward a novel TOPSIS evaluation scheme for cloud service trustworthiness combining objective and subjective aspects. First, we consider the objectivity of cloud services from two facets. For one thing, we concern the reliability of QoS information source and utilize monitored values of QoS attributes rather than feedback ratings from consumers. For another, we employ objective entropy weight for different QoS attributes to decrease the effect of false or artificial parameter information. Second, we introduce trust preference that reflects the subjectivity of trust. Most important of all, we propose the combined weight integrating the two aspects and apply it to TOPSIS method to develop a novel evaluation scheme. The results of two experiments based on the existing QWS dataset from real Web services demonstrate its feasibility, effectiveness, and better Satisfaction Degree from the perspective of consumers. © 2018","Cloud service trustworthiness; QoS; TOPSIS; Trust; Trust preference"
"Retest test selection for product-line regression testing of variants and versions of variants","2019","Journal of Systems and Software","10.1016/j.jss.2018.09.090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054466985&doi=10.1016%2fj.jss.2018.09.090&partnerID=40&md5=adf5898b1db7c7e48730b683ba87ff81","Testing is a crucial activity of product-line engineering. Due to shared commonality, testing each variant individually results in redundant testing processes. By adopting regression testing strategies, variants are tested incrementally by focusing on the variability between variants to reduce the overall testing effort. However, product lines evolve during their life-cycle to adapt, e.g., to changing requirements. Hence, quality assurance has also to be ensured after product-line evolution by efficiently testing respective versions of variants. In this paper, we propose retest test selection for product-line regression testing of variants and versions of variants. Based on delta-oriented test modeling, we capture the commonality and variability of an evolving product line by means of differences between variants and versions of variants. We exploit those differences to apply change impact analyses, where we reason about changed dependencies to be retested when stepping from a variant or a version of a variant to its subsequent one by selecting test cases for reexecution. We prototypically implemented our approach and evaluated its effectiveness and efficiency by means of two evolving product lines showing positive results. © 2018 Elsevier Inc.","Regression testing; Software evolution; Software product lines"
"A new Prefetching-aware Data Replication to decrease access latency in cloud environment","2018","Journal of Systems and Software","10.1016/j.jss.2018.05.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048965855&doi=10.1016%2fj.jss.2018.05.027&partnerID=40&md5=1293e6934fb49b56c4dba478038bd9be","Data replication is an effective technique that decreases retrieval time, thus reducing energy consumption in Cloud. When necessary files aren't locally available, they will be fetched from remote locations that is very high-time consuming process. Therefore, it is superior to pre-replicate the popular files. Even though few previous works considered prediction-based replication strategy, the prediction is not precise at many situations and occupies the storage. To address these challenges, a new dynamic replication strategy called Prefetching-aware Data Replication (PDR) is proposed, which determines the correlation of the data files using the file access history and pre-fetches the most popular files. So, the next time that this site requires a file, it will be locally available. In addition, due to the storage space restriction, replica replacement strategy plays a vital role. PDR strategy can ascertain the importance of valuable replicas based on the fuzzy inference system with four input parameters (i.e., number of accesses, cost of replica, the last time the replica was accessed, and data availability). Extensive experiments with CloudSim show that PDR achieves high data availability, high hit ratio, low storage and bandwidth consumption. On average PDR reduces over 35% of response time when compared to the other algorithms. © 2018 Elsevier Inc.","Cloud computing; Data replication; Fuzzy logic; Simulation"
"What's in a GitHub Star? Understanding Repository Starring Practices in a Social Coding Platform","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053752644&doi=10.1016%2fj.jss.2018.09.016&partnerID=40&md5=539ba2003efbac792b873a3607d448f4","Besides a git-based version control system, GitHub integrates several social coding features. Particularly, GitHub users can star a repository, presumably to manifest interest or satisfaction with an open source project. However, the real and practical meaning of starring a project was never the subject of an in-depth and well-founded empirical investigation. Therefore, we provide in this paper a throughout study on the meaning, characteristics, and dynamic growth of GitHub stars. First, by surveying 791 developers, we report that three out of four developers consider the number of stars before using or contributing to a GitHub project. Then, we report a quantitative analysis on the characteristics of the top-5,000 most starred GitHub repositories. We propose four patterns to describe stars growth, which are derived after clustering the time series representing the number of stars of the studied repositories; we also reveal the perception of 115 developers about these growth patterns. To conclude, we provide a list of recommendations to open source project managers (e.g., on the importance of social media promotion) and to GitHub users and Software Engineering researchers (e.g., on the risks faced when selecting projects by GitHub stars). © 2018 Elsevier Inc.","GitHub stars; Social coding; Software popularity"
"A novel dynamic resource adjustment architecture for virtual tenant networks in SDN","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047765690&doi=10.1016%2fj.jss.2018.04.033&partnerID=40&md5=911e5957f5d5b9d416d244964c72134d","This work proposes a novel dynamic resource adjustment architecture that includes a routing planning mechanism and a bandwidth resource planning mechanism for a virtual tenant network (VTN) with software-defined networking (SDN). The routing planning mechanism helps to assign the routing paths in a physical network to solve the problem of their uneven distribution, with the ultimate goal of improving the utilization of the network routing. The bandwidth resource planning mechanism helps to manage the tenant demand for bandwidth resources to solve the problem of an excessive bandwidth requirement and thereby to improve the utilization of network bandwidth resources. The proposed architecture helps not only to plan routing paths in a physical network to satisfy a VTN user request but also to guarantee bandwidth usage based on overall network conditions. Analytical results indicate that the proposed routing planning mechanism increases the efficiency of routing assignment and that the proposed bandwidth resource scheduling increases bandwidth utilization by 10.92%. © 2018 Elsevier Inc.","Network virtualization; Resource scheduling; Routing path; Service level agreement; Software-defined networking; Virtual tenant network"
"Introducing TRAILS: A tool supporting traceability, integration and visualisation of engineering knowledge for product service systems development","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049903342&doi=10.1016%2fj.jss.2018.06.079&partnerID=40&md5=7ae20e4543e1b027c1eeaa8f378433db","Developing state of the art product service systems (PSS) requires the intense collaboration of different engineering domains, such as mechanical, software and service engineering. This can be a challenging task, since each engineering domain uses their own specification artefacts, software tools and data formats. However, to be able to seamlessly integrate the various components that constitute a PSS and also being able to provide comprehensive traceability throughout the entire solution life cycle it is essential to have a common representation of engineering data. To address this issue, we present TRAILS, a novel software tool that joins the heterogeneous artefacts, such as process models, requirements specifications or diagrams of the systems structure. For this purpose, our tool uses a semantic model integration ontology onto which various source formats can be mapped. Overall, our tool provides a wide range of features that supports engineers in ensuring traceability, avoiding system inconsistencies and putting collaborative engineering into practice. Subsequently, we show the practical implementation of our approach using the case study of a bike sharing system and discuss limitations as well as possibilities for future enhancement of TRAILS. © 2018 Elsevier Inc.","Model integration; Model-based systems engineering; Product service systems; Traceability"
"Genetic algorithm for energy-efficient clustering and routing in wireless sensor networks","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054181611&doi=10.1016%2fj.jss.2018.09.067&partnerID=40&md5=688602aac9c95870318ce27bf11d0a52","Wireless sensor networks have been employed widely in various fields, including military, health care, and manufacturing applications. However, the sensor nodes are limited in terms of their energy supply, storage capability, and computational power. Thus, in order to improve the energy efficiency and prolong the network life cycle, we present a genetic algorithm-based energy-efficient clustering and routing approach GECR. We add the optimal solution obtained in the previous network round to the initial population for the current round, thereby improving the search efficiency. In addition, the clustering and routing scheme are combined into a single chromosome to calculate the total energy consumption. We construct the fitness function directly based on the total energy consumption thereby improving the energy efficiency. Moreover, load balancing is considered when constructing the fitness function. Thus, the energy consumption among the nodes can be balanced. The experimental results demonstrated that the GECR performed better than other five methods. The GECR achieved the best load balancing with the lowest variances in the loads on the cluster heads under different scenarios. In addition, the GECR was the most energy-efficient with the lowest average energy consumed by the cluster heads and the lowest energy consumed by all the nodes. © 2018 Elsevier Inc.","Clustering algorithm; Energy-efficiency; Genetic algorithm; Network life cycle; Routing; Wireless sensor networks"
"The pains and gains of microservices: A Systematic grey literature review","2018","Journal of Systems and Software","10.1016/j.jss.2018.09.082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054175054&doi=10.1016%2fj.jss.2018.09.082&partnerID=40&md5=fa91692f126bd35cc53bca9866c6583f","The design, development, and operation of microservices are picking up more and more momentum in the IT industry. At the same time, academic work on the topic is at an early stage, and still on the way to distilling the actual “Pains & Gains” of microservices as an architectural style. Having witnessed this gap, we set forth to systematically analyze the industrial grey literature on microservices, to identify the technical/operational pains and gains of the microservice-based architectural style. We conclude by discussing research directions stemming out from our analysis. © 2018 Elsevier Inc.","Microservices; Microservices design; Microservices development; Microservices operation; Systematic grey literature review; Systematic literature review"
"Quality of software requirements specification in agile projects: A cross-case analysis of six companies","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046681152&doi=10.1016%2fj.jss.2018.04.064&partnerID=40&md5=14e8f2ea07d3b4be4141066baabb83f1","Agile Software Development (ASD) has several limitations concerning its requirements engineering activities. Improving the quality of Software Requirements Specifications (SRSs) in ASD may help to gain a competitive advantage in the software industry. Based on the findings of a Systematic Mapping study, six industrial case studies in different contexts were conducted to investigate and characterize the requirements specification activity in ASD. Data collected from documents, observations, and interviews with software engineers were triangulated, analyzed, and synthesized using Grounded Theory and Meta-Ethnography. The analysis and cross-synthesis of the six case studies resulted in a model describing the phenomenon. This model defines the simplicity and objectivity as essential quality factors of SRSs in ASD. The main factors that affect the SRSs quality in ASD projects are related to their customer-driven nature that leads to prolix SRSs, hindering its understanding from the developer perspective. The emerged model is supported by explanations and provides a deeper understanding of the requirements specification activity in ASD. This creates opportunities for further studies and improvements in SRSs for ASD in industry. © 2018 Elsevier Inc.","Agile methods; Agile Requirements Engineering; Empirical study; Requirements specification"
"Software startup engineering: A systematic mapping study","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049306962&doi=10.1016%2fj.jss.2018.06.043&partnerID=40&md5=380a8dee56280f316a3048472b6b01c1","Software startups have long been a significant driver in economic growth and innovation. The on-going failure of the major number of startups calls for a better understanding of state-of-the-practice of startup activities. Objective With a focus on engineering perspective, this study aims at identifying the change in focus of research area and thematic concepts operating startup research. A systematic mapping study on 74 primary papers (in which 27 papers are newly selected) from 1994 to 2017 was conducted with a comparison with findings from previous mapping studies. A classification schema was developed, and the primary studies were ranked according to their rigour. We discovered that most research has been conducted within the SWEBOK knowledge areas software engineering process, management, construction, design, and requirements, with the shift of focus towards process and management areas. We also provide an alternative classification for future startup research. We find that the rigour of the primary papers was assessed to be higher between 2013–2017 than that of 1994–2013. We also find an inconsistency of characterizing startups. Future work can focus on certain research themes, such as startup evolution models and human aspects, and consolidate the thematic concepts describing software startups. © 2018","Software development; Software engineering; Software startup; Startup; Systematic mapping study"
"Cross lifecycle variability analysis: Utilizing requirements and testing artifacts","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048478062&doi=10.1016%2fj.jss.2018.04.062&partnerID=40&md5=4ba2494886c221bd1d2d991644a7f560","Variability analysis is an essential activity that supports increasing and systemizing reuse across similar software products. Current studies use different types of artifacts for analyzing variability, most notably are architecture or design, requirements, and code. While architecture, design, and code help understand and model the differences in solutions and realizations, requirements enable capturing differences in a higher level of abstraction through the intended use of the software products or their behavior. However, analyzing variability based on requirements may result in inaccurate outcomes, due to the informal and incomplete nature of requirements. To tackle this deficiency, we call for augmenting requirements-based variability analysis with other behavior-related cross-lifecycle artifacts. Particularly, we extend an approach that compares and analyzes software behaviors based on requirements taking into account both ontological and semantic considerations. Using test cases and their relations to requirements, our extension, named SOVA R-TC, extract software behaviors more comprehensively, including their preconditions, post-conditions, and expected results. The outputs of SOVA R-TC are feature diagrams, which group similar behaviors and present variability of software products in a tree structure. Empirically evaluating outcomes of SOVA R-TC, they seem to be perceived as significantly better than outcomes generated based on requirements only. © 2018 Elsevier Inc.","Application lifecycle management; Ontology; Software product lines; Software reuse; Variability analysis"
"Artifact-based vs. human-perceived understandability and modifiability of refactored business processes: An experiment","2018","Journal of Systems and Software","10.1016/j.jss.2018.06.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048821588&doi=10.1016%2fj.jss.2018.06.026&partnerID=40&md5=863eacfbd771d0cc4dd7e2b7d6ff9e22","Business processes modeling has proven to be effective and reverse engineering techniques with which to recover business process models when they are missing or outmoded have therefore emerged. Regrettably, these techniques often lead to models with quality flaws and consequently to models with low levels of understandability and modifiability. Refactoring has been widely used to deal with such flaws, altering the internal structure of models while preserving their semantics. There are several studies concerning how understandability and modifiability are affected by refactoring in terms of several artifact-based measures. However, there is little evidence regarding how refactoring affects quality in terms of human-perceived measures. The goals of this paper are, therefore: to collect further empirical evidence about the influence of refactoring on understandability and modifiability of business process models and to investigate the correlation between artifact-based understandability and modifiability and human-perceived ones. The obtained results are not trivial and show that business process obtained by means of reverse engineering has recurrent quality flaws, and the understandability and modifiability of business process models cannot be assessed by using artifact-based measures only. Human-perceived measures need to be taken in to consideration in order to have a more accurate evaluation. © 2018","Business process; Controlled experiment; Empirical study; Modifiability; Refactoring; Understandability"
"Evaluations of JaguarCode: A web-based object-oriented programming environment with static and dynamic visualization","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051937366&doi=10.1016%2fj.jss.2018.07.037&partnerID=40&md5=4742d6b5ca9b7abfe5ec5748ce851abd","To increase program comprehension and overcome learning obstacles of Object-Oriented Programming (OOP), various visualization techniques have been adopted in educational OOP environments. Some provide software development with visual notations without source code, while others support programming with visual aids. Our research supports Java programming along with UML diagrams (class, object, and sequence) and dynamic execution traces of program synchronized in a web-based programming environment - JaguarCode. It aims to help students better understand the static structure and dynamic behavior of Java programs, as well as object-oriented design concepts. This paper reports on the evaluation results of JaguarCode to investigate its effectiveness and user satisfaction through quantitative and qualitative experiments. The experimental results revealed that having both static and dynamic visualizations did positively impact the correctness of program understanding and tracing problems, while the visual representations did affect students’ understanding on the program execution of the problems to higher accuracy. It was also observed that students were satisfied with the aspects of those visualizations provided in JaguarCode. © 2018 Elsevier Inc.","Object-oriented; Programming environment; Static and dynamic visualization; UML diagram; Web-based"
"Applying a maturity model during a software engineering course – How planning and task-solving processes influence the course performance","2018","Journal of Systems and Software","10.1016/j.jss.2018.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050099483&doi=10.1016%2fj.jss.2018.07.009&partnerID=40&md5=c2fdbeae233ae26c79b5008fae4e06bb","In industry, the benefit of maturity models is uncontested, and models like CMMI are normally taught in at least advanced software engineering courses. However, when not being part of real-world projects, the added values are difficult to be experienced on first hand by our students. In this paper we report on a study and teaching approach where, in three successive semesters and at two different institutions, we started rating the process-maturity of students solving tasks in our software engineering courses and transparently related the maturity levels to the task performances. It turned out that not only the quality of the students’ task preparation plays a crucial role, but that there is also a non-negligible correlation between the individual process maturity and performances. Considering this finding, the approach might yield to students’ process-improvement steps during our courses, help in fostering the understanding of the term process maturity, and finally, also might help in improving the overall students’ performances. © 2018 Elsevier Inc.","Maturity model; Software Engineering Education; Teaching methodology"
"Efficient modeling and optimizing of checkpointing in concurrent component-based software systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041461926&doi=10.1016%2fj.jss.2018.01.032&partnerID=40&md5=bba132375f9033216923631ef13cc33a","A common mechanism to improve availability and performance is checkpointing and rollback. When it is time to checkpoint, a system stores a job's state to nonvolatile memory, and, when a failure occurs, it rolls back to the latest stored state instead of restarting the job from the beginning, thus improving performance in the presence of failures. Too frequent checkpointing reduces the amount of work to be redone in case of failures but generates excessive overhead, degrading performance. This paper presents a novel and very efficient queuing network model that addresses software component contention for hardware resources and shows how it can be used to model checkpointing in heterogeneous component-based software systems. We validated this model against a previous model, developed by the authors, that used Markov Chains. Our new model is orders of magnitude faster than the previous one and can be used to plan for checkpointing at run-time. As an additional contribution of this paper, we present an optimizer to find, for each software component, the optimal checkpointing interval that minimizes execution time, maximizes availability, or minimizes checkpointing overhead. © 2018 Elsevier Inc.","Checkpointing; Concurrent and heterogeneous component-based software systems; Markov Chains; Mean value analysis; Performance modeling"
"Empirical validation of cyber-foraging architectural tactics for surrogate provisioning","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039452421&doi=10.1016%2fj.jss.2017.11.047&partnerID=40&md5=0f51355aee06ff57b3e55ce2fd46d9c2","Background Cyber-foraging architectural tactics are used to build mobile applications that leverage proximate, intermediate cloud surrogates for computation offload and data staging. Compared to direct access to cloud resources, the use of intermediate surrogates improves system qualities such as response time, energy efficiency, and resilience. However, the state-of-the-art mostly focuses on introducing new architectural tactics rather than quantitatively comparing the existing tactics, which can help software architects and software engineers with new insights on each tactic. Aim Our work aims at empirically evaluating the architectural tactics for surrogate provisioning, specifically with respect to resilience and energy efficiency. Method We follow a systematic experimentation framework to collect relevant data on Static Surrogate Provisioning and Dynamic Surrogate Provisioning tactics. Our experimentation approach can be reused for validation of other cyber-foraging tactics. We perform statistical analysis to support our hypotheses, as compared to baseline measurements with no cyber-foraging tactics deployed. Results Our findings show that Static Surrogate Provisioning tactics provide higher resilience than Dynamic Surrogate Provisioning tactics for runtime environmental changes. Both surrogate provisioning tactics perform with no significant difference with respect to their energy efficiency. We observe that the overhead of the runtime optimization algorithm is similar for both tactic types. Conclusions The presented quantitative evidence on the impact of different tactics empowers software architects and software engineers with the ability to make more conscious design decisions. This contribution, as a starting point, emphasizes the use of quantifiable metrics to make better-informed trade-offs between desired quality attributes. Our next step is to focus on the impact of runtime programmable infrastructure on the quality of cyber-foraging systems. © 2017 Elsevier Inc.","Cyber-foraging; Energy efficiency; Resilience; Software architecture; Software engineering; Software sustainability"
"Combinatorial double auction-based resource allocation mechanism in cloud computing market","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038811417&doi=10.1016%2fj.jss.2017.11.044&partnerID=40&md5=25bf7b956c48a895f1144ef51914cc07","The cloud computing environment may be considered as market for computing and storage resources. Providers rent their available resources in the form of Virtual Machines (VM) and charge the users accordingly. One of the challenges in this market is providing a mechanism for the allocation of resources and their pricing, such that the proper benefit of both users and providers are guaranteed. In this paper, a combinatorial double auction-based market is studied in which a broker performs the allocation of the providers’ VMs according to the users’ requests. The proposed allocation problem is formulated as an integer linear programming model aiming at maximizing the total profit of users and providers. It is proved that the proposed model satisfies the desirable properties including: truthfulness, fairness, economic efficiency and allocation efficiency. Furthermore, due to the high complexity of the proposed model, a heuristic resource allocation algorithm with a quasi linear time complexity is presented. The results of evaluations confirm the good agreement of the heuristic algorithm with the optimization model in terms of allocation performance. Moreover, simulation results using CloudSim indicate that, compared to the previous works in literature, the proposed algorithm increases the profit of providers and users and reduces the resource wastage. © 2017 Elsevier Inc.","Cloud computing; Combinatorial double auction; Resource allocation; Virtual machine (VM)"
"Broadcast tree construction framework in tactile internet via dynamic algorithm","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034082032&doi=10.1016%2fj.jss.2017.11.020&partnerID=40&md5=dd226426a2bf89410ef3d058e91f277c","Extremely low-latency and real-time communications are required in Tactile Internet to transfer physical tactile experiences remotely. In addition, its traffic has stringent requirements on bandwidth and quality of service (QoS). To minimize total costs of establishing the network and satisfy a pre-defined global upper delay-bound on the paths from the server to any other client for message broadcast in Tactile Internet, this paper presents a Rooted Delay-Constrained Minimum Spanning Tree (RDCMST) construction framework based on dynamic algorithm. The network is modeled as a connected weighted and undirected graph. Infeasible and suboptimal edges are discarded first by preprocessing techniques to reduce the processing complexity of the algorithm. Then the edges of the graph are processed based on a dynamic graph algorithm, which can maintain a single-source shortest path tree for the online edge deletions, such that total costs can be minimized while ensuring the delay-constraint and the tree structure. Experimental results show that our proposed approach greatly outperforms existing competing RDCMST formation algorithms, in terms of both average cost and stability of solutions. © 2017 Elsevier Inc.","Dynamic algorithm; Real-time; Rooted delay-constrained minimum spanning tree; Tactile internet"
"The impact of Software Testing education on code reliability: An empirical assessment","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015018996&doi=10.1016%2fj.jss.2017.02.042&partnerID=40&md5=174249705427381a7449cb4f277c2d45","Software Testing (ST) is an indispensable part of software development. Proper testing education is thus of paramount importance. Indeed, the mere exposition to ST knowledge might have an impact on programming skills. In particular, it can encourage the production of more correct - and thus reliable - code. Although this is intuitive, to the best of our knowledge, there are no studies about such effects. Concerned with this, we have conducted two investigations related to ST education: (1) a large experiment with students to evaluate the possible impact of ST knowledge on the production of reliable code; and (2) a survey with professors that teach introductory programming courses to evaluate their level of ST knowledge. Our study involved 60 senior-level computer science students, 8 auxiliary functions with 92 test cases, a total of 248 implementations, and 53 professors of diverse subfields that completed our survey. The investigation with students shows that ST knowledge can improve code reliability in terms of correctness in as much as 20%, on average. On the other hand, the survey with professors reveals that, in general, university instructors tend to lack the same knowledge that would help students increase their programming skills toward more reliable code. © 2017 Elsevier Inc.","Computer science education; Software Testing; Student experiments"
"Software search is not a science, even among scientists: A survey of how scientists and engineers find software","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045694306&doi=10.1016%2fj.jss.2018.03.047&partnerID=40&md5=a2eb5e0852d1a4be411bb6551b22be57","Improved software discovery is a prerequisite for greater software reuse: after all, if someone cannot find software for a particular task, they cannot reuse it. Understanding people's approaches and preferences when they look for software could help improve facilities for software discovery. We surveyed people working in several scientific and engineering fields to better understand their approaches and selection criteria. We found that even among highly-trained people, the rudimentary approaches of relying on general Web searches, the opinions of colleagues, and the literature were still the most commonly used. However, those who were involved in software development differed from nondevelopers in their use of social help sites, software project repositories, software catalogs, and organization-specific mailing lists or forums. For example, software developers in our sample were more likely to search in community sites such as Stack Overflow even when seeking ready-to-run software rather than source code, and likewise, asking colleagues was significantly more important when looking for ready-to-run software. Our survey also provides insight into the criteria that matter most to people when they are searching for ready-to-run software. Finally, our survey also identifies some factors that can prevent people from finding software. © 2018 The Authors","Software catalogues; Software reuse; Software search; Survey"
"Reliability over consecutive releases of a semiconductor Optical Endpoint Detection software system developed in a small company","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038871030&doi=10.1016%2fj.jss.2017.12.006&partnerID=40&md5=a15f935f972863d5175d28e6479ac0e8","Demonstrating software reliability across multiple software releases has become essential in making informed decisions of upgrading software releases without impacting significantly end users’ characterized processes and software quality standards. Standard defect and workload data normally collected in a typical small software development organization can be used for this purpose. Most of these organizations are normally under aggressive schedules with limited resources and data availability that are significantly different from large commercial software organizations where software reliability engineering has been successfully applied. Trend test, input domain reliability models (IDRM), and software reliability growth models (SRGM) were used in this paper on a semiconductor Optical Endpoint Detection (OED) software system to examine its overall trend and stability, to assess the system's operational reliability, and to track its reliability growth over multiple releases. These techniques also provided evidence that continuous defect fixes increased software reliability substantially over time for this software system. © 2017 Elsevier Inc.","Defects analysis; Reliability; Semiconductor software; Software versions and releases; Testing"
"Do android developers neglect error handling? a maintenance-Centric study on the relationship between android abstractions and uncaught exceptions","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032923420&doi=10.1016%2fj.jss.2017.10.032&partnerID=40&md5=165c7021058798300d5dcd5b0f93ee91","All the mainstream programming languages in widespread use for mobile app development provide error handling mechanisms to support the implementation of robust apps. Android apps, in particular, are usually written in the Java programming language. Java includes an exception handling mechanism that allows programs to signal the occurrence of errors by throwing exceptions and to handle these exceptions by catching them. All the Android-specific abstractions, such as activities and asynctasks, can throw exceptions when errors occur. When an app catches the exceptions that it or the libraries upon which it depends throw, it can resume its activity or, at least, fail in a graceful way. On the other hand, uncaught exceptions can lead an app to crash, particularly if they occur within the main thread. Previous work has shown that, in real Android apps available at the Play Store, uncaught exceptions thrown by Android-specific abstractions often cause these apps to fail. This paper presents an empirical study on the relationship between the usage of Android abstractions and uncaught exceptions. Our approach is quantitative and maintenance-centric. We analyzed changes to both normal and exception handling code in 112 versions extracted from 16 software projects covering a number of domains, amounting to more than 3 million LOC. Change impact analysis and exception flow analysis were performed on those versions of the projects. The main finding of this study is that, during the evolution of the analyzed apps, an increase in the use of Android abstractions exhibits a positive and statistically significant correlation with the number of uncaught exception flows. Since uncaught exceptions cause apps to crash, this result suggests that these apps are becoming potentially less robust as a consequence of exception handling misuse. Analysis of multiple versions of these apps revealed that Android developers usually employ abstractions that may throw exceptions without adding the appropriate handlers for these exceptions. This study highlights the need for better testing and verification tools with a focus on exception handling code and for a change of culture in Android development or, at least, in the design of its APIs. © 2017 Elsevier Inc.","Android; Exception handling; Maintainability; Robustness"
"Localizing multiple software faults based on evolution algorithm","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042861251&doi=10.1016%2fj.jss.2018.02.001&partnerID=40&md5=4a38efb07c3bfc3ca5c012109737c4e0","During software debugging, a significant amount of effort is required for programmers to identify the root cause of manifested failures. Various spectrum-based fault localization techniques have been proposed to automate the procedure. However, most of the existing fault localization approaches do not consider the fact that programs tend to have multiple faults. Considering faults in isolation results in less accurate analysis. In this paper, we propose a flexible framework called FSMFL for localizing multiple faults simultaneously based on genetic algorithms with simulated annealing. FSMFL can be easily extended by different fitness functions for the purpose of localizing multiple faults simultaneously. We have implemented a prototype and conducted extensive experiments to compare FSMFL against existing spectrum based fault localization approaches. The experimental results show that FSMFL is competitive in single-fault localization and superior in multi-fault localization. © 2018 Elsevier Inc.","Genetic algorithm; Multi-fault localization; Program spectrum; Search based software engineering"
"On the implementation of dynamic software product lines: An exploratory study","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034019086&doi=10.1016%2fj.jss.2017.11.004&partnerID=40&md5=923c7c5b6a47bb619b9784bca17d81ec","Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation. © 2017 Elsevier Inc.","Dynamic software product lines; Evidence-based software engineering; Software evolution; Variability mechanisms"
"Understanding the impact of cloud patterns on performance and energy consumption","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045462134&doi=10.1016%2fj.jss.2018.03.063&partnerID=40&md5=0d07058d3d463220ec67779d9373bb32","Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application. © 2018 Elsevier Inc.","Cloud patterns; Energy consumption; Energy efficiency; Performance optimization"
"IoT–TEG: Test event generator system","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023770296&doi=10.1016%2fj.jss.2017.06.037&partnerID=40&md5=90c1ff5072ac21fcec4a328625bc3902","Internet of Things (IoT) has been paid increasingly attention by the government, academe and industry all over the world. One of the main drawbacks of the IoT systems is the amount of information they have to handle. This information arrives as events that need to be processed in real time in order to make correct decisions. Given that processing the data is crucial, testing the IoT systems that will manage that information is required. In order to test IoT systems, it is necessary to generate a huge number of events with specific structures and values to test the functionalities required by these systems. As this task is very hard and very prone to error if done by hand, this paper addresses the automated generation of appropriate events for testing. For this purpose, a general specification to define event types and its representation are proposed and an event generator is developed based on this definition. Thanks to the adaptability of the proposed specification, the event generator can generate events of an event type, or events which combine the relevant attributes of several event types. Results from experiments and real-world tests show that the developed system meets the demanded requirements. © 2017 Elsevier Inc.","Complex event processing; Event generator; Event type definition; Internet of Things; Testing"
"Kanban in software engineering: A systematic mapping study","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036632703&doi=10.1016%2fj.jss.2017.11.045&partnerID=40&md5=451a033958126384fa567d08919af494","Following a well-established track record of success in other domains such as manufacturing, Kanban is increasingly used to achieve continuous development and delivery of value in the software industry. However, while research on Kanban in software is growing, these articles are largely descriptive, and there is limited rigorous research on its application and with little cohesive building of cumulative knowledge. As a result, it is extremely difficult to determine the true value of Kanban in software engineering. This study investigates the scientific evidence to date regarding Kanban by conducting a systematic mapping of Kanban literature in software engineering between 2006 and 2016. The search strategy resulted in 382 studies, of which 23 were identified as primary papers relevant to this research. This study is unique as it compares the findings of these primary papers with insights from a review of 23 Kanban experience reports during the same period. This study makes four important contributions, (i) a state-of-the-art of Kanban research is provided, (ii) the reported benefits and challenges are identified in both the primary papers and experience reports, (iii) recommended practices from both the primary papers and experience reports are listed and (iv) opportunities for future Kanban research are identified. © 2017 Elsevier Inc.","Kanban; Lean; Software development; Software engineering"
"The relation between developers’ communication and fix-Inducing changes: An empirical study","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044093614&doi=10.1016%2fj.jss.2018.02.065&partnerID=40&md5=7e6f543e349a755f359b0161e3a4289a","Background Many open source and industrial projects involve several developers spread around the world and working in different timezones. Such developers usually communicate through mailing lists, issue tracking systems or chats. Lack of adequate communication can create misunderstanding and could possibly cause the introduction of bugs. Aim This paper aims at investigating the relation between the bug inducing and fixing phenomenon and the lack of written communication between committers in open source projects. Method We performed an empirical study that involved four open source projects, namely Apache httpd, GNU GCC, Mozilla Firefox, and Xorg Xserver. For each project change history data, issue tracker comments, mailing list messages, and chat logs were analyzed in order to answer four research questions about the relation between the social importance and communication level of committers and their proneness to induce bug fixes. Results and implications Results indicate that the majority of bugs are fixed by committers who did not induce them, a smaller but substantial percentage of bugs is fixed by committers that induced them, and very few bugs are fixed by committers that were not directly involved in previous changes on the same files of the fix. More importantly, committers inducing fixes tend to have a lower level of communication between each other than that of other committers. This last finding suggests that increasing the level of communication between fix-inducing committers could reduce the number of fixes induced in a software project. © 2018 Elsevier Inc.","Bug management; Developers’ communication; Empirical study; Social network analysis"
"Efficient graph pattern matching framework for network-based in-vehicle fault detection","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042870770&doi=10.1016%2fj.jss.2018.02.050&partnerID=40&md5=231536ce188fa1b33164d35e8f63ac4a","Abnormal messages propagated from faulty operations in an in-vehicle network may severely harm a vehicular system, but they cannot be easily detected when their information is not known in advance. To support an efficient detection of faulty message patterns propagated in the in-vehicle network, this paper presents a novel graph pattern matching framework built upon a message log-driven graph modeling. Our framework models the unknown operation of the in-vehicle network as a query graph and the reference database of normal operations as data graphs. Given a query graph and data graphs, we determine whether the query graph represents normal or fault operation by using the distance measure on the data graphs. The analysis of the faulty message propagation requires to consider the sequence of events in the distance measure, and thus, using the conventional graph distance measures can generate false negatives due to the lack of consideration of the sequence relationships among the events. We therefore propose a novel distance metric based on the maximum common subgraph (MCS) between two graphs and the sequence numbers of messages, which works robustly even for the abnormal faulty patterns and can avoid false negatives in large databases. Since the problem of MCS computation is NP-hard, we also propose two efficient filtering techniques, one based on the lower bound of the MCS distance for a polynomial-time approximation and the other based on edge pruning. Experiments performed on real and synthetic datasets to assess our framework show that ours significantly outperforms the previously existing methods in terms of both performance and accuracy of query responses. © 2018 Elsevier Inc.","Graph pattern matching; Graph similarity; Top-k query answering; Vehicle fault detection"
"Characterizing and diagnosing out of memory errors in MapReduce applications","2018","Journal of Systems and Software","10.1016/j.jss.2017.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018661228&doi=10.1016%2fj.jss.2017.03.013&partnerID=40&md5=cc904d155753e56946c590a844f89360","Out of memory (OOM) errors are common and serious in MapReduce applications. Since MapReduce framework hides the details of distributed execution, it is challenging for users to pinpoint the OOM root causes. Current memory analyzers and memory leak detectors can only figure out what objects are (unnecessarily) persisted in memory but cannot figure out where the objects come from and why the objects become so large. Thus, they cannot identify the OOM root causes. Our empirical study on 56 OOM errors in real-world MapReduce applications found that the OOM root causes are improper job configurations, data skew, and memory-consuming user code. To identify the root causes of OOM errors in MapReduce applications, we design a memory profiling tool Mprof. Mprof can automatically profile and quantify the correlation between a MapReduce application's runtime memory usage and its static information (input data, configurations, user code). Mprof achieves this through modeling and profiling the application's dataflow, the memory usage of user code, and performing correlation analysis on them. Based on this correlation, Mprof uses quantitative rules to trace OOM errors back to the problematic user code, data, and configurations. We evaluated Mprof through diagnosing 28 real-world OOM errors in diverse MapReduce applications. Our evaluation shows that Mprof can accurately identify the root causes of 23 OOM errors, and partly identify the root causes of the other 5 OOM errors. © 2017 Elsevier Inc.","Error diagnosis; Mapreduce; Memory profiler; Out of memory"
"Anomaly detection and diagnosis for cloud services: Practical experiments and lessons learned","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042747168&doi=10.1016%2fj.jss.2018.01.039&partnerID=40&md5=d5901dcc74f2bfcc464d9b2c1dd03b20","The dependability of cloud computing services is a major concern of cloud providers. In particular, anomaly detection techniques are crucial to detect anomalous service behaviors that may lead to the violation of service level agreements (SLAs) drawn with users. This paper describes an anomaly detection system (ADS) designed to detect errors related to the erroneous behavior of the service, and SLA violations in cloud services. One major objective is to help providers to diagnose the anomalous virtual machines (VMs) on which a service is deployed as well as the type of error associated to the anomaly. Our ADS includes a system monitoring entity that collects software counters characterizing the cloud service, as well as a detection entity based on machine learning models. Additionally, a fault injection entity is integrated into the ADS for the training the machine learning models. This entity is also used to validate the ADS and to assess its anomaly detection and diagnosis performance. We validated our ADS with two case studies deployments: a NoSQL database, and a virtual IP Multimedia Subsystem developed implementing a virtual network function. Experimental results show that our ADS can achieve a high detection and diagnosis performance. © 2018 Elsevier Inc.","Anomaly detection; Diagnosis; Fault injection; Machine learning; SLA; System monitoring; Virtualization"
"Secure multi-keyword ranked search over encrypted cloud data for multiple data owners","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038904320&doi=10.1016%2fj.jss.2017.12.008&partnerID=40&md5=f9c1804620728f5499d097580eeb07b4","Secure multi-keyword ranked search over outsourced cloud data has become a hot research field. Most existing works follow the model of “Single Owner”, which just supports searching on the outsourced data belong to only one data owner. But the more realistic scenario is “Multiple Owners”, users could search on all datasets outsourced by different data owners. However, directly extending “Single Owner” schemes into “Multiple Owners” scenario still face the major two challenges: (1) the inconvenient key management and the resulting high communication cost; (2) due to the different authorities of owners, the qualities of documents are also different even if they are about the similar topic, but current rank functions in this area cannot rank documents based on their qualities. In this paper, we propose a secure multi-keyword ranked search scheme for multiple data owners. A trusted third party is imported to solve the problem of key management. We exploit the vector space model for generating index and query, and our new-designed KDO algorithm is utilized for providing keyword weight, so that the rank function not only considers about the relevance between query and document, but also takes into account the document quality. In order to protect privacy for both owners and users, the Asymmetric Scalar-product Preserving Encryption approach is utilized for encrypting weighted index and query. Besides, we construct the Grouped Balanced Binary tree index, which could further improve the search efficiency by Greedy Depth-first search algorithm. Extensive experiments demonstrate that our proposed scheme is secure, accurate and efficient. © 2017 Elsevier Inc.","Cloud computing; Document quality; Multiple owners; Secure keyword search"
"Security slicing for auditing common injection vulnerabilities","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014466419&doi=10.1016%2fj.jss.2017.02.040&partnerID=40&md5=dd25e5075271bfe0d314d79bc49ed44f","Cross-site scripting and injection vulnerabilities are among the most common and serious security issues for Web applications. Although existing static analysis approaches can detect potential vulnerabilities in source code, they generate many false warnings and source-sink traces with irrelevant information, making their adoption impractical for security auditing. One suitable approach to support security auditing is to compute a program slice for each sink, which contains all the information required for security auditing. However, such slices are likely to contain a large amount of information that is irrelevant to security, thus raising scalability issues for security audits. In this paper, we propose an approach to assist security auditors by defining and experimenting with pruning techniques to reduce original program slices to what we refer to as security slices, which contain sound and precise information. To evaluate the proposed approach, we compared our security slices to the slices generated by a state-of-the-art program slicing tool, based on a number of open-source benchmarks. On average, our security slices are 76% smaller than the original slices. More importantly, with security slicing, one needs to audit approximately 1% of the total code to fix all the vulnerabilities, thus suggesting significant reduction in auditing costs. © 2018 Elsevier Inc.","Automated code fixing; Security auditing; Static analysis; Vulnerability"
"Combining heterogeneous anomaly detectors for improved software security","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015698998&doi=10.1016%2fj.jss.2017.02.050&partnerID=40&md5=e5f241a92e9ae27c5b555c7c413959fc","Host-based Anomaly Detection Systems (ADSs) monitor for significant deviations from normal software behavior. Several techniques have been investigated for detecting anomalies in system call sequences. Among these, Sequence Time-Delay Embedding (STIDE), Hidden Markov Model (HMM), and One-Class Support Vector Machine (OCSVM) have shown a high level of anomaly detection accuracy. Although ADSs can detect novel attacks, they generate a large number of false alarms due to the difficulty in obtaining complete descriptions of normal software behavior. This paper presents a multiple-detector ADS that efficiently combines the decisions from heterogeneous detectors (e.g., STIDE, HMM, and OCSVM), using Boolean combination in the Receiver Operating Characteristics (ROC) space, to reduce the false alarms. Results on two modern and large system call datasets generated from Linux and Windows operating systems show that the proposed ADS consistently outperforms an ADS based on a single best detector and on an ensemble of homogeneous detectors. At an operating point of zero percent alarm rate, the proposed multiple-detector ADS increased the true positive rate by 500% on the Linux dataset and by 25% on the Window dataset. Furthermore, the combinations of decisions from multiple heterogeneous detectors make the ADS more reliable and resilient against evasion and adversarial attacks. © 2017","Anomaly detection systems; Decision-level combination; Heterogeneous and reliable systems; Intrusion detection systems"
"Compositional execution semantics for business process verification","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038009405&doi=10.1016%2fj.jss.2017.11.003&partnerID=40&md5=b38cd5e4430453e6291d47272fd3e90b","Service compositions are programmed as executable business processes in languages like WS-BPEL (or BPEL in short). In such programs, activities are nested within concurrency, isolation, compensation and event handling constructs that cause an overwhelming number of execution paths. Program correctness has to be verified based on a formal definition of the language semantics. For BPEL, previous works have proposed execution semantics in formal languages amenable to model checking. Most of the times the service composition structure is not preserved in the formal model, which impedes tracing the verification findings in the original program. Here, we propose a compositional semantics and a structure-preserving translator of BPEL programs onto the BIP component framework. In addition, we verify essential correctness properties that affect process responsiveness, and the compliance with partner services. The scalability of the proposed translation and analysis is demonstrated on BPEL programs of various sizes. Our compositional translation approach can be also applied to other executable languages with nesting syntax. © 2017","BIP; Formal verification; Programming language semantics; WS-BPEL"
"Distributing relational model transformation on MapReduce","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045642775&doi=10.1016%2fj.jss.2018.04.014&partnerID=40&md5=18a7567940fe129fa18acc2df9f92d69","MDE has been successfully adopted in the production of software for several domains. As the models that need to be handled in MDE grow in scale, it becomes necessary to design scalable algorithms for model transformation (MT) as well as suitable frameworks for storing and retrieving models efficiently. One way to cope with scalability is to exploit the wide availability of distributed clusters in the Cloud for the parallel execution of MT. However, because of the dense interconnectivity of models and the complexity of transformation logic, the efficient use of these solutions in distributed model processing and persistence is not trivial. This paper exploits the high level of abstraction of an existing relational MT language, ATL, and the semantics of a distributed programming model, MapReduce, to build an ATL engine with implicitly distributed execution. The syntax of the language is not modified and no primitive for distribution is added. Efficient distribution of model elements is achieved thanks to a distributed persistence layer, specifically designed for relational MT. We demonstrate the effectiveness of our approach by making an implementation of our solution publicly available and using it to experimentally measure the speed-up of the transformation system while scaling to larger models and clusters. © 2018 Elsevier Inc.","ATL; Distributed computing; MapReduce; Model transformation; NeoEMF"
"The impacts of techniques, programs and tests on automated program repair: An empirical study","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021454419&doi=10.1016%2fj.jss.2017.06.039&partnerID=40&md5=21d6b53dc930fffc1449479184510426","Manual program repair is notoriously tedious, error-prone, and costly, especially for the modern large-scale projects. Automated program repair can automatically find program patches without much human intervention, greatly reducing the burden of developers as well as accelerating software delivery. Therefore, much research effort has been dedicated to design powerful program repair techniques. To date, although various program repair techniques have been proposed, to our knowledge, there lacks extensive study on the impacts of repair techniques, subject programs, and test suites on the repair effectiveness and efficiency. In this paper, we perform such an extensive study on repairing 180 seeded and real faults from 17 small to large sized programs. We study the impacts of five representative automated program repair techniques, including GenProg, RSRepair, Brute-force-based technique, AE and Kali, on the repair results. We further investigate the impacts of different subject programs and test suites on effectiveness and efficiency of program repair techniques. Our study demonstrates a number of interesting findings: Brute-force-based technique generates the maximum number of patches but is also the most costly technique, while Kali is the most efficient and has medium effectiveness among the studied techniques; techniques that work well with small programs become too costly or ineffective when applied to large sized programs; since tool-reported patches may overfit the selected test cases, we calculate the false positive rates and find that the influence of failed test cases is much larger than that of passed test cases; finally, surprisingly, all the studied techniques except RSRepair can find more than 80% of successful patches within the first 50% of search space. © 2017","Automated program repair; Empirical study"
"A survey of model transformation design patterns in practice","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043534057&doi=10.1016%2fj.jss.2018.03.001&partnerID=40&md5=a0cc1ebb13b19a289f66bbbe34ef4c7f","Model transformation design patterns have been proposed by a number of researchers, but their usage appears to be sporadic and sometimes patterns are applied without recognition of the pattern. In this paper we provide a systematic literature review of transformation design pattern applications. We evaluate how widely patterns have been used, and how their use differs in different transformation languages and for different categories of transformation. We identify what benefits appear to arise from the use of patterns, and consider how the application of patterns can be improved. The paper also identifies several new patterns which have not previously been catalogued. © 2018 Elsevier Inc.","Design Patterns; Empirical Software Engineering; Model Transformations"
"Domain model slicing and constraint classification for local validation on rich clients","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021807247&doi=10.1016%2fj.jss.2017.06.036&partnerID=40&md5=8bca8eb14a6246e7caf030739e50781d","Web-based rich client applications have emerged as a solid and popular approach in both web and native applications. Their capability to manage their own domain model and locally verify constraints provides a more responsive and robust user experience. This local model is often a subset of the application's global domain model (GDM) that is managed on the server. Both ends should always manage their entities, relationships and constraints consistently between them. Designing such client model manually implies identifying the GDM domain elements and constraints that should also be present on the client and adapting each one of them if needed. This is a complex and error-prone task, and any additional modification to the server model requires reviewing the client side. In our opinion, all the information needed for automating the client model generation can be derived from the GDM and the set of entities involved in the client functionality. This work includes a formal description of a method that, from that initial information, combines model slicing and constraint analysis techniques to create the client domain model, and classifies the constraints according to their server independency. © 2017 Elsevier Inc.","Constraints; Domain modeling; Model slicing; OCL; Rich client; UML"
"APPSPEAR: Automating the hidden-code extraction and reassembling of packed android malware","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042717494&doi=10.1016%2fj.jss.2018.02.040&partnerID=40&md5=ec1922a13b60e9f6e5df041951e7fbe6","Code packing is one of the most frequently used protection techniques for malware to evade detection. Particularly, Android packers originally designed to protect intellectual property are widely utilized by Android malware nowadays to hide their malicious behaviors. What's worse, Android code packing techniques are evolving rapidly with new features of Android system (e.g., the use of new Android runtime). Meanwhile, unpacking techniques and tools generally do not respond to the evolving of packers immediately, which weakens the effectiveness of new malware detection. To address the unpacking challenge especially for Android packers with advanced code hiding strategies, in this paper we propose APPSPEAR, an automated unpacking system for both Dalvik and ART. APPSPEAR adopts a universal unpacking strategy that combines runtime instrumentation, interpreter-enforced execution, and executable reassembling to guarantee the hidden code is extracted and reconstructed as a complete executable. Our experimental evaluation with 530 packed samples shows that APPSPEAR is able to unpack protected code generated by latest versions of mainstream Android packers effectively. © 2018 Elsevier Inc.","Android security; Code packing technique; Code unpacking; Malware detection"
"Cloud service evaluation method-based Multi-Criteria Decision-Making: A systematic literature review","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042180333&doi=10.1016%2fj.jss.2018.01.038&partnerID=40&md5=c49f71703b5c9acb95933c2ff8015462","A substantial effort has been made to solve the cloud-service evaluation problem. Different Cloud Service Evaluation Methods (CSEMs) have been developed to address the problem. Cloud services are evaluated against multiple criteria, which leads to a Multi-Criteria Decision-Making (MCDM) problem. Yet, studies that assess, analyse, and summarize the unresolved problems and shortcomings of current CSEM-based MCDM are limited. In the existing review studies, only individual parts of CSEMs, rarely the full solution, are reviewed and examined. To investigate CSEMs comprehensively, we present a systematic literature review based on Evaluation Theory, a theory that generalizes six evaluation components, target, criteria, yardstick, data gathering techniques, synthesis techniques, and evaluation process. These six evaluation components and the CSEMs validation approach are the seven dimensions used to assess and analyse 77 papers published from 2006 to 2016. Sixteen research deficiencies were identified. The results confirm that the majority of the studies of the proposed CSEMs were either incomplete or lacked sufficient evidence. This research not only provides the relative strengths and weaknesses of the different CSEMs but also offers a basis for researchers and decision makers to develop improved CSEMs. © 2018 Elsevier Inc.","Cloud computing service; Cloud service evaluation method; Evaluation theory; Multi-Criteria Decision-Making (MCDM); Systematic literature review"
"Automated generation of (F)LTL oracles for testing and debugging","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042802652&doi=10.1016%2fj.jss.2018.02.002&partnerID=40&md5=d4a65bf6f354c533591360006a76d786","For being able to draw on automated reasoning that helps us in improving the quality of some software artifact or cyber-physical system, we have to express desired system traits in precise formal requirements. Verifying that a system adheres to these requirements allows us then to gain the crucial level of confidence in its capabilities and quality. Complementing related methods like model checking or runtime monitors, for testing and most importantly debugging recognized problems, we would certainly be interested in automated oracles. These oracles would allow us to judge whether observed (test) data really adhere to desired properties, and also to derive program spectra that have been shown to be an effective reasoning basis for debugging purposes. In this paper, we show how to automatically derive such an oracle as a dedicated satisfiability encoding that is specifically tuned to the considered test data at hand. In particular, we instantiate a dedicated SAT problem in conjunctive normal form directly from the requirements and a test case's execution data. Our corresponding experiments illustrate that our approach shows attractive performance and can be fully automated. © 2018 Elsevier Inc.","Linear temporal logic; SAT; Test oracle"
"Metric selection and anomaly detection for cloud operations using log and metric correlation analysis","2018","Journal of Systems and Software","10.1016/j.jss.2017.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016586904&doi=10.1016%2fj.jss.2017.03.012&partnerID=40&md5=038b4d5e75b7f83b1f31dd423527f62b","Cloud computing systems provide the facilities to make application services resilient against failures of individual computing resources. However, resiliency is typically limited by a cloud consumer's use and operation of cloud resources. In particular, system operations have been reported as one of the leading causes of system-wide outages. This applies specifically to DevOps operations, such as backup, redeployment, upgrade, customized scaling, and migration – which are executed at much higher frequencies now than a decade ago. We address this problem by proposing a novel approach to detect errors in the execution of these kinds of operations, in particular for rolling upgrade operations. Our regression-based approach leverages the correlation between operations’ activity logs and the effect of operation activities on cloud resources. First, we present a metric selection approach based on regression analysis. Second, the output of a regression model of selected metrics is used to derive assertion specifications, which can be used for runtime verification of running operations. We have conducted a set of experiments with different configurations of an upgrade operation on Amazon Web Services, with and without randomly injected faults to demonstrate the utility of our new approach. © 2017 Elsevier Inc.","Anomaly detection; Cloud application operations; Cloud monitoring; Error detection; Log analysis; Metric selection"
"Software traceability in the automotive domain: Challenges and solutions","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045187448&doi=10.1016%2fj.jss.2018.03.060&partnerID=40&md5=453e2dd25ab1b7a993df68baef20c80b","In the automotive domain, the development of all safety-critical systems has to comply with safety standards such as ISO 26262. These standards require established traceability, the ability to relate artifacts created during development of a system, to ensure resulting systems are well-tested and therefore safe. This paper contrasts general traceability challenges and solutions with those specific to the automotive domain, and investigates how they manifest in practice. We combine three data sources: a tertiary literature review to identify general challenges and solutions; a case study with an automotive supplier as validation for how the challenges and solutions are experienced in practice; and a multi-vocal literature review to identify challenges and solutions specific to the automotive domain. We found 22 challenges and 16 unique solutions in the reviews. 17 challenges were identified in the case study; six remain unsolved. We discuss challenges and solutions from the perspectives of academia, tool vendors, consultants and users, and identify differences between scientific and “grey” literature. We discuss why challenges remain unsolved and propose solutions. Our findings indicate that there is a significant overlap between general traceability challenges and those in the automotive domain but that they are experienced differently. © 2018 Elsevier Inc.","Automotive; Automotive SPICE; ISO 26262; Safety; Traceability"
"Smells in software test code: A survey of knowledge in industry and academia","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039428979&doi=10.1016%2fj.jss.2017.12.013&partnerID=40&md5=712a9941e4e4d1c5e9ecff82a16a1075","As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal’ literature mapping (classification) on both the scientific literature and also practitioners’ grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects. © 2017 Elsevier Inc.","Automated testing; Multivocal literature mapping; Software testing; Survey; Systematic mapping; Test anti-patterns; Test automation; Test scripts; Test smells"
"Time-space efficient regression testing for configurable systems","2018","Journal of Systems and Software","10.1016/j.jss.2017.08.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028026550&doi=10.1016%2fj.jss.2017.08.010&partnerID=40&md5=026a18f592d3d2d4d3f1015f8303ed3f","Configurable systems are those that can be adapted from a set of input options, reflected in code in form of variations. Testing these systems is challenging because of the vast array of configuration possibilities where bugs can hide. In the context of evolution, testing becomes even more challenging — not only code but also the set of plausible configurations can change across versions. This paper proposes EvoSPLat, a regression testing technique for configurable systems that explores all dynamically reachable configurations from a test. EvoSPLat supports two important application scenarios of regression testing. In the RCS scenario EvoSPLat prunes configurations (not tests) that are not impacted by changes. In the RTS scenario EvoSPLat prunes tests (not configurations) which are not impacted by changes. To evaluate EvoSPLat under the RCS scenario we used a selection of configurable Java programs. Results indicate that EvoSPLat reduced time by ∼22% and reduced the number of configurations tested by ∼45%. To evaluate EvoSPLat under the RTS scenario we used GCC. Results indicate that EvoSPLat reduced time to run tests by ∼35%. Overall, results suggest that EvoSPLat is a promising technique to test configurable systems in the prevalent scenario of evolution. © 2017 Elsevier Inc.","Configurable systems; Regression testing"
"Monitoring self-adaptive applications within edge computing frameworks: A state-of-the-art review","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034055307&doi=10.1016%2fj.jss.2017.10.033&partnerID=40&md5=e1e83d0a3ef8672ffad14237de5432d7","Recently, a promising trend has evolved from previous centralized computation to decentralized edge computing in the proximity of end-users to provide cloud applications. To ensure the Quality of Service (QoS) of such applications and Quality of Experience (QoE) for the end-users, it is necessary to employ a comprehensive monitoring approach. Requirement analysis is a key software engineering task in the whole lifecycle of applications; however, the requirements for monitoring systems within edge computing scenarios are not yet fully established. The goal of the present survey study is therefore threefold: to identify the main challenges in the field of monitoring edge computing applications that are as yet not fully solved; to present a new taxonomy of monitoring requirements for adaptive applications orchestrated upon edge computing frameworks; and to discuss and compare the use of widely-used cloud monitoring technologies to assure the performance of these applications. Our analysis shows that none of existing widely-used cloud monitoring tools yet provides an integrated monitoring solution within edge computing frameworks. Moreover, some monitoring requirements have not been thoroughly met by any of them. © 2017 The Author(s)","Adaptive applications; Cloud computing; Edge computing; Monitoring technologies; Requirement analysis; Taxonomy of monitoring requirements"
"Towards efficiently supporting database as a service with QoS guarantees","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041470897&doi=10.1016%2fj.jss.2018.01.034&partnerID=40&md5=9462192991ec9310b4b50c1cb4dc5bf5","Quality of Service (QoS) is at the core of the vision of Database as a Service (DBaaS), which provides guarantees to database users on the usability of database services, even when the underlying database infrastructure is shared by multiple users. When QoS guarantees are necessitated, traditional approaches in DBaaS often have to reserve computation resources (e.g. CPU and memory) for tenants according to their performance Service Level Objectives (SLOs), so that the database engine always possesses sufficient resources to accomplish the expected workloads under any circumstance. Such resource reservation schemes inevitably result in poor resource utilization, as the actual workloads of tenants are usually below their maximal workload expectation described in their SLOs. In this paper, we propose a novel scheme called FrugalDB to further improve resource utilization and thus reduce operational cost for DBaaS systems with QoS guarantees. FrugalDB accommodates two independent database engines, an in-memory engine for heavy workloads with tight SLOs, and a disk-based engine for light workloads with loose SLOs. The in-memory database is leveraged to migrate temporary heavy workloads from the disk-based database, when the latter itself does not suffice to handle all active tenants’ workloads, and thus it could be relieved of the tremendous data serving pressure yielded by those heavy workloads, and focus on processing massive numbers of light workloads. When the heavy workloads fade out, FrugalDB reversely migrates a tenant's workload from the in-memory database into the disk-based database, so that the occupied memory resources could be recollected for subsequent workload migrations. By an effective workload estimation method and an efficient migration schedule algorithm, FrugalDB tries to minimize workload migration cost incurred in moving workloads between the two engines. By allocating each tenant's workload to an appropriate engine via workload migration, this dual-engine scheme can substantially save computation resources, and thus consolidate more tenants on a single database server. We evaluate FrugalDB with extensive experiments, which show that it has a higher tenant consolidation rate with performance SLO guarantees and fewer performance SLO violations than the existing systems, as well as with acceptable response latency. © 2018 Elsevier Inc.","Cloud computing; Database-as-a-service; Multi-tenancy; Workload consolidation; Workload migration"
"Efficient detection and validation of atomicity violations in concurrent programs","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020425764&doi=10.1016%2fj.jss.2017.06.001&partnerID=40&md5=a634861b525e2113bec4b5f1f33835e5","Atomicity violations are a major source of bugs in concurrent programs. Empirical studies have shown that the majority of atomicity violations are instances of the three-access pattern, where two accesses to a shared variable by a thread are interleaved by an access to the same variable by another thread. This article describes two advancements in atomicity violation detection. First, we describe a new technique that directs the execution of a dynamic analysis tool towards three-access pattern (TAP) instances. The directed search is based on constraint solving and concolic execution. We implemented this technique in a tool called AtomChase. Using 27 benchmarks comprising 5.4 million lines of Java, we compared AtomChase to five other tools. AtomChase found 20% more TAP instances than all five tools combined. Second, we show that not all TAP instances are atomicity violations and present a formally grounded approach to validating the non-atomicity of TAP instances. This approach, called HyperCV, prevents the inclusion of false positives in results presented to users. HyperCV uses a set of provably sufficient conditions for non-atomicity to efficiently validate TAP instances. Using the same benchmarks, HyperCV validated 79% of TAP instances in linear rather than exponential execution time. © 2017 Elsevier Inc.","Atomicity violation; Concurrency; Software testing; Three-access pattern"
"Transferring interactive search-based software testing to industry","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046428017&doi=10.1016%2fj.jss.2018.04.061&partnerID=40&md5=98d0eaceaac04bc6aefdf73d3a5b4b45","Context: Search-Based Software Testing (SBST), and the wider area of Search-Based Software Engineering (SBSE), is the application of optimization algorithms to problems in software testing, and software engineering, respectively. New algorithms, methods, and tools are being developed and validated on benchmark problems. In previous work, we have also implemented and evaluated Interactive Search-Based Software Testing (ISBST) tool prototypes, with a goal to successfully transfer the technique to industry. Objective: While SBST and SBSE solutions are often validated on benchmark problems, there is a need to validate them in an operational setting, and to assess their performance in practice. The present paper discusses the development and deployment of SBST tools for use in industry, and reflects on the transfer of these techniques to industry. Method: In addition to previous work discussing the development and validation of an ISBST prototype, a new version of the prototype ISBST system was evaluated in the laboratory and in industry. This evaluation is based on an industrial System under Test (SUT) and was carried out with industrial practitioners. The Technology Transfer Model is used as a framework to describe the progression of the development and evaluation of the ISBST system, as it progresses through the first five of its seven steps. Results: The paper presents a synthesis of previous work developing and evaluating the ISBST prototype, as well as presenting an evaluation, in both academia and industry, of that prototype's latest version. In addition to the evaluation, the paper also discusses the lessons learned from this transfer. Conclusions: This paper presents an overview of the development and deployment of the ISBST system in an industrial setting, using the framework of the Technology Transfer Model. We conclude that the ISBST system is capable of evolving useful test cases for that setting, though improvements in the means the system uses to communicate that information to the user are still required. In addition, a set of lessons learned from the project are listed and discussed. Our objective is to help other researchers that wish to validate search-based systems in industry, and provide more information about the benefits and drawbacks of these systems. © 2018 Elsevier Inc.","Industrial evaluation; Interactive search-based software testing; Search-based software testing"
"Efficient validation of self-adaptive applications by counterexample probability maximization","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039732137&doi=10.1016%2fj.jss.2017.12.009&partnerID=40&md5=aada90df5bad97757c2227c55fc88fee","Self-adaptive applications’ executions can be affected by uncertainty factors like unreliable sensing and flawed adaptation and therefore often error-prone. Existing methods can verify the applications suffering uncertainty and report counterexamples. However, such verification results can deviate from reality when the uncertainty specification used in verification is itself imprecise. This thus calls for further validation of reported counterexamples. One outstanding challenge in counterexample validation is that the probabilities of counterexamples occurring in real environment are usually very low, which makes the validation extremely inefficient. In this paper, we propose a novel approach to systematically deriving path-equivalent counterexamples with respect to original ones. The derived counterexamples guarantee to have higher probabilities, making them capable of being validated efficiently in field test. We evaluated our approach with real-world self-adaptive applications. The results reported that our approach significantly increased counterexample probabilities, and the derived counterexamples were also consistently and efficiently validated in both real environment and simulation. © 2017 Elsevier Inc.","Optimization; Probability; Self-adaptation; Validation"
"Gamification in software engineering education: A systematic mapping","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045264602&doi=10.1016%2fj.jss.2018.03.065&partnerID=40&md5=bae5b8620d8fcc7c4c0b19d3e96b50ab","The potential of gamification in education is based on the hypothesis that it supports and motivates students and can thus lead to enhanced learning processes and outcomes. Gamification in software engineering (SE) education is in its infancy. However, as SE educators we are particularly interested in understanding how gamification is pollinating our field and the extent to which the above claim is valid in our context. A systematic literature mapping has underscored the difficulty in fully corroborating the above claim because few empirical data are available so far. However, key trends and challenges have been identified. We found that the purpose of applying gamification in the SE field is mostly directly related to improving student engagement and, to a lesser extent, to improving student knowledge, although other targets are the application of SE best practices and socialization. We have also discussed insightful issues regarding the implementation cost of gamification, patterns in the most often used gamification elements, and the SE processes and teaching activities addressed. Of the identified challenges, we should highlight the complexity of deciding which gamification approach to follow, the lack of information for choosing gamification elements and the need to control the impact of gamification. © 2018 Elsevier Inc.","Education; Gamification; Software engineering; Systematic mapping"
"Software engineering problems and their relationship to perceived learning and customer satisfaction on a software capstone project","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035038197&doi=10.1016%2fj.jss.2017.11.021&partnerID=40&md5=46e540a86277b9ba7ebf03817cbeefa3","In educational projects, having students encounter problems is desirable, if it increases learning. However, in capstone projects with industrial customers, negative effects problems can have on customer satisfaction must be considered. We conducted a survey in a capstone project course in order to study problems, learning and customer satisfaction related to eleven software engineering topics. On the average, students working in the managerial roles learned quite a lot about each topic, and the developers learned moderately, but the degree of learning varied a lot among the teams, and among the team members. The most extensively encountered problems were related to testing, task management, effort estimation and technology skills. The developers contributed quite a lot to solving problems with technology skills, but only moderately or less with other topics, whereas the managers contributed quite a lot with most of the topics. Contributing to solving problems increased learning moderately for most of the topics. The increases were highest with maintaining motivation and technology skills. Encountering problems with task management, customer expectations and customer communication affected customer satisfaction very negatively. When considering both learning and customer satisfaction, the best topics to encounter problems in were effort estimation, testing, and technology skills. © 2017 The Authors","Capstone project; Customer satisfaction; Education; Learning; Problems; Software engineering"
"Model-based mutant equivalence detection using automata language equivalence and simulations","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044481061&doi=10.1016%2fj.jss.2018.03.010&partnerID=40&md5=b7234a5c9c71360538149c8ba2fb5686","Mutation analysis is a popular technique for assessing the strength of test suites. It relies on the mutation score, which indicates their fault-revealing potential. Yet, there are mutants whose behaviour is equivalent to the original system, wasting analysis resources and preventing the satisfaction of a 100% mutation score. For finite behavioural models, the Equivalent Mutant Problem (EMP) can be transformed to the language equivalence problem of non-deterministic finite automata for which many solutions exist. However, these solutions are quite expensive, making computation unbearable when used for tackling the EMP. In this paper, we report on our assessment of a state-of-the-art exact language equivalence tool and two heuristics we proposed. We used 12 models, composed of (up to) 15,000 states, and 4710 mutants. We introduce a random and a mutation-biased simulation heuristics, used as baselines for comparison. Our results show that the exact approach is often more than ten times faster in the weak mutation scenario. For strong mutation, our biased simulations can be up to 1000 times faster for models larger than 300 states, while limiting the error of misclassifying non-equivalent mutants as equivalent to 8% on average. We therefore conclude that the approaches can be combined for improved efficiency. © 2018 Elsevier Inc.","Automata language equivalence; Model-based mutation analysis; Random simulations"
"A Metrics Suite for code annotation assessment","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037536057&doi=10.1016%2fj.jss.2017.11.024&partnerID=40&md5=bf556544c496755d48e464b5b310b310","Code annotation is a language feature that enables the introduction of custom metadata on programming elements. In Java, this feature was introduced on version 5, and today it is widely used by main enterprise application frameworks and APIs. Although this language feature potentially simplifies metadata configuration, its abuse and misuse can reduce source code readability and complicate its maintenance. The goal of this paper is to propose software metrics regarding annotations in the source code and analyze their distribution in real-world projects. We have defined a suite of metrics to assess characteristics of the usage of source code annotations in a code base. Our study collected data from 24947 classes extracted from open source projects to analyze the distribution of the proposed metrics. We developed a tool to automatically extract the metrics and provide a full report on annotations usage. Based on the analysis of the distribution, we defined an appropriate approach for the calculation of thresholds to interpret the metric values. The results allow the assessment of annotated code characteristics. Using the thresholds values, we proposed a way to interpret the use of annotations, which can reveal potential problems in the source code. © 2017 Elsevier Inc.","Code annotation; Software metrics; Thresholds"
"Architecture-level hazard analysis using AADL","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025066854&doi=10.1016%2fj.jss.2017.06.018&partnerID=40&md5=751daefe0f89dfdfe91284c86d7de24a","Software systems are becoming increasingly important in safety-critical areas. Designing safe software requires a significant emphasis on hazards in the early design phase of software development. In this paper, we propose a hazard analysis approach based on Architecture Analysis and Design Language (AADL). First, to make up the deficiencies of Error Model Annex (EMV2), we create Hazard Model Annex (HMA) to specify the hazard sources, hazards, hazard trigger mechanisms, and mishaps. By using HMA, a safety model can be built by annotating an architecture model with the error model and hazard model. Then, an architecture-level hazard analysis approach is proposed to automatically generate the hazard analysis table. The approach contains the model transformation from a safety model to a Deterministic Stochastic Petri Nets (DSPNs) model for calculating the occurrence probability of hazards and mishaps. In addition, we present the formal semantics for each constituent part of the safety model, define the model mapping rules, and verify the semantic preservation of the transformation. Finally, HMA is implemented to build safety models and two Eclipse plug-ins of our methodology are also implemented. A case study on a flight control software system has been employed to demonstrate the feasibility of our proposed technique. © 2017 Elsevier Inc.","AADL; Hazard analysis; Hazard model annex; Model transformation; Semantic preservation"
"PreX: A predictive model to prevent exceptions","2018","Journal of Systems and Software","10.1016/j.jss.2017.07.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026675790&doi=10.1016%2fj.jss.2017.07.026&partnerID=40&md5=7c3ac5192e380826d39cf5f58e3c8ad1","The exception handling mechanism has been one of the most used reliability tools in programming languages in the last decades. However, this model has not changed much with time, in spite of advances in programming languages, which include concurrent programming and a shift towards more reactive paradigms, the basic principle remains the same - an exception occurs, and the mechanism reacts. We propose a new paradigm, inspired by Online Failure Prediction (OFP), to predict exceptions and possibly avert them by triggering the execution of preventive actions. The proposed model - PreX - is, thus, proactive, operating in a much finer-grained level than any other form of online failure prediction. OFP has shown promising results in predicting failures at a higher level, but has never been available to the developer, being mainly a system level technique. Thus, PreX will offer developers a new range of revitalization strategies. In this work, we describe the model and evaluate its performance by applying it to a real e-commerce solution, demonstrating how it is capable of predicting and preventing exceptions at run-time. Furthermore, we also show that PreX increases the overall availability and performance of the system under the same conditions. © 2017 Elsevier Inc.","Exception handling; Failure prediction; Predictive; Preventive; Proactive; Self-healing"
"The impact of tailoring criteria on agile practices adoption: A survey with novice agile practitioners in Brazil","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038917895&doi=10.1016%2fj.jss.2017.12.012&partnerID=40&md5=8ce2f85240fe37f152930aae2c1d5c1b","The software development industry adopts agile methods in different ways by considering contextual requirements. To fulfill organizational needs, adoption strategies consider agile methods tailoring. However, tailoring according to the context of the organization remains a problem to be solved. Literature on criteria for adopting software development methods exists, but not specifically for agile methods. Given this scenario, the following research question arises: what is the impact of software method tailoring criteria on the adoption of agile practices? To answer this question, we conducted a survey among agile practitioners in Brazil to gather data about importance of the tailoring criteria and agile practices adopted. A model for agile practices adoption based on the tailoring criteria is proposed using the results of the survey with a majority of novice agile practitioners. The proposed model was validated using PLS-SEM (partial least squares structural equation modeling) and the survey data. Results show that adoption of agile practices was influenced by criteria such as external environment, previous knowledge and internal environment. Results also indicate that organizations tend to use hybrid/custom software methods and select agile practices according to their needs. © 2017 Elsevier Inc.","Agile method tailoring; Agile practices adoption; Agile practices selection; PLS-SEM; Software method tailoring; Tailoring criteria"
"Failure patterns in operating systems: An exploratory and observational study","2018","Journal of Systems and Software","10.1016/j.jss.2017.03.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017397415&doi=10.1016%2fj.jss.2017.03.058&partnerID=40&md5=569b1f4d45e6684f933487eecc0ffeda","Sophisticated critical computer applications need to run on top of operating system (OS) software. Given the natural intrinsic dependency of user applications on the OS software, OS failures can severely impact even the most reliable applications. Thus, it is essential to understand how OS failures occur in order to improve software reliability. In this paper, we present an exploratory and observational study on OS failure patterns. We analyze 7007 real OS failures collected from 566 computers used in different workplaces. We start with a general characterization of the failure dataset examined in this study, where interesting findings are presented, e.g., the most frequent failure types per period of a day and per different workplaces. Next, we investigate the existence of failure patterns. For this purpose, we introduce an OS failure pattern discovery protocol that identifies failure patterns exhibiting consistency across different computers used in the same as well as different workplaces. In total, we discovered 45 failure patterns with 153,511 occurrences. Based on these patterns, we found that the most prevalent failures were related to the software updates of the OS components. The main causes of these failures involved infrastructural and environmental factors such as disk-space unavailability and concurrent execution of OS services. Empirical evidence of time-correlated failures of these OS components is also discussed in this paper. Other findings include the OS components that contributed more to create the discovered failure patterns and the most prevalent combination of failure events and their temporal order. This study aims at to contribute to a better understanding of the mechanisms behind OS failures. © 2017","Operating systems; Pattern discovery; Software failure; Software reliability"
"Analyzing inconsistencies in software product lines using an ontological rule-based approach","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020680971&doi=10.1016%2fj.jss.2017.06.002&partnerID=40&md5=059f6209d507250d96872f1006ec5414","Software product line engineering (SPLE) is an evolving technical paradigm for generating software products. Feature model (FM) represents commonality and variability of a group of software products that appears within a specific domain. The quality of FMs is one of the factors that impacts the correctness of software product line (SPL). Developing FMs might also incorporate inaccurate relationships among features which cause numerous defects in models. Inconsistency is one of such defect that decreases the benefits of SPL. Existing approaches have focused in identifying inconsistencies in FMs however, only a few of these approaches are able to provide their causes. In this paper FM is formalized from an ontological view by converting model into a predicate-based ontology and defining a set of first-order logic based rules for identifying FM inconsistencies along with their causes in natural language in order to assist developers with solutions to fix defects. A FM available in software product lines online tools repository has been used to explain the presented approach and validated using 24 FMs of varied sizes up to 22,035 features. Evaluation results demonstrate that our approach is effective and accurate for the FMs scalable up to thousands of features and thus, improves SPL. © 2017 Elsevier Inc.","Defects; Feature model; Inconsistency; Ontology; Rule-based approach; Software product line"
"Efficiently detecting structural design pattern instances based on ordered sequences","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046114702&doi=10.1016%2fj.jss.2018.04.015&partnerID=40&md5=62cd448699c200314f028b299a042bde","In software engineering, a design pattern is a general reusable solution to a commonly occurring problem within a given context in software design. Design patterns reveal much about the high-level abstract designs of software systems. Accurately discovering design pattern instances in software systems helps developers and maintainers to understand the original design and implementation, and to facilitate the re-development, upgrade and maintenance. In recent years, numerous approaches have been proposed to discover design pattern instances from system source code. Among them, many transform the source code and design patterns into graphs, and then discover the isomorphic sub-graphs of design patterns from the graph of software system. However, as testing sub-graph isomorphism is an NP-complete problem, those approaches usually fail to achieve satisfactory efficiency. On the other hand, a real time response of detecting pattern instance is in fact essential. To address this problem, we propose a novel efficient approach to detect structural design pattern instances based on directed sub-graph isomorphism. In particular, we put forward a well-designed search order, or Ordered Sequences, by which the candidate pattern instances can be rapidly detected. Because the Ordered Sequences guide the search process in such an order that the most representative classes are discovered first, a large number of irrelevant classes can be filtered out at early stage, which greatly reduces the search space. We employ our approach on four well-known open-source systems. The results of extensive experiments for discovering instances of all GoF structural design patterns verify that our approach obtains 100% recall and the high precision. In addition, the experiment conducted on two other large scale open-source projects indicates that our approach runs significantly faster than the compared approach. © 2018 Elsevier Inc.","Design pattern; Design pattern discovering; Directed sub-graph isomorphism; Ordered sequence; Structural design pattern"
"Developing an integrated framework for using data mining techniques and ontology concepts for process improvement","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036630225&doi=10.1016%2fj.jss.2017.11.019&partnerID=40&md5=7d2fe5e670e2c044e667b992f686292f","Process, as an important knowledge resource, must be effectively managed and improved. The main problems are the large number of processes, their specific features, and the complicated relationships between them, which all lead to the increase in complexity and create a high-dimensionality problem. Traditional process management systems are unable to manage and improve processes with a high volume of data. Data mining techniques, however, can be employed to identify valuable patterns. With the aid of these patterns, suggestions for process improvement can be presented. Further, process ontology can be applied to share the process patterns between people, facilitate the process understanding, and develop the reusability of the extracted patterns for process improvement. This study presents a combined three-part, five-stage framework of data mining, process improvement, and process ontology. To evaluate the applicability and effectiveness of the proposed framework, a real process dataset is applied. Two clustering and classification techniques are used to discover valuable patterns as the process ontology. The output of these two techniques can be considered as the recommendations for improving the processes. The proposed framework can be exploited to support process improvement methodologies in organizations. © 2017 Elsevier Inc.","Classification; Clustering; Data mining; Ontology; Process improvement"
"A survey of schedulability analysis techniques for rate-dependent tasks","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039728570&doi=10.1016%2fj.jss.2017.12.033&partnerID=40&md5=31be58a3344b41d2bd6dbd6426d31cd1","In automotive embedded real-time systems, such as the engine control unit, there are tasks that are activated whenever the crankshaft arrives at a specific angular position. As a consequence the frequency of activation changes with the crankshaft's angular speed (i.e., engine rpm). Additionally, execution times and deadlines may also depend on angular speeds and positions. This paper provides a survey on schedulability analysis techniques for tasks with this rate-dependent behaviour. It covers different task-models and analysis methods for both fixed priority and earliest deadline first scheduling. A taxonomy of the different analysis methods, classifying them according to the assumptions made and the precision of the analysis, is provided at the end of the paper. © 2017 Elsevier Inc.","Adaptive variable-Rate; Automotive; Engine control unit; Rate-dependent; Real-time analysis; Schedulability test"
"Using reliability risk analysis to prioritize test cases","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041473776&doi=10.1016%2fj.jss.2018.01.033&partnerID=40&md5=4e7559c6051db89c2993b6cd38127bc9","In this paper, we present a risk-based test case prioritization (Ri-TCP) algorithm based on the transmission of information flows among software components. Most of the existing approaches rely on the historical code changes or test case execution data, few of them effectively use the system topology information covered by test cases when scheduling the execution of test cases. From the perspective of code structure, the proposed algorithm firstly maps software into an information flow-based directed network model. Then, functional paths covered by each test case are represented by a set of barbell motifs. Finally, combining with probabilistic risk analysis (PRA) and fault tree model, we assign a priority to each test case by calculating the sum of risk indexes of all the barbells covered by it. Experimental results demonstrate that Ri-TCP technique has a higher detection rate of faults with serious risk indicators and performs stably in different systems, compared with the other state-of-the-art algorithms. © 2018 Elsevier Inc.","Complex network; Information flow; Probabilistic risk analysis; Regression testing; Test case prioritization"
"Extracting SBVR business vocabularies and business rules from UML use case diagrams","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045243250&doi=10.1016%2fj.jss.2018.03.061&partnerID=40&md5=37dbc0f347e7ff1c2ec0e83ea84318fb","In model-driven information systems engineering, model transformations reside at the very core of this paradigm. Indeed, model transformations (in particular, model-to-model, or M2M) are a must-have feature of any modern model-driven approach supported by CASE technology. Model transformations are intended to raise quality of the models under development, and also speed-up the modeling itself by bringing in certain level of automation into the development process. Nevertheless, due to certain objective reasons, the level of such automation is spread unevenly throughout the development process – in this respect, Business Modeling and System Analysis are, arguably, the most underdeveloped phases of the model-driven information systems development life cycle. In this paper, we show how M2M transformation technology was used to extract well-structured business vocabularies and business rules from formal use case models represented through a set of use case diagrams; Object Management Group's (OMG) standards Semantics for Business Vocabulary and Rules (SBVR) and Unified Modeling Language (UML) were used for this purpose. The proposed solution consists of two concurrent approaches, namely, automatic and semi-automatic, which may be used selectively to achieve the best expected result. Basic implementation aspects of the solution integrating both approaches are also briefly presented in the paper. While UML use case models is the main subject in this research, the proposed solution may be adopted for other UML and MOF-based models as well. © 2018 Elsevier Inc.","Model-to-model transformation; SBVR business rules; SBVR business vocabulary; UML use case diagram"
"Is cloud storage ready? Performance comparison of representative IP-based storage systems","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041410413&doi=10.1016%2fj.jss.2018.01.015&partnerID=40&md5=1c91ca1bd7d57b72307a69fd53c690df","Network based storage systems have traditionally been dominated by Network Attached Storage (NAS) and Storage Area Network (SAN). Cloud based storage systems, including object storage, have gained growing popularity among both private and enterprise users in recent years. Certain enterprises have even considered replacing traditional storage systems with cloud-based systems. Nevertheless, there still lacks a systematic comparative study on the performance of the aforementioned systems to assist such a transition. To fill in this gap, in this paper, we conduct a comprehensive study on the three major network storage systems with realistic network conditions and application behaviours. Specifically, we select one representative from each category for comparison, i.e., Network File System (NFS) from NAS, Internet Small Computer System Interface (iSCSI) from SAN, and OpenStack Swift from cloud storage. As the first study of its kind, we mainly focus on the client-side and take performance as the perspective for comparison. We build a testbed and a suite of micro-benchmarks to study the impact of network complexities and access behaviours on performance. In addition, we employ two widely used macro-benchmarks – PostMark and FileBench – to test the three systems under realistic workloads. Through a set of comprehensive experiments and thorough analysis, we make several key observations. (1) iSCSI excels under good network conditions, e.g., in local area networks (LANs); when network complexities like network delay and packet loss exist, its performance degrades significantly, especially for data-intensive operations. (2) For Internet-like environments, NFS performs poorly, while Swift demonstrates much resilience. (3) Overall, Swift is a viable replacement for NFS in all network scenarios, while it is not ready yet to replace iSCSI for performance-critical environments. (4) System configuration on the client side impacts storage performance significantly and deserves adequate attention. Based on our experimental study, we also make several recommendations to practitioners and pinpoint aspects for system designers to improve each storage system further. © 2018 Elsevier Inc.","Cloud storage; iSCSI; NAS; NFS; OpenStack Swift; SAN"
"A Systematic Literature Review of iStar extensions","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034790900&doi=10.1016%2fj.jss.2017.11.023&partnerID=40&md5=c546819964b8d477fa6273faa8c1f04a","iStar is a general-purpose goal-based modelling language used to model requirements at early and late phases of software development. It has been used in industrial and academic projects. Often the language is extended to incorporate new constructs related to an application area. The language is currently undergoing standardisation, so several studies have focused on the analysis of iStar variations to identify the similarities and defining a core iStar. However, we believe it will continue to be extended and it is important to understand how iStar is extended. This paper contributes to this purpose through the identification and analysis of the existing extensions and its constructs. A Systematic Literature Review was conducted to guide identification and analysis. The results point to 96 papers and 307 constructs proposed. The extensions and constructs were analysed according to well-defined questions in three dimensions: a general analysis; model-based analysis (to characterise the extensions from semantic and syntactic definitions); and a third dimension related to semiotic clarity. The application area targeted by the iStar extensions and their evolutions are presented as results of our analysis. The results point to the need for more complete, consistent and careful development of iStar extensions. The paper concludes with some discussions and future directions for this research field. © 2017 Elsevier Inc.","Goal modelling; iStar; Modelling language extensions; Systematic Literature Review"
"A resource efficient framework to run automotive embedded software on multi-core ECUs","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041901291&doi=10.1016%2fj.jss.2018.01.040&partnerID=40&md5=9d026a9f683ed003b1882bc1e09275c3","The increasing functionality and complexity of automotive applications requires not only the use of more powerful hardware, e.g., multi-core processors, but also efficient methods and tools to support design decisions. Component-based software engineering proved to be a promising solution for managing software complexity and allowing for reuse. However, there are several challenges inherent in the intersection of resource efficiency and predictability of multi-core processors when it comes to running component-based embedded software. In this paper, we present a software design framework addressing these challenges. The framework includes both mapping of software components onto executable tasks, and the partitioning of the generated task set onto the cores of a multi-core processor. This paper aims at enhancing resource efficiency by optimizing the software design with respect to: 1) the inter-software-components communication cost, 2) the cost of synchronization among dependent transactions of software components, and 3) the interaction of software components with the basic software services. An engine management system, one of the most complex automotive sub-systems, is considered as a use case, and the experimental results show a reduction of up to 11.2% total CPU usage on a quad-core processor, in comparison with the common framework in the literature. © 2018 Elsevier Inc.",""
"Stateless techniques for generating global and local test oracles for message-passing concurrent programs","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034760129&doi=10.1016%2fj.jss.2017.11.026&partnerID=40&md5=b3628cdeba7c5f89ecf8ed5ec17cd1c5","A test oracle for a concurrent program is a method for checking whether an observed behavior of the program is consistent with the program's specification. Abstract specification models for message-passing concurrent programs are often expressed as, or can be translated into, a labeled transition system (LTS). Stateful techniques for generating test oracles from LTS specification models are often limited by the state explosion problem. In this paper, we present a stateless technique for generating global and local test oracles from LTS specification models. A global test oracle uses tests generated from a global LTS model of the complete system to verify a global implementation relation between the model of the system and its implementation. Global test oracles, however, may require too many test sequences to be executed by the implementation. A local test oracle verifies local implementation relations between individual component models and their implementation threads. Local tests are executed against individual threads, without testing the system as a whole. Verifying the local implementation relations implies that a corresponding global implementation relation holds between the complete system model and its implementation. Empirical results indicate that using local test oracles can significantly reduce the number of executed test sequences. © 2017 Elsevier Inc.","Concurrent programs; Model-based testing; Stateless search; Test oracle; Test sequences"
"New deep learning method to detect code injection attacks on hybrid applications","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035777971&doi=10.1016%2fj.jss.2017.11.001&partnerID=40&md5=001db53694341b5690a89f335cdbac30","Mobile phones are becoming increasingly pervasive. Among them, HTML5-based hybrid applications are more and more popular because of their portability on different systems. However these applications suffer from code injection attacks. In this paper, we construct a novel deep learning network, Hybrid Deep Learning Network (HDLN), and use it to detect these attacks. At first, based on our previous work, we extract more features from Abstract Syntax Tree (AST) of JavaScript and employ three methods to select key features. Then we get the feature vectors and train HDLN to distinguish vulnerable applications from normal ones. Finally thorough experiments are done to validate our methods. The results show our detection approach with HDLN achieves 97.55% in accuracy and 97.60% in AUC, which outperforms those with other traditional classifiers and gets higher average precision than other detection methods. © 2017 Elsevier Inc.","Abstract syntax tree; Code injection; Deep learning; Hybrid application"
"Measuring object-oriented design principles: The results of focus group-based research","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045938151&doi=10.1016%2fj.jss.2018.03.002&partnerID=40&md5=d7d7dcd7f0b75011618ef647dfd9f083","Object-oriented design principles are fundamental concepts that carry important design knowledge and foster the development of software-intensive systems with a focus on good design quality. They emerged after the first steps in the field of object-oriented programming and the recognition of best practices in using this programming paradigm to build maintainable software. Although design principles are known by software developers, it is difficult to apply them in practice without concrete rules to follow. We recognized this gap and systematically derived design best practices for a number of design principles and provide tool support for automatic measurement of these practices. The aim of this paper is to examine the relationship between design best practices and 10 selected design principles. This should provide evidence whether the key design aspects of the design principles are covered. We conducted focus group research with six focus groups and 31 participants in total. In parallel, each group discussed five design principles and assessed the coverage by using the Delphi method. Despite suggestions of additional design practices that were added by the participants, the result reveals the impact of each design best practice to the design principle and shows that the main design aspects of the design principles are covered by our approach and is therefore feasible to derive concrete design improvement actions. © 2018 Elsevier Inc.","Design best practices; Design improvement; Design principles; Design rules; Software design quality"
"Hora: Architecture-aware online failure prediction","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015275198&doi=10.1016%2fj.jss.2017.02.041&partnerID=40&md5=ec974f085d8cfef043b5ed207b77a861","Complex software systems experience failures at runtime even though a lot of effort is put into the development and operation. Reactive approaches detect these failures after they have occurred and already caused serious consequences. In order to execute proactive actions, the goal of online failure prediction is to detect these failures in advance by monitoring the quality of service or the system events. Current failure prediction approaches look at the system or individual components as a monolith without considering the architecture of the system. They disregard the fact that the failure in one component can propagate through the system and cause problems in other components. In this paper, we propose a hierarchical online failure prediction approach, called HORA, which combines component failure predictors with architectural knowledge. The failure propagation is modeled using Bayesian networks which incorporate both prediction results and component dependencies extracted from the architectural models. Our approach is evaluated using Netflix's server-side distributed RSS reader application to predict failures caused by three representative types of faults: memory leak, system overload, and sudden node crash. We compare HORA to a monolithic approach and the results show that our approach can improve the area under the ROC curve by 9.9%. © 2017 The Authors","Component-based software systems; Online failure prediction; Reliability"
"AQUArIUM - A suite of software measures for HCI quality evaluation of ubiquitous mobile applications","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034668819&doi=10.1016%2fj.jss.2017.11.022&partnerID=40&md5=1f7d611470590f60beda55167dd87f11","Ubiquitous computing has changed the way users interact with technology. Its applications are everywhere, supporting users in everyday activities in a transparent way with little or no need for attention. To ensure the adoption of these applications, it is essential to assess the quality of the interaction with its users. To do that, measurements can be applied to obtain data about quality characteristics in a software product. In a previous study about quality characteristics and software measures for ubiquitous computing, we have identified a gap in the literature regarding software measures for evaluating essential quality characteristics in ubiquitous systems. Therefore, this paper proposes a suite of well-defined software measures (twenty-four in total), called AQUArIUM, to evaluate human-computer interaction (HCI) in ubiquitous applications for mobile devices. These measures address five quality characteristics of ubiquitous computing: Context-awareness, Mobility, Attention, Calmness and Transparency. The proposed suite was validated theoretically and empirically. The results showed not only that the suite is feasible, but also that it indicates specific problems of HCI quality, which are helpful in improving the evaluated ubiquitous mobile application. © 2017 Elsevier Inc.","Evaluation; Human-computer interaction; Mobile devices; Quality characteristics; Software measures; Ubiquitous computing"
"An effective approach for software project effort and duration estimation with machine learning algorithms","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037534472&doi=10.1016%2fj.jss.2017.11.066&partnerID=40&md5=9db2ffd867c320e60b56603483a60755","During the last two decades, there has been substantial research performed in the field of software estimation using machine learning algorithms that aimed to tackle deficiencies of traditional and parametric estimation techniques, increase project success rates and align with modern development and project management approaches. Nevertheless, mostly due to inconclusive results and vague model building approaches, there are few or none deployments in practice. The purpose of this article is to narrow the gap between up-to-date research results and implementations within organisations by proposing effective and practical machine learning deployment and maintenance approaches by utilization of research findings and industry best practices. This was achieved by applying ISBSG dataset, smart data preparation, an ensemble averaging of three machine learning algorithms (Support Vector Machines, Neural Networks and Generalized Linear Models) and cross validation. The obtained models for effort and duration estimation are intended to provide a decision support tool for organisations that develop or implement software systems. © 2017 Elsevier Inc.","Effort and duration estimation; Ensemble models; ISBSG; Machine learning; Software project estimation"
"Omniscient debugging for executable DSLs","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037973562&doi=10.1016%2fj.jss.2017.11.025&partnerID=40&md5=803be2cd253c6df96df0fefe847f9d0d","Omniscient debugging is a promising technique that relies on execution traces to enable free traversal of the states reached by a model (or program) during an execution. While a few General-Purpose Languages (GPLs) already have support for omniscient debugging, developing such a complex tool for any executable Domain Specific Language (DSL) remains a challenging and error prone task. A generic solution must: support a wide range of executable DSLs independently of the metaprogramming approaches used for implementing their semantics; be efficient for good responsiveness. Our contribution relies on a generic omniscient debugger supported by efficient generic trace management facilities. To support a wide range of executable DSLs, the debugger provides a common set of debugging facilities, and is based on a pattern to define runtime services independently of metaprogramming approaches. Results show that our debugger can be used with various executable DSLs implemented with different metaprogramming approaches. As compared to a solution that copies the model at each step, it is on average sixtimes more efficient in memory, and at least 2.2 faster when exploring past execution states, while only slowing down the execution 1.6 times on average. © 2017 Elsevier Inc.","Domain-Specific Languages; Executable DSL; Execution trace; Omniscient debugging; Software Language Engineering"
"Automatically classifying user requests in crowdsourcing requirements engineering","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039771338&doi=10.1016%2fj.jss.2017.12.028&partnerID=40&md5=88c411b1b66a52d14926f96c629a2b9e","In order to make a software project succeed, it is necessary to determine the requirements for systems and to document them in a suitable manner. Many ways for requirements elicitation have been discussed. One way is to gather requirements with crowdsourcing methods, which has been discussed for years and is called crowdsourcing requirements engineering. User requests forums in open source communities, where users can propose their expected features of a software product, are common examples of platforms for gathering requirements from the crowd. Requirements collected from these platforms are often informal text descriptions and we name them user requests. In order to transform user requests into structured software requirements, it is better to know the class of requirements that each request belongs to so that each request can be rewritten according to corresponding requirement templates. In this paper, we propose an effective classification methodology by employing both project-specific and non-project-specific keywords and machine learning algorithms. The proposed strategy does well in achieving high classification accuracy by using keywords as features, reducing considerable manual efforts in building machine learning based classifiers, and having stable performance in finding minority classes no matter how few instances they have. © 2017","Crowdsourcing requirements engineering; Machine learning; Natural language processing; Software requirements classification"
"Towards cognitive support for unit testing: A qualitative study with practitioners","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056413411&doi=10.1016%2fj.jss.2018.03.052&partnerID=40&md5=fede857f8e77427072fe73f25cdf3d49","Unit testing is an important component of software quality improvement. Several researchers proposed automated tools to improve this activity over the years. However, these research efforts have not been sufficient to help the practitioners to address some associated mental tasks. Motivated by this gap, we conducted a qualitative study of professionals with unit testing experience. The goal was to understand how to improve the cognitive support provided by the testing tools, by considering the practitioners’ perspective on their unit testing review practices. We obtained the responses from our volunteers through a questionnaire composed both of open-ended and closed questions. Our results revealed some primary tasks which require cognitive support, including monitoring of pending and executed unit testing tasks, and navigating across unit testing related artifacts. We summarize our results in a framework, and based on it, we develop a research agenda as an actionable instrument to the community. Our study's contributions comprise practical improvement suggestions for the current tools and describe further opportunities for research in software testing. Moreover, we comprehensively explain our qualitative methods. © 2018 Elsevier Inc.","Cognitive support; Qualitative study; Software Testing; Test Review Practice; Unit testing"
"Characterizing the contribution of quality requirements to software sustainability","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038215582&doi=10.1016%2fj.jss.2017.12.005&partnerID=40&md5=25e186e653748cd719f94ef63920de98","Most respondents considered modifiability as relevant for addressing both technical and environmental sustainability. Functional correctness, availability, modifiability, interoperability and recoverability favor positively the endurability of software systems. This study has also identified security, satisfaction, and freedom from risk as very good contributors to social sustainability. Satisfaction was also considered by the respondents as a good contributor to economic sustainability.; Background Since sustainability became a challenge in software engineering, researchers mainly from requirements engineering and software architecture communities have contributed to defining the basis of the notion of sustainability-aware software. Problem Despite these valuable efforts, the assessment and design based on the notion of sustainability as a software quality is still poorly understood. There is no consensus on which sustainability requirements should be considered. Aim and Method To fill this gap, a survey was designed with a double objective: i) determine to which extent quality requirements contribute to the sustainability of software-intensive systems; and ii) identify direct dependencies among the sustainability dimensions. The survey involved different target audiences (e.g. software architects, ICT practitioners with expertise in Sustainability). We evaluated the perceived importance/relevance of each sustainability dimension, and the perceived usefulness of exploiting a sustainability model in different software engineering activities. Results © 2017 Elsevier Inc.","Quality requirements; Software-intensive systems; Survey; Sustainability"
"Effective fault prediction model developed using Least Square Support Vector Machine (LSSVM)","2018","Journal of Systems and Software","10.1016/j.jss.2017.04.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018252505&doi=10.1016%2fj.jss.2017.04.016&partnerID=40&md5=f06546c2627bca95eed6f155e46bcf26","Software developers and project teams spend considerable amount of time in identifying and fixing faults reported by testers and users. Predicting defects and identifying regions in the source code containing faults before it is discovered or invoked by users can be valuable in terms of saving maintenance resources, user satisfaction and preventing major system failures post deployment. Fault prediction can also improve the effectiveness of software quality assurance activities by guiding the test team to focus efforts on fault prone components. The work presented in this paper involves building an effective fault prediction tool by identifying and investigating the predictive power of several well-known and widely used software metrics for fault prediction. We apply ten different feature selection techniques to choose the best set of metrics from a set of twenty source code metrics. We build the fault prediction model using Least Squares Support Vector Machine (LSSVM) learning method associated with linear, polynomial and radial basis function kernel functions. We perform experiments on 30 Open Source Java projects. Experimental results reveals that our prediction model is best suitable for projects with faulty classes less than the threshold value depending on fault identification efficiency (low- 52.139%, median- 46.206%, and high- 32.080%). © 2017 Elsevier Inc.","CK metrics; Cost analysis; Fault; Feature selection techniques; Least Squares Support Vector Machine (LSSVM); Object-oriented software"
"Heuristic-based approaches for speeding up incremental record linkage","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038810939&doi=10.1016%2fj.jss.2017.11.074&partnerID=40&md5=94a5156f41bf2235f32fc0c978d4c669","Record Linkage is the task of processing a dataset in order to identify which records refer to the same real world entity. The intrinsic complexity of this task brings many challenges to traditional or naive approaches, especially in contexts such as Big Data, unstructured data and frequent data increments over the dataset. To deal with these contexts, especially the latter, an incremental record linkage approach may be employed in order to avoid (re)processing the entire dataset to update the deduplication results. For doing so, different classification techniques can be employed to identify duplicate entities. Recently, many algorithms have been proposed to combine collective classification, which employs clustering algorithms, together with the incremental principle. In this article, we propose new metrics for incremental record linkage using collective classification and new heuristics (which combine clustering, coverage component filters and a greedy approach) to speed up even more a solution to incremental record linkage. These heuristics have been evaluated using three different scale datasets and the results were analyzed and discussed based on both classical and the newly proposed metrics. The experiments present different trade-offs, regarding efficacy and efficiency results, which are generated by the considered heuristics. Also, the results indicate that, for large and frequent data increments, it is possible to slightly reduce efficacy results by employing a coverage filter-based heuristic that is reasonably faster than the current state-of-the-art approach. In turn, it is also possible to employ single-pass clustering algorithms, which are able to execute significantly faster than the state-of-the-art approach at the cost of sacrificing precision results. © 2017 Elsevier Inc.","Deduplication; Heuristics; Incremental clustering; Record linkage"
"Early evaluation of technical debt impact on maintainability","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046356434&doi=10.1016%2fj.jss.2018.04.035&partnerID=40&md5=c8dfec977a146d8645d549a00c82a273","It is widely claimed that Technical Debt is related to quality problems being often produced by poor processes, lack of verification or basic incompetence. Several techniques have been proposed to detect Technical Debt in source code, as identification of modularity violations, code smells or grime buildups. These approaches have been used to empirically demonstrate the relation among Technical Debt indicators and quality harms. However, these works are mainly focused on programming level, when the system has already been implemented. There may also be sources of Technical Debt in non-code artifacts, e.g. requirements, and its identification may provide important information to move refactoring efforts to previous stages and reduce future Technical Debt interest. This paper presents an empirical study to evaluate whether modularity anomalies at requirements level are directly related to maintainability attributes affecting systems quality and increasing, thus, system's interest. The study relies on a framework that allows the identification of modularity anomalies and its quantification by using modularity metrics. Maintainability metrics are also used to assess dynamic maintainability properties. The results obtained by both sets of metrics are pairwise compared to check whether the more modularity anomalies the system presents, the less stable and more difficult to maintain it is. © 2018 Elsevier Inc.","Empirical evaluation; Maintainability; Modularity anomalies; Requirements; Technical Debt indicator"
"A truthful combinatorial double auction-based marketplace mechanism for cloud computing","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043784939&doi=10.1016%2fj.jss.2018.03.003&partnerID=40&md5=c27b626337d79fdc852e2f8ff6889970","Designing market-based mechanism that benefits both the cloud customer and cloud provider in a cloud market is a fundamental but complex problem. Double auction is one such mechanism to allocate resources that prevents monopoly and is used to design an unbiased optimal market strategy for cloud market. This work proposes a truthful combinatorial double auction mechanism for allocation and pricing of computing resources in cloud. For resource allocation, utilitarian social welfare maximization problem is formulated using Integer Linear Programming (ILP) and a near optimal solution is obtained using Linear Programming based padded method. For payment, truthful and novel schemes are designed for both customers and providers. Moreover, the proposed mechanism is individual rational, computationally tractable, weakly budget-balance and asymptotic efficient. Performance evaluation and comparative study exhibit that the proposed mechanism is effective on various performance metrics such as utilitarian social welfare, total utility, customers’ satisfaction, providers’ revenue and hence is applicable in real cloud environments. © 2018 Elsevier Inc.","Cloud computing; Double auction mechanism; Resource allocation; Resource pricing; Truthfulness"
"DIANNE: a modular framework for designing, training and deploying deep neural networks on heterogeneous distributed infrastructure","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044594132&doi=10.1016%2fj.jss.2018.03.032&partnerID=40&md5=7e088d3da4fc61f55c1e8b70b2ef9250","Deep learning has shown tremendous results on various machine learning tasks, but the nature of the problems being tackled and the size of state-of-the-art deep neural networks often require training and deploying models on distributed infrastructure. DIANNE is a modular framework designed for dynamic (re)distribution of deep learning models and procedures. Besides providing elementary network building blocks as well as various training and evaluation routines, DIANNE focuses on dynamic deployment on heterogeneous distributed infrastructure, abstraction of Internet of Things (IoT) sensors, integration with external systems and graphical user interfaces to build and deploy networks, while retaining the performance of similar deep learning frameworks. In this paper the DIANNE framework is proposed as an all-in-one solution for deep learning, enabling data and model parallelism though a modular design, offloading to local compute power, and the ability to abstract between simulation and real environment. © 2018 Elsevier Inc.","Artificial neural networks; Distributed applications; Internet of Things; Machine learning"
"Fault-aware management protocols for multi-component applications","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042472693&doi=10.1016%2fj.jss.2018.02.005&partnerID=40&md5=8173fc78f2865e5a581657bab4c91669","Nowadays, applications are composed by multiple heterogeneous components, whose management must be suitably coordinated by taking into account inter-component dependencies and potential failures. In this paper, we first present fault-aware management protocols, which allow to model the management behaviour of application components, and we then illustrate how such protocols can be composed to analyse and automate the overall management of a multi-component application. We also show how to recover applications that got stuck because a fault was not handled properly, or because a component is behaving differently than expected. To illustrate the feasibility of our approach, we present BARREL, a proof-of-concept application that permits editing and analysing fault-aware management protocols in multi-component applications. We also discuss the usefulness of BARREL by showing how it was fruitfully exploited it in a concrete case study and in a controlled experiment. © 2018 Elsevier Inc.",""
"Model checking real-time conditional commitment logic using transformation","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041293009&doi=10.1016%2fj.jss.2017.12.042&partnerID=40&md5=700479c28d123571b9a66a5870f464d7","A new logical language for real-time conditional commitments called RTCTLcc has been developed by extending the CTL logic with interval bounded until modalities, conditional commitment modalities, and fulfillment modalities. RTCTLcc allows us to express qualitative and quantitative commitment requirements in a convenient way. These requirements can be used to model multi-agent systems (MASs) employed in environments that react properly and timely to events occurring at time instants or within time intervals. However, the timing requirements and behaviors of MASs need an appropriate way to scale and bundle and should be carefully analyzed to ensure their correctness, especially when agents are autonomous. In this paper, we develop transformation algorithms that are fully implemented in a new Java toolkit for automatically transforming the problem of model checking RTCTLcc into the problem of model checking RTCTL (real-time CTL). The toolkit engine is built on top of the NuSMV tool, effectively used to automatically verify and analyze the correctness of real-time distributed systems. We analyzed the time and space computational complexity of the RTCTLcc model checking problem. We proved the soundness and completeness of the transformation technique and experimentally evaluated the validity of the toolkit using a set of business scenarios. Moreover, we added a capability in the toolkit to automatically scale MASs and to bundle requirements in a parametric form. We experimentally evaluated the scalability aspect of our approach using the standard ordering protocol. We further validated the approach using an industrial case study. © 2018 Elsevier Inc.","Complexity; Qualitative and quantitative commitment requirements; Real-time; Transformation technique"
"Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037521968&doi=10.1016%2fj.jss.2017.10.031&partnerID=40&md5=a4529ef4d3402e215bb9b0aad65e8e03","In this paper we propose a new method for generating test scenarios for black-box autonomous systems that demonstrate critical transitions in performance modes. This method provides a test engineer with key insights into the software's decision-making engine and how those decisions affect transitions between performance modes. We achieve this via adaptive, simulation-based testing of the autonomous system where each sample represents a simulated scenario. The test scenario, i.e the system input, represents a given configuration of environmental or mission parameters and the resulting outputs are the system's performance based on high-level success criteria. For realistic testing scenarios, the dimensionality of the configuration space and the computational expense of high-fidelity simulations precludes exhaustive or uniform sampling. Thus, we have developed specialized adaptive search algorithms designed to discover performance boundaries of the autonomy using a minimal number of samples. Further, unsupervised clustering techniques are presented that can group test scenarios by the resulting performance modes and sort them by those which are most effective at diagnosing changes in the autonomous system's behavior. The result is a testing framework that gives the test engineer a set of diverse scenarios that exercises the decision boundaries of the autonomous system under test. © 2017 Elsevier Inc.","Autonomous vehicles; Optimization; Simulation based testing"
"An approach for optimized feature selection in large-scale software product lines","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014360284&doi=10.1016%2fj.jss.2017.02.044&partnerID=40&md5=1cc5daed769058a3aab34af86e4b42e5","Context: Feature selection in product line engineering is an essential step for individual product customization, in which the multiple objectives, that are often competing and conflicting, have to be taken into consideration. These objectives always need to be balanced during selection, leading to a process of multi-objective optimization. What's more, the massive complex dependency and constraint relationships between features present another huge challenge for optimization. Objective: In this work, we propose a multi-objective optimization algorithm, IVEA-II, to automatically search through configurations to obtain an optimal balance between various objectives. Additionally, all the relationships between features must be conformed to by the optimal feature solutions. Method: Firstly, a two-dimensional fitness function in our previous work is reserved. Secondly, to prevent the negative impact of this 2D fitness on the diversity of final Pareto Fronts, the crowding distance is introduced into each fitness-based selection. Lastly, a new mutation operator is designed to improve the scalability of IVEA-II. Results: A series of experiments were conducted to verify the effectiveness of IVEA-II on five large-scale feature models with five optimization goals. Conclusion: Experiments showed that IVEA-II can generate more valid solutions over a set period of time, with optimal solutions also having better diversity and convergence. © 2017 Elsevier Inc.","Feature selection; Multi-objective optimization; Product derivation; Software product lines"
"A survey on reliable distributed communication","2018","Journal of Systems and Software","10.1016/j.jss.2017.03.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017386749&doi=10.1016%2fj.jss.2017.03.028&partnerID=40&md5=357156c87637b69b3dc6a57ed9fae896","From entertainment to personal communication, and from business to safety-critical applications, the world increasingly relies on distributed systems. Despite looking simple, distributed systems hide a major source of complexity: tolerating faults and component crashes is very difficult, due to the incompleteness of (remote) knowledge. The need to overcome this problem, and provide different guarantees to applications, sparked a huge research effort and resulted in a large body of communication protocols, and middleware. Thus, it is worthwhile to survey the state of the art in distributed systems, with a particular emphasis on reliable communication. We discuss key concepts in reliable communication, such as interaction patterns (e.g., one-way vs. request-response, synchronous vs. asynchronous), reliability semantics (e.g., at-least-once, at-most-once), and reliability targets (e.g., message, conversation), and we analyze a wide set of current communication solutions, which map to the different concepts. Building on the concepts, we analyze applications that have different reliable communication needs. As a result, we observe that, in most cases, elaborate communication solutions offering superior guarantees are purely academic efforts that cannot compete with the popularity and maturity of established, albeit poorer solutions. Based on our analysis, we identify and discuss open research topics in this area. © 2017 Elsevier Inc.","Distributed systems; Fault-tolerance; Reliability mechanisms; Reliable communication"
"What happens when software developers are (un)happy","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043363144&doi=10.1016%2fj.jss.2018.02.041&partnerID=40&md5=24352978c578b019d7a23c22199e3e65","The growing literature on affect among software developers mostly reports on the linkage between happiness, software quality, and developer productivity. Understanding happiness and unhappiness in all its components – positive and negative emotions and moods – is an attractive and important endeavor. Scholars in industrial and organizational psychology have suggested that understanding happiness and unhappiness could lead to cost-effective ways of enhancing working conditions, job performance, and to limiting the occurrence of psychological disorders. Our comprehension of the consequences of (un)happiness among developers is still too shallow, being mainly expressed in terms of development productivity and software quality. In this paper, we study what happens when developers are happy and unhappy while developing software. Qualitative data analysis of responses given by 317 questionnaire participants identified 42 consequences of unhappiness and 32 of happiness. We found consequences of happiness and unhappiness that are beneficial and detrimental for developers’ mental well-being, the software development process, and the produced artifacts. Our classification scheme, available as open data enables new happiness research opportunities of cause-effect type, and it can act as a guideline for practitioners for identifying damaging effects of unhappiness and for fostering happiness on the job. © 2018","Affect; Behavioral software engineering; Developer experience; Emotion; Happiness; Human aspects"
"A framework for semi-automated co-evolution of security knowledge and system models","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043281006&doi=10.1016%2fj.jss.2018.02.003&partnerID=40&md5=a6ec4e3a1775b0421f6aa02a243f64b0","Security is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge. We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model. As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems. © 2018 Elsevier Inc.","Co-evolution; Security impact analysis; Security requirements; Software design; Software evolution"
"LASCAD: Language-agnostic software categorization and similar application detection","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045576612&doi=10.1016%2fj.jss.2018.04.018&partnerID=40&md5=8a56008d99669624b34bd7978dea6b78","Categorizing software and detecting similar programs are useful for various purposes including expertise sharing, program comprehension, and rapid prototyping. However, existing categorization and similar software detection tools are not sufficient. Some tools only handle applications written in certain languages or belonging to specific domains like Java or Android. Other tools require significant configuration effort due to their sensitivity to parameter settings, and may produce excessively large numbers of categories. In this paper, we present a more usable and reliable approach of Language-Agnostic Software Categorization and similar Application Detection (LASCAD). Our approach applies Latent Dirichlet Allocation (LDA) and hierarchical clustering to programs’ source code in order to reveal which applications implement similar functionalities. LASCAD is easier to use in cases when no domain-specific tool is available or when users want to find similar software in different programming languages. To evaluate LASCAD's capability of categorizing software, we used three labeled data sets: two sets from prior work and one larger set that we created with 103 applications implemented in 19 different languages. By comparing LASCAD with prior approaches on these data sets, we found LASCAD to be more usable and outperform existing tools. To evaluate LASCAD's capability of similar application detection, we reused our 103-application data set and a newly created unlabeled data set of 5220 applications. The relevance scores of the Top-1 retrieved applications within these two data sets were, separately, 70% and 71%. Overall, LASCAD effectively categorizes and detects similar programs across languages. © 2018 Elsevier Inc.","LDA; Similar applications detection; Software categorization; Source code analysis; Topic modeling"
"Formal semantics of OMG's Interaction Flow Modeling Language (IFML) for mobile and rich-client application model driven development","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037997527&doi=10.1016%2fj.jss.2017.11.067&partnerID=40&md5=571d4edcbeee1f83edfd597ed86083c9","Model Driven Engineering relies on the availability of software models and of development tools supporting the transition from models to code. The generation of code from models requires the unambiguous interpretation of the semantics of the modeling languages used to specify the application. This paper presents the formalization of the semantics of the Interaction Flow Modeling Language (IFML), a recent OMG MDA standard conceived for the specification of the front-end part of interactive applications. IFML constructs are mapped to equivalent structures of Place Chart Nets (PCN), which allows their precise interpretation. The defined semantic mapping is implemented in an online Model-driven development environment that enables the creation and inspection of PCNs from IFML, the analysis of the behavior of IFML specifications via PCN simulation, and the generation of code for mobile and web-based architectures. © 2017 Elsevier Inc.","Mobile applications; Model-driven development; Rich-client applications; Translational semantics"
"A survey on software smells","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044651549&doi=10.1016%2fj.jss.2017.12.034&partnerID=40&md5=0275655072c9d7072652bb43654908e7","Context: Smells in software systems impair software quality and make them hard to maintain and evolve. The software engineering community has explored various dimensions concerning smells and produced extensive research related to smells. The plethora of information poses challenges to the community to comprehend the state-of-the-art tools and techniques. Objective: We aim to present the current knowledge related to software smells and identify challenges as well as opportunities in the current practices. Method: We explore the definitions of smells, their causes as well as effects, and their detection mechanisms presented in the current literature. We studied 445 primary studies in detail, synthesized the information, and documented our observations. Results: The study reveals five possible defining characteristics of smells — indicator, poor solution, violates best-practices, impacts quality, and recurrence. We curate ten common factors that cause smells to occur including lack of skill or awareness and priority to features over quality. We classify existing smell detection methods into five groups — metrics, rules/heuristics, history, machine learning, and optimization-based detection. Challenges in the smells detection include the tools’ proneness to false-positives and poor coverage of smells detectable by existing tools. © 2017","Antipatterns; Code smells; Maintainability; Smell detection tools; Software quality; Software smells; Technical debt"
"JMove: A novel heuristic and tool to detect move method refactoring opportunities","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039165137&doi=10.1016%2fj.jss.2017.11.073&partnerID=40&md5=9791ca0bd18b32894c408a3ac69be93b","This paper presents a recommendation approach that suggests Move Method refactorings using the static dependencies established by methods. This approach, implemented in a publicly available tool called JMove, compares the similarity of the dependencies established by a method with the dependencies established by the methods in possible target classes. We first evaluate JMove using 195 Move Method refactoring opportunities, synthesized in 10 open-source systems. In this evaluation, JMove precision ranges from 21% (small methods) to 32% (large methods) and its median recall ranges from 21% (small methods) to 60% (large methods). In the same scenario, JDeodorant, which is a state-of-the-art Move Method recommender, has a maximal precision of 15% (large methods) and a maximal median recall of 40% (small methods). Therefore, we claim that JMove is specially useful to provide recommendations for large methods. We reinforce this claim by means of two other studies. First, by investigating the overlapping of the recommendations provided by JMove and three other recommenders (JDeodorant, inCode, and Methodbook). Second, by validating JMove and JDeodorant recommendations with experts in two industrial-strength systems. © 2017 Elsevier Inc.","Dependency sets; JDeodorant; JMove; Methodbook; Move method refactorings; Recommendation systems"
"An input-centric performance model for computational offloading of mobile applications","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038911915&doi=10.1016%2fj.jss.2017.12.010&partnerID=40&md5=0f74c6e6255785e45c0316dcc976ebbc","Computational offloading frameworks are a widely-researched technology for optimising mobile applications through the use of cloud resources. Existing frameworks fail to fully account for the effect of input data characteristics on application behaviour. Comprehensive timing models exist in the literature, but feature information requirements and performance overheads that preclude use on mobile devices. In this paper, we propose a conceptual model for an input-centric view of application performance. Our proposed model simplifies the existing count-and-weights and pipeline timing models to significantly reduce their information and processing requirements, facilitating use on resource-constrained mobile devices. Our proposed model also utilises symbolic execution techniques to account for the effects of application input data characteristics. Validation with both synthetic and real device datasets demonstrates that our model provides an extremely accurate approximation of the count-and-weights model. Results demonstrate the predictive power of our model for linear execution paths with no loops or recursion. Further work with improved symbolic execution techniques may look to expand application of our proposed model to real-world use cases. The proposed input-centric approach provides a promising foundation for incorporating a deeper level of application-specific knowledge into computational offloading framework cost models, with the potential to contribute to higher-quality offloading decisions. © 2017 Elsevier Inc.","Computational offloading; Symbolic execution; Timing model"
"JSTrace: Fast reproducing web application errors","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021750946&doi=10.1016%2fj.jss.2017.06.038&partnerID=40&md5=0dc66e77e85948f27c06b7d9bd27d985","JavaScript has become the most popular language for client-side web applications. Due to JavaScript's highly-dynamic and event-driven features, it is challenging to diagnose web application errors. Record-replay techniques are used to reproduce errors in web applications. After a long run, these techniques will record a long event trace that triggers an error. Although the error-related events are few, they are interleaved with other massive error-irrelevant events. It is time-consuming to diagnose errors with long event traces. In this article, we present JSTrace, which effectively removes error-irrelevant events from the long event trace, and further facilitates error diagnosis. Based on fine-grained dependences of JavaScript and DOM instructions, we develop a novel dynamic slicing technique that can remove events irrelevant to the error. We further present rules to remove irrelevant events, which cannot be removed by dynamic slicing. In this process, many events and related instructions are removed without losing the error reproducing accuracy. Our evaluation on 13 real-world web application errors shows that the reduced event traces can faithfully reproduce errors with an average reduction rate of 97%. We further performed case studies on 4 real-world errors, and the result shows that JSTrace is useful to diagnose web application errors. © 2017","Dependence analysis; Dynamic slicing; Event trace reduction; Record-replay"
"FPA-FL: Incorporating static fault-proneness analysis into statistical fault localization","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033606653&doi=10.1016%2fj.jss.2017.11.002&partnerID=40&md5=3b0fdd2c21805a1de3f58af43cf0da59","Despite the proven applicability of the statistical methods in automatic fault localization, these approaches are biased by data collected from different executions of the program. This biasness could result in unstable statistical models which may vary dependent on test data provided for trial executions of the program. To resolve the difficulty, in this article a new ‘fault-proneness’-aware statistical approach based on Elastic-Net regression, namely FPA-FL is proposed. The main idea behind FPA-FL is to consider the static structure and the fault-proneness of the program statements in addition to their dynamic correlations with the program termination state. The grouping effect of FPA-FL is helpful for finding multiple faults and supporting scalability. To provide the context of failure, cause-effect chains of program faults are discovered. FPA-FL is evaluated from different viewpoints on well-known test suites. The results reveal high fault localization performance of our approach, compared with similar techniques in the literature. © 2017 Elsevier Inc.","Backward dynamic slice; Coincidental correctness; Elastic-net regression; Fault localization; Fault-proneness; Statistical debugging"
"Automated and reliable resource release in device drivers based on dynamic analysis","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022065646&doi=10.1016%2fj.jss.2017.06.004&partnerID=40&md5=bde16d1869d89712403a1857b2b15e1d","In a modern operating system, device drivers acquire system resources to work. The acquired resources should be explicitly released by the drivers, because the operating system never reclaims them. Moreover, improper resource release can cause system crashes or hangs. Thus resource release is very important to driver reliability. However, according to our study on Linux driver mailing lists, many applied patches involve the modifications of resource release. Thus current resource management in drivers is not reliable enough. In this paper, we propose a novel approach named AutoRR, which can automatically and reliably release resources based on dynamic analysis. To identify resource handling operations, we use the dynamic specification-mining technique to mine resource acquiring and releasing functions. During execution, we maintain a resource-state list by intercepting the mined functions. If the driver fails to release acquired resources, AutoRR will report bugs and call corresponding releasing functions to safely release the resources. Dynamic analyses of resource dependency, allocation hierarchy, error handling form and releasing time are performed to avoid introducing new bugs when releasing resources. The evaluation on 12 Linux drivers shows that 40 detected bugs are all successfully and safely tolerated, and the overhead is only 7.84%. © 2017","Device drivers; Dynamic analysis; Reliability; Resource management"
"Reusing empirical knowledge during cloud computing adoption","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040085997&doi=10.1016%2fj.jss.2017.12.011&partnerID=40&md5=eb95139224e9fe8341490fc01e10d537","Moving existing legacy systems to cloud platforms is an ever popular option. But, such endeavour may not be hazard-free and demands a proper understanding of requirements and risks involved prior to taking any action. The time is indeed ripe to undertake a realistic view of what migrating legacy systems to cloud may offer, an understanding of exceptional situations causing system quality goal failure in such a transition, and insights on countermeasures. The cloud migration body of knowledge, although is useful, is dispersed over the current literature. It is hard for busy practitioners to digest, synthesize, and harness this body of knowledge into practice when integrating legacy systems with cloud services. We address this issue by creating an innovative synergy between the approaches evidence-based software engineering and goal-oriented modelling. We develop an evidential repository of commonly occurred obstacles and platform agnostic resolution tactics related to cloud enablement of legacy systems. The repository is further utilized during systematic goal-obstacle elaboration of given cloud migration scenarios. The applicability of our proposed framework is also demonstrated. © 2017 Elsevier Inc.","Cloud computing adoption; Evidence-based software engineering; Goal-oriented requirement engineering; Legacy system reengineering; Legacy systems"
"Filling in the missing link between simulation and application in opportunistic networking","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046367533&doi=10.1016%2fj.jss.2018.04.025&partnerID=40&md5=1332e39355526db33c705a8ea5270606","In the domain of opportunistic networking, just like in any other domain of computer science, the engineering process should span all stages between an original idea and the validation of its implementation in real conditions. Yet most researchers often stop halfway along this process: they rely on simulation to validate the protocols and distributed applications they design, and neglect to go further. Their algorithms are thus only rarely implemented for real, and when they are, the validation of the resulting code is usually performed at a very small scale. Therefore, the results obtained are hardly repeatable or comparable to others. LEPTON is an emulation platform that can help bridge the gap between pure simulation and fully operational implementation, thus allowing developers to observe how the software they develop (instead of pseudo-code that simulates its behavior) performs in controlled, repeatable conditions. In this paper we present LEPTON, an emulation platform we developed, and we show how existing opportunistic networking systems can be adapted to run with this platform. Taking two existing middleware systems as use cases, we also demonstrate that running demanding scenarios with LEPTON constitute an excellent stress test and a powerful tool to improve the opportunistic systems under test. © 2018 Elsevier Inc.","Distributed systems; Emulation systems; Opportunistic networking; Software development process; Software evaluation"
"On early detection of application-level resource exhaustion and starvation","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014460361&doi=10.1016%2fj.jss.2017.02.043&partnerID=40&md5=4229c3cb0f5166dd2de7410f726cbb03","Software systems are often engineered and tested for functionality under normal rather than worst-case conditions. This makes the systems vulnerable to denial-of-service attacks, where attackers engineer conditions that result in overconsumption of resources or starvation and stalling of execution. While the security community is well familiar with volumetric resource exhaustion attacks at the network and transport layers, application-specific attacks pose a challenging threat. In this paper, we present Radmin, a novel system for early detection of application-level resource exhaustion and starvation attacks. Radmin works directly on compiled binaries. It learns and executes multiple probabilistic finite automata from benign runs of target programs. Radmin confines the resource usage of target programs to the learned automata and detects resource usage anomalies at their early stages. We demonstrate the effectiveness of Radmin by testing it using a variety of synthetic and in-the-wild attacks. We provide a theoretical analysis of the attacker's knowledge of Radmin and provide a metric for the degree of vulnerability of a program that is protected by Radmin. Finally, we also compare the accuracy and effectiveness of two different architectures, Radmin which works in both the user and kernel spaces, and URadmin which works solely in user space. © 2017 Elsevier Inc.","Early detection; Kernel tracing; Probabilistic finite automata; Resource exhaustion; Starvation"
"A configurable V&V framework using formal behavioral patterns for OSEK/VDX operating systems","2018","Journal of Systems and Software","10.1016/j.jss.2017.07.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044636448&doi=10.1016%2fj.jss.2017.07.040&partnerID=40&md5=2f416188ce973bc1fc9883be81ef0533","Verification and Validation (V&V) of small-scale embedded software must consider the operating system. Unlike general-purpose systems, the underlying operating system is closely coupled with the application logic, generating potentially an infinite number of different control programs depending on the application configuration and application logic. Verifying this software individually is time-consuming and costly, especially when the objective is rigorous verification. To assist in rigorous V&V activities for such embedded software, the proposed work suggests a pattern-based framework that can be used to generate configurable formal OS and test models. At the core of the framework, lies a set of predefined behavioral patterns and constraint patterns that can be composed for the auto-generation of formal models for variously configured operating systems. These configurable formal models form the basis of formal validation and verification activities such as model checking safety properties, model-based test generation, and formal application simulation. We have implemented a prototype tool, specially designed for embedded control software based on the OSEK/VDX international standard, to demonstrate the benefits of the framework in task simulation, test generation, and formal verification. A series of experiments and analysis demonstrate that the suggested pattern-based framework is more efficient in test sequence generation and more effective in identifying problems compared to existing approaches. © 2017 Elsevier Inc.","Embedded software; Formal patterns; OSEK/VDX; V&V framework"
"Scalable code clone detection and search based on adaptive prefix filtering","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037328579&doi=10.1016%2fj.jss.2017.11.039&partnerID=40&md5=945257a6a592299da26f94b8a9437d9d","Code clone detection is a well-known software engineering problem that aims to detect all the groups of code blocks or code fragments that are functionally equivalent in a code base. It has numerous and wide ranging important uses in areas such as software metrics, plagiarism detection, aspect mining, copyright infringement investigation, code compaction, virus detection, and detecting bugs. A scalable code clone detection technique, able to process large source code repositories, is crucial in the context of multi-project or Internet-scale code clone detection scenarios. In this paper, we focus on improving the scalability of code clone detection, relative to current state of the art techniques. Our adaptive prefix filtering technique improves the performance of code clone detection for many common execution parameters, when tested on common benchmarks. The experimental results exhibit improvements for commonly used similarity thresholds of between 40% and 80%, in the best case decreasing the execution time up to 11% and increasing the number of filtered candidates up to 63%. © 2017 Elsevier Inc.","Code clone detection; Prefix filtering; Software maintenance"
"Multi-paradigm deception modeling for cyber defense","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044574554&doi=10.1016%2fj.jss.2018.03.031&partnerID=40&md5=94d4f8437a751a0b873d4f299a35c50e","Security-critical systems demand multiple well-balanced mechanisms to detect ill-intentioned actions and protect valuable assets from damage while keeping costs in acceptable levels. The use of deception to enhance security has been studied for more than two decades. However, deception is still included in the software development process in an ad-hoc fashion, typically realized as single tools or entire solutions repackaged as honeypot machines. We propose a multi-paradigm modeling approach to specify deception tactics during the software development process so that conflicts and risks can be found in the initial phases of the development, reducing costs of ill-planned decisions. We describe a metamodel containing deception concepts that integrates other models, such as a goal-oriented model, feature model, and behavioral UML models to specify static and dynamic aspects of a deception operation. The outcome of this process is a set of deception tactics that is realized by a set of deception components integrated with the system components. The feasibility of this multi-paradigm approach is shown by designing deception defense strategies for a students’ presence control system for the Faculty of Science and Technology of Universidade NOVA de Lisboa. © 2018 Elsevier Inc.","Deception; Model-driven; Security"
"A replicated experiment for evaluating the effectiveness of pairing practice in PSP education","2018","Journal of Systems and Software","10.1016/j.jss.2017.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034035615&doi=10.1016%2fj.jss.2017.08.011&partnerID=40&md5=4968186b53c5be65c2d8f46f2837bc51","Background: Handling large-sized classes is one of the major challenges in Personal Software Process (PSP) education in a tertiary education environment. We applied a pairing approach in PSP education and managed to mitigate the size challenge without sacrificing education effectiveness, which has been verified in an experiment in 2010 (PSP2010). However, there are several issues (e.g., mutual interference among student pairs, confusing evaluation comments, untraceable corrections, etc.) existing in this experiment, which may create mist towards proper understanding of the education approach. Objective: In order to address the identified issues and better understand both pros and cons of the pairing approach, we replicated the experiment in 2014. Method: With new lab arrangement and evaluation mechanism devised, the replication (PSP2014) involved 120 students after their first academic year, who were separated into two groups with 40 pairs of students in one group and 40 solo students in the other. Results: Results of the replication include: 1) paired students conformed process discipline no worse (sometime better) than solo students; 2) paired students performed better than solo students in the final exam; 3) both groups spent comparable amount of time in preparing submissions; 4) both groups performed similar in size estimation and time estimation of the course assignments; 5) the quality of the programs developed by paired students is no less (sometime better) than solo students. Conclusion: The replication together with the original study confirms that, as an education approach, the pairing practice could reduce the amount of submissions required in a PSP training without sacrificing (sometime improving) the education effectiveness. © 2017 Elsevier Inc.","Personal software process; Replication; Software engineering education"
"On the use of replacement messages in API deprecation: An empirical study","2018","Journal of Systems and Software","10.1016/j.jss.2017.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042222030&doi=10.1016%2fj.jss.2017.12.007&partnerID=40&md5=9eaf775ca7768ea3eac148d44cb169c6","Libraries are commonly used to support code reuse and increase productivity. As any other system, they evolve over time, and so do their APIs. Consequently, client applications should be updated to benefit from better APIs. To facilitate this task, API elements should always be deprecated with replacement messages. However, in practice, there are evidences that API elements are deprecated without these messages. In this paper, we study questions regarding the adoption of deprecation messages. Our goal is twofold: to measure the real usage of deprecation messages and to investigate whether a tool is needed to recommend them. We assess (i) the frequency of deprecated elements with replacement messages, (ii) the impact of software evolution on this frequency, and (iii) the characteristics of systems that deprecate API elements in a correct way. Our analysis on 622 Java and 229 C# systems shows that: (i) on the median, 66.7% and 77.8% of the API elements are deprecated with replacement messages per project, (ii) there is no major effort to improve deprecation messages, and (iii) systems that deprecated API elements with messages are different in terms of size and community. As a result, we provide the basis for creating a tool to support clients detecting missing deprecation messages. © 2017 Elsevier Inc.","API deprecation; Empirical software engineering; Mining software repositories; Software evolution"
"An empirical study of collaborative model and its security risk in Android","2018","Journal of Systems and Software","10.1016/j.jss.2017.07.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026630009&doi=10.1016%2fj.jss.2017.07.042&partnerID=40&md5=1b08f016114ea1bdff559abce8cc30e4","Android provides a framework for the development of collaborative applications, which is considered as one of the reasons behind its success. Collaborative model provides flexibility to an application in utilizing services offered by other applications. This approach offers several advantages to developers, such as allowing them to dedicate all of their resources in developing only core functionalities of an application while leveraging services offered by other applications for its auxiliary functionalities. However, the collaborative model also has some disadvantages, such as opening of attack surfaces in an application during exposure of some of its components as it offers its services. Malicious actions can be performed through the exposed components of the application. Android provides permission-based security to protect the exposed components. However, developers must implement the security correctly. In this paper, we empirically evaluate the scale of the collaborative model adopted by Android applications. We also investigate various methods to achieve collaboration among applications. Furthermore, we evaluate the scale of security risk instigated by the collaborative model and perform several other empirical studies on 13,944 Android applications. © 2017 Elsevier Inc.","Android applications; Collaborative application model; Inter-application communications; Permission-based security; Security risk assessment"
"Change impact analysis for evolving configuration decisions in product line use case models","2018","Journal of Systems and Software","10.1016/j.jss.2018.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042435912&doi=10.1016%2fj.jss.2018.02.021&partnerID=40&md5=a4dcacb89fddff65a94b553da6285fb0","Product Line Engineering is becoming a key practice in many software development environments where complex systems are developed for multiple customers with varying needs. In many business contexts, use cases are the main artifacts for communicating requirements among stakeholders. In such contexts, Product Line (PL) use cases capture variable and common requirements while use case-driven configuration generates Product Specific (PS) use cases for each new customer in a product family. In this paper, we propose, apply, and assess a change impact analysis approach for evolving configuration decisions in PL use case models. Our approach includes: (1) automated support to identify the impact of decision changes on prior and subsequent decisions in PL use case diagrams and (2) automated incremental regeneration of PS use case models from PL use case models and evolving configuration decisions. Our tool support is integrated with IBM Doors. Our approach has been evaluated in an industrial case study, which provides evidence that it is practical and beneficial to analyze the impact of decision changes and to incrementally regenerate PS use case models in industrial settings. © 2018 Elsevier Inc.","Change impact analysis; Evolving decisions; Incremental reconfiguration; Product line engineering; Use case configurator; Use case driven development"
"Multi-level contention-free policy for real-time multiprocessor scheduling","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034640520&doi=10.1016%2fj.jss.2017.11.027&partnerID=40&md5=66954eb3594605a8f2d0e36ddde9a7e2","The contention-free policy has received attention in real-time multiprocessor scheduling owing to its wide applicability and significant improvement in offline schedulability guarantees. Utilizing the notion of contention-free slots in which the number of active jobs is smaller than or equal to the number of processors, the policy improves the schedulability by offloading executions in contending time slots to contention-free ones. In this paper, we propose the multi-level contention-free policy by exploiting a new, generalized notion of multi-level contention-free slots. In a case study, we present how the multi-level contention-free policy is applied to EDF (Earliest Deadline First) scheduling and develop a schedulability test for EDF that adopts the new policy. Our evaluation results demonstrate that the multi-level contention-free policy significantly improves the schedulability by up to 4188% and 127%, compared to vanilla EDF and EDF adopting the existing contention-free policy, respectively. © 2017 Elsevier Inc.","Multi-level contention-free policy; Real-time multiprocessor scheduling; Real-time systems; Schedulability analysis"
"Web service discovery based on goal-oriented query expansion","2018","Journal of Systems and Software","10.1016/j.jss.2018.04.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046150897&doi=10.1016%2fj.jss.2018.04.046&partnerID=40&md5=0423e9bb9a140832122c93816a5db767","With the broad adoption of service-oriented architecture, many software systems have been developed by composing loosely-coupled Web services. Service discovery, a critical step of building service-based systems (SBSs), aims to find a set of candidate services for each functional task to be performed by an SBS. The keyword-based search technology adopted by existing service registries is insufficient to retrieve semantically similar services for queries. Although many semantics-aware service discovery approaches have been proposed, they are hard to apply in practice due to the difficulties in ontology construction and semantic annotation. This paper aims to help service requesters (e.g., SBS designers) obtain relevant services accurately with a keyword query by exploiting domain knowledge about service functionalities (i.e., service goals) mined from textual descriptions of services. We firstly extract service goals from services’ textual descriptions using an NLP-based method and cluster service goals by measuring their semantic similarities. A query expansion approach is then proposed to help service requesters refine initial queries by recommending similar service goals. Finally, we develop a hybrid service discovery approach by integrating goal-based matching with two practical approaches: keyword-based and topic model-based. Experiments conducted on a real-world dataset show the effectiveness of our approach. © 2018 The Author(s)","Query expansion; Service discovery; Service goal knowledge; Service-based system (SBS); Web service"
"Requirements engineering: A systematic mapping study in agile software development","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041469765&doi=10.1016%2fj.jss.2018.01.036&partnerID=40&md5=3ce1190e4d0f45c4d2cd0ba04ebe16f8","Context: Requirements engineering in agile software development is a relatively recent software engineering topic and it is not completely explored and understood. The understanding of how this process works on agile world needs a deeper analysis. Objective: The goal of this paper is to map the subject area of requirements engineering in agile context to identify the main topics that have been researched and to identify gaps to develop future researches. It is also intended to identify the obstacles that practitioners face when using agile requirements engineering. Method: A systematic mapping study was conducted and as a result 2171 papers were initially identified and further narrowed to 104 by applying exclusion criteria and analysis. Conclusion: After completing the classification and the analysis of the selected studies it was possible to identify 15 areas (13 based on SWEBOK) where researches were developed. Five of such areas points to the need of future researches, among them are requirements elicitation, change management, measuring requirements, software requirements tools and comparative studies between traditional and agile requirements. In this research, some obstacles that practitioners face dealing with requirements engineering in agile context were also identified. They are related to environment, people and resources. © 2018 Elsevier Inc.","Agile software development; Requirements engineering; Systematic mapping study"
"An efficient method for uncertainty propagation in robust software performance estimation","2018","Journal of Systems and Software","10.1016/j.jss.2018.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041298151&doi=10.1016%2fj.jss.2018.01.010&partnerID=40&md5=77ccc1d5979956d0e84c3703513442e9","Software engineers often have to estimate the performance of a software system before having full knowledge of the system parameters, such as workload and operational profile. These uncertain parameters inevitably affect the accuracy of quality evaluations, and the ability to judge if the system can continue to fulfil performance requirements if parameter results are different from expected. Previous work has addressed this problem by modelling the potential values of uncertain parameters as probability distribution functions, and estimating the robustness of the system using Monte Carlo-based methods. These approaches require a large number of samples, which results in high computational cost and long waiting times. To address the computational inefficiency of existing approaches, we employ Polynomial Chaos Expansion (PCE) as a rigorous method for uncertainty propagation and further extend its use to robust performance estimation. The aim is to assess if the software system is robust, i.e., it can withstand possible changes in parameter values, and continue to meet performance requirements. PCE is a very efficient technique, and requires significantly less computations to accurately estimate the distribution of performance indices. Through three very different case studies from different phases of software development and heterogeneous application domains, we show that PCE can accurately (> 97%) estimate the robustness of various performance indices, and saves up to 225 h of performance evaluation time when compared to Monte Carlo Simulation. © 2018 Elsevier Inc.","Polynomial chaos expansion; Software performance engineering; Uncertainty propagation"
"Mining repair model for exception-related bug","2018","Journal of Systems and Software","10.1016/j.jss.2018.03.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044617398&doi=10.1016%2fj.jss.2018.03.046&partnerID=40&md5=5ff81feb82e40849366596153ec91a55","It has long been a hot research topic to detect and to repair bugs automatically. As a common practice, researchers propose approaches for specific bugs, and their approaches typically are limited in handling the variety among bugs. Recently, researchers start to explore automatic program repair. With predefined repair operators and test cases, test-based repair approaches use search algorithms to generate patches for a bug, until a patch passes all the test cases. To improve the effectiveness to generate patches, Martinez and Monperrus (2013b) proposed an approach that mines repair models from past fixes. Although their approach produces positive results, we argue that it can be feasible to further improve their approach, if we mine repair models for bug categories, instead of all bugs. However, the benefits are still unclear, since existing benchmarks do not classify bugs into categories and existing approaches cannot mine repair models for bug categories. In this paper, we implement a tool, called EXFI, that classifies bugs into categories based on their related exceptions. With its support, we construct a benchmark, in which bug categories are marked. Furthermore, we propose an approach, called MIMO, that mines a repair model for each exception. We compared the general repair model with our mined repair models. Our results show that our mined models are all significantly different from the general model. Outside of the projects where our models are mined, we selected 59 real bugs. For each bug, we used our models and the general model to generate correct repair shapes for these bugs. The results show that for 43 out of 59 bugs, our models found faster a correct shape than the general repair model (Martinez and Monperrus, 2013b), and for 5 bugs, our models were able to find correct shapes that were not found by the compared model. © 2018 Elsevier Inc.","Benchmark; Exception-related bugs; Repair models"
"Crowdsourcing user reviews to support the evolution of mobile apps","2018","Journal of Systems and Software","10.1016/j.jss.2017.11.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037538202&doi=10.1016%2fj.jss.2017.11.043&partnerID=40&md5=fbc0715104455d7bbe18d28732440f07","In recent software development and distribution scenarios, app stores are playing a major role, especially for mobile apps. On one hand, app stores allow continuous releases of app updates. On the other hand, they have become the premier point of interaction between app providers and users. After installing/updating apps, users can post reviews and provide ratings, expressing their level of satisfaction with apps, and possibly pointing out bugs or desired features. In this paper we empirically investigate—by performing a study on the evolution of 100 open source Android apps and by surveying 73 developers—to what extent app developers take user reviews into account, and whether addressing them contributes to apps’ success in terms of ratings. In order to perform the study, as well as to provide a monitoring mechanism for developers and project managers, we devised an approach, named CRISTAL, for tracing informative crowd reviews onto source code changes, and for monitoring the extent to which developers accommodate crowd requests and follow-up user reactions as reflected in their ratings. The results of our study indicate that (i) on average, half of the informative reviews are addressed, and over 75% of the interviewed developers claimed to take them into account often or very often, and that (ii) developers implementing user reviews are rewarded in terms of significantly increased user ratings. © 2017 Elsevier Inc.","Empirical study; Mining app stores; Mobile app evolution; User reviews"
"Evaluating Lehman's Laws of software evolution within software product lines industrial projects","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999663630&doi=10.1016%2fj.jss.2016.07.038&partnerID=40&md5=a699a65ad2018e0c1d7b7aaaf7b96821","The evolution of a single system is a task where we deal with the modification of a single product. Lehman's Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehman's Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality). © 2016 Elsevier Inc.","Empirical study; Lehman's Laws of software evolution; Software evolution; Software product lines"
"Analysing and modelling runtime architectural stability for self-adaptive software","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028078659&doi=10.1016%2fj.jss.2017.07.041&partnerID=40&md5=5f726689fadd0c17b6fd95d344a2c829","With the increased dependence on software, there is a pressing need for engineering long-lived software. As architectures have a profound effect on the life-span of the software and the provisioned quality of service, stable architectures are significant assets. Architectural stability tends to reflect the success of the system in supporting continuous changes without phasing-out. For self-adaptive architectures, the behavioural aspect of stability is essential for seamless operation, to continuously keep the provision of quality requirements stable and prevent unnecessary adaptations that will risk degrading the system. In this paper, we introduce a systematic approach for analysing and modelling architectural stability. Specifically, we leverage architectural concerns and viewpoints to explicitly analyse stability attributes of the intended behaviour. Due to the probabilistic nature of systems’ behaviour, stability modelling is based on a probabilistic relational model for knowledge representation of stability multiple viewpoints. The model, empowered by the quantitative analysis of Bayesian networks, is capable to conduct runtime inference for reasoning about stability under runtime uncertainty. To illustrate the applicability and evaluate the proposed approach, we consider the case of cloud architectures. The results show that the approach increases the efficiency of the architecture in keeping the expected behaviour stable during runtime operation. © 2017 Elsevier Inc.","Architectural stability; Cloud architecture; Quality of service; Self-adaptive architecture; Software architecture; Sustainability"
"A feature matching and transfer approach for cross-company defect prediction","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022069038&doi=10.1016%2fj.jss.2017.06.070&partnerID=40&md5=f0bc3ce293daa6a5eef02ac9a3dd5837","Software defect prediction has drawn much attention of researchers in software engineering. Traditional defect prediction methods aim to build the prediction model based on historical data. For a new project or a project with limited historical data, we cannot build a good prediction model. Therefore, researchers have proposed the cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) methods to share the historical data among different projects. However, the features of cross-company datasets are often heterogeneous, which may affect the feasibility of CCDP. To address the heterogeneous features of CCDP, this paper presents a feature matching and transfer (FMT) approach. First, we conduct feature selection for the source project and get the distribution curves of selected features. Similarly, we also get the distribution curves of all features in the target project. Second, according to the ‘distance’ of different distribution curves, we design a feature matching algorithm to convert the heterogeneous features into the matched features. Finally, we can achieve feature transfer from the source project to the target project. All experiments are conducted on 16 datasets from NASA and PROMISE, and the results show that FMT is effective for CCDP. © 2017 Elsevier Inc.","Feature matching; Feature transfer; Heterogeneous features; Software defect prediction"
"Automated inference of likely metamorphic relations for model transformations","2018","Journal of Systems and Software","10.1016/j.jss.2017.05.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019585597&doi=10.1016%2fj.jss.2017.05.043&partnerID=40&md5=807c4031f3a9777b45533612cf17ff7e","Model transformations play a cornerstone role in Model-Driven Engineering (MDE) as they provide the essential mechanisms for manipulating and transforming models. Checking whether the output of a model transformation is correct is a manual and error-prone task, referred to as the oracle problem. Metamorphic testing alleviates the oracle problem by exploiting the relations among different inputs and outputs of the program under test, so-called metamorphic relations (MRs). One of the main challenges in metamorphic testing is the automated inference of likely MRs. This paper proposes an approach to automatically infer likely MRs for ATL model transformations, where the tester does not need to have any knowledge of the transformation. The inferred MRs aim at detecting faults in model transformations in three application scenarios, namely regression testing, incremental transformations and migrations among transformation languages. In the experiments performed, the inferred likely MRs have proved to be quite accurate, with a precision of 96.4% from a total of 4101 true positives out of 4254 MRs inferred. Furthermore, they have been useful for identifying mutants in regression testing scenarios, with a mutation score of 93.3%. Finally, our approach can be used in conjunction with current approaches for the automatic generation of test cases. © 2017 Elsevier Inc.","Automatic inference; Generic approach; Metamorphic relations; Metamorphic testing; Model transformations; Model-Driven engineering"
"Construction and utilization of problem-solving knowledge in open source software environments","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003977009&doi=10.1016%2fj.jss.2016.06.062&partnerID=40&md5=bc10d7e0ce2f66dbb219348825ee55e4","Open Source Software (OSS) has become an important environment where developers can share reusable software assets in a collaborative manner. Although developers can find useful software assets to reuse in the OSS environment, they may face difficulties in finding solutions to problems that occur while integrating the assets with their own software. In OSS, sharing the experiences of solving similar problems among developers usually plays an important role in reducing problem-solving efforts. We analyzed how developers interact with each other to solve problems in OSS, and found that there is a common pattern of exchanging information about symptoms and causes of a problem. In particular, we found that many problems involve multiple symptoms and causes and it is critical to identify those symptoms and causes early to solve the problems more efficiently. We developed a Bayesian network based approach to semi-automatically construct a knowledge base for dealing with problems, and to recommend potential causes of a problem based on multiple symptoms reported in OSS. Our experiments showed that the approach is effective to recommend the core causes of a problem, and contributes to solving the problem in an efficient manner. © 2016 Elsevier Inc.","Bayesian network; Knowledge-based software reuse; Open source software; Software reuse"
"The Bayesian Network based program dependence graph and its application to fault localization","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028944868&doi=10.1016%2fj.jss.2017.08.025&partnerID=40&md5=ff39f2b2b2f73cda5d48d62bcefbd0bd","Fault localization is an important and expensive task in software debugging. Some probabilistic graphical models such as probabilistic program dependence graph (PPDG) have been used in fault localization. However, PPDG is insufficient to reason across nonadjacent nodes and only support making inference about local anomaly. In this paper, we propose a novel probabilistic graphical model called Bayesian Network based Program Dependence Graph (BNPDG) that has the excellent inference capability for reasoning across nonadjacent nodes. We focus on applying the BNPDG to fault localization. Compared with the PPDG, our BNPDG-based fault localization approach overcomes the reasoning limitation across nonadjacent nodes and provides more precise fault localization by taking its output nodes as the common conditions to calculate the conditional probability of each non-output node. The experimental results show that our BNPDG-based fault localization approach can significantly improve the effectiveness of fault localization. © 2017 Elsevier Inc.","Bayesian network; Fault localization; Program analysis"
"Modeling and automatic code generation for wireless sensor network applications using model-driven or business process approaches: A systematic mapping study","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021624190&doi=10.1016%2fj.jss.2017.06.024&partnerID=40&md5=9791edeefbe15e8292fc6fc21ad22717","This systematic mapping study investigates the modeling and automatic code generation initiatives for wireless sensor network applications based on the IEEE 802.15.4 standard, trying to understand the reasons, characteristics and methods used in the approaches available in the scientific literature, identifying research gaps and potential approaches that can be better exploited, indicating new possibilities of research. The focus is on studies that follow the Model-Driven or Business Process approaches. © 2017 Elsevier Inc.","Business process; Model-driven development; Systematic mapping study; Wireless sensor networks"
"Efficient query processing on large spatial databases: A performance study","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024097362&doi=10.1016%2fj.jss.2017.07.005&partnerID=40&md5=3e8a14687d5301c8727805b54e416351","Processing of spatial queries has been studied extensively in the literature. In most cases, it is accomplished by indexing spatial data using spatial access methods. Spatial indexes, such as those based on the Quadtree, are important in spatial databases for efficient execution of queries involving spatial constraints and objects. In this paper, we study a recent balanced disk-based index structure for point data, called xBR+-tree, that belongs to the Quadtree family and hierarchically decomposes space in a regular manner. For the most common spatial queries, like Point Location, Window, Distance Range, Nearest Neighbor and Distance-based Join, the R-tree family is a very popular choice of spatial index, due to its excellent query performance. For this reason, we compare the performance of the xBR+-tree with respect to the R*-tree and the R+-tree for tree building and processing the most studied spatial queries. To perform this comparison, we utilize existing algorithms and present new ones. We demonstrate through extensive experimental performance results (I/O efficiency and execution time), based on medium and large real and synthetic datasets, that the xBR+-tree is a big winner in execution time in all cases and a winner in I/O in most cases. © 2017 Elsevier Inc.","Performance evaluation; Quadtrees; Query processing; R-trees; Spatial access methods; Spatial databases; xBR-trees"
"A training process for improving the quality of software projects developed by a practitioner","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019656115&doi=10.1016%2fj.jss.2017.05.050&partnerID=40&md5=e540b6df703d61a773fc61bc501635f5","Background The quality of a software product depends on the quality of the software process followed in developing the product. Therefore, many higher education institutions (HEI) and software organizations have implemented software process improvement (SPI) training courses to improve the software quality. Objective Because the duration of a course is a concern for HEI and software organizations, we investigate whether the quality of software projects will be improved by reorganizing the activities of the ten assignments of the original personal software process (PSP) course into a modified PSP having fewer assignments (i.e., seven assignments). Method The assignments were developed by following a modified PSP with fewer assignments but including the phases, forms, standards, and logs suggested in the original PSP. The measurement of the quality of the software assignments was based on defect density. Results When the activities in the original PSP were reordered into fewer assignments, as practitioners progress through the PSP training, the defect density improved with statistical significance. Conclusions Our modified PSP could be applied in academy and industrial environments which are concerned in the sense of reducing the PSP training time. © 2017 Elsevier Inc.","Personal software process; Software engineering education and training; Software process improvement; Software quality improvement"
"Test case prioritization for object-oriented software: An adaptive random sequence approach based on clustering","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032007990&doi=10.1016%2fj.jss.2017.09.031&partnerID=40&md5=b657182eefc5bdd268044b5beca3b75f","Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling the important test cases to be executed earlier, where the importance is determined by some criteria or strategies. Adaptive random sequences (ARSs) can be used to improve the effectiveness of TCP based on white-box information (such as code coverage information) or black-box information (such as test input information). To improve the testing effectiveness for object-oriented software in regression testing, in this paper, we present an ARS approach based on clustering techniques using black-box information. We use two clustering methods: (1) clustering test cases according to the number of objects and methods, using the K-means and K-medoids clustering algorithms; and (2) clustered based on an object and method invocation sequence similarity metric using the K-medoids clustering algorithm. Our approach can construct ARSs that attempt to make their neighboring test cases as diverse as possible. Experimental studies were also conducted to verify the proposed approach, with the results showing both enhanced probability of earlier fault detection, and higher effectiveness than random prioritization and method coverage TCP technique. © 2017 Elsevier Inc.","Adaptive random sequence; Cluster analysis; Object-oriented software; Test cases prioritization; Test cases selection"
"Patterns of developers behaviour: A 1000-hour industrial study","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022015886&doi=10.1016%2fj.jss.2017.06.072&partnerID=40&md5=eff27cb76ac73df078943d3b3fda434c","Monitoring developers’ activity in the Integrated Development Environment (IDE) and, in general, in their working environment, can be useful to provide context to recommender systems, and, in perspective, to develop smarter IDEs. This paper reports results of a long (about 1000 h) observational study conducted in an industrial environment, in which we captured developers’ interaction with the IDE, with various applications available in their workstation, and related them with activities performed on source code files. Specifically, the study involved six developers working on three software systems and investigated (i) how much time developers spent on various activities and how they shift from one activity to another (ii) how developers navigate through the software architecture during their task, and (iii) how the complexity and readability of source code may trigger further actions, such as requests for help or browsing/changing other files. Results of our study suggest that: (i) not surprisingly, developers spend most or their time (∼ 61%) in development activities while the usage of online help is limited (2%) but intensive in specific development sessions; (ii) developers often execute the system under development after working on code, likely to verify the effect of applied changes on the system's behaviour; (iii) while working on files having a high complexity, developers tend to more frequently execute the system as well as to use more online help websites. © 2017 Elsevier Inc.","Case study; Monitoring developers’ activities"
"An experimental replication on the effect of the practice of mindfulness in conceptual modeling performance","2018","Journal of Systems and Software","10.1016/j.jss.2016.06.104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002559536&doi=10.1016%2fj.jss.2016.06.104&partnerID=40&md5=b6d9acffd01f20c0af8c8a7888ad6cb9","Context: Mindfulness is a meditation technique aimed to increase clearness of mind and awareness. In the 2013–2014 academic year, an experiment was carried out to test whether the practice of mindfulness during 4 weeks improved or not the conceptual modeling performance using UML class diagrams of 32 second–year students of Software Engineering at the University of Seville. Objective: An internal replication with some changes in the original design was performed in the first semester of the 2014–2015 academic year in order to confirm the insights provided by the original study and increase the confidence in its conclusions. The sample were 53 students with the same profile than in the original study. Method: Half the students (27 subjects) practiced mindfulness during 6 weeks, while the other half (26 subjects), i.e. the control group, received no treatment during that time. All the students developed two conceptual models using UML class diagrams from a transcript of an interview, one before and another after the 6 weeks of mindfulness sessions, and the results were compared in terms of conceptual modeling effectiveness and efficiency. Results: The results of both experiments were similar, showing that the practice of mindfulness significantly improves conceptual modeling efficiency. Regarding conceptual modeling effectiveness, an improvement is observed in practice, but the analysis shows that such improvement is not statistically significant. After a reanalysis of data, consistent results have also been obtained. Conclusion: After a replication that leads to the same conclusions as the original study, the adequacy of the original experiment is confirmed and the credibility of its results is increased. Thus, we can state that the practice of mindfulness can improve the efficiency of Software Engineering students in the development of conceptual models, although further experimentation is needed in order to confirm the results in other contexts and other Software Engineering activities different from conceptual modeling. © 2016 Elsevier Inc.","Conceptual modeling; Mindfulness; Replication; Software psychology"
"Automatic clustering constraints derivation from object-oriented software using weighted complex network with graph theory analysis","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028058482&doi=10.1016%2fj.jss.2017.08.017&partnerID=40&md5=85d44fc2bba91f523de7e2c6d5d47736","Constrained clustering or semi-supervised clustering has received a lot of attention due to its flexibility of incorporating minimal supervision of domain experts or side information to help improve clustering results of classic unsupervised clustering techniques. In the domain of software remodularisation, classic unsupervised software clustering techniques have proven to be useful to aid in recovering a high-level abstraction of the software design of poorly documented or designed software systems. However, there is a lack of work that integrates constrained clustering for the same purpose to help improve the modularity of software systems. Nevertheless, due to time and budget constraints, it is laborious and unrealistic for domain experts who have prior knowledge about the software to review each and every software artifact and provide supervision on an on-demand basis. We aim to fill this research gap by proposing an automated approach to derive clustering constraints from the implicit structure of software system based on graph theory analysis of the analysed software. Evaluations conducted on 40 open-source object-oriented software systems show that the proposed approach can serve as an alternative solution to derive clustering constraints in situations where domain experts are non-existent, thus helping to improve the overall accuracy of clustering results. © 2017 Elsevier Inc.","Complex network; Constrained clustering; Graph theory; Software clustering; Software remodularisation"
"Lean Internal Startups for Software Product Innovation in Large Companies: Enablers and Inhibitors","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031500265&doi=10.1016%2fj.jss.2017.09.034&partnerID=40&md5=96341e5836b9eb0e134be31a1a6607bc","Context: Startups are disrupting traditional markets and replacing well-established actors with their innovative products.To compete in this age of disruption, large and established companies cannot rely on traditional ways of advancement, which focus on cost efficiency, lead time reduction and quality improvement. Corporate management is now looking for possibilities to innovate like startups. Along with it, the awareness and the use of the Lean startup approach have grown rapidly amongst the software startup community and large companies in recent years. Objective: The aim of this study is to investigate how Lean internal startup facilitates software product innovation in large companies. This study also identifies the enablers and inhibitors for Lean internal startups. Method: A multiple case study approach is followed in the investigation. Two software product innovation projects from two different large companies are examined, using a conceptual framework that is based on the method-in-action framework and extended with the previously developed Lean-Internal Corporate Venture model. Seven face-to-face in-depth interviews of the employees with different roles and responsibilities are conducted. The collected data is analysed through a careful coding process. Within-case analysis and cross-case comparison are applied to draw the findings from the two cases. Results: A generic process flow summarises the common key processes of Lean internal startups in the context of large companies. The findings suggest that an internal startup can be initiated top-down by management, or bottom-up by employees, which faces different challenges. A list of enablers and inhibitors of applying Lean startup in large companies are identified, including top management support and cross-functional team as key enablers. Both cases face different inhibitors due to the different process of inception, objective of the team and type of the product. Conclusions: The contribution of this study for research is threefold. First, this study is one of the first attempt to investigate the use of Lean startup approach in the context of large companies empirically. Second, the study shows the potential of the method-in-action framework to investigate the Lean startup approach in non-startup context. The third contribution is a general process of Lean internal startup and the evidence of the enablers and inhibitors of implementing it, which are both theory-informed and empirically grounded. Future studies could extend our study by addressing the limitations of the research approach undertaken in this study. © 2017 Elsevier Inc.","Internal startup; Large companies; Lean internal startup; Lean startup; Method-in-action; Software product innovation"
"Energy-efficient heterogeneous resource management for wireless monitoring systems","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020481032&doi=10.1016%2fj.jss.2016.09.061&partnerID=40&md5=120fd0dfd730845302fd95d80351e570","Various energy-saving designs have been proposed for reducing the power consumption of processors through dynamic voltage and frequency scaling (DVFS). When dynamic random access memory (DRAM) or peripheral power consumption is high, dynamic power management (DPM) can be adopted to dynamically activate or deactivate devices or to switch them into energy-saving states during idle periods. This paper proposes a heterogeneous resource management mechanism to manage device scheduling for multiple tasks and task scheduling in a processor. A wireless network monitoring system was analyzed as a case study, wherein a resource sharing mechanism was developed for managing the scheduling of multiple wireless adapters, and the concept of instantaneous utilization was leveraged to enable chain-based task scheduling. This paper explores DVFS and DPM energy saving techniques for peripherals and a processor by considering both the required device time and processor time for each task without violating performance requirements under constraints of buffer size. The proposed algorithms were then implemented on a wireless network monitoring system and real traces were collected from a laboratory and downloaded from the UMass Trace Repository for use as inputs. A series of experiments was conducted to evaluate the quality of our algorithms for energy saving within the constraints of system performance requirements and hardware resources. © 2016","Dynamic power management; Dynamic voltage and frequency scaling; Energy-saving designs; Heterogeneous resource management; Resource scheduling; Wireless monitoring systems"
"Taxonomy of workflow partitioning problems and methods in distributed environments","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025602647&doi=10.1016%2fj.jss.2017.05.017&partnerID=40&md5=d8b3920d875eab5260f2fc947fda7ace","A workflow model is the computerized representation of a business or scientific process. It defines the starting and ending conditions of the process, the activities in the process, control flow and data flow among these activities, etc. A partitioning method creates workflow fragments that group some of the workflow model elements (activities, control flows, data flows). Workflow partitioning forms the foundation of decentralised workflow execution and increases scalability, and reuse of partitions. In the literature, different methods have been presented for workflow partitioning and offer a variety of execution approaches; however, there is no existing comprehensive survey and taxonomy of workflow partitioning methods. This article presents an overview of taxonomies characterizing the key concepts of the workflow life cycle and workflow partitioning methods through a comprehensive survey of business and scientific domains in decentralised environments. This in-depth analysis of taxonomies can provide researchers, designers and applications developers with clear guideline to compare current workflow partitioning methods to choose, reuse and compose more vigorous approaches. The article further presents research discussions and future challenges in this area. © 2017 Elsevier Inc.","Decentralised environments; Workflow models; Workflow partitioning method; Workflow taxonomy"
"A decision-making process-line for selection of software asset origins and components","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032856583&doi=10.1016%2fj.jss.2017.09.033&partnerID=40&md5=408a5b3a8913396f6b30e0bda2242837","Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company. © 2017 Elsevier Inc.","Case study; Component-based software engineering; Decision-making"
"An empirical investigation of the influence of persona with personality traits on conceptual design","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030565352&doi=10.1016%2fj.jss.2017.09.020&partnerID=40&md5=01c92d3edc50d193ac9781d9754a8da0","Persona, an archetypical user, is increasingly becoming a popular tool for Software Engineers to design and communicate with stakeholders. A persona is a representative of a class of end users of a product or service. However, the majority of personas presented in the literature do not take into consideration that the personality of users affects the way they interact with a product or service. This study empirically explores variations in conceptual design based on the personality of a persona. We carried out two studies in Australia and one study in Denmark. We presented four personas with different personalities to 91 participants who collectively completed 218 design artifacts. The results from the studies indicate that the participants' views and prioritization of the needs and system requirements were influenced by the personality traits of the provided personas. For an introverted and emotionally unstable personality, inclusion of confidence building and socializer design features had a higher priority compared with the identified requirements for an extravert and emotionally stable personality. The findings support the proposition that personas with personality traits can aid software engineers to produce conceptual designs tailored to the needs of specific personalities. © 2017 Elsevier Inc.","Conceptual design; Design features; Empirical study; Holistic Persona; Persona; Personality traits; User-centered design"
"VISOR: A fast image processing pipeline with scaling and translation invariance for test oracle automation of visual output systems","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020844453&doi=10.1016%2fj.jss.2017.06.023&partnerID=40&md5=646c483dcf65030a99c15d38ff84dd32","Test oracles differentiate between the correct and incorrect system behavior. Hence, test oracle automation is essential to achieve overall test automation. Otherwise, testers have to manually check the system behavior for all test cases. A common test oracle automation approach for testing systems with visual output is based on exact matching between a snapshot of the observed output and a previously taken reference image. However, images can be subject to scaling and translation variations. These variations lead to a high number of false positives, where an error is reported due to a mismatch between the compared images although an error does not exist. To address this problem, we introduce an automated test oracle, named VISOR, that employs a fast image processing pipeline. This pipeline includes a series of image filters that align the compared images and remove noise to eliminate differences caused by scaling and translation. We evaluated our approach in the context of an industrial case study for regression testing of Digital TVs. Results show that VISOR can avoid 90% of false positive cases after training the system for 4 h. Following this one-time training, VISOR can compare thousands of image pairs within seconds on a laptop computer. © 2017 Elsevier Inc.","Black-box testing; Computer vision; Image processing; Test automation; Test oracle"
"Towards collaborative storage scheduling using alternating direction method of multipliers for mobile edge cloud","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028725240&doi=10.1016%2fj.jss.2017.08.032&partnerID=40&md5=399660f3eed524a886b49855283341f0","Performance of cloud computing would be much improved by extending storage capabilities to devices at the edge of network. Unfortunately, the commonly employed algorithms fail to be adaptive to the new storage pattern on mobile edge cloud. To address this issue, we propose a collaborative storage architecture model and an alternating-direction-method-of-multipliers-based collaborative storage scheduling algorithm called ACMES (Algorithm of Collaborative Mobile Edge Storage), in which heterogeneous information of nodes in mobile edge cloud is considered and integrated to make decisions. Besides, feasible solutions for storage will be acquired after iterations of computing. By formulating the collaborative storage scheduling problem in the mobile edge cloud and designing the collaborative decision-making process with the theory of Alternating Direction Method of Multipliers (ADMM), the proposed ACMES is able to minimize power usage and the risk of node withdrawal without reducing the reliability of node storage, and meanwhile make storage scheduling decisions at the edge environment directly and work in a distributed and parallel way. The convergence analysis shows that ACMES has the ability to solve complicated mobile edge cloud storage problems in reality. Extensive experiments validate its effectiveness as well as its superiority to three existing strategies (ADM, RDM and ERASURE) in total cost, reliability, power usage and withdrawal risks. © 2017 Elsevier Inc.","ADMM; Collaborative decision-making; Data storage; Mobile edge cloud"
"Reusability of open source software across domains: A case study","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029586970&doi=10.1016%2fj.jss.2017.09.009&partnerID=40&md5=f72a45af9f2d910e1b0bc70aab5da47a","Exploiting the enormous amount of open source software (OSS) as a vehicle for reuse is a promising opportunity for software engineers. However, this task is far from trivial, since such projects are sometimes not easy to understand and adapt to target systems, whereas at the same time the reusable assets are not obvious to identify. In this study, we assess open source software projects, with respect to their reusability, i.e., the easiness to adapt them in a new system. By taking into account that domain-specific reuse is more beneficial than domain-agnostic; we focus this study on identifying the application domains that contain the most reusable software projects. To achieve this goal, we compared the reusability of approximately 600 OSS projects from ten application domains through a case study. The results of the study suggested that in every aspect of reusability, there are different dominant application domains. However, Science and Engineering Applications and Software Development Tools, have proven to be the ones that are the most reuse-friendly. Based on this observation, we suggest software engineers, who are focusing on the specific application domains, to consider reusing assets from open source software projects. © 2017 Elsevier Inc.","Application domains; Open source; Reusability"
"Minimum-cost deployment of adjustable readers to provide complete coverage of tags in RFID systems","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029941831&doi=10.1016%2fj.jss.2017.09.015&partnerID=40&md5=2f98b8109aa4ee2bea84bd6b32c15335","In Internet of things (IoT), radio frequency identification (RFID) plays an important role to help people rapidly obtain information of objects associated with tags. Passive tags are cheap and require no batteries to operate, so they are widely used in RFID applications. Readers, on the other hand, have to provide power to activate passive tags to get their data. However, collision occurs when two readers send signals to a tag at the same time. Therefore, it is critical to decide the locations of readers, namely reader deployment, to avoid collision. This paper considers adjustable readers, whose transmitted power is configurable to provide different communication range, and proposes a minimum-cost RFID reader deployment (MR2D) problem. Given the positions of tags, it determines how to deploy readers and adjust their transmitted power to cover all tags, such that we can use the minimum number of readers and save their energy. To facilitate data transmission and reduce hardware cost, we restrict the number of tags that each reader can cover and allow readers to have few overlapped tags in their communication range. Then, we develop an efficient solution to the MR2D problem by clustering tags into groups and placing a reader to cover each group to meet the above conditions. Simulation results show that our proposed solution not only saves the number of RFID readers but also reduces their energy consumption, as compared with existing methods. © 2017 Elsevier Inc.","Collision; Coverage; Passive tag; Reader deployment; RFID"
"Stepwise API usage assistance using n-gram language models","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003443942&doi=10.1016%2fj.jss.2016.06.063&partnerID=40&md5=f0f5854667243fb0305845e673529321","Reusing software involves learning third-party APIs, a process that is often time-consuming and error-prone. Recommendation systems for API usage assistance based on statistical models built from source code corpora are capable of assisting API users through code completion mechanisms in IDEs. A valid sequence of API calls involving different types may be regarded as a well-formed sentence of tokens from the API vocabulary. In this article we describe an approach for recommending subsequent tokens to complete API sentences using n-gram language models built from source code corpora. The provided system was integrated in the code completion facilities of the Eclipse IDE, providing contextualized completion proposals for Java taking into account the nearest lines of code. The approach was evaluated against existing client code of four widely used APIs, revealing that in more than 90% of the cases the expected subsequent token is within the 10-top-most proposals of our models. The high score provides evidence that the recommendations could help on API learning and exploration, namely through the assistance on writing valid API sentences. © 2016 Elsevier Inc.","API; Code completion; IDE; N-grams; Usability"
"Quantum genetic algorithm based scheduler for batch of precedence constrained jobs on heterogeneous computing systems","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032223281&doi=10.1016%2fj.jss.2017.10.001&partnerID=40&md5=d60b8730a906bec55660124367e26d3f","Distributed systems are efficient means of realizing High-Performance Computing (HPC). They are used in meeting the demand of executing large-scale high-performance computational jobs. Scheduling the tasks on such computational resources is one of the prime concerns in the heterogeneous distributed systems. Scheduling requires optimizing either single objective or multiple objectives. Load Imbalance (LIB) and Load balancing Cost Ratio (LCR) are two such objectives. LIB and LCR are used in deciding the distribution of load over available resources for utilization and the computational cost respectively. Scheduling jobs with precedence constraints are NP-complete in nature. Scheduling requires either heuristic or metaheuristic approach for sub-optimal but acceptable solutions. Quantum computing is one such metaheuristic approach. It has proven to be promising in addressing the multi-objective scheduling problems with an effective exploration of the search space. This work proposes a dual-objective Quantum-inspired Genetic Algorithm based Load Balancing Strategy (QGLBS) for workflow application with the objective of optimizing both LIB and LCR. The DAG representation of the batch of jobs helps in effective exploitation of the parallelism available at the job level as well as the sub-job level with the modules at the same level of the batch executing at the same time. The strategy ensures that the communication cost and the critical path are considered in making scheduling decisions. QGLBS exploits the features of Quantum Computing and Genetic Algorithm to establish the Pareto fronts using non-dominated sorting based on NSGA-II. Hypervolume measures are used to compare the results of the quality assessment of Pareto fronts. Simulation study on Gridsim ensures that the QGLBS approach works effectively for real life scenario with various workflow applications. © 2017 Elsevier Inc.","Batch scheduling; Heterogeneous distributed systems; Load Balancing; Load balancing cost ratio; Load Imbalance; Workflow application"
"Analyzing software evolution and quality by extracting Asynchrony change patterns","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021174086&doi=10.1016%2fj.jss.2017.05.047&partnerID=40&md5=c7bf0f5894b517c3f232c3f10fb0d4fd","Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones. Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns. © 2017 Elsevier Inc.","Anti-patterns; Change patterns; Clones; Fault-proneness; Software quality"
"Sacbe: A building block approach for constructing efficient and flexible end-to-end cloud storage","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032207809&doi=10.1016%2fj.jss.2017.10.004&partnerID=40&md5=7bf9630a432baca675294e5f11153786","End-to-end solutions enable users to protect their data, before sending them to the cloud, from confidentiality violations, service outages and vendor lock-in incidents. These solutions however require the integration and orchestration of multiple applications that affect the end-user service experience. This paper presents Sacbe, an approach for building efficient and flexible end-to-end cloud storage based on building blocks (BB), which are logical representations of independent applications encapsulated into containers. The developers can build structures such as pipelines, stacks and/or clusters of applications by chaining BBs through I/O interfaces. These structures enable users to move/process data/metadata in continuous dataflows from their devices to the cloud and enables organizations to build cloud storage services. We implemented a complete realization of an end-to-end cloud storage solution, which includes pipelines of BBs running on client-side for end-users to ensure in-house the confidentiality and reliability of data as well as stacks and clusters of BBs to build authentication, sharing, and storage services in a private cloud. This prototype was evaluated through controlled experimentation and a case study based on a satellite imagery, which revealed the feasibility of end-to-end solutions built with Sacbe as the end-user service experience was significantly improved in comparison with other solutions. © 2017 Elsevier Inc.","Building blocks; Cloud storage; End-to-end applications; Pipelines"
"Ontology-based recommender system for COTS components","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026478388&doi=10.1016%2fj.jss.2017.07.031&partnerID=40&md5=3fdf93b0f91608257aadbac1b48719fc","Commercial Off-The-Shelf (COTS) components are coarse-grained software components that satisfy high-level requirements by integrating several services and offering several interfaces. They are usually used to build larger systems. The paper proposes an ontology-based recommender system for COTS components, that contributes to COTS-based development by improving COTS components identification. It combines into a single framework information retrieval technologies and knowledge about COTS components and users in order to provide the most relevant COTS components meeting users needs. The recommender system is based on (1) an ontology of COTS components, named ONTOCOTS, that describes COTS components and unifies their heterogeneous descriptions available on the Web, and (2) a user model that represents user preferences and interest domains. The proposed recommender system is broken down on two main processes. The first one is responsible for extracting information about COTS components from COTS repositories and representing it as ONTOCOTS instances. The second one is the recommendation process during which the user query is expanded using the linguistic ontology WordNet, and is used along with the user profile and the domain ontology ODP (Open Directory Project) to generate a formal query. Results list is ranked according to the satisfaction degree of user requirements and preferences. Experimentations show an amelioration in recommendations relevance by placing the relevant COTS components at the top of the recommendation list. © 2017 Elsevier Inc.","COTS component; Identification; Information extraction; Ontology; Recommender system; User model"
"Introducing continuous experimentation in large software-intensive product and service organisations","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025679495&doi=10.1016%2fj.jss.2017.07.009&partnerID=40&md5=8a88ea0139e8ae47e49587f0850b3999","Software development in highly dynamic environments imposes high risks to development organizations. One such risk is that the developed software may be of only little or no value to customers, wasting the invested development efforts. Continuous experimentation, as an experiment-driven development approach, may reduce such development risks by iteratively testing product and service assumptions that are critical to the success of the software. Although several experiment-driven development approaches are available, there is little guidance available on how to introduce continuous experimentation into an organization. This article presents a multiple-case study that aims at better understanding the process of introducing continuous experimentation into an organization with an already established development process. The results from the study show that companies are open to adopting such an approach and learning throughout the introduction process. Several benefits were obtained, such as reduced development efforts, deeper customer insights, and better support for development decisions. Challenges included complex stakeholder structures, difficulties in defining success criteria, and building experimentation skills. Our findings indicate that organizational factors may limit the benefits of experimentation. Moreover, introducing continuous experimentation requires fundamental changes in how companies operate, and a systematic introduction process can increase the chances of a successful start. © 2017","Agile software development; Continuous experimentation; Experiment-driven software development; Lean software development; Lean startup; Product management"
"Embedding statecharts into Teleo-Reactive programs to model interactions between agents","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019753039&doi=10.1016%2fj.jss.2017.05.081&partnerID=40&md5=9e9f3fe6a49a1f611b82909d44a1dfb8","Context The Teleo-Reactive (TR) approach offers many possibilities for goal-oriented modeling of reactive systems, but it also has drawbacks when the number of interactions among agents is high, leading to barely legible specifications and losing the original benefits of the approach. Objective This work combines the TR paradigm with statecharts and provides advantages for modeling reactive systems and removing the shortcomings detected. Method A basic example is adopted to reveal the problem that appears when agents are modeled only with the TR approach and have frequent interactions with others. This paper proposes an extension to the TR approach that integrates the modeling using statecharts. A transformation procedure from statecharts to TR programs makes it possible to continue using the infrastructure of existing execution platforms such as TeleoR. The approach has been validated for a particular domain by considering a more complex case study in which traditionally there have been no results on the application of the TR paradigm. A survey was carried out on students to verify the benefits of the approach. Results A method to consider statecharts when modeling TR programs. Conclusions Statecharts can facilitate the adoption of the TR approach. © 2017 The Author(s)","Modeling reactive-systems; Statecharts; Teleo-Reactive"
"Enhancing developer recommendation with supplementary information via mining historical commits","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030973152&doi=10.1016%2fj.jss.2017.09.021&partnerID=40&md5=9f04f4a1979cfade92699da191041c4c","Given a software issue request, one important activity is to recommend suitable developers to resolve it. A number of approaches have been proposed on developer recommendation. These developer recommendation techniques tend to recommend experienced developers, i.e., the more experienced a developer is, the more possible he/she is recommended. However, if the experienced developers are hectic, the junior developers may be employed to finish the incoming issue. But they may have difficulty in these tasks for lack of development experience. In this article, we propose an approach, EDR_SI, to enhance developer recommendation by considering their expertise and developing habits. Furthermore, EDR_SI also provides the personalized supplementary information for developers to use, such as personalized source code files, developer network and source-code change history. An empirical study on five open source subjects is conducted to evaluate the effectiveness of EDR_SI. In our study, EDR_SI is also compared with the state-of-art developer recommendation techniques, iMacPro, Location and ABA-Time-tf-idf, to evaluate the effectiveness of developer recommendation, and the results show that EDR_SI can not only improve the accuracy of developer recommendation, but also effectively provide useful supplementary information for them to use when they implement the incoming issue requests. © 2017 Elsevier Inc.","Bug assignment; Collaborative topic modeling; Commit repository; Developer recommendation; Personalized recommendation; Supplementary information recommendation"
"Refining a model for sustained usage of agile methodologies","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026757561&doi=10.1016%2fj.jss.2017.07.010&partnerID=40&md5=3743a851c7b63027115badf140227370","This paper refines a model of Sustained Agile Usage to present a comprehensive understanding of the key factors that are pertinent to the sustained usage of agile methodologies. It describes our qualitative study which involves: (i) a focus group with twenty-nine software industry agile practitioners, and (ii) semi-structured interviews with twenty agile practitioners from five different organizational backgrounds. Data from both methods is used to develop the refined model of Sustained Agile Usage. The refined Sustained Agile Usage Model includes the following three categories: Agile Team Factors, Technological Factors, and Organizational Factors. These revisions are discussed in this research. Finally, we see implications for research: the study offers a useful complement to the few studies that have examined the long-term acceptance of agile methods. The refined model can be used as a reference model to guide future studies to understand sustained usage in different agile domains (e.g. Kanban). Additionally, implications for practice include valuable insights that can help agile teams and others (e.g. top management) to better understand and benchmark how agile methods can be effectively sustained in organizations. © 2017 Elsevier Inc.","Agile assimilation; Agile effectiveness; Multi-method research; Post-adoptive agile method use; Sustained agile usage"
"A statistical analysis approach to predict user's changing requirements for software service evolution","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023616412&doi=10.1016%2fj.jss.2017.06.071&partnerID=40&md5=e74bcdc739d08bf0d0c61e493b398fdc","Evolution is inevitable for almost all software, and may be driven by users’ continuous requests for changes and improvement, the enablement of technology development, among other factors. The evolution of software services can be seen as the evolution of system-user interactions. The capability to accurately and efficiently observe users’ volatile requirements is critical to making timely system improvements to adapt to rapidly changing environments. In this paper, we propose a methodology that employs Conditional Random Fields (CRF) as a means to provide quantitative exploration of system-user interactions that often lead to the discovery of users’ potential needs and requirements. By analyzing users’ run-time behavioral patterns, domain experts can make prompt predictions on how users’ intentions shift, and timely propose system improvements or remedies to help address emerging needs. Our ultimate research goal is to speed up software service evolution to a great extent with automated tools, knowing that the challenge can be undoubtedly steep. The evolution of an online research library service is used to illustrate and evaluate the proposed approach in detail. © 2017 Elsevier Inc.","Conditional Random Fields; Goal inference; Human intention detection; Requirements; Service; Software evolution"
"Reliability and temperature constrained task scheduling for makespan minimization on heterogeneous multi-core platforms","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026838411&doi=10.1016%2fj.jss.2017.07.032&partnerID=40&md5=bb9f6e9b84889314ef48d4f08c7dfff1","We study the problem of scheduling tasks onto a heterogeneous multi-core processor platform for makespan minimization, where each cluster on the platform has a probability of failure governed by an exponential law and the processor platform has a thermal constraint specified by a peak temperature threshold. The goal of our work is to design algorithms that optimize makespan under the constraints of reliability and temperature. We first provide a mixed-integer linear programming (MILP) formulation for assigning and scheduling independent tasks with reliability and temperature constraints on the heterogeneous platform to minimize the makespan. However, MILP takes exponential time to finish. We then propose a two-stage heuristic that determines the assignment, replication, operating frequency, and execution order of tasks to minimize the makespan while satisfying the real-time, reliability, and temperature constraints based on the analysis of the effects of task assignment on makespan, reliability, and temperature. We finally carry out extensive simulation experiments to validate our proposed MILP formulation and two-stage heuristic. Simulation results demonstrate that the proposed MILP formulation can achieve the best performance in reducing makespan among all the methods used in the comparison. The results also show that the proposed two-stage heuristic has a close performance as the representative existing approach ESTS and a better performance when compared to the representative existing approach RBSA, in terms of reducing makespan. In addition, the proposed two-stage heuristic has the highest feasibility as compared to RBSA and ESTS. © 2017 Elsevier Inc.","Makespan minimization; Reliability; Task assignment and scheduling; Temperature"
"Automating the license compatibility process in open source software with SPDX","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003510868&doi=10.1016%2fj.jss.2016.06.064&partnerID=40&md5=c0d6df0b27c54343e58f0645709c5921","Free and Open Source Software (FOSS) promotes software reuse and distribution at different levels for both creator and users, but at the same time imposes some challenges in terms of FOSS licenses that can be selected and combined. The main problem linked to this selection is the presence of a large set of licenses that define different rights and obligations in software use. The problem becomes more evident in cases of complex combinations of software that carries different – often conflicting – licenses. In this paper we are presenting our work on automating license compatibility by proposing a process that examines the structure of Software Package Data Exchange (SPDX) for license compatibility issues assisting in their correct use and combination. We are offering the possibility to detect license violations in existing software projects and make suggestions on appropriate combinations of different software packages. We are also elaborating on the complexity and ambiguity of licensing detection in software products through representative case studies. Our work constitutes a useful process towards automating the analysis of software systems in terms of license use and compatibilities. © 2016 Elsevier Inc.","License compatibility; License violations; Open Source Software; Software Package Data Exchange"
"Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019735323&doi=10.1016%2fj.jss.2017.05.048&partnerID=40&md5=b1d16f8c40086e8fa83de190455e9de4","Context-Aware Software Systems (CASS) use environmental information to provide better service to the systems’ actors to fulfill their goals. Testing of ubiquitous software systems can be challenging since it is unlikely that, while designing the test cases, the tester can identify all possible context variations. A quasi-Systematic Literature Review has been undertaken to characterize the methods usually used for testing CASS. The analysis and generation of knowledge in this work rely on classifying the extracted information. Established taxonomies of software testing and context-aware were used to characterize and interpret the findings. The results show that, although it is possible to observe the utilization of some software testing methods, few empirical studies are evaluating such methods when testing CASS. The selected technical literature conveys a lack of consensus on the understanding of context and CASS, and on the meaning of software testing. Furthermore, context variation in CASS has only been partially addressed by the identified approaches. They either rely on simulating context or in fixing the values of context variables during testing. We argue that the tests of context-aware software systems need to deal with the diversity of context instead of mitigating their effects. © 2017 Elsevier Inc.","Context-aware; Software testing; Systematic literature review; Test case design"
"Understanding the interplay between the logical and structural coupling of software classes","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028968180&doi=10.1016%2fj.jss.2017.08.042&partnerID=40&md5=7a093efd2bbc3a188633bde5d6d2ac45","During the lifetime of object-Oriented (OO) software systems, new classes are added to increase functionality, also increasing the inter-dependencies between classes. Logical coupling depicts the change dependencies between classes, while structural coupling measures source code dependencies induced via the system architecture. The relationship or dependency between logical and structural coupling have been debated in the past, but no large study has confirmed yet their interplay. In this study, we have analysed 79 open-source software projects of different sizes to investigate the interplay between the two types of coupling. First, we quantified the overlapping or intersection of structural and logical class dependencies. Second, we statistically computed the correlation between the strengths of logical and structural dependencies. Third, we propose a simple technique to determine the stability of OO software systems, by clustering the pairs of classes as “stable” or “unstable”, based on their co-change pattern. The results from our statistical analysis show that although there is no strong evidence of a linear correlation between the strengths of the coupling types, there is substantial evidence to conclude that structurally coupled class pairs usually include logical dependencies. However, not all co-changed class pairs are also linked by structural dependencies. Finally, we identified that only a low proportion of structural coupling shows excessive instability in the studied OSS projects. © 2017 Elsevier Inc.","Co-changed structural dependencies (CSD); Coupled logical dependencies (CLD); Object-oriented (OO); Open-source software (OSS); References; Structural coupling"
"Striving for balance: A look at gameplay requirements of massively multiplayer online role-playing games","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028936139&doi=10.1016%2fj.jss.2017.08.009&partnerID=40&md5=291a400cd014381508ec169e78f49f62","Engineering gameplay requirements is the most important task for game development organizations. Game industry discourse is concerned with continuous redesign of gameplay to enhance players’ experience and boost game's appeal. However, accounts of gameplay requirements practices are rare. In responding to calls for more research into gameplay requirements engineering, we performed an exploratory study in the context of massively multiplayer online role-playing games (MMORPGs), from the perspective of practitioners involved in the field. Sixteen practitioners from three leading MMORPG-producing companies were interviewed and their gameplay requirements documents were reviewed. Interviewing and qualitative data analysis occurred in a cyclical process with results at each stage of the study informing decisions about data collection and analysis in the next. The analysis revealed a process of striving to reach a balance among three perspectives of gameplay requirements: a process perspective, an artifact perspective and a player-designer relationship perspective. This balance-driven process is co-created by game developers and players, is endless within the MMORPG, and is happening both in-game and off-game. It heavily relies on ‘paper-prototyping’ and play-testing for the purpose of gameplay requirements validation. The study concludes with discussion on validity threats and on implications for requirements engineering research, practice and education. © 2017 Elsevier Inc.","Empirical research method; Gameplay requirements; Qualitative interview-based study; Requirements elicitation; Requirements engineering"
"Task Scheduling in Big Data Platforms: A Systematic Literature Review","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029718105&doi=10.1016%2fj.jss.2017.09.001&partnerID=40&md5=4053a73dc9624b565403bafbf2cca70c","Context: Hadoop, Spark, Storm, and Mesos are very well known frameworks in both research and industrial communities that allow expressing and processing distributed computations on massive amounts of data. Multiple scheduling algorithms have been proposed to ensure that short interactive jobs, large batch jobs, and guaranteed-capacity production jobs running on these frameworks can deliver results quickly while maintaining a high throughput. However, only a few works have examined the effectiveness of these algorithms. Objective: The Evidence-based Software Engineering (EBSE) paradigm and its core tool, i.e., the Systematic Literature Review (SLR), have been introduced to the Software Engineering community in 2004 to help researchers systematically and objectively gather and aggregate research evidences about different topics. In this paper, we conduct a SLR of task scheduling algorithms that have been proposed for big data platforms. Method: We analyse the design decisions of different scheduling models proposed in the literature for Hadoop, Spark, Storm, and Mesos over the period between 2005 and 2016. We provide a research taxonomy for succinct classification of these scheduling models. We also compare the algorithms in terms of performance, resources utilization, and failure recovery mechanisms. Results: Our searches identifies 586 studies from journals, conferences and workshops having the highest quality in this field. This SLR reports about different types of scheduling models (dynamic, constrained, and adaptive) and the main motivations behind them (including data locality, workload balancing, resources utilization, and energy efficiency). A discussion of some open issues and future challenges pertaining to improving the current studies is provided. © 2017 Elsevier Inc.","Hadoop; Mesos; Spark; Storm; Systematic Literature Review; Task Scheduling"
"Transition of organizational roles in Agile transformation process: A grounded theory approach","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025115622&doi=10.1016%2fj.jss.2017.07.008&partnerID=40&md5=897c7ad6b33d691e0b074a615018718e","This study aims to identify how traditional organizational roles are transformed towards Agile roles. A grounded theory study with 5 software teams in a large software development company in Spain was conducted. We interviewed 21 people, participated in Agile Coaching sessions and observed daily activities in the company during a six-month period. The transition of organizational roles during the Agile transformation process is influenced by many circumstances such as: higher management commitment and support, knowledge about Agile, project team size and culture. The theory demonstrates how enterprise factors influence the transition of organizational roles and how practitioners cope with the challenges during the Agile transformation process. Based on our results, we proposed recommendations and guidelines for organizational roles transition during the Agile method adoption. © 2017 Elsevier Inc.","Agile transformation process; Grounded theory; Organizational roles; Software development"
"Software product lines adoption in small organizations","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020316281&doi=10.1016%2fj.jss.2017.05.052&partnerID=40&md5=8dab176c2398cc79d283e3b7b4f990b2","Context An increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of SPL engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations. Objective The aim of this study is to investigate SPL adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach. Method This paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings. Results The study provides a better understanding of SPL adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an SPL approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the SPL adoption. Conclusion This research has synthesized the available evidence in SPL adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for SPL adoption. © 2017 Elsevier Inc.","Adoption barriers; Case study; Mapping study; Multi-method approach; Software product lines; SPL adoption; Survey"
"Towards a standardized cloud service description based on USDL","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021251249&doi=10.1016%2fj.jss.2017.06.067&partnerID=40&md5=fe4ff722a2a4569efd8c97b246c0b206","In recent years, cloud computing paradigm has attracted a lot of attention from both industry and academia. However, each cloud provider uses its own techniques (languages, standards, ontologies, or models, etc.) to describe cloud services. The diversity of these techniques leads to the vendor lock-in problem, and thus, the lack of a cloud service description standardization. In addition, existing service descriptions cover only particular aspects and neglect others. For example, WSDL covers only technical aspect and does not cover business and semantic ones. Our objective is to define a standardized cloud service description that covers technical, operational, business, and semantic aspects. In this paper, we introduce different approaches that have dealt with cloud service description, and thus, we adopt USDL language as an appropriate technique to describe cloud services thanks to its expressivity by covering three perspectives (technical, operational, and business). But, USDL is still limited because it cannot cover semantic aspect and it is not intended for cloud computing domain. After that, we highlight USDL limitations that can appear in cloud computing domain and that should be taken into consideration in our research work. This paper will focus on establishing a WSMO-based ontology to define semantically cloud services. This new cloud service description is based on USDL and we will enhance it by taking into consideration some USDL limitations. Finally, we test our proposed cloud service description model on a case study to prove its applicability. © 2017 Elsevier Inc.","Cloud computing; Cloud service; Generic cloud service description; Semantic service description; USDL; WSMO Ontology"
"CollabRDL: A language to coordinate collaborative reuse","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012878938&doi=10.1016%2fj.jss.2017.01.031&partnerID=40&md5=fd8149148ac606464439485f96779a16","Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort. © 2017 Elsevier Inc.","Collaboration; Framework; Language; Reuse process; Software reuse"
"Deadlock detection in complex software systems specified through graph transformation using Bayesian optimization algorithm","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020376530&doi=10.1016%2fj.jss.2017.05.128&partnerID=40&md5=caf9d752683c0877711056a4b8be554d","While developing concurrent systems, one of the important properties to be checked is deadlock freedom. Model checking is an accurate technique to detect errors, such as deadlocks. However, the problem of model checking in complex software systems is state space explosion in which all reachable states cannot be generated due to exponential memory usage. When a state space is too large to be explored exhaustively, using meta-heuristic and evolutionary approaches seems a proper solution to address this problem. Recently, a few methods using genetic algorithm, particle swarm optimization and similar approaches have been proposed to handle this problem. Even though the results of recent approaches are promising, the accuracy and convergence speed may still be a problem. In this paper, a novel method is proposed using Bayesian Optimization Algorithm (BOA) to detect deadlocks in systems specified formally through graph transformations. BOA is an Estimation of Distribution Algorithm in which a Bayesian network (as a probabilistic model) is learned from the population and then sampled to generate new solutions. Three different structures are considered for the Bayesian network to investigate deadlocks in the benchmark problems. To evaluate the efficiency of the proposed approach, it is implemented in GROOVE, an open source toolset for designing and model checking graph transformation systems. Experimental results show that the proposed approach is faster and more accurate than existing algorithms in discovering deadlock states in the most of case studies with large state spaces. © 2017 Elsevier Inc.","Bayesian network; Bayesian optimization algorithm; Deadlock detection; Graph transformation system; State space explosion"
"Motivating the contributions: An Open Innovation perspective on what to share as Open Source Software","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030847129&doi=10.1016%2fj.jss.2017.09.032&partnerID=40&md5=8d93a7b2ef817e93016c725261327403","Open Source Software (OSS) ecosystems have reshaped the ways how software-intensive firms develop products and deliver value to customers. However, firms still need support for strategic product planning in terms of what to develop internally and what to share as OSS. Existing models accurately capture commoditization in software business, but lack operational support to decide what contribution strategy to employ in terms of what and when to contribute. This study proposes a Contribution Acceptance Process (CAP) model from which firms can adopt contribution strategies that align with product strategies and planning. In a design science influenced case study executed at Sony Mobile, the CAP model was iteratively developed in close collaboration with the firm's practitioners. The CAP model helps classify artifacts according to business impact and control complexity so firms may estimate and plan whether an artifact should be contributed or not. Further, an information meta-model is proposed that helps operationalize the CAP model at the organization. The CAP model provides an operational OI perspective on what firms involved in OSS ecosystems should share, by helping them motivate contributions through the creation of contribution strategies. The goal is to help maximize return on investment and sustain needed influence in OSS ecosystems. © 2017","Contribution strategy; Open innovation; Open Source Software; Product planning; Product strategy; Software ecosystem"
"Measuring social networks when forming information system project teams","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030538772&doi=10.1016%2fj.jss.2017.09.019&partnerID=40&md5=a44b1f361c23f6dc58622c01ab1b536f","Despite the advances in the project management field, little is known about how creating performing software teams in a systematical and repeatable way. The technical dimension is not enough to achieve this. Software development is a complex collaborative process where people and interpersonal relationships – i.e., how people interact, behave and organize – significantly influence the project success. In this paper, we define a framework to assign people to projects from this socio-technical perspective. Social networks characterizing the interplay between teammates are built and analyzed to predict productive collaborations and identify adequate team-members depending on the organization needs and the kind of project. A noteworthy novelty of these social networks is that they estimate compatibility between coworkers according to previous collaborations, but also according to individuals' social skills. This allows analyzing the compatibility among people who have not worked together before. We present results of using the proposed framework in a multinational corporation during a more-than-two-year period. Our in-company experiments emphasize that we can significantly improve the expected outcomes characterizing and measuring the social interaction among coworkers. Social aspects discussed may be highly relevant in the context of distributed software engineering, since it implies new challenges in the interplay among coworkers. © 2017 Elsevier Inc.","Collaboration network; Interpersonal relationships; Social network analysis; Social skills; Team formation problem; Team member allocation"
"Self-organizing multi-agent systems for the control of complex systems","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028704529&doi=10.1016%2fj.jss.2017.08.038&partnerID=40&md5=1eec80c5399a640633abc90d35d46b9b","Because of the law of requisite variety, designing a controller for complex systems implies designing a complex system. In software engineering, usual top-down approaches become inadequate to design such systems. The Adaptive Multi-Agent Systems (AMAS) approach relies on the cooperative self-organization of autonomous micro-level agents to tackle macro-level complexity. This bottom-up approach provides adaptive, scalable, and robust systems. This paper presents a complex system controller that has been designed following this approach, and shows results obtained with the automatic tuning of a real internal combustion engine. © 2017 Elsevier Inc.","Complex systems; Control; Internal combustion engines; Multi-Agent systems; Self-organization"
"Going with the flow: An activity theory analysis of flow techniques in software development","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002194726&doi=10.1016%2fj.jss.2016.10.003&partnerID=40&md5=01833d60fc38fb2faf99123befcd71a8","Managing flow is fundamental to continuous development, particularly in knowledge intensive work activities such as software development. However, while numerous articles describe flow tools and practice there is little research on their application in context. This is a significant limitation given that software development is a highly complex and socially embedded activity. This research applies activity theory (AT) to examine the adoption of flow techniques by using the multiple-case method in two companies. AT is particularly pertinent in this study as it identifies contradictions, which manifest themselves as problems such as errors or a breakdown of communication in the organisation and congruencies between flow techniques and the development context and indeed contradictions between components of flow techniques themselves. Rather than view contradictions as a threat to flow or as an argument to abandon, a theoretical contribution of this study is that it shows how contradictions and congruencies can be used to reflect, learn, and identify new ways of structuring and enacting the flow activity. It also provides an immediate practical contribution by identifying a set of lessons drawn from the cases studied that may be applicable in future implementations of flow techniques. © 2016 Elsevier Inc.","Activity theory; Continuous software development; Flow; Kanban; Lean"
"Managing architectural technical debt: A unified model and systematic literature review","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030857946&doi=10.1016%2fj.jss.2017.09.025&partnerID=40&md5=7dfc8f932b4d05f452e75a3136f3685c","Large Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD. © 2017 Elsevier Inc.","Architectural technical debt; Software architecture; Software maintenance; Systematic literature review"
"Scalable model exploration for model-driven engineering","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024895767&doi=10.1016%2fj.jss.2017.07.011&partnerID=40&md5=4e452f20a6af824e6046cb49446c67e3","Model-Driven Engineering (MDE) promotes the use of models to conduct all phases of software development in an automated way. However, for complex systems, these models may become large and unwieldy, and hence difficult to process and comprehend. In order to alleviate this situation, we combine model fragmentation strategies – to split models into more manageable chunks – with model abstraction and visualisation mechanisms, able to provide simpler views of the models. In this paper, we describe the underlying methods and techniques, as well as the supporting tools. The feasibility and benefits of our approach are confirmed based on evaluations in the embedded systems, and the reverse engineering domains, where large benefits in terms of visualisation time (speeds up of up to 55 × ), and reduction in memory consumption (reduction of 97%) are obtained. © 2017 Elsevier Inc.","Model abstraction; Model fragmentation; Model scalability; Model visualisation; Model-driven engineering"
"Dataclay: A distributed data store for effective inter-player data sharing","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020444771&doi=10.1016%2fj.jss.2017.05.080&partnerID=40&md5=ed19596cab8549ad3d65837081dd4b5e","In the Big Data era, both the academic community and industry agree that a crucial point to obtain the maximum benefits from the explosive data growth is integrating information from different sources, and also combining methodologies to analyze and process it. For this reason, sharing data so that third parties can build new applications or services based on it is nowadays a trend. Although most data sharing initiatives are based on public data, the ability to reuse data generated by private companies is starting to gain importance as some of them (such as Google, Twitter, BBC or New York Times) are providing access to part of their data. However, current solutions for sharing data with third parties are not fully convenient to either or both data owners and data consumers. Therefore we present dataClay, a distributed data store designed to share data with external players in a secure and flexible way based on the concepts of identity and encapsulation. We also prove that dataClay is comparable in terms of performance with trendy NoSQL technologies while providing extra functionality, and resolves impedance mismatch issues based on the Object Oriented paradigm for data representation. © 2017 Elsevier Inc.","Data sharing; Distributed databases; NoSQL; Storage systems"
"Contract-based testing for PHP with Praspel","2018","Journal of Systems and Software","10.1016/j.jss.2017.06.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020938682&doi=10.1016%2fj.jss.2017.06.017&partnerID=40&md5=34817105bdd422c3af56355e763cad34","We summarize several contributions related to the PHP Realistic Annotation and SPEcification Language (Praspel). This language extends PHP programs with annotations for the formal specification of the behavior of their functions and for the declaration of types for their data. These contracts are used to automate test generation, by deriving test cases and test data, and test execution, by checking assertions at run-time in order to establish the test verdict. Our approach to contract-based testing for PHP is fully implemented into a PHP framework currently in use by several web companies. © 2017 Elsevier Inc.","Annotations; Design by Contract; PHP; Test generation"
"CHAIN: Developing model-driven contextual help for adaptive user interfaces","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032440442&doi=10.1016%2fj.jss.2017.10.017&partnerID=40&md5=9caccf18b9ea5bc6c039fcd187ee3f67","Adaptive user interfaces (UIs) change their presentation at runtime to remain usable in different contexts-of-use. Such changes can cause discrepancies between the UI and static help materials, e.g., videos and screenshots, thereby negatively impacting the latter's usefulness (usability and utility). This paper presents Contextual Help for Adaptive INterfaces (CHAIN), which is an approach for developing model-driven contextual help that maintains its usefulness across UI adaptations. This trait is achieved by interpreting the help models at runtime and overlaying instructions on the running adapted UI. A language called Contextual Help for Adaptive INterfaces eXtensible Markup Language (CHAINXML) and a visual notation were developed for expressing and depicting help models. A technique was also devised for presenting CHAIN help models over legacy applications, whether or not their source-code is available. A supporting tool was developed as an extension to Cedar Studio. This work was empirically evaluated in two studies. The first study performed a preliminary evaluation of CHAIN's visual notation. The second study evaluated CHAIN's strengths and shortcomings after using it to develop help for real-life adaptive UIs. The results gave a positive indication about CHAIN's technical qualities and provided insights that could inform future work. © 2017 Elsevier Inc.","Adaptive user interfaces; Contextual help; Design tools and techniques; Model-driven help; Software support"
"Efficient exact Boolean schedulability tests for fixed priority preemption threshold scheduling","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029543862&doi=10.1016%2fj.jss.2017.09.008&partnerID=40&md5=aaaf6b0915bbbb5d130717106d4534f0","Fixed priority preemption threshold scheduling (PTS) is effective in scalable real-time system design, which requires system tuning processes, where the performance of schedulability tests for PTS matters much. This paper proposes five methods for efficient exact Boolean schedulability tests for PTS, which returns the Boolean result that tells whether the given task is schedulable or not. We regard the sufficient test for fully preemptive fixed priority scheduling (FPS) and the early exit in the response time calculations as the conventional approach. We propose (1) a new sufficient test, (2) new initial values for the start/finish times, (3) pre-calculation of the interference time within the start time, (4) incremental start/finish time calculation, and (5) early exits in start/finish time calculations. These are based on some previous work for FPS. The new initial start time, pre-calculation, and the incremental calculations also can be used for the exact response time analysis for PTS. Our empirical results show that the overall proposed methods reduce the iteration count/run time of the conventional test by about 60%/40%, regardless of the number of tasks and the total utilization. © 2017 Elsevier Inc.","Fixed priority scheduling; Preemption threshold scheduling; Real-time systems and embedded systems; System integration and implementation"
"Fault localisation for WS-BPEL programs based on predicate switching and program slicing","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032839118&doi=10.1016%2fj.jss.2017.10.030&partnerID=40&md5=2f61da38ee16a4d6e02a8a8a3fae819e","Service-Oriented Architecture (SOA) enables the coordination of multiple loosely coupled services. This allows users to choose any service provided by the SOA without knowing implementation details, thus making coding easier and more flexible. Web services are basic units of SOA. However, the functionality of a single Web service is limited, and usually cannot completely satisfy the actual demand. Hence, it is necessary to coordinate multiple independent Web services to achieve complex business processes. Business Process Execution Language for Web Services (WS-BPEL) makes the coordination possible, by helping the integration of multiple Web services and providing an interface for users to invoke. When coordinating these services, however, illegal or faulty operations may be encountered, but current tools are not yet powerful enough to support the localisation and removal of these problems. In this paper, we propose a fault localisation technique for WS-BPEL programs based on predicate switching and program slicing, allowing developers to more precisely locate the suspicious faulty code. Case studies were conducted to investigate the effectiveness of the proposed technique, which was compared with predicate switching only, slicing only, and one existing fault localisation technique, namely Tarantula. The experimental results show that the proposed technique has a higher fault localisation effectiveness and precision than the baseline techniques. © 2017 Elsevier Inc.","Business process execution language for web services; Debugging; Fault localisation; Web services"
"A theory of power in emerging software ecosystems formed by small-to-medium enterprises","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028881203&doi=10.1016%2fj.jss.2017.08.044&partnerID=40&md5=b238e24e67c1ad7b3252e0a9f9e4c493","In a software ecosystem, partner companies rely on each other to succeed and survive. This scenario of mutual dependence entails a flow of power among companies. Power is an intrinsic property of their relationship and an asset to be exercised with a degree of intentionality. This paper presents a substantive theory to explain how power and dependence manifest in partnerships among small-to-medium enterprises (SMEs) building a software ecosystem. We performed exploratory case studies of two emerging software ecosystems formed by SMEs. We interpreted the results in light of a theoretical framework underpinned by French and Raven's power taxonomy. Finally, we performed a cross-case analysis to evaluate our findings and build the theory. The proposed theory highlights the interactions among different forms of power and corresponding sources of power employed by companies. It provides a better understanding on how power and dependence influence the behaviour and coordination of companies within a software ecosystem. The theory is a useful lens for researchers to explore ecosystem partnerships by understanding the structure and impact of power relationships between partners. In addition, it is a valuable tool for companies to analyse power distribution and define sustainable strategies for software ecosystem governance. © 2017 Elsevier Inc.","Dependence; Multiple case studies; Power; Small-to-medium enterprises; Software ecosystem"
"Increasing the capturing angle in print-cam robust watermarking","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032914624&doi=10.1016%2fj.jss.2017.10.029&partnerID=40&md5=6600bdfab5f307c1d453eb8762810965","It is nowadays more probable that a print media is captured and shared with a mobile phone than with a scanner. The reasons for photographing the print range from intention of copying the image to simply sharing an interesting add with friends. Watermarking offers a solution for carrying side information in the images, and if the watermarking method being used is robust to the print-cam process, the information can be read with a mobile phone camera. In this paper, we present a print-cam robust watermarking method that is also implemented on a mobile phone and evaluated with user tests. Especially, the lens focusing problem when the picture is captured in a wide angle with respect to the printout is addressed. The results show that the method is highly robust to capturing the watermark without errors in angles up to 60° with processing times that are acceptable for real-life applications. © 2017 Elsevier Inc.","Camera phone; Computational photography; Focal stack optimization; Mobile phone application; Watermark application"
"An industrial case study on an architectural assumption documentation framework","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029575478&doi=10.1016%2fj.jss.2017.09.007&partnerID=40&md5=e7acc71d3b88f997f5dd40071017d612","As an important type of architectural knowledge, documenting architectural assumptions (AAs) is critical to the success of projects. In this work, we proposed and validated an Architectural Assumption Documentation Framework (AADF), which is composed of four viewpoints (i.e., the Detail, Relationship, Tracing, and Evolution viewpoint), to document AAs in projects. One case study with two cases was conducted at two companies from different domains and countries. The main findings are: (1) AADF can be understood by architects in a short time (i.e., a half day workshop); (2) the AA Evolution view requires the least time to create, followed by the AA Detail view and the AA Relationship view; (3) AADF can help stakeholders to identify risks and understand AAs documented by other stakeholders; and (4) understanding and applying AADF is related to various factors, including factors regarding the framework per se (e.g., tutorial, examples, concepts, and terms), personal experience, resources (e.g., time), tool support, and project context (e.g., project size and number of AAs). Adjusting these factors in an appropriate way can facilitate the usage of AADF and further benefit the projects. © 2017 Elsevier Inc.","Architectural assumption; Case study; Documentation framework; Software architecture"
"A PSO-GA approach targeting fault-prone software modules","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021632215&doi=10.1016%2fj.jss.2017.06.059&partnerID=40&md5=3b26b3213c1ae5681e163a117f6f44e1","We present an algorithm to classify software modules as fault-prone or not using object-oriented metrics. Our algorithm is a combination of particle swarm intelligence and genetic algorithms. We empirically validate it on eight different data sets. We also compare it to well known classification techniques. Results show that our algorithm has several advantages over other techniques. © 2017 Elsevier Inc.","Classification; Fault-proneness; Genetic algorithms; Heuristics; Metrics; Particle swarm optimization; Swarm intelligence"
"R-SHT: A state history tree with R-Tree properties for analysis and visualization of highly parallel system traces","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031507778&doi=10.1016%2fj.jss.2017.09.023&partnerID=40&md5=8d21d921bafa517c3f75342841387a1e","Understanding the behaviour of distributed computer systems with many threads and resources is a challenging task. Dynamic analysis tools such as tracers have been developed to assist programmers in debugging and optimizing the performance of such systems. However, complex systems can generate huge traces, with billions of events, which are hard to analyze manually. Trace visualization and analysis programs aim to solve this problem. Such software needs fast access to data, which a linear search through the trace cannot provide. Several programs have resorted to stateful analysis to rearrange data into more query friendly structures. In previous work, we suggested modifications to the State History Tree (SHT) data structure to correct its disk and memory usage. While the improved structure, eSHT, made near optimal disk usage and had reduced memory usage, we found that query performance, while twice as fast, exhibited scaling limitations. In this paper, we proposed a new structure using R-Tree techniques to improve query performance. We explain the hybrid scheme and algorithms used to optimize the structure to model the expected behaviour. Finally, we benchmark the data structure on highly parallel traces and on a demanding trace visualization use case. Our results show that the hybrid R-SHT structure retains the eSHT's optimal disk usage properties while providing several orders of magnitude speed up to queries on highly parallel traces. © 2017 Elsevier Inc.","Data structure; External memory; Stateful analysis; Tracing; Tree"
"Examining decision characteristics & challenges for agile software development","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021182926&doi=10.1016%2fj.jss.2017.06.003&partnerID=40&md5=3c85938288ee0d24e8407302293def45","Although agile software development is often associated with improved decision making, existing studies tend to focus on narrow aspects of decision making in such environments. There is a lack of clarity on how teams make and evaluate a myriad of decisions from software feature inception to product delivery and refinement. Indeed there is relatively little known about a) the decision characteristics related to agile values, and b) the challenges they present for decision making on agile teams. We present an in-depth exploratory case study based on a pluralistic approach comprising semi-structured interviews, focus groups, team meeting observations, and document analysis. The study identifies failings of decision making in an agile setting. Explicitly considering the decision process, information intelligence used in decision making, and decision quality, the key contribution of this paper is the development of an over-arching framework of agile decision making, which identifies particular decision characteristics across 4 key agile values and the related challenges for agile team decision making. It provides a framework for researchers and practitioners to evaluate the decision challenges of an agile software development team and to improve decision quality. © 2017 Elsevier Inc.","Agile decision making; Agile software development; Decision characteristics; Decision intelligence; Decision process; Decision quality"
"Rendex: A method for automated reviews of textual requirements","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019738337&doi=10.1016%2fj.jss.2017.05.079&partnerID=40&md5=bd6d01c334450ce384b7a89c7ac05b47","Conducting requirements reviews before the start of software design is one of the central goals in requirements management. Fast and accurate reviews promise to facilitate software development process and mitigate technical risks of late design modifications. In large software development companies, however, it is difficult to conduct reviews as fast as needed, because the number of regularly incoming requirements is typically several thousand. Manually reviewing thousands of requirements is a time-consuming task and disrupts the process of continuous software development. As a consequence, software engineers review requirements in parallel with designing the software, thus partially accepting the technical risks. In this paper we present a measurement-based method for automating requirements reviews in large software development companies. The method, Rendex, is developed in an action research project in a large software development organization and evaluated in four large companies. The evaluation shows that the assessment results of Rendex have 73%-80% agreement with the manual assessment results of software engineers. Succeeding the evaluation, Rendex was integrated with the requirements management environment in two of the collaborating companies and is regularly used for proactive reviews of requirements. © 2017 Elsevier Inc.","Complexity; Measure; Metric; Quality prediction; Requirements quality; Requirements review"
"Large universe attribute based access control with efficient decryption in cloud storage system","2018","Journal of Systems and Software","10.1016/j.jss.2017.10.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032299210&doi=10.1016%2fj.jss.2017.10.020&partnerID=40&md5=379fc3c1fbcffe4bfab84c14ebf3df8e","Ciphertext Policy Attribute Based Encryption scheme is a promising technique for access control in the cloud storage, since it allows the data owner to define access policy over the outsourced data. However, the existing attribute based access control mechanism in the cloud storage is based on small universe construction, where the attribute set is defined at setup, and the size of the public parameters scales with the number of attributes. A large number of new attributes need to be added to the system over time, small universe attribute based access control is no longer suitable for cloud storage, whereas large universe attribute based encryption where any string can be employed as an attribute and attributes are not required to be enumerated at system setup meets this requirement. Unfortunately, one of the main efficiency drawbacks of existing large universe attribute based encryption is that ciphertext size and decryption time scale with the complexity of the access structure. In this work, we propose large universe attribute based access control scheme with efficient decryption. The user provides the cloud computing server with a transformation key with which the cloud computing server transforms the ciphertext associated with the access structure satisfied by the attributes associated with the private key into a simple and short ciphertext; thus it significantly reduces the time for the user to decrypt the ciphertext without the cloud computing server knowing the underlying plaintext; the user can check whether the transformation done by the cloud computing server is correct to verify transformation correctness. Security analysis and performance evaluation show our scheme is secure and efficient. © 2017 Elsevier Inc.","Attribute based encryption; Cloud storage; Decryption outsourcing; Fine grained access control; Large universe construction"
"Implementation relations and probabilistic schedulers in the distributed test architecture","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017441420&doi=10.1016%2fj.jss.2017.03.011&partnerID=40&md5=f5af7e6ddf44cadf41dab1add6ad6e27","We present a complete framework to formally test systems with distributed ports where some choices are probabilistically quantified while other choices are non-deterministic. We define different implementation relations, that is, relations that state what it means for a system to be a valid implementation of a specification. We also study how these relate. In order to define these implementation relations we use probabilistic schedulers, a more powerful version, including probabilistic choices, of a notion of scheduler introduced in our previous work. Probabilistic schedulers, when applied to either a specification or an implementation, resolve all the possible non-determinism, so that we can compare purely probabilistic systems. © 2017 Elsevier Inc.","Distributed systems; Model-based testing; Probabilistic systems"
"A framework for automatically ensuring the conformance of agent designs","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021148728&doi=10.1016%2fj.jss.2017.05.098&partnerID=40&md5=116e5fa8187fa8a4127f700619a3ba1c","Multi-agent systems are increasingly being used in complex applications due to features such as autonomy, pro-activity, flexibility, robustness and social ability. These very features also make verifying multi-agent systems a challenging task. In this article, we propose a mechanism, including automated tool support, for early phase defect detection by comparing the plan structures of a Belief-Desire-Intention agent design against the requirements models and interaction protocols. The basic intuition of our approach is to extract sets of possible behaviour runs from the agents’ behaviour models and to verify whether these runs conform to the specifications of the system-to-be or not. This approach is applicable at design time, not requiring source code, thus enabling detection and removal of some defects at an early phase of the software development lifecycle. We followed an experimental approach for evaluating the proposed verification framework. Our evaluation shows that even simple system's specifications developed by relatively experienced developers are prone to defects, and our approach is successful in uncovering most of these defects. In addition, we conducted a scalability analysis on the approach, and the outcomes show that our approach can scale when designs grow in size. © 2017 Elsevier Inc.","Agent-oriented software engineering; Multi-agent systems; Verification"
"Quality of service approaches in IoT: A systematic mapping","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024501531&doi=10.1016%2fj.jss.2017.05.125&partnerID=40&md5=dcaaf4716e6c7a375462cb74a153e7bf","In an Internet of Things (IoT) environment, the existence of a huge number of heterogeneous devices, which are potentially resource-constrained and/or mobile has led to quality of service (QoS) concerns. Quality approaches have been proposed at various layers of the IoT architecture and take into consideration a number of different QoS factors. This paper evaluates the current state of the art of proposed QoS approaches in the IoT, specifically: (1) What layers of the IoT architecture have had the most research on QoS? (2) What quality factors do the quality approaches take into account when measuring performance? (3) What types of research have been conducted in this area? We have conducted a systematic mapping using a number of automated searches from the most relevant academic databases to address these questions. This mapping has identified a number of state of the art approaches which provides a good reference for researchers. The paper also identifies a number of gaps in the research literature at specific layers of the IoT architecture. It identifies which quality factors, research and contribution facets have been underutilised in the state of the art. © 2017 Elsevier Inc.","Internet of things (IoT); Monitoring; Quality model; Quality of service (QoS); Systematic mapping"
"Environmental factors analysis and comparison affecting software reliability in development of multi-release software","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021675237&doi=10.1016%2fj.jss.2017.05.097&partnerID=40&md5=3fa5b184ea49733c6d1066ad6a782cfe","As the application of the principles of agile and lean software development, software multiple release becomes very common in the modern society. Short iteration and short release cycle have driven the significant changes of the development process of multi-release software product, compared with single release software product. Thus, it is time to conduct a new study investigating the impact level of environmental factors on affecting software reliability in the development of multi-release software to provide a sound and concise guidance to software practitioners and researchers. Statistical learning methods, like principle component analysis, stepwise backward elimination, lasso regression, multiple linear regression, and Tukey method, are applied in this study. Comparisons regarding significant environmental factors during the whole development process, principle components, significant environmental factors in each development phase and significance level of each development phase between the development of single release software and multi-release software are also discussed. © 2017 Elsevier Inc.","Environmental factors; Lasso regression; Multi-release software; Multiple linear regression; Principle component analysis; Stepwise backward elimination"
"Obscured by the cloud: A resource allocation framework to model cloud outage events","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021142877&doi=10.1016%2fj.jss.2017.06.022&partnerID=40&md5=c4a329037cc111b4a45ceaa3ece4b9bd","As Small Medium Enterprises (SMEs) adopt Cloud technologies to provide high value customer offerings, uptime is considered important. Cloud outages represent a challenge to SMEs and micro teams to maintain a services platform. If a Cloud platform suffers from downtime this can have a negative effect on business revenue. Additionally, outages can divert resources from product development/delivery tasks to reactive remediation. These challenges are immediate for SMEs or micro teams with a small levels of resources. In this paper we present a framework that can model the arrival of Cloud outage events. This framework can be used by DevOps teams to manage their scarce pool of resources to resolve outages, thereby minimising impact to service delivery. We analysed over 300 Cloud outage events from an enterprise data set. We modelled the inter-arrival and service times of each outage event and found a Pareto and a lognormal distribution to be a suitable fit. We used this result to produce a special case of the G/G/1 queue system to predict busy times of DevOps personnel. We also investigated dependence between overlapping outage events. Our predictive queuing model compared favourably with observed data, 72% precision was achieved using one million simulations. © 2017 Elsevier Inc.","Cloud computing; Outage simulation; Queuing theory; Resource allocation model"
"Scope-aided test prioritization, selection and minimization for software reuse","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004178522&doi=10.1016%2fj.jss.2016.06.058&partnerID=40&md5=812d52e4d62c18d3e07271cb51629e21","Software reuse can improve productivity, but does not exempt developers from the need to test the reused code into the new context. For this purpose, we propose here specific approaches to white-box test prioritization, selection and minimization that take into account the reuse context when reordering or selecting test cases, by leveraging possible constraints delimiting the new input domain scope. Our scope-aided testing approach aims at detecting those faults that under such constraints would be more likely triggered in the new reuse context, and is proposed as a boost to existing approaches. Our empirical evaluation shows that in test suite prioritization we can improve the average rate of faults detected when considering faults that are in scope, while remaining competitive considering all faults; in test case selection and minimization we can considerably reduce the test suite size, with small to no extra impact on fault detection effectiveness considering both in-scope and all faults. Indeed, in minimization, we improve the in-scope fault detection effectiveness in all cases. © 2016 Elsevier Inc.","In-scope entity; Test case prioritization; Test case selection; Test of reused code; Test suite minimization; Testing scope"
"A framework for gamification in software engineering","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021411634&doi=10.1016%2fj.jss.2017.06.021&partnerID=40&md5=a87f6211f771b2f9115ec3e5acd2a4c1","Gamification seeks for improvement of the user's engagement, motivation, and performance when carrying out a certain task; it does so by incorporating game mechanics and elements, thus making that task more attractive. The application of gamification in Software Engineering can be promising; software projects can be organized as a set of challenges which can be ordered and that need to be fulfilled, for which some skills, and mainly much collective effort, are required. The objective of this paper is to propose a complete framework for the introduction of gamification in software engineering environments. This framework is composed of an ontology, a methodology guiding the process, and a support gamification engine. We carried out a case study in which the proposed framework was applied in a real company. In this project the company used the framework to gamify the areas of project management, requirements management, and testing. As a result, the methodology has clearly enabled the company to introduce gamification in its work environment, achieving a quality solution with appropriate design and development effort. The support tool allowed the company to gamify its current tools very easily. © 2017 Elsevier Inc.","Gamification; Methodology; Ontology; Software engineering"
"Predictive runtime verification of timed properties","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023597142&doi=10.1016%2fj.jss.2017.06.060&partnerID=40&md5=084bc81f3b8fe9e22160d5b6cf4af7ff","Runtime verification (RV) techniques are used to continuously check whether the (un-trustworthy) output of a black-box system satisfies or violates a desired property. When we consider runtime verification of timed properties, physical time elapsing between actions influences the satisfiability of the property. This paper introduces predictive runtime verification of timed properties where the system is not entirely a black-box but something about its behaviour is known a priori. A priori knowledge about the behaviour of the system allows the verification monitor to foresee the satisfaction (or violation) of the monitored property. In addition to providing a conclusive verdict earlier, the verification monitor also provides additional information such as the minimum (maximum) time when the property can be violated (satisfied) in the future. The feasibility of the proposed approach is demonstrated by a prototype implementation, which is able to synthesize predictive runtime verification monitors from timed automata. © 2017","Monitoring; Real-time systems; Runtime verification; Timed automata"
"Reverse engineering language product lines from existing DSL variants","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019863772&doi=10.1016%2fj.jss.2017.05.042&partnerID=40&md5=9044fed0e6dd6ea28924b4e8244dd4bd","The use of domain-specific languages (DSLs) has become a successful technique to develop complex systems. In this context, an emerging phenomenon is the existence of DSL variants, which are different versions of a DSL adapted to specific purposes but that still share commonalities. In such a case, the challenge for language designers is to reuse, as much as possible, previously defined language constructs to narrow implementation from scratch. To overcome this challenge, recent research in software languages engineering introduced the notion of language product lines. Similarly to software product lines, language product lines are often built from a set of existing DSL variants. In this article, we propose a reverse-engineering technique to ease-off such a development scenario. Our approach receives a set of DSL variants which are used to automatically recover a language modular design and to synthesize the corresponding variability models. The validation is performed in a project involving industrial partners that required three different variants of a DSL for finite state machines. This validation shows that our approach is able to correctly identify commonalities and variability. © 2017 Elsevier Inc.","Domain-specific languages; Language product lines; Reverse-engineering; Software languages engineering"
"Chord: Checkpoint-based scheduling using hybrid waiting list in shared clusters","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019716385&doi=10.1016%2fj.jss.2017.05.049&partnerID=40&md5=dce62d2fdc5cecd7a32ef90dce37a7a2","Cloud platforms supported by shared clusters are getting increasingly effective. Numerous tasks are submitted into clusters by a variety of users. Cloud platforms usually assign tasks with different priorities based on different Quality of Services (QoS) chosen by users. High-priority tasks can be executed primarily. As a consequence, preemption frequently occurs in almost all the commercial cloud platforms, such as Google and Amazon cluster. Although kill-based preemption is adopted as an optimal solution for high-priority tasks, it severely harms low-priority tasks. Especially, during the peak time, some low-priority tasks may be preempted and restarted repeatedly resulting in consuming much more precious resources including CPU cores, RAM and hard drives. Thanks to the checkpoint technology that provides an efficient solution to addressing the preemption issue. However, using checkpoint blindly will cause more resource waste. To address this issue, in this paper, we propose a concept of hybrid waiting list that holds all unfinished tasks and makes the resumption of tasks regularly. We leverage the checkpoint technology and design a novel approach based on the hybrid waiting list named Chord (Checkpoint with hybrid scheduling method) which effectively improves the performance of shared clusters. Specifically, by checking the occupancy of resources periodically and making checkpoints for certain tasks, our approach can effectively reduce unnecessary checkpoints and improve the performance of the whole cluster, especially for low-priority tasks. Extensive simulation experiments injecting tasks from the Google cloud trace logs were conducted to validate the superiority of our approach. Compared with the ordinary priority scheduling methods adopt by several commercial clouds, the improvement of response time gained by our Chord can reach 18.94%. © 2017","Checkpoint; Occupancy of resources; Priority; Response time; Shared cluster; Utility"
"Adaptive Ensemble Undersampling-Boost: A novel learning framework for imbalanced data","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026228060&doi=10.1016%2fj.jss.2017.07.006&partnerID=40&md5=34b30efac69820763ce3fc4ddf262ade","As one of the most challenging and attractive problems in the pattern recognition and machine intelligence field, imbalanced classification has received a large amount of research attention for many years. In binary classification tasks, one class usually tends to be underrepresented when it consists of far fewer patterns than the other class, which results in undesirable classification results, especially for the minority class. Several techniques, including resampling, boosting and cost-sensitive methods have been proposed to alleviate this problem. Recently, some ensemble methods that focus on combining individual techniques to obtain better performance have been observed to present better classification performance on the minority class. In this paper, we propose a novel ensemble framework called Adaptive Ensemble Undersampling-Boost for imbalanced learning. Our proposal combines the Ensemble of Undersampling (EUS) technique, Real Adaboost, cost-sensitive weight modification, and adaptive boundary decision strategy to build a hybrid algorithm. The superiority of our method over other state-of-the-art ensemble methods is demonstrated by experiments on 18 real world data sets with various data distributions and different imbalance ratios. Given the experimental results and further analysis, our proposal is proven to be a promising alternative that can be applied to various imbalanced classification domains. © 2017 Elsevier Inc.","Adaptive decision boundary; Classification; Ensemble Undersampling; Imbalanced data sets; Real Adaboost; Voting algorithm"
"Cross-validation based K nearest neighbor imputation for software quality datasets: An empirical study","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024833152&doi=10.1016%2fj.jss.2017.07.012&partnerID=40&md5=69d3f010b51fccd7e2b9e5719f182806","Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation. © 2017 Elsevier Inc.","Cross-validation; Empirical software engineering estimation; Imputation; KNN; Missing data"
"On the value of a prioritization scheme for resolving Self-admitted technical debt","2018","Journal of Systems and Software","10.1016/j.jss.2017.09.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031008685&doi=10.1016%2fj.jss.2017.09.026&partnerID=40&md5=38bde174d903e196f4c42505e21c36f7","Programmers tend to leave incomplete, temporary workarounds and buggy codes that require rework in software development and such pitfall is referred to as Self-admitted Technical Debt (SATD). Previous studies have shown that SATD negatively affects software project and incurs high maintenance overheads. In this study, we introduce a prioritization scheme comprising mainly of identification, examination and rework effort estimation of prioritized tasks in order to make a final decision prior to software release. Using the proposed prioritization scheme, we perform an exploratory analysis on four open source projects to investigate how SATD can be minimized. Four prominent causes of SATD are identified, namely code smells (23.2%), complicated and complex tasks (22.0%), inadequate code testing (21.2%) and unexpected code performance (17.4%). Results show that, among all the types of SATD, design debts on average are highly prone to software bugs across the four projects analysed. Our findings show that a rework effort of approximately 10 to 25 commented LOC per SATD source file is needed to address the highly prioritized SATD (vital few) tasks. The proposed prioritization scheme is a novel technique that will aid in decision making prior to software release in an attempt to minimize high maintenance overheads. © 2017 Elsevier Inc.","Open source projects; Prioritization scheme; Self-admitted technical debt; Source code comment; Textual indicators"
"Reverse engineering reusable software components from object-oriented APIs","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002957064&doi=10.1016%2fj.jss.2016.06.101&partnerID=40&md5=c1d6513afff820e171aa3013bb284ad6","Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs. © 2016 Elsevier Inc.","API; Frequent usage pattern; Object-oriented; Reverse engineering; Software component; Software reuse"
"Evolution of the R software ecosystem: Metrics, relationships, and their impact on qualities","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022032339&doi=10.1016%2fj.jss.2017.06.095&partnerID=40&md5=1efc0130aeacd463a1ce50a3d784a416","Software ecosystems are an important new concept for collaborative software development, and empirical studies on their development are important towards understanding the underlying dynamics and modelling their behaviour. We conducted an explorative analysis of the R ecosystem as an exemplar on high-level, ecosystem-wide assessment. Based principally on the documentation metadata of the R packages, we generated a variety of metrics that allow the quantification of the R ecosystem. We also categorized the ecosystem participants, both in the software marketplace and in the developer community, by characteristics that measure their activity and impact. By viewing our metrics across the ecosystem's lifecycle for the various participant categories, we discovered interrelationships between them and determined the contribution of each category to the ecosystem as a whole. © 2017 Elsevier Inc.","Empirical study; Evolution; Quantitative analysis; R; Software ecosystems"
"Be more familiar with our enemies and pave the way forward: A review of the roles bugs played in software failures","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028333037&doi=10.1016%2fj.jss.2017.06.069&partnerID=40&md5=4f7d2c283702b50f96c6284fdacd9ed3","There has been an increasing frequency of failures due to defective software that cost millions of dollars. Recent high profile incidents have drawn increased attention to the risks of failed software systems to the public. Yet aside from the Therac-25 case, very few incidents of software failure causing humans harm have been proven and widely reported. With increased government oversight and the expanded use of social networking for real time reporting of problems, we are only beginning to understand the potential for major injury or death related to software failures. However, debugging defective software can be costly and time consuming. Moreover, undetected bugs could induce great harm to the public when software systems are applied in safety-critical areas, such as consumer products, public infrastructure, transportation systems, etc. Therefore, it is vital that we remove these bugs as early as possible. To gain more understanding of the nature of these bugs, we review the reported software failures that have impacted the health, safety, and welfare of the public. A focus on lessons learned and implications for future software systems is also provided which acts as guidelines for engineers to improve the quality of their products and avoid similar failures from happening. © 2017 Elsevier Inc.","Accidents; Bugged software systems; Lessons learned; Mishaps; Software failures"
"Resource management in cloud platform as a service systems: Analysis and opportunities","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022098813&doi=10.1016%2fj.jss.2017.05.035&partnerID=40&md5=554d607b10b5325ef923b15f62907d7e","Platform-as-a-Service (PaaS) clouds offer services to automate the deployment and management of applications, relieving application owners of the complexity of managing the underlying infrastructure resources. However, application owners have an increasingly larger diversity and volume of workloads, which they want to execute at minimum cost while maintaining desired performance guarantees. In this paper we investigate how existing PaaS systems cope with this challenge. In particular, we first present a taxonomy of commonly-encountered design decisions regarding how PaaS systems manage underlying resources. We then use this taxonomy to analyze an extensive set of PaaS systems targeting different application domains. Based on this analysis, we identify several future research opportunities in the PaaS design space, which will enable PaaS owners to reduce hosting costs while coping with the workload variety. © 2017 Elsevier Inc.","Cloud computing; Platform-as-a-service; Resource management"
"A multi-level feedback approach for the class integration and test order problem","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027717116&doi=10.1016%2fj.jss.2017.08.026&partnerID=40&md5=d5b1ed2d5431e925ce7552933002106c","Class integration and test order (CITO) problem is to devise an optimal inter-class order which can minimize stubbing efforts. The existing approach for this problem, whether it is graph-based or search-based, usually wastes a significant amount of time and efforts in finding test orders, and sometimes may devise sub-optimal solutions. To overcome this limitation, we introduce a multi-level feedback approach to better solve the CITO problem. In this method, we use a multi-level feedback strategy to calculate test profit for each class, and according to test profit, propose a reward and punishment mechanism to assess the performance of class and set its test priority. Instead of breaking cycles or searching for optimum in the previous methods, our method integrates classes by their test priority, therefore significantly reduces the running time. The experiments are conducted on five benchmark programs and eight industrial programs, and the obtained results are compared with graph-based and search-based approaches. The results indicate that our approach can minimize the stubbing cost efficiently for most programs of all typical approaches compared in this work. © 2017","Feedback; Stub minimization; Test cost; Test order"
"QuickFuzz testing for fun and profit","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030668983&doi=10.1016%2fj.jss.2017.09.018&partnerID=40&md5=0b74ae07093694f091d4c878864c093d","Fuzzing is a popular technique to find flaws in programs using invalid or erroneous inputs but not without its drawbacks. At one hand, mutational fuzzers require a set of valid inputs as a starting point, in which modifications are then introduced. On the other hand, generational fuzzing allows to synthesize somehow valid inputs according to a specification. Unfortunately, this requires to have a deep knowledge of the file formats under test to write specifications of them to guide the test case generation process. In this paper we introduce an extended and improved version of QuickFuzz, a tool written in Haskell designed for testing unexpected inputs of common file formats on third-party software, taking advantage of off-the-self well known fuzzers. Unlike other generational fuzzers, QuickFuzz does not require to write specifications for the file formats in question since it relies on existing file-format-handling libraries available on the Haskell code repository. It supports almost 40 different complex file-types including images, documents, source code and digital certificates. In particular, we found QuickFuzz useful enough to discover many previously unknown vulnerabilities on real-world implementations of web browsers and image processing libraries among others. © 2017 Elsevier Inc.","Fuzzing; Haskell; QuickCheck; Testing"
"Predicting change consistency in a clone group","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028911281&doi=10.1016%2fj.jss.2017.08.045&partnerID=40&md5=09bbc7cc5adcb8e18a46717422147707","Code cloning has been accepted as one of the general code reuse methods in software development, thanks to the increasing demand in rapid software production. The introduction of clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group; they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make consistent change to other clones in the group. Failure in making such consistent change to a clone group when necessary is commonly known as “clone consistency-defect”, which can adversely impact software reusability. In this work, we propose an approach to predict the need for making consistent change in clones within a clone group at the time when changes have been made to one of its clones. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three attribute sets from clone groups as input for predicting the need for consistent clone change. These three attribute sets are code attributes, context attributes and evolution attributes respectively. Together, they provide a holistic view about clone changes. We conduct experiments on four open source projects. Our experiments show that our approach has reasonable precision and recall in predicting whether a clone group requires (or is free of) consistent change. This holistic approach can aid developers in maintaining clone changes, and avoid potential consistency-defect, which can improve software quality and reusability. © 2017 Elsevier Inc.","Bayesian network; Clone attributes; Clone maintenance; Code clones; Consistency-requirement prediction; Software reuse"
"Time series forecasting for dynamic quality of web services: An empirical study","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030467719&doi=10.1016%2fj.jss.2017.09.011&partnerID=40&md5=a0e25350e524629038d80f087793e1d2","Web Services (WSs) constitute a critical component of modern software development. Knowing the dynamic qualities of WSs is mandatory during use, and these qualities vary continuously over time. However, in most cases, the quality information furnished by service providers is static and does not consider dynamic variations in quality over time. Thus, it is necessary to determine a method for acquiring accurate quality values for WSs. The motivation for this research is that the most suitable time-series method for dynamic quality prediction of WSs remains unknown because no comprehensive empirical comparison of the representative time-series methods has been performed. Therefore, in this paper, we implement all the representative time-series methods and compare their dynamic quality predictions for WSs using a real-world quality dataset. For empirical comparison, we have ensured that our study is reproducible and referenceable by providing diverse specifics and evaluating their validity in detail. The experimental results and diverse discussions presented in this paper may act as a valuable reference to both academic researchers and WS consumers and providers in industry because they can depend on the results when selecting the most suitable time-series method for direct use or as a starting point for further modifications. Based on our experimental results, among the included time-series forecasting approaches, genetic programming (GP) can achieve the highest quality of service (QoS) forecasting accuracy (in our experiments, the average mean absolute error and mean absolute percentage error are 1258 and 20%, respectively); however, this approach also requires the longest time to produce a QoS predictor (67.7 s on average). Though auto-regressive integrated moving average (ARIMA, with average error measures of 1343 and 25.4%) and exponential smoothing (ES, with average error measures of 1354 and 25.7%) present slightly worse accuracy than GP, ARIMA and ES require much less time to generate a predictor than GP (on average, 0.1612 and 0.1519 s, respectively); thus, these approaches might represent a compromise between forecasting accuracy and cost. © 2017 Elsevier Inc.","QoS prediction; Time-series forecasting; Web service"
"User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009477583&doi=10.1016%2fj.jss.2017.01.002&partnerID=40&md5=a3b5961af49a1917d3f85e30afc0d8c4","User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customer's needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session. © 2017 Elsevier Inc.","Agile development; Test automation; User acceptance testing"
"Exploring the usefulness of unlabelled test cases in software fault localization","2018","Journal of Systems and Software","10.1016/j.jss.2017.07.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026832982&doi=10.1016%2fj.jss.2017.07.027&partnerID=40&md5=25ea7d366697f8cc3ce99f7af9e6af6e","In automatic software fault localization techniques, both the coverage and the testing outcomes of the provided test suite are considered to be essential information. The problem occurs when test oracles do not exist. Specifically, the test suite will contain a large number of unlabelled test cases, i.e., test cases whose output is not identified as being either correct (passing) or incorrect (failing). Such unlabelled test cases cannot be directly used, thereby leading to a degradation of localization effectiveness. In this paper, we propose an approach based on test classification to enable the use of unlabelled test cases in localizing faults. In our approach, unlabelled test cases are classified based on their execution information and are then assigned corresponding estimated labels to allow them to be utilized in fault localization. Experimental results show that with the utilization of these newly labelled test cases, the effectiveness of fault localization can indeed be improved. © 2017 Elsevier Inc.","Oracle problem; Software fault localization; Software testing; Test classification; Unlabelled test cases"
"A novel analysis approach for the design and the development of context-aware applications","2017","Journal of Systems and Software","10.1016/j.jss.2017.07.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026450983&doi=10.1016%2fj.jss.2017.07.013&partnerID=40&md5=775436f7ed04bf0bf7f859d49f73d11e","In this paper, we propose a novel analysis approach, called ANALOG, for the design and the development of context-aware applications able to detect context change and to predict context parameter values. Our approach is described by two analysis procedures, (a) an analysis procedure for detection and (b) an analysis procedure for prediction. The proposed analysis procedures aim to offer a support for application designers allowing them to design easily context-aware applications. Each procedure is achieved by a sequence of steps performed by the designers. We describe first our analysis approach presented by the analysis procedures. Then, we give some implementation details of our approach. Afterwards, we show the usefulness of the analysis approach through presenting two execution scenarios related to a smart building case study. Finally, to illustrate the effectiveness of our approach, we present different experiments related to (i) the processing time of the analysis approach and (ii) the CPU and the memory overhead introduced by our approach. © 2017 Elsevier Inc.","Adaptive thresholds; Analysis procedure for detection; Analysis procedure for prediction; Context-aware applications; Extreme Value Theory; Prediction models"
"Improving feature location in long-living model-based product families designed with sustainability goals","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030453862&doi=10.1016%2fj.jss.2017.09.022&partnerID=40&md5=de279429c0a066dc297b4c09175aeb4d","The benefits of Software Product Lines (SPL) are very appealing: software development becomes better, faster, and cheaper. Unfortunately, these benefits come at the expense of a migration from a family of products to a SPL. Feature Location could be useful in achieving the transition to SPLs. This work presents our FeLLaCaM approach for Feature Location. Our approach calculates similarity to a description of the feature to locate, occurrences where the candidate features remain unchanged, and changes performed to the candidate features throughout the retrospective of the product family. We evaluated our approach in two long-living industrial domains: a model-based family of firmwares for induction hobs that was developed over more than 15 years, and a model-based family of PLC software to control trains that was developed over more than 25 years. In our evaluation, we compare our FeLLaCaM approach with two other approaches for Feature Location: (1) FLL (Feature Location through Latent Semantic Analysis) and (2) FLC (Feature Location through Comparisons). We measure the performance of FeLLaCaM, FLL, and FLC in terms of recall, precision, Matthews Correlation Coefficient, and Area Under the Receiver Operating Characteristics curve. The results show that FeLLaCaM outperforms FLL and FLC. © 2017 Elsevier Inc.","Architecture sustainability; Feature location; Long-Living software systems; Model–Driven engineering; Software product lines"
"Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019744077&doi=10.1016%2fj.jss.2017.05.051&partnerID=40&md5=aa790cc2b608ffa1480c29de64dc5bca","Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies’ solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry. © 2017","Automated production systems; Control software; Factory automation; Maturity; Modularity; Programmable logic controller"
"Mining domain knowledge from app descriptions","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027980934&doi=10.1016%2fj.jss.2017.08.024&partnerID=40&md5=cb27f59a3ef6165d6151f2e30a74c244","Domain analysis aims at gaining knowledge to a particular domain in the early stage of software development. A key challenge in domain analysis is to extract features automatically from related product artifacts. Compared with other kinds of artifacts, high volume of descriptions can be collected from App marketplaces (such as Google Play and Apple Store) easily when developing a new mobile application (App), so it is essential for the success of domain analysis to gain features and relationships from them using data analysis techniques. In this paper, we propose an approach to mine domain knowledge from App descriptions automatically, where the information of features in a single App description is firstly extracted and formally described by a Concern-based Description Model (CDM), which is based on predefined rules of feature extraction and a modified topic modeling method; then the overall knowledge in the domain is identified by classifying, clustering and merging the knowledge in the set of CDMs and topics, and the results are formalized by a Data-based Raw Domain Model (DRDM). Furthermore, we propose a quantified evaluation method for prioritizing the knowledge in DRDM. The proposed approach is validated by a series of experiments. © 2017","App descriptions; Data analysis; Domain analysis; Feature extraction"
"Temporal algebraic query of test sequences","2018","Journal of Systems and Software","10.1016/j.jss.2017.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026472507&doi=10.1016%2fj.jss.2017.07.014&partnerID=40&md5=42ec79b19dc9491b212ab64e97eee807","Nowadays tools can generate test suites consisting of large number of test sequences. The used algorithms are typically random-based. Although more advanced variations may incorporate an advanced search algorithm to cover difficult scenarios, many decisions still have to be made randomly simply because no information is available to calculate the best decision. Because of this, many of the generated sequences may be redundant, while some others may be rare and hard to get. This paper presents a rich formalism that is based on a mix of algebraic relations and Linear Temporal Logic (LTL) to query test suites, and an efficient algorithm to execute such queries. Queries can be used as correctness specifications (oracles) to validate a test suite. They are however more general as they can be used to filter out test sequences with interesting properties, e.g. to archive them for future use. The proposed formalism is quite expressive: it can express algebraic equations with logical variables, Hoare triples, class invariants, as well as their temporal modalities. An evaluation of the query algorithm's performance is included in this paper. The whole query framework has been implemented in a testing tool for Java called T3i. © 2017 Elsevier Inc.","Algebraic test oracles; Dynamic analysis; LTL Test oracles; Property based testing; Query based testing"
"MeshFS: A distributed file system for cloud-based wireless mesh network","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020945450&doi=10.1016%2fj.jss.2017.06.020&partnerID=40&md5=bd6e5e8146c83585fc5969d3541aa9eb","Wireless mesh networks have attracted considerable interest in recent years in both the academic and industrial communities. As wireless mesh routers can be interconnected through wireless links, wireless mesh networks provide greater flexibility and better cost-effectiveness. In particular, due to their ease of installation and maintenance, they can be used in different environments, especially where cable installation is difficult. Apart from providing routing service, wireless mesh networks can also be employed to provide other value-added services. Inspired by cloud computing and other distributed file systems, this paper presents the design and implementation of MeshFS, a distributed file system specifically for wireless mesh networks. A key technical challenge is to develop a lightweight software system that can be implemented over memory-limited wireless mesh network environments. With the aim of providing a lightweight distributed file system, allowing limited resources to be utilized more effectively, MeshFS integrates scattered storage resources from wireless mesh routers to provide a mountable file system interface to Unix/Linux file system with fault-tolerant capabilities and cloud computing-like storage functions. © 2017 Elsevier Inc.","Cloud computing; Data management; Distributed file system; Wireless mesh network"
"A method to localize faults in concurrent C programs","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015765249&doi=10.1016%2fj.jss.2017.03.010&partnerID=40&md5=9fd371ba2ca66285baa4c274af5f039e","We describe a new approach to localize faults in concurrent programs, which is based on bounded model checking and sequentialization techniques. The main novelty is the idea of reproducing a faulty behavior, in a sequential version of a concurrent program. In order to pinpoint faulty lines, we analyze counterexamples generated by a model checker, to the new instrumented sequential program, and search for a diagnostic value, which corresponds to actual lines in a program. This approach is useful to improve debugging processes for concurrent programs, since it tells which line should be corrected and what values lead to a successful execution. We implemented this approach as a code-to-code transformation from concurrent into non-deterministic sequential programs, which are used as inputs to existing verification tools. Experimental results show that our approach is effective and capable of identifying faults in our benchmark set, which was extracted from the SV-COMP 2016 suite. © 2017 Elsevier Inc.","Bounded model checking; Concurrent software; Fault localization; Non-determinism; Sequentialization"
"Recovering software product line architecture of a family of object-oriented product variants","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84998705870&doi=10.1016%2fj.jss.2016.07.039&partnerID=40&md5=b5bbac721f5c3e5c5868511e33ae480d","Software Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81%, 91%, 67% and 100%, respectively. © 2016 Elsevier Inc.","Formal concept analysis; Object-oriented product variants; Software architecture recovery; Software component; Software product line; Software reuse"
"Multi-cloud service composition using Formal Concept Analysis","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029168443&doi=10.1016%2fj.jss.2017.08.016&partnerID=40&md5=1296f210ec8fcad7c4a43dd2be65dc84","Recent years have witnessed a rapid growth in exploiting Cloud environments to deliver various types of resources as services. To improve the efficiency of software development, service reuse and composition is viewed as a powerful means. However, effectively composing services from multiple clouds has not been solved yet. Indeed, existing solutions assume that the services participating to a composition come from a single cloud. This approach is unrealistic since the other existing clouds may host more suitable services. In order to deliver high quality service compositions, the user request must be checked against the services in the multi-cloud environment (MCE) or at least clouds in the availability zone of the user. In this paper, we propose a multi-cloud service composition (MCSC) approach based on Formal Concept Analysis (FCA). We use FCA to represent and combine information of multiple clouds. FCA is based on the concept lattice which is a powerful mean to classify clouds and services information. We first model the multi-cloud environment as a set of formal contexts. Then, we extract and combine candidate clouds from formal concepts. Finally, the optimal cloud combination is selected and the MCSC is transformed into a classical service composition problem. Conducted experiments proved the effectiveness and the ability of FCA based method to regroup and find cloud combinations with a minimal number of clouds and a low communication cost. Also, the comparison with two well-known combinatorial optimization approaches showed that the adopted top-down strategy allowed to rapidly select services hosted on the same and closest clouds, which directly reduced the inter-cloud communication cost, compared to existing approaches. © 2017","Cloud computing; Formal concept analysis; Lattice theory; Multi-cloud; Service composition"
"Predicting bug-fixing time: A replication study using an open source software project","2018","Journal of Systems and Software","10.1016/j.jss.2017.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014053590&doi=10.1016%2fj.jss.2017.02.021&partnerID=40&md5=247b29b699e29dbd357cf9dc95e3354c","Background: On projects with tight schedules and limited budgets, it may not be possible to resolve all known bugs before the next release. Estimates of the time required to fix known bugs (the “bug fixing time”) would assist managers in allocating bug fixing resources when faced with a high volume of bug reports. Aim: In this work, we aim to replicate a model for predicting bug fixing time with open source data from Bugzilla Firefox. Method: To perform the replication study, we follow the replication guidelines put forth by Carver [J. C. Carver, Towards reporting guidelines for experimental replications: a proposal, in: 1st International Workshop on Replication in Empirical Software Engineering, 2010.]. Similar to the original study, we apply a Markov-based model to predict the number of bugs that can be fixed monthly. In addition, we employ Monte-Carlo simulation to predict the total fixing time for a given number of bugs. We then use the k-nearest neighbors algorithm to classify fixing times into slow and fast. Result: The results of the replicated study on Firefox are consistent with those of the original study. The results show that there are similarities in the bug handling behaviour of both systems. Conclusion: We conclude that the model that estimates the bug fixing time is robust enough to be generalized, and we can rely on this model for our future research. © 2017 Elsevier Inc.","Bug fixing time; Deferred bugs; Effort estimation; Replication study; Software maintainability"
"A semi-automatic maintenance and co-evolution of OCL constraints with (meta)model evolution","2017","Journal of Systems and Software","10.1016/j.jss.2017.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030093980&doi=10.1016%2fj.jss.2017.09.010&partnerID=40&md5=3d3db4bf0442523506ac6ed9256dee08","Metamodels are core components of modeling languages to define structural aspects of a business domain. As a complement, OCL constraints are used to specify detailed aspects of the business domain, e.g. more than 750 constraints come with the UML metamodel. As the metamodel evolves, its OCL constraints may need to be co-evolved too. Our systematic analysis shows that semantically different resolutions can be applied depending not only on the metamodel changes, but also on the user intent and on the structure of the impacted constraints. In this paper, we first investigate the syntactical reasons that lead to apply different resolutions. We then propose a co-evolution approach that offers alternative resolutions while allowing the user to choose the best applicable one. We evaluated our approach on six case studies of metamodel evolution and their OCL constraints co-evolution. The results show the usefulness of alternative resolutions along with user decision to cope with real co-evolution scenarios. Within our six case studies our approach led to an average of 92% (syntactically) and 93% (semantically) matching co-evolution w.r.t. the user intent. © 2017 Elsevier Inc.","Co-evolution; Constraints; Evolution; Metamodel; OCL"
"A framework for dynamic selection of backoff stages during initial ranging process in wireless networks","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026858829&doi=10.1016%2fj.jss.2017.08.013&partnerID=40&md5=5b2d4482f60fcf82308f4c07002bdb74","The only available solution in the IEEE 802.22 standard for avoiding collision amongst various contending customer premises equipment (CPEs) attempting to associate with a base station (BS) is binary exponential random backoff process in which the contending CPEs retransmit their association requests. The number of attempts the CPEs send their requests to the BS are fixed in an IEEE 802.22 network. This paper presents a mathematical framework that helps the BS in determining at which attempt the majority of the CPEs become part of the wireless regional area network from a particular number of contending CPEs. Based on a particular attempt, the ranging request collision probability for any number of contending CPEs with respect to contention window size is approximated. The numerical results validate the effectiveness of the approximation. Moreover, the average ranging success delay experienced by the majority of the CPEs is also determined. © 2017 Elsevier Inc.","average ranging success delay; Contention window; Customer premises equipment; ranging request collision probability"
"Stochastic modeling for performance and availability evaluation of hybrid storage systems","2017","Journal of Systems and Software","10.1016/j.jss.2017.08.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028526955&doi=10.1016%2fj.jss.2017.08.043&partnerID=40&md5=ef31e4cbd8f92eaef091e2a2f9fbaadd","Improvements in computational systems may be constrained by the efficiency of storage drives. Therefore, replacing hard disk drives (HDD) with solid-state drives (SSD) may also be an effective way to improve system performance, but the higher cost per gigabyte and reduced lifetime of SSDs hinder a thorough replacement. To mitigate these issues, several architectures have been conceived based on hybrid storage systems, but performance and availability models have not been proposed to better assess such different architectures. This paper presents an approach based on stochastic models (i.e., stochastic Petri nets and reliability block diagrams) for performance and availability evaluation of hybrid storage systems. The proposed models can represent write and read operations, and they may estimate response time, throughput and availability. A case study based on OpenStack Swift platform is adopted to demonstrate the feasibility of the proposed approach. © 2017 Elsevier Inc.","Availability; Cloud computing; Hybrid storage; Performance evaluation; Reliability block diagrams; Stochastic petri nets"
"A case study of black box fail-safe testing in web applications","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020416606&doi=10.1016%2fj.jss.2016.09.031&partnerID=40&md5=2f245140a1aea24a32abaea6ab103185","External failures like network changes can affect system operation negatively. Mitigation requirements try to prevent or reduce their effects. This paper presents a black box testing approach that tests fail-safe behavior in web applications. Failure mitigation tests are built based on a functional test suite. Mitigation requirements are used to build mitigation models and mitigation tests paths through them. A genetic algorithm is used to generate failure scenarios (failure mitigation test requirements). Weaving rules describe how mitigation test paths are combined with behavioral test paths to create failure mitigation test paths. These are then transformed into an executable test suite. The paper also evaluates the genetic algorithm used to generate test requirements with respect to efficiency and effectiveness. A large case study, a commercial mortgage lending system, is used to explore applicability, scalability, and effectiveness of the approach. © 2016","Failure mitigation patterns; Genetic algorithm; Web testing"
"No silver brick: Opportunities and limitations of teaching Scrum with Lego workshops","2017","Journal of Systems and Software","10.1016/j.jss.2017.06.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021138321&doi=10.1016%2fj.jss.2017.06.019&partnerID=40&md5=5cee67e85a8b4e3c9f9b9c31e684f6ed","Education in Software Engineering has to both teach technical content such as databases and programming but also organisational skills such as team work and project management. While the former can be evaluated from a product perspective, the latter are usually embedded in a Software Engineering process and need to be assessed and adapted throughout their implementation. The in-action property of processes puts a strain on teachers since we cannot be present throughout the students’ work. To address this challenge we have adopted workshops to teach Scrum by building a Lego city in short sprints to focus on the methodological content. In this way we can be present throughout the process and coach the students. We have applied the exercise in six different courses, across five different educational programmes and observed more than 450 participating students. In this paper, we report on our experiences with this approach, based on quantitative data from the students and qualitative data from both students and teachers. We give recommendations for learning opportunities and best practices and discuss the limitations of these workshops in a classroom setting. We also report on how the students transferred their methodological knowledge to software development projects in an academic setting. © 2017","Agile software engineering; Scrum; Software engineering education"
"Empirical study on refactoring large-scale industrial systems and its effects on maintainability","2017","Journal of Systems and Software","10.1016/j.jss.2016.08.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019644895&doi=10.1016%2fj.jss.2016.08.071&partnerID=40&md5=248e1352b09a07ff445f507ebd5f337b","Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time occasionally on improving our source code, its maintainability will inevitably decrease. The literature tells us that we can improve the maintainability of a software system by regularly refactoring it. But does refactoring really increase software maintainability? Can it happen that refactoring decreases the maintainability? Empirical studies show contradicting answers to these questions and there have been only a few studies which were performed in a large-scale, industrial context. In our paper, we assess these questions in an in vivo context, where we analyzed the source code and measured the maintainability of 6 large-scale, proprietary software systems in their manual refactoring phase. We analyzed 2.5 million lines of code and studied the effects on maintainability of 315 refactoring commits which fixed 1273 coding issues. We found that single refactorings only make a very little difference (sometimes even decrease maintainability), but a whole refactoring period, in general, can significantly increase maintainability, which can result not only in the local, but also in the global improvement of the code. © 2016 Elsevier Inc.","Antipatterns; Coding issues; ISO/IEC 25010; Maintainability; Refactoring; Software quality"
"Design pattern-based model transformation supported by QVT","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007418329&doi=10.1016%2fj.jss.2016.12.019&partnerID=40&md5=94d9064082665e2bdffd8141277e4566","A design pattern helps to improve the quality of a software system by providing a proven solution for recurring design problems. However, the abstract and informal nature of prevailing pattern descriptions makes it difficult to use design patterns. There have been significant works on formalizing design patterns which found a base for systematic application of a design pattern. Pattern-based model transformation has emerged as an approach for incorporating pattern properties into a design model. However, the existing work mostly focuses on the solution domain of a pattern while leaving out the problem domain, structural pattern aspects with little attention to behavioral aspects, and general methodologies without concrete implementations. In this work, we present an approach for transforming an application model using both the structural and behavioral properties of a design pattern defined in terms of the problem and solution domain and its implementation using Query/View/Transformation (QVT). In the approach, we define pattern consistency for structural and behavioral pattern properties and pattern conformance for pattern applicability before transformation solution conformance after transformation. We demonstrate the approach using the Observer pattern applied to a graph application. Besides the Observer pattern, we also define transformation rules for the Visitor and Adapter patterns. © 2016 Elsevier Inc.","Design pattern; Model transformation; QVT; UML"
"Recursive prediction algorithm for non-stationary Gaussian Process","2017","Journal of Systems and Software","10.1016/j.jss.2016.08.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994160270&doi=10.1016%2fj.jss.2016.08.036&partnerID=40&md5=0bd092e870bf09001fdae5e13af2fe5a","Gaussian Process is a theoretically rigorous model for prediction problems. One of the deficiencies of this model is that its original exact inference algorithm is computationally intractable. Therefore, its applications are limited in the field of real-time online predictions. In this paper, a recursive prediction algorithm based on the Gaussian Process model is proposed. In recursive algorithms, the computational time of the next step can be greatly reduced by utilizing the intermediate results of the current step. The proposed recursive algorithm accelerates the prediction and avoids the loss of accuracy at the same time. Experiments are done on an ultra-short term electric load data set and the results are demonstrated to show the accuracy and efficiency of the new algorithm. © 2016 Elsevier Inc.",""
"Simplification of UML/OCL schemas for efficient reasoning","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017006837&doi=10.1016%2fj.jss.2017.03.015&partnerID=40&md5=cd97c2cf130ebb12ddcecce2e09dc21d","Ensuring the correctness of a conceptual schema is an essential task in order to avoid the propagation of errors during software development. The kind of reasoning required to perform such task is known to be exponential for UML class diagrams alone and even harder when considering OCL constraints. Motivated by this issue, we propose an innovative method aimed at removing constraints and other UML elements of the schema to obtain a simplified one that preserve the same reasoning outcomes. In this way, we can reason about the correctness of the initial artifact by reasoning on a simplified version of it. Thus, the efficiency of the reasoning process is significantly improved. In addition, since our method is independent from the reasoning engine used, any reasoning method may benefit from it. © 2017 Elsevier Inc.","OCL; Reasoning; Simplification; UML"
"Topic-based software defect explanation","2017","Journal of Systems and Software","10.1016/j.jss.2016.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969506710&doi=10.1016%2fj.jss.2016.05.015&partnerID=40&md5=d82ccab35afcb478b56781f7b9ccd52f","Researchers continue to propose metrics using measurable aspects of software systems to understand software quality. However, these metrics largely ignore the functionality, i.e., the conceptual concerns, of software systems. Such concerns are the technical concepts that reflect the system's business logic. For instance, while lines of code may be a good general measure for defects, a large file responsible for simple I/O tasks is likely to have fewer defects than a small file responsible for complicated compiler implementation details. In this paper, we study the effect of concerns on software quality. We use a statistical topic modeling approach to approximate software concerns as topics (related words in source code). We propose various metrics using these topics to help explain the file defect-proneness. Case studies on multiple versions of Firefox, Eclipse, Mylyn, and NetBeans show that (i) some topics are more defect-prone than others; (ii) defect-prone topics tend to remain so over time; (iii) our topic-based metrics provide additional explanatory power for software quality over existing structural and historical metrics; and (iv) our topic-based cohesion metric outperforms state-of-the-art topic-based cohesion and coupling metrics in terms of defect explanatory power, while being simpler to implement and more intuitive to interpret. © 2016 Elsevier Inc.","Code quality; Cohesion; Coupling; LDA; Metrics; Topic modeling"
"On the use of developers’ context for automatic refactoring of software anti-patterns","2017","Journal of Systems and Software","10.1016/j.jss.2016.05.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028248184&doi=10.1016%2fj.jss.2016.05.042&partnerID=40&md5=c7768526a3d9ed2be2f9d5eae855e209","Anti-patterns are poor solutions to design problems that make software systems hard to understand and extend. Entities involved in anti-patterns are reported to be consistently related to high change and fault rates. Refactorings, which are behavior preserving changes are often performed to remove anti-patterns from software systems. Developers are advised to interleave refactoring activities with their regular coding tasks to remove anti-patterns, and consequently improve software design quality. However, because the number of anti-patterns in a software system can be very large, and their interactions can require a solution in a set of conflicting objectives, the process of manual refactoring can be overwhelming. To automate this process, previous works have modeled anti-patterns refactoring as a batch process where a program provides a solution for the total number of classes in a system, and the developer has to examine a long list of refactorings, which is not feasible in most situations. Moreover, these proposed solutions often require that developers modify classes on which they never worked before (i.e., classes on which they have little or no knowledge). To improve on these limitations, this paper proposes an automated refactoring approach, ReCon (Refactoring approach based on task Context), that leverages information about a developer's task (i.e., the list of code entities relevant to the developer's task) and metaheuristics techniques to compute the best sequence of refactorings that affects only entities in the developer's context. We mine 1705 task contexts (collected using the Eclipse plug-in Mylyn) and 1013 code snapshots from three open-source software projects (Mylyn, PDE, Eclipse Platform) to assess the performance of our proposed approach. Results show that ReCon can remove more than 50% of anti-patterns in a software system, using fewer resources than the traditional approaches from the literature. © 2016 Elsevier Inc.","Anti-patterns; Automatic refactoring; Interaction traces; Metaheuristics; Software maintenance; Task context"
"Repeating patterns as symbols for long time series representation","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006357039&doi=10.1016%2fj.jss.2016.06.008&partnerID=40&md5=3a8368b9e4d034f4ada476514a8c367f","Over the past years, many representations for time series were proposed with the main purpose of dimensionality reduction and as a support for various algorithms in the domain of time series data processing. However, most of the transformation algorithms are not directly applicable on streams of data but only on static collections of the data as they are iterative in their nature. In this work we propose a symbolic representation of time series along with a method for transformation of time series data into the proposed representation. As one of the basic requirements for applicable representation is the distance measure which would accurately reflect the true shape of the data, we propose a distance measure operating on the proposed representation and lower bounding the Euclidean distance on the original data. We evaluate properties of the proposed representation and the distance measure on the UCR collection of datasets. As we focus on stream data processing, we evaluate the properties and limitations of the proposed representation on very long time series from the domain of electricity consumption monitoring, simulating the processing of potentially unbound data stream. © 2016 Elsevier Inc.","Lower bound; Stream processing; Symbolic representation; Time series representation"
"A survey on feature drift adaptation: Definition, benchmark, challenges and future directions","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002827336&doi=10.1016%2fj.jss.2016.07.005&partnerID=40&md5=786b9c5a2a58efd5bbbb85c3a2317ad5","Data stream mining is a fast growing research topic due to the ubiquity of data in several real-world problems. Given their ephemeral nature, data stream sources are expected to undergo changes in data distribution, a phenomenon called concept drift. This paper focuses on one specific type of drift that has not yet been thoroughly studied, namely feature drift. Feature drift occurs whenever a subset of features becomes, or ceases to be, relevant to the learning task; thus, learners must detect and adapt to these changes accordingly. We survey existing work on feature drift adaptation with both explicit and implicit approaches. Additionally, we benchmark several algorithms and a naive feature drift detection approach using synthetic and real-world datasets. The results from our experiments indicate the need for future research in this area as even naive approaches produced gains in accuracy while reducing resources usage. Finally, we state current research topics, challenges and future directions for feature drift adaptation. © 2016 Elsevier Inc.","Data stream mining; Feature drift; Feature selection"
"Using contexts similarity to predict relationships between tasks","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018458943&doi=10.1016%2fj.jss.2016.11.033&partnerID=40&md5=6ecdc4ef105a18c75b64447849776fa7","Developers’ tasks are often interrelated. A task might succeed, precede, block, or depend on another task. Or, two tasks might simply have a similar aim or require similar expertise. When working on tasks, developers interact with artifacts and tools, which constitute the contexts of the tasks. This work investigates the extent to which the similarity of the contexts predicts whether and how the respective tasks are related. The underlying assumption is simple: if during two tasks the same artifacts are touched or similar interactions are observed, the tasks might be interrelated. We define a task context as the set of all developer's interactions with the artifacts during the task. We then apply Jaccard index, a popular similarity measure to compare two contexts. Instead of only counting the artifacts in the intersection and union of the contexts as Jaccard does, we scale the artifacts with their relevance to the task. For this, we suggest a simple heuristic based on the Frequency, Duration, and Age of the interactions with the artifacts (FDA). Alternatively, artifact relevance can be estimated by the Degree-of-Interest (DOI) used in task-focused programming. To compare the accuracy of the context similarity models for predicting task relationships, we conducted a field study with professionals, analyzed data from the open source task repository Bugzilla, and ran an experiment with students. We studied two types of relationships useful for work coordination (dependsOn and blocks) and two types useful for personal work management (isNextTo and isSimilarTo). We found that context similarity models clearly outperform a random prediction for all studied task relationships. We also found evidence that, the more interrelated the tasks are, the more accurate the context similarity predictions are. Our results show that context similarity is roughly as accurate to predict task relationships as comparing the textual content of the task descriptions. Context and content similarity models might thus be complementary in practice, depending on the availability of text descriptions or context data. We discuss several use cases for this research, e.g. to assist developers choose the next task or to recommend other tasks they should be aware of. © 2017 The Authors","Developer productivity; Empirical study; Recommender systems; Task context; Task dependencies; Task management"
"How effectively can spreadsheet anomalies be detected: An empirical study","2017","Journal of Systems and Software","10.1016/j.jss.2016.03.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963553383&doi=10.1016%2fj.jss.2016.03.061&partnerID=40&md5=2588c059de15b0bc36d5ad20d8c92bee","While spreadsheets are widely used, they have been found to be error-prone. Various techniques have been proposed to detect anomalies in spreadsheets, with varying scopes and effectiveness. Nevertheless, there is no empirical study comparing these techniques’ practical usefulness and effectiveness. In this work, we conducted a large-scale empirical study of three state-of-the-art techniques on their effectiveness in detecting spreadsheet anomalies. Our study focused on the precision, recall rate, efficiency and scope. We found that one technique outperforms the other two in precision and recall rate of spreadsheet anomaly detection. Efficiency of the three techniques is acceptable for most spreadsheets, but they may not be scalable to large spreadsheets with complex formulas. Besides, they have different scopes for detecting different spreadsheet anomalies, thus complementing to each other. We also discussed limitations of these three techniques. Based on our findings, we give suggestions for future spreadsheet research. © 2016","Anomaly detection; Empirical study; Spreadsheet"
"Data stream classification using random feature functions and novel method combinations","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006469957&doi=10.1016%2fj.jss.2016.06.009&partnerID=40&md5=8888cf8d6d556d791ccf934339addb57","Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyper-parameter options and initial conditions to be considered an effective ‘off-the-shelf’ data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbor, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification. © 2016 Elsevier Inc.","Big data; Classification; Data stream mining; GPUs"
"A domain-specific language for the control of self-adaptive component-based architecture","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011965743&doi=10.1016%2fj.jss.2017.01.030&partnerID=40&md5=14ac2204dc22a03e570647f7d7d87e40","Self-adaptive behaviours in the context of Component-based Architecture are generally designed based on past monitoring events, configurations (component assemblies) as well as behavioural programs defining the adaptation logics and invariant properties. Providing assurances on the navigation through the configuration space remains a challenge. That requires taking decisions on predictions on the possible futures of the system in order to avoid going into branches of the behavioural program leading to bad configurations. We propose the design of self-adaptive software components based on logical discrete control approaches, in which the self-adaptive behavioural models enrich component controllers with a knowledge not only on events, configurations and past history, but also with possible future configurations. This article provides the description, implementation and discussion of Ctrl-F, a Domain-specific Language whose objective is to provide high-level support for describing these control policies. Ctrl-Fis formally defined by translation to Finite State Automata models, which allow for the exploration of behavioural programs by verification or Discrete Controller Synthesis, i.e., by automatically generating a controller to enforce correct self-adaptive behaviours. We integrate Ctrl-F with FraSCAti, a Service Component Architecture middleware platform and we illustrate the use of Ctrl-Fby applying it to two case studies. © 2017 Elsevier Inc.","Component-based architecture; Discrete control; Self-adaptation"
"A metamorphic testing approach for supporting program repair without the need for a test oracle","2017","Journal of Systems and Software","10.1016/j.jss.2016.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966376822&doi=10.1016%2fj.jss.2016.04.002&partnerID=40&md5=1dc6e2705192ff4a1049eda40386b9e1","Test suite based automated program repair (APR) relies on a test oracle to determine the execution result of individual test cases. The applicability of APR techniques, therefore, is limited by the fact that test oracles may not exist. Metamorphic Testing (MT) is a testing approach that, rather than checking the correctness of individual test outputs, checks testing results through verification of relations among multiple test cases and their outputs: MT can therefore be applied without test oracles. This paper presents an integration of MT with APR that enables application of APR without the need for a test oracle. Two important issues for this integration which have been thoroughly investigated and addressed are: (1) feasibility — which is addressed by proposing a framework to support the integration, and then presenting MT-GenProg, a tool incorporating MT with the popular APR technique GenProg; and (2) effectiveness — which is confirmed through an empirical study of GenProg and MT-GenProg on 1,143 program versions from the IntroClass benchmark suite, demonstrating MT-GenProg's comparable performance to GenProg, in terms of repair effectiveness. We conclude that the proposed integration is both practically feasible and effective, and thus successfully extends APR techniques to a broader application domain. © 2016","Metamorphic testing; Test oracle; Test suite based automated program repair"
"A systematic literature review: Opinion mining studies from mobile app store user reviews","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003875563&doi=10.1016%2fj.jss.2016.11.027&partnerID=40&md5=320e0bd738a694513c6444018a983816","As mobile devices have overtaken fixed Internet access, mobile applications and distribution platforms have gained in importance. App stores enable users to search for, purchase and install mobile applications and then give feedback in the form of reviews and ratings. A review might contain information about the user's experience with the app and opinion of it, feature requests and bug reports. Hence, reviews are valuable not only to users who would like to find out what others think about an app, but also to developers and software companies interested in customer feedback. The rapid increase in the number of applications and total app store revenue has accelerated app store data mining and opinion aggregation studies. While development companies and app store regulators have pursued upfront opinion mining studies for business intelligence and marketing purposes, research interest into app ecosystem and user reviews is relatively new. In addition to studies examining online product reviews, there are now some academic studies focused on mobile app stores and user reviews. The objectives of this systematic literature review are to identify proposed solutions for mining online opinions in app store user reviews, challenges and unsolved problems in the domain, any new contributions to software requirements evolution and future research direction. © 2016","App stores opinion mining; Mobile application; Requirements engineering; Systematic literature review"
"API usage pattern recommendation for software development","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979030646&doi=10.1016%2fj.jss.2016.07.026&partnerID=40&md5=e34bb8627ed90edc15858f1e94df86f2","Application Programming Interfaces (APIs) facilitate pragmatic reuse and improve the productivity of software development. An API usage pattern documents a set of method calls from multiple API classes to achieve a reusable functionality. Existing approaches often use frequent-sequence mining to extract API usage patterns. However, as reported by earlier studies, frequent-sequence mining may not produce a complete set of usage patterns. In this paper, we explore the possibility of mining API usage patterns without relying on frequent-pattern mining. Our approach represents the source code as a network of object usages where an object usage is a set of method calls invoked on a single API class. We automatically extract usage patterns by clustering the data based on the co-existence relations between object usages. We conduct an empirical study using a corpus of 11,510 Android applications. The results demonstrate that our approach can effectively mine API usage patterns with high completeness and low redundancy. We observe 18% and 38% improvement on F-measure and response time respectively comparing to usage pattern extraction using frequent-sequence mining. © 2016 Elsevier Inc.","Clustering; Object usage; Usage pattern"
"A supercomputing framework for the evaluation of real-time analysis and optimization techniques","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996606740&doi=10.1016%2fj.jss.2016.11.010&partnerID=40&md5=23a51f9c01d76329f2361f35ee695076","The evaluation of new approaches in the analysis and optimization of real-time systems usually relies on synthetic test systems. Therefore, the development of tools to create these test systems in an efficient way is highly desirable. It is usual for these evaluations to be constrained by the processing power of current personal computers. For example, in order to assess whether a specific technique generally performs better than others or whether the improvement observed is constrained to a limited set of circumstances, a vast set of examples must be tested, making the execution infeasible in a common PC. In this paper, we present a framework that defines the building blocks of a tool to enable the validation of real-time techniques, through the efficient execution of massive evaluations of event-driven synthetic distributed systems. Its main characteristic is that it can leverage the computing power of a supercomputer to perform large studies that otherwise could not be performed with a PC. The framework also defines different generation methods so that the generated systems can cover a wide range of characteristics that can be present in different application domains. As a case study, we also implement this framework based on a previously developed prototype. © 2016 Elsevier Inc.","Distributed systems; Real-time systems; Supercomputer exploitation; Technique validation"
"A similarity query system for road traffic data based on a NoSQL document store","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011396325&doi=10.1016%2fj.jss.2017.01.016&partnerID=40&md5=e60a32d44bfc357f4ee3f57f45973a24","Advancements in sensing and communication technologies are enabling intelligent transportation systems (ITS) to easily acquire large volumes of road traffic big data. Querying road traffic data is a crucial task for providing citizens with more insightful information on traffic conditions. In this paper, we have developed a similarity query system for road traffic big data, called SigTrac, that runs on top of an existing MongoDB document store. The SigTrac system represents road traffic sensor data having spatio-temporal characteristics into traffic matrices and stores them into a MongoDB NoSQL document store by exploiting map-reduce operations of MongoDB. In addition, SigTrac efficiently processes similarity queries for traffic data with singular value decomposition (SVD)-based and incremental SVD-based algorithms. Our experimental studies with real traffic data demonstrate the efficiency of SiqTrac for similarity query processing for road traffic big data. © 2017 Elsevier Inc.","MongoDB; NoSQL database; Road traffic data; Similarity queries; SVD-based query processing; Traffic matrices"
"Analysis and selection of a regression model for the Use Case Points method using a stepwise approach","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000774224&doi=10.1016%2fj.jss.2016.11.029&partnerID=40&md5=ca89c5cdc41c4cf92f713ed96523dfd4","This study investigates the significance of use case points (UCP) variables and the influence of the complexity of multiple linear regression models on software size estimation and accuracy. Stepwise multiple linear regression models and residual analysis were used to analyse the impact of model complexity. The impact of each variable was studied using correlation analysis. The estimated size of software depends mainly on the values of the weights of unadjusted UCP, which represent a number of use cases. Moreover, all other variables (unadjusted actors' weights, technical complexity factors, and environmental complexity factors) from the UCP method also have an impact on software size and therefore cannot be omitted from the regression model. The best performing model (Model D) contains an intercept, linear terms, and squared terms. The results of several evaluation measures show that this model's estimation ability is better than that of the other models tested. Model D also performs better when compared to the UCP model, whose Sum of Squared Error was 268,620 points on Dataset 1 and 87,055 on Dataset 2. Model D achieved a greater than 90% reduction in the Sum of Squared Errors compared to the Use Case Points method on Dataset 1 and a greater than 91% reduction on Dataset 2. The medians of the Sum of Squared Errors for both methods are significantly different at the 95% confidence level (p < 0.01), while the medians for Model D (312 and 37.26) are lower than Use Case Points (3134 and 3712) on Datasets 1 and 2, respectively. © 2016 The Authors","Dataset; Multiple linear regression; Software size estimation; Stepwise approach; Use Case Points; Variables analysis"
"Container-based virtual elastic clusters","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011114814&doi=10.1016%2fj.jss.2017.01.007&partnerID=40&md5=07a2b826b514595f7888933e6a7ee2d6","eScience demands large-scale computing clusters to support the efficient execution of resource-intensive scientific applications. Virtual Machines (VMs) have introduced the ability to provide customizable execution environments, at the expense of performance loss for applications. However, in recent years, containers have emerged as a light-weight virtualization technology compared to VMs. Indeed, the usage of containers for virtual clusters allows better performance for the applications and fast deployment of additional working nodes, for enhanced elasticity. This paper focuses on the deployment, configuration and management of Virtual Elastic computer Clusters (VEC) dedicated to process scientific workloads. The nodes of the scientific cluster are hosted in containers running on bare-metal machines. The open-source tool Elastic Cluster for Docker (EC4Docker) is introduced, integrated with Docker Swarm to create auto-scaled virtual computer clusters of containers across distributed deployments. We also discuss the benefits and limitations of this solution and analyse the performance of the developed tools under a real scenario by means of a scientific use case that demonstrates the feasibility of the proposed approach. © 2017 Elsevier Inc.","Cluster computing; Computing; Containers; Elasticity"
"Platform design space exploration using architecture decision viewpoints–A longitudinal study","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997171423&doi=10.1016%2fj.jss.2016.10.031&partnerID=40&md5=706e588f48b67c2d0ac00da536c2a56c","Design space exploration is the simultaneous analysis of problem and solution spaces for a specific domain or application scope. Performing this activity as part of the architectural design is beneficial, especially for software platforms, which are shared across organizations. Exploring the design space of software platforms in a multi-product and multi-domain context is not trivial, and only few methods exist to support this activity systematically. This paper reports on a longitudinal technical action research (TAR) study conducted to adapt and evaluate architecture decision viewpoints for supporting platform design space exploration. The study was conducted in the context of an ABB project, which was performed to explore the design space for a common software platform for mobile device support in several product-specific software platforms at ABB. The results indicate that the adapted decision viewpoints are well suitable for dealing with diverging stakeholder concerns, evaluating technological alternatives and uncovering relationships between decisions to be made. © 2016 Elsevier Inc.","Architecture decision viewpoints; Design space exploration; Technical action research"
"The continuity of continuous integration: Correlations and consequences","2017","Journal of Systems and Software","10.1016/j.jss.2017.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013224881&doi=10.1016%2fj.jss.2017.02.003&partnerID=40&md5=e1401f4ca64974936464862d493551a1","The practice of continuous integration has firmly established itself in the mainstream of the software engineering industry, yet many questions surrounding it remain unanswered. Prominent among these is the issue of scalability: continuous integration has been reported to be possible to scale, but with difficulties. Understanding of the underlying mechanisms causing these difficulties is shallow, however: what is it about size that is problematic, which kind of size, and what aspect of continuous integration does it impede? Based on quantitative data from six industry cases encompassing close to 2000 engineers, complemented by interviews with engineers from five companies, this paper investigates the correlation between the continuity of continuous integration and size. It is found that not only is there indeed a correlation between the size and composition of a development organization and its tendency to integrate continuously; there is evidence that the size of the organization influences ways of working, which in turn correlate with the degree of continuity, raising the question of software manufacturability. It is further observed that developer behavior in ostensibly continuously integrating cases does not necessarily match expectations, and that frequent integration of the product itself does not automatically imply that each individual developer commits frequently. © 2017 Elsevier Inc.","Agile; Complexity; Continuity; Continuous integration; Developability; Manufacturability; Modularity; Scale; Size"
"Profiling and accelerating commodity NFV service chains with SCC","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012296964&doi=10.1016%2fj.jss.2017.01.005&partnerID=40&md5=2edfba0d03cca40382ef6011154440f9","Recent approaches to network functions virtualization (NFV) have shown that commodity network stacks and drivers struggle to keep up with increasing hardware speed. Despite this, popular cloud networking services still rely on commodity operating systems (OSs) and device drivers. Taking into account the hardware underlying of commodity servers, we built an NFV profiler that tracks the movement of packets across the system's memory hierarchy by collecting key hardware and OS-level performance counters. Leveraging the profiler's data, our Service Chain Coordinator's (SCC) run-time accelerates user-space NFV service chains, based on commodity drivers. To do so, SCC combines multiplexing of system calls with scheduling strategies, taking time, priority, and processing load into account. By granting longer time quanta to chained network functions (NFs), combined with I/O multiplexing, SCC reduces unnecessary scheduling and I/O overheads, resulting in three-fold latency reduction due to cache and main memory utilization improvements. More importantly, SCC reduces the latency variance of NFV service chains by up to 40x compared to standard FastClick chains by making the average case for an NFV chain to perform as well as the best case. These improvements are possible because of our profiler's accuracy. © 2017 The Authors","I/O multiplexing; NFV; Profiler; Scheduling; Service chains"
"Integration between requirements engineering and safety analysis: A systematic literature review","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000399255&doi=10.1016%2fj.jss.2016.11.031&partnerID=40&md5=50b479eb0bb4370a35cc0fa6cbc3b574","Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying SCS by RE, we investigate the approaches proposed to improve the communication or integration between RE and safety engineering in SCS development. We analyze the activities that should be performed by RE during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help RE during specification of SCS that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to RE in the specification of SCS that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners. © 2016 Elsevier Inc.","Communication; Integration; Requirements engineering; Safety analysis; Safety-critical systems; Systematic literature review"
"The state of the art on design patterns: A systematic mapping of the literature","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002599816&doi=10.1016%2fj.jss.2016.11.030&partnerID=40&md5=2155c81217c8e4c44668cec1062d12a4","Design patterns are widely used by software developers to build complex systems. Hence, they have been investigated by many researchers in recent decades. This leads to the emergence of various topics in the design patterns field. The objective of this paper is to present an overview of the research efforts on design patterns for those researchers who seek to enter this area. The main contributions are as follows: (a) identifying research topics in design patterns, (b) quantifying the research emphasis on each topic, and (c) describing the demographics of design patterns research. The last secondary study with similar goals in the design patterns field considers the Gang of Four design patterns only. However, the scope of the current study is all of the design patterns. Moreover, our review covers about six additional years and a larger number of publications and venues. In this systematic mapping study, a total of 2775 papers were identified as relevant, and 637 of them were included. According to the results, design patterns can be classified into six different research topics. As a consequence, it is concluded that Pattern Development, Pattern Mining, and Pattern Usage are the most active topics in the field of design patterns. © 2016 Elsevier Inc.","Design patterns; Systematic mapping study; Systematic review"
"Accurate modeling and efficient QoS analysis of scalable adaptive systems under bursty workload","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019454236&doi=10.1016%2fj.jss.2017.05.022&partnerID=40&md5=e7befc99d5f33292667bb99cd882bd99","Fulfillment of QoS requirements for systems deployed in the Internet is becoming a must. A widespread characteristic of this kind of systems is that they are usually subject to highly variable and bursty workloads. The allocation of resources to fulfill QoS requirements during the peak workloads could entail a waste of computing resources. A solution is to provide the system with self-adaptive techniques that can allocate resources only when and where they are required. We pursue the QoS evaluation of workload-aware self-adaptive systems based on stochastic models. In particular, this work proposes an accurate modeling of the workload variability and burstiness phenomena based on previous approaches that use Markov Modulated Poisson Processes. We extend these approaches in order to accurately model the variations of the workload strongly influence the QoS of the self-adaptive system. Unfortunately, this stochastic modeling may lead to a non tractable QoS analysis. Consequently, this work also develops an efficient procedure for carrying out the QoS analysis. © 2017 Elsevier Inc.","Adaptability; Markov models; Quality of service; Stochastic petri nets; Workload modeling"
"XTRAITJ: Traits for the Java platform","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994226132&doi=10.1016%2fj.jss.2016.07.035&partnerID=40&md5=c40e5534e2bbe837a6c9fd9301ec495f","Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. In this paper we present the new version of XTRAITJ, a trait-based programming language that features complete compatibility and interoperability with the JAVA platform. XTRAITJ is implemented in XTEXT and XBASE, and it provides a full Eclipse IDE that supports an incremental adoption of traits in existing JAVA projects. The new version of XTRAITJ allows traits to be accessed from any JAVA project or library, even if the original XTRAITJ source code is not available, since traits can be accessed in their byte-code format. This allows developers to create XTRAITJ libraries that can be provided in their binary only format. We detail the technique we used to achieve such an implementation; this technique can be reused in other languages implemented in XTEXT for the JAVA platform. We formalize our traits by means of flattening semantics and we provide some performance benchmarks that show that the runtime overhead introduced by our traits is acceptable. © 2016 Elsevier Inc.","Eclipse; IDE; Implementation; Java; Trait"
"Impacts of organizational commitment, interpersonal closeness, and Confucian ethics on willingness to report bad news in software projects","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006804686&doi=10.1016%2fj.jss.2016.12.004&partnerID=40&md5=afd1f32d08bb5344397ae0c72d091684","Individuals working on troubled software projects are often reluctant to report bad news concerning the project to senior management. This problem may be particularly acute when a subordinate must bypass their direct manager because the manager engages in a wrongful effort to suppress the bad news, which is the context for our study. In this research, we examine the impacts of organizational commitment, interpersonal closeness, and Confucian ethics on individuals’ willingness to report bad news that is deliberately hidden from upper management and we do this in the Chinese culture context. Based on data collected from 158 Chinese software engineers, we found that organizational commitment positively affects individuals’ willingness to report, while interpersonal closeness with the wrongdoer negatively affects willingness to report. With respect to the influence of Confucian ethics, our findings suggest that: (1) individuals’ ethical disposition toward loyalty between sovereign and subject (interpreted in this research as loyalty to one's organization) strengthens the positive effect of organizational commitment on willingness to report, and (2) individuals’ ethical disposition on trust between friends strengthens the negative effect of interpersonal closeness with the wrongdoer on willingness to report. © 2016 Elsevier Inc.","Confucian ethics; Interpersonal closeness; IT project management; Organizational commitment; Whistleblowing"
"Automatic verification and validation wizard in web-centred end-user software engineering","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002397974&doi=10.1016%2fj.jss.2016.11.025&partnerID=40&md5=dd43ec9817b6cc0212b471628978aab2","This paper addresses one of the major web end-user software engineering (WEUSE) challenges, namely, how to verify and validate software products built using a life cycle enacted by end-user programmers. Few end-user development support tools implement an engineering life cycle adapted to the needs of end users. End users do not have the programming knowledge, training or experience to perform development tasks requiring creativity. Elsewhere we published a life cycle adapted to this challenge. With the support of a wizard, end-user programmers follow this life cycle and develop rich internet applications (RIA) to meet specific end-user requirements. However, end-user programmers regard verification and validation activities as being secondary or unnecessary for opportunistic programming tasks. Hence, although the solutions that they develop may satisfy specific requirements, it is impossible to guarantee the quality or the reusability of this software either for this user or for other developments by future end-user programmers. The challenge, then, is to find means of adopting a verification and validation workflow and adding verification and validation activities to the existing WEUSE life cycle. This should not involve users having to make substantial changes to the type of work that they do or to their priorities. In this paper, we set out a verification and validation life cycle supported by a wizard that walks the user through test case-based component, integration and acceptance testing. This wizard is well-aligned with WEUSE's characteristic informality, ambiguity and opportunisticity. Users applying this verification and validation process manage to find bugs and errors that they would otherwise be unable to identify. They also receive instructions for error correction. This assures that their composite applications are of better quality and can be reliably reused. We also report a user study in which users develop web software with and without a wizard to drive verification and validation. The aim of this user study is to confirm the applicability and effectiveness of our wizard in the verification and validation of a RIA. © 2016 Elsevier Inc.","End-user programming; End-user software engineering; human-computer interaction; Reliability; visual programming; Web engineering"
"Performance evaluation of cloud-based log file analysis with Apache Hadoop and Apache Spark","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001086191&doi=10.1016%2fj.jss.2016.11.037&partnerID=40&md5=a895b194b9cfb53377df28d35b65f36d","Log files are generated in many different formats by a plethora of devices and software. The proper analysis of these files can lead to useful information about various aspects of each system. Cloud computing appears to be suitable for this type of analysis, as it is capable to manage the high production rate, the large size and the diversity of log files. In this paper we investigated log file analysis with the cloud computational frameworks Apache™Hadoop® and Apache Spark™. We developed realistic log file analysis applications in both frameworks and we performed SQL-type queries in real Apache Web Server log files. Various experiments were performed with different parameters in order to study and compare the performance of the two frameworks. © 2016 Elsevier Inc.","Apache Hadoop; Apache Spark; Cloud; Log Analysis; Performance Evaluation"
"Understanding cloud-native applications after 10 years of cloud computing - A systematic mapping study","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009186306&doi=10.1016%2fj.jss.2017.01.001&partnerID=40&md5=00833465689b49e90c3ae3861f0862ab","It is common sense that cloud-native applications (CNA) are intentionally designed for the cloud. Although this understanding can be broadly used it does not guide and explain what a cloud-native application exactly is. The term “cloud-native” was used quite frequently in birthday times of cloud computing (2006) which seems somehow obvious nowadays. But the term disappeared almost completely. Suddenly and in the last years the term is used again more and more frequently and shows increasing momentum. This paper summarizes the outcomes of a systematic mapping study analyzing research papers covering “cloud-native” topics, research questions and engineering methodologies. We summarize research focuses and trends dealing with cloud-native application engineering approaches. Furthermore, we provide a definition for the term “cloud-native application” which takes all findings, insights of analyzed publications and already existing and well-defined terminology into account. © 2017 Elsevier Inc.","Cloud-native application; CNA; Elastic platform; Microservice; Pattern; Self service; Softwareization; Systematic mapping study"
"Issues in complex event processing: Status and prospects in the Big Data era","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008324741&doi=10.1016%2fj.jss.2016.06.011&partnerID=40&md5=167d530d86d3aab0dfd19b8c8976e3f4","Many Big Data technologies were built to enable the processing of human generated data, setting aside the enormous amount of data generated from Machine-to-Machine (M2M) interactions and Internet-of-Things (IoT) platforms. Such interactions create real-time data streams that are much more structured, often in the form of series of event occurrences. In this paper, we provide an overview on the main research issues confronted by existing Complex Event Processing (CEP) techniques, with an emphasis on query optimization aspects. Our study expands on both deterministic and probabilistic event models and spans from centralized to distributed network settings. In that, we cover a wide range of approaches in the CEP domain and review the current status of techniques that tackle efficient query processing. These techniques serve as a starting point for developing Big Data oriented CEP applications. Therefore, we further study the issues that arise upon trying to apply those techniques over Big Data enabling technologies, as is the case with cloud platforms. Furthermore, we expand on the synergies among Predictive Analytics and CEP with an emphasis on scalability and elasticity considerations in cloud platforms with potentially dispersed resource pools. © 2016 Elsevier Inc.","Cloud computing; Complex event processing; Predictive analytics"
"Distributed analysis and filtering of application event streams","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018470279&doi=10.1016%2fj.jss.2017.03.057&partnerID=40&md5=e508757a5b59297277513a3e371887d0","Large information systems comprise different interconnected hardware and software components, that collectively generate large volumes of data. Furthermore, the run-time analysis of such data involves computationally expensive algorithms, and is pivotal to a number of software engineering activities such as, system understanding, diagnostics, and root cause analysis. In a quest to increase the performance of run-time analysis for large sets of logged data, we present an approach that allows for the real time reduction of one or more event streams by utilizing a set of filtering criteria. More specifically, the approach employs a similarity measure that is based on information theory principles, and is applied between the features of the incoming events, and the features of a set of retrieved or constructed events, that we refer to as beacons. The proposed approach is domain and event schema agnostic, can handle infinite data streams using a caching algorithm, and can be parallelized in order to tractably process high volume, high frequency, and high variability data. Experimental results obtained using the KDD’99 and CTU-13 labeled data sets, indicate that the approach is scalable, and can yield highly reduced sets with high recall values with respect to a use case. © 2017 Elsevier Inc.","Dynamic analysis; Event stream filtering; Information theory; Software engineering; System understanding"
"A theoretical analysis on cloning the failed test cases to improve spectrum-based fault localization","2017","Journal of Systems and Software","10.1016/j.jss.2017.04.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018958478&doi=10.1016%2fj.jss.2017.04.017&partnerID=40&md5=dde5a1b48c9fdcbf8d176793280965d2","Fault localization is the activity to locate faults in programs. Spectrum-based fault localization (SBFL) is a class of techniques for it. It contrasts the code coverage achieved by passed runs and that by failed runs, and estimates program entities responsible for the latter. Although previous work has empirically shown that the effectiveness of typical SBFL techniques can be improved by incorporating more failed runs, debugging often takes place when there are very few of them. In this paper, we report a comprehensive study to investigate the impact of cloning the failed test cases on the effectiveness of SBFL techniques. We include 33 popular such techniques, and examine the accuracy of their formulas on twelve benchmark programs, using four accuracy metrics and in three scenarios. The empirical results show that on 22, 21, and 23 of them the fault-localization accuracy can be significantly improved, when the failed test cases are cloned in the single-fault, double-fault, and triple-fault scenarios, respectively. We also analytically show that on 19 of them the improvements are provable for an arbitrary program and an arbitrary test suite, in the single-fault scenario; and moreover, for ten of the rest formulas, their accuracy are proved unaffected in all scenarios. © 2017 Elsevier Inc.","Class imbalance; Fault localization; Software debugging; Test suite cloning"
"Providing fair-share scheduling on multicore computing systems via progress balancing","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002578907&doi=10.1016%2fj.jss.2016.11.053&partnerID=40&md5=27c7bba1e8b9996b9d3bcbc9930751e6","Performance isolation in a scalable multicore system is often attempted through periodic load balancing paired with per-core fair-share scheduling. Unfortunately, load balancing cannot guarantee the desired level of multicore fairness since it may produce unbounded differences in the progress of tasks. In reality, the balancing of load across cores is only indirectly related to multicore fairness. To address this limitation and ultimately achieve multicore fairness, we propose a new task migration policy we name progress balancing, and present an algorithm for its realization. Progress balancing periodically distributes tasks among cores to directly balance the progress of tasks by bounding their virtual runtime differences. In doing so, it partitions runnable tasks into task groups and allocates them onto cores such that tasks with larger virtual runtimes run on a core with a larger load and thus proceed more slowly. We formally prove the fairness property of our algorithm. To demonstrate its effectiveness, we implemented our algorithm into Linux kernel 3.10 and performed extensive experiments. In the target system, our algorithm yields the maximum virtual runtime difference of 1.07 s, regardless of the uptime of tasks, whereas the Linux CFS produces unbounded virtual runtime differences. © 2016","Fair-share scheduling; Load balancing; Multicore scheduling; Resource management"
"Employing traditional machine learning algorithms for big data streams analysis: The case of object trajectory prediction","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008336840&doi=10.1016%2fj.jss.2016.06.016&partnerID=40&md5=fd59a5f03faedab84f2214fe1eb76c4e","In this paper, we model the trajectory of sea vessels and provide a service that predicts in near-real time the position of any given vessel in 4′, 10′, 20′ and 40′ time intervals. We explore the necessary tradeoffs between accuracy, performance and resource utilization is explored given the large volume and update rates of input data. We start with building models based on well-established machine learning algorithms using static datasets and multi-scan training approaches and identify the best candidate to be used in implementing a single-pass predictive approach, under real-time constraints. The results are measured in terms of accuracy and performance and are compared against the baseline kinematic equations. Results show that it is possible to efficiently model the trajectory of multiple vessels using a single model, which is trained and evaluated using an adequately large, static dataset, thus achieving a significant gain in terms of resource usage while not compromising accuracy. © 2016 Elsevier Inc.","Data streams; Machine learning; Real-time query response; Trajectory prediction"
"Semantic versioning and impact of breaking changes in the Maven repository","2017","Journal of Systems and Software","10.1016/j.jss.2016.04.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007591252&doi=10.1016%2fj.jss.2016.04.008&partnerID=40&md5=462c432788ea3bf69c3438445284e211","Systems that depend on third-party libraries may have to be updated when updates to these libraries become available in order to benefit from new functionality, security patches, bug fixes, or API improvements. However, often such changes come with changes to the existing interfaces of these libraries, possibly causing rework on the client system. In this paper, we investigate versioning practices in a set of more than 100,000 jar files from Maven Central, spanning over 7 years of history of more than 22,000 different libraries. We investigate to what degree versioning conventions are followed in this repository. Semantic versioning provides strict rules regarding major (breaking changes allowed), minor (no breaking changes allowed), and patch releases (only backward-compatible bug fixes allowed). We find that around one third of all releases introduce at least one breaking change. We perform an empirical study on potential rework caused by breaking changes in library releases and find that breaking changes have a significant impact on client libraries using the changed functionality. We find out that minor releases generally have larger release intervals than major releases. We also investigate the use of deprecation tags and find out that these tags are applied improperly in our dataset. © 2016 Elsevier Inc.","Breaking changes; Semantic versioning; Software libraries"
"Continuous Delivery: Overcoming adoption challenges","2017","Journal of Systems and Software","10.1016/j.jss.2017.02.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016081845&doi=10.1016%2fj.jss.2017.02.013&partnerID=40&md5=43909814037b8d34d55efc68c6dec662","Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted CD have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting CD can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different from—or even conflict with—our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to CD requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling CD as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual CD pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing CD at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for CD adoption. © 2017 The Author","Adoption; Agile Software Development; Continuous Delivery; Continuous Deployment; Continuous Software Engineering; DevOps"
"A search engine for finding and reusing architecturally significant code","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007565367&doi=10.1016%2fj.jss.2016.11.034&partnerID=40&md5=9304127b456767594634293d888fe052","Architectural tactics are the building blocks of software architecture. They describe solutions for addressing specific quality concerns, and are prevalent across many software systems. Once a decision is made to utilize a tactic, the developer must generate a concrete plan for writing code and implementing the tactic. Unfortunately, this is a non-trivial task even for experienced developers. Often, developers resort to using search engines, crowd-sourcing websites, or discussion forums to find sample code snippets to implement a tactic. A fundamental problem of finding implementation for architectural tactics/patterns is the mismatch between the high-level intent reflected in the descriptions of these patterns and the low-level implementation details of them. To reduce this mismatch, we created a novel Tactic Search Engine called ArchEngine (ARCHitecture search ENGINE). ArchEngine can replace this manual internet-based search process and help developers find and reuse tactical code from a wide range of open source systems. ArchEngine helps developers find implementation examples of an architectural tactic for a given technical context. It uses information retrieval and program analysis techniques to retrieve applications that implement these design concepts. Furthermore, it lists and rank the code snippets where the patterns/tactics are located. Our case study with 21 graduate students (with experience level of junior software developers) shows that ArchEngine is more effective than other search engines (e.g., Krugle and Koders) in helping programmers to quickly find implementations of architectural tactics/patterns. © 2016","Architecture; Information; Models; Tactics; Traceability"
"Generating reusable, searchable and executable “architecture constraints as services”","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012057884&doi=10.1016%2fj.jss.2017.01.032&partnerID=40&md5=f5e8cf1834eb17a9fa83ccd28335db85","Architecture constraints are components of design documentation. They enable designers to enforce rules that architecture descriptions should respect. Many systems make it possible to associate constraints to models at design stage but very few enable their association to code at implementation stage. When possible, this is done manually, which is a tedious, error prone and time consuming task. Therefore, we propose in this work a process to automatically generate executable constraints associated to programs’ code from model-based constraints. First, the process translates the constraints specified at design-time into constraint-components described with an ADL, called CLACS. Then, it creates constraint-services which can be registered and later invoked to check their embedded constraints on component- and service-based applications. We chose to target components and services in order to make architecture constraints reusable, searchable in registries, customizable and checkable at the implementation stage. The generated constraint-services use the standard reflective (meta) layer provided by the programing language to introspect elements of the architecture. We experimented our work on a set of 15 architecture constraints and on a real-world system in order to evaluate the effectiveness of the process. © 2017 Elsevier Inc.","Architecture constraint; Automatic translation; Constraint-component; Constraint-service; Introspection; OCL; OSGi"
"A survey of the use of crowdsourcing in software engineering","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995569607&doi=10.1016%2fj.jss.2016.09.015&partnerID=40&md5=6afcf90b7efddb88a3b8359f254d2a3f","The term ‘crowdsourcing’ was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced Software Engineering. © 2016","Crowdsourced software engineering; Crowdsourcing; Literature survey; Software crowdsourcing"
"Experimentally assessing the combination of multiple visualization strategies for software evolution analysis","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016037440&doi=10.1016%2fj.jss.2017.03.006&partnerID=40&md5=44ba7d75bb1b354efb05c38469117131","Software engineers need to comprehend large amounts of data to maintain software. Software Visualization is an area that helps users to analyze software through the use of visual resources. It can be effectively used to understand the large amount of data produced during software evolution. A key challenge in the area is to create strategies to consistently visualize the many software attributes, modules and versions produced during its lifecycle. Most of the current visualization strategies seek to present data as a whole, including all available versions of the software in one visual scene. The area lacks strategies visualizing software in detail through the analysis of the evolution of specific software modules. Both strategies are useful, and should be selected according to the task at hand. This work focuses on combining software evolution visualization strategies, experimentally validating the benefits of the approach. Its goal was to build empirical evidence on the use of the combined multiple strategies for software evolution comprehension. It presents an experimental study that exploits the benefits of combining multiple visual strategies of software evolution analysis. The results show that combined visualization strategies perform better in terms of correctness and analysis time. © 2017 Elsevier Inc.","Experimental evaluation; Software evolution visualization; Visual strategies"
"ReSeer: Efficient search-based replay for multiprocessor virtual machines","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979741017&doi=10.1016%2fj.jss.2016.07.032&partnerID=40&md5=74aa699af906a30a6defe49eb3c89b6e","Efficient replay of virtual machines is important for software debugging, fault tolerance, and performance analysis. The current approaches of replaying virtual machines record the details of system execution at runtime. However, these approaches incur much overhead, which affects the system performance. Especially, in a multiprocessor system, recording the shared memory operations of multiple processors leads to a large amount of computing overhead and log files. To address the above issue, this paper proposes ReSeer—a search-based replay approach for multiprocessor virtual machines. ReSeer consists of three phases including record, search, and replay. In the record phase, we record only necessary non-deterministic events at runtime, and incrementally take memory checkpoints at a defined interval. In the search phase, we encode all the possible execution paths as binary strings, and use a genetic algorithm to search expected execution paths achieving the expected checkpoint. In the replay phase, we replay the system execution according to the searched execution paths and the logged non-deterministic events. Compared with current approaches, ReSeer significantly reduces performance overhead at runtime by searching expected execution paths instead of recording all the operations of accessing shared memory. We have implemented ReSeer, and then evaluated it with a series of typical benchmarks deployed on an open source virtual machine—Xen. The experimental results show that ReSeer can reduce the record overhead at runtime efficiently. © 2016 Elsevier Inc.","Deterministic replay; Genetic algorithm; Memory checkpoint; Virtual machine; Xen"
"An empirical study of data decomposition for software parallelization","2017","Journal of Systems and Software","10.1016/j.jss.2016.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964330300&doi=10.1016%2fj.jss.2016.02.002&partnerID=40&md5=f6a61f6885f77fa02a0004f203d0ca08","Context: Multi-core architectures are becoming increasingly ubiquitous and software professionals are seeking to leverage the capabilities of distributed-memory architectures. The process of parallelizing software applications can be very tedious and error-prone, in particular the task of data decomposition. Empirical studies investigating the complexity of data decomposition and communication are lacking. Objective: Our objective is threefold: (i) to gain an empirical-based understanding of the task of data decomposition as part of the parallelization of software applications; (ii) to identify key requirements for tools to assist developers in this task, and (iii) assess the current state-of-the-art. Methods: Our empirical investigation employed a multi-method approach, using an interview study, participant-observer case study, focus group study, and a sample survey. The empirical investigation involved collaborations with three industry partners: IBM's High Performance Computing Center, the Irish Centre for High-End Computing (ICHEC), and JBA Consulting. Results: This article presents data decomposition as one of the most prevalent tasks of parallelizing applications for multi-core architectures. Based on our studies, we identify ten key requirements for tool support to help HPC developers in this area. Our evaluation of the state-of-the-art shows that none of the extant tool support implements all 10 requirements. Conclusion: While there is a considerable body of research in the area of HPC, a few empirical studies exist which explicitly focus on the challenges faced by practitioners in this area; this research aims to address this gap. The empirical studies in this article provide insights that may help researchers and tool vendors to better understand the needs of parallel programmers. © 2016 Elsevier Inc.","Data decomposition; Empirical studies; Parallelization"
"Proactive elasticity and energy awareness in data stream processing","2017","Journal of Systems and Software","10.1016/j.jss.2016.08.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981719834&doi=10.1016%2fj.jss.2016.08.037&partnerID=40&md5=51c7555483c4d8ee489b66625eaf4b44","Data stream processing applications have a long running nature (24 hr/7 d) with workload conditions that may exhibit wide variations at run-time. Elasticity is the term coined to describe the capability of applications to change dynamically their resource usage in response to workload fluctuations. This paper focuses on strategies for elastic data stream processing targeting multicore systems. The key idea is to exploit Model Predictive Control, a control-theoretic method that takes into account the system behavior over a future time horizon in order to decide the best reconfiguration to execute. We design a set of energy-aware proactive strategies, optimized for throughput and latency QoS requirements, which regulate the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support offered by modern multicore CPUs. We evaluate our strategies in a high-frequency trading application fed by synthetic and real-world workload traces. We introduce specific properties to effectively compare different elastic approaches, and the results show that our strategies are able to achieve the best outcome. © 2016 Elsevier Inc.","Data stream processing; Elasticity; Frequency scaling; Model predictive control"
"Automotive software engineering: A systematic mapping study","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015261358&doi=10.1016%2fj.jss.2017.03.005&partnerID=40&md5=fffc3b97ee5499acebd80afa1ca6cf6f","The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor. © 2017 Elsevier Inc.","Automotive software engineering; Automotive systems; Embedded systems; Literature survey; Software-intensive systems; Systematic mapping study"
"Design annotations to improve API discoverability","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009268752&doi=10.1016%2fj.jss.2016.12.036&partnerID=40&md5=f1c24ccf306abeb206a150784c8bbc9d","User studies have revealed that programmers face several obstacles when learning application programming interfaces (APIs). A considerable part of such difficulties relate to discovery of API elements and the relationships among them. To address discoverability problems, we show how to complement APIs with design annotations, which document design decisions in a program-processable form for types, methods, and parameters. The information provided by the annotations is consumed by the integrated development environment (IDE) in order to assist API users with useful code completion proposals regarding object creation and manipulation, which facilitate API exploration and learning. As a proof of concept, we developed Dacite, a tool which comprises a set of Java annotations and an accompanying plugin for the Eclipse IDE. A user study revealed that Dacite is usable and effective, and Dacite's proposals enable programmers to be more successful in solving programming tasks involving unfamiliar APIs. © 2017 Elsevier Inc.","Annotations; API usability; Code completion; Eclipse; IDE"
"Bayesian network model for task effort estimation in agile software development","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012069104&doi=10.1016%2fj.jss.2017.01.027&partnerID=40&md5=5fd38fedbacd309964550d5789606087","Even though the use of agile methods in software development is increasing, the problem of effort estimation remains quite a challenge, mostly due to the lack of many standard metrics to be used for effort prediction in plan-driven software development. The Bayesian network model presented in this paper is suitable for effort prediction in any agile method. Simple and small, with inputs that can be easily gathered, the suggested model has no practical impact on agility. This model can be used as early as possible, during the planning stage. The structure of the proposed model is defined by the authors, while the parameter estimation is automatically learned from a dataset. The data are elicited from completed agile projects of a single software company. This paper describes various statistics used to assess the precision of the model: mean magnitude of relative error, prediction at level m, accuracy (the percentage of successfully predicted instances over the total number of instances), mean absolute error, root mean squared error, relative absolute error and root relative squared error. The obtained results indicate very good prediction accuracy. © 2017 Elsevier Inc.","Agile software development; Bayesian network; Effort prediction"
"An exploratory study on the usage of common interface elements in android applications","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979497676&doi=10.1016%2fj.jss.2016.07.010&partnerID=40&md5=e81d19d7259666166b5db0ba4258317d","The number of mobile applications has increased drastically in the past few years. A recent study has shown that reusing source code is a common practice for Android application development. However, reuse in mobile applications is not necessarily limited to the source code (i.e., program logic). User interface (UI) design plays a vital role in constructing the user-perceived quality of a mobile application. The user-perceived quality reflects the users’ opinions of a product. For mobile applications, it can be quantified by the number of downloads and raters. In this study, we extract commonly used UI elements, denoted as Common Element Sets (CESs), from user interfaces of applications. Moreover, we highlight the characteristics of CESs that can result in a high user-perceived quality by proposing various metrics. Through an empirical study on 1292 mobile applications, we observe that (i) CESs of mobile applications widely occur among and across different categories; (ii) certain characteristics of CESs can provide a high user-perceived quality; and (iii) through a manual analysis, we recommend UI templates that are extracted and summarized from CESs for developers. Developers and quality assurance personnel can use our guidelines to improve the quality of mobile applications. © 2016 Elsevier Inc.","Common UI elements; Mobile applications; User-perceived quality"
"A parallelization approach for resource-restricted embedded heterogeneous MPSoCs inspired by OpenMP","2017","Journal of Systems and Software","10.1016/j.jss.2016.08.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011411870&doi=10.1016%2fj.jss.2016.08.069&partnerID=40&md5=cf026344f2a2e45c75680e04e44c470e","Future low-end embedded systems will make an increased use of heterogeneous MPSoCs. To utilize these systems efficiently, methods and tools are required that support the extraction and implementation of parallelism typically found in embedded applications. Ideally, large amounts of existing legacy code should be reused and ported to these new systems. Existing parallelization infrastructures, however, mostly support parallelization according to the requirements of HPEC systems. For resource-restricted embedded systems, different parallelization strategies are necessary to achieve additional non-functional objectives such as the reduction of energy consumption. HPC-focused parallelization also assumes processor, memory and communication structures different from low-end embedded systems and therefore wastes optimization opportunities essential for improving the performance of resource-constrained embedded systems. This paper describes a new approach and infrastructure inspired by the OpenMP API to support the extraction and implementation of pipeline parallelism, which is commonly found in complex embedded applications. In addition, advanced techniques to extract parallelism from legacy applications requiring only minimal code modifications are presented. Further, the resulting toolflow combines advanced parallelization, mapping and communication optimization tools leading to a more efficient approach to exploit parallelism for typical embedded applications on heterogeneous MPSoCs running distributed real-time operating systems. © 2016 Elsevier Inc.","Embedded systems; Heterogeneous multiprocessor system-on-chip; Parallelization"
"Versatile workload-aware power management performability analysis of server virtualized systems","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008895600&doi=10.1016%2fj.jss.2016.12.037&partnerID=40&md5=04d28740b3a15e23396d505f60c96791","The widespread integration of virtualization technologies in data centers has enabled in the last few years several benefits in terms of operating costs and flexibility. These benefits maybe boosted through join optimization of power management (PM) and dependability for virtualized systems. This indeed involves developing appropriate models to better understand their performability behavior whenever they are exposed to predictable (e.g. rejuvenation) and unpredictable breakdowns. We propose in this paper a performability analysis of server virtualized systems (SVSs) using a workload-aware PM mechanism based on non-Markovian Stochastic Reward Nets (SRNs) modeling approach. This analysis investigates interactions and correlations between several modules involving workload-aware PM mechanism, dynamic speed scaling processing, virtual machine (VM) and virtual machine monitor (VMM) both subject to software aging, failure and rejuvenation. We show through numerical results, using quantitative and qualitative metrics, how performance, power usage and efficiency are impacted by workload-aware PM mechanism. We show also how judicious choice of tunable attribute (i.e. Timeout) of the proposed PM mechanism with respect to workload can lead to a good power-performance trade-off. © 2017 Elsevier Inc.","Performability; Power-performance trade-off; SRNs; SVSs; Virtualization; Workload-aware PM"
"Critical-blame analysis for OpenMP 4.0 offloading on Intel Xeon Phi","2017","Journal of Systems and Software","10.1016/j.jss.2015.12.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955312939&doi=10.1016%2fj.jss.2015.12.050&partnerID=40&md5=c4ab4b176202b3654849be865b89e530","Recent supercomputers rated in the TOP 500 list increasingly utilize accelerator or co-processor devices to improve performance and energy efficiency. Since version 4.0 of the specification OpenMP addresses this heterogeneity in computing with the target directives, which enable programmers to offload portions of the code to massively-parallel target devices. Due to this new complexity in hardware and software design, performance optimization of large-scale parallel programs becomes more and more challenging. As manual performance analysis is getting infeasible for complex high performance computing (HPC) codes, we propose an approach to automatically detect bottlenecks such as load imbalances in heterogeneous OpenMP applications. We developed a method to perform critical-path and root-cause analysis for the OpenMP 4.0 offloading model and integrated it into the tool CASITA. The post-mortem analysis is based on execution traces that are generated with an implementation of the evolving OpenMP Tools Interface into the measurement system Score-P. To validate the implementation of our method we ported several existing codes to OpenMP 4.0 , executed them with an Intel Xeon Phi as the target device and analyzed the resulting trace files. © 2016 Elsevier Inc.","Offloading; OpenMP; Performance analysis"
"Fine-grained access control system based on fully outsourced attribute-based encryption","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007569676&doi=10.1016%2fj.jss.2016.12.018&partnerID=40&md5=c83b5eba3d8e3e872bb0fc6106e3e71f","Attribute-based encryption (ABE) has potential to be applied in cloud computing applications to provide fine-grained access control over encrypted data. However, the computation cost of ABE is considerably expensive, because the pairing and exponentiation operations grow with the complexity of access formula. In this work, we propose a fully outsourced ciphertext-policy ABE scheme that for the first time achieves outsourced key generation, encryption and decryption simultaneously. In our scheme, heavy computations are outsourced to public cloud service providers, leaving no complex operations for the private key generator (PKG) and only one modular exponentiation for the sender or the receiver, and the communication cost of the PKG and users is optimized. Moreover, we give the security proof and implement our scheme in Charm, and the experimental results indicate that our scheme is efficient and practical. © 2016 Elsevier Inc.","Access control; Attribute-based encryption; Low communication cost; Outsourced computation"
"A context model for IDE-based recommendation systems","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028246809&doi=10.1016%2fj.jss.2016.09.012&partnerID=40&md5=c17cf31464ee463b77299f9b5fb06f63","Context, as modeled through variables called contextual factors, can improve human-computer interaction. To date, in applications supporting software development, such as integrated development environments (IDEs) and recommendation systems for software engineering (RSSEs), contextual factors have generally been constrained to project artifacts, such as source code. In this paper, we present a context model that includes thirteen contextual factors, which capture various situations in which developers interact with an IDE. This context model can be used to support and enhance user interaction with an IDE or to improve the accuracy and timing of recommendations produced by RSSEs. To assess whether the proposed contextual factors are informative for a context model, we statistically evaluated the correlations between IDE command usage and different situations, as they are described by the factors. If a contextual factor correlates with the usage of a command this means that the user is using the command differently when the values of the contextual factor change. We discovered that different factors correlate with different commands and that all the factors correlate with some commands, hence, when a context change is detected, we can also expect a change in the interaction with an IDE. © 2016 Elsevier Inc.","Commands; Context; Integrated development environment; Recommendation systems for software engineering; Software development; Usage"
"Development and use of a new task model for cyber-physical systems: A real-time scheduling perspective","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009982914&doi=10.1016%2fj.jss.2017.01.004&partnerID=40&md5=95cbf0309da3a84234ba5df3b97aa3b1","In a typical cyber-physical system (CPS), the cyber/computation subsystem controls the physical subsystem, and therefore the computer society has recently paid considerable attention to CPS research. To keep such a CPS stable, feedback control with periodic computation tasks has been widely used, and its theoretical guarantee of stability has been made with periodic real-time task models that enforce strict periodic control updates. However, some control update misses are usually allowed (e.g., via system over-design) in certain physical subsystem states (PSSes) without causing system instability, and the resources required for strict periodic control updates can thus be reduced or used for other purposes, achieving efficient controls for the entire CPS in terms of the operational cost, such as fuel consumption or tracking accuracy. In this paper, we propose a new periodic, fault-tolerant CPS task model, which not only expresses efficiency and stability of the underlying physical subsystem, but also generalizes existing periodic real-time task models, by capturing a tolerable number of control update misses in different PSSes. To demonstrate the utility of this model, we develop a new scheduling mechanism that prioritizes jobs (i.e., periodic invocations) of a set of tasks not only by the nature of each task, but also by the number of consecutive prior job deadline misses. Based on its analysis in terms of stability and efficiency, we also propose a priority-assignment policy that lowers the system operation cost without compromising stability. Our in-depth analysis and simulation results show that the scheduling mechanism and its analysis, as well as the priority-assignment policy under the proposed model not only generalize the existing periodic real-time task models, but also significantly lower the system operation cost without losing stability. © 2017 Elsevier Inc.","Cyber-physical systems; Periodic real-time task model; Priority assignment; Real-time scheduling; Task-level fixed-priority scheduling"
"A general framework for comparing automatic testing techniques of Android mobile apps","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008239831&doi=10.1016%2fj.jss.2016.12.017&partnerID=40&md5=f2180779df5f23f1458621602aba7e16","As an increasing number of new techniques are developed for quality assurance of Android applications (apps), there is a need to evaluate and empirically compare them. Researchers as well as practitioners will be able to use the results of such comparative studies to answer questions such as, “What technique should I use to test my app?” Unfortunately, there is a severe lack of rigorous empirical studies on this subject. In this paper, for the first time, we present an empirical study comparing all existing fully automatic “online” testing techniques developed for the Android platform. We do so by first reformulating each technique within the context of a general framework. We recognize the commonalities between the techniques to develop the framework. We then use the salient features of each technique to develop parameters of the framework. The result is a general recasting of all existing approaches in a plug-in based formulation, allowing us to vary the parameters to create instances of each technique, and empirically evaluate them on a common set of subjects. Our results show that (1) the proposed general framework abstracts all the common characteristics of online testing techniques proposed in the literature, (2) it can be exploited to design experiments aimed at performing objective comparisons among different online testing approaches and (3) some parameters that we have identified influence the performance of the testing techniques. © 2016 Elsevier Inc.","Android testing; Automated testing; Comparing android testing techniques; Event-based testing; Mobile apps testing; Random testing"
"Discovering partial periodic-frequent patterns in a transactional database","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002263399&doi=10.1016%2fj.jss.2016.11.035&partnerID=40&md5=6e1af3dc68e975800a771dee78283a21","Time and frequency are two important dimensions to determine the interestingness of a pattern in a database. Periodic-frequent patterns are an important class of regularities that exist in a database with respect to these two dimensions. Current studies on periodic-frequent pattern mining have focused on discovering full periodic-frequent patterns, i.e., finding all frequent patterns that have exhibited complete cyclic repetitions in a database. However, partial periodic-frequent patterns are more common due to the imperfect nature of real-world. This paper proposes a flexible and generic model to find partial periodic-frequent patterns. A new interesting measure, periodic-ratio, has been introduced to determine the periodic interestingness of a frequent pattern by taking into account its proportion of cyclic repetitions in a database. The proposed patterns do not satisfy the anti-monotonic property. A novel pruning technique has been introduced to reduce the search space effectively. A pattern-growth algorithm to find all partial periodic-frequent patterns has also been presented in this paper. Experimental results demonstrate that the proposed model can discover useful information, and the algorithm is efficient. © 2016","Algorithms; Data mining; Knowledge discovery in databases; Partial periodicity; Pattern mining"
"DOTS: An online and near-optimal trajectory simplification algorithm","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009383078&doi=10.1016%2fj.jss.2017.01.003&partnerID=40&md5=d65968df49d9b8902e9c72027d55d041","The last decade witnessed an increasing number of location acquisition equipments such as mobile phone, smart watch etc. Trajectory data is collected with such a high speed that the location based services (LBS) meet challenge to process and take advantage of that big data. Good trajectory simplification (TS) algorithm thus plays an important role for both LBS providers and users as it significantly reduces processing and response time by minimizing the trajectory size with acceptable precision loss. State of the art TS algorithms work in batch mode and are not suitable for streaming data. The online solutions, on the other hand, usually use some heuristics which won't hold the optimality. This paper proposed a Directed acyclic graph based Online Trajectory Simplification (DOTS) method which solves the problem by optimization. Time complexity of DOTS is O(N2/M). A cascaded version of DOTS with time complexity of O(N) is also proposed. To our best knowledge, this is the first time that an optimal and online TS method is proposed. We find both the normal and cascaded DOTS outperform current TS methods like Douglas–Peucker (Douglas and Peucker, 1973), SQUISH (Muckell et al., 2011) etc. with pretty big margin. © 2017 Elsevier Inc.","Data management; Directed acyclic graph; GPS; Location based services; Priority queue; Trajectory simplification"
"Hot spots profiling and dataflow analysis in custom dataflow computing SoftProcessors","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002678813&doi=10.1016%2fj.jss.2016.07.025&partnerID=40&md5=76d67436e6fc7555bf1f38f84482fae6","In the past decades, instruction set extension problem has been a key research area for state-of-the-art design automation of Very Large Scale Integration (VLSI) systems. Meanwhile, recently there is a renewed interest for hot spot profiling and dataflow analysis in custom instruction set processors. This paper proposes HOTISE, an architecture framework for adaptive reconfigurable instruction set processors (RISP) with dynamic profiling and dataflow analysis. A dynamic profiler is employed to obtain hot spots for each application at run-time. Then the selected hot spots will be considered as custom instructions and implemented in reconfigurable logic arrays. An instruction generator based on dataflow generation provides a mapping scheme from each selected instruction to hardware processing element in the array. To demonstrate the accuracy and feasibility of HOTISE, we have implemented a profiler prototype using simulator-based RTL codes. Experimental results show that the profiling results can cover more than 97% hot spots of MiBench and NetBench applications. In particular, the custom instruction of CRC and MD5 application proves the effectiveness of the mapping mechanism, the code sizes of CRC and MD5 could decrease to 32.5% and 37%, while achieving the speedup at 4.7x and 5.1x, respectively. © 2016 Elsevier Inc.","Adaptive processor; Dynamic profiling; Hot spot; Instruction set extension"
"A comparison framework for runtime monitoring approaches","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007565914&doi=10.1016%2fj.jss.2016.12.034&partnerID=40&md5=e67fb715b3fff9b074dd4a54219ff9ec","The full behavior of complex software systems often only emerges during operation. They thus need to be monitored at run time to check that they adhere to their requirements. Diverse runtime monitoring approaches have been developed in various domains and for different purposes. Their sheer number and heterogeneity, however, make it hard to find the right approach for a specific application or purpose. The aim of our research therefore was to develop a comparison framework for runtime monitoring approaches. Our framework is based on an analysis of the literature and existing taxonomies for monitoring languages and patterns. We use examples from existing monitoring approaches to explain the framework. We demonstrate its usefulness by applying it to 32 existing approaches and by comparing 3 selected approaches in the light of different monitoring scenarios. We also discuss perspectives for researchers. © 2016 Elsevier Inc.","Comparison framework; Literature review; Runtime monitoring"
"Nosv: A lightweight nested-virtualization VMM for hosting high performance computing on cloud","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997112151&doi=10.1016%2fj.jss.2016.11.001&partnerID=40&md5=dfd6542e84bc06699af342a7fcb0b862","Moving the high performance computing (HPC) to Cloud not only reduces the costs but also gives users the ability to customize their system. Besides, compared with the traditional HPC computing environments, such as grid and cluster which run HPC applications on bare-metal, cloud equipped with virtualization not only improves resource utilization but also reduces maintenance cost. While for some reasons, current virtualization-based cloud has limited performance for HPC. Such performance overhead could be caused by Virtual Machine Monitor (VMM) interceptions, virtualized I/O devices or cross Virtual Machine (VM) interference, etc. In order to guarantee the performance of HPC applications on Cloud, the VMM should interfere guest VMs as less as possible and allocate dedicated resources such as CPU cores, DRAM and devices to guest VMs running HPC applications. In this paper, we propose a novel cloud infrastructure to serve the HPC applications and normal applications concurrently. This novel infrastructure is based on a lightweight high performance VMM named nOSV. For HPC applications, nOSV constructs a strong isolated high performance guest VM with dedicated resources. At runtime, the high performance VM manages all resources by itself and is not interfered by nOSV. By supporting nested virtualization, nOSV can run HPC with commodity application and keep the flexibility of traditional Cloud. nOSV runs other virtualization environments, like Xen and Docker, as high performance guest VMs. All commodity Cloud applications are hosted in these virtualization environments and share hardware resources with each other. The prototype of nOSV shows that it provides a bare-metal like performance for HPC applications and has about 23% improvement compared to HPC applications running on Xen. © 2016 Elsevier Inc.","Cloud computing; HPC; Multi-core; Nested-virtualization"
"Resemblance and mergence based indexing for high performance data deduplication","2017","Journal of Systems and Software","10.1016/j.jss.2017.02.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015430911&doi=10.1016%2fj.jss.2017.02.039&partnerID=40&md5=cedfb81ce0cfdae58db3a1ccfab106f3","Data deduplication, a data redundancy elimination technique, has been widely employed in many application environments to reduce data storage space. However, it is challenging to provide a fast and scalable key-value fingerprint index particularly for large datasets, while the index performance is critical to the overall deduplication performance. This paper proposes RMD, a resemblance and mergence based deduplication scheme, which aims to provide quick responses to fingerprint queries. The key idea of RMD is to leverage a bloom filter array and a data resemblance algorithm to dramatically reduce the query range. At data ingesting time, RMD uses a resemblance algorithm to detect resemble data segments and put resemblance segments in the same bin. As a result, at querying time, it only needs to search in the corresponding bin to detect duplicate content, which significantly speeds up the query process. Moreover, RMD uses a mergence strategy to accumulate resemblance segments to relevant bins, and exploits frequency-based fingerprint retention policy to cap the bin capacity to improve query throughput and data deduplication ratio. Extensive experimental results with real-world datasets have shown that RMD is able to achieve high query performance and outperforms several well-known deduplication schemes. © 2017 Elsevier Inc.","Deduplication; Fast index; Fingerprint retrieval; Key value index; Resemblance mergence"
"Software Systems Engineering programmes a capability approach","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008352097&doi=10.1016%2fj.jss.2016.12.016&partnerID=40&md5=64dfa0a11f335daf68a9c59072ef6951","This paper discusses third-level educational programmes that are intended to prepare their graduates for a career building systems in which software plays a major role. Such programmes are modelled on traditional Engineering programmes but have been tailored to applications that depend heavily on software. Rather than describe knowledge that should be taught, we describe capabilities that students should acquire in these programmes. The paper begins with some historical observations about the software development field. © 2016 Elsevier Inc.","Education; Engineering; Information systems; Software design; Software development; Software documentation; Software education"
"A value-oriented approach to business process specialization: Principles, proof-of-concept, and validation","2017","Journal of Systems and Software","10.1016/j.jss.2017.02.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012982813&doi=10.1016%2fj.jss.2017.02.002&partnerID=40&md5=80fff96f0a8f71f221de2f1501a18b48","Organizations build information systems to support their business processes. Precise modeling of an organization's processes is a prerequisite for building information systems that support those processes. Our goal is to help business analysts produce detailed models of the business processes that best reflect the needs of their organizations. To this end, we propose to a) leverage the best practices in terms of a kernel of generic business processes, and b) provide analysts with tools to customize those processes by generating new process variants. We use business patterns from the Resource Event Agent ontology to identify variation points, and to codify the transformations inherent in the generation of the process variants. We developed a prototype process specialization tool using the Eclipse modeling ecosystem. We tested our approach on a set of processes from the Enterprise Resource Planning literature, and a set of variation points to assess the extent to which: 1) the identified variation points made sense, and 2) whether the generated variants made sense, from a business point of view. The results showed that 94.12% of the variation points made sense, and that 80.6% of the generated process variants corresponded to what the business process management specialists expected. © 2017 Elsevier Inc.","Business ontology; Business pattern; Business process; Model transformation; Reuse; Specialization"
"Identification multi-level frequent usage patterns from APIs","2017","Journal of Systems and Software","10.1016/j.jss.2017.05.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019638976&doi=10.1016%2fj.jss.2017.05.039&partnerID=40&md5=6def0b3ae63a9ff80bf9a2d4e67c0fbe","Software developers increasingly rely on application programming interfaces (APIs) of frameworks to increase productivity. An API method is generally used within code snippets along with other methods of the API of interest. When developers invoke API methods in a framework, they often encounter difficulty to determine which methods to call due to the huge number of included methods in that API. Developers usually exploit a source code search tool searching for code snippets that use the API methods of interest. However, the number of returned code snippets is very large which hinders the developer to locate useful ones. Moreover, co-usage relationships between API methods are often not documented. This article presents an approach to identify multi-level frequent usage patterns (IML-FUP) to help developers understand API usage and facilitate the development tasks when they use new APIs. An identified pattern represents a set of API methods that are frequently called together across interfering usage scenarios. In order to investigate the efficiency of the proposed approach, an experimental evaluation is conducted using four APIs and 89 client programs. For all studied APIs, the experimental results show that the proposed approach identifies usage patterns that are always strongly cohesive and highly consistent. © 2017 Elsevier Inc.","API documentation; API usage; Formal concept analysis; Identification; Usage patterns"
"On scaling dynamic programming problems with a multithreaded tabling Prolog system","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003666511&doi=10.1016%2fj.jss.2016.06.060&partnerID=40&md5=0da48ca1f4b1a6928ffa7bda199cf342","Tabling is a powerful implementation technique that improves the declarativeness and expressiveness of traditional Prolog systems in dealing with recursion and redundant computations. It can be viewed as a natural tool to implement dynamic programming problems, where a general recursive strategy divides a problem in simple sub-problems that are often the same. When tabling is combined with multithreading, we have the best of both worlds, since we can exploit the combination of higher declarative semantics with higher procedural control. However, at the engine level, such combination for dynamic programming problems is very difficult to exploit in order to achieve execution scalability as we increase the number of running threads. In this work, we focus on two well-known dynamic programming problems, the Knapsack and the Longest Common Subsequence problems, and we discuss how we were able to scale their execution by using the multithreaded tabling engine of the Yap Prolog system. To the best of our knowledge, this is the first work showing a Prolog system to be able to scale the execution of multithreaded dynamic programming problems. Our experiments also show that our system can achieve comparable or even better speedup results than other parallel implementations of the same problems. © 2016 Elsevier Inc.","Dynamic programming; Multithreading; Prolog; Scalability; Tabling"
"Large scale opinion mining for social, news and blog data","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006368896&doi=10.1016%2fj.jss.2016.06.012&partnerID=40&md5=c5a7de9ef4314480685a1044a72628e7","Companies that collect and analyze data from social media, news and other data streams are faced with several challenges that concern storage and processing of huge amounts of data. When they want to serve the processed information to their customers and moreover, when they want to cover different information needs for each customer, they need solutions that process data in near real time in order to gain insights on the data in motion. The volume and volatility of opinionated data that is published in social media, in combination with the variety of data sources has created a demanding ecosystem for stream processing. Although, there are several solutions that can handle information of static nature and small volume quite efficiently, they usually do not scale up properly because of their high complexity. Moreover, such solutions have been designed to run once or to run in a fixed dataset and they are not sufficient for processing huge volumes of streamed data. To address this problem, a platform for real-time opinion mining is proposed. Based on prior research and real application services that have been developed, a new platform called “PaloPro” is presented to cover the needs for brand monitoring. © 2016 Elsevier Inc.","News streams; Opinion mining; Social media"
"A Formal Approach to implement java exceptions in cooperative systems","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981736041&doi=10.1016%2fj.jss.2016.07.033&partnerID=40&md5=ff70044b316a822095688ade600726fc","The increasing number of systems that work on the top of cooperating elements have required new techniques to control cooperation on both normal and abnormal behaviors of systems. The controllability of the normal behaviors has received more attention because they are concerned with the users expectations, while for the abnormal behaviors it is left to designers and programmers. However, for cooperative systems, the abnormal behaviors, mostly represented by exceptions at programming level, become an important issue in software development because they can affect the overall system behavior. If an exception is raised and not handled accordingly, the system may collapse. To avoid such situation, certain concepts and models have been proposed to coordinate propagation and recovering of exceptional behaviors, including the Coordinated Atomic Actions (CAA). Regardless of the effort in creating these conceptual models, an actual implementation of them in real systems is not very straightforward. This article provides a reliable framework for the implementation of Java exceptions propagation and recovery using CAA concepts. To do this, a Java framework (based on a formal specification) is presented, together with a set of properties to be preserved and proved with the Java Pathfinder (JPF) model checker. In practice, to develop new systems based on the given coordination concepts, designers/programmers can instantiate the framework to implement the exceptional behavior and then verify the correctness of the resulting code using JPF. Therefore, by using the framework, designers/programmers can reuse the provided CAA implementation and instantiate fault-tolerant Java systems. © 2016 Elsevier Inc.","concurrent exception handling; Coordinated atomic actions model; java framework; program verification"
"How to securely outsource the inversion modulo a large composite number","2017","Journal of Systems and Software","10.1016/j.jss.2017.04.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018165153&doi=10.1016%2fj.jss.2017.04.015&partnerID=40&md5=6de0fb052da81ccaadff7c5ed73c65cb","Modular inversion is one of the most basic computations in algorithmic number theory. When it comes to cryptosystems, this computation is very time-consuming since the modulus is generally a large number. It is unrealistic for some devices with limited computation capability (e.g. mobile devices and IC cards) to conduct such a time-consuming computation. In this paper, we investigate how to securely outsource the inversion modulo a large composite number. Based on the Chinese Remainder Theorem (CRT), we design a secure outsourcing algorithm for inversion modulo a large composite number with two known prime factors for the client. Besides the privacy of the number and its modular inversion, our algorithm also protects the privacy of the modulus. We can verify the correctness of the result with probability 1. Traditionally, the complexity of modular inversion for a l-bit modulus is O(l3). By leveraging the cloud, our algorithm reduces the complexity to O(l2) on the client side. Also, we prove the security of our algorithm based on the one-malicious version of two untrusted program model (one-malicious model). We conduct several experiments to demonstrate the validity and the practicality of our proposed algorithm. In appendix, we show that our proposed algorithm can be extended and applied in the secret key generation of RSA algorithm on the resource-constrained devices. © 2017 Elsevier Inc.","Chinese remainder theorem; Cloud computing; Modular inversion; Outsource-secure algorithms"
"Eye gaze and interaction contexts for change tasks – Observations and potential","2017","Journal of Systems and Software","10.1016/j.jss.2016.03.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028242578&doi=10.1016%2fj.jss.2016.03.030&partnerID=40&md5=4cf225c79acfd2a118f83128d3f29c1e","The more we know about software developers’ detailed navigation behavior for change tasks, the better we are able to provide effective tool support. Currently, most empirical studies on developers performing change tasks are, however, limited to very small code snippets or limited by the granularity and detail of the data collected on developer's navigation behavior. In our research, we extend this work by combining user interaction monitoring to gather interaction context – the code elements a developer selects and edits – with eye-tracking to gather more detailed and fine-granular gaze context-code elements a developer looked at. In a study with 12 professional and 10 student developers we gathered interaction and gaze contexts from participants working on three change tasks of an open source system. Based on an analysis of the data we found, amongst other results, that gaze context captures different aspects than interaction context and that developers only read small portions of code elements. We further explore the potential of the more detailed and fine-granular data by examining the use of the captured change task context to predict perceived task difficulty and to provide better and more fine-grained navigation recommendations. We discuss our findings and their implications for better tool support. © 2016 Elsevier Inc.","Empirical study; Eye-tracking; Interactions"
"The technology of processing intensive structured dataflow on a supercomputer","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002818117&doi=10.1016%2fj.jss.2016.07.003&partnerID=40&md5=7067f00650a854dbbb428c9d890e6ce2","Modern experimental setups generate prolonged and intense data streams. For example, non-contact measurement techniques PIV (Particle Image Velocimetry), based on continuous image processing, are widely used in the experimental aerodynamics and hydrodynamics. These experimental stereo or volumetric velocimetry 3D PIV setups are able to generate long-lasting and intensive flows of images by using two or more cameras. High computational complexity of the image processing algorithms is the main limiting factor in the conditions of the low computational performance of PIV setups itself. Removal of these limitations by moving image processing tasks on a remote supercomputer by using our proposed technology “Distributed PIV”, which will allow users to apply their new high-precision parallel algorithms in a real-time and implement feedback to the experimental setup. This paper describes the innovative technology for high-performance processing of the intensive flows of the structured data generated by an experimental setup and delivered through a high-speed DWDM backbone directly into computing nodes of a remote supercomputer. A high BDP (Bandwidth-Delay Product) problem case in the design of protocols such as TCP in respect of performance tuning to achieve maximum network throughput is solved by designed middleware. © 2016 Elsevier Inc.","BDP; Experimental setup; Intense dataflow; Optical network; Queue manager; Supercomputer; TCP"
"A comprehensive framework for cloud computing migration using Meta-synthesis approach","2017","Journal of Systems and Software","10.1016/j.jss.2017.02.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016136266&doi=10.1016%2fj.jss.2017.02.049&partnerID=40&md5=ceb9f0868daca0ba83fe43176b5e582a","Migration to the cloud computing environment is a strategic organizational decision. Using a reliable framework for migration ensures managers to mitigate risks in the cloud computing technology. Therefore, organizations always search for cloud migration frameworks with dynamic nature as well as integrity beside their simplicity. In previous studies, these important features have received less attention and have not been achieved in an integrated and comprehensive way. The aim of this study is to use a meta-synthesis method for the first time for analysis and synthesis of previous published studies and suggests a comprehensive cloud migration framework. We review more than 657 papers from relevant journals and conference proceedings. The concepts which are extracted from these papers are classified to related sub-categories and categories. Then, our proposed framework based on these concepts and categories is developed. It includes seven main phases (categories) and fifteen sub-categories. To improve the migration process a maturity model called “ClM3” is introduced. Finally, proposed framework and maturity model is evaluated by forming different focus group meetings and taking advantages of the cloud experts’ opinion. The results of this research can help managers have a safe and effective migration to cloud computing environment. © 2017 Elsevier Inc.","Cloud computing; Meta-synthesis; Migration framework; Process maturity model"
"Automated extraction of product comparison matrices from informal product descriptions","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996593342&doi=10.1016%2fj.jss.2016.11.018&partnerID=40&md5=6c44aaadfbc3fb4cbc5292ddb3503d98","Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM – despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users. © 2016 Elsevier Inc.","Feature mining; Product comparison matrices; Reverse engineering; Software product lines; Variability mining"
"Observational slicing based on visual semantics","2017","Journal of Systems and Software","10.1016/j.jss.2016.04.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967190374&doi=10.1016%2fj.jss.2016.04.009&partnerID=40&md5=dfe06d03c3c294c83ef2fff1e7300cce","Program slicing has seen a plethora of applications and variations since its introduction over 35 years ago. The dominant method for computing slices involves significant complex source-code analysis to model the dependencies in the code. A recently introduced alternative, observation-based slicing, sidesteps this complexity by observing the behavior of candidate slices. Observation-based slicing has several other strengths, including the ability to easily slice multi-language systems. However, the initial implementation of observation-based slicing, ORBS, remains rooted in tradition as it captures semantics by comparing sequences of values. This raises the question of whether it is possible to extend slicing beyond its traditional semantic roots. A few existing projects have attempted this but the extension requires considerable effort. If it is possible to build on the ORBS platform to more easily generalize slicing to languages with non-traditional semantics, then there is the potential to vastly increase the range of programming languages to which slicing can be applied. ORBS supports this by reducing the problem to that of generalizing how semantics are captured. Taking Picture Description Languages as a case study, the challenges and effectiveness of such a generalization are considered. The results show that not only is it possible to generalize the ORBS implementation, but the resulting slicer is quite effective, removing from 8% to 98% of the original source code with an average of 83%. Finally a qualitative look at the slices finds the technique very effective, at times producing minimal slices. © 2016 Elsevier Inc.","Non-traditional semantics; Observation"
"Energy minimization for on-line real-time scheduling with reliability awareness","2017","Journal of Systems and Software","10.1016/j.jss.2017.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013230578&doi=10.1016%2fj.jss.2017.02.004&partnerID=40&md5=69f97c32e20896c74f0d1083000a8fe7","Under current development of semiconductor technology, there is an exponential increase in transistor density on a single processing chip. This aggressive transistor integration significantly boosts the computing performance. However, it also results in a power explosion, which immediately decreases the system reliability. Moreover, some well-known power/energy reduction techniques, i.e. Dynamic Voltage and Frequency Scaling (DVFS), can cause adverse impact on system reliability. How to effectively manage the power/energy consumption, meanwhile keep the system reliability under control, is critical for the design of high performance computing systems. In this paper, we present an online power management approach to minimize the energy consumption for single processor real-time scheduling under reliability constraint. We formally prove that the proposed algorithm can guarantee the system reliability requirement. Our simulation results show that, by exploiting the run-time dynamics, the proposed approach can achieve more energy savings over previous work under reliability constraint. © 2017 Elsevier Inc.","Energy minimization; Multi-core systems; Real-time scheduling; Reliability"
"Software component and the semantic Web: An in-depth content analysis and integration history","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001052447&doi=10.1016%2fj.jss.2016.11.028&partnerID=40&md5=29be73e9f47cfce3b483636d0f0d1a7d","With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed. © 2016 Elsevier Inc.","Component-based software engineering; Linked Data; Ontology; Reasoners; Semantic Web; Web services"
"Using contextual information to predict co-changes","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028275894&doi=10.1016%2fj.jss.2016.07.016&partnerID=40&md5=44d236574257265814b1b03bf35ca257","Background: Co-change prediction makes developers aware of which artifacts will change together with the artifact they are working on. In the past, researchers relied on structural analysis to build prediction models. More recently, hybrid approaches relying on historical information and textual analysis have been proposed. Despite the advances in the area, software developers still do not use these approaches widely, presumably because of the number of false recommendations. We conjecture that the contextual information of software changes collected from issues, developers' communication, and commit metadata captures the change patterns of software artifacts and can improve the prediction models. Objective: Our goal is to develop more accurate co-change prediction models by using contextual information from software changes. Method: We selected pairs of files based on relevant association rules and built a prediction model for each pair relying on their associated contextual information. We evaluated our approach on two open source projects, namely Apache CXF and Derby. Besides calculating model accuracy metrics, we also performed a feature selection analysis to identify the best predictors when characterizing co-changes and to reduce overfitting. Results: Our models presented low rates of false negatives (∼8% average rate) and false positives (∼11% average rate). We obtained prediction models with AUC values ranging from 0.89 to 1.00 and our models outperformed association rules, our baseline model, when we compared their precision values. Commit-related metrics were the most frequently selected ones for both projects. On average, 6 out of 23 metrics were necessary to build the classifiers. Conclusions: Prediction models based on contextual information from software changes are accurate and, consequently, they can be used to support software maintenance and evolution, warning developers when they miss relevant artifacts while performing a software change. © 2016 Elsevier Inc.","Change coupling; Change impact analysis; Change propagation; Co-change prediction; Contextual information; Software change context"
"A systematic study of double auction mechanisms in cloud computing","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006760739&doi=10.1016%2fj.jss.2016.12.009&partnerID=40&md5=801a295fa81e5f9f30bcb64d1d37fb79","The cloud system is designed, implemented and conceptualized as a marketplace where resources are traded. This demands efficient allocation of resources to benefit both the cloud users and the cloud service providers. Accordingly, market based resource allocation models for cloud computing have been proposed. These models apply economy based approaches e.g. auction, negotiation etc. This work makes a detailed study of the double auction mechanisms and their applicability for the cloud markets. A framework for a future cloud market using double auction is also proposed. As most of the existing works in double auction confines only resource allocation, therefore, a Truthful Multi-Unit Double Auction mechanism (TMDA) is proposed that would help researchers to understand how a truthful double auction mechanism can be designed. TMDA is proven to be asymptotically efficient, individual rational, truthful and budget-balanced. TMDA would also encourage researchers to contribute in this emerging area. The performance of TMDA, which addresses the interests of both the cloud user and the provider, has been validated through simulation study. Various challenges in the realization of double auction mechanisms in cloud computing along-with the future possibilities are also presented. © 2016 Elsevier Inc.","Cloud computing; Double auction; Mechanism design; Quality of Service (QoS); Resource allocation"
"A new structure and access mechanism for secure and efficient XML data broadcast in mobile wireless networks","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000786807&doi=10.1016%2fj.jss.2016.11.036&partnerID=40&md5=55815514d8ec38c3e059b6f98c325034","Recently, the use of XML for data broadcasting in mobile wireless networks has gained many attentions. One of the most essential requirements for such networks is data confidentiality. In order to secure XML data broadcast in mobile wireless networks, mobile clients should obey a set of access authorizations specified on the original XML document. In such environments, mobile clients can only access authorized parts of encrypted XML stream based on their access authorizations. Several indexing methods have been proposed in order to have selective access to XML data over the XML stream. However, these indexing methods cannot be used for encrypted XML data. In this paper, we define a new structure for XML stream which supports data confidentiality of XML data over the wireless broadcast channel. We also define an access mechanism for our proposed structure to efficiently process XML queries over the encrypted XML stream. The experimental results demonstrate that the use of our proposed structure and access mechanism for XML data broadcast efficiently disseminates XML data in mobile wireless networks. © 2016 Elsevier Inc.","Secure XML data broadcast; XML access control; XML query processing; XML stream"
"Adopters’ trust in enterprise open source vendors: An empirical examination","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007102894&doi=10.1016%2fj.jss.2016.12.006&partnerID=40&md5=ab3a1b14f7b293b73f5ab9c67d415642","Although significant research attention has been directed at understanding open source software (OSS) adoption, very little attention has been paid to understanding what leads potential adopters to trust enterprise open source vendors. This study identifies organizational trust factors in enterprise open source vendors, namely vendors’ security, embracement of open standards, and support services. It also examines the impact of system trust on adopters’ attitudes and intentions. The study draws upon a total of 192 questionnaires collected from enterprise IT and project managers. Our results show that trust factors have a positive effect on system trust. We also found system trust to be effective in increasing adopters’ attitudes and intentions. Finally, our results provide several managerial implications for organizations as well as enterprise open source vendors. © 2016 Elsevier Inc.","Adoption intent; Enterprise open source software vendor; Organizational adoption; System trust"
"Source code metrics: A systematic mapping study","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017511513&doi=10.1016%2fj.jss.2017.03.044&partnerID=40&md5=7fab941e657739597d1f3abd20fa9523","Context Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. Objectives This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. Method A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. Results Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. Conclusions Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends. © 2017 Elsevier Inc.","Aspect-oriented metrics; Feature-oriented metrics; Object-oriented metrics; Software metrics; Source code metrics; Systematic mapping study"
"A multivariate and quantitative model for predicting cross-application interference in virtual environments","2017","Journal of Systems and Software","10.1016/j.jss.2017.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017384564&doi=10.1016%2fj.jss.2017.04.001&partnerID=40&md5=9604b10c54469890f89735795f272cdb","Cross-application interference can drastically affect performance of HPC applications executed in clouds. The problem is caused by concurrent access of co-located applications to shared resources such as cache and main memory. Several works of the related literature have considered general characteristics of HPC applications or the total amount of SLLC accesses to determine the cross-application interference. However, our experiments showed that the cross-application interference problem is related to the amount of simultaneous access to several shared resources, revealing its multivariate and quantitative nature. Thus, in this work we propose a multivariate and quantitative model able to predict cross-application interference level that considers the amount of concurrent accesses to SLLC, DRAM and virtual network, and the similarity between the amount of those accesses. An experimental analysis of our prediction model by using a real reservoir petroleum simulator and applications from a well-known HPC benchmark showed that our model could estimate the interference, reaching an average and maximum prediction errors around 4% and 12%, and achieving errors less than 10% in approximately 96% of all tested cases. © 2017 Elsevier Inc.","Cloud computing; Cross-application interference; High Performance Computing; Virtual Machine Placement"
"A mapping study on design-time quality attributes and metrics","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011850905&doi=10.1016%2fj.jss.2017.01.026&partnerID=40&md5=a072a29a4129d182e5809f63b86dc918","Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners. © 2017 Elsevier Inc.","Design-time quality attributes; Mapping study; Measurement; Software quality"
"Designing and applying an approach to software architecting in agile projects in education","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011930962&doi=10.1016%2fj.jss.2017.01.029&partnerID=40&md5=c21828eb0da2b567c1d68971230b4176","Software architecting activities are not discussed in most agile software development methods. That is why, the combination of software architecting and agile methods has been in the focus of numerous publications. However, there is little literature on how to approach software architecting in agile projects in education. In this paper, we present our approach to the introduction of software architecting activities in an agile project course. The approach is based on literature sources and is tailored to fit our educational goals and context. The approach has been applied in two consecutive executions of the course. We observe improved understanding on the value of architecting activities and appreciation among students on the combination of architecting activities and agile development. We applied the approach predominantly in cases with an architecturally savvy Product Owner. Further research is required to understand how the approach performs in scenarios with architecturally unsavvy Product Owners and if it needs to be adapted for these scenarios. We also conclude that more research is needed on the challenges that architects face in agile projects in order to better prepare students for practice. © 2017 Elsevier Inc.","Agile; Course; Project; Scrum; Software architecture; Software engineering education; Students; Teaching"
"A hybrid and learning agent architecture for network intrusion detection","2017","Journal of Systems and Software","10.1016/j.jss.2017.01.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013212230&doi=10.1016%2fj.jss.2017.01.028&partnerID=40&md5=1155499bf3bf32e4845c6237bc07ed45","Learning is an effective way for automating the adaptation of systems to their environment. This ability is especially relevant in dynamic environments as computer networks where new intrusions are constantly emerging, most of them having similarities and occurring frequently. Traditional intrusion detection systems still have limitations of adaptability because they are just able to detect intrusions previously set in system design. This paper proposes HyLAA a software agent architecture that combines case-based reasoning, reactive behavior and learning. Through its learning mechanism, HyLAA can adapt itself to its environment and identify new intrusions not previously specified in system design. This is done by learning new reactive rules by observing recurrent good solutions to the same perception from the case-based reasoning system, which will be stored in the agent knowledge base. The effectiveness of HyLAA to detect intrusions using case-based reasoning behavior, the accuracy of the classifier learned by the learning component and both the performance and effectiveness of HyLAA to detect intrusions using hybrid behavior with learning and without learning were evaluated, respectively, by conducting four experiments. In the first experiment, HyLAA exhibited good effectiveness to detect intrusions. In the second experiment the classifiers learned by the learning component presented high accuracy. Both the hybrid agent behavior with learning and without learning (third and fourth experiment, respectively) presented greater effectiveness and a balance between performance and effectiveness, but only the hybrid behavior showed better effectiveness and performance as long as the agent learns. © 2017 Elsevier Inc.","Case-based reasoning; Hybrid agents; Information security; Intrusion detection systems; Learning agents; Ontologies"
"Reusing business components and objects for modeling business systems: The influence of decomposition characteristics and analyst experience","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997610070&doi=10.1016%2fj.jss.2016.07.036&partnerID=40&md5=4aabaa933de3b79493841480853ba642","Component-based development (CBD) relies on the use of pre-fabricated business components to develop new application systems, rather than developing them from scratch. It provides an attractive alternative to more established development methods such as object-oriented analysis and design (OOAD). Given the growing demands for using agile methods for software development, we examine if systems analysts are more effective at modeling business systems by reusing business components than by reusing objects. We also examine whether the influence of component or object reuse on modeling performance is moderated by prior experience in systems analysis and design. We evaluate the representational constructs of the two based on a set of decomposition characteristics, and postulate hypotheses comparing the two based on theories in cognitive psychology and human factors. We find that models generated by reusing business components are of higher accuracy than those developed by reusing objects. An interesting finding of our study is that IT professionals who are less experienced in systems analysis and design perform on par with experienced professionals, when modeling business systems by reusing components. We argue that the decomposition characteristics of components—with respect to granularity, quality, and focus—enable less experienced analysts to perform on par with more experienced analysts. © 2016 Elsevier Inc.","Analyst experience; Chunking; Cognitive fit; Conceptual modeling; Software reuse; Systems development"
"Assignment strategies for ground truths in the crowdsourcing of labeling tasks","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002870388&doi=10.1016%2fj.jss.2016.06.061&partnerID=40&md5=0dcd6f85df4df57515e594eaeb785a1d","It is expected that ground truths can result in many good labels in the crowdsourcing of labeling tasks. However, the use of ground truths has so far not been adequately addressed. In this paper, we develop algorithms that determine the number of ground truths that are necessary. We determine this number by iteratively calculating the expected quality of labels for tasks with various sets of ground truths, and then comparing the quality with the limit of the estimated label quality expected to be obtained by crowdsourcing. We assume that each worker has a different unknown labeling ability and performs a different number of tasks. Under this assumption, we develop assignment strategies for ground truths based on the estimated confidence intervals of the workers. Our algorithms can utilize different approaches based on the expectation maximization to estimate good-quality consensus labels. An experimental evaluation demonstrates that our algorithms work well in various situations. © 2016 Elsevier Inc.","Condorcet jury theorem; Confidence interval; Expectation maximization algorithm; Human computation"
"Creating an invalid defect classification model using text mining on server development","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003827397&doi=10.1016%2fj.jss.2016.12.005&partnerID=40&md5=1d512e30cf5d0e1cef442cd066a12dd1","Invalid defects, which are often overlooked, reduce development productivity and efficiency. This study used exploratory study and text mining to answer three research questions related to invalid defects in two research stages. In the first stage, we filtered 231 invalid BIOS (basic input/output system) defects from the 3347 defects of three server projects. These defects were from numerous function areas owned by virtual teams located in Taiwan, China, and the United States. Results indicated that BIOS firmware demonstrates the maximum number of defects and invalid defects. This firmware accounted for 43.3% defects and 33% invalid defects in server development. Results determined that invalid defect classification that includes four types, namely, working as designed (WAD), user error, duplicate, and others. All of these types can be grouped under the term WUDO. WAD accounts for the maximum of 45% of invalid defects in the WUDO classification. In the second stage, this study determined a stable classification algorithm, namely, decision tree C4.5, to classify the invalid defect types. This study helps project teams for information technology products to classify the different invalid defect types that developers and testers face. Results can improve project team productivity and mitigate project risks in project management. © 2016 Elsevier Inc.","BIOS; Classification; Invalid defect; Project management; Server development; Text mining"
"How developers micro-optimize Android apps","2017","Journal of Systems and Software","10.1016/j.jss.2017.04.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019184864&doi=10.1016%2fj.jss.2017.04.018&partnerID=40&md5=c6dec21b5331a73ff23b112d7aaadb55","Optimizing mobile apps early on in the development cycle is supposed to be a key strategy for obtaining higher user rankings, more downloads, and higher retention. In fact, mobile platform designers publish specific guidelines, and tools aimed at optimizing apps. However, little research has been done with respect to identifying and understanding actual optimization practices performed by developers. In this paper, we present the results of three empirical studies aimed at investigating practices of Android developers towards improving the performance of their apps, by means of micro-optimizations. We mined change histories of 3513 apps to identify the most frequent micro-optimization opportunities in 297K+ snapshots and to understand if (and when) developers implement these optimizations. Then, we performed an in-depth analysis into whether implementing micro-optimizations can help reduce memory/CPU usage. Finally, we conducted a survey with 389 open-source developers to understand how they use micro-optimizations to improve the performance of Android apps. Surprisingly, our results indicate that developers rarely implement micro-optimizations. Also, the impact of the analyzed micro-optimization on CPU/memory consumption is negligible in most of the cases. Finally, the results from the survey shed some light into why this happens as well as upon which practices developers rely upon. © 2017 Elsevier Inc.","Android; Empirical studies; Measurement; Mining software repositories; Optimizations"
"A tool to support the definition and enactment of model-driven migration processes","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016268711&doi=10.1016%2fj.jss.2017.03.009&partnerID=40&md5=f2de1a9af820434207431fc7790d31df","One of the main challenges to achieve the industrial adoption of Model-Driven Engineering (MDE) paradigm is building tools able to support model-driven software processes. We present a tool for the definition and enactment of model-driven migration processes. We have created a SPEM-based language for defining Abstract Migration models that represent an MDE migration solution for a particular pair of source and target technologies. For each legacy application to be migrated, the Abstract Migration model is transformed into a Concrete Migration model which contains all the information needed for the enactment. Then, these models are enacted by means of a process interpreter which generates Trac tickets for executing automated tasks by means of Ant scripts and managing manual tasks with the Mylyn tool. Our work has therefore two main contributions: i) it proposes a novel solution for the enactment that integrates the execution of the automated tasks with the generation of tickets to support the manual tasks, and ii) it describes how MDE techniques can be used to implement process engineering tools, in particular migration processes. The article presents the approach and describes in detail the essential aspects of our tool. © 2017 Elsevier Inc.","Model-driven engineering; Process enactment; Software migrations; Software processes"
"Nebo: An efficient, parallel, and portable domain-specific language for numerically solving partial differential equations","2017","Journal of Systems and Software","10.1016/j.jss.2016.01.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958073935&doi=10.1016%2fj.jss.2016.01.023&partnerID=40&md5=e5622879bc9c2ee8b56c399c081af228","This paper presents Nebo, a declarative domain-specific language embedded in C++ for discretizing partial differential equations for transport phenomena on multiple architectures. Application programmers use Nebo to write code that appears sequential but can be run in parallel, without editing the code. Currently Nebo supports single-thread execution, multi-thread execution, and many-core (GPU-based) execution. With single-thread execution, Nebo performs on par with code written by domain experts. With multi-thread execution, Nebo can linearly scale (with roughly 90% efficiency) up to 12 cores, compared to its single-thread execution. Moreover, Nebo's many-core execution can be over 140x faster than its single-thread execution. © 2016 Elsevier Inc.","Domain-specific language embedded in C++; GPGPU"
"Self-adaptive processing graph with operator fission for elastic stream processing","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006516228&doi=10.1016%2fj.jss.2016.06.010&partnerID=40&md5=ca8ba578ce9c3972639a7e4311b5e832","Nowadays, information generated by the Internet interactions is growing exponentially, creating massive and continuous flows of events from the most diverse sources. These interactions contain valuable information for domains such as government, commerce, and banks, among others. Extracting information in near real-time from such data requires powerful processing tools to cope with the high-velocity and the high-volume stream of events. Specially designed distributed processing engines build a graph-based topology of a static number of processing operators creating bottlenecks and load balance problems when processing dynamic flows of events. In this work we propose a self-adaptive processing graph that provides elasticity and scalability by automatically increasing or decreasing the number of processing operators to improve performance and resource utilization of the system. Our solution uses a model that monitors, analyzes and changes the graph topology with a control algorithm that is both reactive and proactive to the flow of events. We have evaluated our solution with three stream processing applications and results show that our model can adapt the graph topology when receiving events at high rate with sudden peaks, producing very low costs of memory and CPU usage. © 2016 Elsevier Inc.","Elastic processing; S4; Scalable processing; Self-adaptable graph; Stream processing"
"A method to generate reusable safety case argument-fragments from compositional safety analysis","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997796738&doi=10.1016%2fj.jss.2016.07.034&partnerID=40&md5=4c8e4549f42d55c2d6c3752a17e946a8","Safety-critical systems usually need to be accompanied by an explained and well-founded body of evidence to show that the system is acceptably safe. While reuse within such systems covers mainly code, reusing accompanying safety artefacts is limited due to a wide range of context dependencies that need to be satisfied for safety evidence to be valid in a different context. Currently, the most commonly used approaches that facilitate reuse lack support for systematic reuse of safety artefacts. To facilitate systematic reuse of safety artefacts we provide a method to generate reusable safety case argument-fragments that include supporting evidence related to compositional safety analysis. The generation is performed from safety contracts that capture safety-relevant behaviour of components in assumption/guarantee pairs backed up by the supporting evidence. We evaluate the feasibility of our approach in a real-world case study where a safety related component developed in isolation is reused within a wheel-loader. © 2016 Elsevier Inc.","Component-based architectures; Compositional safety analysis; Contract-based architectures; Modular argumentation; Safety argumentation reuse"
"deExploit: Identifying misuses of input data to diagnose memory-corruption exploits at the binary level","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997693939&doi=10.1016%2fj.jss.2016.11.026&partnerID=40&md5=11d60515ab1fae4a7a08fbd97b6cc869","Memory-corruption exploits are one of the major threats to the Internet security. Once an exploit has been detected, exploit diagnosis techniques can be used to identify the unknown vulnerability and attack vector. In the security landscape, exploit diagnosis is always performed by third-party security experts who cannot access the source code. This makes binary-level exploit diagnosis a time-consuming and error-prone process. Despite considerable efforts to defend against exploits, automatic exploit diagnosis remains a significant challenge. In this paper, we propose a novel insight for detecting memory corruption at the binary level by identifying the misuses of input data and present an exploit diagnosis approach called deExploit. Our approach requires no knowledge of the source code or debugging information. For exploit diagnosis, deExploit is generic in terms of the detection of both control-flow-hijack and data-oriented exploits. In addition, deExploit automatically provides precise information regarding the corruption point, the memory operation that causes the corruption, and the key attack steps used to bypass existing defense mechanisms. We implement deExploit and perform it to diagnose multiple realistic exploits. The results show that deExploit is able to diagnose memory-corruption exploits. © 2016 Elsevier Inc.","Exploit diagnosis; Memory corruption; Reverse engineering; Software vulnerability"
"Logical query optimization for Cloudera Impala system","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000351129&doi=10.1016%2fj.jss.2016.11.038&partnerID=40&md5=6d3396c9da3abe1b68bdfac817b9084c","Cloudera Impala, an analytic database system for Apache Hadoop, has a severe problem with query plan generation: the system can only generate query plans in left-deep tree form, which restricts the ability of parallel execution. In this paper, we present a logical query optimization scheme for Impala system. First, an improved McCHyp (MinCutConservative Hypergraph) logical query plan generation algorithm is proposed for Impala system. It can reduce the plan generation time by introducing a pruning strategy. Second, a new cost model that takes the characteristics of Impala system into account is proposed. Finally, Impala system is extended to support query plans in bushy tree form by integrating the plan generation algorithm. We evaluated our scheme using TPC-DS test suit. Experimental results show that the extended Impala system generally performs better than the original system, and the improved plan generation algorithm has less execution time than McCHyp. In addition, our cost model fits better for Impala system, which supports query plans in bushy tree form. © 2016","Bushy tree; Cloudera Impala system; Cost model; Logical query"
"An efficient spark-based adaptive windowing for entity matching","2017","Journal of Systems and Software","10.1016/j.jss.2017.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015694094&doi=10.1016%2fj.jss.2017.03.003&partnerID=40&md5=1492946d6e309de24f4db3c8426a6bcf","Entity Matching (EM), i.e., the task of identifying records that refer to the same entity, is a fundamental problem in every information integration and data cleansing system, e.g., to find similar product descriptions in databases. The EM task is known to be challenging when the datasets involved in the matching process have a high volume due to its pair-wise nature. For this reason, studies about challenges and possible solutions of how EM can benefit from modern parallel computing programming models, such as Apache Spark (Spark), have become an important demand nowadays (Christen, 2012a; Kolb et al., 2012b). The effectiveness and scalability of Spark-based implementations for EM depend on how well the workload distribution is balanced among all workers. In this article, we investigate how Spark can be used to perform efficiently (load balanced) parallel EM using a variation of the Sorted Neighborhood Method (SNM) that uses a varying (adaptive) window size. We propose Spark Duplicate Count Strategy (S-DCS++), a Spark-based approach for adaptive SNM, aiming to increase even more the performance of this method. The evaluation results, based on real-world datasets and cluster infrastructure, show that our approach increases the performance of parallel DCS++ regarding the EM execution time. © 2017 Elsevier Inc.","Adaptive windowing; Entity matching; Load balancing; Spark"
"A study of value in agile software development organizations","2017","Journal of Systems and Software","10.1016/j.jss.2016.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007247171&doi=10.1016%2fj.jss.2016.12.007&partnerID=40&md5=2741e93188ebd9f3a2912fc1c7ba8201","The Agile manifesto focuses on the delivery of valuable software. In Lean, the principles emphasise value, where every activity that does not add value is seen as waste. Despite the strong focus on value, and that the primary critical success factor for software intensive product development lies in the value domain, no empirical study has investigated specifically what value is. This paper presents an empirical study that investigates how value is interpreted and prioritised, and how value is assured and measured. Data was collected through semi-structured interviews with 23 participants from 14 agile software development organisations. The contribution of this study is fourfold. First, it examines how value is perceived amongst agile software development organisations. Second, it compares the perceptions and priorities of the perceived values by domains and roles. Third, it includes an examination of what practices are used to achieve value in industry, and what hinders the achievement of value. Fourth, it characterises what measurements are used to assure, and evaluate value-creation activities. © 2016 Elsevier Inc.","Agile software development; Empirical; Value"
"Exploiting traceability uncertainty between software architectural models and extra-functional results","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85000352336&doi=10.1016%2fj.jss.2016.11.032&partnerID=40&md5=89e6ab4e38c1ff39692ffd2de534cc29","Deriving extra-functional properties (e.g., performance, security, reliability) from software architectural models is the cornerstone of software development as it supports the designers with quantitative predictions of system qualities. However, the problem of interpreting results from quantitative analysis of extra-functional properties is still challenging because it is hard to understand how the analysis results (e.g., response time, data confidentiality, mean time to failure) trace back to the architectural model elements (i.e., software components, interactions among components, deployment nodes). The goal of this paper is to automate the traceability between software architectural models and extra-functional results, such as performance and security, by investigating the uncertainty while bridging these two domains. Our approach makes use of extra-functional patterns and antipatterns, such as performance antipatterns and security patterns, to deduce the logical consequences between the architectural elements and analysis results and automatically build a graph of traces, thus to identify the most critical causes of extra-functional flaws. We developed a tool that jointly considers SOftware and Extra-Functional concepts (SoEfTraceAnalyzer), and it automatically builds model-to-results traceability links. This paper demonstrates the effectiveness of our automated and tool supported approach on three case studies, i.e., two academic research projects and one industrial system. © 2016 Elsevier Inc.","Extra-functional results; Software modeling; Traceability; Uncertainty"
"DRank: A semi-automated requirements prioritization method based on preferences and dependencies","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006086282&doi=10.1016%2fj.jss.2016.09.043&partnerID=40&md5=48ab4722cb1a18662a1b3b0fb0608591","There are many types of dependencies between software requirements, such as the contributions dependencies (Make, Some+, Help, Break, Some-, Hurt) and business dependencies modeled in the i* framework. However, current approaches for prioritizing requirements seldom take these dependencies into consideration, because it is difficult for stakeholders to prioritize requirements considering their preferences as well as the dependencies between requirements. To make requirement prioritization more practical, a method called DRank is proposed. DRank has the following advantages: 1) a prioritization evaluation attributes tree is constructed to make the ranking criteria selection easier and more operable; 2) RankBoost is employed to calculate the subjective requirements prioritization according to stakeholder preferences, which reduces the difficulty of evaluating the prioritization; 3) an algorithm based on the weighted PageRank is proposed to analyze the dependencies between requirements, allowing the objective dependencies to be automatically transformed into partial order relations; and 4) an integrated requirements prioritization method is developed to amend the stakeholders’ subjective preferences with the objective requirements dependencies and make the process of prioritization more reasonable and applicable. A controlled experiment performed to validate the effectiveness of DRank based on comparisons with Case Based Ranking, Analytical Hierarchy Process, and EVOLVE. The results demonstrate that DRank is less time-consuming and more effective than alternative approaches. A simulation experiment demonstrates that taking requirement dependencies into consideration can improve the accuracy of the final prioritization sequence. © 2016","Link analysis; Machine learning; Requirements dependency; Software requirements prioritization"
"Exploring quality measures for the evaluation of feature models: a case study","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997701970&doi=10.1016%2fj.jss.2016.07.040&partnerID=40&md5=01aaca45d5a909e5b66a04df062a8689","Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman's rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models. © 2016 Elsevier Inc.","Feature models; Measures; Quality evaluation; Software product line"
"HB2DS: A behavior-driven high-bandwidth network mining system","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002530328&doi=10.1016%2fj.jss.2016.07.004&partnerID=40&md5=ff1ba3e9ec5dc71334841275e15452eb","This paper proposes a behavior detection system, HB2DS, to address the behavior-detection challenges in high-bandwidth networks. In HB2DS, a summarization of network traffic is represented through some meta-events. The relationships amongst meta-events are used to mine end-user behaviors. HB2DS satisfies the main constraints exist in analyzing of high-bandwidth networks, namely online learning and outlier handling, as well as one-pass processing, delay, and memory limitations. Our evaluation indicates significant improvement in big data stream analyzing in terms of accuracy and efficiency. © 2016 Elsevier Inc.","Behavior detection; big data stream; Data stream clustering; Network analysis"
"Robust and reliable reconfiguration of cloud applications","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944089240&doi=10.1016%2fj.jss.2015.09.020&partnerID=40&md5=9c60d6da4dc1968b01d90f5b03269767","Cloud applications involve a set of interconnected software components running on remote virtual machines. The deployment and dynamic reconfiguration of cloud applications, involving the addition/removal of virtual machines and components hosted on these virtual machines, are error-prone tasks. They must preserve the application consistency and respect important architectural invariants related to software dependencies. In this paper, we introduce a protocol for automating these reconfiguration tasks. In order to ensure its correctness and robustness, we implement the protocol with the support of the Maude system for rapid prototyping purposes, and we verify it with its formal analysis tools. © 2015 Elsevier Inc.","Cloud computing; Dynamic reconfiguration; Model checking; Rewriting logic"
"A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961161360&doi=10.1016%2fj.jss.2016.02.026&partnerID=40&md5=56ae996865c605b37682d5762c2dc862","The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized PLA design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in PLA design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential PLA designs), with higher feature-based cohesion, and less feature scattering and tangling. © 2016 Elsevier Inc.","Crossover operator; Empirical study; Multi-objective genetic algorithm; Product line architecture design"
"The RIGHT model for Continuous Experimentation","2017","Journal of Systems and Software","10.1016/j.jss.2016.03.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963563079&doi=10.1016%2fj.jss.2016.03.034&partnerID=40&md5=e26761c76431b0b73a189c16061663e7","Context: Development of software-intensive products and services increasingly occurs by continuously deploying product or service increments, such as new features and enhancements, to customers. Product and service developers must continuously find out what customers want by direct customer feedback and usage behaviour observation. Objective: This paper examines the preconditions for setting up an experimentation system for continuous customer experiments. It describes the RIGHT model for Continuous Experimentation (Rapid Iterative value creation Gained through High-frequency Testing), illustrating the building blocks required for such a system. Method: An initial model for continuous experimentation is analytically derived from prior work. The model is matched against empirical case study findings from two startup companies and further developed. Results: Building blocks for a continuous experimentation system and infrastructure are presented. Conclusions: A suitable experimentation system requires at least the ability to release minimum viable products or features with suitable instrumentation, design and manage experiment plans, link experiment results with a product roadmap, and manage a flexible business strategy. The main challenges are proper, rapid design of experiments, advanced instrumentation of software to collect, analyse, and store relevant data, and the integration of experiment results in both the product development cycle and the software development process. © 2016 Elsevier Inc.","Agile software development; Continuous experimentation; Lean software development; Product development; Software architecture; Software development process"
"Optimal control based regression test selection for service-oriented workflow applications","2017","Journal of Systems and Software","10.1016/j.jss.2016.06.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977108920&doi=10.1016%2fj.jss.2016.06.065&partnerID=40&md5=42f5a7542222382b978ce0e22bcec3ee","Regression test selection, which is well known as an effective technology to ensure the quality of modified BPEL applications, is regarded as an optimal control issue. The BPEL applications under test serves as a controlled object and the regression test selection strategy functions as the corresponding controller. The performance index is to select fewest test cases to test modified BPEL applications. In addition, a promising controller (regression test selection approach) should be safe, which means that it can select all test cases in which faults might be exposed in modified versions under controlled regression testing from the original test suite. However, existing safe controllers may rerun some test cases without exposing fault. In addition, the unique features (e.g., dead path elimination semantics, communication mechanism, multi-assignment etc.) of BPEL applications also raise enormous problems in regression test selection. To address these issues, we present in this paper a safe optimal controller for BPEL applications. Firstly, to handle the unique features mentioned above, we transform BPEL applications and their modified versions into universal BPEL forms. Secondly, For our optimal controller, BPEL program dependence graphs corresponding to the two universal BPEL forms are established. Finally, guided by behavioral differences between the two versions, we construct an optimal controller and select test cases to be rerun. By contrast with the previous approaches, our approach can eliminate some unnecessary test cases to be selected. We conducted experiments with 8 BPEL applications to compare our approach with other typical approaches. Experimental results show that the test cases selected using our approach are fewer than other approaches. © 2016 Elsevier Inc.","Behavioral difference; BPEL program dependence graph; Optimal control; Regression test selection; Safe; Service-oriented workflow applications; Software cybernetics"
"Multi-objective test case prioritization in highly configurable systems: A case study","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990208605&doi=10.1016%2fj.jss.2016.09.045&partnerID=40&md5=797c93a9c4dc79cce7d447025ccb228f","Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non–functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization. © 2016 Elsevier Inc.","Automated software testing; Highly-configurable systems; Test case prioritization; Variability"
"Incorporating architecture-based self-adaptation into an adaptive industrial software system","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992679340&doi=10.1016%2fj.jss.2015.09.021&partnerID=40&md5=286495d9c74007ccff1b50de1e2f6a7d","Complex software-intensive systems are increasingly relied upon for all kinds of activities in society, leading to the requirement that these systems should be resilient to changes that may occur to the system, its environment, or its goals. Traditionally, resilience has been achieved either through: (i) low-level mechanisms embedded in the implementation (e.g., exception handling, timeouts, redundancies), which are unable to detect subtle but important anomalies (e.g., progressive performance degradation); or (ii) human oversight, which is costly and unreliable. Architecture-based self-adaptation (ABSA) is regarded as a promising approach to improve the resilience and reduce the development/operation costs of such systems. Although researchers have illustrated the benefits of ABSA through a number of small-scale case studies, it remains to be seen whether ABSA is truly effective in handling changes at run-time in industrial-scale systems. In this paper, we report on our experience applying an ABSA framework (Rainbow) to a large-scale commercial software system, called Data Acquisition and Control Service (DCAS), which is used to monitor and manage highly populated networks of devices in renewable energy production plants. In the approach followed, we have replaced some of the existing adaptive mechanisms embedded in DCAS by those advocated by ABSA proponents. This has allowed us to assess the development costs associated with the reengineering of adaptive mechanisms when using an ABSA solution, and to make effective comparisons, in terms of operational performance, between a baseline industrial system and one that uses ABSA. Our results show that using the ABSA concepts as embodied in Rainbow enabled an independent team of developers to: (i) effectively implement the adaptation behavior required from such industrial systems; and (ii) obtain important benefits in terms of maintainability and extensibility of adaptation mechanisms. © 2015 Elsevier Inc.","Architecture-based self-adaptation; Evolution; Rainbow"
"Model-based M2M transformations based on drag-and-drop actions: Approach and implementation","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990879603&doi=10.1016%2fj.jss.2016.09.046&partnerID=40&md5=16ac780690c061ce102948ce987726de","In the context of model-driven development, model-to-model (M2M) transformations are often positioned as one of the key selling features of this ever-growing paradigm. Indeed, M2M transformations can speed up the development process by automating certain modeling tasks, enable reusability of existing models within a single or even multiple projects, and bring other actual benefits to a systems developer. Nevertheless, CASE tool-supported M2M transformations are quite often represented as hard-coded “black-box” solutions lacking flexibility and customization features. This paper presents main conceptual and implementation aspects of a practical approach for both the development and application of model-based, customizable M2M transformations. The transformation is triggered by so called drag-and-drop action, which is enacted after a certain element is dragged from a model browser and dropped into a diagram or onto some other element representation in the diagram. Another distinctive feature of the presented approach is the introduction of “partial M2M transformation”. The term assumes a transformation of a user-defined fragment of the source model into a fragment of the target model, instead of taking whole models as in case of full M2M transformation. The presented solution improves overall usability of such M2M transformations in an actual CASE tool environment. © 2016 Elsevier Inc.","CASE tool; Drag-and-drop actions; MDA; Model-driven development of transformations; Model-to-model transformation; UML profile"
"Rule- and context-based dynamic business process modelling and simulation","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983567952&doi=10.1016%2fj.jss.2016.08.048&partnerID=40&md5=92ab2d747c810c57d20a747ba230ed50","The traditional approach used to implement a business process (BP) in today's information systems (IS) no longer covers the actual needs of the dynamically changing business. Therefore, a necessity for a new approach of dynamic business process (DBP) modelling and simulation has arisen. To date, existing approaches to DBP modelling and simulation have been incomplete, i.e. they lack theory or a case study or both. Furthermore, there is no commonly accepted definition of BDP. Current BP modelling tools are suitable almost solely for the modelling and simulation of a static BP that strictly prescribes which activities, and in which sequence, to execute. Usually, a DBP is not defined strictly at the beginning of its execution, and it changes under new conditions at runtime. In our paper, we propose six requirements of DBP and an approach for rule- and context-based DBP modelling and simulation. The approach is based on changing BP rules, BP actions and their sequences at process instance runtime, according to the new business system context. Based on the proposed approach, a reference architecture and prototype of a DBP simulation tool were developed. Modelling and simulation were carried out using this prototype, and the case study shows correspondence to the needs of dynamically changing business, as well as possibilities for modelling and simulating DBP. © 2016 Elsevier Inc.","Business process modelling; Business rules; Context; Dynamic business process; Simulation"
"Understanding the syntactic rule usage in java","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994644415&doi=10.1016%2fj.jss.2016.10.017&partnerID=40&md5=866e621360efc91a00587bc7a4046bda","Context: Syntax is fundamental to any programming language: syntax defines valid programs. In the 1970s, computer scientists rigorously and empirically studied programming languages to guide and inform language design. Since then, language design has been artistic, driven by the aesthetic concerns and intuitions of language architects. Despite recent studies on small sets of selected language features, we lack a comprehensive, quantitative, empirical analysis of how modern, real-world source code exercises the syntax of its programming language. Objective: This study aims to understand how programming language syntax is employed in actual development and explore their potential applications based on the results of syntax usage analysis. Method: We present our results on the first such study on Java, a modern, mature, and widely-used programming language. Our corpus contains over 5000 open-source Java projects, totalling 150 million source lines of code (SLoC). We study both independent (i.e. applications of a single syntax rule) and dependent (i.e. applications of multiple syntax rules) rule usage, and quantify their impact over time and project size. Results: Our study provides detailed quantitative information and yields insight, particularly (i) confirming the conventional wisdom that the usage of syntax rules is Zipfian; (ii) showing that the adoption of new rules and their impact on the usage of pre-existing rules vary significantly over time; and (iii) showing that rule usage is highly contextual. Conclusions: Our findings suggest potential applications across language design, code suggestion and completion, automatic syntactic sugaring, and language restriction. © 2016 Elsevier Inc.","Empirical study; Language syntax; Practical language usage"
"A mobile agent based communication protocol to optimize message delivery cost","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980325744&doi=10.1016%2fj.jss.2016.08.001&partnerID=40&md5=72432c54798756e52bef24419d0315e4","Agents are applicable to many computing areas like distributed systems because of their autonomy and mobility factors. Reliable and efficient communication between agents in a mobile agent system is a challenging task. To deliver the messages efficiently and reliably, mobile agent communication schemes generally have two phases. The first phase is agent tracking phase which deals with locating the agent current location. Second phase deals with the delivery of message to the agent at its current location. The existing approaches suffer from high communication overhead or problems such as message chase problem, triangle problem, or delayed delivery of messages. Approaches such as I-update deliver the message very efficiently without any overhead in message delivery phase but increase the memory and network overhead in agent tracking phase. In this paper, we present a novel approach that minimizes message delivery delay while maintaining a balance between the costs of agent tracking phase and message delivery phase. Our approach enhances the home agency scheme by maintaining track of agent migrations and costs of message delivery through different channels at the home agency. This approach allows the home agency to send the message to an agent via optimal transmission cost path, which minimizes message delivery delay and reduces the probability of occurrence of message chase problem. © 2016","Agent mobility management; Agents; Distributed systems; Mobile agents; Mobile agents communication"
"Service querying to support process variant development","2016","Journal of Systems and Software","10.1016/j.jss.2015.07.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940063041&doi=10.1016%2fj.jss.2015.07.050&partnerID=40&md5=4cf3f8d678dce9bc0302fddf00589649","Developing process variants enables enterprises to effectively adapt their business models to different markets. Existing approaches focus on business process models to support the variant development. The assignment of services in a business process, which ensures the process variability, has not been widely examined. In this paper, we present an innovative approach that focuses on component services instead of process models. We target to recommend services to a selected position in a business process. We define the service composition context as the relationships between a service and its neighbors. We compute the similarity between services based on the matching of their composition contexts. Then, we propose a query language that considers the composition context matching for service querying. We developed an application to demonstrate our approach and performed different experiments on a public dataset of real process models. Experimental results show that our approach is feasible and efficient. © 2015 Elsevier Inc.","Composition context matching; Service querying; Service-based business process"
"Reducing age stereotypes in software development: The effects of awareness- and cooperation-based diversity interventions","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979995367&doi=10.1016%2fj.jss.2016.07.041&partnerID=40&md5=2a3ce72097175eb4344c2e4055bfa9ef","Negative age stereotypes about older employees are present across industries and they are particularly strong in technology-related jobs. They can hinder cooperation and team processes, which are of utmost importance in software development. This paper proposes and compares two interventions to reduce age stereotypes in software development. An awareness-based intervention was conducted on-the job, as a quasi-experiment with 56 participants. A cooperation-based workshop was conducted as a field experiment with 74 employees. Both types of interventions reduced bias in performance and innovation expectations favoring middle-aged over older employees. The reduction in biases held by developers was particularly strong. Only the cooperation-based intervention reduced bias toward both older and younger employees. This intervention led to a long-term (six months) reduction in bias, regarding developer performance expectations. The study extends the diversity training literature in establishing causal, long-term effects for age stereotype reduction in the field. Furthermore, it contributes to the literature by indicating that contact hypothesis can not only be applied to reduce age stereotypes toward older but also toward younger employees. The design enables practitioners to create on-the-job diversity interventions that employees are willing to attend, and, thus, to reach a majority of employees without interventions being mandatory. © 2016 Elsevier Inc.","Age stereotypes; Bias; Diversity training; Innovation expectations; Performance expectations; Software development"
"Casper: Automatic tracking of null dereferences to inception with causality traces","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984629817&doi=10.1016%2fj.jss.2016.08.062&partnerID=40&md5=241543b7eb84d15dae170c18a46af625","Fixing a software error requires understanding its root cause. In this paper, we introduce “causality traces”, crafted execution traces augmented with the information needed to reconstruct the causal chain from the root cause of a bug to an execution error. We propose an approach and a tool, called CASPER, based on code transformation, which dynamically constructs causality traces for null dereference errors. The core idea of CASPER is to replace nulls with special objects, called “ghosts”, that track the propagation of the nulls from inception to their error-triggering dereference. Causality traces are extracted from these ghosts. We evaluate our contribution by providing and assessing the causality traces of 14 real null dereference bugs collected over six large, popular open-source projects. © 2016 Elsevier Inc.","Causality analysis; Debugging; Null pointer"
"An energy efficient and load balanced distributed routing scheme for wireless sensor networks with holes","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992146690&doi=10.1016%2fj.jss.2016.10.004&partnerID=40&md5=b69470d02b965c55798b3f8994ae26b8","In this paper we present a new approach to route packets in the presence of routing holes. In our proposal, nodes cooperate to determine the approximate polygon of a specific hole and then exchange information about the approximate polygon. Based on the hole covering parallelogram and the hole view angle of a specific node, packets can be forwarded along an escape route that bends around the hole. We rigorously prove that the Euclidean stretch of an escape route is bounded. Simulation results show that the proposed scheme can save more than 16% of the energy consumption and 7% of the network lifetime in the comparison with existing routing algorithms. The average length of routing paths in our approach is less than 60% of other routing schemes. © 2016 Elsevier Inc.","Angle of view; Hole; Routing; Wireless sensor"
"SAND: A fault-tolerant streaming architecture for network traffic analytics","2016","Journal of Systems and Software","10.1016/j.jss.2015.07.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940055251&doi=10.1016%2fj.jss.2015.07.049&partnerID=40&md5=34bb4259e4bfae58ac1fccd39e741a05","Many long-running network analytics applications (e.g., flow size estimation and heavy traffic detection) impose a high-throughput and high reliability requirements on stream processing systems. However, previous stream processing systems which are designed for higher layer applications cannot sustain high-speed traffic at the core router level. Furthermore, due to the nondeterministic nature of message passing among workers, the fault-tolerant schemes of previous streaming architectures based on the continuous operator model cannot provide strong consistency which is essential for network analytics. In this paper, we present the design and implementation of SAND, a fault-tolerant distributed stream processing system for network analytics. SAND is designed to operate under high-speed network traffic, and it uses a novel checkpointing protocol which can perform failure recovery based on upstream backup and checkpointing. We prove our fault-tolerant scheme provides strong consistency even under multiple node failure. We implement several real-world network analytics applications on SAND, including heavy traffic hitter detection as well as policy and charging control for cellular networks, and we evaluate their performance using network traffic captured from commercial cellular core networks. We demonstrate that SAND can sustain high-speed network traffic and that our fault-tolerant scheme is efficient. © 2015 Elsevier Inc.","Fault-tolerance; Network analytics; Stream processing"
"Test coverage of impacted code elements for detecting refactoring faults: An exploratory study","2017","Journal of Systems and Software","10.1016/j.jss.2016.02.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959125153&doi=10.1016%2fj.jss.2016.02.001&partnerID=40&md5=ee22a473614dc1f6b432d42b992e6268","Refactoring validation by testing is critical for quality in agile development. However, this activity may be misleading when a test suite is insufficiently robust for revealing faults. Particularly, refactoring faults can be tricky and difficult to detect. Coverage analysis is a standard practice to evaluate fault detection capability of test suites. However, there is usually a low correlation between coverage and fault detection. In this paper, we present an exploratory study on the use of coverage data of mostly impacted code elements to identify shortcomings in a test suite. We consider three real open source projects and their original test suites. The results show that a test suite not directly calling the refactored method and/or its callers increases the chance of missing the fault. Additional analysis of branch coverage on test cases shows that there are higher chances of detecting a refactoring fault when branch coverage is high. These results give evidence that a combination of impact analysis with branch coverage could be highly effective in detecting faults introduced by refactoring edits. Furthermore, we propose a statistic model that evidences the correlation of coverage over certain code elements and the suite's capability of revealing refactoring faults. © 2016 Elsevier Inc.","Coverage; Refactoring; Testing"
"Network-aware embedding of virtual machine clusters onto federated cloud infrastructure","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978414648&doi=10.1016%2fj.jss.2016.07.007&partnerID=40&md5=55f7501ff6514c230953237b842aff6a","Federated clouds are continuously developing as the demands of cloud users get more complicated. Contemporary cloud management technologies like Open-Stack (Sefraoui et al., 2012) and OpenNebula (Milojičić et al., 2011) allow users to define network topologies among virtual machines that are requested. Therefore, federated clouds currently face the challenge of network topology mapping in addition to conventional resource allocation problems. In this paper, topology based mapping of virtual machine clusters onto the federated cloud infrastructures is studied. A novel algorithm is presented to perform the mapping operation that work towards minimizing network latency and optimizing bandwidth utilization. To realize and evaluate the algorithm, a widely used cloud simulation environment, CloudSim (Calheiros et al., 2011), is extended to support several additional capabilities in network and cost modeling. Evaluation is performed by comparing the proposed algorithm to a number of conventional heuristics such as least latency first and round-robin. Results under different request characteristics indicate that the proposed algorithm performs significantly better than the compared conventional approaches regarding various QoS parameters such as inter-cloud latency and throughput. © 2016 Elsevier Inc.","Cloud computing; Federated cloud; Infrastructure as a service; Resource allocation; Subgraph isomorphism; VM cluster embedding"
"Software architectures for robotic systems: A systematic mapping study","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984664038&doi=10.1016%2fj.jss.2016.08.039&partnerID=40&md5=94fe2ac4ced5bc1c0b2a1ca7c4c6266c","Context Several research efforts have been targeted to support architecture centric development and evolution of software for robotic systems for the last two decades. Objective We aimed to systematically identify and classify the existing solutions, research progress and directions that influence architecture-driven modeling, development and evolution of robotic software. Research Method We have used Systematic Mapping Study (SMS) method for identifying and analyzing 56 peer-reviewed papers. Our review has (i) taxonomically classified the existing research and (ii) systematically mapped the solutions, frameworks, notations and evaluation methods to highlight the role of software architecture in robotic systems. Results and Conclusions We have identified eight themes that support architectural solutions to enable (i) operations, (ii) evolution and (iii) development specific activities of robotic software. The research in this area has progressed from object-oriented to component-based and now to service-driven robotics representing different architectural models that emerged overtime. An emerging solution is cloud robotics that exploits the foundations of service-driven architectures to support an interconnected web of robots. The results of this SMS facilitate knowledge transfer – benefiting researchers and practitioners – focused on exploiting software architecture to model, develop and evolve robotic systems. © 2016 Elsevier Inc.","Evidence-based software engineering; Robotic systems; Software architecture; Software architecture for robotics; Systematic mapping study"
"How many interesting points should be used in a template attack?","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979245206&doi=10.1016%2fj.jss.2016.07.028&partnerID=40&md5=3e38e1eae30b75d15a44b4d833405dc9","Considering that one can fully characterize and exploit the power leakages of the reference device in the process of recovering the secret key used by the target device, template attack (TA) is broadly accepted as the strongest power analysis attack from the perspective of information theory. In order to fully exploit the power leakages of the reference device, one usually has to concern the power leakages at different interesting points. Then, a natural question is how many interesting points should be used in a TA? We note that the number of interesting points one uses directly decides the profiling efficiency of TA. In light of this, we evaluate the optimal number of interesting points in simulated scenarios, and the evaluation results bring us an empirically useful formula. Then, in order to validate the empirical formula, we perform TA using power traces provided by DPA Contest v4.1. In the real scenario, the correlation method is used to select the interesting points, and the S-Box output of the 1st round AES encryption is chosen as the target intermediate value. Evaluation results show that the empirical formula is indeed correct and can be useful in practice. © 2016 Elsevier Inc.","Interesting points; Power analysis attacks; Profiling efficiency; Side channel attacks; Template attack"
"Approaches to strategic alignment of software process improvement: A systematic literature review","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991217529&doi=10.1016%2fj.jss.2016.09.030&partnerID=40&md5=8c93d1945411257ef5b8cc2578dd499b","Context: Software process improvement (SPI) aims to increase the effectiveness of a software organization. Many studies indicate that the strategic alignment is a critical factor for the SPI success. However, little is known about practical approaches to achieving and maintaining such alignment. Objective: The goal of this study is to evaluate the validation evidence of the existing approaches to the strategic alignment of SPI. Method: We develop a search protocol that combines database search and snowballing to perform the systematic literature review and evaluate empirical studies by applying rigor and relevance criteria. To evaluate the efficiency of our protocol, we use a “quasi-gold standard” to compute the sensitivity and precision of the search. Result: We identified 30 studies (18 empirical) and 19 approaches to strategic alignment of SPI from 495 retrieved studies. Only three out of the 18 empirical studies were rated as high in the categories rigor and relevance, suggesting the need for a stronger validation of the approaches. Conclusion: We conclude that the lack of empirical validation indicates that the results of the existing approaches have not been adequately transferred to practitioners yet, calling for more rigorous studies on the subject. © 2016 Elsevier Inc.","Business alignment; Software process improvement; Strategic alignment; Systematic literature review"
"Privacy protection by typing in ubiquitous computing systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979765531&doi=10.1016%2fj.jss.2016.07.037&partnerID=40&md5=39027965113d1b0608d627a49dbb40dc","Ubiquitous computing systems collect and share a great deal of information upon the users and their environment; including private or highly sensitive personal information. Unless users are confident enough that their privacy is protected, many will be deterred from using such systems. This paper proposes a privacy type system that controls the behaviour of concurrent, context-aware and mobile processes to ensure that private information is not accidentally disclosed. We prove the subject reduction property and the soundness of the proposed type system; which guarantee that a well-typed process cannot accidentally disclose private information. We demonstrate the pragmatics of our approach with a case study. © 2016 Elsevier Inc.","Pervasive systems; Privacy; Security; Simulation; Type system; Type-checking; Ubiquitous computing"
"Growing a language: An empirical study on how (and why) developers use some recently-introduced and/or recently-evolving JavaScript features","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975527439&doi=10.1016%2fj.jss.2016.04.045&partnerID=40&md5=7fb01c0724d9034e5c0c7601eb46d324","We describe an empirical study to understand how different language features in JavaScript are used by developers, with the goal of using this information to assist future extensions of JavaScript. We inspected more than one million unique scripts (over 80 MLOC) from various sources: JavaScript programs in the wild collected by a spider, (supposedly) better JavaScript programs collected from the top 100 URLs from the Alexa list, JavaScript programs with new language features used in Firefox Add-ons, widely used JavaScript libraries, and Node.js applications. Our corpus is larger and more diversified than those in prior studies. We also performed two explanatory studies to understand the reasons behind some of the language feature choices. One study was conducted on 107 JavaScript developers; the other was conducted on 45 developers of Node.js applications. Our study shows that there is a widespread confusion about newly introduced JavaScript features, a continuing misuse of existing problematic features, and a surprising lack of adoption of object-oriented features. It also hints at why developers choose to use language features this way. This information is valuable to the language designers and the stakeholders, e.g., IDE and tool builders, all of whom are responsible for growing a language. © 2016 Elsevier Inc.","Empirical study; JavaScript; Language evolution"
"Self-adaptive architecture evolution with model checking: A software cybernetics approach","2017","Journal of Systems and Software","10.1016/j.jss.2016.03.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963795932&doi=10.1016%2fj.jss.2016.03.010&partnerID=40&md5=3b9b0fc06b21554a4b98afe5e536aa18","The cloud computing era requires software architecture to be self-adaptive to the dynamic environment. This autonomous feature brings uncertainty and makes software behavior difficult to control. The uncontrollable behavior is caused by ill-defined architecture and might lead to system disruption. To address this problem, we propose a novel framework which applies software cybernetics to guide self-adaptive architecture evolution. In our framework, we formulate the architecture evolution process as a feedback control process. In the process, we take the self-adaptive architecture model and the model checking technique as the controlled object and controller, respectively. First, the self-adaptive architecture is specified by Breeze/ADL. Second, the framework leverages model checking to validate adaptive Breeze/ADL specifications. Third, a learning algorithm is designed to regulate validation results to generate feedback rules – Productions to guide the architecture evolution. A smart phone application example is chosen to demonstrate the feasibility of our framework. The results show that our framework facilitates architects to detect undesired states which are caused by error-prone adaptation rules. © 2016 Elsevier Inc.","Architecture evolution; Model checking; Self-adaptive software architecture; Software cybernetics"
"Technical debt reduction using search based automated refactoring","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969523698&doi=10.1016%2fj.jss.2016.05.019&partnerID=40&md5=07eba65643d86978f95d37848a0257a0","Software refactoring has been recognized as a valuable process during software development and is often aimed at repaying technical debt. Technical debt arises when a software product has been built or amended without full care for structure and extensibility. Refactoring is useful to keep technical debt low and if it can be automated there are obvious efficiency benefits. Using a combination of automated refactoring techniques, software metrics and metaheuristic searches, an automated refactoring tool can improve the structure of a software system without affecting its functionality. In this paper, four different refactoring approaches are compared using an automated software refactoring tool. Weighted sums of metrics are used to form different fitness functions that drive the search process towards certain aspects of software quality. Metrics are combined to measure coupling, abstraction and inheritance and a fourth fitness function is proposed to measure reduction in technical debt. The 4 functions are compared against each other using 3 different searches on 6 different open source programs. Four out of the 6 programs show a larger improvement in the technical debt function after the search based refactoring process. The results show that the technical debt function is useful for assessing improvement in quality. © 2016 Elsevier Inc.","Automated refactoring; Refactoring tools; Search based software engineering; Simulated annealing; Software metrics; Technical debt"
"Rapid quality assurance with Requirements Smells","2017","Journal of Systems and Software","10.1016/j.jss.2016.02.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961231169&doi=10.1016%2fj.jss.2016.02.047&partnerID=40&md5=ce76dbe3f620fdb37faafe1ceadaf170","Bad requirements quality can cause expensive consequences during the software development lifecycle, especially if iterations are long and feedback comes late. We aim at a light-weight static requirements analysis approach that allows for rapid checks immediately when requirements are written down. We transfer the concept of code smells to requirements engineering as Requirements Smells. To evaluate the benefits and limitations, we define Requirements Smells, realize our concepts for a smell detection in a prototype called Smella and apply Smella in a series of cases provided by three industrial and a university context. The automatic detection yields an average precision of 59% at an average recall of 82% with high variation. The evaluation in practical environments indicates benefits such as an increase of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Lightweight smell detection can uncover many practically relevant requirements defects in a reasonably precise way. Although some smells need to be defined more clearly, smell detection provides a helpful means to support quality assurance in requirements engineering, for instance, as a supplement to reviews. © 2016","Automatic defect detection; Requirements engineering; Requirements Smells"
"Method-level program dependence abstraction and its application to impact analysis","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990205299&doi=10.1016%2fj.jss.2016.09.048&partnerID=40&md5=4ec80053e2508a29d9aada31bafe774d","The traditional software dependence (TSD) model based on the system dependence graph enables precise fine-grained program dependence analysis that supports a range of software analysis and testing tasks. However, this model often faces scalability challenges that hinder its applications as it can be unnecessarily expensive, especially for client analyses where coarser results suffice. This paper revisits the static-execute-after (SEA), the most recent TSD abstraction approach, for its accuracy in approximating method-level forward dependencies relative to the TSD model. It also presents an alternative approach called the method dependence graph (MDG), compares its accuracy against the SEA, and explores applications of the dependence abstraction in the context of dependence-based impact analysis. Unlike the SEA approach which roughly approximates dependencies via method-level control flows only, the MDG incorporates more fine-grained analyses of control and data dependencies to avoid being overly conservative. Meanwhile, the MDG avoids being overly expensive by ignoring context sensitivity in transitive interprocedural dependence computation and flow sensitivity in computing data dependencies induced by heap objects. Our empirical studies revealed that (1) the MDG can approximate the TSD model safely, for method-level forward dependence at least, at much lower cost yet with low loss of precision, (2) for the same purpose, while both are safe and more efficient than the TSD model, the MDG can achieve higher precision than the SEA with better efficiency, both significantly, and (3) as example applications, the MDG can greatly enhance the cost-effectiveness of both static and dynamic impact analysis techniques that are based on program dependence analysis. More generally, as a program dependence representation, the MDG provides a viable solution to many challenges that can be reduced to balancing cost and effectiveness faced by dependence-based tasks other than impact analysis. © 2016 Elsevier Inc.","Accuracy; Cost-effectiveness; Dependence abstraction; Dependence analysis; Impact analysis; Method dependence graph (MDG)"
"Theoretical conceptualization of TD: A practical perspective","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006778786&doi=10.1016%2fj.jss.2016.05.043&partnerID=40&md5=faa73f1711032a4d0a81df3ef695f3ac","The Technical Debt (TD) metaphor has been used as a way to manage and communicate long-term consequences that some technical decisions may cause. Although intuitive, a lack of practical analysis and demonstrations defers its dissemination over the software community. This exploratory study applied two research methods to characterize the effects associated with a TD item during six years of a real software project lifecycle. First, a quantitative analysis was carried out to characterize the TD item in terms of concrete numbers. Then, Grounded Theory techniques were used to identify categories, properties and their relations, which could together provide a fuller definition of the TD metaphor. The resultant Grounded Theory, in the form of a concept map, confirmed some elements already identified by the technical literature, but also raised up new concepts that should be considered during analysis of TD items. Thus, this work contributes to the effort in building a formal theory about TD and provides directions to assist the work of developers/managers who intend to identify and monitor TD items in their projects, given the practical nature of this study. © 2016 Elsevier Inc.","Decision making; Project management; Software maintenance; Software TD"
"Reconciling software architecture and source code in support of software evolution","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993949496&doi=10.1016%2fj.jss.2016.10.012&partnerID=40&md5=19d7241207238dbb98ba4ac9a11002d5","Even in the eighties, the need of managing software evolution has been detected as one of the most complex aspects of the software lifecycle. In this context, software architecture has been highlighted as an integral element of the software evolution process. However, no matter how much effort is put into the architecture, it must eventually be translated into source code. The potential misalignment between architecture and code can lead to failures in the evolution process in terms of economic impacts, failed expectations, and so on. In this article we report on a design science research study that we pursued to answer three research questions. First, we have studied whether and in how far it is possible to design an approach that both enforces the integration between software architecture and source code to avoid architectural erosion and architectural drift and, at the same time, provides automatic guidance to developers to carry out the required change tasks in each evolution steps. Second, we have studied whether this approach may be applied in realistic (open source) cases. Finally, we have analysed whether it is realizable at acceptable costs (in terms of development effort) in comparison to the overall development efforts roughly spent on the evolution of the projects in focus. © 2016 Elsevier Inc.","ADLs; Architectural knowledge; Architecture reconstruction; Components; Evolution styles; M2M transformation"
"An MDE performance testing framework based on random model generation","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967335467&doi=10.1016%2fj.jss.2016.04.044&partnerID=40&md5=e5a3d7768f87beb3264b9a4ceb735935","The scalability of model-related operations (e.g., model transformations), when they are to be applied in industrial model-driven engineering, becomes an important issue. However, there is a lack of an automated performance testing framework for those operations, since the existing ones for ordinary programs are ill-suited. Such a framework is required to provide the function of creating and organizing test cases, and the ability of generating test input of large size automatically, because large scale models are not widely available, making it hard to test the performance and coverage of those operations without any bias. This paper proposes a performance testing framework, integrated with a random model generation algorithm, for model-related operations. The framework, based on a test model, can be used to specify and arrange test cases into test suites. And the model generation algorithm can generate a random model correctly and efficiently, according to the metamodel and user-defined constraints. Finally, we present two case studies, one experiment in randomness, and two experiments in generation efficiency to evaluate the framework and algorithm. Results show that the framework is competent to support performance testing of model-related operations, and the algorithm is random and efficient enough to generate test data for performance testing. © 2016 Elsevier Inc.","Model generation; Model-driven engineering; Model-related operation; Performance testing"
"Integrating social features into mobile local search","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991274555&doi=10.1016%2fj.jss.2016.09.013&partnerID=40&md5=665216bf5657aeec41d6521897866a54","As availability of Internet access on mobile devices develops year after year, users have been able to make use of search services while on the go. Location information on these devices has enabled mobile users to use local search services to access various types of location-related information easily. Mobile local search is inherently different from general web search. Namely, it focuses on local businesses and points of interest instead of general web pages, and finds relevant search results by evaluating different ranking features. It also strongly depends on several contextual factors, such as time, weather, location etc. In previous studies, rankings and mobile user context have been investigated with a small set of features. We developed a mobile local search application, Gezinio, and collected a data set of local search queries with novice social features. We also built ranking models to re-rank search results. We reveal that social features can improve performance of the machine-learned ranking models with respect to a baseline that solely ranks the results based on their distance to user. Furthermore, we find out that a feature that is important for ranking results of a certain query category may not be so useful for other categories. © 2016 Elsevier Inc.","Location-based social networks; Mobile local search; Mobile search"
"Adapting collections and arrays: Another step towards the automated adaptation of object ensembles","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993917401&doi=10.1016%2fj.jss.2016.10.002&partnerID=40&md5=e8bc7c449415b4829355630a930cb706","An important obstacle to reuse in object-oriented development is that objects or more generally components often cannot be plugged together directly due to interface mismatches. Consequently, automating the adaptation of software building blocks has been on the research agenda for quite a while. However, apart from various approaches based on (semi-)formal specifications, adaptation approaches based on test cases have only recently demonstrated that practically useable implementations of this idea are feasible. This article addresses the automated adaptation of arrays and collections in order to increase the applicability of existing test-based adaptation approaches. © 2016 Elsevier Inc.","Object adaptation; Signature mismatches; Test-driven adaptation"
"Cost benefits of flexible hybrid cloud storage: Mitigating volume variation with shorter acquisition cycle","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988643235&doi=10.1016%2fj.jss.2016.09.008&partnerID=40&md5=1246c7a78be1cb00d97617855e3f2e99","Hybrid cloud storage combines cost-effective but inflexible private storage along with flexible but premium-priced public cloud storage. As a form of concurrent sourcing, it offers flexibility and cost benefits to organizations by allowing them to operate at a cost-optimal scale and scope under demand volume uncertainty. However, the extant literature offers limited analytical insight into the effect that the non-stationarity (i.e., variability) and non-determinism (i.e., uncertainty) of the demand volume – in other words, the demand variation – have on the cost-efficient mix of internal and external sourcing. In this paper, we focus on the reassessment interval – that is, the interval at which the organization re-assesses its storage needs and acquires additional resources –, as well as on the impacts it has on the optimal mix of sourcing. We introduce an analytical cost model that captures the compound effect of the reassessment interval and volume variation on the cost-efficiency of hybrid cloud storage. The model is analytically investigated and empirically evaluated in simulation studies reflecting real-life scenarios. The results confirm that shortening the reassessment interval allows volume variability to be reduced, yielding a reduction of the overall costs. The overall costs are further reduced if, by shortening the interval, the demand uncertainty is also reduced. © 2016 Elsevier Inc.","Acquisition interval; Concurrent sourcing; Hybrid cloud storage; Plural governance; Reassessment interval; Volume variation"
"Identification and analysis of the elements required to manage technical debt by means of a systematic mapping study","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994877375&doi=10.1016%2fj.jss.2016.10.018&partnerID=40&md5=e7f9284a43254542c30289b900d2a138","Technical debt, a metaphor for the long-term consequences of weak software development, must be managed to keep it under control. The main goal of this article is to identify and analyze the elements required to manage technical debt. The research method used to identify the elements is a systematic mapping, including a synthesis step to synthesize the elements definitions. Our perspective differs from previous literature reviews because it focused on the elements required to manage technical debt and not on the phenomenon of technical debt or the activities used in performing technical debt management. Additionally, the rigor and relevance for industry of the current techniques used to manage technical debt are studied. The elements were classified into three groups (basic decision-making factors, cost estimation techniques, practices and techniques for decision-making) and mapped according three stakeholders’ points of view (engineering, engineering management, and business-organizational management). The definitions, classification, and analysis of the elements provide a framework that can be deployed to help in the development of models that are adapted to the specific stakeholders’ interests to assist the decision-making required in technical debt management and to assess existing models and methods. The analysis indicated that technical debt management is context dependent. © 2016 Elsevier Inc.","Basic decision-making factors; Business-organizational management; Cost estimation techniques; Decision making; Engineering; Engineering management; Framework; Practices and techniques for decision-making; Stakeholders’ points of view; Systematic mapping; Technical debt; Technical debt management"
"Integrating quality requirements in engineering web service orchestrations","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949294522&doi=10.1016%2fj.jss.2015.11.009&partnerID=40&md5=86fbdbdc24305bd2db75e706485a6b14","Today's Web services are considered as one of the leading technologies for implementing components of service-oriented software architectures for desktop, Web or mobile applications. When designing workflows of activities that involve the invocation of these Web Services, we build either orchestrations or choreographies. The engineering of such applications is an emerging research topic with many challenges. Among them, we can stress out the crucial question of how to answer quality requirements in such engineering processes. This paper, presents a method which aims at assisting software architects of Web Service orchestrations in integrating quality requirements in their artifacts. In order to satisfy a quality requirement, this method suggests a list of service-oriented patterns. We base our work on the postulate stating that quality can be implemented through patterns, which can be specified with checkable/processable languages. This method helps architects to reach concrete architecture changes that can be automatically performed on the orchestration in order to apply a pattern, and thus integrate its associated quality. We experimented our method on a set of real-world orchestrations (BPEL processes) to measure the overhead of using it in engineering such service-oriented applications. The obtained results showed that our method brings a significant gain of time. © 2015 Elsevier Inc.","BPEL; Quality attribute; SOA pattern"
"Computational offloading mechanism for native and android runtime based mobile applications","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981725652&doi=10.1016%2fj.jss.2016.07.043&partnerID=40&md5=2929dd12667fd26e7f9454ecd022e3df","Mobile cloud computing is a promising approach to augment the computational capabilities of mobile devices for emerging resource-hungry mobile applications. Android-based smartphones have opened real-world venues for mobile cloud applications mainly because of the open source nature of Android. Computational offloading mechanism enables the augmentation of smartphone capabilities. The problem is majority of existing computational offloading solutions for Android-based smartphones heavily depends on Dalvik VM (an application-level VM). Apart from being a discontinued product, Dalvik VM consumes extra time and energy because of the just-in-time (JIT) compilation of bytecode into machine instructions. With regard to this problem, Google has introduced Android Runtime (ART) featuring ahead-of-time (AHOT) compilation to native instructions in place of Dalvik VM. However, current state-of-the-art offloading solutions do not consider AHOT compilations to native binaries in the ART environment. To address the issue in offloading ART-based mobile applications, we propose a computational offloading framework. The proposed framework requires infrastructural support from cloud data centers to provide offloading as a service for heterogeneous mobile devices. Numerical results from proof-of-concept implementation revealed that the proposed framework improves the execution time of the experimental application by 76% and reduces its energy consumption by 70%. © 2016 Elsevier Inc.","Android runtime environment; Application partitioning; Computational offloading; Mobile cloud computing; Mobile cloud scheduling"
"Requirements cybernetics: Elicitation based on user behavioral data","2017","Journal of Systems and Software","10.1016/j.jss.2015.12.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956868007&doi=10.1016%2fj.jss.2015.12.030&partnerID=40&md5=9357590984d1ceaa5063810adf4a3ac4","Users’ behavioral data provides important cue for product improvement. Today's web based applications collect various kinds of service data, which is an ideal source of information for product designers to better understand users’ needs and behaviors. This paper first discusses the types of data collected so far, and then such data-driven requirements elicitation process is formulated as a feedback control system, where the classical requirements elicitation philosophy turns into a continuous optimization to user behavioral models. To this end, it is important to know how the data collection function reflects user behavior, and how specific data analysis approaches help making design decisions. This is an attempt to seek practical synergies between the two disciplines of requirements and cybernetics, to explore the possibilities of formulating problems in requirements with concepts and frameworks from cybernetics, and understand to what extent that known research results from cybernetics can be applied to address requirements problems. In particular, control frameworks for the user data driven requirements elicitation process are experimented, and potential control variables are discussed. We use two example cases to illustrate the proposed approach, an online dictionary service and a mobile music player service. © 2015 Elsevier Inc.","Cybernetics; Data analysis; Requirements elicitation"
"Clustering-based acceleration for virtual machine image deduplication in the cloud environment","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989941165&doi=10.1016%2fj.jss.2016.02.021&partnerID=40&md5=fedea97162c4fe6fadca8328b7255575","More and more virtual machine (VM) images are continuously created in datacenters. Duplicated data segments may exist in such VM images, and it leads to a waste of storage resource. As a result, VM image deduplication is a common daily activity in datacenters. Our previous work Crab is such a product and it is on duty regularly in our datacenter. The size of VM images is large and the amount of VM images is huge, and it is inefficient and impractical to load massive VM image fingerprints into memory for a fast comparison to recognize duplicated segments. To address this issue, we in this paper propose a clustering-based acceleration method. It uses an improved k-means clustering to find images having high chances to contain duplicated segments. With such a candidate selection phase, only limited VM image candidate fingerprints are loaded into memory. We empirically evaluate the effectiveness, robustness, and complexity of the proposed system. Experimental results show that it significantly reduces the performance interference to hosting virtual machine with an acceptable increase in disk space usage, compared with existing deduplication methods. © 2016 Elsevier Inc.","Cloud computing; Deduplication; Virtualization; VM image"
"A Novel Fitness function of metaheuristic algorithms for test data generation for simulink models based on mutation analysis","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978296497&doi=10.1016%2fj.jss.2016.07.001&partnerID=40&md5=54b4aa438e517abb41122f7056ab338d","Testing is one of the crucial activities to assure the software quality. The main objective of testing is to generate test data uncovering faults in software modules. There are a variety of testing techniques in which mutation testing is a popular approach to generate test sets and evaluate their fault detection ability. Simulink is an environment widely used in industry to design and simulate critical systems. Testing such a system at the design phase could help to detect faults earlier. This study aims to propose a novel fitness function of metaheuristic algorithms to generate test data based on the mutation technique for the Simulink models. The fitness function is designed by analyzing each mutation operator and the features of blocks in the Simulink environment in order to guide the search process to reach the test data killing mutants more easily. Then, this fitness function is used in the multi-parent crossover genetic algorithm to generate test sets. The obtained results indicated that the mutation score has been significantly improved for all models when using the novel fitness function. In addition, each stubborn mutant was killed with a lower number of test data evaluations in comparison with the work of other authors. © 2016 Elsevier Inc.","Fitness function; Genetic algorithm; Mutation testing; Mutation-based test data generation; Simulink; Test data generation"
"Safe reconfiguration of Coqcots and Pycots components","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949683066&doi=10.1016%2fj.jss.2015.11.039&partnerID=40&md5=1b4e37c3df5235e5ff9a72d3d4bee955","Software systems have to face evolutions of their running context and users. Therefore, the so-called dynamic reconfiguration has been commonly adopted for modifying some components and/or the architecture at runtime. Traditional approaches typically stop the needed components, apply the changes, and restart the components. However, this scheme is not suitable for critical systems and degrades user experience. This paper proposes to switch from the stop/restart scheme to dynamic software updating (DSU) techniques. Instead of stopping a component, its implementation is replaced by another one specifically built to apply the modifications while maintaining the best quality of service possible. The major contributions of this work are: (i) the integration of DSU techniques in a component model; (ii) a reconfiguration development process including specification, proof of correctness using Coq, and; (iii) a systematic method to produce the executable script. In this perspective, the use of DSU techniques brings higher quality of service when reconfiguring component-based software. Moreover, the formalization allows ensuring the safety and consistency of the reconfiguration process. © 2015 Elsevier Inc.","Component model; Dynamic reconfiguration; Dynamic software updating; Runtime evolution"
"An assessment of extended finite state machine test selection criteria","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992420701&doi=10.1016%2fj.jss.2016.09.044&partnerID=40&md5=37b2efb9c47cd3754f8a6647c5a93ae8","Extended finite state machines (EFSMs) provide a rigorous model for the derivation of functional tests for software systems and protocols. Various types of data-flow, control-flow, graph-based, and state machine based test selection criteria can be used for deriving tests from a given EFSM specification. Also, traditional types of state machine based notions of faults, such as transfer and output parameter faults, and common types of assignment faults can be used to describe the fault domains of EFSMs. We present an assessment of the most known types of EFSM test selection criteria such as test suites that cover single transfer faults, double transfer faults, single output parameter faults, and many types of single assignment faults of a given EFSM specification. Also, test suites that cover edge-pair, prime path, prime path with side trip, and all-uses criterion are derived from the graph and flow-graph representations of the specification. We also consider transition tour and random test suites. The assessment ranks the considered test suites in terms of their length and their coverage of single transfer, double transfer, and different type of single assignment faults. Dispersion of the obtained results is assessed and results are summarized. © 2016 Elsevier Inc.","Extended finite state machines; Fault coverage assessment; Model based testing; Mutation testing; Test derivation"
"Continuous software engineering: A roadmap and agenda","2017","Journal of Systems and Software","10.1016/j.jss.2015.06.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937109068&doi=10.1016%2fj.jss.2015.06.063&partnerID=40&md5=161802bdf6e0f875009ab61c13b7c5a3","Throughout its short history, software development has been characterized by harmful disconnects between important activities such as planning, development and implementation. The problem is further exacerbated by the episodic and infrequent performance of activities such as planning, testing, integration and releases. Several emerging phenomena reflect attempts to address these problems. For example, Continuous Integration is a practice which has emerged to eliminate discontinuities between development and deployment. In a similar vein, the recent emphasis on DevOps recognizes that the integration between software development and its operational deployment needs to be a continuous one. We argue a similar continuity is required between business strategy and development, BizDev being the term we coin for this. These disconnects are even more problematic given the need for reliability and resilience in the complex and data-intensive systems being developed today. We identify a number of continuous activities which together we label as ‘Continuous *’ (i.e. Continuous Star) which we present as part of an overall roadmap for Continuous Software engineering. We argue for a continuous (but not necessarily rapid) software engineering delivery pipeline. We conclude the paper with a research agenda. © 2015 Elsevier Inc.","Continuous software engineering; DevOps; Lean software development"
"A cybernetics Social Cloud","2017","Journal of Systems and Software","10.1016/j.jss.2015.12.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955621963&doi=10.1016%2fj.jss.2015.12.031&partnerID=40&md5=76a18abd15ad48e426b750831e9b9daf","This paper proposes a Social Cloud, which presents the system design, development and analysis. The technology is based on the BOINC open source software, our hybrid Cloud, Facebook Graph API and our development in a new Facebook API, SocialMedia. The creation of SocialMedia API with its four functions can ensure a smooth delivery of Big Data processing in the Social Cloud, with four selected examples provided. The proposed solution is focused on processing the contacts who click like or comment on the author's posts. Outputs result in visualization with their core syntax being demonstrated. Four functions in the SocialMedia API have evaluation test and each client-server API processing can be completed efficiently and effectively within 1.36 s. We demonstrate large scale simulations involved with 50,000 simulations and all the execution time can be completed within 70,000 s. Cybernetics functions are created to ensure that 100% job completion rate for Big Data processing. Results support our case for Big Data processing on Social Cloud with no costs involved. All the steps involved have closely followed system design, implementation, experiments and validation for Cybernetics to ensure a high quality of outputs and services at all times. This offers a unique contribution for Cybernetics to meet Big Data research challenges. © 2015 Elsevier Inc.","Big Data cybernetics; Data visualization; SocialMedia API"
"An improved genetic algorithm for task scheduling in the cloud environments using the priority queues: Formal verification, simulation, and statistical testing","2017","Journal of Systems and Software","10.1016/j.jss.2016.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994005147&doi=10.1016%2fj.jss.2016.07.006&partnerID=40&md5=10b2ce0fd6a0978dff7932f9205a4132","Cloud computing is a new platform to manage and provide services on the internet. Lately, researchers have paid attention a lot to this new subject. One of the reasons to have high performance in a cloud environment is the task scheduling. Since the task scheduling is an NP-Complete problem, in many cases, meta-heuristics scheduling algorithms are used. In this paper to optimize the task scheduling solutions, a powerful and improved genetic algorithm is proposed. The proposed algorithm uses the advantages of evolutionary genetic algorithm along with heuristic approaches. For analyzing the correctness of the proposed algorithm, we have presented a behavioral modeling approach based on model checking techniques. Then, the expected specifications of the proposed algorithm is extracted in the form of Linear Temporal Logic (LTL) formulas. To achieve the best performance in verification of the proposed algorithm, we use the Labeled Transition System (LTS) method. Also, the proposed behavioral models are verified using NuSMV and PAT model checkers. Then, the correctness of the proposed algorithm is analyzed according to the verification results in terms of some expected specifications, reachability, fairness, and deadlock-free. The simulation and statistical results revealed that the proposed algorithm outperformed the makespans of the three well-known heuristic algorithms and also the execution time of our recently meta-heuristics algorithm. © 2016 Elsevier Inc.","Cloud computing; Directed acyclic graph; Formal verification; Genetic algorithm; Model checking; Task scheduling"
"A software cybernetics approach to self-tuning performance of on-line transaction processing systems","2017","Journal of Systems and Software","10.1016/j.jss.2016.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961798862&doi=10.1016%2fj.jss.2016.03.012&partnerID=40&md5=72c45c3b708212e7b1c87f594d5cbd66","Self-tuning performance of On-Line Transaction Processing (OLTP) Systems is a challenging and time-consuming task since multiple performance parameters are needed to be automatically configured in Database Management Systems (DBMSs). In this paper, we present a software cybernetics approach to self-tune the performance of DBMSs. A DBMS is designed with an adaptive control based on fuzzy logic such that it has the capability to control objects, i.e., the performance parameters, and update the controller itself, i.e., a set of fuzzy rules in our case. The principles and concepts in software cybernetics are applied to guide the synthesis of software controllers for monitoring and adapting system behaviors. Experimental results for On-Line Transaction Processing using TPC-C, a benchmark of the Transaction Processing Performance Council, show that the proposed method is feasible and effective. © 2016","Database Management System (DBMS); Fuzzy rules; Self-tuning"
"Explicit connection patterns (ECP) profile and semantics for modelling and generating explicit connections in complex UML composite structures","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961164855&doi=10.1016%2fj.jss.2016.02.025&partnerID=40&md5=740ac150785d8263a2acece2961babaa","Model-driven engineering can help in mitigating ever-growing complexity of modern software systems. In this sense, the Unified Modelling Language (UML) has gained a thick share in the market of modelling languages adopted in industry. Nevertheless, the generality of UML can make it hard to build complete code generators, simulators, model-based analysis or testing tools without setting variability in the semantics of the language. To tailor semantics variability the notion of semantic variation point has been introduced in UML 2.0. Our research focuses on the semantic variation point that leaves the rules for matching multiplicities of connected instances of components and ports undecided in UML composite structures. In order to allow model analysability, simulation and code generation, this semantics needs to be set. At the same time, leaving the burden of this task to the developers is often overwhelming for complex systems. In this paper we provide a solution for supporting modelling and automatic calculation and generation of explicit interconnections in complex UML composite structures. This is achieved by (i) defining a set of connection patterns, in terms of a UML profile, and related semantic rules for driving the calculation, (ii) providing a generation algorithm to calculate the explicit interconnections. © 2016 Elsevier Inc.","Composite structure; Explicit interconnections; Semantic variation point; UML profile"
"An efficient validation approach for quasi-synchronous checkpointing oriented to distributed diagnosability","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969584498&doi=10.1016%2fj.jss.2016.04.070&partnerID=40&md5=7bb64c2a70a402e8f7be66b75960ddc1","The autonomic computing paradigm is oriented towards enabling complex distributed systems to manage themselves, even in faulty situations. The diagnosability analysis is a priori a study through which a system can be self-aware about its current state. It is from the determination of a consistent state that a system can take some action to repair or reconfigure itself. Nevertheless, in a distributed system it is hard to determine consistent states since we cannot observe simultaneously all the local variables of different processes. In this context, the challenge is to efficiently monitor the system execution over time to capture trace information in order to determine if the system accomplishes both functional and non-functional requirements. Quasi-synchronous checkpointing is a technique that collects information from which a system can establish consistent snapshots. Based on this technique, several checkpointing algorithms have been developed. According to the checkpoint properties detected and ensured, they are classified into: Strictly Z-Path Free (SZPF), Z-Path Free (ZPF) and Z-Cycle Free (ZCF). Generally, the method adopted for the performance evaluation of checkpointing algorithms involves simulation. However, few works have been designed to validate their correctness. In this paper, we propose an efficient validation approach based on a graph transformation oriented towards the automatic detection of the previously mentioned properties. To achieve this, we took the vector clocks resulting from an algorithm execution, and we modeled them into the happened-before graph and the immediate dependency graph (which is the minimal causal graph). Then, we designed a set of transformation rules to verify if in these graphs, the algorithm is exempt from non-desirable patterns, such as Z-paths or Z-cycles, according to the case. © 2016 Elsevier Inc.","Autonomic computing; Graph transformation; Happened before relation; Immediate dependency relation; Quasi-synchronous checkpointing"
"An empirical study to quantify the characteristics of Java programs that may influence symbolic execution from a unit testing perspective","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963579194&doi=10.1016%2fj.jss.2016.03.020&partnerID=40&md5=2d61a22c9501a9cd021e7faa16cfad5a","In software testing, a program is executed in hopes of revealing faults. Over the years, specific testing criteria have been proposed to help testers to devise test cases that cover the most relevant faulty scenarios. Symbolic execution has been used as an effective way of automatically generating test data that meet those criteria. Although this technique has been used for over three decades, several challenges remain and there is a lack of research on how often they appear in real-world applications. In this paper, we analyzed two samples of open source Java projects in order to understand the characteristics that may hinder the generation of unit test data using symbolic execution. The first sample, named SF100, is a third party corpus of classes obtained from 100 projects hosted by SourceForge. The second sample, called R47, is a set of 47 well-known and mature projects we selected from different repositories. Both samples are compared with respect to four dimensions that influence symbolic execution: path explosion, constraint complexity, dependency, and exception-dependent paths. The results provide valuable insight into how researchers and practitioners can tailor symbolic execution techniques and tools to better suit the needs of different Java applications. © 2016 Elsevier Inc.","Software testing; Symbolic execution; Test data generation"
"Extracting finite state models from i* models","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964344861&doi=10.1016%2fj.jss.2016.03.038&partnerID=40&md5=525d0b84a8f14df0ce9c8c67eb3bc0bd","i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm. © 2016","i<sup>*</sup> model; Model checking; Model transformation"
"Flexible software process lines in practice: A metamodel-based approach to effectively construct and manage families of software process models","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981318364&doi=10.1016%2fj.jss.2016.07.031&partnerID=40&md5=b72b8ca811c79877c3c4fafe04a7eb40","Process flexibility and adaptability is a frequently discussed topic in literature, and several approaches propose techniques to improve and optimize software processes for a given organization- or project context. A software process line (SPrL) is an instrument to systematically construct and manage variable software processes, by combining pre-defined and standardized process assets that can be reused, modified, and extended using a well-defined customization approach. Hence, process engineers can ground context-specific process variants in a standardized or domain-specific reference model that can be adapted to the respective context. In this article, we present an approach to construct flexible software process lines and show its practical application in the German V-Modell XT. The presented approach emerges from a 10-year research endeavor and was used to enhance the metamodel of the V-Modell XT and to allow for improved process variability and lifecycle management. Practical dissemination and complementing empirical research show the suitability of the concept. We therefore contribute a proven approach that is presented as metamodel fragment for reuse and implementation in further process modeling approaches. © 2016 Elsevier Inc.","Process design; Process realisation; Software process; Software process lines; Software process metamodel; V-Modell XT metamodel"
"Segmenting large traces of inter-process communication with a focus on high performance computing systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978100583&doi=10.1016%2fj.jss.2016.06.067&partnerID=40&md5=58e54b0ccc17bee0174a45bb7273b900","The understanding of the interactions among processes of a High Performance Computing (HPC) system can be made easier if trace analysis is used. Traces, however, can be quite large, making it difficult to analyze their content unless some abstraction is provided. This paper presents a novel trace abstraction approach that aims to facilitate the analysis of large execution traces generated from HPC applications. Our approach allows automatic segmentation of large traces into smaller and meaningful clusters that reflect the various execution phases of the traced scenarios. Our approach is based on the application of information theory principles to the analysis of sequences of communication patterns extracted from traces of HPC systems. This work is inspired by recent studies in the field of bioinformatics where several techniques have been proposed to segment DNA sequences into homogeneous sub-domains, where each sub-domain exhibits a certain degree of internal homogeneity. Trace segments can be used in a number of applications such as recovering high-level views of the system behavior and program understanding. We demonstrate the usefulness of our approach by applying it to different traces of hundreds of millions of events, generated from two HPC systems. © 2016 Elsevier Inc.","Dynamic analysis; High performance computing systems; Inter-process communication traces; Program comprehension; Software maintenance; Trace abstraction and analysis"
"Hybrid functional link artificial neural network approach for predicting maintainability of object-oriented software","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956857230&doi=10.1016%2fj.jss.2016.01.003&partnerID=40&md5=de21298ebe88db7e1c2fc5f56dd5b937","In present day, software development methodology is mostly based on object-oriented paradigm. With the increase in the number of these software system, their effective maintenance aspects becomes a crucial factor. Most of the maintainability prediction models in literature are based on techniques such as regression analysis and simple neural network. In this paper, three artificial intelligence techniques (AI) such as hybrid approach of functional link artificial neural network (FLANN) with genetic algorithm (GA), particle swarm optimization (PSO) and clonal selection algorithm (CSA), i.e., FLANN-Genetic (FGA and AFGA), FLANN-PSO (FPSO and MFPSO), FLANN-CSA (FCSA) are applied to design a model for predicting maintainability. These three AI techniques are applied to predict maintainability on two case studies such as Quality Evaluation System (QUES) and User Interface System (UIMS). This paper also focuses on the effectiveness of feature reduction techniques such as rough set analysis (RSA) and principal component analysis (PCA) when they are applied for predicting maintainability. The results show that feature reduction techniques are very effective in obtaining better results while using FLANN-Genetic. © 2016","Artificial neural network; CK metrics suite; Maintainability"
"Software defect prediction using doubly stochastic Poisson processes driven by stochastic belief networks","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986300617&doi=10.1016%2fj.jss.2016.09.001&partnerID=40&md5=09655cc8b6f32f99c6aaa7e8b57e34b3","Accurate prediction of software defects is of crucial importance in software engineering. Software defect prediction comprises two major procedures: (i) Design of appropriate software metrics to represent characteristic software system properties; and (ii) development of effective regression models for count data, allowing for accurate prediction of the number of software defects. Although significant research effort has been devoted to software metrics design, research in count data regression has been rather limited. More specifically, most used methods have not been explicitly designed to tackle the problem of metrics-driven software defect counts prediction, thus postulating irrelevant assumptions, such as (log-)linearity of the modeled data. In addition, a lack of simple and efficient algorithms for posterior computation has made more elaborate hierarchical Bayesian approaches appear unattractive in the context of software defect prediction. To address these issues, in this paper we introduce a doubly stochastic Poisson process for count data regression, the failure log-rate of which is driven by a novel latent space stochastic feedforward neural network. Our approach yields simple and efficient updates for its complicated conditional distributions by means of sampling importance resampling and error backpropagation. We exhibit the efficacy of our approach using publicly available and benchmark datasets. © 2016 Elsevier Inc.","Doubly stochastic Poisson process; Sampling importance resampling; Software defect prediction; Stochastic belief network"
"The verification of program relationships in the context of software cybernetics","2017","Journal of Systems and Software","10.1016/j.jss.2016.01.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957402063&doi=10.1016%2fj.jss.2016.01.031&partnerID=40&md5=b644498b35532719a3eb0b453d8ea358","Software cybernetics aims at improving the reliability of software by introducing the control theory into software engineering domain systematically. A key issue in software verification is to improve the reliability of software by inspecting whether the software can achieve its expected behaviors. In this paper, the thought of software cybernetics is applied in the process of verification to address this issue and a nested control system is established. The proposed method verifies functional requirements in a dynamic environment with constantly changing user requirements, in which the program serves as a controlled object, and the verification strategy determined by software behavioral model (SBM) serves as a controller. The main contribution of this paper includes: (1) SBM is established in software design phase, and a concern-based construction approach is proposed, which starts from obtaining the software expected functionality extracted from a requirement text; (2) Program abstract-relationship model (PARM) is constructed basing on a process of gradual abstract to be a controlled object; (3) Feedback in a form of intermediate code is generated in the process of verification. The proposed method is validated by our case study. © 2016","Control system; Software cybernetics; Software verification"
"A tiny hypervisor-based trusted geolocation framework with minimized TPM operations","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988734634&doi=10.1016%2fj.jss.2016.09.026&partnerID=40&md5=84c194231229f8954c4c068710305bc0","Geographic locations of user devices are widely used to provide rich user experience in various environments such as proximity-based marketing, travel information, and cloud computing. Especially, cloud service providers require to utilize actual cloud user's locations in location-based cloud services like Amazon GovCloud. However, it is not trivial to obtain the trusted geolocations of the user devices because there are many points for attackers to forge the current geolocations of the cloud user devices. In order to solve this security issue, we propose a novel trusted geolocation framework for the cloud user device. The primary mechanism of the proposed framework is to deliver a trusted channel between a geolocation server and a tiny hypervisor in each mobile client. We leverage the Trusted Platform Module and dynamic root of trust measurement to securely attest the geolocations of the cloud devices. To show the feasibility of the proposed framework, we port Etherpad, a cloud word processor, into the trusted geolocation-based cloud service. We also evaluate the performance overhead of our framework in the cloud device and show that it causes only 8.3% overhead in JavaScript benchmark, which indicates the practicality of the proposed framework. © 2016 Elsevier Inc.","Hypervisor; Trusted execution environment; Trusted geolocation"
"On building a cloud-based mobile testing infrastructure service system","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995752387&doi=10.1016%2fj.jss.2016.11.016&partnerID=40&md5=d6d562abdcdf5d40a396590538f9d0be","With the rapid advance of mobile computing, cloud computing and wireless network, there is a significant increasing number of mobile subscriptions. This brings new business requirements and demands in mobile testing service, and causes new issues and challenges. In this paper, informative discussions about cloud-based mobile testing-as-a-service (mobile TaaS) are offered, including the essential concepts, focuses, test process, and the expected testing infrastructures. To address the need of infrastructure level service for mobile TaaS, this paper presents a developed system known as MTaaS to provide an infrastructure-as-a-service (IaaS) for mobile testing, in order to indicate the feasibility and effectiveness of cloud-based mobile testing service. In addition, the paper presents a comparison among cloud-based mobile TaaS approaches and several best practices in industry are discussed. Finally, the primary issues, challenges, and needs existed in current mobile TaaS are analyzed. © 2016 Elsevier Inc.","Cloud-based infrastructure -as-a-service; Mobile application testing; Mobile testing as a service"
"Fault localization using disparities of dynamic invariants","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988001487&doi=10.1016%2fj.jss.2016.09.014&partnerID=40&md5=02e10f30bcf34758c7ad1f8dc8a1f55a","Violations of dynamic invariants may offer useful clues for identifying faults in programs. Although techniques that use violations of dynamic invariants to detect anomalies have been developed, some of them are restrained by the high computational cost of invariant detecting, false positive filtering, and redundancy removing, and others can only discover a few specific types of faults under a complete monitoring environment. This paper presents a novel fault localization approach using disparities of dynamic invariants, named FDDI. To make more efficient use of invariant detecting tools, FDDI first selects highly suspect functions via spectrum-based fault localization techniques, and then applies invariant detecting tools to these functions one by one. For each suspect function, FDDI uses variables that are involved in dynamic invariants that do not simultaneously hold in a set of passed and a set of failed tests to do further analysis, which reduces the time cost in filtering false positives and redundant invariants. Finally, FDDI locates statements that are data-related to these variables. The experimental results show that FDDI is able to locate 75% of 360 common faults in utility programs when examining up to 10% of the executed code, while Naish2, Ochiai and Jaccard all locate around 53%. © 2016 Elsevier Inc.","Dynamic invariant; Fault localization; Program analysis; Software debugging"
"Continuous performance evaluation and capacity planning using resource profiles for enterprise applications","2017","Journal of Systems and Software","10.1016/j.jss.2015.08.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944799971&doi=10.1016%2fj.jss.2015.08.030&partnerID=40&md5=3afe531d5a24420c19e2ba32c6c1a982","Continuous delivery (CD) is a software release process that helps to make features and bug fixes rapidly available in new enterprise application (EA) versions. Evaluating the performance of each EA version in a CD process requires a test environment comparable to a production system. Maintaining such systems is labor intensive and expensive. If multiple deployments of the same EA exist, it is often not feasible to maintain test instances for all of these systems. Furthermore, not all deployments are known at the time of a release (e.g., for off-the-shelf products). To address these challenges, this work proposes the use of resource profiles which describe the resource demand per transaction for each component of an EA and allow for performance predictions for different hardware environments and workloads without the need to own corresponding test environments. Within a CD process, resource profiles can be used to detect performance changes in EA versions. Once a version is released, resource profiles can be distributed along with the application binaries to support capacity planning for new deployments. Three integrated experiments for a representative EA provide validation for these capabilities. © 2015 Elsevier Inc.","Capacity planning; Performance evaluation; Resource profile"
"A fuzzy-based credibility model to assess Web services trust under uncertainty","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949580160&doi=10.1016%2fj.jss.2015.09.040&partnerID=40&md5=4e13bf1b33e78c98604b250f282281bf","This paper discusses the assessment of Web services trust. This assessment is undermined by the uncertainty that raises due to end-users’ ratings that can be questioned and variations in Web services performance at run-time. To tackle the first uncertainty a fuzzy-based credibility model is suggested so that the gap between end-users (known as strict) and the current majority is reduced. To deal with the second uncertainty two trust approaches (i.e., deterministic and probabilistic) are proposed so that trust levels for future interactions with WSs are made available to users. The deterministic approach takes account end-users’ credibility values and the probabilistic one is built upon probabilistic databases and a fuzzy-based credibility model. A series of experiments are carried out to validate the suggested credibility model and these trust approaches. The results show that the probabilistic approach improves significantly trust quality and is more robust compared to the deterministic one. Future work consists of incorporating several credibility models into a single probabilistic trust model. © 2015 Elsevier Inc.","Credibility; Trust; Web service"
"SIT: Sampling-based interactive testing for self-adaptive apps","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978530253&doi=10.1016%2fj.jss.2016.07.002&partnerID=40&md5=d034fd3545d6e724b7462910f83f8de0","Self-adaptive applications (“apps” for short) are useful but error-prone. This stems from developers’ inadequate consideration of environmental dynamics and uncertainty. Two features of self-adaptive apps, infinite reaction loop and uncertain interaction, bring additional challenges to software testing and make existing approaches ineffective. In this article, we propose a novel approach SIT (Sample-based Interactive Testing) to testing self-adaptive apps effectively and in a light-weight way. Our key insight is that a self-adaptive app's input space can be systematically split, adaptively explored, and mapped to the testing of the app's different behavior. This is achieved by our approach's two components, an interactive app model and a test generation technique. The former captures characteristics of interactions between an app and its environment, and the latter uses adaptive sampling to explore an app's input space and test its behavior. We experimentally evaluated our approach with real-world self-adaptive apps. The experimental results reported that our SIT improved the bug detection by 22.4–42.2%, but with a smaller time cost. Besides, SIT is also scalable with our tailored optimization techniques. © 2016 Elsevier Inc.","Interactive application model; Sample-based testing; Self-adaptive application testing"
"Software component decision-making: In-house, OSS, COTS or outsourcing - A systematic literature review","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982300670&doi=10.1016%2fj.jss.2016.07.027&partnerID=40&md5=146f780e388ad81158efa4b8268061c6","Context: Component-based software systems require decisions on component origins for acquiring components. A component origin is an alternative of where to get a component from. Objective: To identify factors that could influence the decision to choose among different component origins and solutions for decision-making (For example, optimization) in the literature. Method: A systematic review study of peer-reviewed literature has been conducted. Results: In total we included 24 primary studies. The component origins compared were mainly focused on in-house vs. COTS and COTS vs. OSS. We identified 11 factors affecting or influencing the decision to select a component origin. When component origins were compared, there was little evidence on the relative (either positive or negative) effect of a component origin on the factor. Most of the solutions were proposed for in-house vs. COTS selection and time, cost and reliability were the most considered factors in the solutions. Optimization models were the most commonly proposed technique used in the solutions. Conclusion: The topic of choosing component origins is a green field for research, and in great need of empirical comparisons between the component origins, as well of how to decide between different combinations of them. © 2016 Elsevier Inc.","Component-based software engineering; COTS; Decision-making; In-house development; OSS; Outsourcing"
"ScapeGoat: Spotting abnormal resource usage in component-based reconfigurable software systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994360360&doi=10.1016%2fj.jss.2016.02.027&partnerID=40&md5=f4ed4881bc590e2999f3d611dcd203bd","Modern component frameworks support continuous deployment and simultaneous execution of multiple software components on top of the same virtual machine. However, isolation between the various components is limited. A faulty version of any one of the software components can compromise the whole system by consuming all available resources. In this paper, we address the problem of efficiently identifying faulty software components running simultaneously in a single virtual machine. Current solutions that perform permanent and extensive monitoring to detect anomalies induce high overhead on the system, and can, by themselves, make the system unstable. In this paper we present an optimistic adaptive monitoring system to determine the faulty components of an application. Suspected components are finely analyzed by the monitoring system, but only when required. Unsuspected components are left untouched and execute normally. Thus, we perform localized just-in-time monitoring that decreases the accumulated overhead of the monitoring system. We evaluate our approach on two case studies against a state-of-the-art monitoring system and show that our technique correctly detects faulty components, while reducing overhead by an average of 93%. © 2016 Elsevier Inc.","Component; Models@Run.Time; Resource monitoring"
"Automatic web content personalization through reinforcement learning","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975757906&doi=10.1016%2fj.jss.2016.02.008&partnerID=40&md5=8d7c3661bc0d939a042f1b25211981a7","This paper deals with the automatic adaptation of Web contents. It is recognized that quite often users need some personalized adaptations to access Web contents. This is more evident when we focus on people with some accessibility needs. Based on the user profile, it is possible to transcode or modify contents (e.g., adapt text fonts) so as to meet the user preferences. The problem is that applying such a kind of transformations to the whole content might significantly alter Web pages that might become unreadable, hence making matters worse. We present a system that employs Web intelligence to perform automatic adaptations on single elements composing a Web page. A reinforcement learning algorithm is utilized to manage user profiles. We evaluate our system through simulation and a real assessment where elderly users where asked to use for a time period our system prototype. Results confirm the feasibility of the proposal. © 2016 Elsevier Inc.","Reinforcement learning; User profiling; Web personalization"
"Enhancements for duplication detection in bug reports with manifold correlation features","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961223695&doi=10.1016%2fj.jss.2016.02.022&partnerID=40&md5=903cdd3b0b9d2d40230a71388be086fa","In software maintenance activities, bug report processing is a major task in deriving crucial information for bug fixing. Because a considerable fraction of bug reports comprises duplicates in many projects, the duplicate reports must be identified for processing efficiency. Various text mining schemes have been proposed to handle this detection problem. This paper proposes an enhanced support vector machines (SVM) model (SVM-SBCTC) by considering the manifold textual and semantic correlation features based on a previous SVM-based discriminative scheme (SVM-54). We conducted empirical studies on three open source software projects: Apache, ArgoUML, and SVN. Compared with the SVM-54 scheme, SVM-SBCTC demonstrates promising detection performance in achieving relative improvements ranging 2.79%–28.97% in the top-5 recall rates among three projects. Furthermore, SVM-SBCTC demonstrates the top performance among various other weighting schemes in most cases. © 2016","Bug reports; Correlation features; Duplication detection"
"Automatic message compression with overload protection","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966728710&doi=10.1016%2fj.jss.2016.04.010&partnerID=40&md5=89414498ae2d0d13e02721bfcb993b7e","In this paper, we show that it is possible to increase the message throughput of a large-scale industrial system by selectively compress messages. The demand for new high-performance message processing systems conflicts with the cost effectiveness of legacy systems. The result is often a mixed environment with several concurrent system generations. Such a mixed environment does not allow a complete replacement of the communication backbone to provide the increased messaging performance. Thus, performance-enhancing software solutions are highly attractive. Our contribution is (1) an online compression mechanism that automatically selects the most appropriate compression algorithm to minimize the message round trip time; (2) a compression overload mechanism that ensures ample resources for other processes sharing the same CPU. We have integrated 11 well-known compression algorithms/configurations and tested them with production node traffic. In our target system, automatic message compression results is a 9.6% reduction of message round trip time. The selection procedure is fully automatic and does not require any manual intervention. The automatic behavior makes it particularly suitable for large systems where it is difficult to predict future system behavior. © 2016","Automatic compression; Feedback control; Message compression; Mobile systems; Network performance; Performance prediction"
"PF-Miner: A practical paired functions mining method for Android kernel in error paths","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960539328&doi=10.1016%2fj.jss.2016.02.007&partnerID=40&md5=4f7778d63ba62c5cb435124a6624aa2e","Generally, drivers have many errors to handle, and the functions called in the normal execution paths and error handling paths are in pairs, which are named as paired functions. However, some developers do not handle the errors completely as they forget or unaware of releasing the acquired resources, thus memory leaks and other potential problems can be easily introduced into the system. Therefore, it is highly valuable to automatically extract paired functions and detect violations for programmers. This paper proposes an efficient tool named PF-Miner, which can automatically extract paired functions and detect violations between normal execution paths and error handling paths from the source code of drivers with the data mining and statistical methods. We have evaluated PF-Miner on two different versions of Android kernel 2.6.39 and 3.10.0, and 81 bugs reported by PF-Miner in 2.6.39 have been fixed before the latest version 3.10.0. PF-Miner only needs about 150 s to analyze the source code of 3.10.0, and 983 violations have been detected from 546 paired functions which have been extracted. We have reported the top 51 violations as potential bugs to the developers, and 15 bugs have been confirmed so far. © 2016 Elsevier Inc.","Error path checking; Paired function mining; Violations detection"
"Using indirect coupling metrics to predict package maintainability and testability","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990044964&doi=10.1016%2fj.jss.2016.02.024&partnerID=40&md5=8d6a4dd702b4c636e959b4a8412f1f1f","Object-oriented systems are dynamic and have to be constantly maintained, or they become aged and irrelevant. Many costly software issues exist due to poorly designed systems and due to systems which are not easy to test or maintain because of poor designs. Martin's metrics (Martin, 2003) are well-known package design metrics that can be used in early stages of software development. However, since Martin's metrics only measure direct coupling, the authors believe that this limits their accuracy. In previous papers (Almugrin et al., 2014; Almugrin and Melton), we began with Martin's principles and used them to modify his coupling, instability and abstractness metrics based on direct and indirect coupling, respectively. In this paper, we present an experimental study to validate the modified global metrics by showing their relationship to maintainability and testability, and then we construct prediction models for these two external quality attributes. The study results indicate that the new metrics are very promising and lead to improved results. © 2016","Abstractness; Coupling; Instability"
"Extending OCCI for autonomic management in the cloud","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956600412&doi=10.1016%2fj.jss.2016.01.002&partnerID=40&md5=7adeb43bb9d43caf82e075cd854f57b4","Cloud Computing is an emerging paradigm involving different kinds of Information Technologies (IT) services. One of the major advantages of this paradigm resides on its pay-as-you-go economic model. To remain efficient, it becomes necessary to couple this model with autonomic computing. By autonomic computing we mean the ability of the system to automatically and dynamically manage its resources to respond to the requirements of the business based on Service Level Agreement (SLA). In this paper, we propose an extension for Open Cloud Computing Interface (OCCI) to support the different aspects of autonomic computing. This OCCI extension describes new Resources and Links that are generic Kinds and are specialized using OCCI Mixins. We introduce the Autonomic Manager as a special Resource that starting from a SLA instantiates all needed entities to automatically establish an infrastructure to enable an autonomic management of Cloud resources. The other introduced OCCI Resources are: Analyzer to analyze monitoring data based on specific analysis rules and Reconfiguration Manager to generate reconfiguration actions based on reconfiguration strategies. These Resources are linked using new defined Link entities. We describe herein, a real use case to show that we can apply our approach to the different levels of the Cloud (i.e., IaaS, PaaS and SaaS) at the same time. We present also the implementation details as well as evaluation preliminary results that are encouraging. © 2016 Elsevier Inc.","Autonomic computing; Cloud computing; OCCI"
"Distributed architecture for developing mixed-criticality systems in multi-core platforms","2017","Journal of Systems and Software","10.1016/j.jss.2016.08.088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993999933&doi=10.1016%2fj.jss.2016.08.088&partnerID=40&md5=4df34b4820ed687e485f2c8dbecce571","Partitioning is a widespread technique that enables the execution of mixed-criticality applications in the same hardware platform. New challenges for the next generation of partitioned systems include the use of multiprocessor architectures and distribution standards in order to open up this technique to a heterogeneous set of emerging scenarios (e.g., cyber-physical systems). This work describes a system architecture that enables the use of data-centric distribution middleware in partitioned real-time embedded systems based on a hypervisor for multi-core, and it focuses on the analysis of the available architectural configurations. We also present an application-case study to evaluate and identify the possible trade-offs among the different configurations. © 2016 Elsevier Inc.","Application virtualization; Middleware; Multi-core; Real-time systems and embedded systems"
"A formal framework for context-aware systems specification and verification","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949844080&doi=10.1016%2fj.jss.2015.11.035&partnerID=40&md5=b29512f9214ffc19ff22142edfd55e2f","Context-aware applications development is still a challenging issue due to their adaptive behavior complexity and uncertainty features. A conceptual framework, as an ideal reuse technique, is one of the most suitable solutions to simplify the development of such systems and overcome their development complexity. We aim in this paper to design a framework that promotes the ability to specify and verify context-aware systems to assist and facilitate designer's task. The objective is gained here by combining two complementary modeling techniques: Model-driven Engineering (MDE) and Formal Methods. Model-driven technique is adopted to design a modeling framework for context-aware systems. Nevertheless, this technique generally lacks formal semantics and it is unfit for model analysis. Therefore, we define a formal semantics to overcome these drawbacks. Moreover, we show how context-aware system's adaptive behavior can be verified according to their invariants by applying model-checking techniques. © 2015 Elsevier Inc.","Context-aware adaptive systems; Formal methods; Model-driven engineering"
"Securing native XML database-driven web applications from XQuery injection vulnerabilities","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986573073&doi=10.1016%2fj.jss.2016.08.094&partnerID=40&md5=2e1e0cf708f63ce8e8a806d016bab12b","Database-driven web applications today are XML-based as they handle highly diverse information and favor integration of data with other applications. Web applications have become the most popular way to deliver essential services to customers, and the increasing dependency of individuals on web applications makes them an attractive target for adversaries. The adversaries exploit vulnerabilities in the database-driven applications to craft injection attacks which include SQL, XQuery and XPath injections. A large amount of work has been done on identification of SQL injection vulnerabilities resulting in several tools available for the purpose. However, a limited work has been done so far for the identification of XML injection vulnerabilities and the existing tools only identify XML injection vulnerabilities which could lead to a specific type of attack. Hence, this work proposes a black-box fuzzing approach to detect different types of XQuery injection vulnerabilities in web applications driven by native XML databases. A prototype XQueryFuzzer is developed and tested on various vulnerable applications developed with BaseX as the native XML database. An experimental evaluation demonstrates that the prototype is effective against detection of XQuery injection vulnerabilities. Three new categories of attacks specific to XQuery, but not listed in OWASP are identified during testing. © 2016 Elsevier Inc.","Fuzz testing; Injection attacks; Vulnerability scanner; Web application security; XML injection; XPath injection"
"Design and implementation of a novel service management framework for IoT devices in cloud","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977271482&doi=10.1016%2fj.jss.2016.06.059&partnerID=40&md5=2beb768677d99e3505c2c23d24e024ed","With advent of new technologies, we are surrounded by several tiny but powerful mobile devices through which we can communicate with the outside world to store and retrieve data from the Cloud. These devices are considered as smart objects as they can sense the medium, collect data, interact with nearby smart objects, and transmit data to the cloud for processing and storage through internet. Internet of Things (IoT) create an environment for smart home, health care and smart business decisions by transmitting data through internet. Cloud computing, on the other hand leverages the capability of IoT by providing computation and storage power to each smart object. Researches and developers combine the cloud computing environment with that of IoT to reduce the transmission and processing cost in the cloud and to provide better services for processing and storing the realtime data generated from those IoT devices. In this paper, a novel framework is designed for the Cloud to manage the realtime IoT data and scientific non-IoT data. In order to demonstrate the services in Cloud, real experimental result of implementing the Docker container for virtualization is introduced to provide Software as a Service (SaaS) in a hybrid cloud environment. © 2016 Elsevier Inc.","Cloud computing; Docker; IoT; SaaS"
"Self-adaptation in software-intensive cyber–physical systems: From system goals to architecture configurations","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960976259&doi=10.1016%2fj.jss.2016.02.028&partnerID=40&md5=17808c5482bbf8923bd5b5b46a557049","Design of self-adaptive software-intensive cyber–physical systems (siCPS) operating in dynamic environments is a significant challenge when a sufficient level of dependability is required. This stems partly from the fact that the concerns of self-adaptivity and dependability are to an extent contradictory. In this paper, we introduce IRM-SA (Invariant Refinement Method for Self-Adaptation)—a design method and associated formally grounded model targeting siCPS—that addresses self-adaptivity and supports dependability by providing traceability between system requirements, distinct situations in the environment, and predefined configurations of system architecture. Additionally, IRM-SA allows for architecture self-adaptation at runtime and integrates the mechanism of predictive monitoring that deals with operational uncertainty. As a proof of concept, it was implemented in DEECo, a component framework that is based on dynamic ensembles of components. Furthermore, its feasibility was evaluated in experimental settings assuming decentralized system operation. © 2016","Cyber–physical systems; Dependability; Self-adaptivity"
"Big data mining with parallel computing: A comparison of distributed and MapReduce methodologies","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986596877&doi=10.1016%2fj.jss.2016.09.007&partnerID=40&md5=971616ef6bf2734a4c9f85c97952b29c","Mining with big data or big data mining has become an active research area. It is very difficult using current methodologies and data mining software tools for a single personal computer to efficiently deal with very large datasets. The parallel and cloud computing platforms are considered a better solution for big data mining. The concept of parallel computing is based on dividing a large problem into smaller ones and each of them is carried out by one single processor individually. In addition, these processes are performed concurrently in a distributed and parallel manner. There are two common methodologies used to tackle the big data problem. The first one is the distributed procedure based on the data parallelism paradigm, where a given big dataset can be manually divided into n subsets, and n algorithms are respectively executed for the corresponding n subsets. The final result can be obtained from a combination of the outputs produced by the n algorithms. The second one is the MapReduce based procedure under the cloud computing platform. This procedure is composed of the map and reduce processes, in which the former performs filtering and sorting and the later performs a summary operation in order to produce the final result. In this paper, we aim to compare the performance differences between the distributed and MapReduce methodologies over large scale datasets in terms of mining accuracy and efficiency. The experiments are based on four large scale datasets, which are used for the data classification problems. The results show that the classification performances of the MapReduce based procedure are very stable no matter how many computer nodes are used, better than the baseline single machine and distributed procedures except for the class imbalance dataset. In addition, the MapReduce procedure requires the least computational cost to process these big datasets. © 2016 Elsevier Inc.","Big data; Cloud computing; Data mining; Distributed; MapReduce; Parallel computing"
"A product-line model-driven engineering approach for generating feature-based mobile applications","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991221546&doi=10.1016%2fj.jss.2016.09.049&partnerID=40&md5=ed8bb39053d6229343132f57b665dd57","A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. The current industrial practice is to develop and maintain these variants separately. Any potential change has to be applied across variants manually, which is neither efficient nor scalable. We consider the problem of supporting multiple platforms as a ‘software product-line engineering’ problem. The paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. Specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. We develop a tool MOPPET that automates the proposed approach. Finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time. © 2016 Elsevier Inc.","Feature model; Mobile applications; Software product-line engineering"
"Static analysis by abstract interpretation of functional properties of device drivers in TinyOS","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979707901&doi=10.1016%2fj.jss.2016.07.030&partnerID=40&md5=a49ad648d1c3dcfa182c057b5d544815","In this paper, we present a static analysis by Abstract Interpretation of device drivers developed in the TinyOS operating system, which is considered as the de facto system in wireless sensor networks. We focus on verifying user-defined functional properties describing safety rules that programs should obey in order to interact correctly with the hardware. Our analysis is sound by construction and can prove that all possible execution paths follow the correct interaction patterns specified by the functional property. The soundness of the analysis is justified with respect to a preemptive execution model where interrupts can occur during execution depending on the configuration of specific hardware registers. The proposed solution performs a modular analysis that analyzes every interrupt independently and aggregates their results to over-approximate the effect of preemption. By doing so, we avoid reanalyzing interrupts in every context where they are enabled which improves considerably the scalability of the solution. A number of partitioning techniques are also presented in order to track precisely some crucial information, such as the hardware state and the tasks queue. We have performed several experiments on real-world TinyOS device drivers of the ATmega128 MCU and promising results demonstrate the effectiveness of our analysis. © 2016 Elsevier Inc.","abstract interpretation; device drivers; Static analysis; wireless sensor networks"
"Continuous deployment of software intensive products and services: A systematic mapping study","2017","Journal of Systems and Software","10.1016/j.jss.2015.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957818901&doi=10.1016%2fj.jss.2015.12.015&partnerID=40&md5=d1237921af6321ad5318118207a87143","The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies. © 2016 Elsevier Inc.","Continuous deployment; Software development; Systematic mapping study"
"Cloud migration process—A survey, evaluation framework, and open challenges","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978904039&doi=10.1016%2fj.jss.2016.06.068&partnerID=40&md5=241d111dc9e675bae72ca1f16feb48cf","Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours. © 2016 Elsevier Inc.","Cloud computing; Cloud migration; Evaluation framework; Legacy application; Migration methodology; Process model"
"Costs and obstacles encountered in technical debt management – A case study","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001578612&doi=10.1016%2fj.jss.2016.07.008&partnerID=40&md5=e7685fa76a96747bac7e2b92327b5e10","Technical debt (TD) is a metaphor that characterizes the effect of immature software artifacts. The costs and benefits of TD, along with the uncertainty of its interest repayment, provide leverage for software managers, but also could lead to problems such as increased costs and lower quality during maintenance if it is left unattended. Therefore, effective approaches to TD management are needed by software practitioners. As one of our series of studies on TD management, this study was originally designed to reveal the cost side of explicit TD management. The study design required applying a simple proposed TD management approach to the subject project and then collecting cost information. Not surprisingly, we observed some deviation of the actual management process from our proposed one, which provided us with an opportunity to investigate the obstacles to explicitly managing TD. We also identified some costs and cost patterns related to TD management. Based on the insights gained from this study, we further propose strategies to overcome the obstacles and improve the application of TD management in practice. © 2016 Elsevier Inc.","Case study; Project management; Software maintenance; Software technical debt; Technical debt management"
"PHash: A memory-efficient, high-performance key-value store for large-scale data-intensive applications","2017","Journal of Systems and Software","10.1016/j.jss.2016.09.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991236913&doi=10.1016%2fj.jss.2016.09.047&partnerID=40&md5=0afb2a47fc99ad2b15d76a940949aa9d","Large-scale data-intensive web services are evolving faster than ever, accelerating global growth in data usage and traffic at a rapid rate. This rapid growth is demanding the expansion of high-cost data infrastructures, which also underscores the industry's need for cost-effective, high-performance distributed key-value stores. Designing key-value stores often involves a trade-off between performance and memory usage. For example, many previous studies focusing on minimizing the memory usage have developed on-disk indexing schemes, leading to lower performance. An alternative design based on in-memory indexing allows better performance, but at the expense of greater memory usage. This paper proposes a novel key-value store called PHash (Packed Hash) based on an advanced design of index and data structures that ensures both high performance and small memory usage. These advantages make the proposed scheme a better fit for processing demanding workloads in large-scale data-intensive applications. Compared to the best-performing competitor, FAWN-DS, the proposed scheme significantly reduces the memory consumption (bytes per key-value) by 83% and improves the GET throughput by up to 27.3% while the PUT throughput decreases by 12.6%. In particular, the GET performance of the proposed scheme reaches up to 99.4% of the optimal performance of the raw SSD (Solid State Drive). © 2016 Elsevier Inc.","Datastore; Distributed storage; Key-value store; NoSQL; Solid state drives"
"A mixed integer linear programming optimization approach for multi-cloud capacity allocation","2017","Journal of Systems and Software","10.1016/j.jss.2016.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991810740&doi=10.1016%2fj.jss.2016.10.001&partnerID=40&md5=074b59f1814b1918958b8040dcca8a51","The large success of the Cloud computing, its strong impact on the ICT world and on everyday life testifies the maturity and effectiveness this paradigm achieved in the last few years. Presently, the Cloud market offers a multitude of heterogeneous solutions. However, despite the undeniable advantages, Cloud computing introduced new issues and challenges. In particular, the heterogeneity of the available Cloud services and their pricing models makes the identification of a configuration that minimizes the operating costs of a Cloud application, guaranteeing at the same time the Quality of Service, a challenging task. This situation requires new processes and models to design software architectures and predict costs and performance considering together the large variability in price models and the intrinsic dynamism and multi-tenancy of the Cloud environments. This work aims at providing a novel mathematical approach to this problem presenting a queuing theory based Mixed Integer Linear Program (MILP) to find a promising multi-cloud configuration for a given software architecture. The effectiveness of the proposed model has been favorably evaluated against first principle heuristics currently adopted by practitioners. Furthermore, the configuration returned by the model has been also used as initial solution for a local-search based optimization engine, which exploits more accurate but time-consuming performance models. This combined approach has been shown to improve the quality of the returned solutions by 37% on average and reducing the overall search time by 50% with respect to state-of-the-art heuristics based on tiers utilization thresholds. © 2016 Elsevier Inc.","MILP; Multi-cloud capacity allocation; Optimization"
"Analytical decisional model for latency aware publish/subscribe systems on MANET","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949451045&doi=10.1016%2fj.jss.2015.11.003&partnerID=40&md5=9cbe75c7029c63176acc3ab01cb6d7db","This paper proposes an analytical model for latency aware publish/subscribe systems on mobile ad hoc networks. The proposed approach combines both proactive and reactive statistical analysis. On the one hand, the reactive analysis, suitable for multimedia applications, detects failures by approximating latency series with the Gumbel distribution. On the other hand, the proactive analysis, suitable for crisis management applications, forecasts failures occurrence relying on Auto Regression or Auto Regressive Integrated Moving Average Formulas. Finally, a hybrid analysis was proposed by dynamically switching from reactive to predictive forms of analysis whenever quality of service violations are noticed. In order to extract failure cause, we refer to the correlation method once failure was detected or predicted. Simulations done under different scenarios proved the efficiency and accuracy of the proposed scheme. © 2015 Elsevier Inc.","Analysis; MANET; Publish/subscribe"
"Adjusting software revenue and pricing strategies in the era of cloud computing","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984655296&doi=10.1016%2fj.jss.2016.08.070&partnerID=40&md5=4e945d70f20afa7e3b5d7fb1db60987f","Recent research has recognized cloud computing as a new paradigm of servitization in which software products are offered based on service contracts. Thus, instead of selling software licenses, software vendors can rent software as a service to customers. However, it is still unclear how software providers can use software renting as a competitive strategy in the software market. Based on 37 interviews with software professionals from five case firms, this paper focuses on the connection between competitive forces and the factors influencing the selection of a pricing model. The findings indicate that servitization of the software offering makes it possible to adjust revenue and pricing strategies relative to market competition. Depending on the competitive situation in the market, firms apply mixed revenue models, or else a hybrid pricing mechanism, to protect their business against rivalry and substitutes. The software renting model has several advantages which significantly help software vendors to expand their business opportunities. However, in some cases, powerful customers are able to limit the revenue and pricing options. The findings also indicate that software renting is related to cost leadership and differentiation strategies, whereas software licensing is linked to a focus strategy. © 2016 Elsevier Inc.","Cloud computing; Competitive strategy; SaaS; Servitization; Software pricing"
"Technical debt and system architecture: The impact of coupling on defect-related activity","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027922641&doi=10.1016%2fj.jss.2016.06.007&partnerID=40&md5=d4284ce8afa13e6d20115d0d13d58fa1","Technical Debt is created when design decisions that are expedient in the short term increase the costs of maintaining and adapting this system in future. An important component of technical debt relates to decisions about system architecture. As systems grow and evolve, their architectures can degrade, increasing maintenance costs and reducing developer productivity. This raises the question if and when it might be appropriate to redesign (“refactor”) a system, to reduce what has been called “architectural debt”. Unfortunately, we lack robust data by which to evaluate the relationship between architectural design choices and system maintenance costs, and hence to predict the value that might be released through such refactoring efforts. We address this gap by analyzing the relationship between system architecture and maintenance costs for two software systems of similar size, but with very different structures; one has a “Hierarchical” design, the other has a “Core-Periphery” design. We measure the level of system coupling for the 20,000+ components in each system, and use these measures to predict maintenance efforts, or “defect-related activity.” We show that in both systems, the tightly-coupled Core or Central components cost significantly more to maintain then loosely-coupled Peripheral components. In essence, a small number of components generate a large proportion of system costs. However, we find major differences in the potential benefits available from refactoring these systems, related to their differing designs. Our results generate insight into how architectural debt can be assessed by understanding patterns of coupling among components in a system. © 2016 Elsevier Inc.","Complexity; Modularity; Software architecture; Software maintenance; Technical debt"
"Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989809865&doi=10.1016%2fj.jss.2016.06.102&partnerID=40&md5=108cee85753b141144e48b4ac91f3cd6","More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service. © 2016 Elsevier Inc.","Autonomic computing; Distributed and concurrent architecture; Elastic computing; Patterns; Performance; Reliability; Software architecture"
"Combining instance selection for better missing value imputation","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984848051&doi=10.1016%2fj.jss.2016.08.093&partnerID=40&md5=518093b2dbb5522ffe87ff69d0ba5340","In practice, the data collected from data mining usually contain some missing values. Imputation is the process of replacing the missing values in incomplete datasets. It is usually based on providing estimations for missing values by reasoning from the observed data. Consequently, the effectiveness of missing value imputation is heavily dependent on the observed data (or complete data) in the incomplete datasets. The objective of this study is to investigate the effect of performing instance selection to filter out some noisy data (or outliers) from a given dataset on the imputation task. Specifically, four different processes for combining instance selection and missing value imputation are proposed and compared in terms of data classification. The experimental results based on 29 datasets containing categorical, numerical, and mixed attribute types of data show that the process of performing instance selection first and imputation second allows the k-NN and SVM classifiers to outperform the other processes over the categorical and numerical datasets. For the mixed type of datasets, k-NN performs the best when instance selection is performed again on the datasets produced by the second process. Finally, some specific decision rules about when to employ which process are also provided for future research. © 2016 Elsevier Inc.","Data mining; Incomplete data; Instance selection; Missing value imputation"
"Group development and group maturity when building agile teams: A qualitative and quantitative investigation at eight large companies","2017","Journal of Systems and Software","10.1016/j.jss.2016.11.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996563490&doi=10.1016%2fj.jss.2016.11.024&partnerID=40&md5=081cb4cf2c4c18acf2c1bba448ab998d","The agile approach to projects focuses more on close-knit teams than traditional waterfall projects, which means that aspects of group maturity become even more important. This psychological aspect is not much researched in connection to the building of an “agile team.” The purpose of this study is to investigate how building agile teams is connected to a group development model taken from social psychology. We conducted ten semi-structured interviews with coaches, Scrum Masters, and managers responsible for the agile process from seven different companies, and collected survey data from 66 group-members from four companies (a total of eight different companies). The survey included an agile measurement tool and the one part of the Group Development Questionnaire. The results show that the practitioners define group developmental aspects as key factors to a successful agile transition. Also, the quantitative measurement of agility was significantly correlated to the group maturity measurement. We conclude that adding these psychological aspects to the description of the “agile team” could increase the understanding of agility and partly help define an “agile team.” We propose that future work should develop specific guidelines for how software development teams at different maturity levels might adopt agile principles and practices differently. © 2016 Elsevier Inc.","Agile processes; Empirical study; Group psychology; Maturity; Measurement"
"Supporting pattern-based dependability engineering via model-driven development: Approach, tool-support and empirical validation","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988808560&doi=10.1016%2fj.jss.2016.09.027&partnerID=40&md5=12001608a3742e664c7ff494a41f534c","Safety-critical systems require a high level of safety and integrity. Therefore, generating such systems involves specific software building processes. Many domains are not traditionally involved in these types of software problems and must adapt their current processes accordingly. Typically, such requirements are developed ad hoc for each system, preventing further reuse beyond the domain-specific boundaries. This paper proposes a solution for software system development based on the reuse of dedicated subsystems, i.e., so-called dependability patterns that have been pre-engineered to adapt to a specific domain. We use Model-Driven Engineering (MDE) to describe dependability patterns and a methodology for developing dependable software systems using these patterns. Moreover, we describe an operational architecture for development tools to support the approach. An empirical evaluation of the proposed approach is presented through its practical application to a case study in the railway domain, which has strong dependability requirements, to support a pattern-based development approach. This case study is followed by a survey to better understand the perceptions of practitioners regarding our approach. © 2016 Elsevier Inc.","Dependability; Meta-modeling; Model driven engineering; Patterns; Safety; System engineering"
"Adding data analytics capabilities to scaled-out object store","2016","Journal of Systems and Software","10.1016/j.jss.2016.07.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980315574&doi=10.1016%2fj.jss.2016.07.029&partnerID=40&md5=8576501d93134862cf84ab1fbc5ac92b","This work focuses on enabling effective data analytics on scaled-out object storage systems. Typically, applications perform MapReduce computations by first copying large amounts of data to a separate compute cluster (i.e. a Hadoop cluster). However; this approach is not very efficient considering that storage systems can host hundreds of petabytes of data. Network bandwidth can be easily saturated and the overall energy consumption would increase during large-scale data transfer. Instead of moving data between remote clusters; we propose the implementation of a data analytics layer on an object-based storage cluster to perform in-place MapReduce computation on existing data. The analytics layer is tied to the underlying object store, utilizing its data redundancy and distribution policies across the cluster. We implemented this approach with Ceph object storage system and Hadoop, and conducted evaluations with various benchmarks. Performance evaluations show that initial data copy performance is improved by up to 96% and the MapReduce performance is improved by up to 20% compared to the stock Hadoop implementation. © 2016 Elsevier Inc.","Attribute-based storage; In-situ data analytics; MapReduce; Object storage"
"Achieving resilience in distributed software systems via self-reconfiguration","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971597171&doi=10.1016%2fj.jss.2016.05.038&partnerID=40&md5=985ce8335f8302f1ebb22c78c8d46c52","Improvements in mobile networking combined with the ubiquitous availability and adoption of low-cost development boards have enabled the vision of mobile platforms of Cyber-Physical Systems (CPS), such as fractionated spacecraft and UAV swarms. Computation and communication resources, sensors, and actuators that are shared among different applications characterize these systems. The cyber-physical nature of these systems means that physical environments can affect both the resource availability and software applications that depend on resource availability. While many application development and management challenges associated with such systems have been described in existing literature, resilient operation and execution have received less attention. This paper describes our work on improving runtime support for resilience in mobile CPS, with a special focus on our runtime infrastructure that provides autonomous resilience via self-reconfiguration. We also describe the interplay between this runtime infrastructure and our design-time tools, as the later is used to statically determine the resilience properties of the former. Finally, we present a use case study to demonstrate and evaluate our design-time resilience analysis and runtime self-reconfiguration infrastructure. © 2016 Elsevier Inc.","Algorithms; Distributed systems; Performance evaluation; Resilient systems; Wireless computing clusters"
"Towards uniform management of multi-layered cloud services by applying model-driven development","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955236756&doi=10.1016%2fj.jss.2016.01.001&partnerID=40&md5=5d1f8aba84909a0e6ed937f27834970c","Cloud Computing started by renting computing infrastructures in form of virtual machines, which include hardware resources such as memory and processors. However, due to its popularity it gave birth to Everything-as-a-Service concept, where each service can comprise large variety of software/hardware elements. Although having the same concept, services represent complex environments that have to be deployed and managed by a provider using individual tools. The tools are usually used manually or specifically integrated for a single service. This requires changing an entire deployment procedure in case the service gets modified, while additionally limiting consolidation capabilities due to tight service integration. In this paper, we utilize Model-Driven Development approach for managing arbitrary Cloud services. We define a metamodel of a Cloud service called CoPS, which describes a service as a composition of software/hardware elements by using three sequential models, namely Component, Product and Service. We also present an architecture of a Cloud Management System (CMS) used for automatic service management, which transforms the models from an abstract representation to an actual deployment. The approach is validated by realizing four real-world use cases using a prototype implementation. Finally, we evaluate its consolidation capabilities by simulating resource consumption and deployment time. © 2016 Elsevier Inc.","Cloud computing; Cloud management system; Cloud service model"
"Kernel mechanisms with dynamic task-aware scheduling to reduce resource contention in NUMA multi-core systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982182651&doi=10.1016%2fj.jss.2016.08.038&partnerID=40&md5=d723f7b2f4291c824fcd522cba677d4b","In NUMA multi-core systems, processors may share different levels of system resources such as the bus, the memory or the cache. When processors attempt to access shared resources simultaneously, resource contention leads to a decrease in performance. This paper proposes a dynamic task-aware scheduling mechanism that reduces resource contention in NUMA multi-core systems. To avoid resource contention, processor cores that share resources are arranged to run complementary tasks. Tasks are classified as either compute-bound or memory-bound, during the run time. Processors are also classified as either compute-bound or memory-bound for running different types of tasks. The proposed dynamic task-aware scheduler then dispatches tasks to run on suitable processors, according to their current types. If power is to be conserved, the processors that are responsible for running memory-bound tasks can be set to the lowest processor frequency to reduce power consumption. Processors can also be set to the highest processor frequency for running compute-bound tasks efficiently. These mechanisms are implemented in the Linux kernel. The experiment results demonstrate that the proposed dynamic task-aware scheduling mechanism improves system performance and reduces power consumption by reducing resource contention among processor cores. © 2016 Elsevier Inc.","Linux kernel; Multi-core; NUMA; Resource contention; Scheduling"
"A license to kill – Improving UCSD in Agile development","2017","Journal of Systems and Software","10.1016/j.jss.2016.01.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957895226&doi=10.1016%2fj.jss.2016.01.024&partnerID=40&md5=3f906719b39c30f6c7d07821562ccd4c","Agile development processes, such as Scrum, focus on communication, developer collaboration and delivery of working software early and continuously. User-centered systems design (UCSD) is a process emphasizing usability and the user experience throughout the system life cycle. It highlights the UCSD activities: understanding the context of use, iterative prototyping to explore the design space and active collaboration with users throughout the software development. Agile processes are by many assumed to address similar issues as UCSD, hence, by applying Agile processes the systems would become usable for the end-users and their user experience should improve. This paper discusses and interprets findings on UCSD activities in Agile projects in practice, that are analyzed according to the fundamental principles from the Agile manifesto. We show that Agile development has much to gain from integrating UCSD, and give guidance on how to integrate UCSD in Agile processes. User experience (UX) professionals need a more explicit role in the Agile projects and more authority. We recommend that they receive a “license to kill” design suggestions that are not perceived as useful by the users. © 2016","Agile development; Scrum; User-centered system design"
"Context-awareness in the software domain—A semantic web enabled modeling approach","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962510240&doi=10.1016%2fj.jss.2016.02.023&partnerID=40&md5=1c6240f3565d26c4b0c8ea5f5dcba3d0","Recent years have witnessed rapid advances in the use of contextual information in ubiquitous and ambient computing. Such information improves situated cognition and awareness as well as stakeholders’ usage experience. While domains such as Web 3.0 – the next generation of the web – have made context-awareness a main requirement of their solution space, the software engineering domain still lacks the same rate of adoption. In our research, we introduce an ontology based context-aware meta-model that takes advantage of Semantic Web technologies to capture and formalize context information. Providing such formal context representation allows us to make context information an integrated and reusable part of the software engineering domain. We present several case studies related to the software evolution domain to illustrate the benefit of sharing and reusing context for various software engineering tasks, such as mentor recommendation, code search, and result ranking. © 2016 Elsevier Ltd","Context-awareness; Meta-modeling; Semantic Web"
"GTCharge: A game theoretical collaborative charging scheme for wireless rechargeable sensor networks","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982150696&doi=10.1016%2fj.jss.2016.08.046&partnerID=40&md5=445433e9c3180c8ebbd7dd4f8f504d55","Collaborative charging schemes are indeed helpful for energy replenishment. However, classic and traditional collaborative charging schemes are still suffering from a series of severe problems, which are almost neglected. The lack of homogeneity and dynamic charging decisions on collaborative charging schemes in Wireless Rechargeable Sensor Networks (WRSN) deteriorate the charging efficiency. To enhance charging performance, especially in terms of charging efficiency, in this paper, a game theoretical collaborative charging scheme, namely GTCharge is devised. The charging process is converted into a collaborative game taken between wireless charging vehicles (WCVs). We investigate the functionalities of contribution degree, charging priority and profits. Then GTCharge is demonstrated in detail, in which each WCV seeks for the maximum profit when fulfilling charging tasks. The conditions including all WCVs’ charging strategies are proven to reach a Nash Equilibrium point. Finally, extensive simulations are conducted to show the advantages of the proposed scheme. Simulation results demonstrate the merits of the proposed scheme in terms of charging efficiency. © 2016 Elsevier Inc.","Collaborative charging; Game theory; Wireless rechargeable sensor networks"
"Software cybernetics in BPM: Modeling software behavior as feedback for evolution by a novel discovery method based on augmented event logs","2017","Journal of Systems and Software","10.1016/j.jss.2016.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961840046&doi=10.1016%2fj.jss.2016.03.013&partnerID=40&md5=ba13135879bd55149442d95d48115a5b","Business Process Management (BPM) is a quickly developing management theory in recent years. The goal of BPM is to improve corporate performance by managing and optimizing the businesses process in and among enterprises. The goal is easier to achieve with the closed-loop feedback mechanism from business process execution to redesign in BPM life cycle, where the business process itself and the set of activities in BPM are viewed as a controlled object and a controller respectively. In this feedback control system, process mining plays an important role in generating feedback of process execution for redesign. However, the existing discovery methods cannot mine certain special structures from execution logs (e.g., implicit dependency, implicit place and short loops) correctly and their mining efficiencies cannot meet the requirements of online process mining. In this paper, we propose a novel discovery method to overcome these challenges based on a kind of augmented event log that will also bring new research directions for process discovery. A case study is presented for introducing how the mined model can be used in business process evolution. Results of experiments are described to show the improvements of the proposed algorithm compared with others. © 2016","Petri nets; Process discovery; Software cybernetics"
"How do software development teams manage technical debt? – An empirical study","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969206783&doi=10.1016%2fj.jss.2016.05.018&partnerID=40&md5=ddad6957a54013c53c4bb80ba5c89d7b","Technical debt (TD) is a metaphor for taking shortcuts or workarounds in technical decisions to gain short-term benefit in time-to-market and earlier software release. In this study, one large software development organization is investigated to gather empirical evidence related to the concept of technical debt management (TDM). We used the exploratory case study method to collect and analyze empirical data in the case organization by interviewing a total of 25 persons in eight software development teams. We were able to identify teams where the current strategy for TDM was only to fix TD when necessary, when it started to cause too much trouble for development. We also identified teams where the management had a systematic strategy to identify, measure and monitor TD during the development process. It seems that TDM can be associated with a similar maturity concept as software development in general. Development teams may raise their maturity by increasing their awareness and applying more advanced processes, techniques and tools in TDM. TDM is an essential part of sustainable software development, and companies have to find right approaches to deal with TD to produce healthy software that can be developed and maintained in the future. © 2016 The Authors","Exploratory case study; Technical debt; Technical debt management"
"Self-adaptation of multi-agent systems in dynamic environments based on experience exchanges","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988526116&doi=10.1016%2fj.jss.2016.09.025&partnerID=40&md5=a54d7ea726099c07d373ba947ee7d398","In complex and changing environments, MASs (Multi-Agent Systems) have to be faced with many challenges and their adaptivity can hardly emerge from the localized, phased and timely behaviors and decisions of agents. In this paper, a self-adaptation mechanism based on experience exchanges is proposed for MASs. In the mechanism, agents can dynamically induce environmental constraints and behavior patterns for generating new adaptation policies from two types of experiences exchanged by others, i.e., immediate and retrospective experiences, and then build action matrix based on the adaptation policies for determining their behaviors that can positively contribute to the improvement of global adaptivity of MASs. In the end, an example MAS system simulating a delivery system is described and experiments are conducted on the system to validate the mechanism for self-adaptive MASs. © 2016","Adaptive systems; Behavior pattern; Distributed decision-making; Multi-agent systems"
"Requirement-driven evolution in software product lines: A systematic mapping study","2016","Journal of Systems and Software","10.1016/j.jss.2016.08.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986877763&doi=10.1016%2fj.jss.2016.08.053&partnerID=40&md5=2393c4b7788eef213b578fee8927bea1","CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products). CONCLUSION. Analyses of the results indicate that “Solution proposals” are the most common type of contribution (31%). Regarding the evolution activity, “Implement change” (43%) and “Analyze and plan change” (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included. © 2016 Elsevier Inc.","Evolution; Software product lines; Systematic mapping study"
"Usage-based chunking of Software Architecture information to assist information finding","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988700608&doi=10.1016%2fj.jss.2016.09.009&partnerID=40&md5=7792b006cda2a8b65633cefad630c85c","One of the key problems with Software Architecture Documents (ADs)2 Software Architecture Document (AD). is the difficulty of finding information required from them. Most existing studies focus on the production of ADs or Architectural Knowledge (AK)3 Architectural Knowledge (AK)., to allow them to support information finding. However, there has been little focus placed on the consumption of ADs. To address this, we postulate the existence of a concept of “usage-based chunks” of architectural information discoverable from consumers’ usage of ADs when they engage in information-seeking tasks. In a set of user studies, we have found evidence that such usage-based chunks exist and that useful chunks can be identified from one type of usage data, namely, consumer's ratings of sections of ADs. This has implications for tool design to support the effective reuse of AK. © 2016 Elsevier Inc.","Information finding; Software architecture document; Task; Usage-based chunking"
"Teamwork quality and project success in software development: A survey of agile development teams","2016","Journal of Systems and Software","10.1016/j.jss.2016.09.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991498357&doi=10.1016%2fj.jss.2016.09.028&partnerID=40&md5=fdaeec7bdc4008ae0636a1bb3365b49f","Small, self-directed teams are central in agile development. This article investigates the effect of teamwork quality on team performance, learning and work satisfaction in agile software teams, and whether this effect differs from that of traditional software teams. A survey was administered to 477 respondents from 71 agile software teams in 26 companies and analyzed using structural equation modeling. A positive effect of teamwork quality on team performance was found when team members and team leaders rated team performance. In contrast, a negligible effect was found when product owners rated team performance. The effect of teamwork quality on team members´ learning and work satisfaction was strongly positive, but was only rated by the team members. Despite claims of the importance of teamwork in agile teams, this study did not find teamwork quality to be higher than in a similar survey on traditional teams. The effect of teamwork quality on team performance was only marginally greater for the agile teams than for the traditional teams. © 2016 The Authors","Agile development; Learning; Project management; Team performance; Teamwork quality; Work Satisfaction"
"Evaluating refactorings for spreadsheet models","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971441815&doi=10.1016%2fj.jss.2016.04.043&partnerID=40&md5=3e3b59f67219342406adf481fe54f4d5","Software refactoring is a well-known technique that provides transformations on software artifacts with the aim of improving their overall quality. We have previously proposed a catalog of refactorings for spreadsheet models expressed in the ClassSheets modeling language, which allows us to specify the business logic of a spreadsheet in an object-oriented fashion. Reasoning about spreadsheets at the model level enhances a model-driven spreadsheet environment where a ClassSheet model and its conforming instance (spreadsheet data) automatically co-evolves after applying a refactoring at the model level. Research motivation was to improve the model and its conforming instance: the spreadsheet data. In this paper we define such refactorings using previously proposed evolution steps for models and instances. We also present an empirical study we designed and conducted in order to confirm our original intuition that these refactorings have a positive impact on end-user productivity, both in terms of effectiveness and efficiency. The results are not only presented in terms of productivity changes between refactored and non-refactored scenarios, but also the overall user satisfaction, relevance, and experience. In almost all cases the refactorings improved end-users productivity. Moreover, in most cases users were more engaged with the refactored version of the spreadsheets they worked with. © 2016 Elsevier Inc. All rights reserved.","Empirical study; Model-driven engineering; Software refactoring; Spreadsheets"
"Unit effects in software project effort estimation: Work-hours gives lower effort estimates than workdays","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962427440&doi=10.1016%2fj.jss.2016.03.048&partnerID=40&md5=7d02d67e8d1799bfb49c7159da570952","Software development effort estimates are typically expert judgment-based and too low to reflect the actual use of effort. Our goal is to understand how the choice of effort unit affects expert judgement-based effort estimates, and to use this knowledge to increase the realism of effort estimates. We conducted two experiments where the software professionals were randomly instructed to estimate the effort of the same projects in work-hours or in workdays. In both experiment, the software professionals estimating in work-hours had much lower estimates (on average 33%-59% lower) than those estimating in workdays. We argue that the unitosity effect - i.e., that we tend to infer information about the quantity from the choice of unit - is the main explanation for the large difference in effort estimates. A practical implication of the unit effect is that, in contexts where there is a tendency toward effort under estimation, the instruction to estimate in higher granularity effort units, such as workdays instead of work-hours, is likely to lead to more realistic effort estimates. © 2016 Elsevier Inc. All rights reserved.","Effort estimation; Effort unit; Judgment bias"
"Privacy in mobile participatory sensing: Current trends and future challenges","2016","Journal of Systems and Software","10.1016/j.jss.2015.03.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926670968&doi=10.1016%2fj.jss.2015.03.067&partnerID=40&md5=4d0007fa543dd7d8cf6600c5ef30c373","Mobile participatory sensing has opened the doors to numerous sensing scenarios that were unimaginable few years ago. In absence of protection mechanisms, most of these applications may however endanger the privacy of the participants and end users. In this manuscript, we highlight both sources and targets of these threats to privacy and analyze how they are addressed in recent privacy-preserving mechanisms tailored to the characteristics of participatory sensing. We further provide an overview of current trends and future research challenges in this area. © 2015 Elsevier Inc. All rights reserved.","Mobile sensing; Participatory sensing; Privacy"
"A machine learning based software process model recommendation method","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84967329239&doi=10.1016%2fj.jss.2016.05.002&partnerID=40&md5=b8ee9c307fae3ef0d564d435a6f6c55a","Among many factors that influence the success of a software project, the software process model employed is an essential one. An improper process model will be time consuming, error-prone and cost expensive, and further lower the quality of software. Therefore, how to choose an appropriate software process model is a very important problem for software development. Current works focus on the selection criteria and often lead to subjective results. In this paper, we propose a software process model recommendation method, to help project managers choose the most appropriate software process model for a new project at an early stage of development process according to historical software engineering data. The proposed method casts the process model recommendation into a classification problem. It first evaluates the different combinations of the alternative classification and attribute selection algorithms, and the best one is used to build the recommendation model with historical software engineering data; then, the constructed recommendation model is used to predict process models for a new software project with only a few data. We also analyze the mutual impacts between process models and different types of project factors, to further help managers locate the most suitable process model. We found process models are also responsible for defect count, defect severity and software change. Experiments on the data sets from 37 different development teams of different countries show that the average recommendation accuracy of our method reaches up to 82.5%, which makes it potentially useful in practice. © 2016 Elsevier Inc. All rights reserved.","Impact analysis; Machine learning; Model recommendation; Software process model; Software project management"
"A roadmap for scalable agent organizations in the Internet of Everything","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960887351&doi=10.1016%2fj.jss.2016.01.022&partnerID=40&md5=d8c6605627f5365155e55a1be15a2fc2","Computing is increasingly ubiquitous, with everyday items including smartphones, cars, clothes and household appliances gaining increasingly sophisticated computing and communication capacities. With the development of the Internet of Things, it is just a matter of time before devices have to collaborate and compete with each other, in order to provide better services to mankind. These embedded software systems are increasingly autonomous and connected, and can thus be modeled as multiagent systems (MAS). Only 30 years ago it was science fiction that over a billion people will exchange billions of e-mails on a daily basis. Today a scenario of millions of collaborating agents sometimes embedded in gadgets and appliances, sometimes in form of networked and big data services, may also sound futuristic. However given the current rate of development in electronics, we will soon have to manage large scale MAS where millions of agents exist, collaborate and compete. Organization theory provides the necessary methodology to approach complex systems in order to design, implement and strategically manage them towards success. In this paper a state-of-the-art on organizational design techniques for large-scale MAS will be presented, missing advancements will be identified and a roadmap for future developments and application scenarios will be provided. © 2016 Elsevier Inc. All rights reserved.","Internet of Things; Large-scale multiagent system; Organizational design"
"A new framework for implementing identity-based cryptosystems","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966392060&doi=10.1016%2fj.jss.2016.04.059&partnerID=40&md5=28bf651af70d88f71d6b36920e088ccf","Identity-Based Encryption (IBE) suffers from the problem of trust in the Private Key Generator (PKG), which translates into the ability of the PKG to produce and distribute multiple private keys or multiple copies of a single key without knowledge of the genuine user. This problem makes the deployment of these systems limited to areas where trust in the PKG must have a high level. Many works addressed this problem and proposed a wide range of key generation protocols which grew from simple protocols run between user and PKG to complex and interactive protocols involving distributed computations. However, the implementation of such complex protocols requires much programming efforts and the few existing tools and libraries deal with special case protocols. In this paper, we present the first complete, efficient and modular framework, composed of a set of libraries, which brings together the most known techniques of private-key generation for identity-based cryptosystems. Our framework aims at providing robust tools designed in a modular and reusable manner to allow developers to implement the latest results coming from theoretical cryptography. © 2016 Elsevier Inc. All rights reserved.","Cryptographic protocols; Distributed protocols; Software implementation"
"EventHealer: Bypassing data races in event-driven programs","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970005969&doi=10.1016%2fj.jss.2016.02.051&partnerID=40&md5=561c024edb7a53a4a04e0cadc65e8d01","Data races represent a serious threat to the reliability of shared-memory concurrent programs including event-driven programs which handle asynchronous events. Despite the important number of existing testing and detection tools, data races often remain undetectable until the exploitation phase leading the application into unpredictable executions sometimes with disastrous consequences. To heal data races, current approaches which focus only on multithreaded programs are not directly applicable to event-driven programs since they are still incomplete or incur a high runtime overhead which makes them also inappropriate for the exploitation phase. Thus, this paper proposes a hybrid technique that statically disassembles a program binary to collect information about critical sections and event handlers accessing each shared variable, to dynamically prevent data races from occurring by injecting a disabling and an enabling instruction respectively before and after every critical section, in order to enforce the synchronization property of the potentially harmful events. We implemented a prototype of this technique for sequential programs with signal handlers on top of the Pin instrumentation framework. An evaluation of this prototype proved its effectiveness and showed that our technique incurs only a negligible overhead to the monitored program. © 2016 Elsevier Inc. All rights reserved.","Data races; Event-driven programs; Healing techniques"
"A reversible data transform algorithm using integer transform for privacy-preserving data mining","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960089032&doi=10.1016%2fj.jss.2016.02.005&partnerID=40&md5=5373c1ce98da7564ec896ad58b2d8c60","In the cloud computing environment, since data owners worry about private information in their data being disclosed without permission, they try to retain the knowledge within the data, while applying privacy-preserving techniques to the data. In the past, a data perturbation approach was commonly used to modify the original data content, but it also results in data distortion, and hence leads to significant loss of knowledge within the data. To solve this problem, this study introduced the concept of reversible integer transformation in the image processing domain and developed a Reversible Data Transform (RDT) algorithm that can disrupt and restore data. In the RDT algorithm, using an adjustable weighting mechanism, the degree of data perturbation was adjusted to increase the flexibility of privacy-preserving. In addition, it allows the data to be embedded with a watermark, in order to identify whether the perturbed data has been tampered with. Experimental results show that, compared with the existing algorithms, RDT has better knowledge reservation and is better in terms of effectively reducing information loss and privacy disclosure risk. In addition, it has a high watermark payload. © 2016 Elsevier Inc. All rights reserved.","Cloud computing; Privacy-preserving; Reversible data hiding"
"The impacts of agile and lean practices on project constraints: A tertiary study","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977123198&doi=10.1016%2fj.jss.2016.06.043&partnerID=40&md5=36dae6cd08c43c3d6c306b3955be2cab","The growing interest in Agile and Lean software development is reflected in the increasing number of secondary studies on the benefits and limitations of Agile and Lean processes and practices. The aim of this tertiary study is to consolidate empirical evidence regarding Agile and Lean practices and their respective impacts on project constraints as defined in the Project Management Body of Knowledge (PMBOK): scope, quality, schedule, budget, resources, communication, and risk. In this tertiary study, 13 secondary studies were included for detailed analysis. Given the heterogeneity of the data, we were unable to perform a rigorous synthesis. Instead, we mapped the identified Agile and Lean practices, and their impacts on the project constraints described in PMBOK. From 13 secondary studies, we identified 13 Agile and Lean practices. Test-Driven Development (TDD) is studied in ten secondary studies, meanwhile other practices are studied in only one or two secondary studies. This tertiary study provides a consolidated view of the impacts of Agile and Lean practices. The result of this tertiary study indicates that TDD has a positive impact on external quality. However, due to insufficient data or contradictory results, we were unable to make inferences on other Agile and Lean practices. Implications for research and practice are further discussed in the paper. © 2016 Elsevier Inc.","Agile software development; Lean software development; Project constraints; Tertiary study"
"Hyper-heuristic approach for multi-objective software module clustering","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964345567&doi=10.1016%2fj.jss.2016.04.007&partnerID=40&md5=a2640adc00b79d385d8142e1a995349f","In the software maintenance phase of software development life cycle, one of the main concerns of software engineers is to group the modules into clusters with maximum cohesion and minimum coupling. To analyze the efficacy of Multi-objective Hyper-heuristic Evolutionary Algorithm (MHypEA) in solving real-world clustering problems and to compare the results with the reported results in the literature for single as well as multi-objective formulations of the problem and also to present a CASE tool that assists software engineers in software module clustering process. The paper reports on empirical evaluation of the performance of MHypEA with the reported results in the literature. The comparison is mainly based on two factors - quality of the obtained solutions and the computational effort. On all the attempted problems, MHypEA reported good results in comparison to all the studies that were reported on multi-objective formulation of the problem, with a computational effort of nearly one-twentieth of the computational effort required by the other multi-objective algorithms. The hyper-heuristic approach is able to produce high quality clustered systems with less computational effort. © 2016 Elsevier Inc. All rights reserved.","Hyper-heuristics; Multi-objective optimization; Software maintenance; Software module clustering"
"A mobile payment mechanism with anonymity for cloud computing","2016","Journal of Systems and Software","10.1016/j.jss.2015.07.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939162119&doi=10.1016%2fj.jss.2015.07.023&partnerID=40&md5=f4bbda34dbadd2bd7127505e424769ca","In recent years, traditional transactions have been replaced by electronic transactions. To protect the security of the electronic transactions, various electronic payment (e-payment) mechanisms have been proposed. However, we find the previous e-payment mechanisms do not provide the non-repudiation requirement in the client side. Thus, a malicious client can easily deny the transaction and the merchant may not get the payment. In addition, these mechanisms have large computation and communication costs so they cannot be applied to the mobile payment for cloud computing. To solve the above problems, we propose a new mobile payment mechanism with anonymity for cloud computing in this paper. The proposed mechanism not only reduces the computation cost but also provides the non-repudiation requirement in the client side. Compared with the related works, the proposed mechanism is securer, fairer, and more efficient. Therefore, the proposed mobile payment mechanism is more suitable and practical for the cloud computing. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Electronic transaction; Mobile payment"
"Open source FreeRTOS as a case study in real-time operating system evolution","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965124935&doi=10.1016%2fj.jss.2016.04.063&partnerID=40&md5=a8a99bd99723f2edd1de474fbe8fa690","This paper studies the evolution of a real-time operating system, the open source FreeRTOS. We focus on the changes in real-time performance and behaviour over the last ten years. Six major release versions are benchmarked, presenting quantitative and qualitative development trends. We also use the available source code to discover the reasons for the changes. By analysing the results, we draw some conclusions related to this RTOS's evolution which can be useful for the FreeRTOS group, other RTOS developments, and also RTOS users. © 2016 Elsevier Inc. All rights reserved.","Embedded system; FreeRTOS; Real-time OS; Software evolution"
"IKAROS: A scalable I/O framework for high-performance computing systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971414288&doi=10.1016%2fj.jss.2016.05.027&partnerID=40&md5=ac9b49d80c71f2b4b6b27341edaa7928","High performance computing (HPC) has crossed the Petaflop mark and is reaching the Exaflop range quickly. The exascale system is projected to have millions of nodes, with thousands of cores for each node. At such an extreme scale, the substantial amount of concurrency can cause a critical contention issue for I/O system. This study proposes a dynamically coordinated I/O architecture for addressing some of the limitations that current parallel file systems and storage architectures are facing with very large-scale systems. The fundamental idea is to coordinate I/O accesses according to the topology/profile of the infrastructure, the load metrics, and the I/O demands of each application. The measurements have shown that by using IKAROS approach we can fully utilize the provided I/O and network resources, minimize disk and network contention, and achieve better performance. © 2016 Elsevier Inc. All rights reserved.","Data management; Distributed systems; Exascale systems; GPFS; Grid computing; High performance computing; Parallel file systems; Soho-NAS; Storage"
"CALA: ClAssifying Links Automatically based on their URL","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958999121&doi=10.1016%2fj.jss.2016.02.006&partnerID=40&md5=d41c776657afcce9ebb9a0ace431279c","Web page classification refers to the problem of automatically assigning a web page to one or more classes after analysing its features. Automated web page classifiers have many applications, and many researchers have proposed techniques and tools to perform web page classification. Unfortunately, the existing tools have a number of drawbacks that makes them unappealing for real-world scenarios, namely: they require a previous extensive crawling, they are supervised, they need to download a page before classifying it, or they are site-, language-, or domain-dependent. In this article, we propose CALA, a tool for URL-based web page classification. The strongest features of our tool are that it does not require a previous extensive crawling to achieve good classification results, it is unsupervised, it is based exclusively on URL features, which means that pages can be classified without downloading them, and it is site-, language-, and domain-independent, which makes it generally applicable. We have validated our tool with 22 real-world web sites from multiple domains and languages, and our conclusion is that CALA is very effective and efficient in practice. © 2016 Elsevier Inc. All rights reserved.","URL patterns; Web page classification"
"Real-time hierarchical systems with arbitrary scheduling at global level","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974571247&doi=10.1016%2fj.jss.2016.05.040&partnerID=40&md5=1d652ff71366df7ce1cb52eb12508f7e","Partitioned architectures isolate software components into independent partitions whose execution will not interfere with other partitions, preserving temporal and spatial isolation. Hierarchical scheduling can effectively be used to schedule these systems. Schedulability analysis of hierarchical real-time systems is based on prior knowledge of the local and the global scheduling algorithms. In a partitioned system with safety and security issues and certification assurance levels, global scheduling is usually generated using a static table. Therefore, each partition must allocate task jobs only in the temporal windows reserved for that partition. Even if the static table can come originally from a periodic server or other scheduling policy, the final plan may be modified due to changes in the system requirements. As a consequence, the CPU assignment to a partition does not have to correspond to any known policy. In this case, it is not possible to use existing scheduling analysis for hierarchical systems. This paper studies a new scheduling problem: a hierarchical system in which global policy is not known but provided as a set of arbitrary time windows. © 2016 Elsevier Inc. All rights reserved.","Embedded systems; Partitioned systems; Real-time algorithms; Real-time systems; Real-time systems scheduling"
"5W+1H pattern: A perspective of systematic mapping studies and a case study on cloud software testing","2016","Journal of Systems and Software","10.1016/j.jss.2015.01.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923328222&doi=10.1016%2fj.jss.2015.01.058&partnerID=40&md5=e8e93bcd981da01275002872ad924112","A common type of study used by researchers to map out the landscape of a research topic is known as mapping study. Such a study typically begins with an exploratory search on the possible ideas of the research topic, which is often done in an unsystematic manner. Hence, the activity of formulating research questions in mapping studies is ill-defined, rendering it difficult for researchers who are new to the topic. There is a need to guide them kicking off a mapping study of an unfamiliar domain. This paper proposes a 5W+1H pattern to help investigators systematically examine a generic set of dimensions in a mapping study toward the formulation of research questions before identifying, reading, and analyzing sufficient articles of the topic. We have validated the feasibility of our proposal by conducting a case study of a mapping study on cloud software testing, that is, software testing for and on cloud computing platforms. The case study reveals that the 5W+1H pattern can lead investigators to define a set of systematic, generic, and complementary research questions, enabling them to kick off and expedite the mapping study process in a well-defined manner. We also share our experiences and lessons learned from our case study on the use of the 5W+1H pattern in mapping studies. © 2015 Elsevier Inc. All rights reserved.","5W+1H pattern; Cloud software testing; Systematic mapping study"
"Missing data techniques in analogy-based software development effort estimation","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966340503&doi=10.1016%2fj.jss.2016.04.058&partnerID=40&md5=014ecb6ca07ef9fbb58c0cbfb8c1dc3a","Missing Data (MD) is a widespread problem that can affect the ability to use data to construct effective software development effort prediction systems. This paper investigates the use of missing data (MD) techniques with two analogy-based software development effort estimation techniques: Classical Analogy and Fuzzy Analogy. More specifically, we analyze the predictive performance of these two analogy-based techniques when using toleration, deletion or k-nearest neighbors (KNN) imputation techniques. A total of 1512 experiments were conducted involving seven data sets, three MD techniques (toleration, deletion and KNN imputation), three missingness mechanisms (MCAR: missing completely at random, MAR: missing at random, NIM: non-ignorable missing), and MD percentages from 10 percent to 90 percent. The results suggest that Fuzzy Analogy generates more accurate estimates in terms of the Standardized Accuracy measure (SA) than Classical Analogy regardless of the MD technique, the data set used, the missingness mechanism or the MD percentage. Moreover, this study found that the use of KNN imputation, rather than toleration or deletion, may improve the prediction accuracy of both analogy-based techniques. However, toleration, deletion and KNN imputation are affected by the missingness mechanism and the MD percentage, both of which have a strong negative impact upon effort prediction accuracy. © 2016 Elsevier Inc. All rights reserved.","Analogy-based software development effort estimation; Imputation; Missing data"
"A new multi-rat scheduling algorithm for heterogeneous wireless networks","2016","Journal of Systems and Software","10.1016/j.jss.2015.02.073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960813602&doi=10.1016%2fj.jss.2015.02.073&partnerID=40&md5=0fd67c567701ebde00b84544c402900c","The concept of heterogeneous wireless networks (HWNs) is based on the coexistence and interoperability of different types of radio access technologies (RATs) such as long term evolution (LTE) and wireless local area network (WLAN) in a unified wireless heterogeneous platform. Guaranteeing the quality of service (QoS) is an important issue for the next generation wireless networks which are characterized by providing different types of services. To schedule different types of service in HWN, distinct scheduling algorithms have been studied intensively in the literature. Thus in our research work, we focus on a common scheduling algorithm for the HWN where the traffic streams are classified into different categories, and each category has its own set of QoS parameters such as data rate and delay. In this article, we propose a new dynamic scheduling algorithm for HWN. The proposed solution introduces a new approach in scheduling packets while maintaining performance in wireless networks. The scheduling scheme is mainly based on transmission links' condition from the media independent handover (MIH) module, type of call (handoff call prioritization) and classes of service. In order to study the performance of the proposed scheme, we use simulation analysis and compare the performance of our scheme with a competing reference scheme called NSA (new scheduling algorithm) for wireless mesh networks in order to reveal its ability to adapt to the specific service and channel conditions. Simulation results show that under large number of users, the proposed algorithm has lower packet loss and blocking calls ratio while offers allowable average packet delay. © 2015 Elsevier Inc. All rights reserved.","Heterogeneous wireless networks; QoS; Scheduling algorithm"
"HV2M: A novel approach to boost inter-VM network performance for Xen-based HVMs","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960128398&doi=10.1016%2fj.jss.2015.12.002&partnerID=40&md5=2ca3962eb789dfa8734cfc68e49509ab","Despite the rapid development and the wide use, virtualization confronts new challenges on improving network I/O performance. For virtual machines co-existed on a server, inter-VM network performance turns out to be worse than expected. Although many efforts have been dedicated to that issue on paravirtualized (PV) machines, HVMs (Hardware Virtual Machines) get less attention. However, co-resident HVMs are also suffering from low network I/O performance which troubles IaaS cloud infrastructure providers. Such problem is prominent as in the front-back network model several performance bottlenecks appear when inter-VM traffic is being handled. In this paper, we propose a novel approach and implement a prototype called ""HV2M"" to optimize inter-HVM network performance, which mainly address the throughput issues. HV2M takes advantages of the existing mechanisms for HVMs provided by Xen and is totally hardware independent. Unlike the existing idea, HV2M pushes data to the receivers without using shared memory buffer to transfer the pending network data. To evaluate our work, we test and compare network response time, network throughput and CPU utilization with other popular approaches such as SR-IOV. The result shows that the new communication model has great improvements in network delay and throughput with little pressure on Domain-0. © 2015 Elsevier Inc. All rights reserved.","HVM; I/O; Inter-VM communication; Network; Virtualization; Xen"
"Improving self-adaptation planning through software architecture-based stochastic modeling","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960942679&doi=10.1016%2fj.jss.2016.01.026&partnerID=40&md5=275865cce6d4972bf19cba0b12db9079","The ever-growing complexity of software systems makes it increasingly challenging to foresee at design time all interactions between a system and its environment. Most self-adaptive systems trigger adaptations through operators that are statically configured for specific environment and system conditions. However, in the occurrence of uncertain conditions, self-adaptive decisions may not be effective and might lead to a disruption of the desired non-functional attributes. To address this, we propose an approach that improves the planning stage by predicting the outcome of each strategy. In detail, we automatically derive a stochastic model from a formal architecture description of the managed system with the changes imposed by each strategy. Such information is used to optimize the self-adaptation decisions to fulfill the desired quality goals. To assess the effectiveness of our approach we apply it to a cloud-based news system and predicted the reliability for each possible adaptation strategy. The results obtained from our approach are compared to a representative static planning algorithm as well as to an oracle that always makes the ideal decision. Experiments show that our method improves both availability and cost when compared to the static planning algorithm, while being close to the oracle. Our approach may therefore be used to optimize self-adaptation planning. © 2016 Elsevier Inc. All rights reserved.","Impact prediction; Reliability prediction; Self-adaptive systems"
"Efficient quality-driven source selection from massive data sources","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971231426&doi=10.1016%2fj.jss.2016.05.026&partnerID=40&md5=11de47560354405d164ccdde7c131bbe","The query based on massive database is time-consuming and difficult. And the uneven quality of data source makes the multiple source selection more challenging. The low-quality data source can even make the result of the information unexpected. How to efficiently select quality-driven data sources on massive database remains a hard problem. In this paper, we study the efficient source selection problem on massive data set considering the quality of data sources. Our approach evaluates the quality of data source and balances the limitation of resources and the completeness of data source. For data source selection for a specific query, our method could select the data sources with the number of keywords larger than a given threshold. And the selected sources are ranked according to the values of information in data sources. Experimental results demonstrate that our method can scale to millions of data sources and perform pretty efficiently. © 2016 Elsevier Inc. All rights reserved.","Data quality; Data source selection; Information integration"
"Towards more accurate severity prediction and fixer recommendation of software bugs","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961211131&doi=10.1016%2fj.jss.2016.02.034&partnerID=40&md5=e95979e168b49d2b6209760883ce4b33","Due to the unavoidable bugs appearing in the most of the software systems, bug resolution has become one of the most important activities in software maintenance. For large-scale software programs, developers usually depend on bug reports to fix the given bugs. When a new bug is reported, a triager has to complete two important tasks that include severity identification and fixer assignment. The purpose of severity identification is to decide how quickly the bug report should be addressed while fixer assignment means that the new bug needs to be assigned to an appropriate developer for fixing. However, a large number of bug reports submitted every day increase triagers' workload, thus leading to the reduction in the accuracy of severity identification and fixer assignment. Therefore it is necessary to develop an automatic approach to perform severity prediction and fixer recommendation instead of manual work. This article proposes a more accurate approach to accomplish the goal. We firstly utilize modified REP algorithm (i.e., REPtopic) and K-Nearest Neighbor (KNN) classification to search the historical bug reports that are similar to a new bug. Next, we extract their features (e.g., assignees and similarity) to develop the severity prediction and fixer recommendation algorithms. Finally, by adopting the proposed algorithms, we achieve severity prediction and semi-automatic fixer recommendation on five popular open source projects, including GNU Compiler Collection (GCC), OpenOffice, Eclipse, NetBeans, and Mozilla. The results demonstrated that our method can improve the performance of severity prediction and fixer recommendation through comparison with the cutting-edge studies. © 2016 Elsevier Inc. All rights reserved.","Fixer recommendation; Severity prediction; Topic model"
"Mining trends and patterns of software vulnerabilities","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961838451&doi=10.1016%2fj.jss.2016.02.048&partnerID=40&md5=d162dcea14d04abda1936da4d7923843","Zero-day vulnerabilities continue to be a threat as they are unknown to vendors; when attacks occur, vendors have zero days to provide remedies. New techniques for the detection of zero-day vulnerabilities on software systems are being developed but they have their own limitations; e.g., anomaly detection techniques are prone to false alarms. To better protect software systems, it is also important to understand the relationship between vulnerabilities and their patterns over a period of time. The mining of trends and patterns of vulnerabilities is useful because it can help software vendors prepare solutions ahead of time for vulnerabilities that may occur in a software application. In this paper, we investigate the use of historical patterns of vulnerabilities in order to predict future vulnerabilities in software applications. In addition, we examine whether the trends of vulnerabilities in software applications have any significant meaning or not. We use the National Vulnerability Database (NVD) as the main resource of vulnerabilities in software applications. We mine vulnerabilities of the last six years from 2009 to 2014 from NVD. Our results show that sequences of the same vulnerabilities (e.g., buffer errors) may occur 150 times in a software product. Our results also depict that the number of SQL injection vulnerabilities have decreased in the last six years while cryptographic vulnerabilities have seen an important increase. However, we have not found any statistical significance in the trends of the occurrence of vulnerabilities over time. The most interesting finding is that the sequential patterns of vulnerability events follow a first order Markov property; that is, we can predict the next vulnerability by using only the previous vulnerability with a recall of approximately 80% and precision of around 90%. © 2016 Elsevier Inc. All rights reserved.","Software vulnerabilities; Vulnerability prediction; Vulnerability trends"
"Performance analysis of contending customer equipment in wireless networks","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963860046&doi=10.1016%2fj.jss.2016.03.062&partnerID=40&md5=e56948d2950582f3e0b8d5e3bb499f65","Initial ranging is the primary and important process in wireless networks for the customer premise equipments (CPEs) to access the network and establish their connections with the base station. Contention may occur during the initial ranging process. To avoid contention, the mandatory solution defined in the standards is based on a truncated binary exponential random backoff (TBERB) algorithm with a fixed initial contention window size. However, the TBERB algorithm does not take into account the possibility that the number of contended CPEs may change dynamically over time, leading to a dynamically changing collision probability. To the best of our knowledge, this is the first attempt to address this issue. There are three major contributions presented in this paper. First, a comprehensive analysis of initial ranging mechanisms in wireless networks is provided and initial ranging request success probability is derived based on number of contending CPEs and the initial contention window size. Second, the average ranging success delay is derived for the maximum backoff stages. It is found that the collision probability is highly dependent on the size of the initial contention window and the number of contending CPEs. To achieve the higher success probability or to reduce the collision probability among CPEs, the BS needs to adjust the initial contention window size. To keep the collision probability at a specific value for the particular number of contending CPEs, it is necessary for the BS to schedule the required size of the initial contention window to facilitate the maximum number of CPEs to establish their connections with reasonable delay. In our third contribution, the initial window size is optimized to provide the least upper bound that meets the collision probability constraint for a particular number of contending CPEs. The numerical results validate our analysis. © 2016 Elsevier Inc. All rights reserved.","Customer premise equipment; Initial ranging process; Wireless network"
"The daily stand-up meeting: A grounded theory study","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960090527&doi=10.1016%2fj.jss.2016.01.004&partnerID=40&md5=32904d05f5f9b4a428b268f61c3124c4","The daily stand-up meeting is one of the most used agile practices but has rarely been the subject of empirical research. The present study aims to identify how daily stand-up meetings are conducted and what the attitudes towards them are. A grounded theory study with 12 software teams in three companies in Malaysia, Norway, Poland and the United Kingdom was conducted. We interviewed 60 people, observed 79 daily stand-up meetings and collected supplementary data. The factors that contributed the most to a positive attitude towards the daily stand-up meeting were information sharing with the team and the opportunity to discuss and solve problems. The factors that contributed the most to a negative attitude were status reporting to the manager and that the frequency of the meeting was perceived to be too high and the duration too long. Based on our results, we developed a grounded theory of daily stand-up meetings and proposed empirically based recommendations and guidelines on how to organize them. Organizations should be aware of the factors that may affect the attitude towards daily stand-up meetings and should consider our recommendations and guidelines to make this agile practice as valuable as possible. © 2016 Elsevier Inc. All rights reserved.","Agile software development; Daily meeting; Daily Scrum meeting"
"Collision detection and resolution of hazard prevention actions in safety critical systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965152279&doi=10.1016%2fj.jss.2016.04.056&partnerID=40&md5=80afac745fe0148b0eff72d07694a692","The importance of safety-critical systems can never be overemphasized, as we have witnessed how devastating the accidents were with the recent nuclear power plant explosions and also with airplane and spaceship crashes. To ensure the safety of such systems, system engineers should consider how to prevent system hazards during the design phase of system development. Although existing techniques, such as event tree analysis and cause-consequence analysis, suggest various ways of hazard prevention for safety-critical systems, they per se do not deal with situations where two distinct actions of two separate safety devices can fail to prevent the very hazard they are supposed to prevent, since they collide, or conflict, with each other. In this paper, we propose a technique for identifying and analyzing the colliding actions of safety devices, using fault prevention tree and resource map. We also propose the use of a mediator for coordinating the actions that otherwise would lead to a collision. Through an empirical study, we demonstrate that this technique can help design (more) robust systems that can prevent hazards, while meeting the software safety requirements in practical system development. © 2016 Elsevier Inc. All rights reserved.","Collision analysis; Fault prevention tree; Prevention action; Resource map; Software safety"
"Backwards reasoning for model transformations: Method and applications","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941242234&doi=10.1016%2fj.jss.2015.08.017&partnerID=40&md5=2ac868ce39abb7d9c1e60f7625cd1479","Model transformations are key elements of model driven engineering. Current challenges for transformation languages include improving usability (i.e., succinct means to express the transformation intent) and devising powerful analysis methods. In this paper, we show how backwards reasoning helps in both respects. The reasoning is based on a method that, given an OCL expression and a transformation rule, calculates a constraint that is satisfiable before the rule application if and only if the original OCL expression is satisfiable afterwards. With this method we can improve the usability of the rule execution process by automatically deriving suitable application conditions for a rule (or rule sequence) to guarantee that applying that rule does not break any integrity constraint (e.g. meta-model constraints). When combined with model finders, this method facilitates the validation, verification, testing and diagnosis of transformations, and we show several applications for both in-place and exogenous transformations. © 2015 Elsevier Inc. All rights reserved.","Model transformation; OCL; Weakest pre-condition"
"Mutation testing cost reduction by clustering overlapped mutants","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960906097&doi=10.1016%2fj.jss.2016.01.007&partnerID=40&md5=813130da8e724734fafad07f792654a3","Mutation testing is a powerful but computationally expensive testing technique. Several approaches have been developed to reduce the cost of mutation testing by decreasing the number of mutants to be executed; however, most of these approaches are not as effective as mutation testing which uses a full set of mutants. This paper presents a new approach for executing fewer mutants while retaining nearly the same degree of effectiveness as is produced by mutation testing using a full set of mutants. Our approach dynamically clusters expression-level weakly killed mutants that are expected to produce the same result under a test case; only one mutant from each cluster is fully executed under the test case. We implemented this approach and demonstrated that our approach efficiently reduced the cost of mutation testing without loss of effectiveness. © 2016 Elsevier Inc. All rights reserved.","Mutation testing; Software testing"
"Predicting software reliability via completely monotone nonparametric estimator with grouped data","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963777356&doi=10.1016%2fj.jss.2016.03.047&partnerID=40&md5=998b0ebebc8e8a218525caa444013752","Nonparametric software reliability analysis is a challenging issue to predict software reliability under incomplete knowledge on software fault-detection time distribution, because the underlying stochastic model is a function of only software fault data and is not predictable in principle for unknown patterns in future long term. Sofer and Miller (1991) develop a unique approach based on a completely monotone nonparametric estimator, but just focus on the fault-detection time data. However, such data sets are seldom available in practice, and their approach is not applicable to many actual software development processes. In this paper, we revisit the seminal completely monotone estimator by Sofer and Miller (1991) to use in the major case with grouped data, and develop both estimation and prediction methods of software fault count. We investigate the potential performance of our distribution-free method with thirteen actual software development project data, and compare it with the existing parametric models known as nonhomogeneous Poisson process-based software reliability models. © 2016 Elsevier Inc. All rights reserved.","Completely monotone nonparametric estimator; Grouped data; Software reliability"
"An approach to modeling and developing teleo-reactive systems considering timing constraints","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963624895&doi=10.1016%2fj.jss.2016.03.064&partnerID=40&md5=28f43640feb46f6513304cdbebdc31ff","Context TeleoR is an extension and implementation of teleo-reactive (TR) language for defining the behavior of reactive systems when the consideration of timing constraints is a matter of interest. Objective This paper analyzes how to consider real-time constraints when a TR approach is followed from modeling to implementation. Method After carrying out a study of the type of timing constraints from the TR perspective, the possibility of using TeleoR for incorporating such constraints was considered. Some extensions on TRiStar notation were then made to represent temporal requirements. A drone-based case study was carried out to demonstrate the usefulness of this approach. Finally, a survey was conducted to validate the approach. Results TeleoR can, to a great extent, support the kind of real-time constraints required for developing real-time systems, offering a direct solution to five of the eight temporal requirements identified, which can be implemented using the basic features of the language. Conclusions Considering real-time requirements should be part of the specification of reactive systems implemented when using the TR approach and should be supported by the implementation platform. In this regard, TeleoR offers reasonable possibilities that should be extended by taking into account the limitations identified here. © 2016 Elsevier Inc. All rights reserved.","Requirements engineering; Teleo-reactive; Timing constraints; TRiStar"
"A comparative study of energy-aware scheduling algorithms for computational grids","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960899758&doi=10.1016%2fj.jss.2016.02.017&partnerID=40&md5=576ad63872a2c3da96130e94b3e80b7a","Recent advances in High Performance Computing (HPC) have required the attention of scientific community regarding aspects that do not concern only performance. In order to enhance computational capacity, modern parallel and distributed architectures are designed with more processing units, causing an increase in energy consumption. Currently, one of the most representative HPC platforms are computational grids, which are used in many scientific and academic projects. In this work, we propose four energy-aware scheduling algorithms to efficiently manage the energy consumption in computational grids, trying to mitigate performance loss. Our algorithms propose an efficient management of idle resources and a clever use of active ones. We have evaluated our algorithms using the SimGrid framework and an energy consumption estimation method we proposed for Bag-of-Tasks-type (BoT) applications. We compared our algorithms against five others developed to work with computational grids. In a set of experimental scenarios, our results show that by using our algorithms it is possible to achieve up to 75.90% of reduction in the energy consumption combined with 5.28% of performance loss compared with the best algorithm in performance. © 2016 Elsevier Inc. All rights reserved.","Energy-aware scheduling algorithms; High performance computing; Simulation"
"From benchmarks to real apps: Exploring the energy impacts of performance-directed changes","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963543321&doi=10.1016%2fj.jss.2016.03.031&partnerID=40&md5=b48de64dffc1847ff18dbbe79cfaaaa7","Battery life is an increasing concern for mobile devices. Recent studies have provided initial evidence that applying performance tips is an effective mechanism for decreasing energy usage. However, the generalizability of such studies to real applications is unclear. We aim to provide deeper insights into whether mobile application developers can effectively reduce the energy consumption of their applications by applying performance tips. We conducted an empirical study to investigate the energy impacts of applying four commonly suggested performance tips to eight real Android applications. Considered performance tips are unlikely to impact energy usage in a statistically significant manner and, even when the impacts are statistically significant, the change in battery life is around 1%. Mobile application developers cannot expect to improve the energy usage of their applications as a by product of performance improvements. Tools and techniques that specifically target energy usage are necessary for significant improvements. © 2016 Elsevier Inc. All rights reserved.","Android applications; Energy efficiency; Performance tips"
"Engineering context-aware systems and applications: A survey","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960194491&doi=10.1016%2fj.jss.2016.02.010&partnerID=40&md5=c0f79421f5b06abfff03fe4a6505b4cf","Context-awareness is an essential component of systems developed in areas like Intelligent Environments, Pervasive & Ubiquitous Computing and Ambient Intelligence. In these emerging fields, there is a need for computerized systems to have a higher understanding of the situations in which to provide services or functionalities, to adapt accordingly. The literature shows that researchers modify existing engineering methods in order to better fit the needs of context-aware computing. These efforts are typically disconnected from each other and generally focus on solving specific development issues. We encourage the creation of a more holistic and unified engineering process that is tailored for the demands of these systems. For this purpose, we study the state-of-the-art in the development of context-aware systems, focusing on: (A) Methodologies for developing context-aware systems, analyzing the reasons behind their lack of adoption and features that the community wish they can use; (B) Context-aware system engineering challenges and techniques applied during the most common development stages; (C) Context-aware systems conceptualization. © 2016 Elsevier Inc. All rights reserved.","Ambient Intelligence; Context-aware computing; Context-Aware Systems Engineering; Context-awareness; Context-sensitive; Intelligent Environments; Pervasive & Ubiquitous Computing; Sentient computing; Software engineering"
"DSS from an RE Perspective: A systematic mapping","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964284186&doi=10.1016%2fj.jss.2016.03.046&partnerID=40&md5=53e43f7a3fba85341dd89f56cdd64504","Decision support systems (DSS) provide a unified analytical view of business data to better support decision-making processes. Such systems have shown a high level of user satisfaction and return on investment. However, several surveys stress the high failure rate of DSS projects. This problem results from setting the wrong requirements by approaching DSS in the same way as operational systems, whereas a specific approach is needed. Although this is well-known, there is still a surprising gap on how to address requirements engineering (RE) for DSS. To overcome this problem, we conducted a systematic mapping study to identify and classify the literature on DSS from an RE perspective. Twenty-seven primary studies that addressed the main stages of RE were selected, mapped, and classified into 39 models, 27 techniques, and 54 items of guidance. We have also identified a gap in the literature on how to design the DSS main constructs (typically, the data warehouse and data flows) in a methodological manner from the business needs. We believe this study will help practitioners better address the RE stages of DSS projects. © 2016 Elsevier Inc. All rights reserved.","Business intelligence; Decision support systems; Requirements engineering"
"Decision making in software architecture","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956623197&doi=10.1016%2fj.jss.2016.01.017&partnerID=40&md5=8a06ea5ab015ff3e0576015892d0d3c9","Traditionally, software architecture is seen as the result of the software architecture design process, the solution, usually represented by a set of components and connectors. Recently, the why of the solution, the set of design decisions made by the software architect, is complementing or even replacing the solution-oriented definition of software architecture. This in turn leads to the study of the process of making these decisions. We outline some research directions that may help us understand and improve the software architecture design process. © 2016 Elsevier Inc.","Design decisions; Software architecture"
"Agile methods in embedded system development: Multiple-case study of three industrial cases","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969262130&doi=10.1016%2fj.jss.2016.05.001&partnerID=40&md5=88ba2766f699adf5699f8887b76ba548","Agile methods are widely utilized in software development but their usage in embedded system development is often limited to software. A case study of three industrial cases was carried out to understand how to tailor agile methods effectively including also hardware development. Agile practices, mostly derived from Scrum, were tailored to fit the needs of each team and the method development was closely followed. Surveys conducted in the beginning and in the end of the cases were compared and complemented with interviews to understand the new working methods and their effects. Case evidence shows that interdependencies between work of each developer were taken into account better, visibility over the whole product increased and need for internal documentation diminished due to improved communication, but dividing hardware tasks into iterations was experienced difficult. With some tailoring, agile practices are beneficial also in the embedded system development. To successfully adopt agile methods into embedded system development, the team must consist of all the project members, the natural cycle lengths of different disciplines and different knowledge between the developers must be accepted and built upon, and the progress of the product must be presented or visualized in the end of each iteration. © 2016 Elsevier Inc. All rights reserved.","Agile; Agile method; Case study; Embedded system"
"Mobile sink based fault diagnosis scheme for wireless sensor networks","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973917875&doi=10.1016%2fj.jss.2016.05.041&partnerID=40&md5=6d3f24e459526026453a37e3ac041e3a","Network diagnosis in Wireless Sensor Networks (WSNs) is a difficult task due to their improvisational nature, invisibility of internal running status, and particularly since the network structure can frequently change due to link failure. To solve this problem, we propose a Mobile Sink (MS) based distributed fault diagnosis algorithm for WSNs. An MS, or mobile fault detector is usually a mobile robot or vehicle equipped with a wireless transceiver that performs the task of a mobile base station while also diagnosing the hardware and software status of deployed network sensors. Our MS mobile fault detector moves through the network area polling each static sensor node to diagnose the hardware and software status of nearby sensor nodes using only single hop communication. Therefore, the fault detection accuracy and functionality of the network is significantly increased. In order to maintain an excellent Quality of Service (QoS), we employ an optimal fault diagnosis tour planning algorithm. In addition to saving energy and time, the tour planning algorithm excludes faulty sensor nodes from the next diagnosis tour. We demonstrate the effectiveness of the proposed algorithms through simulation and real life experimental results. © 2016 Elsevier Inc. All rights reserved.","Diagnostics; Infrastructure protection; Network monitoring; Wireless sensor networks"
"Shorter hash-based signatures","2016","Journal of Systems and Software","10.1016/j.jss.2015.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937934773&doi=10.1016%2fj.jss.2015.07.007&partnerID=40&md5=ae097decc527015e2e9efa52a557d023","We describe an efficient hash-based signature scheme that yields shorter signatures than the state of the art. Signing and verification are faster as well, and the overall scheme is suitable for constrained platforms typical of the Internet of Things. We describe an efficient implementation of our improved scheme and show memory, time, and energy consumption benchmarks over a real device, i.e. the ATmega128l 8-bit AVR microcontroller embedded in MICAz, a typical sensor node used in wireless sensor networks. © 2015 Elsevier Inc. All rights reserved.","Embedded security; Hash-based signatures; Internet of things"
"LCBM: A fast and lightweight collaborative filtering algorithm for binary ratings","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966359127&doi=10.1016%2fj.jss.2016.04.062&partnerID=40&md5=e0bf6a4fe89f55a76453aacb27b90f27","In the last ten years, recommendation systems evolved from novelties to powerful business tools, deeply changing the internet industry. Collaborative Filtering (CF) represents a widely adopted strategy today to build recommendation engines. The most advanced CF techniques (i.e. those based on matrix factorization) provide high quality results, but may incur prohibitive computational costs when applied to very large data sets. In this paper we present Linear Classifier of Beta distributions Means (LCBM), a novel collaborative filtering algorithm for binary ratings that is (i) inherently parallelizable (ii) provides results whose quality is on-par with state-of-the-art solutions (iii) at a fraction of the computational cost. These characteristics allow LCBM to efficiently handle large instances of the collaborative filtering problem on a single machine in short timeframes. © 2016 Published by Elsevier Inc.","Big data; Collaborative filtering; Personalization; Recommendation systems"
"Incorrect results in software engineering experiments: How to improve research practices","2016","Journal of Systems and Software","10.1016/j.jss.2015.03.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927142981&doi=10.1016%2fj.jss.2015.03.065&partnerID=40&md5=c03170ad2daebaed4036e84601a8b328","Context The trustworthiness of research results is a growing concern in many empirical disciplines. Aim The goals of this paper are to assess how much the trustworthiness of results reported in software engineering experiments is affected by researcher and publication bias, given typical statistical power and significance levels, and to suggest improved research practices. Method First, we conducted a small-scale survey to document the presence of researcher and publication biases in software engineering experiments. Then, we built a model that estimates the proportion of correct results for different levels of researcher and publication bias. A review of 150 randomly selected software engineering experiments published in the period 2002-2013 was conducted to provide input to the model. Results The survey indicates that researcher and publication bias is quite common. This finding is supported by the observation that the actual proportion of statistically significant results reported in the reviewed papers was about twice as high as the one expected assuming no researcher and publication bias. Our models suggest a high proportion of incorrect results even with quite conservative assumptions. Conclusion Research practices must improve to increase the trustworthiness of software engineering experiments. A key to this improvement is to avoid conducting studies with unsatisfactory low statistical power. © 2015 Elsevier Inc. All rights reserved.","Controlled experiments; Empirical software engineering; Statistical hypothesis testing"
"Understanding cloud computing adoption issues: A Delphi study approach","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966430132&doi=10.1016%2fj.jss.2016.04.061&partnerID=40&md5=fbe3ab103aa1b717fb6d3aa578f9bf12","This research paper reports on a Delphi study focusing on the most important issues enterprises are confronted with when making cloud computing (CC) adoption decisions. We had 34 experts from different domain backgrounds participated in a Delphi panel. The panelists were IT and CC specialists representing a heterogeneous group of clients, providers and academics, divided into three subpanels. The Delphi procedure comprised three stages: brainstorming, narrowing down and ranking. The panelists identified 55 issues of concerns in the first stage, which were analyzed and grouped into 10 categories: security, strategy, legal and ethical, IT governance, migration, culture, business, awareness, availability and impact. The top 18 issues for each subpanel were ranked, and a moderate intrapanel consensus was obtained. Additionally, 16 follow-up interviews were conducted with the experts to get a deeper understanding of the issues and why certain issues were more significant than others. The findings indicate that security, strategy and legal and ethical issues are the most important. The discussion resulted in highlighting certain inhibitors and drivers for CC adoption into a framework. The paper is concluded with key recommendations with focus on change management, competence and maturity to inform decision-makers in CC adoption decisions. © 2016 Elsevier Inc. All rights reserved.","Adoption; Cloud computing; Cloud computing adoption drivers; Cloud computing adoption inhibitors; Decision-making; Delphi method; IT outsourcing; Snowden effect"
"What situational information would help developers when using a graphical code recommender?","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962514405&doi=10.1016%2fj.jss.2016.02.050&partnerID=40&md5=bf4a60bedbf9ca4a27e25bc0f9b6e20d","Developers spend a significant amount of time trying to understand code bases. To aid developers' comprehension of code, researchers have developed software visualization tools. However, the uses of these tools in situ have rarely been investigated. To make matters worse, as studies have revealed, developers seldom use diagramming tools, making such investigations a challenge. To determine the possible uses of such tools in real practice, we conduct a diary study in which eleven developers in real-world developments use a novel visualization tool (a graphical code recommender) for one month. In the study, we ask what information and features the visualization and diagramming tools should provide to aid developers' work according to their situations. The study reveals the situations in which developers would use such visualization and diagramming tools and also the concrete requirements for such tools that would make them useful. © 2016 Elsevier Inc. All rights reserved .","Code navigation; Design requirement; Diagramming tool; Diary study; Software visualization"
"A systematic mapping study of mobile application testing techniques","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963641809&doi=10.1016%2fj.jss.2016.03.065&partnerID=40&md5=d20e461643ccfb679c60e8ba0b0daff0","The importance of mobile application specific testing techniques and methods has been attracting much attention of software engineers over the past few years. This is due to the fact that mobile applications are different than traditional web and desktop applications, and more and more they are moving to being used in critical domains. Mobile applications require a different approach to application quality and dependability and require an effective testing approach to build high quality and more reliable software. We performed a systematic mapping study to categorize and to structure the research evidence that has been published in the area of mobile application testing techniques and challenges that they have reported. Seventy nine (79) empirical studies are mapped to a classification schema. Several research gaps are identified and specific key testing issues for practitioners are identified: there is a need for eliciting testing requirements early during development process; the need to conduct research in real-world development environments; specific testing techniques targeting application life-cycle conformance and mobile services testing; and comparative studies for security and usability testing. © 2016 Elsevier Inc. All rights reserved.","Mobile application testing; Software testing; Systematic mapping"
"The role of absorptive capacity, communication and trust in ERP adoption","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973925883&doi=10.1016%2fj.jss.2016.05.025&partnerID=40&md5=741d19e9309da7af70043d7d89fca2b6","The use of Enterprise Resource Planning (ERP) systems is proven to be valuable in several ways and it is considered a necessity in today's business. However, despite the high cost and efforts required in implementing ERPs, the success rate is reported unsatisfactory in Iranian organizations. It is argued that the success of ERP implementation is significantly related to the users' adoption behavior. As one of the most important predictors of adoption behavior, this study investigates factors affecting the intention to use ERP systems. In particular, using Technology Acceptance Model (TAM), we examined the effects of absorptive capacity, communication and trust on the intention to use ERP systems. A questionnaire was sent to ERP users in 7 organizations in Iran, and 184 responses were used for the analysis. The findings suggest that trust, together with perceived ease of use and perceived usefulness, have a positive significant relationship with intention to use ERP. Furthermore, absorptive capacity and communication have a direct effect on the perceived ease of use which, in turn, impacts the intention to use ERP. As such, this study advances the current knowledge of adoption behavior by investigating the role of trust, communication and absorptive capacity on the intention to use. © 2016 Elsevier Inc. All rights reserved.","Absorptive capacity; Communication; ERP; Intention to use; Trust"
"Integration of a preemptive priority based scheduler in the Palladio Workbench","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960085220&doi=10.1016%2fj.jss.2015.12.029&partnerID=40&md5=a9ea81bd69c6dc7e3dae9959eb7bf611","This paper presents an extension to the Palladio Component Model (PCM), together with a new performance analysis infrastructure that supports the fixed-priority preemptive scheduling policy. The proposed solution allows modelling and analysing component-based embedded software applications that are defined using a specific pattern in which each component is executed by a task with a specific priority. The infrastructure is also capable of analysing the system performance when the tasks access shared resources, using either immediate priority ceiling, or priority inheritance protocols, in order to avoid the priority inversion problem. The paper shows the set of rules that enable the transformation between an application, compliant with the proposed design pattern, and its corresponding PCM. Finally, a use case example based on a real system, and a set of tests that validates the analysis infrastructure, are provided. This system is the on-board software of a satellite payload that is currently being developed by the Space Research Group of the University of Alcala. This software is in charge of managing the Instrument Control Unit of the Energetic Particle Detector, which will be launched as part of the Solar Orbiter mission of the European Space Agency and United States National Aeronautics and Space Administration (NASA). © 2016 Elsevier Inc. All rights reserved.","Component-Based Software Engineering; Model-driven engineering; Real time software; Solar Orbiter"
"Exploiting thread-related system calls for plagiarism detection of multithreaded programs","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976350850&doi=10.1016%2fj.jss.2016.06.014&partnerID=40&md5=a2f5350071d2058159d1581dba789f23","Dynamic birthmarking used to be an effective approach to detecting software plagiarism. Yet the new trend towards multithreaded programming renders existing algorithms almost useless, due to the fact that thread scheduling nondeterminism severely perturbs birthmark generation and comparison. In this paper, we redesign birthmark based software plagiarism detection algorithms to make such approach effective for multithreaded programs. Our birthmarks are abstractions of program behavioral characteristics based on thread-related system calls. Such birthmarks are less susceptible to thread scheduling as the system calls are the sources that impose thread scheduling rather than being affected. We have conducted an empirical study on a benchmark that consists of 234 versions of 35 different multithreaded programs. Our experiments show that the new birthmarks are superior to existing birthmarks and are resilient against most state-of-the-art obfuscation techniques. © 2016 Elsevier Inc.","Multithreaded program; Software birthmark; Software plagiarism detection; Thread-aware birthmark"
"Effect of developer collaboration activity on software quality in two large scale projects","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973138759&doi=10.1016%2fj.jss.2016.03.055&partnerID=40&md5=3eaaee85b93f2c589fdfde1083a3b071","Developers work together during software development and maintenance to resolve issues and implement features in large software projects. The structure of their development collaboration activity may have impact on the quality of the final product in terms of higher number of defects. In this paper, we aim to understand the effect of collaboration on the defect proneness software. We model the collaboration of developers as an undirected network. We extract the centrality of the developers from the collaboration network using different measures that quantifies the importance of the nodes. We analyze the defect inducing and fixing data of the developers in two large software projects. Our findings in this study can be summarized as follows: (a) Centrality and source code change activity of developers in the collaboration network may change their defect induction rates i.e. the defect proneness of their change sets, (b) Contrary to the common perception, more experienced people have relatively higher defect induction rates.","Collaboration networks; Developer collaboration; Human factor in software engineering; Software quality"
"A systematic review on the engineering of software for ubiquitous systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971440974&doi=10.1016%2fj.jss.2016.05.024&partnerID=40&md5=9636847243835a1749c2b859d386cbc7","Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it. Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems. Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations. Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers. Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback. © 2016 Elsevier Inc. All rights reserved.","Development methods; Empirical software engineering; Evidence-based software engineering; Pervasive systems; Research synthesis; Software development cycle; Systematic review; Ubiquitous systems"
"DISARM: A social distributed agent reputation model based on defeasible logic","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960329337&doi=10.1016%2fj.jss.2016.02.016&partnerID=40&md5=74a7e2cd67536b813ef023c4cfdb50b3","Agents act in open and thus risky environments with limited or no human intervention. Making the appropriate decision about who to trust in order to interact with is not only necessary but it is also a challenging process. To this end, trust and reputation models, based on interaction trust or witness reputation, have been proposed. Yet, they are often faced with skepticism since they usually presuppose the use of a centralized authority, the trustworthiness and robustness of which may be questioned. Distributed models, on the other hand, are more complex but they are more suitable for personalized estimations based on each agent's interests and preferences. Furthermore, distributed approaches allow the study of a really challenging aspect of multi-agent systems, that of social relations among agents. To this end, this article proposes DISARM, a novel distributed reputation model. DISARM treats Multi-agent Systems as social networks, enabling agents to establish and maintain relationships, limiting the disadvantages of the common distributed approaches. Additionally, it is based on defeasible logic, modeling the way intelligent agents, like humans, draw reasonable conclusions from incomplete and possibly conflicting (thus inconclusive) information. Finally, we provide an evaluation that illustrates the usability of the proposed model. © 2016 Elsevier Inc. All rights reserved.","Defeasible reasoning; Distributed trust management; Multi-agent systems"
"Unveiling parallelization opportunities in sequential programs","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964040296&doi=10.1016%2fj.jss.2016.03.045&partnerID=40&md5=797434b4ce6d83ee49e2328fc57d9d97","The stagnation of single-core performance leaves application developers with software parallelism as the only option to further benefit from Moore's Law. However, in view of the complexity of writing parallel programs, the parallelization of myriads of sequential legacy programs presents a serious economic challenge. A key task in this process is the identification of suitable parallelization targets in the source code. In this paper, we present an approach to automatically identify potential parallelism in sequential programs of realistic size. In comparison to earlier approaches, our work combines a unique set of features that make it superior in terms of functionality: It not only (i) detects available parallelism with high accuracy but also (ii) identifies the parts of the code that can run in parallel - even if they are spread widely across the code, (iii) ranks parallelization opportunities according to the speedup expected for the entire program, while (iv) maintaining competitive overhead both in terms of time and memory. © 2016 Elsevier Inc. All rights reserved.","Data dependence; Parallelism discovery; Parallelization; Profiling; Program analysis"
"Quality assurance in software ecosystems: A systematic literature mapping and research agenda","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960112461&doi=10.1016%2fj.jss.2015.12.020&partnerID=40&md5=42bab5ce1ed2cde0537a7940e405a843","Software ecosystems are becoming a common model for software development in which different actors cooperate around a shared platform. However, it is not clear what the implications are on software quality when moving from a traditional approach to an ecosystem, and this is becoming increasingly important as ecosystems emerge in critical domains such as embedded applications. Therefore, this paper investigates the challenges related to quality assurance in software ecosystems, and identifies what approaches have been proposed in the literature. The research method used is a systematic literature mapping, which however only resulted in a small set of six papers. The literature findings are complemented with a constructive approach where areas are identified that merit further research, resulting in a set of research topics that form a research agenda for quality assurance in software ecosystems. The agenda spans the entire system life-cycle, and focuses on challenges particular to an ecosystem setting, which are mainly the results of the interactions across organizational borders, and the dynamic system integration being controlled by the users. © 2015 Elsevier Inc. All rights reserved.","Quality; Software ecosystems; Testing; Verification"
"ProMARTES: Accurate network and computation delay prediction for component-based distributed systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964240905&doi=10.1016%2fj.jss.2016.03.068&partnerID=40&md5=ea23ba9678d0b8857627ecf4ba25189f","This paper proposes a cycle-accurate performance analysis method for real-time component-based distributed systems (CB-RTDS). The method involves the following phases: (a) profiling SW components at cycle execution level and modeling the obtained performance measurements in MARTE-compatible component resource models, (b) guided composition of the system architecture from available SW and HW components, (c) automated generation of a system model, specifying both computation and network loads, and (d) performance analysis (scheduling, simulation and network analysis) of the composed system model. The method is demonstrated for a real-world case study of 3 autonomously navigating robots with advanced sensing capabilities. The case study is challenging because of the SW/HW mapping, real-time requirements and data synchronization among multiple nodes. This case-study proved that, thanks to the adopted low-level performance metrics, we are able to obtain accurate performance predictions of both computation and network delays. Moreover, the combination of analytical and simulation analysis methods enables the computation of both the guaranteed Worst Case Execution Time (WCET) and the detailed execution time-line data for real-time tasks. As a result, the analysis yields the identification of an optimal architecture, with respect to real-time deadlines, robustness and system costs. The paper main contributions are the cycle-accurate performance analysis workflow and supportive open-source ProMARTES tool-chain, both incorporating a network prediction model in all the performance analysis phases. © 2016 Elsevier Inc. All rights reserved.","Analysis; Distributed; Performance; Real-time; Scheduling; Simulation"
"Prioritized static slicing and its application to fault localization","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960116028&doi=10.1016%2fj.jss.2015.10.052&partnerID=40&md5=0acbd7585e7adf930b0da35e516a1991","Static slicing is a popular program analysis used in software engineering to find which parts of a program affect other parts. Unfortunately, static slicing often produces large and imprecise results because of its conservative nature. Dynamic slicing can be an alternative in some cases, but it requires detailed runtime information that can be hard or impossible to obtain or re-create. This is often the case when users report bugs in deployed software. In this paper, we significantly improve the precision of static slicing through PrioSlice, a novel technique that exploits the insight that not all statements in a static slice are equally likely to affect another statement such as a failing point. PrioSlice first computes a probabilistic model of the dependencies in the program. In this model, some data dependencies are more likely to occur than others and control dependencies are less likely than data dependencies to propagate effects (e.g., errors). PrioSlice then traverses the program backwards, like static slicing, but in an order defined by the computed dependence probabilities. Our study of fault localization on various Java subjects indicates that PrioSlice can help localize faults much more effectively than existing static-slicing approaches. © 2015 Elsevier Inc. All rights reserved.","Dependence analysis; Fault localization; Probabilistic slicing; Program analysis; Static slicing; Thin slicing"
"Asymmetric-histogram based reversible information hiding scheme using edge sensitivity detection","2016","Journal of Systems and Software","10.1016/j.jss.2015.04.085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929629233&doi=10.1016%2fj.jss.2015.04.085&partnerID=40&md5=2f6fd834613e989bf91a3078253e2d51","Histogram shifting is a common reversible data hiding method that can effectively embed secret information with the distribution of image pixel value (or error). However, the image quality of histogram shifting depends on the distance between the zero point and peak point, and a long distance may cause serious image distortion. This study employs an edge sensitivity analysis method established by Lukac et al. to reduce the prediction error and integrates the asymmetric-histogram shifting established by Chen et al. to restore the error value to a place near the original image pixel value in the second shift. The results show that the pixel complementary mechanism has better image quality in multi-level embedding, especially for smooth images. Images with different characteristics will have better information capacity and image quality through two predictive methods. © 2015 Elsevier Inc. All rights reserved.","Edge sensitivity analysis; Histogram shifting; Reversible data hiding"
"Certificate-based encryption resilient to key leakage","2016","Journal of Systems and Software","10.1016/j.jss.2015.05.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936805285&doi=10.1016%2fj.jss.2015.05.066&partnerID=40&md5=7678cac40b889d00dff3fc915ba9ecfa","Certificate-based encryption (CBE) is an important class of public key encryption but the existing schemes are secure only under the premise that the decryption key (or private key) and master secret key are absolutely secret. In fact, a lot of side channel attacks and cold boot attacks can leak secret information of a cryptographic system. In this case, the security of the cryptographic system is destroyed, so a new model called leakage-resilient (LR) cryptography is introduced to solve this problem. While some traditional public key encryption and identity-based encryption with resilient-leakage schemes have been constructed, as far as we know, there is no leakage-resilient scheme in certificate-based cryptosystems. This paper puts forward the first certificate-based encryption scheme which can resist not only the decryption key leakage but also the master secret key leakage. Based on composite order bilinear group assumption, the security of the scheme is proved by using dual system encryption. The relative leakage rate of key is close to 1/3. © 2015 Elsevier Inc.","Certificate-based encryption; Dual system encryption; Master secret key leakage"
"An approach to estimation of degree of customization for ERP projects using prioritized requirements","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964203444&doi=10.1016%2fj.jss.2016.04.006&partnerID=40&md5=415df9ce3efc266a7990ee664a1a595a","Customization in ERP projects is a risky, but unavoidable undertaking that companies need to initiate in order to achieve alignment between their acquired ERP solution and their organizational goals and business processes. Conscious about the risks, many companies commit to leveraging the off-the-shelf built-in functionality in their chosen ERP package, keeping customization at a minimum level so that it does not jeopardize the project or the future projects that would build upon it. However, many organizations experience that once the project team enters the stage of implementing the solution, requests for customization increase in volume and diversity. Managing properly the process of customization gets increasingly harder. This paper addresses the problem of estimating the degree of customization at an early stage of ERP implementation. This will support customization decision makers in making value and cost trade-offs when approving requests for customization. We propose a solution approach in which customization requirements are reasoned in quantitative terms. Our approach uses client-prioritized requirements for the estimation of degree of customization during the ERP implementation. A case study is used to illustrate the application of the proposed approach. We also discuss the strengths and limitations of our approach as well as its implications for research and practice. © 2016 Elsevier Inc. All rights reserved.","Case study; Customization; Decision making; Enterprise resource planning (ERP) projects; Requirements prioritization"
"Modeling and analysis of reliability of multi-release open source software incorporating both fault detection and correction processes","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960862129&doi=10.1016%2fj.jss.2016.01.025&partnerID=40&md5=3ce81149b133cbcd29741a0f3c27b561","Large software systems require regular upgrading that tries to correct the reported faults in previous versions and add some functions to meet new requirements. It is thus necessary to investigate changes in reliability in the face of ongoing releases. However, the current modeling frameworks mostly rely on the idealized assumption that all faults will be removed instantaneously and perfectly. In this paper, the failure processes in testing multi-release software are investigated by taking into consideration the delays in fault repair time based on a proposed time delay model. The model is validated on real test datasets from the software that has been released three times with new features. A comprehensive analysis of optimal release times based on cost-efficiency is also provided, which could help project managers to determine the best time to release the software. © 2016 Elsevier Inc. All rights reserved.","Fault correction process; Multiple upgrading; Software reliability"
"Cost-effective regression testing through Adaptive Test Prioritization strategies","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960881560&doi=10.1016%2fj.jss.2016.01.018&partnerID=40&md5=9b0cfcfba1c7ed8b6ab89a52e5660c8c","Regression testing is an important part of the software development life cycle. It is also very expensive. Many different techniques have been proposed for reducing the cost of regression testing. However, research has shown that the effectiveness of different techniques varies under different testing environments and software change characteristics. In prior work, we developed strategies to investigate ways of choosing the most cost-effective regression testing technique for a particular regression testing session. In this work, we empirically study the existing strategies presented in prior work as well as develop two additional Adaptive Test Prioritization (ATP) strategies using fuzzy analytical hierarchy process (AHP) and the weighted sum model (WSM). We also provide a comparative study examining each of the ATP strategies presented to date. This research will provide researchers and practitioners with strategies to utilize in regression testing plans as well as provide data to use when deciding which of the strategies would best fit their testing needs. The empirical studies provided in this research show that utilizing these strategies can improve the cost-effectiveness of regression testing. © 2016 Elsevier Inc. All rights reserved.","Adaptive regression testing strategy; Regression testing; Test case prioritization"
"Multi-level agile project management challenges: A self-organizing team perspective","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962195713&doi=10.1016%2fj.jss.2016.02.049&partnerID=40&md5=486c2b8d87665f9097dc153fe59e98e0","Agile software development advocates self-organizing teams that display high levels of autonomy. Self-organizing agile teams are meant to share project management activities such as estimation, planning, and requirements elicitation with managers and customers. While prior literature has explored some individual management-related issues, little is known about how the high involvement of self-organizing agile teams influences everyday project management activities. Through a Grounded Theory study involving 21 agile practitioners across six software companies implementing scrum and XP, we identified a set of eight project management challenges as experienced by and as a result of self-organizing agile teams at multiple levels. These include delayed/changing requirements and eliciting senior management sponsorship at the project level; achieving cross-functionality and effective estimations at the team level; asserting autonomy and self-assignment at the individual level, and lack of acceptance criteria and dependencies at the task level. A mapping between the emergent challenges and standard project management activities is also presented. The article also shares practical implications and guidelines for agile teams, their managers, and customers for overcoming some of these challenges. © 2016 Elsevier Inc. All rights reserved.","Agile software development; Project management; Self-organizing teams"
"Automated design of multi-layered web information systems","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965159289&doi=10.1016%2fj.jss.2016.04.060&partnerID=40&md5=73828d0c2a0a000a877ee7e65278f4e1","In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in MDWE approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE. © 2016 Elsevier Inc. All rights reserved.","Automated design; Domain-specific language; Experience report; Mockup; Model-driven web engineering; Prototyping; Rapid application prototype"
"Preserving architectural styles in the search based design of software product line architectures","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959319935&doi=10.1016%2fj.jss.2016.01.039&partnerID=40&md5=26a38167189dc84b18847bdde4d16b12","Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values. © 2016 Elsevier Inc. All rights reserved.","Architectural style; Search based design; Software product line"
"A framework for capturing, statistically modeling and analyzing the evolution of software models","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969190349&doi=10.1016%2fj.jss.2016.05.010&partnerID=40&md5=b1964cd7dcad36eed7033a55fa3dc980","This paper presents a new methodological framework for capturing and statistically modeling the evolution of models in model-driven software development. The framework captures the changes between revisions of models in terms of both low-level (internal) and high-level (developer-visible) edit operations applied between revisions. In our approach, evolution is modeled statistically by using ARMA, GARCH and mixed ARMA-GARCH models. Forecasting and simulation aspects of these time series models are thoroughly assessed. The suitability of the framework is shown by applying it to a large set of design models of real Java systems. Our analysis shows that mixed ARMA-GARCH models are superior to ARMA models. A main motivation for, and application of, the resulting statistical models is to control the generation of realistic model histories which are intended to be used for testing model versioning tools. We present the architecture of the model generator and show how to generate random sequences from the statistical models which control the generation process. Further usages of the statistical models include various forecasting and simulation tasks. © 2016 Elsevier Inc. All rights reserved.","Forecasting; Model driven engineering; Simulation; Software model evolution analysis; Test model generation; Time series analysis"
"Virtualization-based Cognitive Radio Networks","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960172642&doi=10.1016%2fj.jss.2016.02.014&partnerID=40&md5=cfc0b12f5977e11aceda7fbec94eaade","The emerging network virtualization technique is considered as a promising technology that enables the deployment of multiple virtual networks over a single physical network. These virtual networks are allowed to share the set of available resources in order to provide different services to their intended users. While several previous studies have focused on wired network virtualization, the field of wireless network virtualization is not well investigated. One of the promising wireless technologies is the Cognitive Radio (CR) technology that aims to handle the spectrum scarcity problem through efficient Dynamic Spectrum Access (DSA). In this paper, we propose to incorporate virtualization concepts into CR Networks (CRNs) to improve their performance. We start by explaining how the concept of multilayer hypervisors can be used within a CRN cell to manage its resources more efficiently by allowing the CR Base Station (BS) to delegate some of its management responsibilities to the CR users. By reducing the CRN users' reliance on the CRN BS, the amount of control messages can be decreased leading to reduced delay and improved throughput. Moreover, the proposed framework allows CRNs to better utilize its resources and support higher traffic loads which is in accordance with the recent technological advances that enable the Customer-Premises Equipments (CPEs) of potential CR users (such as smart phone users) to concurrently run multiple applications each generating its own traffic. We then show how our framework can be extended to handle multi-cell CRNs. Such an extension requires addressing the self-coexistence problem. To this end, we use a traffic load aware channel distribution algorithm. Through simulations, we show that our proposed framework can significantly enhance the CRN performance in terms of blocking probability and network throughput with different primary user level of activities. © 2016 Elsevier Inc. All rights reserved.","Coexistence problem; Software defined radio; Wireless network virtualization"
"Grounded requirements engineering: An approach to use case driven requirements engineering","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961279419&doi=10.1016%2fj.jss.2015.10.024&partnerID=40&md5=276f0bd432597fc8ee1684a08d9bda7b","Requirements engineering produces specifications of the needs or conditions to meet for a software product. These specifications may be vague and ungrounded, i.e. the relation of the requirements to the observations they are derived from may be unclear or not documented. Furthermore, stakeholders may be influenced by solutions of existing software without knowing if these actually suit the software to be developed. To cope with the above issues, it is important to understand the complete task, before designing a software system to support the task. Thus, we developed a method called Grounded Requirements Engineering (GRE) that leverages the Grounded Theory method to observe and analyze processes and user activities in the real world. GRE is an iterative process consisting of two steps. First, Grounded Theory methods are used to analyze user experiments or interviews. Second, the resulting abstract descriptions of the user behavior are transferred into use cases. GRE produces comprehensible and grounded requirements for the software system to be built, i.e. the requirements are traceable back to their origins. In this paper, we provide an elaborate description of the GRE method and illustrate it by applying it to derive requirements for an interactive software tool for model merging. The development of this tool both served as a basis for the design of GRE as well as to test it. © 2015 Elsevier Inc.","Grounded Theory; Requirements; Software engineering"
"A family of experiments to evaluate the understandability of TRiStar and i∗ for modeling teleo-reactive systems","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960088300&doi=10.1016%2fj.jss.2015.12.056&partnerID=40&md5=96e2199285dbed8b12bf19bba6a1b77c","The teleo-reactive approach facilitates reactive system development without losing sight of the system goals. Objective To introduce TRiStar as an extension of i∗ notation to specify teleo-reactive systems. To evaluate whether the notational extension is an improvement in terms of effectiveness and efficiency over the original language when it is used to specify teleo-reactive systems. Method A family of experiments was carried out with final-year engineering students and experienced software development professionals in which the participants were asked to fill in a form designed to evaluate the efficiency and effectiveness of each of the languages. Results Both the statistical results of the experiments, analyzed separately, and the meta-analysis of the experiments as a whole, allow us to conclude that TRiStar notation is more effective and efficient than i∗ as a requirements specification language for modeling teleo-reactive systems. Conclusion The extensions made on i∗ have led to TRiStar definition, a more effective and efficient goal-oriented notation than the original i∗ language. © 2016 Elsevier Inc. All rights reserved.","i; Requirements engineering; Teleo-reactive; TRiStar"
"Dynamic auto-scaling and scheduling of deadline constrained service workloads on IaaS clouds","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969814716&doi=10.1016%2fj.jss.2016.05.011&partnerID=40&md5=bc4743285ce04c544b72d3db12ce9e44","Cloud systems are becoming attractive for many companies. Rather than over-provisioning the privately owned infrastructure for peak demands, some of the work can be overspilled to external infrastructure to meet deadlines. In this paper, we investigate how to dynamically and automatically provision resources on the private and external clouds such that the number of workloads meeting their deadline is maximized. We specifically focus on jobs consisting of multiple interdependent tasks with a priori an unknown structure and even adaptable at runtime. The proposed approach is model-driven: knowledge on the job structure on the one hand; and resource needs and scaling behavior on the other hand. Information is built up based on monitoring information and simulated 'what-if'-scenarios. Using this dynamically constructed job resource model, the resources needed by each job in order to meet its deadline is derived. Different algorithms are evaluated on how the required resources and jobs are scheduled over time on the available infrastructure. The evaluation is carried out using synthetic workloads. © 2016 Elsevier Ltd. All rights reserved.","Cloud computing; Deadline constrained workflow scheduling; Dynamic resource allocation"
"Analyzing defect inflow distribution and applying Bayesian inference method for software defect prediction in large software projects","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961641102&doi=10.1016%2fj.jss.2016.02.015&partnerID=40&md5=3b637fdba232ae642c577e2147fcc232","Tracking and predicting quality and reliability is a major challenge in large and distributed software development projects. A number of standard distributions have been successfully used in reliability engineering theory and practice, common among these for modeling software defect inflow being exponential, Weibull, beta and Non-Homogeneous Poisson Process (NHPP). Although standard distribution models have been recognized in reliability engineering practice, their ability to fit defect data from proprietary and OSS software projects is not well understood. Lack of knowledge about underlying defect inflow distribution also leads to difficulty in applying Bayesian based inference methods for software defect prediction. In this paper we explore the defect inflow distribution of total of fourteen large software projects/release from two industrial domain and open source community. We evaluate six standard distributions for their ability to fit the defect inflow data and also assess which information criterion is practical for selecting the distribution with best fit. Our results show that beta distribution provides the best fit to the defect inflow data for all industrial projects as well as majority of OSS projects studied. In the paper we also evaluate how information about defect inflow distribution from historical projects is applied for modeling the prior beliefs/experience in Bayesian analysis which is useful for making software defect predictions early during the software project lifecycle. ©2016 Elsevier Inc. All rights reserved.","Defect Inflow; Software; SRGM"
"Revisiting software ecosystems Research: A longitudinal literature study","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960154891&doi=10.1016%2fj.jss.2016.02.003&partnerID=40&md5=8114cbaff593456b53c3c4d8a1d911b8","'Software ecosystems' is argued to first appear as a concept more than 10 years ago and software ecosystem research started to take off in 2010. We conduct a systematic literature study, based on the most extensive literature review in the field up to date, with two primarily aims: (a) to provide an updated overview of the field and (b) to document evolution in the field. In total, we analyze 231 papers from 2007 until 2014 and provide an overview of the research in software ecosystems. Our analysis reveals a field that is rapidly growing, both in volume and empirical focus, while becoming more mature. We identify signs of field maturity from the increase in: (i) the number of journal articles, (ii) the empirical models within the last two years, and (iii) the number of ecosystems studied. However, we note that the field is far from mature and identify a set of challenges that are preventing the field from evolving. We propose means for future research and the community to address them. Finally, our analysis shapes the view of the field having evolved outside the existing definitions of software ecosystems and thus propose the update of the definition of software ecosystems. © 2016 Elsevier Inc. All right sreserved.","Longitudinal literature study; Software ecosystem maturity; Software ecosystems"
"10 years of software architecture knowledge management: Practice and future","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945923743&doi=10.1016%2fj.jss.2015.08.054&partnerID=40&md5=40b36f608da91c11134e3809c1574c6c","The importance of architectural knowledge (AK) management for software development has been highlighted over the past ten years, where a significant amount of research has been done. Since the first systems using design rationale in the seventies and eighties to the more modern approaches using AK for designing software architectures, a variety of models, approaches, and research tools have leveraged the interests of researchers and practitioners in AK management (AKM). Capturing, sharing, and using AK has many benefits for software designers and maintainers, but the cost to capture this relevant knowledge hampers a widespread use by software companies. However, as the improvements made over the last decade didn't boost a wider adoption of AKM approaches, there is a need to identify the successes and shortcomings of current AK approaches and know what industry needs from AK. Therefore, as researchers and promoters of many of the AK research tools in the early stages where AK became relevant for the software architecture community, and based on our experience and observations, we provide in this research an informal retrospective analysis of what has been done and the challenges and trends for a future research agenda to promote AK use in modern software development practices. © 2015 Elsevier Inc. All rights reserved.","Agile development; Architectural design decisions; Architectural knowledge management"
"Systematic literature review of ensemble effort estimation","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969531836&doi=10.1016%2fj.jss.2016.05.016&partnerID=40&md5=4217c2107911348615a692540959501e","The need to overcome the weaknesses of single estimation techniques for prediction tasks has given rise to ensemble methods in software development effort estimation (SDEE). An ensemble effort estimation (EEE) technique combines several of the single/classical models found in the SDEE literature. However, to the best of our knowledge, no systematic review has yet been performed with a focus on the use of EEE techniques in SDEE. The purpose of this review is to analyze EEE techniques from six viewpoints: single models used to construct ensembles, ensemble estimation accuracy, rules used to combine single estimates, accuracy comparison of EEE techniques with single models, accuracy comparison between EEE techniques and methodologies used to construct ensemble methods. We performed a systematic review of EEE studies published between 2000 and 2016, and we selected 24 of them to address the questions raised in this review. We found that EEE techniques may be separated into two types: homogeneous and heterogeneous, and that the machine learning single models are the most frequently employed in constructing EEE techniques. We also found that EEE techniques usually yield acceptable estimation accuracy, and in fact are more accurate than single models. © 2016 Elsevier Inc. All rights reserved.","Ensemble effort estimation; Software development effort estimation; Systematic literature review"
"Challenges and success factors for large-scale agile transformations: A systematic literature review","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975144622&doi=10.1016%2fj.jss.2016.06.013&partnerID=40&md5=4a44356788c2006d60d69246e0527ae4","Agile methods have become an appealing alternative for companies striving to improve their performance, but the methods were originally designed for small and individual teams. This creates unique challenges when introducing agile at scale, when development teams must synchronize their activities, and there might be a need to interface with other organizational units. In this paper we present a systematic literature review on how agile methods and lean software development has been adopted at scale, focusing on reported challenges and success factors in the transformation. We conducted a systematic literature review of industrial large-scale agile transformations. Our keyword search found 1875 papers. We included 52 publications describing 42 industrial cases presenting the process of taking large-scale agile development into use. Almost 90% of the included papers were experience reports, indicating a lack of sound academic research on the topic. We identified 35 reported challenges grouped into nine categories, and 29 success factors, grouped into eleven categories. The most salient success factor categories were management support, choosing and customizing the agile model, training and coaching, and mindset and alignment. © 2016 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license.","Adopting agile software development; Agile software development; Challenges; Large-scale agile; Organizational transformation; Success factors; Systematic literature review"
"Quantitatively measuring a large-scale agile transformation","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962802381&doi=10.1016%2fj.jss.2016.03.029&partnerID=40&md5=7ce0a731114ca82dc2f10b7ea05803b7","Context: Agile software development continues to grow in popularity and is being adopted by more and more organizations. However, there is a need for empirical evidence on the impact, benefits and drawbacks of an agile transformation in an organization since the cost for such a transformation in terms of money, disrupted working routines and quality of development can become considerable. Currently, such evidence exists in the form of success stores and case studies, mostly of qualitative nature. Objective: Provide a metrics model to quantitatively measure the impact of an agile transformation in a software development organization. Method: The metrics model was elicited with the use of the Goal Question Metric approach. Results: A quantitative metrics model containing eight rigorously described metrics is presented and followed by its application to evaluate an agile and lean transformation in a large international telecommunication organization with 350 employees in two sites. Conclusions: The metrics model was sensitive to the changes that occurred in the organization and revealed significant improvements in six of the eight metrics and a deterioration in one of the metrics. © 2016 Elsevier Inc. All rights reserved.","Agile; Metrics; Transformation"
"Comparing reuse practices in two large software-producing companies","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964997210&doi=10.1016%2fj.jss.2016.03.067&partnerID=40&md5=21e4c940cc22cb54f85f8e913a0d28b1","Context Reuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice? Objective We propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies. Method We compare and interpret the study results with a focus on reuse practices, effects, and context. Results Both companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access. Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. In a heterogeneous context with fragmented infrastructure, these benefits did not materialize. Neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. In both cases, a lack of reuse led to duplicate implementations. Conclusion Technological advances have improved the way reuse concepts can be applied in practice. Homogeneity in development process and tool support seem necessary preconditions. Developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging. © 2016 Elsevier Inc. All rights reserved.","Empirical; Software engineering; Software reuse; Survey research; Technology transfer"
"Architecture-based regulatory compliance argumentation","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973636747&doi=10.1016%2fj.jss.2016.04.057&partnerID=40&md5=0da3cd3a88dfae4b949ca1d6effe9937","Standards and regulations are difficult to understand and map to software, which makes compliance with them challenging to argue for software products and development process. This is problematic since lack of compliance may lead to issues with security, safety, and even to economic sanctions. An increasing number of applications (for example in healthcare) are expected to have to live up to regulatory requirements in the future, which will lead to more software development projects having to deal with such requirements. We present an approach that models regulations such that compliance arguments can be made in a principled way based on architectural requirements and architectural decisions. In particular, we discuss how one can form architectural requirements which are linked to regulatory texts. We then argue for completeness and correctness of this bi-directional link. We evaluate the approach on the migration of the telemedicine platform Net4Care to the cloud, where certain regulations (for example privacy) should be concerned. The approach has the potential to support simpler compliance argumentation with the eventual promise of safer and more secure applications. © 2016 Published by Elsevier Inc.","Regulatory compliance; Software architecture; Software development"
"A novel kernel to predict software defectiveness","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975270426&doi=10.1016%2fj.jss.2016.06.006&partnerID=40&md5=9559939e83f4094c5ef7fdd3b7ce66a6","Although the software defect prediction problem has been researched for a long time, the results achieved are not so bright. In this paper, we propose to use novel kernels for defect prediction that are based on the plagiarized source code, software clones and textual similarity. We generate precomputed kernel matrices and compare their performance on different data sets to model the relationship between source code similarity and defectiveness. Each value in a kernel matrix shows how much parallelism exists between the corresponding files of a software system chosen. Our experiments on 10 real world datasets indicate that support vector machines (SVM) with a precomputed kernel matrix performs better than the SVM with the usual linear kernel in terms of F-measure. Similarly, when used with a precomputed kernel, the k-nearest neighbor classifier (KNN) achieves comparable performance with respect to KNN classifier. The results from this preliminary study indicate that source code similarity can be used to predict defect proneness. © 2016 Elsevier Inc. All rights reserved.","Defect prediction; Kernel methods; SVM"
"How project description length and expected duration affect bidding and project success in crowdsourcing software development","2016","Journal of Systems and Software","10.1016/j.jss.2015.03.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925880482&doi=10.1016%2fj.jss.2015.03.039&partnerID=40&md5=9bb8d10be824a08bcf39a2a29eb69627","On crowdsourcing software development sites providers bid on very short term request for proposals (median 7 days) that are described in brief (median 241 words). Because of its size, because buyers have the power to refuse to accept the delivered project, and because all contracts are fixed price, this type of market presents a unique context of software development contracting. We examine this market through the lens of a reverse agency problem. Specifically, we examine how expected project duration and description length affect the amount providers bid and subsequent project success (i.e. that the buyer agreed to pay for it upon delivery). Results show that, as might be expected, projects described at greater length or expected to require more time commanded higher bid prices and were more likely to be successful. However, reviewing the residuals reveals that projects that were eventually unsuccessful were actually bid at higher prices than what they should have been bid at considering their description length and expected duration. Post-hoc analysis suggests that apparently the agents were correct in their relatively higher bidding on the eventually unsuccessful projects because those projects were mostly shorter - and buyers were less accurate in their duration assessment of shorter projects. The risks involved in the possibility that buyers may be signaling wrongly to providers about the project are discussed. © 2015 Elsevier Inc. All rights reserved.","Crowdsourcing software development; Project duration; Request for proposals"
"Code search with input/output queries: Generalizing, ranking, and assessment","2016","Journal of Systems and Software","10.1016/j.jss.2015.04.081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929591037&doi=10.1016%2fj.jss.2015.04.081&partnerID=40&md5=9e785f7eabf7a0604f65cc71ed282a6e","In this work we generalize, improve, and extensively assess our semantic source code search engine through which developers use an input/output query model to specify what behavior they want instead of how it may be implemented. Under this approach a code repository contains programs encoded as constraints and an SMT solver finds encoded programs that match an input/output query. The search engine returns a list of source code snippets that match the specification. The initial instantiation of this approach showed potential but was limited. It only encoded single-path programs, reported just complete matches, did not rank the results, and was only partly assessed. In this work, we explore the use of symbolic execution to address some of these technical shortcomings. We implemented a tool, Satsy, that uses symbolic execution to encode multi-path programs as constraints and a novel ranking algorithm based on the strength of the match between an input/output query and the program paths traversed by symbolic execution. An assessment about the relevance of Satsy's results versus other search engines, Merobase and Google, on eight novice-level programming tasks gathered from StackOverflow, using the opinions of 30 study participants, reveals that Satsy often out-performs the competition in terms of precision, and that matches are found in seconds. © 2015 Elsevier Inc. All rights reserved.","Semantic code search; SMT solvers; Symbolic execution"
"Using traffic filtering rules and OpenFlow devices for transparent flow switching and automatic dynamic-circuit creation in hybrid networks","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960157909&doi=10.1016%2fj.jss.2016.02.019&partnerID=40&md5=f5e6e2444ad5579c703af997455a383f","Hybrid networks that offer both the traditional IP packet forwarding and a virtual dynamic circuit service are becoming widely available in many backbones. The benefits of this approach are well-known and some high-volume traffic applications, e.g., high-energy physics and astronomy, can obtain significant improvements by using circuits. However, end-users still depend on specific-purpose applications or the full assistance of network operators to connect their data sources and sinks to circuits. We present a general-purpose system that transparently connects end-users network infrastructure to both services of a hybrid network. By employing high-level filtering rules, similar to conventional IP packet filters, end-users can choose which part of their traffic must flow through circuits and which part must continue flowing through the traditional forwarding. These rules are applied to OpenFlow devices, which properly switch the flows between the forwarding services and automatically trigger the creation of a circuit as necessary. We have evaluated our system in the laboratory and in the backbone of the Brazilian NREN (RNP). The results illustrate the expected gains while using dynamic circuits and also validate the efficiency and robustness of our system. We also show the advantage of using our system in comparison with a traditional middleware for circuit creation. © 2016 Elsevier Inc. All rights reserved.","Hybrid networks; Software-defined networking; Traffic filtering rules"
"Can we ask you to collaborate? Analyzing app developer relationships in commercial platform ecosystems","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977922743&doi=10.1016%2fj.jss.2015.11.025&partnerID=40&md5=194c3e899aedd3fe110c76acf355c70b","Previous studies have emphasized the necessity for software platform owners to govern their platform ecosystem in order to create durable opportunities for themselves and the app developers that surround the platform. To date, platform ecosystems have been widely analyzed from the perspective of platform owners. However, how and to what extent app developers collaborate with their peers needs to be investigated further. In this article, we study the interfirm relationships among app developers in commercial platform ecosystems and explore the causes of variation in the network structure of these ecosystems. By means of a comparative study of four commercial platform ecosystems of Google (Google Apps and Google Chrome) and Microsoft (Microsoft Office365 and Internet Explorer), we illustrate substantial variation in the extent to which app developers initiated interfirm relationships. Further, we analyze how the degree of enforced entry barriers to the app store, the use of a partnership model, and the domain of the software platform that underpins the ecosystem affect the properties of these commercial platform ecosystems. We present subsequent explanations as a set of propositions that can be tested in future empirical research. © 2015 Elsevier Inc. All rights reserved.","Case study; Interfirm network analysis; Software ecosystem"
"Conceptual model of working space for Agile (Scrum) project team","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966263670&doi=10.1016%2fj.jss.2016.04.071&partnerID=40&md5=505a2d8b3b7c3c60b89b0b4e445747e7","IT project management according to the Agile principles will be less effective if the space where the teams work is not arranged properly. In this paper, a model of office space arrangement for the needs of teams using the Agile methodology is proposed. The main aim of the model is to support Scrum Teams in carrying out project tasks in a more efficient and effective way. The conceptual model is based on requirements that should be fulfilled by offices destined to have Agile (Scrum) working teams and can be adopted in any organization. The model has been implemented and validated. © 2016 Elsevier Inc. All rights reserved.","Agile project management; Agile transition; Office layout; Office rearrangement; Scrum Team; Work environment"
"Spot pricing in the Cloud ecosystem: A comparative investigation","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960130505&doi=10.1016%2fj.jss.2015.10.042&partnerID=40&md5=e6195eb0b919e46237bc91719e5174bc","Background: Spot pricing is considered as a significant supplement for building a full-fledged market economy for the Cloud ecosystem. However, it seems that both providers and consumers are still hesitating to enter the Cloud spot market. The relevant academic community also has conflicting opinions about Cloud spot pricing in terms of revenue generation. Aim: This work aims to systematically identify, assess, synthesize and report the published evidence in favor of or against spot-price scheme compared with fixed-price scheme of Cloud computing, so as to help relieve the aforementioned conflict. Method: We employed the systematic literature review (SLR) method to collect and investigate the empirical studies of Cloud spot pricing indexed by major electronic libraries. Results: This SLR identified 61 primary studies that either delivered discussions or conducted experiments to perform comparison between spot pricing and fixed pricing in the Cloud domain. The reported benefits and limitations were summarized to facilitate cost-benefit analysis of being a Cloud spot pricing player, while four types of theories were distinguished to help both researchers and practitioners better understand the Cloud spot market. Conclusions: This SLR shows that the academic community strongly advocates the emerging Cloud spot market. Although there is still a lack of practical and easily deployable market-driven mechanisms, the overall findings of our work indicate that spot pricing plays a promising role in the sustainability of Cloud resource exploitation. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Cloud spot pricing; Systematic literature review"
"An aesthetic QR code solution based on error correction mechanism","2016","Journal of Systems and Software","10.1016/j.jss.2015.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940094286&doi=10.1016%2fj.jss.2015.07.009&partnerID=40&md5=66967eb51fbe325ad69da3825c3ed663","QR code(Quick Response Code) is a popular two-dimensional matrix that randomly consists of black and white square modules. While the appearance of QR codes are often visually unpleasant, it leads to the increasing demand for the aesthetic QR codes. However, it may turn out to be unreadable if changes to the modules of the QR code are inadequate. Therefore, to resolve this conflict, we propose a method to generate an aesthetic QR code, which is based on the RS(Reed-Solomon) error correction mechanism in QR code encoding rules. First, according to the characteristics of the QR code, we mark the positions of codewords as codeword layout. Then, we detect salient regions of the background image to generate the saliency map. The next step is to combine it with the saliency map and codeword layout to calculate saliency values, then sort and select proper codewords as changeable regions. Finally, we propose the hierarchical module replacement rules. The theoretical maximum value of the changeable areas is the redundancy capacity T of RS error correction. Compared with the existing methods, our algorithm can maximize the changeable areas and highlight the important regions of background image. This algorithm has better aesthetic effects, while maintaining the rate of successful decoding. © 2015 Elsevier Inc. All rights reserved.","2D barcode; Aesthetic QR code; Saliency detection technology"
"Signal: An open-source cross-platform universal messaging system with feedback support","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960171652&doi=10.1016%2fj.jss.2016.02.018&partnerID=40&md5=82e48ff98753d7430ecc30940f74558a","This research addressed challenges of the cross-platform push messaging service where every modern device could send messages and receive feedback among a multitude of different receiving devices. The design and implementation of Signal- A universal cross-platform messaging system-was proposed as a solution. Signal provided both the messaging service middleware and application. It comprised a set of messaging languages for describing, querying and mapping message payloads as well as defining and customizing feedback. These languages were describable, human readable, and extensible, each of which came equipped with its own parser. The Signal architecture was designed for scalability by building extensible modules and applying scalable convenient tools, namely, RabbitMQ, Google-GSON and Quartz Scheduler. Its real-time schedulers were well-suited for a wide range of message payloads. It could deliver messages, practically, to all major messaging platforms with accessible APIs, HTML5-enabled WebSockets, and TCP-based applications. The research was validated via two case studies. The results showed ease of deployment through the unified Signal APIs while being able to attain a 100% reach to receivers with the best achievable performance of more than 1,300,000 basic notification payloads in a second. © 2016 Elsevier Inc. All rights reserved.","Feedback; Messaging; Notification"
"An empirical study on the effect of 3D visualization for project tasks and resources","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960927902&doi=10.1016%2fj.jss.2016.01.011&partnerID=40&md5=ad90bae2639971cd9c8b60949bc667a0","During software development, project managers (PMs) continually monitor, analyze and control the project schedule. The schedule contains tasks, work items, and resources assigned to carry out the tasks. Current state-of-the-art monitoring involves methods such as Gantt charts and spreadsheets/tables to display and analyze the project schedule, tasks and resources information PMBOK (2004). These methods, however, have certain limitations. It is difficult to see the entire schedule in a single view and analyze the tasks and resources especially in the case of large data. There is also little support for interacting with the data, and the Gantt chart does not show history information and trends. In this paper, we develop an approach that uses 3D visualizations to represent information about project tasks and resources, to overcome the above limitations. To assess our approach, we conduct an empirical study on real-world projects using 42 participants from both academia and industry. We developed a prototype tool named 3DProjView for the study. The study compared the effectiveness and efficiency of using 3D visualizations versus Gantt chart and tables. The results indicate that participants using 3D visualizations achieved an average of 40% higher accuracy and spent on average 39% less time analyzing project tasks and resources, further indicating that our approach effectively helps project managers in both accuracy and efficiency for monitoring project performance. © 2016 Elsevier Inc. All rights reserved.","Empirical study; Project management; Software visualization"
"Towards better help desk planning: Predicting incidents and required effort","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964344416&doi=10.1016%2fj.jss.2016.03.063&partnerID=40&md5=003cbd9e5810db7001268ffc921d06ce","In this case study, a cost model for help desk operations is developed. The cost model relates predicted incidents to labor costs. Since incident estimation for hundreds of products is time-consuming, we use cluster analysis to group similarly behaving products in clusters, for which we then estimate incidents based on the representative product in the cluster. Incidents are predicted using software reliability growth models. The cost to resolve the incidents is predicted using historical labor data for the resolution of incidents. Cluster analysis is used to group products with similar help desk incident characteristics. We use Principal Components Analysis to determine one product per cluster for the prediction of incidents for all members of the cluster, so as to reduce estimation cost. We were able to predict incidents for a cluster based on this product alone and do so successfully for all clusters with accuracy comparable to making predictions for each product in the portfolio. Linear regression is used with cost data for the resolution of incidents to relate incident predictions to help desk labor costs. The cost model is then validated by successfully demonstrating cost prediction accuracy for one month prediction intervals over a 22 month period. © 2016 Elsevier Inc. All rights reserved.","Cluster analysis; Cost model; Help desk; IT operations; Principal components analysis; Software reliability growth model"
"Software outsourcing partnership model: An evaluation framework for vendor organizations","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964670659&doi=10.1016%2fj.jss.2016.03.069&partnerID=40&md5=f0804bb193242e007f6b3b5f3da1af4c","Software Outsourcing Partnership (SOP) is a new software development paradigm for developing high quality software products. A SOP is different to ordinary software development outsourcing (SDO) relationship. SOP is the enhanced form of conventional outsourcing relationship. The objective of this research paper is to develop a software outsourcing partnership model (SOPM) to identify and analyze factors that are important for vendors in conversion of their existing outsourcing relationship to partnership. We have performed a systematic literature review (SLR) process for the identification of critical success factors (CSFs) from a sample of 111 articles. Further we have categorized the identified CSFs into five partnership levels based on Capability Maturity Model Integration (CMMI) and Software Outsourcing Vendors' Readiness Model (SOVRM). To validate the SLR findings and to find practices for the identified CSFs a questionnaire survey was conducted in the outsourcing industry in which 35 experts, from 8 different countries participated. Two case studies were conducted for evaluation of the SOPM. In this paper our newly developed model, SOPM, has been presented in detail. SOPM has been built with the intent to assist SDO vendor organizations in measuring their capabilities for successful conversion of their contractual outsourcing relationship to outsourcing partnership. © 2016 Elsevier Inc. All rights reserved.","Critical success factors; Software outsourcing partnership; Systematic literature review"
"Test case prioritization of build acceptance tests for an enterprise cloud application: An industrial case study","2016","Journal of Systems and Software","10.1016/j.jss.2016.06.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975883639&doi=10.1016%2fj.jss.2016.06.017&partnerID=40&md5=2fd0b23046071b1b4a25dbab3b359637","The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize BAT tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't. © 2016 Elsevier Inc.","Cloud computing; Prioritization; Regression testing; Software as a service"
"Software architectural principles in contemporary mobile software: from conception to practice","2016","Journal of Systems and Software","10.1016/j.jss.2016.05.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977104077&doi=10.1016%2fj.jss.2016.05.039&partnerID=40&md5=f3e0452e53bd6d1316b3aa12e32374c9","The meteoric rise of mobile software that we have witnessed in the past decade parallels a paradigm shift in its design, construction, and deployment. In particular, we argue that today's mobile software, with its rich ecosystem of apps, would have not been possible without the pioneering advances in software architecture research in the decade that preceded it. We describe the drivers that elevated software architecture to the centerpiece of contemporary mobile software. We distill the architectural principles found in Android, the predominant mobile platform with the largest market share, and trace those principles to their conception at the turn of century in software architecture literature. Finally, to better understand the extent to which Android's ecosystem of apps employs architectural concepts, we mine the reverse-engineered architecture of hundreds of Android apps in several app markets and report on those results. © 2016 Elsevier Inc.","Android; Architectural styles; Software architecture"
"AspectJ code analysis and verification with GASR","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964683434&doi=10.1016%2fj.jss.2016.04.014&partnerID=40&md5=9cbee64a8b7cf6c41f05ded86ec78abd","Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked. © 2016 Elsevier Inc. All rights reserved.","Aspect oriented programming; Logic program querying; Source code analysis"
"Towards semi-automated assignment of software change requests","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960909432&doi=10.1016%2fj.jss.2016.01.038&partnerID=40&md5=063ff508f9239d01aba3369bb9ef5ec3","Change Requests (CRs) are key elements to software maintenance and evolution. Finding the appropriate developer to a CR is crucial for obtaining the lowest, economically feasible, fixing time. Nevertheless, assigning CRs is a labor-intensive and time consuming task. In this paper, we report on a questionnaire-based survey with practitioners to understand the characteristics of CR assignment, and on a semi-automated approach for CR assignment which combines rule-based and machine learning techniques. In accordance with the results of the survey, the proposed approach emphasizes the use of contextual information, essential to effective assignments, and puts the development team in control of the assignment rules, toward making its adoption easier. The assignment rules can be either extracted from the assignment history or created from scratch. An empirical validation was performed through an offline experiment with CRs from a large software project. The results pointed out that the approach is up to 46.5% more accurate than other approaches which relying solely on machine learning techniques. This indicates that a rule-based approach is a viable and simple method to leverage CR assignments. © 2016 Elsevier Inc. All rights reserved.","Automatic change request assignment; Bug triage; Change request management; Software maintenance and evolution"
"Systematic scalability assessment for feature oriented multi-tenant services","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953333935&doi=10.1016%2fj.jss.2015.12.024&partnerID=40&md5=b8aee3eb9e69cc8542aad5e3003ab181","Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand. In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services. © 2015 Elsevier Inc. All rights reserved.","Distributed systems; Scalability; Tool support"
"METRIC: METamorphic Relation Identification based on the Category-choice framework","2016","Journal of Systems and Software","10.1016/j.jss.2015.07.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940688730&doi=10.1016%2fj.jss.2015.07.037&partnerID=40&md5=dafe8642edc1788c45e54c3333e252a1","Metamorphic testing is a promising technique for testing software systems when the oracle problem exists, and has been successfully applied to various application domains and paradigms. An important and essential task in metamorphic testing is the identification of metamorphic relations, which, due to the absence of a systematic and specification-based methodology, has often been done in an ad hoc manner - something which has hindered the applicability and effectiveness of metamorphic testing. To address this, a systematic methodology for identifying metamorphic relations based on the category-choice framework, called metric, is introduced in this paper. A tool implementing this methodology has been developed and examined in an experiment to determine the viability and effectiveness of metric, with the results of the experiment confirming that metric is both effective and efficient at identifying metamorphic relations. © 2015 Elsevier Inc. All rights reserved.","Metamorphic testing; Oracle problem; Test oracle"
"GPU-SAM: Leveraging multi-GPU split-and-merge execution for system-wide real-time support","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960931724&doi=10.1016%2fj.jss.2016.02.009&partnerID=40&md5=157366e9cb4f6d0751dc5abb137fb2de","Multi-GPUs appear as an attractive platform to speed up data-parallel GPGPU computation. The idea of split-and-merge execution has been introduced to accelerate the parallelism of multiple GPUs even further. However, it has not been explored before how to exploit such an idea for real-time multi-GPU systems properly. This paper presents an open-source real-time multi-GPU scheduling framework, called GPU-SAM, that transparently splits each GPGPU application into smaller computation units and executes them in parallel across multiple GPUs, aiming to satisfy real-time constraints. Multi-GPU split-and-merge execution offers the potential for reducing an overall execution time but at the same time brings various different influences on the schedulability of individual applications. Thereby, we analyze the benefit and cost of split-and-merge execution on multiple GPUs and derive schedulability analysis capturing seemingly conflicting influences. We also propose a GPU parallelism assignment policy that determines the multi-GPU mode of each application from the perspective of system-wide schedulability. Our experiment results show that GPU-SAM is able to improve schedulability in real-time multi-GPU systems by relaxing the restriction of launching a kernel on a single GPU only and choosing better multi-GPU execution modes. © 2016 Elsevier Inc. All rights reserved.","GPGPU; Multi-GPU; Real-time systems"
"Enhanced Boolean-based multi secret image sharing scheme","2016","Journal of Systems and Software","10.1016/j.jss.2015.01.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922722494&doi=10.1016%2fj.jss.2015.01.031&partnerID=40&md5=93f818fb0f5a56111183e61673561ec7","Recently, Chen and Wu propose a (n, n) multi secret image sharing (MSIS) scheme that shares n secret images into n shared images. These n shared images can be together used for reconstructing n secret images by Boolean operations. However, there is an inaccuracy in Chen and Wu's (n, n)-MSIS scheme that one can recover some secret images (partial secret information) from (n-1) or fewer shared images. This compromises the threshold security in (n, n)-MSIS scheme. In this paper, we overcome this inaccuracy and propose a strong threshold (n, n)-MSIS scheme without leaking partial secret information from (n-1) or fewer shared images. Moreover, by an inherent property of image that a permutated image with a chaotic reallocation of pixels without effect on their intensity levels, we propose a modified strong (n, n)-MSIS scheme to further enhance the randomness of shared images. © 2015 Elsevier Inc. All rights reserved.","Boolean operation; Multi secret image secret sharing; Secret image sharing"
"Energy-aware dynamical hosts and tasks assignment for cloud computing","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959419589&doi=10.1016%2fj.jss.2016.01.032&partnerID=40&md5=afb577f33deb3063105aeda548c9bb1b","One feature of MapReduce is to split user request into multiple tasks and then process around multiple datacenters for cloud computing. This study addresses an energy efficiency problem of dynamic cloud hosts (CHs) and task assignments as well as a subset of CH power-on or suspended schedules by controlling the range between the power-on and suspended thresholds for high-energy efficiency. A dynamical CHs and tasks assignment scheme is proposed to reduce the overall system energy consumption. The main concept of the proposed scheme entails setting the thresholds to satisfy the constant and variable traffic loads, nodal load balance, migration overhead, basic required power, and processing power. The reason is the established energy consumption required for initialing power-on and variable rates to keep working. This work evaluates the proposed scheme and compares it with the CHs and tasks assignment schemes to show how the proposed scheme achieves energy efficiency. The simulation results show that the proposed scheme obtains the lowest energy consumption under the tolerable responding time constraints even though the request traffic load is varying. The average improvement rate is 16.3% to balance the number of active hosts and migration overhead as well as 4.8% for task schedule. © 2016 Elsevier Inc. All rights reserved.","Energy efficiency; Load balance; Performance; Scheduling; Threshold"
"ROAR: A QoS-oriented modeling framework for automated cloud resource allocation and optimization","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941255053&doi=10.1016%2fj.jss.2015.08.006&partnerID=40&md5=e797d7d502cbe60f5ad2c641dfe81bc3","Cloud computing offers a fast, easy and cost-effective way to configure and allocate computing resources for web applications, such as consoles for smart grid applications, medical records systems, and security management platforms. Although a diverse collection of cloud resources (e.g., servers) is available, choosing the most optimized and cost-effective set of cloud resources for a given web application and set of quality of service (QoS) goals is not a straightforward task. Optimizing cloud resource allocation is a critical task for offering web applications using a software as a service model in the cloud, where minimizing operational cost while ensuring QoS goals are met is critical to meeting customer demands and maximizing profit. Manual load testing with different sets of cloud resources, followed by comparison of test results to QoS goals is tedious and inaccurate due to the limitations of the load testing tools, challenges characterizing resource utilization, significant manual test orchestration effort, and challenges identifying resource bottlenecks. This paper introduces our work using a modeling framework - ROAR (Resource Optimization, Allocation and Recommendation System) to simplify, optimize, and automate cloud resource allocation decisions to meet QoS goals for web applications, including complex multi-tier application distributed in different server groups. ROAR uses a domain-specific language to describe the configuration of the web application, the APIs to benchmark and the expected QoS requirements (e.g., throughput and latency), and the resource optimization engine uses model-based analysis and code generation to automatically deploy and load test the application in multiple resource configurations in order to derive a cost-optimal resource configuration that meets the QoS goals. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Load testing and benchmarking; Resource optimization"
"Node/Proxy portability: Designing for the two lives of your next WSAN middleware","2016","Journal of Systems and Software","10.1016/j.jss.2016.03.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963819730&doi=10.1016%2fj.jss.2016.03.035&partnerID=40&md5=7330d92f66fa8124079011d71a9900a4","Middleware for wireless sensor and actuator networks (WSANs) provides powerful programming abstractions which simplify application development. While it is highly desirable to reuse WSAN middleware across a wide range of hardware platforms, in practice, complex middleware may not fit in nodes with limited resources. As one possible solution, we propose the so-called proxy approach: the middleware is ported on a general purpose computer, from where the sensors and actuators of the resource-constrained nodes are accessed remotely yet in a way that is transparent to the application, which runs unmodified, as if it resided on the physical nodes. We provide design guidelines and a middleware transformation process for implementing the proxy approach in a structured way. We also present a concrete implementation of the proxy approach for our own middleware, along with a performance evaluation of the proxy software environment in a wired testbed with almost 200 sensor nodes. © 2016 Elsevier Inc. All rights reserved.","Internet of Things; Middleware; Wireless sensor and actuator networks"
"Efficient data dissemination in cooperative multi-RSU Vehicular Ad Hoc Networks (VANETs)","2016","Journal of Systems and Software","10.1016/j.jss.2016.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964294749&doi=10.1016%2fj.jss.2016.04.005&partnerID=40&md5=1e53fedf2299dcd1c30416715507591b","Many safety and non-safety related applications have been envisioned in VANETs. However, efficient data dissemination considering the mobility of vehicle is must for the success of these applications. Although the Road Side Unit (RSU) is a stationary unit, both RSU and vehicle have limited transmission range that restricts to shorter connection time. This endures a higher request drop rate especially at the overloaded RSUs. A cooperative load balancing (CLB) among the RSUs to use their residual bandwidth can be an effective solution to reduce the request drop rate. In this paper, we investigate that considering the remaining delay tolerance of submitted requests and the knowledge of fixed road layout, the performance of the cooperative load balancing system can be further improved significantly. We show that this performance gain comes from serving the requests based on the urgency and the efficient load balancing among the junction-RSUs and edge-RSUs. Based on the observations, we propose an Enhanced CLB (ECLB) approach in this paper. To demonstrate the efficiency of the ECLB approach a number of well-known scheduling algorithms are integrated and an extensive simulation experiments are conducted in the vehicular communication environment that supports the superiority of ECLB over the existing approaches. © 2016 Elsevier Inc. All rights reserved.","Cooperative load balancing; Road Side Units (RSUs); Vehicular Ad Hoc Networks (VANETs)"
"Empirical evaluation of two best practices for energy-efficient software development","2016","Journal of Systems and Software","10.1016/j.jss.2016.02.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961218303&doi=10.1016%2fj.jss.2016.02.035&partnerID=40&md5=2a1ee81bf3f916df5fbf1bcb3e53cf4d","Background. Energy efficiency is an increasingly important property of software. A large number of empirical studies have been conducted on the topic. However, current state-of-the-Art does not provide empirically-validated guidelines for developing energy-efficient software. Aim. This study aims at assessing the impact, in terms of energy savings, of best practices for achieving software energy efficiency, elicited from previous work. By doing so, it identifies which resources are affected by the practices and the possible trade-offs with energy consumption. Method. We performed an empirical experiment in a controlled environment, where we applied two different Green Software practices to two software applications, namely query optimization in MySQL Server and usage of ""sleep"" instruction in the Apache web server. We then performed a comparison of the energy consumption at system-level and at resource-level, before and after applying the practice. Results. Our results show that both practices are effective in improving software energy efficiency, reducing consumption up to 25%. We observe that after applying the practices, resource usage is more energy-proportional i.e., increasing CPU usage increases energy consumption in an almost linear way. We also provide our reflections on empirical experimentation in software energy efficiency. Conclusions. Our contribution shows that significant improvements in software energy efficiency can be gained by applying best practices during design and development. Future work will be devoted to further validate best practices, and to improve their reusability. © 2016 Elsevier Inc. All rights reserved.","Best practices; Energy efficiency; Software engineering"
"Numerical anchors and their strong effects on software development effort estimates","2016","Journal of Systems and Software","10.1016/j.jss.2015.03.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925645345&doi=10.1016%2fj.jss.2015.03.015&partnerID=40&md5=3b7d98328a86197507359a707f06a0c4","The anchoring effect may be described as the tendency for an initial piece of information to influence people's subsequent judgement, even when the information is irrelevant. Previous studies suggest that anchoring is an important source of inaccurate software development effort estimates. This article examines how the preciseness and credibility of anchoring information affects effort estimates. Our hypotheses were that anchors with lower numerical precision and anchor sources with lower credibility would have less impact on effort estimates. The results from three software project effort estimation experiments, with 381 software professionals, support previous findings about the relevance of anchoring effects to software effort estimation. However, we found no decrease in the anchoring effect with decreasing anchor precision or source credibility. This suggests that even implausible anchors from low-credibility sources can lead to anchoring effects, and that all kinds of misleading information potentially acting as estimation anchors in project estimation contexts should be avoided. © 2015 Elsevier Inc. All rights reserved.","Anchoring effects; Numerical preciseness; Software project estimation"
"Evolving models in Model-Driven Engineering: State-of-the-art and future challenges","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949883493&doi=10.1016%2fj.jss.2015.08.047&partnerID=40&md5=5c3a6d4378cdecd08e6b8a3b4cef0b23","The artefacts used in Model-Driven Engineering (MDE) evolve as a matter of course: models are modified and updated as part of the engineering process; metamodels change as a result of domain analysis and standardisation efforts; and the operations applied to models change as engineering requirements change. MDE artefacts are inter-related, and simultaneously constrain each other, making evolution a challenge to manage. We discuss some of the key problems of evolution in MDE, summarise the key state-of-the-art, and look forward to new challenges in research in this area. © 2015 Elsevier Inc. All rights reserved.","Co-evolution; Evolution; Migration"
"Periodic resource integration","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944069598&doi=10.1016%2fj.jss.2015.08.050&partnerID=40&md5=ae1fafb94bee2871150b843d5d8be055","Scheduling periodic real-time tasks on multiple periodic resources is an emerging research issue in the real-time scheduling community and has drawn increased attention over the last few years. This paper studies a sub-category of the scheduling problem which focuses on scheduling a periodic task on multiple periodic resources where none of these resources have sufficient capacity to support the task. Instead of splitting the task into sub-tasks, which is not always practical in real systems, we integrate resources together to jointly support the task. First, we develop a method to integrate two fixed but arbitrary pattern periodic resources into an equivalent periodic resource. Second, for two periodic resources with unknown but fixed resource occurrence patterns, we give the lower and upper bounds of the available time provided by an integrated periodic resource within a period. Third, we present theoretical and empirical analysis on the schedulability of a non-splittable periodic task on two periodic resources and their integrated periodic resource. © 2015 Elsevier Inc. All rights reserved.","Multi-resource scheduling; Periodic resource; Real-time scheduling"
"What to expect of predicates: An empirical analysis of predicates in real world programs","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962343642&doi=10.1016%2fj.jss.2015.12.022&partnerID=40&md5=ca0541d9152aeb001f89e886fcd01d07","One source of complexity in programs is logic expressions, i.e., predicates. Predicates define much of the functional behavior of the software. Many logic-based test criteria have been developed, including the active clause coverage (ACC) criteria and the modified condition/decision coverage (MCDC). The MCDC/ACC criteria is viewed as being expensive, which motivated us to evaluate the cost of applying these criteria using a basic proxy: the number of clauses. We looked at the frequency and percentage of predicates in 63 Java programs. Moreover, we also compared these Java programs with three programs in the safety-critical domain, in which logic-basic testing is often used. Although around 99% of the predicates within Java programs contain at most three clauses, there is a positive linear correlation between overall measures of size and the number of predicates that have more than three clauses. Furthermore, safety-critical C/C++ programs have more complex predicates than non-safety-critical programs. However, similar to the predicates in non-safety-critical programs, most predicates in safety-critical programs have up to three clauses. We conclude that non-safety-critical and safety-critical programs do not have many complex predicates. Thus, MCDC/ACC is only needed on a small fraction of the predicates. © 2015 Elsevier Inc. All rights reserved.","Active clause coverage (ACC) criteria; Logic-based test criteria; Modified condition-decision coverage (MCDC) test criterion"
"Test automation of a measurement system using a domain-specific modelling language","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949818790&doi=10.1016%2fj.jss.2015.09.002&partnerID=40&md5=c9a6992a583ac4768d94be44fe8d8bc8","The construction of domain-specific modelling languages (DSMLs) is only the first step within the needed toolchain. Models need to be maintained, modified or functional errors searched for. Therefore, tool support is vital for the DSML end-user's efficiency. This paper presents SeTT, a simple but very useful tool for DSML end-users, a testing framework integrated within a DSML Sequencer. This Sequencer, part of the DEWESoft data acquisition system, supports the development of model-based tests using a high-level abstraction. The tests are used during the whole data acquisition process and able to test different systems' parts. This paper shows how high-level specifications can be extended to describe a testing infrastructure for a specific DSML. In this manner, the Sequencer and SeTT were combined at the metamodel level. The contribution of the paper is to show that one can leverage on the DSML to build a testing framework with relatively little effort, by implementing assertions to it. © 2015 Elsevier Inc. All rights reserved.","Domain-specific modelling languages; Test automation; Usage experience"
"A programming-level approach for elasticizing parallel scientific applications","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944054377&doi=10.1016%2fj.jss.2015.08.051&partnerID=40&md5=90ce9411199765bf4e16bd4310af9f88","Elasticity is considered one of the fundamental properties of cloud computing. Several mechanisms to provide the feature are offered by public cloud providers and in some academic works. We argue these solutions are inefficient in providing elasticity for scientific applications, since they cannot consider the internal structure and behavior of applications. In this paper we present an approach for exploring the elasticity in scientific applications, in which the elasticity control is embedded in application source code and constructed using elasticity primitives. This approach enables the application itself to request or to release its own resources, taking into account the execution flow and runtime requirements. To support the construction of elastic applications using the presented approach, we developed the Cloudine framework. Cloudine provides all components necessary to construct and execute elastic scientific applications. The Cloudine effectiveness is demonstrated in the experiments where the platform is successfully used to include new features to existing applications, to extend functionalities of other elasticity frameworks and to add elasticity support to parallel programming libraries. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Elasticity; Parallel applications"
"The prospects of a quantitative measurement of agility: A validation study on an agile maturity model","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937458369&doi=10.1016%2fj.jss.2015.05.008&partnerID=40&md5=6870640a1663dc97407fb3a2730a8d2a","Agile development has now become a well-known approach to collaboration in professional work life. Both researchers and practitioners want validated tools to measure agility. This study sets out to validate an agile maturity measurement model with statistical tests and empirical data. First, a pretest was conducted as a case study including a survey and focus group. Second, the main study was conducted with 45 employees from two SAP customers in the US. We used internal consistency (by a Cronbach's alpha) as the main measure for reliability and analyzed construct validity by exploratory principal factor analysis (PFA). The results suggest a new categorization of a subset of items existing in the tool and provides empirical support for these new groups of factors. However, we argue that more work is needed to reach the point where a maturity models with quantitative data can be said to validly measure agility, and even then, such a measurement still needs to include some deeper analysis with cultural and contextual items. © 2015 Elsevier Inc. All rights reserved.","Agility; Empirical study; Validation"
"Scientific software development viewed as knowledge acquisition: Towards understanding the development of risk-averse scientific software","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941280442&doi=10.1016%2fj.jss.2015.07.027&partnerID=40&md5=5ff07b811f4749415fa47ccf9da35d54","This paper presents a model of software development based on knowledge acquisition. The model was formulated from 10 years of studies of scientific software and scientists who develop software as part of their science. The model is used to examine assumptions behind software development models commonly described in software engineering literature, and compare these with the observed way scientists develop software. This paper also explains why a particular type of scientist, one who works in a highly risk-averse application domain, does not conform to the common characterization of all scientists as ""end-user programmers"". We offer observations of how this type of scientist develops trustworthy software. We observe that these scientists work outside the ubiquitous method-based software development paradigms, using instead a knowledge acquisition-based approach to software development. We also observe that the scientist is an integral part of the software system and cannot be excluded from its consideration. We suggest that use of the knowledge acquisition software development model requires research into how to support acquisition of knowledge while developing software, how to satisfy oversight in regulated application domains, and how to successfully manage a scientific group using this model. © 2015 Published by Elsevier Inc. Allrights reserved.","Knowledge model; Scientific software; Software development"
"Automatic detection of system-specific conventions unknown to developers","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941268844&doi=10.1016%2fj.jss.2015.08.007&partnerID=40&md5=1ada0ab398c789efb37cf4d12e6fbcee","In Apache Ant, a convention to improve maintenance was introduced in 2004 stating a new way to close files instead of the Java generic InputStream.close(). Yet, six years after its introduction, this convention was still not generally known to the developers. Two existing solutions could help in these cases. First, one can deprecate entities, but, in our example, one can hardly deprecate Java's method. Second, one can create a system-specific rule to be automatically enforced. In a preceding publication, we showed that system-specific rules are more likely to be noticed by developers than generic ones. However, in practice, developers rarely create specific rules. We therefore propose to free the developers from the need to create rules by automatically detecting such conventions from source code repositories. This is done by mining the change history of the system to discover similar changes being applied over several revisions. The proposed approach is applied to a real-world system, and the extracted rules are validated with the help of experts. The results show that many rules are in fact relevant for the experts. © 2015 ElsevierInc.Allrightsreserved.","Automatic coding convention detection; Mining software repositories; Software evolution"
"Assessing the security of web service frameworks against Denial of Service attacks","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941268327&doi=10.1016%2fj.jss.2015.07.006&partnerID=40&md5=e646954f47dddb1d9e51b1a82e6d7c0f","Web services frequently provide business-critical functionality over the Internet, being widely exposed and thus representing an attractive target for security attacks. In particular, Denial of Service (DoS) attacks may inflict severe damage to web service providers, including financial and reputation losses. This way, it is vital that the software supporting services deployment (i.e.; the web service framework) is able to provide a secure environment, so that the services can be delivered even when facing attacks. In this paper, we present an experimental approach that allows understanding how well a given web service framework is prepared to handle DoS attacks. The approach is based on a set of phases that include the execution of a large number of well-known DoS attacks against a target framework and the classification of the observed behavior. Results show that four out of the six frameworks tested are vulnerable to at least one type of DoS attack, and indicate that even very popular platforms require urgent security improvements. © 2015 Elsevier Inc. Allrights reserved.","Experimental assessment; Security; Web service frameworks"
"Service deployment strategies for efficient execution of composite SaaS applications on cloud platform","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937438596&doi=10.1016%2fj.jss.2015.05.050&partnerID=40&md5=625d94140d3af0e9adb7c2420ba77c01","Cloud computing has caused a revolution in our way of developing and using software. Software development and deployment based on the new models of Software as a Service (SaaS) and Service-Oriented Architecture (SOA) are expected to bring a lot of benefits for users. However, software developers and service providers have to address new challenging issues before such benefits can be realized. This paper explores one of the critical issues, service deployment, for reducing execution time of composite SaaS applications, and proposes an integrated approach to the service deployment problem which takes not only inter-service communication costs but also the potential parallelism among services into consideration. In the approach, two types of graphs are developed to model the communication costs between services, Service Dependency Graph (SDG), and potential parallelism among services, Service Concurrence Graph (SCG), respectively. Then, these two graphs are integrated into a single Service Relationship Graph (SRG) and the service deployment problem is transformed into a minimum k-cut problem for solution. A series of experiments were conducted to evaluate the proposed approach. The experimental results indicate that our approach outperforms previous deployment methods significantly in terms of service response time. © 2015 Elsevier Inc. All rights reserved.","Composite SaaS; Service deployment; Service-oriented architecture"
"Clustering and splitting charging algorithms for large scaled wireless rechargeable sensor networks","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962469355&doi=10.1016%2fj.jss.2015.12.017&partnerID=40&md5=b498014d877c0d52abecde30b383fb6b","As the interdiscipline of wireless communication and control engineering, the periodical charging issue in Wireless Rechargeable Sensor Networks (WRSNs) is a popular research problem. However, existing techniques for periodical charging neglect to focus on the location relationship and topological feature, leading to large charging times and long traveling time. In this paper, we develop a hybrid clustering charging algorithm (HCCA), which firstly constructs a network backbone based on a minimum connected dominating set built from the given network. Next, a hierarchical clustering algorithm which takes advantage of location relationship, is proposed to group nodes into clusters. Afterward, a K-means clustering algorithm is implemented to calculate the energy core set for realizing energy awareness. To further optimize the performance of HCCA, HCCA-TS is proposed to transform the energy charging process into a task splitting model. Tasks generated from HCCA are split into small tasks, which aim at reducing the charging time to enhance the charging efficiency. At last, simulations are carried out to demonstrate the merit of the schemes. Simulation results indicate that HCCA can enhance the performance in terms of reducing charging times, journey time and average charging time simultaneously. Moreover, HCCA-TS can further improve the performance of HCCA. © 2015 Elsevier Inc. All rights reserved.","Charging efficiency; Task splitting; Wireless rechargeable sensor networks"
"Software test process improvement approaches: A systematic literature review and an industrial case study","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949796965&doi=10.1016%2fj.jss.2015.08.048&partnerID=40&md5=2195dee096f5f5faf511f5d578362664","Software test process improvement (STPI) approaches are frameworks that guide software development organizations to improve their software testing process. We have identified existing STPI approaches and their characteristics (such as completeness of development, availability of information and assessment instruments, and domain limitations of the approaches) using a systematic literature review (SLR). Furthermore, two selected approaches (TPI NEXT and TMMi) are evaluated with respect to their content and assessment results in industry. As a result of this study, we have identified 18 STPI approaches and their characteristics. A detailed comparison of the content of TPI NEXT and TMMi is done. We found that many of the STPI approaches do not provide sufficient information or the approaches do not include assessment instruments. This makes it difficult to apply many approaches in industry. Greater similarities were found between TPI NEXT and TMMi and fewer differences. We conclude that numerous STPI approaches are available but not all are generally applicable for industry. One major difference between available approaches is their model representation. Even though the applied approaches generally show strong similarities, differences in the assessment results arise due to their different model representations. © 2015 Elsevier Inc. All rights reserved.","Case study; Software test process improvement; Systematic literature review"
"Ahab's legs in scenario-based requirements validation: An experiment to study communication mistakes","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941285696&doi=10.1016%2fj.jss.2015.07.039&partnerID=40&md5=2d51a8ea49ad6ff105bb28584570b74e","The correct identification of requirements is a crucial step for the implementation of a satisfactory software system. In the validation of requirements with scenarios, a straightforward communication is central to obtain a good participation from stakeholders. Technical specifications are translated into scenarios to make them concrete and easy to understand for non-technical users, and contextual details are added to encourage user engagement. However, additional contextual details (Ahab's legs) could generate a negative impact on the requirements' validation by leading to proliferating comments that are not pertinent to session objective. The objective of this study is to evaluate the impact of Ahab's leg to scenario-based requirement validation sessions. We conducted a controlled experiment with human participants and measured the pertinence of the comments formulated by participants when discussing the requirements. The results of our experiment suggest that the potentially negative impact of Ahab's leg can be effectively controlled by the analyst. © 2015 Elsevier Inc. Allrights reserved.","Human factors of requirement Engineering; Requirement validation"
"A comprehensive modeling framework for role-based access control policies","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937435091&doi=10.1016%2fj.jss.2015.05.015&partnerID=40&md5=b0285e1e58097dd62c3efae186799c62","Prohibiting unauthorized access to critical resources and data has become a major requirement for enterprises; access control (AC) mechanisms manage requests from users to access system resources. One of the most used AC paradigms is role-based access control (RBAC), in which access rights are determined based on the user's role. Many different types of RBAC policies have been proposed in the literature, each one accompanied by the corresponding extension of the original RBAC model. However, there is no unified framework that can be used to define all these types of policies in a coherent way, using a common model. In this paper we propose a model-driven engineering approach, based on UML and the Object Constraint Language (OCL), to enable the precise specification and verification of such policies. More specifically, we first present a taxonomy of the various types of RBAC policies proposed in the literature. We also propose the GemRBAC model, a generalized model for RBAC that includes all the entities required to define the classified policies. This model is a conceptual model that can also serve as data model to operationalize data collection and verification. Lastly, we formalize the classified policies as OCL constraints on the GemRBAC model. © 2015 Elsevier Inc. All rights reserved.","Modeling; Role-based access control; Survey"
"Mobile device power models for energy efficient dynamic offloading at runtime","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962393751&doi=10.1016%2fj.jss.2015.11.042&partnerID=40&md5=0e7ce635d6502325713bf4db56087518","Spectacular advances in hardware and software technologies have resulted in powerful mobile devices, equipped with advanced processing, storage and network capabilities. Therefore, using resource-intensive applications has become a commodity in many contexts. However, the rapid evolution in hardware and software capabilities has not been paralleled by a similar advance in battery technology. A potential avenue to cope with the device energy resource limitation is to offload computational tasks to cloud infrastructure in the network. In order to offload tasks in an energy-aware manner, we present a detailed model of mobile device energy consumption, addressing the main power consuming subsystems, including CPU, display unit, wireless network interface and memory. Applying this model allows to estimate the power consumed by the application when executed locally, remotely or hybridly (i.e. partly on the device and partly in the cloud infrastructure). Offloading parts of the application can subsequently be decided at runtime based on these energy consumption estimates, also taking into account the power consumed by the device-to-cloud communication over the wireless network. The dynamic offloading has been validated with computational and communication intensive applications. Results show that 18-55% energy gains on the mobile device can be achieved, depending on different conditions. © 2015 Elsevier Inc. All rights reserved.","Energy consumption; Energy-aware dynamic offloading; Power model"
"Exploring context-sensitive data flow analysis for early vulnerability detection","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476725&doi=10.1016%2fj.jss.2015.12.021&partnerID=40&md5=61339971fde19a95f6bf60a5e295ff6b","Secure programming is the practice of writing programs that are resistant to attacks by malicious people or programs. Programmers of secure software have to be continuously aware of security vulnerabilities when writing their program statements. In order to improve programmers' awareness, static analysis techniques have been devised to find vulnerabilities in the source code. However, most of these techniques are built to encourage vulnerability detection a posteriori, only when developers have already fully produced (and compiled) one or more modules of a program. Therefore, this approach, also known as late detection, does not support secure programming but rather encourages posterior security analysis. The lateness of vulnerability detection is also influenced by the high rate of false positives yielded by pattern matching, the underlying mechanism used by existing static analysis techniques. The goal of this paper is twofold. First, we propose to perform continuous detection of security vulnerabilities while the developer is editing each program statement, also known as early detection. Early detection can leverage his knowledge on the context of the code being created, contrary to late detection when developers struggle to recall and fix the intricacies of the vulnerable code they produced from hours to weeks ago. Second, we explore context-sensitive data flow analysis (DFA) for improving vulnerability detection and mitigate the limitations of pattern matching. DFA might be suitable for finding if an object has a vulnerable path. To this end, we have implemented a proof-of-concept Eclipse plugin for continuous DFA-based detection of vulnerabilities in Java programs. We also performed two empirical studies based on several industry-strength systems to evaluate if the code security can be improved through DFA and early vulnerability detection. Our studies confirmed that: (i) the use of context-sensitive DFA significantly reduces the rate of false positives when compared to existing techniques, without being detrimental to the detector performance, and (ii) early detection improves the awareness among developers and encourages programmers to fix security vulnerabilities promptly. © 2015 Elsevier Inc. All rights reserved.","Data flow analysis; Early detection; Secure programming"
"The influence of developer multi-homing on competition between software ecosystems","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949783538&doi=10.1016%2fj.jss.2015.08.053&partnerID=40&md5=e575bab80c930adcedeadaa6404ef074","Having a large number of applications in the marketplace is considered a critical success factor for software ecosystems. The number of applications has been claimed to determine which ecosystems holds the greatest competitive advantage and will eventually dominate the market. This paper investigates the influence of developer multi-homing (i.e., participating in more than one ecosystem) in three leading mobile application ecosystems. Our results show that when regarded as a whole, mobile application ecosystems are single-homing markets. The results further show that 3% of all developers generate more than 80% of installed applications and that multi-homing is common among these developers. Finally, we demonstrate that the most installed content actually comprises only a small number of the potential value propositions. The results thus imply that attracting and maintaining developers of superstar applications is more critical for the survival of a mobile application ecosystem than the overall number of developers and applications. Hence, the mobile ecosystem is unlikely to become a monopoly. Since exclusive contracts between application developers and mobile application ecosystems are rare, multi-homing is a viable component of risk management and a publishing strategy. The study advances the theoretical understanding of the influence of multi-homing on competition in software ecosystems. © 2015 The Authors. Published by Elsevier Inc.","Multi-homing; Software ecosystem; Two-sided markets"
"Towards a document-driven approach for designing reference models: From a conceptual process model to its application","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949761049&doi=10.1016%2fj.jss.2015.09.029&partnerID=40&md5=975eeb4eb474833a1a2d9624f53558e4","In IS research, reference models have demonstrated to be a beneficial instrument for providing blueprints for a reasonable, good design of information systems and underlying organizational settings. Researchers assume that the application of reference models allows time savings, cost savings, and quality increases. But these effects may only appear when providing a research-based and empirically evaluated reference model that is profoundly documented. However, research criticizes the often missing identification of similarities in related work and preexisting knowledge, which might lead to arbitrariness. Moreover, linking existing knowledge during development and evaluation processes of reference models can bring new and fruitful insights. Therefore, this paper uses a scientific approach consisting of four steps. First, we develop a requirements framework for designing reference models. Second, we use this framework as a basis for the comparison of well-documented reference models. Thereafter, the gained insights from step one and two are consolidated into a conceptual process model that has a strong regard to preexisting knowledge. Finally, a case study will show the applicability of the determined model. With this paper, we enrich research by a valuable guideline for developing methodologically well-designed reference models that support users to take full advantage of the above mentioned benefits. © 2015 Elsevier Inc. All rights reserved.","Case study research; Conceptual process model; Document analysis"
"Twenty-eight years of component-based software engineering","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949744952&doi=10.1016%2fj.jss.2015.09.019&partnerID=40&md5=6b40cdb03c135e26f3016750dedf404f","The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research. © 2015 Elsevier Inc. All rights reserved.","Component-based software development; Component-based software engineering; Software component; Systematic mapping study"
"Dynamically constructing and maintaining virtual access points in a macro cell with selfish nodes","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937782124&doi=10.1016%2fj.jss.2015.06.002&partnerID=40&md5=20b5ab9b5a7dc23a8be28818e7142f0f","With the appearance of wireless applications generating a huge volume of traffic, the low-power access points (e.g.; pico base stations) are introduced to cellular systems to improve system capacity. However, network operators must make a huge investment in the installation and maintenance. The added relays require flexible site acquisition, which is important yet intractable for network service providers. Therefore, we propose an economical and effective alternative scheme for some outdoor scenarios (e.g.; outdoor festivals). The basic idea is that, inspired by a class of cell-based clustering approaches, we partition a macro cell into a number of small cells, such that a connected backbone network can be constructed and maintained by electing a wireless client device from each small cell to build its inter-subcell links (i.e.; the links between the neighboring small cells). Also, a new game-based incentive mechanism is proposed and integrated into our cell-based approach to stimulate selfish devices to participate in the construction and maintenance of network backbone. The correctness of this incentive mechanism has been proved. Furthermore, we have also proved that the topology derived from our scheme has the property of robustness. The simulation results indicate that our scheme achieves less control messages and longer network lifetime, compared with the existing cell-based clustering schemes. © 2015 Elsevier Inc. All rights reserved.","Robustness; Stimulation; Topology control"
"Automatically classifying software changes via discriminative topic model: Supporting multi-category and cross-project","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962376866&doi=10.1016%2fj.jss.2015.12.019&partnerID=40&md5=b415c725a76c82bb7a1e4d81e4d89808","Accurate classification of software changes as corrective, adaptive and perfective can enhance software decision making activities. However, a major challenge which remains is how to automatically classify multi-category changes. This paper presents a discriminative Probability Latent Semantic Analysis (DPLSA) model with a novel initialization method which initializes the word distributions for different topics using labeled samples. This method creates a one-to-one correspondence between the discovered topics and the change categories. As a result, the discriminative semantic representation of the software change messages whose largest topic entry directly corresponds to the category label of the change message which is directly used to perform single-category and multi-category change classification. In the evaluation on five open source projects, the experimental results show that the proposed approach achieves a more accurate performance than the four baseline methods. Especially with the multi-category classification task which improves the recall rate. Moreover, the different projects share the same vocabulary and the estimated model so that DPLSA is well applicable to cross-project software change message analysis. © 2015 Elsevier Inc. All rights reserved.","Discriminative topic model; Multi-category change; Software change classification"
"Towards a hybrid relational and XML benchmark for loosely-coupled distributed data sources","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941269110&doi=10.1016%2fj.jss.2015.07.029&partnerID=40&md5=5c0b8bc4cacd63d4631bf3cb7577db71","There are known benchmarks for the performance evaluation of relational and XML databases. However, there is an increasing demand for database applications that require access to heterogeneous loosely-coupled distributed data sources. This paper presents a hybrid benchmark based on TPC-H where the data sources are heterogeneous. Specifically, the paper describes the design of the relational and XML data sources as well as the query redesign in the LINQ query language, which supports queries over heterogeneous data sources. The results of a performance evaluation of the hybrid benchmark over various database products is included for untyped and typed XML with and without clearing the database cache. © 2015 Elsevier Inc. Allrights reserved.","Benchmark; Heterogeneous; LINQ"
"Level based batch scheduling strategy with idle slot reduction under DAG constraints for computational grid","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937775817&doi=10.1016%2fj.jss.2015.06.016&partnerID=40&md5=41f300d612c293e05ac468b1cdd6aaf1","Scheduling in a grid environment optimizing the given objective parameters has been proven to be NP-complete. This work proposes a Level based Batch scheduling Strategy with Idle slot Reduction (LBSIR) while considering the inter module communication within the modules of the jobs represented using Direct Acyclic Graph (DAG) with the objective of optimizing the turnaround time and response time for a computational grid. The model works in two phases, allocation phase and idle slot reduction phase. Allocation phase begins by dividing the batch into a number of partitions as per the precedence level/depth level followed by the assignment of sub-jobs/modules from the partition to the best fit node in terms of the execution time offered for all the partitions. The idle slots generated during the allocation phase in each depth level are then reduced by inserting the best fit modules into these slots in the idle slot reduction phase after allocation of modules from higher depth level. Levelized allocation ensures minimizing the average response time being very useful for user interactive applications. An experimental study of the proposed strategy has been performed by comparing it with other similar methods having common objectives for evaluating its place in the middleware. © 2015 ElsevierInc.Allrightsreserved.","Computational grid; DAG constraints; Level based batch scheduler"
"Assessing requirements engineering and software test alignment - Five case studies","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941309378&doi=10.1016%2fj.jss.2015.07.018&partnerID=40&md5=242408fc86ec075be7edb1c7ed5d25fb","The development of large, software-intensive systems is a complex undertaking that we generally tackle by a divide and conquer strategy. Companies thereby face the challenge of coordinating individual aspects of software development, in particular between requirements engineering (RE) and software testing (ST). A lack of REST alignment can not only lead to wasted effort but also to defective software. However, before a company can improve the mechanisms of coordination they need to be understood first. With REST-bench we aim at providing an assessment tool that illustrates the coordination in software development projects and identify concrete improvement opportunities. We have developed REST-bench on the sound fundamentals of a taxonomy on REST alignment methods and validated the method in five case studies. Following the principles of technical action research, we collaborated with five companies, applying REST-bench and iteratively improving the method based on the lessons we learned. We applied REST-bench both in Agile and plan-driven environments, in projects lasting from weeks to years, and staffed as large as 1000 employees. The improvement opportunities we identified and the feedback we received indicate that the assessment was effective and efficient. Furthermore, participants confirmed that their understanding on the coordination between RE and ST improved. © 2015 Elsevier Inc. Allrights reserve.","Coordination; Requirements engineering; Software testing"
"preferTrust: An ordered preferences-based trust model in peer-to-peer networks","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962418108&doi=10.1016%2fj.jss.2015.12.018&partnerID=40&md5=753fdc9a0c5a8a1c03d93d6c78c6536a","Aiming at solving the two problems in existing trust models, rough trust description and simplistic trust decision, an ordered preferences-based trust model, preferTrust, is proposed for peer-to-peer networks. First, we define a service as an n-dimensional attribute vector in order to reflect the service provider's abilities in different attributes in detail. Second, considering the fact that different peers may have different preferences for each attribute of a service, which are usually represented in the qualitative expressions, such as better, worse and so on, we transform those peer's qualitative preferences to the quantitative ordered preferences with the method of analytical hierarchy process (AHP), and based on which to calculate the requester's expectation vector. Finally, we combine the requester's expectation vector with each responder's trust vector to calculate an estimate value for each responder which will help the requester make the final trust decision. The simulation results show that preferTrust is able to both improve the trust decision satisfaction and resist malicious attacks effectively and efficiently. © 2015 Elsevier Inc. All rights reserved.","Expectation vector; P2P network; Trust vector"
"A systematic mapping study on the combination of software architecture and agile development","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949803159&doi=10.1016%2fj.jss.2015.09.028&partnerID=40&md5=677e0674ac6359f003a3a7fd1827f816","Context Combining software architecture and agile development has received significant attention in recent years. However, there exists no comprehensive overview of the state of research on the architecture-agility combination. Objective This work aims to analyze the combination of architecture and agile methods for the purpose of exploration and analysis with respect to architecting activities and approaches, agile methods and practices, costs, benefits, challenges, factors, tools, and lessons learned concerning the combination. Method A systematic mapping study (SMS) was conducted, covering the literature on the architecture-agility combination published between February 2001 and January 2014. Results Fifty-four studies were finally included in this SMS. Some of the highlights: (1) a significant difference exists in the proportion of various architecting activities, agile methods, and agile practices employed in the combination. (2) none of the architecting approaches has been widely used in the combination. (3) there is a lack of description and analysis regarding the costs and failure stories of the combination. (4) twenty challenges, twenty-nine factors, and twenty-five lessons learned were identified. Conclusions The results of this SMS help the software engineering community to reflect on the past thirteen years of research and practice on the architecture-agility combination with a number of implications. © 2015 Elsevier Inc. All rights reserved.","Agile development; Architecting approach; Software architecture"
"An empirically-developed framework for Agile transition and adoption: A Grounded Theory approach","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937427552&doi=10.1016%2fj.jss.2015.06.006&partnerID=40&md5=0191ff9524377275c00e273190814cd7","To date, few Agile transition and adoption frameworks have been proposed in the software industry. However, using them is not easy in practice and primarily requires a huge organizational overhead because of their complex and non-flexible structure. These drawbacks make such frameworks difficult to apply in small and medium-sized companies. We have conducted a large-scale empirical research study using Grounded Theory approach with the participation of 49 Agile experts from 13 different countries. This study inductively developed a substantive Agile transition and adoption framework which appears to be simple and flexible. The main aim of this paper is to present the developed framework. The primary characteristics of this framework, including iterative, gradual, continuous, and value-based are in line with the Agile approach and show promise of being useful in software companies and organizations, regardless of size. This paper also describes how various steps of this framework could help software companies to achieve Agile transformation. © 2015 Elsevier Inc. All rights reserved.","Agile adoption; Agile software development; Agile transformation process; Grounded Theory; Transition framework"
"Towards better Scrum learning using learning styles","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949761274&doi=10.1016%2fj.jss.2015.10.022&partnerID=40&md5=16c41271fd67e12d3cd5f128deeadc08","Considerable attention has been paid to teaching Scrum in software engineering education as an academic response to the software industry's demands. In order to reinforce and strengthen the understanding of Scrum concepts, professors should personalize the learning process, catering for students' individual learning characteristics. To address this issue, learning styles become effective to understand students' different ways of learning. In this context, the meshing hypothesis claims that when both teaching and learning styles are aligned, the students' learning experience is enhanced. However, the literature fails to evidence support for the meshing hypothesis in the context of software engineering education. We aim to corroborate the meshing hypothesis by using teaching strategies matching the Felder-Silverman Learning Style Model in a Scrum course. Based on previous findings, we focused on the processing dimension of the model. To validate our approach, two experiments were conducted in an undergraduate software engineering course in the academic years 2013 and 2014. We provided students with a Scrum class by applying teaching strategies suiting students' learning style. Test results corroborate that students' outcomes improved when receiving the strategy that match their learning styles. Our data highlight opportunities for improving software engineering education by considering the students' learning preferences. © 2015 Elsevier Inc. All rights reserved.","Agile software development; Learning styles; Software engineering education"
"Cost-effective strategies for the regression testing of database applications: Case study and lessons learned","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962339872&doi=10.1016%2fj.jss.2015.12.003&partnerID=40&md5=c2ddabca298afa842823f5a91c70b593","Testing and, more specifically, the regression testing of database applications is highly challenging and costly. One can rely on production data or generate synthetic data, for example based on combinatorial techniques or operational profiles. Both approaches have drawbacks and advantages. Automating testing with production data is impractical and combinatorial test suites might not be representative of system operations. In this paper, based on a large scale case study in a representative development environment, we explore the cost and effectiveness of various approaches and their combination for the regression testing of database applications, based on production data and synthetic data generated through classification tree models of the input domain. The results confirm that combinatorial test suite specifications bear little relation to test suite specifications derived from the system operational profile. Nevertheless, combinatorial testing strategies are effective, both in terms of the number of regression faults discovered but also, more surprisingly, in terms of the importance of these faults. However, our study also shows that relying solely on synthesized test data derived from test models could lead to important faults slipping to production. Thus, we recommend that testing on production data and combinatorial testing be combined to achieve optimal results. © 2015 Elsevier Inc. All rights reserved.","Classification tree modeling; Database applications; Regression testing"
"Behavioral software engineering: A definition and systematic literature review","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937427797&doi=10.1016%2fj.jss.2015.04.084&partnerID=40&md5=6f1aece0cd52901849c6205caf2aad16","Throughout the history of software engineering, the human aspects have repeatedly been recognized as important. Even though research that investigates them has been growing in the past decade, these aspects should be more generally considered. The main objective of this study is to clarify the research area concerned with human aspects of software engineering and to create a common platform for future research. In order to meet the objective, we propose a definition of the research area behavioral software engineering (BSE) and present results from a systematic literature review based on the definition. The result indicates that there are knowledge gaps in the research area of behavioral software engineering and that earlier research has been focused on a few concepts, which have been applied to a limited number of software engineering areas. The individual studies have typically had a narrow perspective focusing on few concepts from a single unit of analysis. Further, the research has rarely been conducted in collaboration by researchers from both software engineering and social science. Altogether, this review can help put a broader set of human aspects higher on the agenda for future software engineering research and practice. © 2015 Elsevier Inc. All rights reserved.","Human aspects; Psychology; Software engineering"
"Links between the personalities, styles and performance in computer programming","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949908499&doi=10.1016%2fj.jss.2015.09.011&partnerID=40&md5=a27be6e49becf15818e4b8bb9dd429e2","There are repetitive patterns in strategies of manipulating source code. For example, modifying source code before acquiring knowledge of how a code works is a depth-first style and reading and understanding before modifying source code is a breadth-first style. To the extent we know there is no study on the influence of personality on them. The objective of this study is to understand the influence of personality on programming styles. We did a correlational study with 65 programmers at the University of Stuttgart. Academic achievement, programming experience, attitude towards programming and five personality factors were measured via self-assessed survey. The programming styles were asked in the survey or mined from the software repositories. Performance in programming was composed of bug-proneness of programmers which was mined from software repositories, the grades they got in a software project course and their estimate of their own programming ability. We did statistical analysis and found that Openness to Experience has a positive association with breadth-first style and Conscientiousness has a positive association with depth-first style. We also found that in addition to having more programming experience and better academic achievement, the styles of working depth-first and saving coarse-grained revisions improve performance in programming. © 2015 Elsevier Inc. All rights reserved.","Five-factor model; Personality; Programming styles"
"Discovering and creating business opportunities for cloud services","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962163497&doi=10.1016%2fj.jss.2015.11.004&partnerID=40&md5=ffc9cb724fe77f806d3a32cbcaa3e70f","Cloud computing provides new business opportunities for firms selling or using cloud services. However, little is known about how software firms detect and exploit these opportunities. Based on in-depth qualitative case studies, this study identified two different pathways followed by software firms when they detect and exploit opportunities. In the first pathway, the opportunity is based on an existing problem and need in the market. In the case firms, the opportunity was exploited by adapting the software to the cloud environment. In the second pathway, the opportunity arises from the founders' prior knowledge and imagination, in the absence of any existing problem or need in the market. In this case, the opportunity was exploited through the features offered by cloud computing. This research contributes to Information Systems (IS) literature by incorporating relevant entrepreneurship theories in such a way as to enrich and extend IS research. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Opportunity creation; Opportunity discovery"
"Aligning codependent Scrum teams to enable fast business value delivery: A governance framework and set of intervention actions","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949668902&doi=10.1016%2fj.jss.2015.11.010&partnerID=40&md5=57217b6008380e7bf093c8c5574976ca","Many enterprises that adopt Agile/Scrum suffer from collaboration issues between Scrum teams that depend on one another to deliver end-to-end functionality. These dependencies delay delivery and as a result deteriorate the business value delivered in such value chains. The objective of our study is to support enterprises that suffer from such dependencies with a governance framework that helps them mitigate collaboration issues between sets of codependent Scrum teams. We first identify a set of intervention actions that aim to mitigate the collaboration issues between codependent Scrum teams. Second, we validate the effectiveness of these intervention actions in a large confirmatory industrial case study. This study was held in a large multi-national financial institute that worked with a large number of codependent Scrum teams. Third, we triangulate the findings in three focus groups. We finally package the intervention actions in a governance framework. The intervention actions led to a delivery time reduction from 29 days to 10 days. The participants in the focus groups confirmed the causality between the intervention actions and the observed delivery improvement. The empirical results show that the intervention actions, packaged in the governance framework, enable codependent sets of Scrum teams to deliver faster. © 2015 Elsevier Inc. All rights reserved.","Agile; Alignment; Chain codependencies; Collaboration; Coordination"
"Cross-factor analysis of software engineering practices versus practitioner demographics: An exploratory study in Turkey","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949883478&doi=10.1016%2fj.jss.2015.09.013&partnerID=40&md5=a4e24dc48df980622bc9d742bf53dfd7","Context Understanding the types of software engineering practices and techniques used in the industry is important. There is a wide spectrum in terms of the types and maturity of software engineering practices conducted in each software team and company. Furthermore, it is important to understand the cross-factor relationship of software engineering practices and practitioner demographics including their companies and projects, e.g., is it the case that practitioners developing software for sectors such as military would utilize software size measurement approaches more, or use different software development methodologies, compared to practitioners developing software for other sectors?, and what kinds of practices are conducted by practitioners in small versus large companies? Objective Our objective is to get an understanding into the cross-factor correlation of various software engineering practices versus practitioner demographics including their companies and projects (e.g., target industry, size and work experience). Such an understanding will enable us to identify patterns and pinpoint special issues which should be studied and addressed in the context of each specific demographic (e.g., small versus large companies). Thus, we decided to conduct an exploratory study in this area and collected real industrial data in the context of Turkey which has a vibrant software industry. Method To achieve the above objective, we use the data from a recent Turkish-wide survey of software engineering practices which was systematically designed with 46 questions based on our past experience in the Canadian context and using the Software Engineering Body of Knowledge (SWEBOK). 202 practicing software engineers participated in the survey. We raise a set of 12 research questions about the cross-factor correlation of software engineering practices and practitioner demographics, and address them using statistical analysis. Results The exploratory study results reveal important and interesting findings about cross-factor relationship of software engineering practices and practitioner demographics. Among some of the most interesting findings are the followings: (1) By analyzing the trends, we were first surprised to see that as a practitioner gets more years of work experience, against what one would expect, s/he experiences more challenges in Software Development Life-Cycle (SDLC) phases and SE tasks; (2) Almost 55% of participants measure software size; (3) Agile/lean development is used the least (16%) by the participants working in the companies serving the military and defense sector; (4) Usage of waterfall is low among participants employed by small-sized companies whereas Agile/lean development is relatively popular among this class of participants; and (5) As company size increases, usage of spiral development slightly increases, whereas usage of extreme programming practices decreases. Conclusion The results of this exploratory study will be useful to software engineering professionals and researchers both in Turkey and world-wide by revealing the cross-factor relationship of software engineering practices versus practitioner demographics. The study raises several new research directions, e.g., (1) Why are not many practitioners using any size nor project estimation metrics and how these fundamental engineering approaches could be utilized more frequently?, (2) What are the best practices, success stories and challenging experiences in using SE tools?, and (3) Why is Agile/lean development is not popular in the military and defense sector and how these approaches could be utilized more frequently. © 2015 Elsevier Inc. All rights reserved.","Cross-factor analysis; Practitioner demographics; Software engineering practices"
"Automated software design using ant colony optimization with semantic network support","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941308438&doi=10.1016%2fj.jss.2015.06.067&partnerID=40&md5=7153d55240b628fda9c76d1cd6d68f54","Software design is an important task that needs to be well performed. In this paper, a method for automated software design using search-based software engineering approach is proposed. This approach can solve software engineering problems using search algorithms. Ant colony optimization is used as the meta-heuristic search algorithm in both single-objective and multi-objective modes. Input data are the analysis phase artifacts and the output is in the form of early life cycle class diagram. To provide human designer's background knowledge, a semantic network is used that is built upon the textual documents of analysis phase plus other resources like WordNet. This semantic network is used to name the output classes, and also to determine structural relations between classes. The proposed method is evaluated by some case studies and results are reported. The evaluation results show that using background knowledge beside optimization algorithm helps to achieve better results. ©2015 Elsevier Inc. Allrights reserved.","Ant colony optimization; Automated software design; Search-based software engineering"
"A comparison analysis of environmental factors affecting software reliability","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941274957&doi=10.1016%2fj.jss.2015.04.083&partnerID=40&md5=16e09f553c71ec570e7f97e86cfb9346","Fifteen years ago, Zhang and Pham launched a survey to investigate the impact of software development environmental factors (EFs) on software reliability assessment. Software development has gone through substantial changes during the past fifteen years. How different the environmental factors have become? This paper aims to revisit the 32 environmental factors and analyze their impact on software development and reliability based on a current survey to software development practitioners. The participants of this study come from 20 various organizations and they hold different positions and work on different application areas. Statistical analysis method, such as principle component analysis, relative weighted method, Tukey method, backward elimination, and correlation analysis are applied to analyze these factors. We compare the findings in the two studies and list the most significant factors based on the general ranking and the principle components. The environmental factors in each development phase are also studied. Recommendations such as time allocation during software development process are also made. © 2015 Elsevier Inc. Allrightsreserved.","Environmental factors; Principle component analysis; Software reliability"
"Thread-level priority assignment in global multiprocessor scheduling for DAG tasks","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962476385&doi=10.1016%2fj.jss.2015.12.004&partnerID=40&md5=9967c4833319a9c8bb7c9e48d93ef174","The advent of multi- and many-core processors offers enormous performance potential for parallel tasks that exhibit sufficient intra-task thread-level parallelism. With a growth of novel parallel programming models (e.g., OpenMP, MapReduce), scheduling parallel tasks in the real-time context has received an increasing attention in the recent past. While most studies focused on schedulability analysis under some well-known scheduling algorithms designed for sequential tasks, little work has been introduced to design new scheduling policies that accommodate the features of parallel tasks, such as their multi-threaded structure. Motivated by this, we refine real-time scheduling algorithm categories according to the basic unit of scheduling and propose a new priority assignment method for global task-wide thread-level fixed-priority scheduling of parallel task systems. Our evaluation results show that a finer-grained, thread-level fixed-priority assignment, when properly assigned, significantly improves schedulability, compared to a coarser-grained, task-level assignment. © 2015 Elsevier Inc. All rights reserved.","Intra-parallel task scheduling; Optimal thread-level priority assignment; Real-time systems"
"Scalable and efficient configuration of time-division multiplexed resources","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962160018&doi=10.1016%2fj.jss.2015.11.019&partnerID=40&md5=eeca321327076d37dbaeb06c16ab3336","Consumer-electronics systems are becoming increasingly complex as the number of integrated applications is growing. Some of these applications have real-time requirements, while other non-real-time applications only require good average performance. For cost-efficient design, contemporary platforms feature an increasing number of cores that share resources, such as memories and interconnects. However, resource sharing causes contention that must be resolved by a resource arbiter, such as Time-Division Multiplexing. A key challenge is to configure this arbiter to satisfy the bandwidth and latency requirements of the real-time applications, while maximizing the slack capacity to improve performance of their non-real-time counterparts. As this configuration problem is NP-hard, a sophisticated automated configuration method is required to avoid negatively impacting design time. The main contributions of this article are: (1) an optimal approach that takes an existing integer linear programming (ILP) model addressing the problem and wraps it in a branch-and-price framework to improve scalability. (2) A faster heuristic algorithm that typically provides near-optimal solutions. (3) An experimental evaluation that quantitatively compares the branch-and-price approach to the previously formulated ILP model and the proposed heuristic. (4) A case study of an HD video and graphics processing system that demonstrates the practical applicability of the approach. © 2015 Elsevier Inc. All rights reserved.","Branch-and-price; Real-time systems; Resource scheduling"
"Exploring community structure of software Call Graph and its applications in class cohesion measurement","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937772675&doi=10.1016%2fj.jss.2015.06.015&partnerID=40&md5=63495da7c68b995398f045c6cbf2be59","Many complex networked systems exhibit natural divisions of network nodes. Each division, or community, is a densely connected subgroup. Such community structure not only helps comprehension but also finds wide applications in complex systems. Software networks, e.g.; Class Dependency Networks, are such networks with community structures, but their characteristics at the function or method call granularity have not been investigated, which are useful for evaluating and improving software intra-class structure. Moreover, existing proposed applications of software community structure have not been directly compared or combined with existing software engineering practices. Comparison with baseline practices is needed to convince practitioners to adopt the proposed approaches. In this paper, we show that networks formed by software methods and their calls exhibit relatively significant community structures. Based on our findings we propose two new class cohesion metrics to measure the cohesiveness of object-oriented programs. Our experiment on 10 large open-source Java programs validate the existence of community structures and the derived metrics give additional and useful measurement of class cohesion. As an application we show that the new metrics are able to predict software faults more effectively than existing metrics. © 2015 Elsevier Inc. All rights reserved.","Class cohesion metrics; Community structure; Complex network"
"An experimental investigation on the innate relationship between quality and refactoring","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937433135&doi=10.1016%2fj.jss.2015.05.024&partnerID=40&md5=1668cf551264926295df1e22c846c9d4","Previous studies have investigated the reasons behind refactoring operations performed by developers, and proposed methods and tools to recommend refactorings based on quality metric profiles, or on the presence of poor design and implementation choices, i.e., code smells. Nevertheless, the existing literature lacks observations about the relations between metrics/code smells and refactoring activities performed by developers. In other words, the characteristics of code components increasing/decreasing their chances of being object of refactoring operations are still unknown. This paper aims at bridging this gap. Specifically, we mined the evolution history of three Java open source projects to investigate whether refactoring activities occur on code components for which certain indicators - such as quality metrics or the presence of smells as detected by tools - suggest there might be need for refactoring operations. Results indicate that, more often than not, quality metrics do not show a clear relationship with refactoring. In other words, refactoring operations are generally focused on code components for which quality metrics do not suggest there might be need for refactoring operations. Finally, 42% of refactoring operations are performed on code entities affected by code smells. However, only 7% of the performed operations actually remove the code smells from the affected class. © 2015 Elsevier Inc. All rights reserved.","Code smells; Empirical study; Refactoring"
"The usage of ISBSG data fields in software effort estimation: A systematic mapping study","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962343712&doi=10.1016%2fj.jss.2015.11.040&partnerID=40&md5=2e17a7b891f2c30d95c2fe597c7a526d","The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects. A common use of the ISBSG dataset is to investigate models to estimate a software project's size, effort, duration, and cost. The aim of this paper is to determine which and to what extent variables in the ISBSG dataset have been used in software engineering to build effort estimation models. For that purpose a systematic mapping study was applied to 107 research papers, obtained after a filtering process, that were published from 2000 until the end of 2013, and which listed the independent variables used in the effort estimation models. The usage of ISBSG variables for filtering, as dependent variables, and as independent variables is described. The 20 variables (out of 71) mostly used as independent variables for effort estimation are identified and analysed in detail, with reference to the papers and types of estimation methods that used them. We propose guidelines that can help researchers make informed decisions about which ISBSG variables to select for their effort estimation models. © 2015 Elsevier Inc. All rights reserved.","ISBSG data field; Software effort estimation; Systematic mapping study"
"Models and evolution: An introduction to the special issue","2016","Journal of Systems and Software","10.1016/j.jss.2015.05.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937861472&doi=10.1016%2fj.jss.2015.05.037&partnerID=40&md5=4c5edf796e4b4cdcd12a453da41979f8","As any other software artifact models are prone to evolution. Changes can be motivated by numerous factors, including technical changes due to technological shifts, and new insights and requirements emerging from the application domain. Changes typically occur at any level in the abstraction hierarchy, from requirements through design models, to source code, documentation and test suites. This special issue describes several approach to leverage the management of changes by means of suitable models, techniques, and tools for coping with changes that accompany the natural evolution of software. © 2015 Elsevier Inc. All rights reserved.","Evolution; Model-Driven Engineering; Models"
"Health and emergency-care platform for the elderly and disabled people in the Smart City","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944049396&doi=10.1016%2fj.jss.2015.08.041&partnerID=40&md5=2995f3fb79a1d39d996c613cbc0b5342","Emergence of context-aware technologies and IoT devices reflect that the quality of a human life has become one of the most essential aspects in Smart Cities. With this goal health monitoring of elderly and disabled people have got plenty of attention and focus in the research. The healthcare systems rely on the components responsible for context sensing, processing, storage and inference, and response. In order to make the interoperability among the various healthcare systems, a typical standard is needed in order to uniformly access the context-aware healthcare information coming through a fundamental infrastructure. In this paper, we propose people-centric sensing framework for the healthcare of elderly and disabled people. Such platform is aimed to monitor health of the elderly and disabled person and provide them with a service oriented emergency response in case of abnormal health condition. We focus on three aspects: (a) context manipulation from the mobile device in people-centric environment; (b) emergency response using context base information; and (c) modeling mobile context sources as services. The most distinctive feature of current work is that medical resources are efficiently used to provide them real-time medical services in case of emergency simultaneously extending social network of the elderly people. The system implementation shows that the proposed people-centric sensing system is efficient and cost-effective in health and emergency care. © 2015 Elsevier Inc. All rights reserved.","Healthcare; Internet of Things; People-centric sensing; Smart city; Wireless body sensor network"
"A UML model-based approach to detect infeasible paths","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937398025&doi=10.1016%2fj.jss.2015.05.007&partnerID=40&md5=181fa0ef9078ec9733348b90c9e48b41","UML model-based analysis is gaining wide acceptance for its cost effectiveness and lower overhead for processing compared to code-based analysis. A possible way to enhance the precision of the results of UML based analysis is by detecting infeasible paths in UML models. Our investigation reveals that two interaction patterns called Null Reference Check (NLC) and Mutually Exclusive (MUX) can cause a large number of infeasible paths in UML sequence diagrams. To detect such infeasible paths, we construct a graph model (called SIG), generate MM paths from the graph model, where an MM path refers to an execution sequence of model elements from the start to end of a method scope. Subsequently, we determine infeasibility of the MM paths with respect to MUX and NLC patterns. Our proposed model-based approach is useful to help exclude generation of test cases and test data for prior-detected infeasible paths, refine test effort estimation, and facilitate better test planning in the early stages of software development life cycle. © 2015 Elsevier Inc. All rights reserved.","Infeasible path detection; Model-based analysis; Softwaretesting; UML sequence diagram"
"A QoS-aware self-correcting observation based load balancer","2016","Journal of Systems and Software","10.1016/j.jss.2016.01.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958950990&doi=10.1016%2fj.jss.2016.01.042&partnerID=40&md5=c98b2a4d1ee6cc950346d332380b7d2a","Any service offered by a load balanced cluster is deployed on every member of the cluster. The Sliding window based Self-Learning and Adaptive Load Balancer (SSAL) is an observation based load balancer that optimizes throughput. It gives single point entry to access any service hosted on the cluster. This paper proposes a QoS-aware and Self-correcting observation based Load Balancer (QSLB) that extends the SSAL to (i) prevent the single point of failure of the load balancer, (ii) manage the cluster capacity, (iii) support the QoS monitoring, and (iv) estimate the cluster capacity needed to meet the QoS benchmarks. Redundant QSLBs collaborate to estimate the capabilities of the individual cluster members, share the available cluster capacity, and monitor the QoS parameters. Two models to estimate the cluster capacity needed to meet the QoS benchmarks are proposed. Experiments were conducted to test the QSLB's features. The experimental results confirmed that (i) the overhead to support these QSLB features is minimal, (ii) the QSLBs retained their share of the cluster capacity even in dynamic environments, and (iii) using the recommended cluster capacity improved the QoS met percentage. The proposed model improves fault tolerance, assists in cluster capacity management, and monitors the QoS. © 2016 Elsevier Inc. All rights reserved.","Dynamic load balancing; Fault tolerance; Quality of Service"
"A survey on software architectural assumptions","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962409014&doi=10.1016%2fj.jss.2015.12.016&partnerID=40&md5=e8b5b926a39846ebdf51d233d179c91a","Context: Managing architectural assumptions (AA) during the software lifecycle, as an important type of architecture knowledge, is critical to the success of projects. However, little empirical evidence exists on the understanding, identification, and recording of AA from the practitioners' perspective. Objective: We investigated the current situation on (1) how practitioners understand AA and its importance, and (2) whether and how practitioners identify and record AA in software development. Method: A web-based survey was conducted with 112 practitioners, who use Chinese as native language and are engaged in software development in China. Results: The main findings are: (1) AA are important in both software architecting and development. However, practitioners understand AA in different ways; (2) only a few respondents identified and recorded AA in their projects, and very few approaches and tools were used for identifying and recording AA; (3) the lack of specific approaches and tools is the major challenge (reason) of (not) identifying and recording AA. Conclusions: The results emphasize the need for a widely accepted understanding of the AA concept in software development, and specific approaches, tools, and guidelines to support AA identification and recording. © 2015 Elsevier Inc. All rights reserved.","Architectural assumption; Industrial survey; Software architecture"
"On the design of a maintainable software development kit to implement integration solutions","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949883486&doi=10.1016%2fj.jss.2015.08.044&partnerID=40&md5=e34df6daec37760fb1c43cade671ae60","Companies typically rely on applications purchased from third parties or developed at home to support their business activities. It is not uncommon that these applications were not designed taking integration into account. Enterprise Application Integration provides methodologies and tools to design and implement integration solutions. Camel, Spring Integration, and Mule range amongst the most popular open-source tools that provide support to implement integration solutions. The adaptive maintenance of a software tool is very important for companies that need to reuse existing tools to build their own. We have analysed 25 maintainability measures on Camel, Spring Integration, and Mule. We have conducted a statistical analysis to confirm the results obtained with the maintainability measures, and it follows that these tools may have problems regarding maintenance. These problems increase the costs of the adaptation process. This motivated us to work on a new proposal that has been carefully designed in order to reduce maintainability efforts. Guaraná SDK is the software tool that we provide to implement integration solutions. We have also computed the maintainability measures regarding Guaraná SDK and the results suggest that maintaining it is easier than maintaining the others. Furthermore, we have conducted an industrial experience to demonstrate the application of our proposal in industry. © 2015 Elsevier Inc. All rights reserved.","Enterprise Application Integration; Integration framework"
"Goal-driven adaptive monitoring of SOA systems","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944062236&doi=10.1016%2fj.jss.2015.08.015&partnerID=40&md5=7cca485903ff0b7175a3a0e7c274a10b","The advent of the Service Oriented Architecture (SOA) enabled implementation of IT systems of increasing complexity, rendering effective monitoring a nontrivial task. The inherent scale and dynamism of such systems does not allow for simultaneous monitoring of all system elements without introducing noticeable overhead. Therefore, an important challenge arises in the field of SOA monitoring. There is a need for a solution which enables on-demand adjustment of the monitoring process in accordance with the demands of system administrators and the changing conditions of the execution environment. The paper addresses the identified need by proposing the concept of goal-driven adaptive monitoring, capable of performing dynamic management of the monitoring process. The proposed concept is used for designing the Dynamic Adaptive Monitoring Framework, referred to as DAMON, which can be installed in an existing SOA environment to provide it with adaptive monitoring features. For the purpose of evaluation, a prototype of the DAMON framework was implemented on the basis of the OSGi technology. The evaluation, presented in the paper, not only verified the functional aspects of DAMON, but it also proved that the framework can decrease the monitoring overhead in large-scale dynamic systems. © 2015 Elsevier Inc. All rights reserved.","Adaptive drill-down; Goal-driven monitoring; Service oriented architecture"
"Evolution of software in automated production systems: Challenges and research directions","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944029939&doi=10.1016%2fj.jss.2015.08.026&partnerID=40&md5=47bec8880c05bacf5c623e70e87db46b","Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. © 2015 Elsevier Inc. All rights reserved.","Automated production systems; Automation; Evolution; Software engineering"
"A goal-oriented approach for representing and using design patterns","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944076649&doi=10.1016%2fj.jss.2015.07.040&partnerID=40&md5=853c900a3bf95667119729d131a57fa1","Design patterns are known as proven solutions to recurring design problems. The role of pattern documentation format is to transfer experience thus making pattern employment a viable technique. This research line proposes a goal-oriented pattern documentation that highlights decision-relevant information. The contribution of this paper is twofold. First, it presents a semi-structural visual notation that visualizes context, forces, alternative solutions and consequences in a compact format. Second, it introduces a systematic reuse process, in which the use of goal-oriented patterns aids the practitioner in selecting and customizing design patterns. An empirical study has been conducted the results of which supports the hypothesis that the goal-oriented format provides benefits for the practitioner. The experiment revealed a trend in which solutions better address requirements when the subjects are equipped with the new pattern documentation. © 2015 Elsevier Inc. All rights reserved.","Design patterns; Goal modeling; Goal reasoning"
"A three-dimensional taxonomy for bidirectional model synchronization","2016","Journal of Systems and Software","10.1016/j.jss.2015.06.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941661877&doi=10.1016%2fj.jss.2015.06.003&partnerID=40&md5=cce61814608ea088fa72c1b8e6387335","Early model-driven engineering (MDE) assumed simple pipeline-like scenarios specified by the Model-Driven Architecture approach: platform-independent models that describe a software system at a high-level of abstraction are transformed stepwise to platform-dependent models from which executable source code is generated. Modern applications require a shift toward networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools, and a solid semantic foundation to understand and classify these features. We address the problem by presenting a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to a specific synchronization semantics with an underlying algebraic model and the respective requirements for the change propagation operations and their properties. The taxonomy aims to help with identifying and communicating a proper specification for the synchronization problem at hand and for the available solutions offered by tools. © 2015 Elsevier Inc. All rights reserved.","Formal semantics; Model synchronization; Taxonomy"
"How do bugs surface? A comprehensive study on the characteristics of software bugs manifestation","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962408965&doi=10.1016%2fj.jss.2015.11.021&partnerID=40&md5=2f4290f2de07be7b82607181bddcff77","The impact of software bugs on today's system failures is of primary concern. Many bugs are detected and removed during testing, while others do not show up easily at development time and manifest themselves only as operational failures. Besides the importance of understanding the bug features from the programmer perspective (i.e., what is wrong in the code), a key role in counteracting bugs is played by the chain that from the bug activation leads to failure. This article investigates the characteristics of the bug manifestation process. Through an extensive empirical study, a set of failure-exposing conditions is first identified as bug manifestation characteristics; 666 bug reports from two applications are then analyzed with respect to these characteristics under several perspectives. Findings highlight: (i) the main occurrence patterns of bug triggering conditions in the selected case studies and the role played by the workload, the application and the environment where it runs; (ii) how such conditions evolve over time; (iii) how they relate to bug exposure and fixing difficulty; (iv) how they impact the user. Results provide a fine-grain characterization of bug manifestation that is expected to increase the perceived importance of this dimension in testing, debugging, and fault tolerance strategies. © 2015 Elsevier Inc. All rights reserved.","Bug characterization; Bug manifestation; Failure"
"What recommendation systems for software engineering recommend: A systematic literature review","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962376126&doi=10.1016%2fj.jss.2015.11.036&partnerID=40&md5=949412aeaf85932acff3fbdef86ec0cc","A recommendation system for software engineering (RSSE) is a software application that provides information items estimated to be valuable for a software engineering task in a given context. Present the results of a systematic literature review to reveal the typical functionality offered by existing RSSEs, research gaps, and possible research directions. We evaluated 46 papers studying the benefits, the data requirements, the information and recommendation types, and the effort requirements of RSSE systems. We include papers describing tools that support source code related development published between 2003 and 2013. The results show that RSSEs typically visualize source code artifacts. They aim to improve system quality, make the development process more efficient and less expensive, lower developer's cognitive load, and help developers to make better decisions. They mainly support reuse actions and debugging, implementation, and maintenance phases. The majority of the systems are reactive. Unexploited opportunities lie in the development of recommender systems outside the source code domain. Furthermore, current RSSE systems use very limited context information and rely on simple models. Context-adapted and proactive behavior could improve the acceptance of RSSE systems in practice. © 2015 Elsevier Inc. Allrights reserved.","Recommendation system for software engineering; Systematic literature review"
"Reported project management effort, project size, and contract type","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941280528&doi=10.1016%2fj.jss.2015.08.008&partnerID=40&md5=df1d7bf49991ad1e65f9ac3b4d42312c","Literature based hypotheses on the proportion of project management effort are created and tested with reported effort data from 117 projects by software supplier firms. The results support most of the literature based hypotheses, but some of the hypotheses are not supported. The supported hypotheses are the correlations between project management effort, project size, and team size. The unsupported hypotheses are the necessity of spending at least some project management effort on a project, and the insignificance of contract type. The conflict with previous studies and practical experience may be a result from market pressures and skewed reporting. The analysis implies that there is a pattern of inaccurate reporting of effort data and some possible explanations for the pattern are discussed. The results suggest that we do not properly understand the internal dynamics of supplier firms. © 2015 Elsevier Inc. Allrights reserved.","Contract type; Effort; Project management"
"Qualitative optimization in software engineering: A short survey","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949921376&doi=10.1016%2fj.jss.2015.09.001&partnerID=40&md5=1a8445f680b9cef6a820cbbce16e6896","Many software engineering problems involve finding optimal solutions from a set of feasible solutions. Such methods often require stakeholders such as developers and testers to specify preferences over multiple attributes/objectives that are to be optimized. However, in many cases it is more natural for stakeholders to express such preferences in simple, qualitative terms. We survey relevant literature within software engineering for problems in which qualitative optimization techniques can be useful. We also present a model of optimization that relies on the stakeholders qualitative preferences leveraging recent advances in decision theoretic artificial intelligence, which could prove useful and spawn connections between qualitative decision theory and software engineering. © 2015 Elsevier Inc. All rights reserved.","Decision-support; Preferences; Qualitative optimization; Search-based software engineering"
"Achieving functional and non functional interoperability through synthesized connectors","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949742738&doi=10.1016%2fj.jss.2015.09.038&partnerID=40&md5=daaa90feb22de6669a4bf38a17930aaf","Our everyday life is pervaded by the use of a number of heterogeneous systems that are continuously and dynamically available in the networked environment to interoperate to achieve some goal. Goals may include both functional and non functional aspects and the evolving nature of such environment requires automated solutions as means to reach the needed level of flexibility. Achieving interoperability in such environment is a challenging problem. Even though some of such systems may in principle interact since they have compatible functionalities and similar interaction protocols, mismatches in their protocols and non functional issues arising from the environment may undermine their seamless interoperability. In this paper, we propose an approach for the automated synthesis of application layer connectors between heterogeneous networked systems (NSs) addressing both functional and some non functional interoperability. Our contributions are: (i) an automated connectors synthesis approach for NSs interoperability taking into account functional, performance and dependability aspects spanning pre-deployment time and run-time; (ii) a connector adaptation process, related to the performance and dependability aspects; and (iii) a stochastic model-based implementation of the performance and dependability analysis. In addition, we implemented, analyzed, and critically discussed a case study. © 2015 Elsevier Inc. All rights reserved.","Connector synthesis for interoperability; Dependability; Performance"
"Optimizing runtime performance of hybrid dynamically and statically typed languages for the .Net platform","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962367259&doi=10.1016%2fj.jss.2015.11.041&partnerID=40&md5=33a8e6e64c8010a53d3f6a182634fd8e","Dynamically typed languages have become popular in scenarios where high flexibility and adaptability are important issues. On the other hand, statically typed languages provide important benefits such as earlier type error detection and, usually, better runtime performance. The main objective of hybrid statically and dynamically typed languages is to provide the benefits of both approaches, combining the adaptability of dynamic typing and the robustness and performance of static typing. The dynamically typed code of hybrid languages for the .Net platform typically use the introspection services provided by the platform, incurring a significant performance penalty. We propose a set of transformation rules to replace the use of introspection with optimized code that uses the services of the Dynamic Language Runtime. These rules have been implemented as a binary optimization tool, and included as part of an existing open source compiler. Our system has been used to optimize 37 programs in 5 different languages, obtaining significant runtime performance improvements. The additional memory resources consumed by optimized programs have always been lower than the corresponding performance gains obtained. © 2015 Elsevier Inc. All rights reserved.","Dynamic language runtime; Hybrid static and dynamic typing; Runtime performance optimization"
"A design methodology for user-centered innovation in the software development area","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944051106&doi=10.1016%2fj.jss.2015.08.029&partnerID=40&md5=1005b7b9f0537a34d3be2b0594b06e40","This paper proposes a methodology for conduct of HCI and Entrepreneurship courses in parallel with the Computing area. DUCI (Design for User-Centered Innovation) methodology aims at guiding the development of projects to produce software which integrates the concepts of both areas. The main proposal is to encourage the conception of new ideas in the field of software development, concerning not only the business but also the user's needs. The end-user requirements and needs are explored from human computer interaction techniques that improve the business idea. In this work, therefore, we proceed to answer the question: do HCI and Entrepreneurship, when taught in parallel, encourage the conception of new ideas in the field of software development? We conducted two case studies to observe the effectiveness of the methodology in an undergraduate and a postgraduate course in computer science, with a total of 107 students, during 2012 and 2013, analyzing the change in students' views from before and after the use of the methodology. © 2015 Elsevier Inc. All rights reserved.","Innovation; Software development; User-centered design"
"A small world based overlay network for improving dynamic load-balancing","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937411192&doi=10.1016%2fj.jss.2015.06.001&partnerID=40&md5=e3f08bf85976a6f3463904705766deac","Load-balancing algorithms play a key role in improving the performance of distributed-computing-systems that consist of heterogeneous nodes with different capacities. The performance of load-balancing algorithms and its convergence-rate deteriorate as the number-of-nodes in the system, the network-diameter, and the communication-overhead increase. Moreover, the load-balancing technical-factors significantly affect the performance of rebalancing the load among nodes. Therefore, we propose an approach that improves the performance of load-balancing algorithms by considering the load-balancing technical-factors and the structure of the network that executes the algorithm. We present the design of an overlay network, namely, functional small world (FSW) that facilitates efficient load-balancing in heterogeneous systems. The FSW achieves the efficiency by reducing the number-of-nodes that exchange their information, decreasing the network diameter, minimizing the communication-overhead, and decreasing the time-delay results from the tasks re-migration process. We propose an improved load-balancing algorithm that will be effectively executed within the constructed FSW, where nodes consider the capacity and calculate the average effective-load. We compared our approach with two significant diffusion methods presented in the literature. The simulation results indicate that our approach considerably outperformed the original neighborhood approach and the nearest neighbor approach in terms of response time, throughput, communication overhead, and movements cost. © 2015 Elsevier Inc. All rights reserved.","Diffusion; Distributed systems; Dynamic load-balancing"
"ISODAC: A high performance solution for indexing and searching heterogeneous data","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979570881&doi=10.1016%2fj.jss.2015.11.043&partnerID=40&md5=5725d78c0d8443690f1f302613346ffe","Searching for words or sentences within large sets of textual documents can be very challenging unless an index of the data has been created in advance. However, indexing can be very time consuming especially if the text is not readily available and has to be extracted from files stored in different formats. Several solutions, based on the MapReduce paradigm, have been proposed to accelerate the process of index creation. These solutions perform well when data are already distributed across the hosts involved in the elaboration. On the other hand, the cost of distributing data can introduce noticeable overhead. We propose ISODAC, a new approach aimed at improving efficiency without sacrificing reliability. Our solution reduces to the bare minimum the number of I/O operations by using a stream of in-memory operations to extract and index text. We further improve the performance by using GPUs for the most computationally intensive tasks of the indexing procedure. ISODAC indexes heterogeneous documents up to 10.6x faster than other widely adopted solutions, such as Apache Spark. As proof-of-concept, we developed a tool to index forensic disk images that can easily be used by investigators through a web interface. © 2015 Elsevier Inc.","Digital forensics; In-Memory processing; Indexing"
"Multi-objective optimization of energy consumption and execution time in a single level cache memory for embedded systems","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949883736&doi=10.1016%2fj.jss.2015.10.012&partnerID=40&md5=3afe69fbb2a43497051d1d807946affd","Current embedded systems are specifically designed to run multimedia applications. These applications have a big impact on both performance and energy consumption. Both metrics can be optimized selecting the best cache configuration for a target set of applications. Multi-objective optimization may help to minimize both conflicting metrics in an independent manner. In this work, we propose an optimization method that based on Multi-Objective Evolutionary Algorithms, is able to find the best cache configuration for a given set of applications. To evaluate the goodness of candidate solutions, the execution of the optimization algorithm is combined with a static profiling methodology using several well-known simulation tools. Results show that our optimization framework is able to obtain an optimized cache for Mediabench applications. Compared to a baseline cache memory, our design method reaches an average improvement of 64.43 and 91.69% in execution time and energy consumption, respectively. © 2015 Elsevier Inc. All rights reserved.","Cache memory; Energy; Performance"
"Mediation information system engineering based on hybrid service composition mechanism","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937786811&doi=10.1016%2fj.jss.2015.05.064&partnerID=40&md5=3fa559fa68857c23851b81e3141d819e","Business-IT alignment nowadays has become crucial, with the expansion of service-based information systems and the need to collaborate with external partners. This research work therefore presents a hybrid service composition mechanism coupling logic-based and syntactic matchmaking of services and messages to transform a business process into an executable workflow. To meet the business requirements, this mechanism is based on both top-down and bottom-up approaches using available technical services and a generic semantic profile as pivot model. Whereas the service matchmaking focuses on the functional coverage of the generated workflow, the messageone generates the message transformation needed. © 2015 Elsevier Inc. All rights reserved.","Interoperability; Mediation Information systems; Semantic web services"
"A framework for modelling tactical decision-making in autonomous systems","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944059460&doi=10.1016%2fj.jss.2015.08.046&partnerID=40&md5=853ea475db237ac1816f8951ccf83f7f","There is an increasing need for autonomous systems that exhibit effective decision-making in unpredictable environments. However, the design of autonomous decision-making systems presents considerable challenges, particularly when they have to achieve their goals within a dynamic context. Tactics designed to handle unexpected environmental change, or attack by an adversary, must balance the need for reactivity with that of remaining focused on the system's overall goal. The lack of a design methodology and supporting tools for representing tactics makes them difficult to understand, maintain and reuse. This is a significant problem in the design of tactical decision-making systems. We describe a methodology and accompanying tool, TDF (Tactics Development Framework), based on the BDI (Beliefs, Desires, Intentions) paradigm. TDF supports structural modelling of missions, goals, scenarios, input/output, messaging and procedures, and generates skeleton code that reflects the overall design. TDF has been evaluated through comparison with UML, indicating that it provides significant benefits to those building autonomous, tactical decision-making systems. © 2015 Elsevier Inc. All rights reserved.","Behaviour modelling; Multi-agent systems; Tactics modelling"
"Using simulation to evaluate error detection strategies: A case study of cloud-based deployment processes","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944071192&doi=10.1016%2fj.jss.2015.08.043&partnerID=40&md5=b124bbc146fb09a9d9d2fc2cfa43cc50","The processes for deploying systems in cloud environments can be the basis for studying strategies for detecting and correcting errors committed during complex process execution. These cloud-based processes encompass diverse activities, and entail complex interactions between cloud infrastructure, application software, tools, and humans. Many of these processes, such as those for making release decisions during continuous deployment and troubleshooting in system upgrades, are highly error-prone. Unlike the typically well-tested deployed software systems, these deployment processes are usually neither well understood nor well tested. Errors that occur during such processes may require time-consuming troubleshooting, undoing and redoing steps, and problem fixing. Consequently, these processes should ideally be guided by strategies for detecting errors that consider trade-offs between efficiency and reliability. This paper presents a framework for systematically exploring such trade-offs. To evaluate the framework and illustrate our approach, we use two representative cloud deployment processes: a continuous deployment process and a rolling upgrade process. We augment an existing process modeling language to represent these processes and model errors that may occur during process execution. We use a process-aware discrete-event simulator to evaluate strategies and empirically validate simulation results by comparing them to experiences in a production environment. Our evaluation demonstrates that our approach supports the study of how error-handling strategies affect how much time is taken for task-completion and error-fixing. © 2015 Elsevier Inc. All rights reserved.","Deployment process; Process modeling; Simulation"
"A closed-loop context aware data acquisition and resource allocation framework for dynamic data driven applications systems (DDDAS) on the cloud","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941275623&doi=10.1016%2fj.jss.2015.07.026&partnerID=40&md5=a262ff5e0749e5099e98ebafadff0bd6","Various dynamic data driven applications systems (DDDAS) such as hazard management, target tracking, and battlefield monitoring often leverage multiple heterogeneous sensors, and generate huge volume of data. Not surprisingly, researchers are investigating ways to support such applications on the cloud. However, in such applications, the importance of a subset of sensors may change quickly due to changes in the execution environment, which often require adaptation of sampling rates accordingly. Additionally, such variations in sampling rates can create significant load imbalance on back-end servers, leading toward performance degradation. To address this, we investigate a closed-loop integrated solution as follows. First, we develop a centralized algorithm that attempts to maximize the overall quality of information for the whole network given the utility functions and the importance rankings of sensor nodes. Next, we present a threshold based heuristic that prevents omission of highly important nodes at critical times. Finally, a proactive resource optimization framework is investigated that adaptively allocate resources (e.g.; servers) in response to changed sampling rates. Extensive evaluation on cloud platform for various scenarios shows that our approach can quickly adapt sampling rates and reallocate resources in response to the changed importance of sensor nodes, minimizing data loss significantly. © 2015Elsevier Inc. Allrights reserved.","Cloud computing; DDDAS; Resource allocation"
"MAS-ML 2.0: Supporting the modelling of multi-agent systems with different agent architectures","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937781230&doi=10.1016%2fj.jss.2015.06.008&partnerID=40&md5=73e035bcb532d66a9922bd4d9842e13c","Multi-agent systems (MAS) involve a wide variety of agents that interact with each other to achieve their goals. Usually each agent has a particular internal architecture defining its main structure that gives support to the interaction among the entities of MAS. Many modelling languages have been proposed in recent years to represent the internal architectures of such agents, for instance the UML profiles. In particular, we highlight MAS-ML, an MAS modelling language that performs a conservative extension of UML while incorporating agent-related concepts to represent proactive agents. However, such languages fail to support the modelling of the heterogeneous architectures that can be used to develop the agents of an MAS. Even worse, little has been done to provide tools to help the systematic design of agents. This paper, therefore, aims to extend the MAS-ML metamodel and evolve its tool to support the modelling of not only proactive agents but also several other architectures described in the literature. © 2015 Elsevier Inc. All rights reserved.","Modelling language; Multi-agent system; Proactive and reactive agents"
"Multi-criteria analysis of measures in benchmarking: Dependability benchmarking as a case study","2016","Journal of Systems and Software","10.1016/j.jss.2015.08.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949870883&doi=10.1016%2fj.jss.2015.08.052&partnerID=40&md5=c9c3ec9b8891354023367eb762e48307","Benchmarks enable the comparison of computer-based systems attending to a variable set of criteria, such as dependability, security, performance, cost and/or power consumption. It is not despite its difficulty, but rather its mathematical accuracy that multi-criteria analysis of results remains today a subjective process rarely addressed in an explicit way in existing benchmarks. It is thus not surprising that industrial benchmarks only rely on the use of a reduced set of easy-to-understand measures, specially when considering complex systems. This is a way to keep the process of result interpretation straightforward, unambiguous and accurate. However, it limits at the same time the richness and depth of the analysis process. As a result, the academia prefers to characterize complex systems with a wider set of measures. Marrying the requirements of industry and academia in a single proposal remains a challenge today. This paper addresses this question by reducing the uncertainty of the analysis process using quality (score-based) models. At measure definition time, these models make explicit (i) which are the requirements imposed to each type of measure, that may vary from one context of use to another, and (ii) which is the type, and intensity, of the relation between considered measures. At measure analysis time, they provide a consistent, straightforward and unambiguous method to interpret resulting measures. The methodology and its practical use are illustrated through three different case studies from the dependability benchmarking domain, a domain where various different criteria, including both performance and dependability, are typically considered during analysis of benchmark results. Although the proposed approach is limited to dependability benchmarks in this document, its usefulness for any type of benchmark seems quite evident attending to the general formulation of the provided solution. © 2015 Elsevier Inc. All rights reserved.","Dependability benchmarking; Multiple-Criteria Decision Making (MCDM); Quality models"
"Static change impact analysis techniques: A comparative study","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941264977&doi=10.1016%2fj.jss.2015.07.047&partnerID=40&md5=6ae256dbd1a48c9372e45f22a58140d7","Software Change Impact Analysis (CIA) is an essential technique to identify the potential effects caused by software changes during software maintenance and evolution. A rich body of CIA techniques, especially static CIA techniques, have continuously emerged in recent years such as structural static analysis, textual analysis, and historical analysis. However, there were only a few works focusing on comparison of static CIA techniques. This article attempts to bridge this gap by presenting a comparative study of three class-level static CIA techniques, i.e.; Columbus, ROSE, and IRC2M. We compare them based on a CIA comparative framework and conduct an empirical study to evaluate these three CIA techniques and their combinations based on five real-world programs. The empirical results show that: (1) IRC2M and ROSE achieve relatively better precision, recall and F-measure compared to Columbus; (2) combination of any two CIA techniques can improve the precision and recall over their individual one; moreover, combining ROSE with IRC2M produces the best impact results; and (3) combining all three CIA techniques obtain a similar precision and recall as combining ROSE with IRC2M. © 2015 Elsevier Inc. Allrights reserved.","Change impact analysis; Comparative study; Empirical study"
"MeSRAM - A method for assessing robustness of measurement programs in large software development organizations and its industrial evaluation","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962199108&doi=10.1016%2fj.jss.2015.10.051&partnerID=40&md5=77765daf16994c889d1d2ce29d6b7c77","Measurement programs in large software development organizations contain a large number of indicators, base and derived measures to monitor products, processes and projects. The diversity and the number of these measures causes the measurement programs to become large, combining multiple needs, measurement tools and organizational goals. For the measurement program to effectively support organization's goals, it should be scalable, automated, standardized and flexible - i.e. robust. In this paper we present a method for assessing the robustness of measurement programs. The method is based on the robustness model which has been developed in collaboration between seven companies and a university. The purpose of the method is to support the companies to optimize the value obtained from the measurement programs and their cost. We evaluated the method at the seven companies and the results from applying the method to each company quantified the robustness of their programs, reflecting the real-world status of the programs and pinpointed strengths and improvements of the programs. © 2015 Elsevier Inc. All rights reserved.","Case study; Measurement program; Metrics; Software engineering"
"Utilizing online serious games to facilitate distributed requirements elicitation","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941283640&doi=10.1016%2fj.jss.2015.07.017&partnerID=40&md5=938c5e6c46e70e64842d78cfbef5f716","Requirements elicitation is one of the most important and challenging activities in software development projects. A variety of challenges related to requirements elicitation are reported in the literature, of which the lack of proper communication and knowledge transfer between software stakeholders are among the most important. Communication and knowledge transfer are becoming even bigger challenges with the current increase in globally distributed software development projects due to the temporal, geographic, and sociocultural diversity among software stakeholders. In this study, we propose a new approach to requirements elicitation, which employs online serious games for gathering requirements from distributed software stakeholders. The feasibility and effectiveness of the proposed approach were evaluated in an empirical study with encouraging results. These results especially reveal that our suggested approach enables less-experienced individuals to identify a higher number of requirements. Our results also reveal that for the majority of subjects, especially individuals with less technical experience, this approach was a pleasant and easy way of participating in requirements elicitation. Based on these results we suggest that using online serious games not only enhances innovation and creativity among end-users but also facilitates collaboration and communication among software stakeholders. Implications for both research and practice are considered. © 2015 Elsevier Inc. Allrights reserved.","Global software development; Innovative requirements elicitation; Serious games"
"A Model-Driven approach for functional test case generation","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941266999&doi=10.1016%2fj.jss.2015.08.001&partnerID=40&md5=2652a32203ffe646e65675e3994e45aa","Test phase is one of the most critical phases in software engineering life cycle to assure the final system quality. In this context, functional system test cases verify that the system under test fulfills its functional specification. Thus, these test cases are frequently designed from the different scenarios and alternatives depicted in functional requirements. The objective of this paper is to introduce a systematic process based on the Model-Driven paradigm to automate the generation of functional test cases from functional requirements. For this aim, a set of metamodels and transformations and also a specific language domain to use them is presented. The paper finishes stating learned lessons from the trenches as well as relevant future work and conclusions that draw new research lines in the test cases generation context. © 2015ElsevierInc.Allrightsreserved.","Early testing; Model-Driven testing; Software quality assurance"
"A survey of software engineering practices in Turkey","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937809720&doi=10.1016%2fj.jss.2015.06.036&partnerID=40&md5=8184c7a4fb646ba7d681fbc178384ba8","Understanding the types of software engineering (SE) practices and techniques used in industry is important. There is a wide spectrum in terms of the types and maturity of SE practices conducted in industry. Turkey has a vibrant software industry and it is important to characterize and understand the state of its SE practices. Our objective is to characterize and grasp a high-level view on type of SE practices in the Turkish software industry. To achieve this objective, we systematically designed an online survey with 46 questions based on our past experience in the Canadian and Turkish contexts and using the Software Engineering Body of Knowledge (SWEBOK). Two hundred and two practicing software engineers from the Turkish software industry participated in the survey. The survey results reveal important and interesting findings about SE practices in Turkey and beyond. They also help track the profession of SE, and suggest areas for improved training, education and research. Among the findings are the followings: (1) The military and defense software sectors are quite prominent in Turkey, especially in the capital Ankara region, and many SE practitioners work for those companies. (2) 54% of the participants reported not using any software size measurement methods, while 33% mentioned that they have measured lines of code (LOC). (3) In terms of effort, after the development phase (on average, 31% of overall project effort), software testing, requirements, design and maintenance phases come next and have similar average values (14%, 12%, 12% and 11% respectively). (4) Respondents experience the most challenge in the requirements phase. (5) Waterfall, as a rather old but still widely used lifecycle model, is the model that more than half of the respondents (53%) use. The next most preferred lifecycle models are incremental and Agile/lean development models with usage rates of 38% and 34%, respectively. (6) The Waterfall and Agile methodologies have slight negative correlations, denoting that if one is used in a company, the other will less likely to be used. The results of our survey will be of interest to SE professionals both in Turkey and world-wide. It will also benefit researchers in observing the latest trends in SE industry identifying the areas of strength and weakness, which would then hopefully encourage further industry-academia collaborations in those areas. © 2015 Elsevier Inc.All rights reserved.","Industry practices; Software engineering; Turkey"
"Modeling and verification of Functional and Non-Functional Requirements of ambient Self-Adaptive Systems","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937425621&doi=10.1016%2fj.jss.2015.05.028&partnerID=40&md5=9868e264ccf9ce1f8fa8f180358c3f90","Self-Adaptive Systems modify their behavior at run-time in response to changing environmental conditions. For these systems, Non-Functional Requirements play an important role, and one has to identify as early as possible the requirements that are adaptable. We propose an integrated approach for modeling and verifying the requirements of Self-Adaptive Systems using Model Driven Engineering techniques. For this, we use Relax, which is a Requirements Engineering language which introduces flexibility in Non-Functional Requirements. We then use the concepts of Goal-Oriented Requirements Engineering for eliciting and modeling the requirements of Self-Adaptive Systems. For properties verification, we use OMEGA2/IFx profile and toolset. We illustrate our proposed approach by applying it on an academic case study. © 2015 Elsevier Inc. All rights reserved.","Dynamic Adaptive Systems; Goal Oriented Requirements Engineering; Model Driven Engineering; Non Functional Requirements; Properties verification; Relax"
"A model-driven approach for constructing ambient assisted-living multi-agent systems customized for Parkinson patients","2016","Journal of Systems and Software","10.1016/j.jss.2015.09.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949883502&doi=10.1016%2fj.jss.2015.09.014&partnerID=40&md5=9f4bad6b1e2654509c9723a8f01ac712","The Parkinson disease affects some people, especially in the last years of their lives. Ambient assisted living systems can support them, especially in the middle stages of the disease. However, these systems usually need to be customized for each Parkinson patient. In this context, the current work follows the model-driven engineering principles to achieve this customized development. It represents each patient with a model. This is transformed into an agent-based model, from which a skeleton of programming code is generated. A case study illustrates this approach. Moreover, 24 engineers expert in model-driven engineering, multi-agent systems and/or health experienced the current approach alongside the three most similar works, by implementing actual systems. Some of these systems were tested by Parkinson patients. The results showed that (1) the current approach reduced the development time, (2) the developed system satisfied a higher percentage of the requirements established for certain Parkinson patients, (3) the usability increased, (4) the performance of the systems improved taking response time into account, and (5) the developers considered that the underlying metamodel is more appropriate for the current goal. © 2015 Elsevier Inc. All rights reserved.","Agent-oriented software engineering; Model-driven engineering; Parkinson disease"
"Agile methods tailoring - A systematic literature review","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944066455&doi=10.1016%2fj.jss.2015.08.035&partnerID=40&md5=27cbc99ac08dc39c656944f5d95c28ab","Background: The software development industry has been adopting agile methods instead of traditional software development methods because they are more flexible and can bring benefits such as handling requirements changes, productivity gains and business alignment. Objective: This study seeks to evaluate, synthesize, and present aspects of research on agile methods tailoring including the method tailoring approaches adopted and the criteria used for agile practice selection. Method: The method adopted was a Systematic Literature Review (SLR) on studies published from 2002 to 2014. Results: 56 out of 783 papers have been identified as describing agile method tailoring approaches. These studies have been identified as case studies regarding the empirical research, as solution proposals regarding the research type, and as evaluation studies regarding the research validation type. Most of the papers used method engineering to implement tailoring and were not specific to any agile method on their scope. Conclusion: Most of agile methods tailoring research papers proposed or improved a technique, were implemented as case studies analyzing one case in details and validated their findings using evaluation. Method engineering was the base for tailoring, the approaches are independent of agile method and the main criteria used are internal environment and objectives variables. © 2015 Elsevier Inc. All rights reserved.","Agile method tailoring; Agile practice selection; Software method tailoring"
"Enabling public auditing for shared data in cloud storage supporting identity privacy and traceability","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962382590&doi=10.1016%2fj.jss.2015.11.044&partnerID=40&md5=fddbc8865f3ad1cc2943f23d65e1c70c","Nowadays, cloud storage service has been widely adopted by diverse organizations, through which users can conveniently share data with others. For security consideration, previous public auditing schemes for shared cloud data concealed the identities of group members. However, the unconstrained identity anonymity will lead to a new problem, that is, a group member can maliciously modify shared data without being identified. Since uncontrolled malicious modifications may wreck the usability of the shared data, the identity traceability should also be retained in data sharing. In this paper, we propose an efficient public auditing solution that can preserve the identity privacy and the identity traceability for group members simultaneously. Specifically, we first design a new framework for data sharing in cloud, and formalize the definition of the public auditing scheme for shared cloud data supporting identity privacy and traceability. And then we construct such a scheme, in which a group manager is introduced to help members generate authenticators to protect the identity privacy and two lists are employed to record the members who perform the latest modification on each block to achieve the identity traceability. Besides, the scheme also achieves data privacy during authenticator generation by utilizing blind signature technique. Based on the proposed scheme, we further design an auditing system for practical scenarios. Finally, we prove the proposed scheme is secure based on several security requirements, and justify its performance by concrete implementations. © 2015 Elsevier Inc. All rights reserved.","Cloud storage; Identity traceability; Public auditing"
"Effective and efficient detection of software theft via dynamic API authority vectors","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944061567&doi=10.1016%2fj.jss.2015.08.018&partnerID=40&md5=115bd3530cbd486cb834d3f113e43333","Software theft has become a very serious threat to both the software industry and individual software developers. A software birthmark indicates unique characteristics of a program in question, which can be used for analyzing the similarity of a pair of programs and detecting theft. This paper proposes a novel birthmark, a dynamic API authority vector (DAAV). DAAV satisfies four essential requirements for good birthmarks - credibility, resiliency, scalability, and packing-free - while existing static birthmarks are unable to handle the packed programs and existing dynamic birthmarks do not satisfy credibility and resiliency. Through our extensive experiments with a set of Windows applications, DAAV is shown to have not only the credibility and resiliency higher than the existing dynamic birthmarks but also the accuracy comparable to that of existing static birthmarks. This result indicates that our proposed birthmark provides high accuracy and also covers packed programs successfully in detecting software theft. © 2015 Elsevier Inc. All rights reserved.","Birthmark; Similarity analysis; Software theft detection"
"FLOW-assisted value stream mapping in the early phases of large-scale software development","2016","Journal of Systems and Software","10.1016/j.jss.2015.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949799650&doi=10.1016%2fj.jss.2015.10.013&partnerID=40&md5=fcdc9fd6cc5f2472574b0b959f660a39","Value stream mapping (VSM) has been successfully applied in the context of software process improvement. However, its current adaptations from Lean manufacturing focus mostly on the flow of artifacts and have taken no account of the essential information flows in software development. A solution specifically targeted toward information flow elicitation and modeling is FLOW. This paper aims to propose and evaluate the combination of VSM and FLOW to identify and alleviate information and communication related challenges in large-scale software development. Using case study research, FLOW-assisted VSM was used for a large product at Ericsson AB, Sweden. Both the process and the outcome of FLOW-assisted VSM have been evaluated from the practitioners' perspective. It was noted that FLOW helped to systematically identify challenges and improvements related to information flow. Practitioners responded favorably to the use of VSM and FLOW, acknowledged the realistic nature and impact on the improvement on software quality, and found the overview of the entire process using the FLOW notation very useful. The combination of FLOW and VSM presented in this study was successful in systematically uncovering issues and characterizing their solutions, indicating their practical usefulness for waste removal with a focus on information flow related issues. © 2015 Elsevier Inc. All rights reserved.","Case study research; Lean software development; Value stream mapping"
"IP packet interleaving for UDP bursty losses","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941271352&doi=10.1016%2fj.jss.2015.07.048&partnerID=40&md5=f27492a2a9c0a378f43bc680cd3b374b","The bursty nature of losses over the Internet is constantly asking for effective solutions. In this work, we use a comprehensive approach to study packet interleaving for coping with loss burstiness. Our aim is to assess if and how packet interleaving can be employed in real networks at the IP layer and what benefits real UDP-based applications may expect. Through an analytical study we determine the interleaving configuration to be used as a function of the loss characteristics. Then, in simulation we confirm these findings and highlight also potential counter effects. Afterwards, we design and develop a real application for packet interleaving. We move a step ahead towards real networks using a testbed that comprises an emulated WAN. Thanks to it, we study and solve a number of issues arising in real environments such as network dynamics and interleaving performance. Finally, we deploy the packet interleaver in the wild and we study the improvements achievable in general and by a real application. Our work shows that providing benefits to real applications is not an easy task. However, our packet interleaver can effectively decorrelate losses at the IP layer in real networks and highly increase real application performance, for example, video distortion can be reduced 65%. © 2015 Elsevier Inc. Allrights reserved.","Packet interleaving; Packet loss; UDP"
"Automatic service derivation from business process model repositories via semantic technology","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937782273&doi=10.1016%2fj.jss.2015.06.007&partnerID=40&md5=c84da36e23f6a81ab5b62cb3835e1ef9","Although several approaches for service identification have been defined in research and practice, there is a notable lack of fully automated techniques. In this paper, we address the problem of manual work in the context of service derivation and present an approach for automatically deriving service candidates from business process model repositories. Our approach leverages semantic technology in order to derive ranked lists of useful service candidates. An evaluation of the approach with three large process model collection from practice indicates that the approach can effectively identify useful services with hardly any manual effort. The evaluation further demonstrates that our approach can address varying degrees of service cohesion by applying different aggregation mechanisms. Hence, the presented approach represents a useful artifact for enabling business and IT managers to quickly spot reuse potential in their company. In addition, our approach improves the alignment between business and IT. As the ranked service candidates give a good impression on the relative importance of a business operation, they can provide companies with first clues on where IT support is needed and where it could be reduced. © 2015 Elsevier Inc. All rights reserved.","Natural language analysis; Process model repositories; Semantic technologies; Service derivation"
"Assessing dynamic models for high priority waste collection in smart cities","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944065770&doi=10.1016%2fj.jss.2015.08.049&partnerID=40&md5=5926f26293129bae2050da55bb3ae58d","Waste Management (WM) represents an important part of Smart Cities (SCs) with significant impact on modern societies. WM involves a set of processes ranging from waste collection to the recycling of the collected materials. The proliferation of sensors and actuators enable the new era of Internet of Things (IoT) that can be adopted in SCs and help in WM. Novel approaches that involve dynamic routing models combined with the IoT capabilities could provide solutions that outperform existing models. In this paper, we focus on a SC where a number of collection bins are located in different areas with sensors attached to them. We study a dynamic waste collection architecture, which is based on data retrieved by sensors. We pay special attention to the possibility of immediate WM service in high priority areas, e.g., schools or hospitals where, possibly, the presence of dangerous waste or the negative effects on human quality of living impose the need for immediate collection. This is very crucial when we focus on sensitive groups of citizens like pupils, elderly or people living close to areas where dangerous waste is rejected. We propose novel algorithms aiming at providing efficient and scalable solutions to the dynamic waste collection problem through the management of the trade-off between the immediate collection and its cost. We describe how the proposed system effectively responds to the demand as realized by sensor observations and alerts originated in high priority areas. Our aim is to minimize the time required for serving high priority areas while keeping the average expected performance at high level. Comprehensive simulations on top of the data retrieved by a SC validate the proposed algorithms on both quantitative and qualitative criteria which are adopted to analyze their strengths and weaknesses. We claim that, local authorities could choose the model that best matches their needs and resources of each city. © 2015 Elsevier Inc. All rights reserved.","Dynamic routing models; Internet of things; Smart cities; Waste collection"
"Extracting reusable design decisions for UML-based domain-specific languages: A multi-method study","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962437221&doi=10.1016%2fj.jss.2015.11.037&partnerID=40&md5=8e5f32ed8bdd2a3c424c6613861be8df","When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the DSML itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other DSML projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 DSML projects. These projects were identified - among others - via an extensive systematic literature review (SLR) for the years 2005-2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based DSMLs publicly available. Furthermore, our study offers insights into UML usage (e.g. diagram types) and into the adoption of UML extension techniques (e.g. metamodel extensions, profiles). © 2015 Elsevier Inc. All rights reserved.","Design decision; Design rationale; Domain-specific language; Domain-specific modeling; Model-driven development; Unified modeling language"
"Co-evolution of metamodels and models through consistent change propagation","2016","Journal of Systems and Software","10.1016/j.jss.2015.03.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925249904&doi=10.1016%2fj.jss.2015.03.003&partnerID=40&md5=61917f9bd5c9578133b187e445aaab16","In model-driven engineering (MDE), metamodels and domain-specific languages are key artifacts as they are used to define syntax and static semantics of domain models. However, metamodels are evolving over time, requiring existing domain models to be co-evolved. Though approaches have been proposed for performing such co-evolution automatically, those approaches typically support only specific metamodel changes. In this paper, we present a vision of co-evolution between metamodels and models through consistent change propagation. The approach addresses co-evolution issues without being limited to specific metamodels or evolution scenarios. It relies on incremental management of metamodel-based constraints that are used to detect co-evolution failures (i.e., inconsistencies between metamodel and model). After failure detection, the approach automatically generates suggestions for correction (i.e., repairs for inconsistencies). A case study with the UML metamodel and 23 UML models shows that the approach is technically feasible and also scalable. © 2015 Elsevier Inc. All rights reserved.","Consistency checking; Consistent change propagation; Metamodel co-evolution"
"Exploiting traceability uncertainty among artifacts and code","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937791308&doi=10.1016%2fj.jss.2015.06.037&partnerID=40&md5=573b78e4884cc2fc4d99baf173b27123","Traceability between software development artifacts and code has proven to save effort and improve quality. However, documenting and maintaining such traces remains highly unreliable. Traceability is rarely captured immediately while artifacts and code co-evolve. Instead they are recovered later. By then key people may have moved on or their recollection of facts may be incomplete and inconsistent. This paper proposes a language for capturing traceability that allows software engineers to express arbitrary assumption about the traceability between artifacts and code - even assumptions that may be inconsistent or incomplete. Our approach takes these assumptions to reasons about their logical consequences (hence increasing completeness) and to reveal inconsistencies (hence increasing correctness). In doing so, our approach's reasoning is correct even in the presence of known inconsistencies. This paper demonstrates the correctness and scalability of our approach on several, large-scale third-party software systems. Our approach is automated and tool supported. ©2015 Elsevier Inc. All rights reserved.","Analysis; Artifacts to code mapping; Traceability"
"Approximating closed fork-join queueing networks using product-form stochastic Petri-nets","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944078165&doi=10.1016%2fj.jss.2015.08.036&partnerID=40&md5=b0195d6c3d02bfc4432f037400647447","Computing paradigms have shifted towards highly parallel processing and massive replication of data. This entails the efficient distribution of requests and the synchronization of results provided to users. Guaranteeing SLAs requires the ability to evaluate the performance of such systems while taking the effect of non-parallel workloads into consideration. This can be achieved with performance models that are able to represent both parallel and sequential workloads. This paper presents a product-form stochastic Petri-net approximation of fork-join queueing networks with interfering requests. We derive the necessary conditions that guarantee the accuracy of the approximations and verify this through examples in comparison to simulation. We apply these approximate models to the performance evaluation of replication in NoSQL cloud datastores and illustrate the composition of large models from smaller models, thus facilitating the ability to model a range of deployment scenarios. We show the efficiency of our solution method, which finds the product-form solution of the models without the representation of the state-space of the underlying CTMC. © 2015 Elsevier Inc. All rights reserved.","NoSQL datastores; Product-form stochastic Petri nets; Reversed Compound Agent Theorem (RCAT)"
"Approximating expressive queries on graph-modeled data: The GeX approach","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941282123&doi=10.1016%2fj.jss.2015.07.028&partnerID=40&md5=00b9e40240fbd1eed9ed726511be3e18","We present the GeX (Graph-eXplorer) approach for the approximate matching of complex queries on graph-modeled data. GeX generalizes existing approaches and provides for a highly expressive graph-based query language that supports queries ranging from keyword-based to structured ones. The GeX query answering model gracefully blends label approximation with structural relaxation, under the primary objective of delivering meaningfully approximated results only. GeX implements ad-hoc data structures that are exploited by a top-k retrieval algorithm which enhances the approximate matching of complex queries. An extensive experimental evaluation on real world datasets demonstrates the efficiency of the GeX query answering. © 2015 Elsevier Inc. Allrights reserved.","Approximate graph query answering; Graph indexing; Top-k query answering"
"ToscaMart: A method for adapting and reusing cloud applications","2016","Journal of Systems and Software","10.1016/j.jss.2015.12.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962467630&doi=10.1016%2fj.jss.2015.12.025&partnerID=40&md5=6c772764c3ad1cab7054399263fd2edc","To fully exploit the potential of cloud computing, design and development of cloud applications should be eased and supported. The OASIS TOSCA standard enables developers to design and develop cloud applications by specifying their topologies as orchestrations of typed nodes. However, building such application topologies often results in reinventing the wheel multiple times when similar solutions are manually created by different developers for different applications having the same requirements. Thus, the reusability of existing TOSCA solutions is crucial to ease and support design and development processes. In this paper, we introduce and assess ToscaMart, a method that enables deriving valid implementations for custom components from a repository of cloud applications. The method enables developers to specify individual components in their application topologies, and illustrates how to match, adapt, and reuse existing fragments of applications to implement these components while fulfiling all their compliance requirements. We also validate ToscaMart by means of a prototypical implementation based on an open source toolchain and a case study. © 2015 Elsevier Inc. All rights reserved.","Cloud application; Reuse; TOSCA"
"Aggregation for adaptive and energy-efficient MAC in wireless sensor networks","2015","Journal of Systems and Software","10.1016/j.jss.2015.07.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941279991&doi=10.1016%2fj.jss.2015.07.038&partnerID=40&md5=1bf9b25048c64dd8a01687984e56b0fd","Time Division Multiple Access (TDMA) protocols are considered an energy-efficient solution to prolong wireless sensor network lifetime. However, their drawbacks such as the complexity of slot assignment and schedule maintenance, clock synchronization and low channel utilization during low traffic conditions are to be overcome in a good and efficient way. In this paper we present On-demand Convergecast Sensor MACS (OCSMACS), a centralized and adaptive multihop scheduling-based TDMA protocol for Wireless Sensor Networks. OCSMACS achieves high energy efficiency by adopting a novel requests aggregation mechanism for adaptive slot assignments such that all requests generated by active nodes are aggregated in a very few small size packets. In addition, knowledge of data correlation is integrated in the design of underlying multihop scheduling schemes to improve energy efficiency and extend network lifetime. Furthermore, OCSMACS adopts an energy-efficient protocol called PROGRESSIVE for the topology discovery and setup phases. The performance of OCSMACS is compared against existing protocols based on simulations using ns-2 and the results show that OCSMACS outperforms these protocols in terms of energy efficiency and network lifetime. Also, the integration of correlation knowledge in the scheduling process improves performance and extends network lifetime. © 2015 Elsevier Inc. Allrights reserved.","Aggregation; Energy efficiency; TDMA; Wireless sensor networks"
"IDF: A framework for the incremental development and conformance verification of UML active primitive components","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962423453&doi=10.1016%2fj.jss.2015.11.020&partnerID=40&md5=b533ddc591b9357cfaee3ca5802e5ffb","Modelling component behaviour is widely recognised as a complex task during the specification and design phases of reactive systems. Our proposal for treating this problem involves an incremental approach that allows UML state machines to be built using a composition of two types of development: model extension for adding services or behaviours, and refinement for adding details or eliminating non-determinism. At each step of the development process, the current model is verified for compliance with the model obtained during the previous step, in such a way that initial liveness properties are preserved. The novelty of this work lies in the possibility to combine and sequence both refinement and extension developments. This iterative process is usually not taken into account in conventional refinement relations. This set of development techniques and verification means are assembled into a framework called IDF (Incremental Development Framework), which is supported by a tool, under the acronym IDCM (Incremental Development of Compliant Models), developed herein in addition to the Topcased UML tool. © 2015 Elsevier Inc. All rights reserved.","Conformance relations; Incremental development; State machine refinement"
"SGEESS: Smart green energy-efficient scheduling strategy with dynamic electricity price for data center","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937795698&doi=10.1016%2fj.jss.2015.06.026&partnerID=40&md5=1032c302f85d0f1f7c43a33b8463145b","Nowadays, it becomes a major trend to use the green renewable energy in the data center when considering the environment protection and the energy crisis. To improve the energy efficiency and save the system cost, the computational tasks of data center should match to the renewable energy supply. This paper aims to develop a smart green energy-efficient scheduling strategy to increase utilization of renewable energy, reduce system running cost and improve the task satisfaction rate in a data center partially powered by the renewable energy. We first define three mathematical models, i.e.; task model, energy model and scheduling model for the proposed problem. Then, a smart green energy-efficient scheduling strategy is proposed for the task scheduling in the data center, based on the renewable energy prediction and the dynamic grid electricity price. In the experiments, three other scheduling strategies, i.e.; Green-Scheduling Strategy, Price-Scheduling Strategy and Greedy-Energy-Efficient Strategy, are provided for comparisons, a real-world trace of Google cloud trace is also tested. The experimental results confirm the superiority and effectiveness of the proposed scheduling strategy. © 2015 Elsevier Inc.All rights reserved.","Energy-efficient; Green data center; Scheduling"
"Architectural tactics for cyber-foraging: Results of a systematic literature review","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937402502&doi=10.1016%2fj.jss.2015.06.005&partnerID=40&md5=562fd9618b6341addafa370f6db7c269","Mobile devices have become for many the preferred way of interacting with the Internet, social media and the enterprise. However, mobile devices still do not have the computing power and battery life that will allow them to perform effectively over long periods of time, or for executing applications that require extensive communication, computation, or low latency. Cyber-foraging is a technique to enable mobile devices to extend their computing power and storage by offloading computation or data to more powerful servers located in the cloud or in single-hop proximity. This article presents the results of a systematic literature review (SLR) on architectures that support cyber-foraging. Elements of the identified architectures were codified in the form of Architectural Tactics for Cyber-Foraging. These tactics will help architects extend their design reasoning toward cyber-foraging as a way to support the mobile applications of the present and the future. © 2015 Elsevier Inc. All rights reserved.","Cyber-foraging; Mobile cloud computing; Software architecture"
"Analyzing maintainability and reliability of object-oriented software using weighted complex network","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944038744&doi=10.1016%2fj.jss.2015.08.014&partnerID=40&md5=430f079fd82a0996db5cd5fd31d364a3","Modeling software systems using complex networks can be an effective technique for analyzing the complexity of software systems. To enhance the technique, the structure of a complex network can be extended by assigning a weight to the edges of the complex network to denote the strength of communicational cohesion between a pair of related software components. This paper proposes an approach to represent an object-oriented software system using a weighted complex network in order to capture its structural characteristics, with respect to its maintainability and reliability. Nodes and edges are modeled based on the complexities of classes and their dependencies. Graph theory metrics are applied onto the transformed network with the purpose to evaluate the software system. Comparative analysis is performed using 40 object-oriented software systems, with different levels of maintenance effort. We found that common statistical patterns from the software systems can be identified easily. It is when these software systems are grouped and compared based on their levels of maintenance effort that their statistical patterns are more distinguishable to represent some common behavior and structural complexity of object-oriented software. The evaluations indicate that the proposed approach is capable of identifying software components that violate common software design principles. © 2015 Elsevier Inc. All rights reserved.","Complex network; Software complexity; Software maintenance"
"Software defined autonomic QoS model for future Internet","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944058348&doi=10.1016%2fj.jss.2015.08.016&partnerID=40&md5=840d5ea429f1f518713ae64e7a501168","Software defined technology has gained enormous momentum in both industry and academia. It may change the existing information flow architecture, which centered at hardware, by granting more privileges to deploy third-party software with more flexible network control and management capabilities. However, how to provide intelligent QoS guarantee in future network is still a hot research issue. Based on top-down design strategy, in this paper, we propose a hierarchical autonomic QoS model by adopting software-defined technologies. In this model, the network is divided into: data-plane, control-plane and management-plane. Then the data-plane is sliced as multiple virtual networks with the dynamic resource allocation capabilities, in order to carry different services or applications. The control-plane is abstracted as resource control layer and service control layer for each virtual network respectively. The management-plane then constructs hierarchical autonomic control loops in terms of different services' bearing virtual networks and the corresponding hierarchical control logic. With the hierarchical autonomic control loop and layered service management functions, this model can provide dynamic QoS guarantee to the underlying data networks. We further proposed a context-aware Collaborative Borrowing Based Packet-Marking (CBBPM) algorithm. As an important underlying QoS component in data-plane, CBBPM can be combined with other QoS components to provide some new QoS functions. Experiment comparisons between different QoS mechanisms demonstrate the effectiveness of our proposed hierarchical autonomic QoS model and the packet marking mechanism. © 2015 Elsevier Inc. All rights reserved.","Packet-marking; QoS; Software defined technology"
"Entity resolution based em for integrating heterogeneous distributed probabilistic data","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937509224&doi=10.1016%2fj.jss.2015.05.035&partnerID=40&md5=de6042e60a735781950440c615aefed6","Distributed computing is linked and equated to the industrial revolution. Its transformational nature is, however, associated with significant instances in the form of internet of thing operations. Entity resolution (ER) is a problem of matching and resolving records that represent the same real world entity. This is a long-standing challenge in distributed databases and information retrieval as a statistic. In a centralized approach, the problem of ER has not been scaled well as large amount of data need to be sent to a central node. In this paper, we present an algorithm which deals with heterogeneous distributed probabilistic data (HDPD) and also reduces processing time in a distributed environment. We propose two different approaches. First, we explore this instance with a matching (identification) problem to integrate different data models with expectation-maximization (EM) algorithm. Second, we apply ER methodology for HDPD to achieve major performance in terms of response time to produce the outcome. We validate HDPD through experiments over a 100-node cluster that records significant performance improvements over naive approaches. This paper is expected to provide insights in to database organizations and new technological development for the growth of distributed environment. © 2015 Elsevier Inc. All rights reserved.","Entity resolution; Heterogeneous database; Probabilistic data"
"A data mining correlated patterns-based periodic decentralized replication strategy for data grids","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944144955&doi=10.1016%2fj.jss.2015.08.019&partnerID=40&md5=4ffad12c0339dfec68925df14da7a39c","Data grids have emerged as a useful technology for managing large amounts of distributed data in many fields like scientific experiments and engineering applications. In this regard, replication in data grids is an efficient technique that aims to improve response time, reduce the bandwidth consumption and maintain reliability. Unfortunately, most of existing replication strategies consider a single file-based granularity and neglect correlations among different data files. However, the analysis of many real data intensive applications reveals that jobs and applications request groups of correlated files. In this paper, we propose a new dynamic periodic decentralized data replication strategy, called RSCP1, which considers a set of correlated files as granularity. In order to find out these correlations, a new maximal frequent correlated pattern mining algorithm of the data mining field is introduced. The data in this work is read-only and so there are no consistency issues involved. The evaluation metrics we analyze in the experiments are mean job execution time, effective network usage, total number of replications, hit ratio and percentage of storage filled. Using the OptorSim simulator, extensive experimentations show that our proposed strategy has better performance in comparison to other strategies under most of access patterns. © 2015 Elsevier Inc. All rights reserved.","Data grid; Data mining; Replication"
"Run-based exception prediction for workflows","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962162232&doi=10.1016%2fj.jss.2015.11.024&partnerID=40&md5=6d217292b6a25c9a1ff74d6f81268000","Events such as iteration of activities or lack of available resources can cause temporal exceptions in business processes. Exception prediction can improve the quality of workflow execution since preventive actions can be taken to reduce the occurrence of exceptions. Thus, it is crucial to provide an accurate and efficient temporal exception prediction capability for workflow management systems. In this paper, we propose a run-based exception prediction algorithm to predict temporal exceptions in workflows. The proposed algorithm is divided into two phases, design-time and run time. At design-time, all possible runs are generated from a workflow and their estimated execution time and mapping probability are calculated. At run time, temporal exceptions are predicted by analyzing the runs. Simulation experiments are performed to evaluate the proposed approach using five workflow models having different characteristics. Simulation experiments show that our approach is efficient and produces good results in prediction accuracy. © 2015 Elsevier Inc. All rights reserved.","Run; Temporal exception prediction; Workflow"
"A fair multi-attribute combinatorial double auction model for resource allocation in cloud computing","2015","Journal of Systems and Software","10.1016/j.jss.2015.06.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937784720&doi=10.1016%2fj.jss.2015.06.025&partnerID=40&md5=97da90627aca432d6a68b8358449ca29","Recently, Cloud computing has emerged as a market where computing related resources are treated as a utility and are priced. There is a big competition among the Cloud service providers and therefore, the providers offer the services strategically. Auction, a market based resource allocation strategy, has received the attention among the Cloud researchers recently. The auction principal of resource allocation is based on demand and supply. This work proposes a multi-attribute combinatorial double auction for the allocation of Cloud resources, which not only considers the price but other quality of service parameters also. Auctioneer extends some of the parameters to the offered bids from the bidders in order to provide fairness and robustness. In case of not meeting the assured quality, a penalty is imposed on the provider and customer is compensated. The reputation of the provider also diminishes in the forthcoming rounds. Performance study of the proposed model is done by simulation which reflects the usefulness of the method. © 2015 Elsevier Inc.","Cloud resources; Double auction; Multi-attribute"
"Alleviating the topology mismatch problem in distributed overlay networks: A survey","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962440620&doi=10.1016%2fj.jss.2015.11.038&partnerID=40&md5=9fce2d78d33d1e5c5098e2bad55dd0e5","Peer-to-peer (P2P) systems have enjoyed immense attention and have been widely deployed on the Internet for well over a decade. They are often implemented via an overlay network abstraction atop the Internet's best-effort IP infrastructure. P2P systems support a plethora of desirable features to distributed applications including anonymity, high availability, robustness, load balancing, quality of service and scalability to name just a few. Unfortunately, inherent weaknesses of early deployments of P2P systems, prevented applications from leveraging the full potential of the paradigm. One major weakness, identified early on, is the topology mismatch problem between the overlay network and the underlying IP topology. This mismatch can impose an extraordinary amount of unnecessary stress on network resources and can adversely affect both the scalability and efficiency of the operating applications. In this paper, we survey over a decade's worth of research efforts aimed at alleviating the topology mismatch problem in both structured and unstructured P2P systems. We provide a fine-grained categorization of the suggested solutions by discussing their novelty, advantages and weaknesses. Finally, we offer an analysis as well as pictorial comparisons of the reviewed approaches since we aim to offer a comprehensive reference for developers, system architects and researchers in the field. © 2015, Elsevier Inc. All rights reserved.","Overlay network; Topology awareness; Topology mismatch"
"A general theory of software engineering: Balancing human, social and organizational capitals","2015","Journal of Systems and Software","10.1016/j.jss.2015.08.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941274078&doi=10.1016%2fj.jss.2015.08.009&partnerID=40&md5=d28ab71e404a50abb924fd191dfa5764","There exists no generally accepted theory in software engineering, and at the same time a scientific discipline needs theories. Some laws, hypotheses and conjectures exist, but yet no generally accepted theory. Several researchers and initiatives emphasize the need for theory in the discipline. The objective of this paper is to formulate a theory of software engineering. The theory is generated from empirical observations of industry practice, including several case studies and many years of experience in working closely between academia and industry. The theory captures the balancing of three different intellectual capitals: human, social and organizational capitals, respectively. The theory is formulated using a method for building theories in software engineering. It results in a theory where the relationships between the three different intellectual capitals are explored and explained. The theory is illustrated based on an industrial case study, where it is shown how decisions made in industry practice are explainable with the formulated theory, and the consequences of the decisions are made explicit. Based on the positive results, it is concluded that the theory may have a good explanatory power, although more evaluations are needed. ©2015TheAuthors.PublishedbyElsevierInc.ThisisanopenaccessarticleundertheCCBY-NC-NDlicense.","Empirical; Intellectual capital; Software engineering theory"
"Cost optimization approaches for scientific workflow scheduling in cloud and grid computing: A review, classifications, and open issues","2016","Journal of Systems and Software","10.1016/j.jss.2015.11.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962339428&doi=10.1016%2fj.jss.2015.11.023&partnerID=40&md5=89418db0dc65ac340591ebfe04001d38","Workflow scheduling in scientific computing systems is one of the most challenging problems that focuses on satisfying user-defined quality of service requirements while minimizing the workflow execution cost. Several cost optimization approaches have been proposed to improve the economic aspect of Scientific Workflow Scheduling (SWFS) in cloud and grid computing. To date, the literature has not yet seen a comprehensive review that focuses on approaches for supporting cost optimization in the context of SWFS in cloud and grid computing. Furthermore, providing valuable guidelines and analysis to understand the cost optimization of SWFS approaches is not well-explored in the current literature. This paper aims to analyze the problem of cost optimization in SWFS by extensively surveying existing SWFS approaches in cloud and grid computing and provide a classification of cost optimization aspects and parameters of SWFS. Moreover, it provides a classification of cost based metrics that are categorized into monetary and temporal cost parameters based on various scheduling stages. We believe that our findings would help researchers and practitioners in selecting the most appropriate cost optimization approach considering identified aspects and parameters. In addition, we highlight potential future research directions in this on-going area of research. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Scheduling; Scientific workflow"
"DRE system performance optimization with the SMACK cache efficiency metric","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027930649&doi=10.1016%2fj.jss.2014.08.031&partnerID=40&md5=1055653ff88b4b05a1c3e420396532e8","System performance improvements are critical for the resource-limited environment of multiple integrated applications executing inside a single distributed real-time and embedded (DRE) system, such as integrated avionics platform or vehtronics systems. While processor caches can effectively reduce execution time there are several factors, such as cache size, system data sharing, and task execution schedule, which make it hard to quantify, predict, and optimize the cache usage of a DRE system. This article presents SMACK, a novel heuristic for estimating the hardware cache usage of a DRE system, and describes a method of varying the runtime behavior of DRE system software without (1) requiring extensive safety recertification or (2) violating the real-time scheduling deadlines. By using SMACK as a maximization target, we were able to reduce integrated DRE system execution time by an average of 2.4% and a maximum of 4.34%. © 2014 Elsevier Inc. All rights reserved.","DRE Deployment Optimization Heuristic Cache"
"Extracting REST resource models from procedure-oriented service interfaces","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919362638&doi=10.1016%2fj.jss.2014.10.038&partnerID=40&md5=b141a28a7a4cebaa84c51f23ab6bef55","During the past decade a number of procedure-oriented protocols and standards have emerged for making service-offering systems available on the Web. The WS-∗ stack of protocols is the most prevalent example. However, this procedure and message-oriented approach has not aligned with the true potential of the Web's own architectural principles, such as the uniform identification and manipulation of resources, caching, hypermedia, and layering. In this respect, Resource Oriented Architectures based on the REST architectural style, have been proposed as a possible alternative to the operation-based view of service offerings. To date, compiling a REST API for back-end procedure-oriented services is considered as a manual process that requires as input specialized models, such as, service requirements and behavioral models. In this paper, we propose a resource extraction method in which service descriptions are analyzed, using natural language processing techniques and graph transformations, in order to yield a collection of hierarchically organized elements forming REST resources that semantically correspond to the functionality offered by the service. The proposed approach has been applied as a proof of concept with positive results, for the extraction of resource models from a sizable number of procedure-oriented Web Service interfaces that have been obtained from an open service directory. © 2014 Elsevier Inc. All rights reserved.","REST; Service oriented architectures; Software reengineering"
"Enhanced fixed-priority real-time scheduling on multi-core platforms by exploiting task period relationship","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912529348&doi=10.1016%2fj.jss.2014.09.010&partnerID=40&md5=8ece7fa169fbdcc97762d63b18f72f97","One common approach for multi-core partitioned scheduling problem is to transform this problem into a traditional bin-packing problem, with the utilization of a task being the ""size"" of the object and the utilization bound of a processing core being the ""capacity"" of the bin. However, this approach ignores the fact that some implicit relations among tasks may significantly affect the feasibility of the tasks allocated to each local core. In this paper, we study the problem of partitioned scheduling of periodic real-time tasks on multi-core platforms under the Rate Monotonic Scheduling (RMS) policy. We present two effective and efficient partitioned scheduling algorithms, i.e. PSER and HAPS, by exploiting the fact that the utilization bound of a task set increases as task periods are closer to harmonic on a single-core platform. We formally prove the schedulability of our partitioned scheduling algorithms. Our extensive experimental results demonstrate that the proposed algorithms can significantly improve the scheduling performance compared with the existing work. © 2014 Elsevier Inc. All rights reserved.","Harmonic; Partitioned scheduling; RMS"
"Quality of service approaches in cloud computing: A systematic mapping study","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921631226&doi=10.1016%2fj.jss.2014.12.015&partnerID=40&md5=7b886517c0094f872885fd3f514f8318","Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area. Objective: The aim of this study is to survey current research on QoS approaches in cloud computing in order to identify where more emphasis should be placed in both current and future research directions. Method: A systematic mapping study was performed to find the related literature, and 67 articles were selected as primary studies that are classified in relation to the focus, research type and contribution type. Result: The majority of the articles are of the validation research type (64%). Infrastructure as a service (48%) was the largest research focus area, followed by software as a service (36%). The majority of contributions concerned methods (48%), followed by models (32%). Conclusion: The results of this study confirm that QoS approaches in cloud computing have become an important topic in the cloud computing area in recent years and there remain open challenges and gaps which require future research exploration. In particular, tools, metrics and evaluation research are needed in order to provide useful and trustworthy cloud computing services that deliver appropriate QoS. Crown Copyright © 2014 Published by Elsevier Inc. All rights reserved.","Cloud services; Quality of service; Systematic mapping study"
"Profiling and classifying the behavior of malicious codes","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919360062&doi=10.1016%2fj.jss.2014.10.031&partnerID=40&md5=85ba192d4cb4327175efc46b6247cf0b","Malware is a major security threat confronting computer systems and networks and has increased in scale and impact from the early days of ICT. Traditional protection mechanisms are largely incapable of dealing with the diversity and volume of malware variants which is evident today. This paper examines the evolution of malware including the nature of its activity and variants, and the implication of this for computer security industry practices. As a first step to address this challenge, I propose a framework to extract features statically and dynamically from malware that reflect the behavior of its code such as the Windows Application Programming Interface (API) calls. Similarity based mining and machine learning methods have been employed to profile and classify malware behaviors. This method is based on the sequences of API sequence calls and frequency of appearance. Experimental analysis results using large datasets show that the proposed method is effective in identifying known malware variants, and also classifies malware with high accuracy and low false alarm rates. This encouraging result indicates that classification is a viable approach for similarity detection to help detect malware. This work advances the detection of zero-day malware and offers researchers another method for understanding impact. © 2014 Elsevier Inc.","Cybercrime; Malware; Profiling"
"Using SAN formalism to evaluate Follow-The-Sun project scenarios","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919346835&doi=10.1016%2fj.jss.2014.10.046&partnerID=40&md5=20a9522ad71022a2b5a21b95d3ce09be","Performance evaluation of projects can be used by companies and institutions as a tool to help the decision making process of Follow-The-Sun (FTS) projects. This paper main goal is to discuss a stochastic model definition to evaluate the performance of different aspects of FTS projects. Examples that can be addressed using the FTS model are provided with results comparing different model instances to evaluate aspects such as project execution time and project costs composition. © 2014 Elsevier Inc. All rights reserved.","Follow-The-Sun; Projects simulation; Stochastic modeling"
"A time-based approach to automatic bug report assignment","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923211338&doi=10.1016%2fj.jss.2014.12.049&partnerID=40&md5=d215fbbbad652cba2e7125b90a3fd9df","Bug assignment is one of the important activities in bug triaging that aims to assign bugs to the appropriate developers for fixing. Many recommended automatic bug assignment approaches are based on text analysis methods such as machine learning and information retrieval methods. Most of these approaches use term-weighting techniques, such as term frequency-inverse document frequency (tf-idf), to determine the value of terms. However, the existing term-weighting techniques only deal with frequency of terms without considering the metadata associated with the terms that exist in software repositories. This paper aims to improve automatic bug assignment by using time-metadata in tf-idf (Time-tf-idf). In the Time-tf-idf technique, the recency of using the term by the developer is considered in determining the values of the developer expertise. An evaluation of the recommended automatic bug assignment approach that uses Time-tf-idf, called ABA-Time-tf-idf, was conducted on three open-source projects. The evaluation shows accuracy and mean reciprocal rank (MRR) improvements of up to 11.8% and 8.94%, respectively, in comparison to the use of tf-idf. Moreover, the ABA-Time-tf-idf approach outperforms the accuracy and MRR of commonly used approaches in automatic bug assignment by up to 45.52% and 55.54%, respectively. Consequently, consideration of time-metadata in term weighting reasonably leads to improvements in automatic bug assignment. © 2014 Elsevier Inc. All rights reserved.","Term weighting technique; Tf-idf Technique; time metadata"
"QoS prediction for dynamic reconfiguration of component based software systems","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923209804&doi=10.1016%2fj.jss.2014.12.001&partnerID=40&md5=529e2e707831e99ee0446055f68b13ca","It is difficult to choose the appropriate reconfiguration approach to satisfy the quality of service (QoS) requirements of a software system if the properties of that approach are not known. This problem significantly restricts the application of dynamic reconfiguration approaches to mission-critical or non-stop systems, where QoS is a major performance indicator. This paper proposes a model to predict how the QoS of a running software system will be affected by dynamic reconfiguration and show how it out-performed the existing methods in this area in three aspects. First, unlike existing simulation based models, this prediction model was based on easily implemented mathematical functions. Second, compared with the time-consuming simulation approaches, QoS prediction using this model was achieved in a shorter timeframe. Third, unlike the existing approaches that are built on different platforms for individual scenarios, this model generalized QoS prediction onto a single virtual platform that was modeled by abstract hardware and software conditions. The proposed model has been verified by reconfiguration simulation to a reasonable level of accuracy and thus the viability and safety for the use of the model has been confirmed. © 2015 Elsevier Inc. All rights reserved.","Dynamic reconfiguration; QoS assurance; QoS prediction"
"LAYER: A cost-efficient mechanism to support multi-tenant database as a service in cloud","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921689447&doi=10.1016%2fj.jss.2014.11.038&partnerID=40&md5=029391d97dae2e2ad1bb52fb0c248ec5","This paper presents a novel mechanism to cost-efficiently support multi-tenant database as a service (MTDBaaS) in cloud for small businesses. We aim at the scenarioswhere a large number of small tenants are served but only some of them are active simultaneously. By small tenants, we mean that a tenant may have many small-sized tables while only a small number of those tables are accessed concurrently for each query. As most MTDBaaS providers, we consolidate multiple tenants' data into the same database management system (DBMS) to reduce the cost of operation. However, our solution distinguishes itself from the existing solutions by a novelmechanism: Load As You quERy (LAYER in short). Concretely, tenants can define and create their own tables with LAYER, and set up possible reference constraints between any two tables. A shared table is used to store all data for all tenants, but only a moderate number of working tables are maintained for answering queries from active tenants. When a new query is submitted, tables involved in the query but not yet in the DBMS will be restored: tables are created, and data are loaded to these newly-created tables. If an active tenant becomes inactive (logs out or no query is issued in a specified time period), tables belonging to the tenant could be dropped when necessary, and updates to these tables would be mirrored to the shared table for backup. We provide two implementations of the LAYER mechanism, one is LAYER-MySQL, which is based on the traditional disk-based relational DBMS MySQL, and can yield high consolidation and acceptable performance; the other is LAYER-VoltDB, which is based on the in-memory relational DBMS VoltDB, and can provide much higher performance. Experimental results validate the feasibility of the proposed mechanism. © 2014 Elsevier Inc. All rights reserved.","Cloud computing; Database-as-a-Service; Multi-tenancy"
"An insight into license tools for open source software systems","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923206097&doi=10.1016%2fj.jss.2014.12.050&partnerID=40&md5=c830da504c4358ecb8df3ae0102d1957","Free/Libre/Open Source Software (FLOSS) has gained a lot of attention lately allowing organizations to incorporate third party source code into their implementations. When open source software libraries are used, software resources may be linked directly or indirectly with multiple open source licenses giving rise to potential license incompatibilities. Adequate support in license use is vital in order to avoid such violations and address how diverse licenses should be handled. In the current work we investigate software licensing giving a critical and comparative overview of existing assistive approaches and tools. These approaches are centered on three main categories: license information identification from source code and binaries, software metadata stored in code repositories, and license modeling and associated reasoning actions. We also give a formalization of the license compatibility problem and demonstrate the role of existing approaches in license use decisions. © 2014 Elsevier Inc. All rights reserved.","Free/Libre/Open Source Software; License compatibility; License identification"
"Power-aware scheduling of compositional real-time frameworks","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923169380&doi=10.1016%2fj.jss.2014.12.031&partnerID=40&md5=57a05e75749a9e26c54e5fe7a17240bf","The energy consumption problem has become a great challenge in all computing areas from modern handheld devices to large data centers. Dynamic voltage scaling (DVS) is widely used as mean to reduce the energy consumption of computer systems by lowering whenever possible the voltage and operating frequency of processors. Unfortunately, existing compositional real-time scheduling frameworks have been focusing only on efficient scheduling of tasks inside their components given a resource model, providing no interest on power/energy consumption. In this paper, we define the real-time DVS problem for a compositional scheduling framework. Considering the periodic resource model, we propose optimal static DVS schemes at system, component, and task levels. We also introduce component and task level dynamic DVS schemes that take advantage of runtime unused slack times and resource availability to provide even better energy savings. Finally, we provide power-aware schedulability conditions to guarantee the feasibility of each component under DVS for the Earliest Deadline First and the Rate Monotonic scheduling algorithms. Through simulations, we showed that our schemes can reduce the energy consumption of a component by up to 96%. © 2015 Elsevier Inc. All rights reserved.","Compositional and hierarchical real-time scheduling; Periodic resource model; Periodic task model; Power-aware scheduling"
"A practical approach to the assessment of quality in use of corporate web sites","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912526468&doi=10.1016%2fj.jss.2014.09.006&partnerID=40&md5=1433dde6461215ddae35269390a6aaa0","The paper presents a practical approach to web site quality, based on a novel perspective that considers the relationships between the web site and its stakeholders. This perspective leads to identify four fundamental concepts of quality: final quality, quality in use, basic quality and internal quality. This paper focuses on quality in use, and proposes a new quality model including a well structured and balanced set of characteristics and sub-characteristics, which aim at capturing the main dimensions that impact on the quality of a web site. The distinction between actual and expected quality is then introduced and a practical assessment methodology for expected quality (EQ-EVAL) is proposed, which employs expert evaluators instead of actual users in order to make the evaluation less expensive, without sacrificing, however, accuracy and reliability. The results of the application of the methodology in the evaluation of a sample set of corporate web sites are finally discussed, showing how the model and the methodology can indeed meet the stated requirements. © 2014 Elsevier Inc. All rights reserved.","Quality assessment; Quality model; Web site quality"
"Software cost estimating for CMMI Level 5 developers","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929176084&doi=10.1016%2fj.jss.2015.03.069&partnerID=40&md5=5009ba67e4d20fb92ad2e01355bbbb28","This article provides analysis results of Capability Maturity Model Integrated Level 5 projects for developers earning the highest level possible, using actual software data from their initial project estimates. Since there were no measures to verify software performance, this level was used a proxy for high quality software. Ordinary least squares regression was used to predict final effort hours with initially estimated variables obviates the need to estimate growth or shrinkage for typical changes occurring in software projects, regardless of software developer (contracted or in-house). The OLS equations, or cost estimating relationship equations, were evaluated by a series of standards: statistical significance, visual inspection, goodness of fit measures, and academically set thresholds for accuracy measures used in software cost estimating: mean magnitude of relative error and prediction (for determining the percentage of records with 25%, or less, based on their magnitude of relative error score). As several initial estimated variables were strongly correlated to the reported final effort hours and each other, each variable was examined separately. Thirty records from software projects completed in 2003-2008 for the highest process maturity level were used to compute statistically significant equations with implicit growth or shrinkage in their make-up.","CMMI Level 5; Ordinary least squares regression; Software effort estimation; US DOD"
"An effective and economical architecture for semantic-based heterogeneous multimedia big data retrieval","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923226318&doi=10.1016%2fj.jss.2014.09.016&partnerID=40&md5=8f1a324e83953f537aebd4ff434d0322","Data variety has been one of the most critical features for multimedia big data. Some multimedia documents, although in different data formats and storage structures, often express similar semantic information. Therefore, the way to manage and retrieve multimedia documents reflecting users' intent in heterogeneous big data environments has become an important issue. In this paper, we present an effective and economical architecture named SHMR (Semantic-based Heterogeneous Multimedia Retrieval), which uses low cost to store and retrieve semantic information from heterogeneous multimedia data. Firstly, the particularity of heterogeneous multimedia retrieval in big data environments is addressed. Secondly, an approach to extract and represent semantic information for heterogeneous multimedia documents is proposed. Thirdly, a NoSQL-based approach to semantic storage, in which multimedia can be parallel processed in distributed nodes is provided. Finally, a MapReduce-based retrieval algorithm is presented and a user feedback supported scheme to achieve high retrieval precision and good user experience is designed. The experimental results indicate that the retrieval performance and economic efficiency of SHMR are suitable for multimedia information retrieval in heterogeneous big data environments. © 2014 Elsevier Inc. All rights reserved.","Big data; Heterogeneous multimedia; Semantic based retrieval"
"MostoDEx: A tool to exchange RDF data using exchange samples","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919451182&doi=10.1016%2fj.jss.2014.10.033&partnerID=40&md5=bbf08711a591b6259e9be6bcbb29ab69","The Web is evolving into a Web of Data in which RDF data are becoming pervasive, and it is organised into datasets that share a common purpose but have been developed in isolation. This motivates the need to devise complex integration tasks, which are usually performed using schema mappings; generating them automatically is appealing to relieve users from the burden of handcrafting them. Many tools are based on the data models to be integrated: classes, properties, and constraints. Unfortunately, many data models in the Web of Data comprise very few or no constraints at all, so relying on constraints to generate schema mappings is not appealing. Other tools rely on handcrafting the schema mappings, which is not appealing at all. A few other tools rely on exchange samples but require user intervention, or are hybrid and require constraints to be available. In this article, we present MostoDEx, a tool to generate schema mappings between two RDF datasets. It uses a single exchange sample and a set of correspondences, but does not require any constraints to be available or any user intervention. We validated and evaluated MostoDEx using many experiments that prove its effectiveness and efficiency in practice. © 2014 Elsevier Inc. All rights reserved.","Data exchange; RDF; Schema mappings"
"Integrating mixed transmission and practical limitations with the worst-case response-time analysis for Controller Area Network","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912523115&doi=10.1016%2fj.jss.2014.09.005&partnerID=40&md5=1585a01d226c1030359c13809b129b00","The existing worst-case response-time analysis for Controller Area Network (CAN) calculates upper bounds on the response times of messages that are queued for transmission either periodically or sporadically. However, it does not support the analysis of mixed messages. These messages do not exhibit a periodic activation pattern and can be queued for transmission both periodically and sporadically. They are implemented by several higher-level protocols based on CAN that are used in the automotive industry. We extend the existing analysis to support worst-case response-time calculations for periodic and sporadic as well as mixed messages. Moreover, we integrate the effect of hardware and software limitations in the CAN controllers and device drivers such as abortable and non-abortable transmit buffers with the extended analysis. The extended analysis is applicable to any higher-level protocol for CAN that uses periodic, sporadic and mixed transmission modes. © 2014 Elsevier Inc. All rights reserved.","Controller Area Network; Real-time network; Response-time analysis"
"Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930795477&doi=10.1016%2fj.jss.2015.05.006&partnerID=40&md5=d3a7c41e22f6cad85abd068fc4b4feaf","Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies. © 2015 Elsevier Inc. All rights reserved.","Feature extractions; Natural language requirements; Requirements reuse; Software product lines; Systematic literature review"
"Automated fault localization via hierarchical multiple predicate switching","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927564140&doi=10.1016%2fj.jss.2015.02.038&partnerID=40&md5=e90a67b096d1dece8a9ab7bf0ff416c0","Single predicate switching forcibly changes the state of a predicate instance at runtime and then identifies the root cause by examining the switched predicate, called critical predicate. However, switching one predicate instance has its limitations: in our experiments, we found that single predicate switching can only find critical predicates for 88 out of 300 common bugs in five real-life utility programs. For other 212 bugs, overcoming them may require switching multiple predicate instances. Nonetheless, taking all possible combinations of predicate instances into consideration will result in exponential explosion. Therefore, we propose a hierarchical multiple predicate switching technique, called HMPS, to locate faults effectively. Specifically, HMPS restricts the search for critical predicates to the scope of highly suspect functions identified by employing spectrum-based fault localization techniques. Besides, instrumentation methods and strategies for switch combination are presented to facilitate the search for critical predicates. The empirical studies show that HMPS is able to find critical predicates for 111 out of 212 bugs mentioned above through switching multiple predicate instances. In addition, HMPS captures 62% of these 300 bugs when examining up to 1% of the executed code, while the Barinel and Ochiai approaches locate 18% and 16% respectively. © 2015 Elsevier Inc. All rights reserved.","Fault localization; Multiple predicate switching; Predicate"
"Capturing urgency and parallelism using quasi-deadlines for real-time multiprocessor scheduling","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921656657&doi=10.1016%2fj.jss.2014.11.019&partnerID=40&md5=4e2abf327331811c7e3106a1e92398a6","Recent trends toward multi-core architectures in real-time embedded systems pose challenges in designing efficient real-time multiprocessor scheduling algorithms. We believe that it is important to take into consideration both timing constraints of tasks (urgency) and parallelism restrictions of multiprocessor platforms (parallelism) together when designing scheduling algorithms. Motivated by this, we define the quasi-deadline of a job as a weighted sum of its absolute deadline (capturing urgency) and its worst case execution time (capturing parallelism) with a system-level control knob to balance urgency and parallelism effectively. Using the quasi-deadline to prioritize jobs, we propose two new scheduling algorithms, called EQDF (earliest quasi-deadline first) and EQDZL (earliest quasi-deadline until zero laxity), that are categorized into joblevel fixed-priority (JFP) scheduling and job-level dynamic-priority (JDP) scheduling, respectively. This paper provides a new schedulability analysis for EQDF/EQDZL scheduling and addresses the problem of priority assignment under EQDF/EQDZL by determining a right value of the system-level control knob. It presents optimal and heuristic solutions to the problem subject to our proposed EQDF and EQDZL analysis. Our simulation results show that EQDF and EQDZL can improve schedulability significantly compared to EDF and EDZL, respectively. © 2014 Elsevier Inc. All rights reserved.","Multiprocessor scheduling; Quasi-deadline; Real-time systems"
"Measuring the veracity of web event via uncertainty","2015","Journal of Systems and Software","10.1016/j.jss.2014.07.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027957797&doi=10.1016%2fj.jss.2014.07.023&partnerID=40&md5=5d90daa38415ee2b2809ceef2c9c74eb","Web events, whose data occur as one kind of big data, have attracted considerable interests during the past years. However, most existing related works fail to measure the veracity of web events. In this research, we propose an approach to measure the veracity of web event via its uncertainty based on its features distribution on different kind of confident websites. Firstly, the proposed approach mines various event features from the data of web event which may influence on the measuring process of uncertainty. Secondly, one computational model is introduced to simulate the influence process of the above features on the evolution process of web event. Thirdly, matrix operations are managed to facilitate practice. Finally, experiments are made based on the analysis above, and the results proved that the proposed uncertainty measuring algorithm is promising to measure the veracity of web event for big data. © 2014 Elsevier Inc. All rights reserved.","Big data; Topic detection and tracking; Web event veracity"
"Service-oriented approach to fault tolerance in CPSs","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929172836&doi=10.1016%2fj.jss.2015.03.041&partnerID=40&md5=ec14ea2276a5e405dcc0b26bfca51501","Cyber-physical systems (CPSs) are open and interconnected embedded systems that control or interact with physical processes. Failures in CPSs can lead to loss of production time, damage to the equipment and environment, or loss of life, meaning that dependability and resilience are key properties for their design. However, existing fault tolerance and safety approaches are inadequate for complex, networked and dynamic CPSs. Service-orientation, on the other hand, is generally considered to be a robust architectural style, but there is a limited amount of research on fault tolerance of service-oriented architecture (SOA), especially on distributed real-time systems. We propose an approach that utilizes the loosely coupled nature of services to implement fault tolerance using a middleware-based real-time SOA (RTSOA) for CPSs. The approach, based on the concepts of fault isolation and recovery at the service level, is empirically evaluated using a demanding bilateral teleoperation (remote handling) application. The empirical evaluation demonstrates that RTSOA supports real-time fault detection and recovery, use of services as a unit of fault isolation, and it provides capability to implement fault tolerance patterns flexibly and without significant overhead.","Dependability; Resilience; RTSOA"
"Progressive Outcomes: A framework for maturing in agile software development","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923197078&doi=10.1016%2fj.jss.2014.12.032&partnerID=40&md5=574673a0f083df24323f6b40c18e6c7e","Maturity models are used to guide improvements in the software engineering field and a number of maturity models for agile methods have been proposed in the last years. These models differ in their underlying structure prescribing different possible paths to maturity in agile software development, neglecting the fact that agile teams struggle to follow prescribed processes and practices. Our objective, therefore, was to empirically investigate how agile teams evolve to maturity, as a means to conceive a theory for agile software development evolvement that considers agile teams nature. The complex adaptive systems theory was used as a lens for analysis and four case studies were conducted to collect qualitative and quantitative data. As a result, we propose the Progressive Outcomes framework to describe the agile software development maturing process. It is a framework in which people have the central role, ambidexterity is a key ability to maturity, and improvement is guided by outcomes agile teams pursue, instead of prescribed practices. © 2015 Elsevier Inc. All rights reserved.","Agile software development; Ambidexterity; Complex adaptive systems; Maturity; Software process improvement"
"A semantic approach for designing Assistive Software Recommender systems","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927564307&doi=10.1016%2fj.jss.2015.03.009&partnerID=40&md5=3ec47c03137768757fb87c9ee1845abd","Assistive Software offers a solution for people with disabilities to manage specialized hardware, devices or services. However, these users may have difficulties in selecting and installing Assistive Software in their devices for managing smart environments. This paper addresses the requirements of these kinds of systems and their design in the context of interoperability architectures. Our solution follows a semantic approach, for which ontologies are a key. The paper also presents an implementation of our design proposal, i.e., a real and usable system which is evaluated according to a set of functional and non-functional requirements here proposed. © 2015 Elsevier Inc. All rights reserved.","Assistive Software; Software design; Software non-functional evaluation"
"Improving multi-objective code-smells correction using development history","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929192035&doi=10.1016%2fj.jss.2015.03.040&partnerID=40&md5=811dafb9849eb14a65c1013b1567c391","One of the widely used techniques to improve the quality of software systems is refactoring. Software refactoring improves the internal structure of the system while preserving its external behavior. These two concerns drive the existing approaches to refactoring automation. However, recent studies demonstrated that these concerns are not enough to produce correct and consistent refactoring solutions. In addition to quality improvement and behavior preservation, studies consider, among others, construct semantics preservation and minimization of changes. From another perspective, development history was proven as a powerful source of knowledge in many maintenance tasks. Still, development history is not widely explored in the context of automated software refactoring. In this paper, we use the development history collected from existing software projects to propose new refactoring solutions taking into account context similarity with situations seen in the past. We propose a multi-objective optimization-based approach to find good refactoring sequences that (1) minimize the number of code-smells, and (2) maximize the use of development history while (3) preserving the construct semantics. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-offs between these three objectives. We evaluate our approach using a benchmark composed of five medium and large-size open-source systems and four types of code-smells (Blob, spaghetti code, functional decomposition, and data class). Our experimental results show the effectiveness of our approach, compared to three different state-of-the-art approaches, with more than 85% of code-smells fixed and 86% of suggested refactorings semantically coherent when the change history is used.","Code-smells; Refactoring; Search-based software engineering"
"Network coding-based energy-efficient multicast routing algorithm for multi-hop wireless networks","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927510924&doi=10.1016%2fj.jss.2015.03.006&partnerID=40&md5=b837ba8427d6efb27d8a5e7ba65fa3ed","Multi-hop multicast routing can provide better communication performance in multi-hop wireless networks. However, existing multi-hop multicast routing hardly take into account energy efficiency of networks. This paper studies the energy-efficient multicast communication aiming at multi-hop wireless networks. Firstly, we analyze energy metric and energy efficiency metric of multi-hop networks. Then the corresponding models are given. Secondly, network coding is used to improve network throughput. Different from previous methods, we here consider that network nodes are satisfied with a certain random distribution. In such a case, it is a challenge to construct the network structure that network coding requires. For the above random network topology, we propose three basic structures of network coding to overcome this problem. Thirdly, we present a flexible energy-efficient multicast routing algorithm for multi-hop wireless networks to extensively exploit the network structure proposed above to maximize network throughput and decrease network energy consumption. Finally, we perform numerical experiments by network simulation. Simulation results indicate that our approach is significantly promising. © 2015 Elsevier Inc. All rights reserved.","Energy efficiency; Multi-hop multicast; Network coding"
"An exploratory study on exception handling bugs in Java programs","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930795664&doi=10.1016%2fj.jss.2015.04.066&partnerID=40&md5=0c744f837440bc050999210f0e87e0ef","Abstract Most mainstream programming languages provide constructs to throw and to handle exceptions. However, several studies argue that exception handling code is usually of poor quality and that it is commonly neglected by developers. Moreover, it is said to be the least understood, documented, and tested part of the implementation of a system. Nevertheless, there are very few studies that analyze the actual exception handling bugs that occur in real software systems or that attempt to understand developers' perceptions of these bugs. In this work we present an exploratory study on exception handling bugs that employs two complementary approaches: a survey of 154 developers and an analysis of 220 exception handling bugs from the repositories of Eclipse and Tomcat. Only 27% of the respondents claimed that policies and standards for the implementation of error handling are part of the culture of their organizations. Moreover, in 70% of the organizations there are no specific tests for the exception handling code. Also, 61% of the respondents stated that no to little importance is given to the documentation of exception handling in the design phase of the projects with which they are involved. In addition, about 40% of the respondents consider the quality of exception handling code to be either good or very good and only 14% of the respondents consider it to be bad or very bad. Furthermore, the repository analysis has shown (with statistical significance) that exception handling bugs are ignored by developers less often than other bugs. We have also observed that while overly general catch blocks are a well-known bad smell related to exceptions, bugs stemming from these catch blocks are rare, even though many overly general catch blocks occur in the code. Furthermore, while developers often mention empty catch blocks as causes of bugs they have fixed in the past, we found very few bug reports caused by them. On top of that, empty catch blocks are frequently used as part of bug fixes, including fixes for exception handling bugs. Based on our findings, we propose a classification of exception handling bugs and their causes. The proposed classification can be used to assist in the design and implementation of test suites, to guide code inspections, or as a basis for static analysis tools. © 2015 Elsevier Inc. All rights reserved.","Bugs; Exception handling; Repository mining"
"Enhancing a model-based engineering approach for distributed manufacturing automation systems with characteristics and design patterns","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921631007&doi=10.1016%2fj.jss.2014.12.028&partnerID=40&md5=932f79afb3ddb3d743072fe9fe33b0d3","Recent trends in modern manufacturing, such as the growing need for flexibility and the increasing degree of automation in industrial facilities, require distributed control solutions. Implementations of such control schemas and underlying architectures come along with an exponential increase of the automation system's complexity. Therefore, methods for supporting automation engineers during the development processes are highly required. This paper presents an approach to supportingmodel-based engineering (MBE) of distributed manufacturing automation systems. The approach is based on the combination of notation, characteristics, and design patterns across multiple levels of an adapted development process. Accordingly, a prototypical support tool has been implemented. The modeling approach has been evaluated by case studies and additional usability experiments to determine the benefit of its application within the design of manufacturing automation systems. © 2014 Elsevier Inc. All rights reserved.","Distributed systems; Manufacturing automation system; Model-based engineering"
"An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921634197&doi=10.1016%2fj.jss.2014.11.041&partnerID=40&md5=f3f18f4b94dbac6b69bbd661938052a6","Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessmentmethods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience. © 2014 Elsevier Inc. All rights reserved.","Assessment method design; Software process assessment; Systematic literature review"
"Web API growing pains: Loosely coupled yet strongly tied","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919339677&doi=10.1016%2fj.jss.2014.10.014&partnerID=40&md5=ea547bb737ddad18f69b4553bf5c9ddb","Web APIs provide a systematic and extensible approach for application-to-application interaction. Developers using web APIs are forced to accompany the API providers in their software evolution tasks. In order to understand the distress caused by this imposition on web API client developers we perform a semi-structured interview with six such developers. We also investigate how major web API providers organize their API evolution, and we explore how this affects source code changes of their clients. Our exploratory qualitative study of the Twitter, Google Maps, Facebook and Netflix web APIs analyzes the state of web API evolution practices and provides insight into the impact of service evolution on client software. In order to complement the picture and also understand how web API providers deal with evolution, we investigate the server-side and client-side evolution of two open-source web APIs, namely VirtualBox and XBMC. Our study is complemented with a set of observations regarding best practices for web API evolution. © 2014 Elsevier Inc. All rights reserved.","API; Breaking changes; Software evolution; Web"
"QualityScan scheme for load balancing efficiency in vehicular ad hoc networks (VANETs)","2015","Journal of Systems and Software","10.1016/j.jss.2015.01.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927522719&doi=10.1016%2fj.jss.2015.01.052&partnerID=40&md5=13048efc83abf8d8e8e6445163ea4333","The main terminal devices in vehicular ad-hoc networks (VANETs) are highly mobile moving cars that handoff much more frequently than handheld devices. Nevertheless, frequent handoff or high handoff latency can influence the quality of service (QoS) of real-time network services. Since conventional handoff mechanisms cannot fulfill the requirements of VANET, many fast handoff schemes have been proposed. However, the schemes based on the signal strength of APs (access points) ignore the loading states of different APs and thus cannot utilize the bandwidth effectively. Whenever some APs are very busy, the QoS will be degraded. In order to solve this problem, we can pre-establish the APs and regulate their number according to different traffic types. In this paper, we present a fast handoff scheme for VANET, QualityScan, which decreases the handoff latency and considers loading states of the regional APs simultaneously. By the pre-established AP controller (APC), our scheme gathers the loading states of the APs regularly and predicts network traffic of the next moment. Based on the parameters obtained by passive scanning, the mobile nodes (MNs) can choose the optimal AP for the optimal QoS. According to our simulation analysis, QualityScan not only achieves load balance of the APs, but also improves QoS and handoff efficiency in VANET. © 2015 Elsevier Inc. All rights reserved.","Load balance index; QualityScan; VANET"
"Adaptive thermal-aware task scheduling for multi-core systems","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912121170&doi=10.1016%2fj.jss.2014.09.037&partnerID=40&md5=d0c1f196cf261578079b611b2f920254","Thermal management is a challenging problem because of on-line thermal dynamics. An adaptive thermal-aware multi-core task scheduling framework based on run-time controllers is proposed in this paper to address the inter-core thermal effects and dynamic variations of task execution. In contrast to dynamic voltage scaling, the service rates of the tasks are adjusted in order to cool the system. Scheduling algorithms are used to prevent the system from overheating and maximize system utilization. This paper also evaluates the capability of the proposed framework through varying the workloads, ultimately demonstrating positive and stable performance. © 2014 Elsevier Inc. All rights reserved.","Multi-core system; Task scheduling; Thermal-aware"
"A computer system architecture providing a user-friendly man machine interface for accessing assistive technology in cloud computing","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919346819&doi=10.1016%2fj.jss.2014.10.035&partnerID=40&md5=fe0e8478bfb5e5acfeb95dd3fac3b443","Assistive Technology (AT) includes hardware peripherals, software applications and systems that enable a user with a disability to use a PC. Thus, when a disabled user needs to work in a particular environment (e.g., at work, at school, in a government office, etc.) he/she has to properly configure the used PC. However, often, the configuration of AT software interfaces is not trivial at all. This paper presents the software design, implementation, and evaluation of a computer system architecture providing a software user-friendly man machine interface for accessing AT software in cloud computing. The main objective of such an architecture is to provide a new type of software human-computer interaction for accessing AT services over the cloud. Thus, end users can interact with their personalized computer environments using any physical networked PC. The advantage of this approach is that users do not have to install and/or setup any additional software on physical PCs and they can access their own AT virtual environments from everywhere. In particular, the usability of prototype based on the Remote Desktop Protocol (RDP) is evaluated in both private and public cloud scenarios. © 2014 Elsevier Inc. All rights reserved.","Assistive technology; Cloud computing; Human-computer interaction"
"Test data generation with a Kalman filter-based adaptive genetic algorithm","2015","Journal of Systems and Software","10.1016/jjss.2014.11.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928042745&doi=10.1016%2fjjss.2014.11.035&partnerID=40&md5=44f41fada0c640c09058e265389a0780","Software testing is a crucial part of software development. It enables quality assurance, such as correctness, completeness and high reliability of the software systems. Current state-of-the-art software testing techniques employ search-based optimisation methods, such as genetic algorithms to handle the difficult and laborious task of test data generation. Despite their general applicability, genetic algorithms have to be parameterised in order to produce results of high quality. Different parameter values may be optimal for different problems and even different problem instances. In this work, we introduce a new approach for generating test data, based on adaptive optimisation. The adaptive optimisation framework uses feedback from the optimisation process to adjust parameter values of a genetic algorithm during the search. Our approach is compared to a state of the art test data optimisation algorithm that does not adapt parameter values online, and a representative adaptive optimisation algorithm, outperforming both methods in a wide range of problems. © 2014 Elsevier Inc. All rights reserved.","Adaptive parameter control; Optimisation; Test data generation"
"HaoLap: A Hadoop based OLAP system for big data","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923220637&doi=10.1016%2fj.jss.2014.09.024&partnerID=40&md5=70ba77dd5ae5e8dbb326b014ed4d9d6a","In recent years, facing information explosion, industry and academia have adopted distributed file system and MapReduce programming model to address new challenges the big data has brought. Based on these technologies, this paper presents HaoLap (Hadoop based oLap), an OLAP (OnLine Analytical Processing) system for big data. Drawing on the experience of Multidimensional OLAP (MOLAP), HaoLap adopts the specified multidimensional model to map the dimensions and the measures; the dimension coding and traverse algorithm to achieve the roll up operation on dimension hierarchy; the partition and linearization algorithm to store dimensions and measures; the chunk selection algorithm to optimize OLAP performance; and MapReduce to execute OLAP. The paper illustrates the key techniques of HaoLap including system architecture, dimension definition, dimension coding and traversing, partition, data storage, OLAP and data loading algorithm. We evaluated HaoLap on a real application and compared it with Hive, HadoopDB, HBaseLattice, and Olap4Cloud. The experiment results show that HaoLap boost the efficiency of data loading, and has a great advantage in the OLAP performance of the data set size and query complexity, and meanwhile HaoLap also completely support dimension operations. © 2014 Elsevier Inc. All rights reserved.","Cloud data warehouse; MapReduce; Multidimensional data model"
"Towards energy-efficient scheduling for real-time tasks under uncertain cloud computing environment","2015","Journal of Systems and Software","10.1016/j.jss.2014.08.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912521798&doi=10.1016%2fj.jss.2014.08.065&partnerID=40&md5=aab5435664249a3d4095bcc37740e08a","Green cloud computing has become a major concern in both industry and academia, and efficient scheduling approaches show promising ways to reduce the energy consumption of cloud computing platforms while guaranteeing QoS requirements of tasks. Existing scheduling approaches are inadequate for real-time tasks running in uncertain cloud environments, because those approaches assume that cloud computing environments are deterministic and pre-computed schedule decisions will be statically followed during schedule execution. In this paper, we address this issue. We introduce an interval number theory to describe the uncertainty of the computing environment and a scheduling architecture to mitigate the impact of uncertainty on the task scheduling quality for a cloud data center. Based on this architecture, we present a novel scheduling algorithm (PRS1) that dynamically exploits proactive and reactive scheduling methods, for scheduling real-time, aperiodic, independent tasks. To improve energy efficiency, we propose three strategies to scale up and down the system's computing resources according to workload to improve resource utilization and to reduce energy consumption for the cloud data center. We conduct extensive experiments to compare PRS with four typical baseline scheduling algorithms. The experimental results show that PRS performs better than those algorithms, and can effectively improve the performance of a cloud data center. © 2014 Elsevier Inc. All rights reserved.","Green cloud computing; Proactive and reactive; Uncertain scheduling"
"Scheduling parallel jobs with tentative runs and consolidation in the cloud","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927511326&doi=10.1016%2fj.jss.2015.03.007&partnerID=40&md5=04cbc902ab5dbcb69c2f971ffaf3e893","Since the success of cloud computing, more and more high performance computing parallel applications run in the cloud. Carefully scheduling parallel jobs is essential for cloud providers to maintain their quality of service. Existing parallel job scheduling mechanisms do not take the parallel workload consolidation into account to improve the scheduling performance. In this paper, after introducing a prioritized two-tier virtual machines architecture for parallel workload consolidation, we propose a consolidation-based parallel job scheduling algorithm. The algorithm employs tentative run and workload consolidation under such a two-tier virtual machines architecture to enhance the popular FCFS algorithm. Extensive experiments on well-known traces show that our algorithm significantly outperforms FCFS, and it can even produce comparable performance to the runtime-estimation-based EASY algorithm, though it does not require users to provide runtime estimation of the job. Moreover, our algorithm allows inaccurate CPU usage estimation and only requires trivial modification on FCFS. It is effective and robust for scheduling parallel workload in the cloud. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Parallel job scheduling; Workload consolidation"
"Hindering data theft with encrypted data trees","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921635792&doi=10.1016%2fj.jss.2014.11.050&partnerID=40&md5=8b2a2071015d0e49f6d696dea85da7a7","Data theft is a major threat formodern organizations with potentially large economic consequences. Although these attacks may well originate outside an organization's information systems, the attacker-or else an insider-must eventually make contact with the system where the information resides and extract it. In this work, we propose a scheme that hinders unauthorized data extraction by modifying the basic file system primitives used to access files. Intuitively, our proposal emulates the chains used to protect valuable items in certain clothing shopping centers, where shoplifting is prevented by forcing the thief to steal the whole rack of items. We achieve this by encrypting sensitive files using nonces (i.e., pseudorandom numbers used only once) as keys. Such nonces are available, also in encrypted form, in other objects of the file system. The system globally resembles a distributed Merkle hash tree, in such a way that getting access to a file requires previous access to a number of other files. This forces any potential attacker to extract not only the targeted sensitive information, but also all the files chained to it that are necessary to compute the associated key. Furthermore, our scheme incorporates a probabilistic rekeying mechanism to limit the damage that might be caused by patient extractors. We report experimental results measuring the time overhead introduced by our proposal and compare it with the effort an attacker would need to successfully extract information from the system. Our results show that the scheme increases substantially the effort required by an insider, while the introduced overhead is feasible for standard computing platforms. © 2014 Elsevier Inc. All rights reserved.","Data leakage prevention; Information theft; Insiders"
"Iterated local search for microaggregation","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919449043&doi=10.1016%2fj.jss.2014.10.012&partnerID=40&md5=e7fc83f1ac677ee1a4d35cdd27dd4927","Microaggregation is a disclosure control method used to protect microdata. We introduce a local search method and employ it in an iterated local search algorithm for the NP-hard minimum information loss microaggregation problem. Experimental results with benchmark data sets demonstrate that our algorithm consistently identifies better quality solutions than extant microaggregation methods. © 2014 Elsevier Inc.","Iterated local search; Microaggregation; Microdata protection"
"A large-scale study on the usage of Java's concurrent programming constructs","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930813372&doi=10.1016%2fj.jss.2015.04.064&partnerID=40&md5=f4ca58de31cec3b3e33b825ceee3988c","Abstract In both academia and industry, there is a strong belief that multicore technology will radically change the way software is built. However, little is known about the current state of use of concurrent programming constructs. In this work we present an empirical work aimed at studying the usage of concurrent programming constructs of 2227 real world, stable and mature Java projects from SourceForge. We have studied the usage of concurrent techniques in the most recent versions of these applications and also how usage has evolved along time. The main findings of our study are: (I) More than 75% of the latest versions of the projects either explicitly create threads or employ some concurrency control mechanism. (II) More than half of these projects exhibit at least 47 synchronized methods and 3 implementations of the Runnable interface per 100,000 LoC, which means that not only concurrent programming constructs are used often but they are also employed intensively. (III) The adoption of the java.util.concurrent library is only moderate (approximately 23% of the concurrent projects employ it). (IV) Efficient and thread-safe data structures, such as ConcurrentHashMap, are not yet widely used, despite the fact that they present numerous advantages. © 2015 Elsevier Inc.","Concurrency; Java; Software evolution"
"On building a consistent framework for executable systems architecture","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908310636&doi=10.1016%2fj.jss.2014.08.049&partnerID=40&md5=2affd08f3591090e30a9dfa084c6cc40","The paper presents a framework for executable systems architecture. Termed as Consistent Systems Architecture Description and Behavior Framework (CSADBF), the framework shows how consistency can be maintained while modeling architectural description of systems as well as their behavior. Convergence of three established modeling techniques: ontology, UML, and Colored Petri Nets (CPN), is used to develop this framework. Each tool complements others in accomplishing the goal of consistency maintenance for the executable systems architecture. The framework suggests various mapping schemes that help in establishing strong concordance among different artifacts of these modeling techniques and maintaining consistency of overall system architecture. The first scheme maps OWL ontology to UML and is responsible for maintaining consistency of the architectural description. The second scheme maps combination of OWL ontology and UML to CPN and is responsible for maintaining consistency between static and dynamic views. The third scheme ensures the behavioral consistency of the architecture by providing mapping between Semantic Web Rule Language (SWRL) and CPN Guard conditions. Thus, the framework allows architects to model the systems architecture requirements in OWL ontology and UML and to analyze the behavior and performance of systems architecture in CPN. The paper demonstrates the framework with the help of a case study and also compares it with the existing frameworks. © 2014 Elsevier Inc. All rights reserved.","Systems engineering Executable systems architecture Colored Petri Nets"
"PROW: A pairwise algorithm with constraints, order and weight","2015","Journal of Systems and Software","10.1016/j.jss.2014.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912078290&doi=10.1016%2fj.jss.2014.08.005&partnerID=40&md5=6d426d811b5ca20a0c87491691eed750","Testing systems with many variables and/or values is often quite expensive due to the huge number of possible combinations to be tested. There are several criteria available to combine test data and produce scalable test suites. One of them is pairwise. With the pairwise criterion, each pair of values of any two parameters is included in at least one test case. Although this is a widely-used coverage criterion, two main characteristics improve considerably pairwise: constraints handling and prioritisation. This paper presents an algorithm and a tool. The algorithm (called PROW: Pairwise with constRaints, Order and Weight) handles constraints and prioritisation for pairwise coverage. The tool called CTWeb adds functionalities to execute PROW in different contexts, one of them is product sampling in Software Product Lines via importing feature models. Software Product Line (SPL) development is a recent paradigm, where a family of software systems is constructed by means of the reuse of a set of common functionalities and some variable functionalities. An essential artefact of a SPL is the feature model, which shows the features offered by the product line, jointly with the relationships (includes and excludes) among them. Pairwise testing could be used to obtain the product sampling to test in a SPL, using features as pairwise parameters. In this context, the constraint handling becomes essential. As a difference with respect to other tools, CTWeb does not require SAT solvers. This paper describes the PROW algorithm, also analysing its complexity and efficiency. The CTWeb tool is presented, including two examples of the PROW application to two real environments: the first corresponds to the migration of the subsystem of transactions processing of a credit card management system from AS400 to Oracle with .NET; the second applies both the algorithm and the tool to a SPL that monitors and controls some parameters of the load in trucks. © 2014 Elsevier Inc. All rights reserved.","Combinatorial testing; Software testing"
"A systematic mapping study on technical debt and its management","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921689098&doi=10.1016%2fj.jss.2014.12.027&partnerID=40&md5=6b8ec56ec6257583ebd441a3b19744cb","Context: Technical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system. Objective: This work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM. Method: A systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013. Results: Ninety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected. Conclusions: The term ""debt"" has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need formore empirical studieswith high-quality evidence on thewhole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process. © 2014 Elsevier Inc. All rights reserved.","Systematic mapping study; Technical debt; Technical debt management"
"A separation-based UI architecture with a DSL for role specialization","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921671860&doi=10.1016%2fj.jss.2014.11.039&partnerID=40&md5=f21617c8daf8540404c20b82679ed1e0","This paper proposes an architecture and associated methodology to separate front end UI concerns from back end coding concerns to improve the platform flexibility, shorten the development time, and increase the productivity of developers. Typical UI development is heavily dependent upon the underlying platform, framework, or tool used to create it, which results in a number of problems. We took a separation-based UI architecture and modified it with a domain specific language to support the independence of UI creation thereby resolving some of the aforementioned problems. Amethodology incorporating this architecture into the development process is proposed. A climate science application was created to verify the validity of the methodology using modern practices of UX, DSLs, code generation, and model-driven engineering. Analyzing related work provides an overview of other methods similar to our method. Subsequently we evaluate the climate science application, conclude, and detail future work. © 2014 Published by Elsevier Inc.","Domain specific language; Model driven engineering; User experience"
"Stochastic thermal-aware real-time task scheduling with considerations of soft errors","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923197058&doi=10.1016%2fj.jss.2014.12.009&partnerID=40&md5=31c787fd03e0bd08b74c71896d9ef8aa","With the continued scaling of the CMOS devices, the exponential increase in power density has strikingly elevated the temperature of on-chip systems. Dynamic voltage/frequency scaling is a widely utilized system level power management technique to reduce the energy consumption and lower the on-chip temperature. However, scaling the voltage or frequency for thermal management leads to an increase in soft error rates, thus has adverse impact on system reliability. In this paper, the authors propose a stochastic thermal-aware task scheduling algorithm that considers soft errors in real-time embedded systems. For the given customer-defined soft error related target reliability and the maximum peak temperature, the proposed scheduling algorithm generates an energy-efficient task schedule by selecting the energy efficient operating frequency for each task and alternating the execution of hot tasks and cool tasks at the scaled operating frequency. The proposed stochastic scheduling algorithm features the consideration of uncertainty in transient fault occurrences. To handle the uncertainty, a fault adaptation variable α is introduced to adapt task execution to the stochastic property of fault occurrences. An energy efficiency factor δ is also introduced to facilitate the enhancement of energy efficiency by maximizing the energy saved per unit slack. Extensive simulations of synthetic real-time tasks and real-life benchmarking tasks were performed to validate the effectiveness of the proposed algorithm. Experimental results show that the proposed algorithm consumes up to 17.8% less energy as compared to the benchmarking schemes, and the peak temperature of the proposed algorithm is always below the maximum temperature limit and can be up to 9.6-°C lower than that of the benchmarking schemes. © 2015 Elsevier Inc. All rights reserved.","Fault-tolerance; Real-time systems; Thermal-aware"
"Manufacturing execution systems: A vision for managing software development","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921695373&doi=10.1016%2fj.jss.2014.11.015&partnerID=40&md5=936a1c5dce7ce5a20fcdc6f9f2da403f","Software development suffers from a lack of predictabilitywith respect to cost, time, and quality. Predictability is one of the major concerns addressed by modern manufacturing execution systems (MESs). A MES does not actually execute the manufacturing (e.g., controlling equipment and producing goods), but rather collects, analyzes, integrates, and presents the data generated in industrial production so that employees have better insights into processes and can react quickly, leading to predictable manufacturing processes. In this paper, we introduce the principles and functional areas of a MES. We then analyze the gaps between MES-visiondriven software development and current practices. These gaps include: (1) lack of a unified data collection infrastructure, (2) lack of integrated people data, (3) lack of common conceptual frameworks driving improvement loops from development data, and (4) lack of support for projection and simulation. Finally, we illustrate the feasibility of leveraging MES principles to manage software development, using a Modularity Debt Management Decision Support System prototype we developed. In this prototype we demonstrate that information integration in MES-vision-driven systems enables new types of analyses, not previously available, for software development decision support.We conclude with suggestions formoving current software development practices closer to the MES vision. © 2014 Elsevier Inc. All rights reserved.","Decision support; Manufacturing execution system; Modularity debt management; Software development management"
"Enhanced healthcare personnel rostering solution using mobile technologies","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919344103&doi=10.1016%2fj.jss.2014.10.015&partnerID=40&md5=e206218eb11c25f0766707dfeadb1239","This paper presents a novel personnel rostering system for healthcare units, which incorporates mobile technologies to minimize time overheads and boost personnel satisfaction. This way, doctors nurses and administrative staff may provide solutions and suggestions to the process of shifts' scheduling and rostering in a group based - social and organized manner, at any given time, using their smartphone or tablet. This system is designed and implemented according to wide research on requirements' specification, carried out in Greek public hospitals and based on a study of healthcare units' organization, at a practical and legal level. The personnel rostering system anticipates to facilitate the staff administration task, through real-time communication between hospital's personnel. It enables the formation of a micro-community with enhanced social communication tools, to provide dynamic management, recording and updating of changes that occur in scheduled duties, without mediators and delays. The proposed solution includes an intelligent mobile device application, designed for smartphones and tablets. It is provided to the personnel and enables them to participate in the process of scheduling duties and shifts. The XML based, back-end, supporting information system offers services that allow a smoother operation of the unit, minimize time overheads in case of arbitrary changes and maximize satisfaction of personnel. The overall operation of the units, that reclaim the features offered by this system, can be improved. Minimizing the time and other bureaucratic delays in personnel scheduling is a vital part of the way a healthcare facility is organized. Thus, facilitating this process, with any available technology, may prove to be cost effective and crucial. Systems that incorporate mobile applications are already widely accepted, and become increasingly important to the healthcare sector, as well. The mobile based, personnel shifts' scheduling solution shown is an approach that already receives encouraging support and indicates that it assists in achieving remarkable results. © 2014 Elsevier Inc.","Health information systems; Mobile applications; Personnel scheduling"
"VM scaling based on Hurst exponent and Markov transition with empirical cloud data","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912133838&doi=10.1016%2fj.jss.2014.10.011&partnerID=40&md5=43cbcad1c6ea56cff69d64f534d60b48","One of the major benefits of cloud computing is virtualization scaling. Compared to existing studies on virtual machine scaling, this paper introduces Hurst exponent which gives additional characteristics for data trends to supplement the often used Markov transition approach. This approach captures both the long and short-term behaviors of the virtual machines (VMs). The dataset for testing of this approach was gathered from the computer usage of key servers supporting a large university. Performance evaluation shows our approach can assist prediction of VM CPU usage toward effective resource allocation. In turn, this allows the cloud resource provider to monitor and allocate the resource usage of all VMs in order to meet the service level agreements for each VM client. © 2014 Elsevier Inc. All rights reserved.","Cloud computing; Resource management; Usage prediction"
"Neural networks for predicting the duration of new software projects","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921652361&doi=10.1016%2fj.jss.2014.12.002&partnerID=40&md5=88cda85d1e1de552a7faa40461916a2a","The duration of software development projects has become a competitive issue: only 39% of them are finished on time relative to the duration planned originally. The techniques for predicting project duration are most often based on expert judgment and mathematical models, such as statistical regression or machine learning. The contribution of this study is to investigate whether or not the duration prediction accuracy obtained with a multilayer feedforward neural network model, also called a multilayer perceptron (MLP), and with a radial basis function neural network (RBFNN) model is statistically better than that obtained by a multiple linear regression (MLR) model when functional size and the maximum size of the team of developers are used as the independent variables. The three models mentioned above are trained and tested by predicting the duration of new software development projects with a set of projects from the International Software Benchmarking Standards Group (ISBSG) release 11. Results based on absolute residuals, Pred(l) and a Friedman statistical test show that prediction accuracy with the MLP and the RBFNN is statistically better than with the MLR model. © 2015, Elsevier Inc. All rights reserved.","Multilayer feedforward neural network; Radial basis function neural network; Software project duration prediction"
"Enabling improved IR-based feature location","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921674390&doi=10.1016%2fj.jss.2014.11.013&partnerID=40&md5=387aad712dfc3f528aefd7b6d62128bf","Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of IR requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrievalmodel has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the SEMERU dataset. In the experiments, fivemethods of scraping queries frommodification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean Reciprocal Rank (MRR), the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers. Results find that models based on query-likelihood perform about twice as well as models in common use in software engineering such as LSI and thus deserve greater attention. Furthermore, corpus preprocessing has a significant impact as the top performing setting is over 100% better than the average. © 2014 Elsevier Inc. All rights reserved.","Feature location; Information retrieval models; Query formulation"
"When did your project start? - The software supplier's perspective","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927523189&doi=10.1016%2fj.jss.2015.02.041&partnerID=40&md5=b69057e09748349c94b77666bb02009c","A software development project may be considered a failure because it is late. In order to be able to assess this, a project start date should be known. The purpose of the paper is to study software development projects in a business context and especially project start from the supplier's perspective. In our research, we observed different ways of assigning project start but did not observe any company-level instructions for defining it. We raise questions regarding knowledge loss, project profitability, and having a project running late even before the project has started. We provide definitions for project start and project start date, and define boundaries for software supplier's project start-up. With this paper, we emphasise the need to study software development projects in a business context. The paper contributes to research on project management success, the software project business, and the management of a software project in a business context. © 2015 Elsevier Inc. All rights reserved.","Project management; Project start-up; Software development project"
"Automated analysis of security requirements through risk-based argumentation","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930798276&doi=10.1016%2fj.jss.2015.04.065&partnerID=40&md5=513147ddcf4881fbb28f4297cd85b70f","Abstract Computer-based systems are increasingly being exposed to evolving security threats, which often reveal new vulnerabilities. A formal analysis of the evolving threats is difficult due to a number of practical considerations such as incomplete knowledge about the design, limited information about attacks, and constraints on organisational resources. In our earlier work on RISA (RIsk assessment in Security Argumentation), we showed that informal risk assessment can complement the formal analysis of security requirements. In this paper, we integrate the formal and informal assessment of security by proposing a unified meta-model and an automated tool for supporting security argumentation called OpenRISA. Using a uniform representation of risks and arguments, our automated checking of formal arguments can identify relevant risks as rebuttals to those arguments, and identify mitigations from publicly available security catalogues when possible. As a result, security engineers are able to make informed and traceable decisions about the security of their computer-based systems. The application of OpenRISA is illustrated with examples from a PIN Entry Device case study. © 2015 Elsevier Inc.","Risk assessment; Security analysis; Structured argumentation"
"An effective approach to estimating the parameters of software reliability growth models using a real-valued genetic algorithm","2015","Journal of Systems and Software","10.1016/j.jss.2015.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923124957&doi=10.1016%2fj.jss.2015.01.001&partnerID=40&md5=edc4c32fb2cde1605a937bf66165fc0a","In this paper, we propose an effective approach to estimate the parameters of software reliability growth model (SRGM) using a real-valued genetic algorithm (RGA). The existing SRGMs require the estimation of the parameters such as the total number of failures or the failure detection rate using numerical methods, maximum likelihood estimation or least square estimation. However, these methods impose certain constraints on the parameter estimation of SRGM like requiring the continuity and existence of derivatives in the modelling function. RGA is free from the constraints on the parameter estimation of SRGM. Moreover, it is more adapted in optimization of continuous domain such as parameter estimation of SRGM than a binary genetic algorithm. Two real-valued genetic operators, heuristic crossover and non-uniform mutation, are applied to improve the accuracy and performance of the parameter estimation of SRGM. We conducted experiments on eight real world datasets for comparing the proposed approach with the numerical methods and other existing genetic algorithms. The results indicate that the RGA is more effective in the parameter estimation of SRGM than other GA approaches. We believe that RGA can be a promising solution to effectively managing software quality through the accurate reliability estimates. © 2015 Elsevier Inc. All rights reserved.","Genetic algorithm; Parameter estimation; Software reliability growth model"
"The discourse on tool integration beyond technology, a literature survey","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930795165&doi=10.1016%2fj.jss.2015.04.082&partnerID=40&md5=22bbc2f20a52a09fc5873024c7504e56","Abstract The tool integration research area emerged in the 1980s. This survey focuses on those strands of tool integration research that discuss issues beyond technology. We reveal a discourse centered around six frequently mentioned non-functional properties. These properties have been discussed in relation to technology and high level issues. However, while technical details have been covered, high level issues and, by extension, the contexts in which tool integration can be found, are treated indifferently. We conclude that this indifference needs to be challenged, and research on a larger set of stakeholders and contexts initiated. An inventory of the use of classification schemes underlines the difficulty of evolving the classical classification scheme published by Wasserman. Two frequently mentioned redefinitions are highlighted to facilitate their wider use. A closer look at the limited number of research methods and the poor attention to research design indicates a need for a changed set of research methods. We propose more critical case studies and method diversification through theory triangulation. Additionally, among disparate discourses we highlight several focusing on standardization which are likely to contain relevant findings. This suggests that open communities employed in the context of (pre-)standardization could be especially important in furthering the targeted discourse. © 2015 Elsevier Inc. All rights reserved.","Support environments; Tool integration"
"Semi-automatic architectural pattern identification and documentation using architectural primitives","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923178582&doi=10.1016%2fj.jss.2014.12.042&partnerID=40&md5=4ca009a7e01db9f3c66680b7edd8edb8","In this article, we propose an interactive approach for the semi-automatic identification and documentation of architectural patterns based on a domain-specific language. To address the rich concepts and variations of patterns, we firstly propose to support pattern description through architectural primitives. These are primitive abstractions at the architectural level that can be found in realizations of multiple patterns, and they can be leveraged by software architects for pattern annotation during software architecture documentation or reconstruction. Secondly, using these annotations, our approach automatically suggests possible pattern instances based on a reusable catalog of patterns and their variants. Once a pattern instance has been documented, the annotated component models and the source code get automatically checked for consistency and traceability links are automatically generated. To study the practical applicability and performance of our approach, we have conducted three case studies for existing, non-trivial open source systems. © 2015 Elsevier Inc. All rights reserved.","Architectural component views; Architectural pattern; Software architecture"
"Improving software reliability prediction through multi-criteria based dynamic model selection and combination","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921644330&doi=10.1016%2fj.jss.2014.12.029&partnerID=40&md5=2e7c4c06dd261f3cd36e83acbdb89fa9","In spite of much research efforts to develop software reliability models, there is no single model which is appropriate in all circumstances. Accordingly, some recent studies on software reliability have attempted to use existing models more effectively in practice (e.g., model selection and combination). However, it is not easy to identify which model is likely to make the most trustworthy predictions and to assign appropriate weights to models for the combination. The improper model selection or weight assignment often causes unsuccessful software reliability prediction in practice, which leads to cost/schedule overrun. In this paper, we propose a systematic reliability prediction framework which dynamically selects and combines multiple software reliability models based on the decision trees learning of multi-criteria. For the model selection, the proposed approach uses the empirical patterns of multi-criteria derived from models. Reduced error pruning decision tree identifies the models with the best predictive patterns and automatically assign a weight to each model. Then, the identified models fall into two groups according to the likelihood of over- or under-prediction, and the competitive models from each group are combined based on their given weights. From the evaluation results, our approach outperformed existing methods on average prediction accuracy. © 2014 Elsevier Inc. All rights reserved.","Decision trees; Multi criteria; Software reliability prediction"
"Engineering Future Internet applications: The Prime approach","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930813995&doi=10.1016%2fj.jss.2015.03.102&partnerID=40&md5=e75b140bc54dcc7ca62fb8362577b63a","Abstract The Future Internet is envisioned as a worldwide environment connecting a large open-ended collection of heterogeneous and autonomous resources, namely Things, Services and Contents, which interact with each other anywhere and anytime. Applications will possibly emerge dynamically as opportunistic aggregation of resources available at a given time, and will be able to self-adapt according to the environment dynamics. In this context, engineers should be provided with proper modeling and programming abstractions to develop applications able to benefit from Future Internet, by being at the same time fluid, as well as dependable. Indeed, such abstractions should (i) facilitate the development of autonomous and independent interacting resources (loose coupling), (ii) deal with the run-time variability of the application in terms of involved resources (flexibility), (iii) provide mechanisms for run-time resources discovery and access (dynamism), and (iv) enable the running application to accommodate unforeseen resources (serendipity). To this end, Prime (P-Rest at design/run tIME) defines the P-REST architectural style, and a set of P-REST oriented modeling and programming abstractions to provide engineers with both design-time and run-time support for specifying, implementing and operating P-RESTful applications. © 2015 Elsevier Inc.","Future Internet; Middleware; Resource-oriented architecture"
"Software rejuvenation via a multi-agent approach","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927515795&doi=10.1016%2fj.jss.2015.02.017&partnerID=40&md5=36bd0dbc43837b6b705a24311713ea75","Usually, development teams devote a huge amount of time and effort on maintaining existing software. Since many of these maintenance tasks are not planned, the software tends to degrade over time, causing side effects mainly on its non-functional requirements. This paper proposes the use of a multi-agent system in order to perform perfective maintenance tasks in a software product through refactorings. The software developer chooses the quality attribute that the agents should improve and the agents are able to autonomously search the code for opportunities to apply perfective maintenance, apply the perfective maintenance, and evaluate if the source code quality has been improved. Its main contributions are: (i) the refactorings are autonomously done by software agents during the idle development time; (ii) all changes are stored in isolated branches in order to facilitate the communication with the developers; (iii) the refactorings are applied only when the program semantics is preserved; (iv) the agents are able to learn the more suitable sequence of refactorings to improve a specific quality attribute; and (v) the approach can be extended with other metrics and refactorings. This paper also presents a set of experimental studies that provide evidences of the benefits of our approach for software rejuvenation. © 2015 Elsevier Inc. All rights reserved.","Multi-agent systems; Refactoring; Software rejuvenation"
"An imperfect software debugging model considering log-logistic distribution fault content function","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919389764&doi=10.1016%2fj.jss.2014.10.040&partnerID=40&md5=e3d1e7701fe66e4ac7788c2c7f2707dc","Numerous software reliability growth models based on the non-homogeneous Poisson process assume perfect debugging. Such models, including the Goel-Okumoto, delayed S-shaped, and inflection S-shaped models, have been successfully validated in software testing. However, complex and uncertain test factors, such as test resource, tester skill, or test tool, can seriously affect the testing process. When detected faults are removed, new faults can be introduced in practical testing. The process is referred to as imperfect debugging. Imperfect software debugging models proposed in the literature generally assume a constantly or monotonically decreasing fault introduction rate per fault. These models cannot adequately describe the fault introduction process in a practical test. In this study, we propose an imperfect software debugging model that considers a log-logistic distribution fault content function, which can capture the increasing and decreasing characteristics of the fault introduction rate per fault. We also use several historical fault data sets to validate the performance of the proposed model. The model can suitably fit historical fault data and accurately predict failure behavior. Confidence interval and sensitivity analyses are also conducted. © 2014 Elsevier Inc. All rights reserved.","Imperfect debugging; Log-logistic distribution fault content function; Non-homogeneous; Poisson process (NHPP)"
"Dynamic cloud service selection using an adaptive learning mechanism in multi-cloud computing","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919460934&doi=10.1016%2fj.jss.2014.10.047&partnerID=40&md5=f0ea9b509a8fc57a403104347aacd249","Cloud service selection in a multi-cloud computing environment is receiving more and more attentions. There is an abundance of emerging cloud service resources that makes it hard for users to select the better services for their applications in a changing multi-cloud environment, especially for online real time applications. To assist users to efficiently select their preferred cloud services, a cloud service selection model adopting the cloud service brokers is given, and based on this model, a dynamic cloud service selection strategy named DCS is put forward. In the process of selecting services, each cloud service broker manages some clustered cloud services, and performs the DCS strategy whose core is an adaptive learning mechanism that comprises the incentive, forgetting and degenerate functions. The mechanism is devised to dynamically optimize the cloud service selection and to return the best service result to the user. Correspondingly, a set of dynamic cloud service selection algorithms are presented in this paper to implement our mechanism. The results of the simulation experiments show that our strategy has better overall performance and efficiency in acquiring high quality service solutions at a lower computing cost than existing relevant approaches. © 2014 Elsevier Inc.","Adaptive learning mechanism; Cloud service broker; Dynamic cloud service selection"
"Integrating non-parametric models with linear components for producing software cost estimations","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912523849&doi=10.1016%2fj.jss.2014.09.025&partnerID=40&md5=78b7b74c570e9d34e8f24d1a14d8b050","A long-lasting endeavor in the area of software project management is minimizing the risks caused by under- or over-estimations of the overall effort required to build new software systems. Deciding which method to use for achieving accurate cost estimations among the many methods proposed in the relevant literature is a significant issue for project managers. This paper investigates whether it is possible to improve the accuracy of estimations produced by popular non-parametric techniques by coupling them with a linear component, thus producing a new set of techniques called semi-parametric models (SPMs). The non-parametric models examined in this work include estimation by analogy (EbA), artificial neural networks (ANN), support vector machines (SVM) and locally weighted regression (LOESS). Our experimentation shows that the estimation ability of SPMs is superior to their non-parametric counterparts, especially in cases where both a linear and non-linear relationship exists between software effort and the related cost drivers. The proposed approach is empirically validated through a statistical framework which uses multiple comparisons to rank and cluster the models examined in non-overlapping groups performing significantly different. © 2014 Elsevier Inc. All rights reserved.","Semi-parametric models; Software cost estimation"
"A controlled experiment to evaluate the understandability of KAOS and i∗ for modeling Teleo-Reactive systems","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919383927&doi=10.1016%2fj.jss.2014.10.010&partnerID=40&md5=88ff6e7bbddf0a79f744156b526ffc9e","Context Teleo-Reactive (TR) specifications allow engineers to define the behavior of reactive systems while taking into account goals and changes in the state of the environment.; Objective This article evaluates two different Goal Oriented Requirements Engineering notations, i∗ and KAOS, to determine their understandability level for specifying TR systems.; Method A controlled experiment was performed by two groups of Bachelor students. Each group first analyzed a requirements model of a mobile robotic system, specified using one of the evaluated languages, and then they filled in a questionnaire to evaluate its understandability. Afterwards, each group proceeded similarly with the model of another system specified with the second language.; Results The statistical analysis of the data obtained by means of the experiment showed that the understandability of i∗ is higher than that of KAOS when modeling TR systems.; Conclusion Both languages are suitable for specifying TR systems although their notations should be specialized to maximize the understandability attribute. i∗ surpasses KAOS due to two main reasons: i∗ models represent dependencies between agents and goals or tasks; and notational differences between tasks and goals in i∗ are more evident than those between goals and requirements in KAOS. © 2014 Elsevier Inc. All rights reserved.","Controlled experiment; Requirements engineering; Teleo-Reactive"
"Aggregate-strength interaction test suite prioritization","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912110079&doi=10.1016%2fj.jss.2014.09.002&partnerID=40&md5=4ab6d81b32a3668d266a1ea894228e69","Combinatorial interaction testing is a widely used approach. In testing, it is often assumed that all combinatorial test cases have equal fault detection capability, however it has been shown that the execution order of an interaction test suite's test cases may be critical, especially when the testing resources are limited. To improve testing cost-effectiveness, test cases in the interaction test suite can be prioritized,and one of the best-known categories of prioritization approaches is based on ""fixed-strength prioritization"", which prioritizes an interaction test suite by choosing new test cases which have the highest uncovered interaction coverage at a fixed strength (level of interaction among parameters). A draw-back of these approaches, however, is that, when selecting each test case, they only consider a fixed strength, not multiple strengths. To overcome this, we propose a new ""aggregate-strength prioritization"",to combine interaction coverage at different strengths. Experimental results show that in most cases ourmethod performs better than the test-case-generation, reverse test-case-generation, and random prioritization techniques. The method also usually outperforms ""fixed-strength prioritization"", while maintaining a similar time cost. © 2014 Elsevier Inc. All rights reserved.","Aggregate-strength prioritization; Combinatorial interaction testing; Fixed-strength prioritization"
"A process to identify relevant substitutes for healing failed WS-∗ orchestrations","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927513528&doi=10.1016%2fj.jss.2015.02.028&partnerID=40&md5=b2609aaf7e9570d12a2ba32a78d4d8c0","Orchestrating web services aims to compose multiple services into workflows that answer complex user requirements. Web services are software components which are exposed to errors and failures that can occur during web service orchestration execution. Thus, many error-handling and healing approaches have been proposed to guarantee reliable orchestrations. Some of these approaches rely on the identification of relevant service substitutes to heal (by substitution) the defected services. In this paper, we propose an identification process of web service substitutes for healing failed web service orchestrations based on the measurement of similarity between service interfaces. The process reveals both simple and complex (compositions of) substitutes. We validated the approach via a set of experiments conducted on a collection of real web services. © 2015 Elsevier Inc. All rights reserved.","Formal concept analysis; Similarity measurement; Web service selection; Web services orchestration"
"Recommender systems based on social networks","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912120718&doi=10.1016%2fj.jss.2014.09.019&partnerID=40&md5=6736d68e0fec7c4136f55968fb5df9b3","The traditional recommender systems, especially the collaborative filtering recommender systems, have been studied by many researchers in the past decade. However, they ignore the social relationships among users. In fact, these relationships can improve the accuracy of recommendation. In recent years, the study of social-based recommender systems has become an active research topic. In this paper, we propose a social regularization approach that incorporates social network information to benefit recommender systems. Both users' friendships and rating records (tags) are employed to predict the missing values (tags) in the user-item matrix. Especially, we use a biclustering algorithm to identify the most suitable group of friends for generating different final recommendations. Empirical analyses on real datasets show that the proposed approach achieves superior performance to existing approaches. © 2014 Elsevier Inc. All rights reserved.","Recommender system; Social network; Social-based recommender system"
"Cost, benefits and quality of software development documentation: A systematic mapping","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912534729&doi=10.1016%2fj.jss.2014.09.042&partnerID=40&md5=a6b1aedb15b6274239f8283766888c91","Context: Software documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit. Objectives: In this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality. Method: We use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively. Results: We present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1-11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one ""System under Study"" (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study. Conclusion: Our study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies; (2) studies involving large-scale development projects, or from large number of study participants of various organizations; (3) more industry-academia collaborations; (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost. © 2014 Elsevier Inc. All rights reserved.","Documentation benefit; Software documentation; Systematic mapping"
"Service portfolio management: A repository-based framework","2015","Journal of Systems and Software","10.1016/j.jss.2015.01.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927514577&doi=10.1016%2fj.jss.2015.01.055&partnerID=40&md5=ed8c1fc2a757338c4b88c6649659a4cc","The paper discusses a framework for managing and evaluating ICT-enabled service portfolios along the service design phase. The framework adopts a service reuse perspective and it is made up of i) a model for the representation of a repository of services, ii) a model for the definition of a service portfolio representing current production lines of a service provider organization, iii) a set of metrics for service portfolio evaluation, and iv) a tool supporting managers in decision making for the achievement of design objectives. The proposed metrics and the tool are supposed to allow decision makers to get an improved view of the service design process. Furthermore, the framework supports managers in decision making for the achievement of production objectives as well as operational strategies, resulting in potential reuse initiatives, likewise. To provide evidence of the impacts of the proposed framework, experimental activities are discussed focusing on a real life case study, referring to an Italian small size service provider. © 2015 Published by Elsevier Inc.","Service portfolio; Service repository; Service reuse"
"Sentiment Analysis in monitoring software development processes: An exploratory case study on GitHub's project issues","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927564469&doi=10.1016%2fj.jss.2015.02.055&partnerID=40&md5=45afdbb408bd89f74b23d5ea466dfefe","Software process models, which allow us to develop software products, can be improved by using the corresponding quality model. However, current tendencies in the application of Global Software Engineering and Global Software Development, which forces geographically dispersed teams to collaborate, make the usual monitoring techniques obsolete. This situation has led to looking for new methods that can help in the decision making process, such as the case of the Social Network Analysis field. In this article we propose the introduction of Sentiment Analysis techniques in order to identify and monitor the underlying sentiments in the text written by developers in issues and tickets. Therefore, in order to check its viability we conducted an exploratory case study analysing polarity and emotional clues in development issues from nine well-known projects that are freely available. Results show that although both polarity and emotional analysis are applicable, the emotional analysis looks to be more suitable to this kind of corpus. The developers leave underlying sentiments in the text, and that information could be monitored as any other feature in the development process. © 2015 Elsevier Inc. All rights reserved.","Github; Sentiment Analysis; Software development process"
"Countering the concept-drift problems in big data by an incrementally optimized stream mining model","2015","Journal of Systems and Software","10.1016/j.jss.2014.07.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923227603&doi=10.1016%2fj.jss.2014.07.010&partnerID=40&md5=9a4917e5713a2d39f41b70a51b7fc776","Mining the potential value hidden behind big data has been a popular research topic around the world. For an infinite big data scenario, the underlying data distribution of newly arrived data may be appeared differently from the old one in the real world. This phenomenon is so-called the concept-drift problem that exists commonly in the scenario of big data mining. In the past decade, decision tree inductions use multi-tree learning to detect the drift using alternative trees as a solution. However, multi-tree algorithms consume more computing resources than the singletree. This paper proposes a singletree with an optimized node-splitting mechanism to detect the drift in a test-then-training tree-building process. In the experiment, we compare the performance of the new method to some state-of-art singletree and multi-tree algorithms. Result shows that the new algorithm performs with good accuracy while a more compact model size and less use of memory than the others. © 2014 Elsevier Inc. All rights reserved.","Concept drift; Data stream mining; Very fast decision tree"
"A cloud-based framework for Home-diagnosis service over big medical data","2015","Journal of Systems and Software","10.1016/j.jss.2014.05.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923257070&doi=10.1016%2fj.jss.2014.05.068&partnerID=40&md5=c0264cfa3ecefe91fe16db915651333a","Self-caring services are becoming more and more important for our daily life, especially under the urgent situation of global aging. Big data such as massive historical medical records makes it possible for users to have self-caring services, such as to get diagnosis by themselves with similar patients' records. Developing such a self-caring service gives rises to challenges including highly concurrent and scalable medical record retrieval, data analysis, as well as privacy protection. In this paper, we propose a cloud-based framework for implementing a self-caring service named Home-diagnosis to address the above challenges. Concretely, a Lucene-based distributed search cluster is designed to support highly concurrent and scalable medical record retrieval, data analysis and privacy protection. Moreover, to speed up medical record retrieval, a Hadoop cluster is adopted for offline data storage and index building. The implementation of the Home-diagnosis service is discussed, where similar historical medical records as well as a disease-symptom lattice are obtained, to help users figure out which kind of disease they are probably infected with. Finally, a prototype system is designed and a running example is presented to demonstrate the scalability and efficiency of our proposal. © 2014 Elsevier Inc. All rights reserved.","Big medical data; Cloud-based framework; Home-diagnosis service"
"A scalable generic transaction model scenario for distributed NoSQL databases","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921664513&doi=10.1016%2fj.jss.2014.11.037&partnerID=40&md5=7cd2a46a224e9052ee2928b8f0f7b4d4","With the development of cloud computing and internet; e-Commerce, e-Business and corporate world revenue are increasing with high rate. These areas not only require scalable and consistent databases but also require inter database transaction support. In this paper, we present, a scalable three-tier architecture along with a distributed middle-ware protocol to support atomic transactions across heterogeneous NoSQL databases. Our methodology does not compromise on any assumption on the accuracy of failure modalities. Hence, it is suitable for a class of heterogeneous distributed systems. To achieve such a target, our architectural model exploits an innovative methodology to achieve distributed atomic transactions. We simulate this architectural setup with different latency tests under different environments to produce reliable impact and correctness. © 2014 Elsevier Inc. All rights reserved.","Atomic transaction; Column-oriented distributed databases; NoSQL databases"
"Learning to detect representative data for large scale instance selection","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930800042&doi=10.1016%2fj.jss.2015.04.038&partnerID=40&md5=f79af6deb671fa9d55aeea50f2a9199c","Abstract Instance selection is an important data pre-processing step in the knowledge discovery process. However, the dataset sizes of various domain problems are usually very large, and some are even non-stationary, composed of both old data and a large amount of new data samples. Current algorithms for solving this type of scalability problem have certain limitations, meaning they require a very high computational cost over very large scale datasets during instance selection. To this end, we introduce the ReDD (Representative Data Detection) approach, which is based on outlier pattern analysis and prediction. First, a machine learning model, or detector, is used to learn the patterns of (un)representative data selected by a specific instance selection method from a small amount of training data. Then, the detector can be used to detect the rest of the large amount of training data, or newly added data. We empirically evaluate ReDD over 50 domain datasets to examine the effectiveness of the learned detector, using four very large scale datasets for validation. The experimental results show that ReDD not only reduces the computational cost nearly two or three times by three baselines, but also maintains the final classification accuracy. © 2015 Elsevier Inc.","Data mining; Data reduction; Instance selection"
"Load-prediction scheduling algorithm for computer simulation of electrocardiogram in hybrid environments","2015","Journal of Systems and Software","10.1016/j.jss.2015.01.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923227619&doi=10.1016%2fj.jss.2015.01.015&partnerID=40&md5=e153340e29ac42e0f0433fdb3c1d5e7d","This paper proposes an algorithm that allows fully utilize the Central Processing Unit-Graphics Processing Unit (CPU-GPU) hybrid architecture to conduct parallel computation and reasonable scheduling for computer simulation of electrocardiogram (ECG). This algorithm is realized by accelerating calculation speed and increasing platform adaptability of the parallel algorithm. Today, many algorithms have been proposed to dynamically schedule a set of tasks in CPU-GPU hybrid environments. Among these scheduling algorithms, only Pure Self-Scheduling (PSS) algorithm can achieve load balancing in such an extremely heterogeneous environment. However, Pure Self-Scheduling can neither fully exploit the advantages of GPU performance, nor efficiently minimize the dynamic scheduling overhead. In this paper, Load-Prediction Scheduling (LPS) has been introduced to solve the aforementioned problems. Furthermore, to meet the demand for the best performance in a hybrid environment, which is formed by many heterogeneous computers, we propose an approach to adjust scheduling parameters dynamically. In order to validate our parallel algorithm and scheduling approach, we performed ECG simulation to confirm the efficiency and accuracy of ECG simulation algorithms based on the proposed method. At first, LPS predicts the workloads of each step in the simulation. The prediction results help to schedule heavy workloads to components with strong computational ability and light workloads to components with weak computational ability. LPS also synthesizes dynamic-scheduling and static-scheduling methods to minimize the disadvantages of these two scheduling methods. In the meantime, a Sliding Window Mechanism (SWM) adjusts the boundary between dynamic-scheduling and static-scheduling to make LPS perform better in hybrid environments. Experimental results of LPS on the computer simulation of ECG show that the LPS algorithm is more efficient than PSS. The ECG simulation is improved by about 20 times by using our proposed method. The ECG simulation of LPS with SWM is about 21% faster than that without SWM. © 2015 Elsevier Inc. All rights reserved.","Computer simulation of ECG; Load-Prediction Scheduling; Sliding Window Mechanism"
"Modeling the QoS parameters of DDS for event-driven real-time applications","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927522112&doi=10.1016%2fj.jss.2015.03.008&partnerID=40&md5=7c1436a29423848509db482d8a601156","The Data Distribution Service (DDS) standard defines a data-centric distribution middleware that supports the development of distributed real-time systems. To this end, the standard includes a wide set of configurable parameters to provide different degrees of Quality of Service (QoS). This paper presents an analysis of these QoS parameters when DDS is used to build reactive applications normally designed under an event-driven paradigm, and shows how to represent them using the real-time end-to-end flow model defined by the MARTE standard. We also present an application-case study to illustrate the use and modeling of DDS in next-generation distributed real-time systems. © 2015 Elsevier Inc. All rights reserved.","Distribution middleware; Model-driven engineering; Real-time systems"
"Modelling large-scale information systems using ADLs - An industrial experience report","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912574659&doi=10.1016%2fj.jss.2014.09.018&partnerID=40&md5=83fdbcb9057862b5dffdfaa6c4069cc3","An organisation that had developed a large information system wanted to embark on a programme that would involve large-scale evolution of it. As a precursor to this, it was decided to create a comprehensive architectural description to capture and understand the system's design. This undertaking faced a number of challenges, including a low general awareness of software modelling and software architecture practices. The approach taken by the software architects tasked with this project included the definition of a simple, very specific, architecture description language. This paper reports our experience of the project and a simple ADL that we created as part of it. © 2014 Elsevier Inc. All rights reserved.","Architecture description language; Industrial experience report; Software architecture discovery"
"Toward the tools selection in model based system engineering for embedded systems - A systematic literature review","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930792473&doi=10.1016%2fj.jss.2015.04.089&partnerID=40&md5=1596d01b2cf558f034bd3728226c09a4","Abstract Model based system engineering (MBSE) is a systematic approach of modeling which is frequently used to support requirement specification, design, verification and validation activities of system development. However, it is difficult to customize MBSE approach for the development of embedded systems due to their diverse behavioral aspects. Furthermore, appropriate tools selection to perform particular MBSE activities is always challenging. This paper focuses on the identification and classification of recent research practices pertaining to embedded systems development through MBSE approach. Consequently, a comprehensive analysis of various MBSE tools has been presented. Systematic literature review (SLR) has been used to identify 61 research practices published during 2008-2014. The identified researches have been classified into six different categories to analyze various aspects of MBSE approach for embedded systems. Consequently, 39 preliminary tools are identified that have been used in recent researches. Furthermore, classification and evaluation of tools have been presented. This research highlights important trends and approaches of MBSE to support development of embedded systems. A comprehensive investigation of tools in this article facilitates researchers, practitioners and developers to select appropriate tools according to their requirements. © 2015 Elsevier Inc.","Embedded systems; MBSE; Tools"
"Investigating security threats in architectural context: Experimental evaluations of misuse case maps","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927547379&doi=10.1016%2fj.jss.2015.02.040&partnerID=40&md5=6b6b5d5a4880d8f9cd0cee22cad7505e","Many techniques have been proposed for eliciting software security requirements during the early requirements engineering phase. However, few techniques so far provide dedicated views of security issues in a software systems architecture context. This is a problem, because almost all requirements work today happens in a given architectural context, and understanding this architecture is vital for identifying security vulnerabilities and corresponding mitigations. Misuse case maps attempt to provide an integrated view of security and architecture by augmenting use case maps with misuse case concepts. This paper evaluates misuse case maps through two controlled experiments where 33 and 54 ICT students worked on complex real-life intrusions described in the literature. The students who used misuse case maps showed significantly better understanding of intrusions and better ability to suggest mitigations than students who used a combination of two existing techniques as an alternative treatment. Misuse case maps were also perceived more favourably overall than the alternative treatment, and participants reported using misuse case maps more when solving their tasks. © 2015 Elsevier Inc. All rights reserved.","computer security; Intrusion analysis; Use case maps"
"Safe evolution templates for software product lines","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930801915&doi=10.1016%2fj.jss.2015.04.024&partnerID=40&md5=a265b139dfb8eb630a7f2f5a15c4fb26","Abstract Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis. © 2015 Elsevier Inc.","Evolution; Refinement; Software product lines"
"Multi-criteria scheduling of Bag-of-Tasks applications on heterogeneous interlinked clouds with simulated annealing","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921671377&doi=10.1016%2fj.jss.2014.11.014&partnerID=40&md5=2878f558949c1b0d9362dad5f4b40e69","Cloud computing has spurred the creation of a multitude of services that use the cloud to deliver their products on-demand. Behind it, stand multiple ""Cloud Providers"" that in the past few years have created data-centers, spread around the world, creating a mesh of distributed resources that can meet high availability and quality of service requirements. The growing number of cloud clients demand reliability, performance and better cost-to-performance ratios. Recently, scientific research has focused on the optimization of interlinked cloud systems, an aim which requires strategies for allocation of resources and distribution of computing tasks between them, while also considering their cost along with any factors that may differentiate them. In this study, we have evaluated the use of simulated annealing and thermodynamic simulated annealing in the scheduling of a dynamic multi-cloud system with virtual machines of heterogeneous performance serving Bag-of-Tasks applications. The scheduling heuristics applied, consider multiple criteria when scheduling said applications and try to optimize both for performance and cost, while also taking into account the heterogeneity of the virtual machines. Simulation results indicate that the use of these heuristics can have a significant impact in performance while maintaining a good cost-performance trade-off. © 2014 Elsevier Inc. All rights reserved.","Bag-of-Tasks; Cloud computing; Simulated annealing"
"Quantifying usability of domain-specific languages: An empirical study on software maintenance","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921689186&doi=10.1016%2fj.jss.2014.11.051&partnerID=40&md5=c3c6de27be5fc26101f9986f980b89b1","A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions. © 2014 Elsevier Inc. All rights reserved.","DSL; Metrics; Usability"
"Integrating usability work into a large inter-organisational agile development project: Tactics developed by usability designers","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919452028&doi=10.1016%2fj.jss.2014.10.036&partnerID=40&md5=3edf81f1986eecc614832c72cd1ee9e5","In this paper we examine the integration of usability activities into a large inter-organisational agile development project. Inter-organisational agile projects possess unique attributes. They involve multiple stakeholders from different organisational contexts and are thus characterised by competing priorities. Team members also lack a mutual awareness of what constitutes work. These issues make the collaboration between project teams challenging. Meanwhile collaboration between usability designers and agile project teams is an integral part of the integration of usability activities into agile development projects. We carried out an interpretive case study on a large inter-organisational agile development project to examine how usability designers and agile project teams collaborate in this project type. Results showed integration goals were achieved through five tactics deployed by the usability designers. These tactics were negotiating inclusion; upward influencing, placating expert users, establishing credibility and diffusing designs. The implications of these findings are summarised in the form of three propositions that pertain to how usability designer-agile project team collaborations might be organised in agile development projects. Further, the role of the usability designer in ensuring the integration of usability activities is also emphasised. © 2014 Elsevier Inc.","Agile software development; Tactics; Usability"
"Semantic based representing and organizing surveillance big data using video structural description technology","2015","Journal of Systems and Software","10.1016/j.jss.2014.07.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923248155&doi=10.1016%2fj.jss.2014.07.024&partnerID=40&md5=6b840cb702d6b94b6450433556721289","Big data is an emerging paradigm applied to datasets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Especially, the data volume of all video surveillance devices in Shanghai, China, is up to 1 TB every day. Thus, it is important to accurately describe the video content and enable the organizing and searching potential videos in order to detect and analyze related surveillance events. Unfortunately, raw data and low level features cannot meet the video based task. In this paper, a semantic based model is proposed for representing and organizing video big data. The proposed surveillance video representation method defines a number of concepts and their relations, which allows users to use them to annotate related surveillance events. The defined concepts include person, vehicles, and traffic sighs, which can be used for annotating and representing video traffic events unambiguous. In addition, the spatial and temporal relation between objects in an event is defined, which can be used for annotating and representing the semantic relation between objects in related surveillance events. Moreover, semantic link network is used for organizing video resources based on their associations. In the application, one case study is presented to analyze the surveillance big data. © 2014 Elsevier Inc. All rights reserved.","Big data representing and organizing; Surveillance big data; Video structural description"
"Bringing Test-Driven Development to web service choreographies","2015","Journal of Systems and Software","10.1016/j.jss.2014.09.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912520833&doi=10.1016%2fj.jss.2014.09.034&partnerID=40&md5=956b66160dea22c407d7e274e22fae57","Choreographies are a distributed approach for composing web services. Compared to orchestrations, which use a centralized scheme for distributed service management, the interaction among the choreographed services is collaborative with decentralized coordination. Despite the advantages, choreography development, including the testing activities, has not yet evolved suf.ciently to support the complexity of the large distributed systems. This substantially impacts the robustness of the products and overall adoption of choreographies. The goal of the research described in this paper is to support the Test-Driven Development (TDD) of choreographies to facilitate the construction of reliable, decentralized distributed systems. To achieve that, we present Rehearsal, a framework supporting the automated testing of choreographies at development-time. In addition, we present a choreography development methodology that guides the developer on applying TDD using Rehearsal. To assess the framework and the methodology, we conducted an exploratory study with developers, whose result was that Rehearsal was considered very helpful for the application of TDD and that the methodology helped the development of robust choreographies. © 2014 Elsevier Inc. All rights reserved.","Automated testing; Test-Driven development; Web service choreographies"
"Design and programming patterns for implementing usability functionalities in web applications","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929177383&doi=10.1016%2fj.jss.2015.04.023&partnerID=40&md5=2e5db5df0dd50bcb744c296a2b1e5bd0","Usability is a software system quality attribute. There are usability issues that have an impact not only on the user interface but also on the core functionality of applications. In this paper, three web applications were developed to discover patterns for implementing two usability functionalities with an impact on core functionality: Abort Operation and Progress Feedback. We applied an inductive process in order to identify reusable elements to implement the selected functionalities. For communication purposes, these elements are specified as design and programming patterns (PHP, VB.NET and Java). Another two web applications were developed in order to evaluate the patterns. The evaluation explores several issues such as ease of pattern understanding and ease of pattern use, as well as the final result of the applications. We found that it is feasible to reuse the identified solutions specified as patterns. The results also show that usability functionalities have features, like the level of coupling with the application or the complexity of each component of the solution, that simplify or complicate their implementation. In this case, the Abort Operation functionality turned out to be more feasible to implement than the Progress Feedback functionality.","Design patterns; Programming patterns; Software engineering"
"Algorithms for automated live migration of virtual machines","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921629478&doi=10.1016%2fj.jss.2014.11.044&partnerID=40&md5=5528b12fd4f7665ecf35f66d23e87de3","We present two strategies to balance the load in a system with multiple virtual machines (VMs) through automated live migration. When the push strategy is used, overloaded hosts try to migrate workload to less loaded nodes. On the other hand, when the pull strategy is employed, the light-loaded hosts take the initiative to offload overloaded nodes. The performance of the proposed strategies was evaluated through simulations. We have discovered that the strategies complement each other, in the sense that each strategy comes out as ""best"" under different types of workload. For example, the pull strategy is able to quickly re-distribute the load of the system when the load is in the range low-to-medium, while the push strategy is faster when the load is medium-to-high. Our evaluation shows that when adding or removing a large number of virtual machines in the system, the ""best"" strategy can re-balance the system in 4-15 min. © 2014 Elsevier Inc. All rights reserved.","Live migration; Load balancing; Virtualization"
"Comprehensible software fault and effort prediction: A data mining approach","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919361139&doi=10.1016%2fj.jss.2014.10.032&partnerID=40&md5=2eab3043a0a68f06cf9b0418d024f57f","Software fault and effort prediction are important tasks to minimize costs of a software project. In software effort prediction the aim is to forecast the effort needed to complete a software project, whereas software fault prediction tries to identify fault-prone modules. In this research both tasks are considered, thereby using different data mining techniques. The predictive models not only need to be accurate but also comprehensible, demanding that the user can understand the motivation behind the model's prediction. Unfortunately, to obtain predictive performance, comprehensibility is often sacrificed and vice versa. To overcome this problem, we extract trees from well performing Random Forests (RFs) and Support Vector Machines for regression (SVRs) making use of a rule extraction algorithm ALPA. This method builds trees (using C4.5 and REPTree) that mimic the black-box model (RF, SVR) as closely as possible. The proposed methodology is applied to publicly available datasets, complemented with new datasets that we have put together based on the Android repository. Surprisingly, the trees extracted from the black-box models by ALPA are not only comprehensible and explain how the black-box model makes (most of) its predictions, but are also more accurate than the trees obtained by working directly on the data. © 2014 Elsevier Inc. All rights reserved.","Comprehensibility; Rule extraction; Software fault and effort prediction"
"Information infrastructure risk prediction through platform vulnerability analysis","2015","Journal of Systems and Software","10.1016/j.jss.2015.04.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930801342&doi=10.1016%2fj.jss.2015.04.062&partnerID=40&md5=81c7611654e1e1d3501e4805f3da8bb8","Abstract The protection of information infrastructures is important for the function of other infrastructure sectors. As vital parts for the information infrastructure operation, software-based platforms, face a series of vulnerabilities and threats. This paper aims to provide a complementary approach to existing vulnerability prediction solutions and launch the measurement of zero-day risk by introducing a risk prediction methodology for an information infrastructure. The proposed methodology consists of four steps and utilizes the outcomes of a proper analysis of security measurements provided by specifications from the Security Content Automation Protocol. First, we identify software platform assets that support an information infrastructure and second we measure the historical rate of vulnerability occurrences. Third, we use a distribution fitting procedure to estimate the statistical correlation between empirical and reference probability distributions and verify the statistical significance of the distribution fitting results with the Kolmogorov - Smirnov test. Fourth, we develop conditional probability tables that constitute a Bayesian Belief Network topology as means to enable risk prediction and estimation on security properties. The practicality of the risk prediction methodology is demonstrated with an implementation example from the electronic banking sector. The contribution of the proposed methodology is to provide auditors with a proactive approach about zero-day risks. © 2015 Elsevier Inc.","Bayesian belief network (BBN); Kolmogorov-Smirnov test; Zero-day risk"
"Input-based adaptive randomized test case prioritization: A local beam search approach","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929158133&doi=10.1016%2fj.jss.2015.03.066&partnerID=40&md5=3b2177b9f964f3e3c75f7df6e4942978","Test case prioritization assigns the execution priorities of the test cases in a given test suite. Many existing test case prioritization techniques assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in real-world software development projects. This paper proposes a novel family of input-based local-beam-search adaptive-randomized techniques. They make adaptive tree-based randomized explorations with a randomized candidate test set strategy to even out the search space explorations among the branches of the exploration trees constructed by the test inputs in the test suite. We report a validation experiment on a suite of four medium-size benchmarks. The results show that our techniques achieve either higher APFD values than or the same mean APFD values as the existing code-coverage-based greedy or search-based prioritization techniques, including Genetic, Greedy and ART, in both our controlled experiment and case study. Our techniques are also significantly more efficient than the Genetic and Greedy, but are less efficient than ART.","Adaptive test case prioritization; Randomized algorithm; Regression testing"
"New approaches to usability evaluation in software development: Barefoot and crowdsourcing","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929190869&doi=10.1016%2fj.jss.2015.03.043&partnerID=40&md5=c55a9684158e037867d45aae3399c1f4","Usability evaluations provide software development teams with insights on the degree to which software applications enable users to achieve their goals, how fast these goals can be achieved, how easy an application is to learn and how satisfactory it is in use. Although such evaluations are crucial in the process of developing software systems with a high level of usability, their use is still limited in small and medium-sized software development companies. Many of these companies are e.g. unable to allocate the resources that are needed to conduct a full-fledged usability evaluation in accordance with a conventional approach. This paper presents and assesses two new approaches to overcome usability evaluation obstacles: a barefoot approach where software development practitioners are trained to drive usability evaluations; and a crowdsourcing approach where end users are given minimalist training to enable them to drive usability evaluations. We have evaluated how these approaches can reduce obstacles related to limited understanding, resistance and resource constraints. We found that these methods are complementary and highly relevant for software companies experiencing these obstacles. The barefoot approach is particularly suitable for reducing obstacles related to limited understanding and resistance while the crowdsourcing approach is cost-effective.","Developers; Usability; Users"
"From source code identifiers to natural language terms","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919460270&doi=10.1016%2fj.jss.2014.10.013&partnerID=40&md5=b2f690f4be9ae86e85985c8c428a0953","Program comprehension techniques often explore program identifiers, to infer knowledge about programs. The relevance of source code identifiers as one relevant source of information about programs is already established in the literature, as well as their direct impact on future comprehension tasks. Most programming languages enforce some constrains on identifiers strings (e.g., white spaces or commas are not allowed). Also, programmers often use word combinations and abbreviations, to devise strings that represent single, or multiple, domain concepts in order to increase programming linguistic efficiency (convey more semantics writing less). These strings do not always use explicit marks to distinguish the terms used (e.g., CamelCase or underscores), so techniques often referred as hard splitting are not enough. This paper introduces Lingua::IdSplitter a dictionary based algorithm for splitting and expanding strings that compose multi-term identifiers. It explores the use of general programming and abbreviations dictionaries, but also a custom dictionary automatically generated from software natural language content, prone to include application domain terms and specific abbreviations. This approach was applied to two software packages, written in C, achieving a f-measure of around 90% for correctly splitting and expanding identifiers. A comparison with current state-of-the-art approaches is also presented. © 2014 Elsevier Inc.","Identifier splitting; Natural language processing; Program comprehension"
"A solution of dynamic VMs placement problem for energy consumption optimization based on evolutionary game theory","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921656830&doi=10.1016%2fj.jss.2014.12.030&partnerID=40&md5=1f8dcd9a58599666df4c6bda2f20d393","Power saving of data centers has become an urgent problem in recent years. For a virtualized data center, optimizing the placement of virtual machines (VMs) dynamically is one of the most effective methods for power savings. Based on a deep study on VMs placement, a solution is proposed and described in this paper to solve the problem of dynamic placement of VMs toward optimization of their energy consumptions. A computationalmodel of energy consumption is proposed and built. A novel algorithm based on evolutionary game theory is also presented, which successfully addresses the challenges faced by dynamic placement of VMs. It is proved that the proposed algorithm can reach the optimal solutions theoretically. Experimental results also demonstrate that, by adjusting VMs placement dynamically, the energy consumption can be reduced correspondingly. In comparison with the existing state of the arts, our proposedmethod outperforms other five algorithms tested and achieves savings of 30-40% on energy consumption. © 2014 Elsevier Inc. All rights reserved.","Dynamic VMs placement; Energy consumption; Evolutionary game theory"
"Defining multi-tenancy: A systematic mapping study on the academic and the industrial perspective","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919352447&doi=10.1016%2fj.jss.2014.10.034&partnerID=40&md5=b9ed2367f2436e9e8da554e130a122dc","Software as a service is frequently offered in a multi-tenant style, where customers of the application and their end-users share resources such as software and hardware among all users, without necessarily sharing data. It is surprising that, with such a popular paradigm, little agreement exists with regard to the definition, domain, and challenges of multi-tenancy. This absence is detrimental to the research community and the industry, as it hampers progress in the domain of multi-tenancy and enables organizations and academics to wield their own definitions to further their commercial or research agendas. In this article, a systematic mapping study on multi-tenancy is described in which 761 academic papers and 371 industrial blogs are analysed. Both the industrial and academic perspective are assessed, in order to get a complete overview. The definition and topic maps provide a comprehensive overview of the domain, while the research agenda, listing four important research topics, provides a roadmap for future research efforts. © 2014 Elsevier Inc. All rights reserved.","Definition; Multi-tenancy; Systematic mapping study"
"Soft competency requirements in requirements engineering, software design, implementation, and testing","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921656831&doi=10.1016%2fj.jss.2014.12.010&partnerID=40&md5=60aa037d623cc34b18aad0f887393623","Global software development changes the requirements in terms of soft competency and increases the complexity of social interaction by including intercultural aspects.While soft competency is often seen as crucial for the success of global software development projects, the concrete competence requirements remain unknown. Internationalization competency represents one of the first attempts to structure and describe the soft competence requirements for global software developers. Based on the diversity of tasks, competence requirements will differ among the various phases of software development. By conducting a survey on the importance of internationalization competences for the different phases of global software development, we identified differences in terms of competence importance and requirements in the phases. ""Adaptability"" (of one's working style) and ""Cultural Awareness"" were the main differences. ""Cultural Awareness"" distinguishes requirements engineering and software design from testing and implementationwhile ""Adaptability"" distinguishes implementation and software design from requirements engineering and testing. © 2014 Elsevier Inc. All rights reserved.","Competency requirements; Global software development; Soft competency"
"SMaRT: A novel framework for addressing range queries over nonlinear trajectories","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929208577&doi=10.1016%2fj.jss.2015.03.068&partnerID=40&md5=10d904ca55816c3074b648332f466e5d","A spatiotemporal database is a database that manages both space and time information. Common examples include tracking of moving objects, intelligent transportation systems, cellular communications and meteorology monitoring. A spatiotemporal query determines the objects included in a region at a specified period of time between two date-time instants referred as time window. In the context of this work, we present SMaRT: A novel Spatiotemporal Mysql ReTrieval framework, based on MySQL and PostgreSQL database management system. Moreover, we propose a demo user interface that implements all of its capabilities, in order to help user determine the most efficient spatiotemporal query method on user-defined 2D trajectories. To our knowledge, we are the first to study and compare methods of addressing range queries on nonlinear moving object trajectories, that are represented both in dual and native dimensional space. In particular, it is the first time a theoretically efficient dual approach was implemented for nonlinear trajectories and incorporated into a well-known open-source RDBMS. An experimental evaluation is included that shows the performance and efficiency of our approach.","Data structures; Indexing; Spatiotemporal databases"
"The effects of different alphabets on free text keystroke authentication: A case study on the Korean-English users","2015","Journal of Systems and Software","10.1016/j.jss.2014.12.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923196448&doi=10.1016%2fj.jss.2014.12.017&partnerID=40&md5=88bcdaa6014506b0abe97ca6a54eade0","Keystroke dynamics is one of the representative behavioral biometrics, and it has been consistently recognized as an alternative to physiological biometrics for user authentication to strengthen the level of security. This paper investigates the effects that languages with different alphabets and different familiarity levels have on the free text keystroke authentication performance using Korean-English data collected from 83 Korean participants. In order to exploit the familiarity level, two typing characteristics are measured and tested. Student's t-test reveals that the participants have higher typing proficiency but lower typing consistency in the language with the more familiar alphabet, i.e., their primary language (Korean). Typing proficiency is found to be a critical factor when only keystroke latencies are utilized during authentication, whereas typing consistency is found to be a critical factor when key sequence information is utilized in addition to keystroke latencies. The experimental results can be applied to build a customized keystroke dynamics-based authentication system, which adaptively determines the authentication method as well as the keystroke size based on a user's typing characteristics. © 2015 Elsevier Inc. All rights reserved.","Free text; Keystroke dynamics; User authentication"
"Energy efficiency heterogeneous wireless access selection for multiple types of applications","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921698048&doi=10.1016%2fj.jss.2014.11.049&partnerID=40&md5=b55eb145c5e95e8cef376d067f88490e","Mobile terminal (MT) users run various types of applications, such as e-mail, APPs, web browsers, and multimedia, through various types of wireless networks. Extending the battery life of MT, which requires a large amount of electricity for wireless transmission, has become critical. This study focused on the energy efficiency of wireless networks, such as 3G, 4G and Wi-Fi, based on application characteristics and transmission loads. The various applications are classified into idle-bound applications (e.g., e-mail service) and transmission-bound applications (e.g., multimedia) that require diverse types of wireless networks. The operation state of a wireless network includes transmitting, receiving, listening, and sleeping modes. According to the game theory of the energy consumption analysis between the characteristics of applications andwireless networks, three wireless network selection schemes IBLB (idle-bound with load-balancing), TBLB (transmission-bound with load balancing), and WLAT (weighted load and application type) were proposed to reduce the amount of power consumption. Previous studies were compared with the proposed schemes through (1) variation of the number of running applications, (2) various numbers of 3G/4G base stations and Wi-Fi access points, and (3) the combinations of various types of applications to evaluate the energy efficiency of Wi-Fi and 3G/4G access networks selections. © 2014 Elsevier Inc. All rights reserved.","Base station orientation; Energy efficiency; Selection"
"Progressive online aggregation in a distributed stream system","2015","Journal of Systems and Software","10.1016/j.jss.2014.11.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923262959&doi=10.1016%2fj.jss.2014.11.027&partnerID=40&md5=1b786579330da1520f13652b6f94597c","Interactive query processing aims at generating approximate results with minimum response time. However, it is quite difficult for a batch-oriented processing system to progressively provide cumulatively accurate results in the context of a distributed environment. MapReduce Online extends the MapReduce framework to support online aggregation, but it is hindered by its processing speed in keeping up with ongoing real-time data events. We deploy the online aggregation algorithm over S4, a scalable stream processing system that is inspired by the combined functionalities of MapReduce and Actor model. Our system applies an asynchronous message communication mechanism from actor model to support online aggregation. It can process large scale data stream with high concurrency in a short response time. In this system, we adopt a distributed weighted random sampling algorithm to solve biased distribution between different streams. Furthermore, a multi-level query processing topology is developed to reduce overlapped processing for multiple queries. Our system can provide continuous window aggregation with a confidence interval and error bound. We have implemented our system and conducted plentiful experiments over the TPC-H benchmark. A large number of experiments are carried out to demonstrate that by using our system, high-quality query results can be generated within a short response time and that the approach outperforms MapReduce Online on data streams. © 2014 Elsevier Inc. All rights reserved.","Actor model; Online aggregation; Stream processing"
"Agent-based Cloud bag-of-tasks execution","2015","Journal of Systems and Software","10.1016/j.jss.2015.02.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927523975&doi=10.1016%2fj.jss.2015.02.039&partnerID=40&md5=1571fae07b8f0978f379ebb9c56dfb58","Bag-of-tasks (BoTs) applications are highly parallel, unconnected and unordered tasks. Since BoT executions often require costly investments in computing infrastructures, Clouds offer an economical solution to BoT executions. Cloud BoT executions involve (1) allocating and deallocating heterogeneous resources with possibly different price rates from multiple Cloud providers, (2) distributing BoT execution across multiple, distributed resources, and (3) coordinating self-interested Cloud participants. This paper proposes a novel agent-based Cloud BoT execution tool (CloudAgent) supported by a 4-stage agent-based protocol capable of dynamically coordinating autonomous Cloud participants to concurrently execute BoTs in multiple Clouds in a parallel manner. CloudAgent is endowed with an autonomous agent-based resource provisioning system supported by the contract net protocol to dynamically allocate resources based on hourly cost rates from multiple Cloud providers. In addition, CloudAgent is also equipped with an agent-based resource deallocation system that autonomously and dynamically deallocates resources assigned to BoT executions. Empirical results show that CloudAgent can efficiently handle concurrent BoT executions, bear low BoT execution costs, and effectively scale. © 2015 Elsevier Inc. All rights reserved.","Cloud computing; Multiagent systems; Resource allocation"
"Emotion-led modelling for people-oriented requirements engineering: The case study of emergency systems","2015","Journal of Systems and Software","10.1016/j.jss.2015.03.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929152153&doi=10.1016%2fj.jss.2015.03.044&partnerID=40&md5=5830f658c90814a1573981df93d77f2a","In the field of design, it is accepted that users' perceptions of systems are influenced by emotion as much as cognition, and functionally-complete products will not be adopted if they do not appeal to emotions. While software engineering methodologies have matured to handle non-functional requirements such as usability, what has not been investigated fully is the emotional needs of people. That is, what do users want to feel, and how do they feel about a system? In this paper, we argue that these emotional desires should be treated as first-class citizens in software engineering methodology, and present preliminary work on including emotions in requirements models using emotional goals. We evaluate these models both with a controlled user study, and on a case study of emergency systems for older people. The results of the controlled user study indicate that people are comfortable interpreting and modifying our models, and view the inclusion of emotions as first-class entities as a positive step in software engineering. The results of our case study indicate that current emergency systems fail to address the emotional needs their users, leading to low adoption and low usage. We conceptualised, designed, and prototyped an improved emergency system, and placed it into the homes of nine older people over a period of approximately two weeks each, showing improved user satisfaction over existing systems.","Agent-oriented modelling; Emotions; Requirements engineering"
"An automated approach for noise identification to assist software architecture recovery techniques","2015","Journal of Systems and Software","10.1016/j.jss.2015.05.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937391311&doi=10.1016%2fj.jss.2015.05.065&partnerID=40&md5=aa47e9543e0b6128a619a6b2b648b1c0","Software systems' concrete architecture often drifts from the intended architecture throughout their evolution. Program comprehension activities, like software architecture recovery, become very demanding, especially for large and complex systems due to the existence of noise, which is created by omnipresent and utility classes that obscure the system structure. Omnipresent classes represent crosscutting concerns, utilities or elementary domain concepts. The identification and filtering of noise is a necessary preprocessing step before attempting program comprehension techniques, especially for undocumented systems. In this paper, we propose an automated methodology for noise identification. Our methodology is based on the notion that noisy classes are widely used in a system, directly or indirectly. We combine classes' usage significance with their participation in the system's subgraphs, in order to identify the classes that are persistently used. Usage significance is measured according to Component Rank, a well-established metric in the literature, which ranks software artifacts according to their usage significance. The experimental results show that the proposed methodology successfully captures classes that produce noise and improves the results of existing algorithms for software systems' architectural decomposition. © 2015 Elsevier Inc. All rights reserved.","Noise identification; Omnipresent classes; Software architecture recovery"
"A benchmarking process to assess software requirements documentation for space applications","2015","Journal of Systems and Software","10.1016/j.jss.2014.10.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919343973&doi=10.1016%2fj.jss.2014.10.054&partnerID=40&md5=010c6410b22d8537b49aaeee716797d1","Poorly written requirements are a common source of software defects and, in application areas like space systems, the cost of malfunctioning software can be very high. This work proposes a benchmarking procedure for assessing the quality of software requirements that adopt the Packet Utilization Standard (PUS) defined by the European Cooperation for Space Standardization (ECSS) standards. The benchmark uses three checklists that aim at guaranteeing that the specifications comply with the PUS standard, consider faulty behaviour, and do not include errors typically found in this type of documents. The benchmark is defined for two services of the PUS standard: the telecommand verification and on board operating scheduling. A benchmark validation approach is also proposed in the paper. It uses the concept of fault injection to insert known errors in software requirements specification documents. The benchmark validation is performed through its application to three projects from different countries. Results show that our proposal provides a simple and effective way for identifying weaknesses and compare the degree of maturity of requirements documents. © 2014 Elsevier Inc.","Benchmarking; ECSS standards; Software requirements quality"
"An evaluation model for dependability of Internet-scale software on basis of Bayesian Networks and trustworthiness","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895062236&doi=10.1016%2fj.jss.2013.08.035&partnerID=40&md5=5849951bcf68da726763eeb416c513f6","Internet-scale software becomes more and more important as a mode to construct software systems when Internet is developing rapidly. Internet-scale software comprises a set of widely distributed software entities which are running in open, dynamic and uncontrollable Internet environment. There are several aspects impacting dependability of Internet-scale software, such as technical, organizational, decisional and human aspects. It is very important to evaluate dependability of Internet-scale software by integrating all the aspects and analyzing system architecture from the most foundational elements. However, it is lack of such an evaluation model. An evaluation model of dependability for Internet-scale software on the basis of Bayesian Networks is proposed in this paper. The structure of Internet-scale software is analyzed. An evaluating system of dependability for Internet-scale software is established. It includes static metrics, dynamic metrics, prior metrics and correction metrics. A process of trust attenuation based on assessment is proposed to integrate subjective trust factors and objective dependability factors which impact on system quality. In this paper, a Bayesian Network is build according to the structure analysis. A bottom-up method that use Bayesian reasoning to analyses and calculate entity dependability and integration dependability layer by layer is described. A unified dependability of the whole system is worked out and is corrected by objective data. The analysis of experiment in a real system proves that the model in this paper is capable of evaluating the dependability of Internet-scale software clearly and objectively. Moreover, it offers effective help to the design, development, deployment and assessment of Internet-scale software. © 2013 Elsevier Inc.","Bayesian Network; Dependability; Internet-scale software"
"SecureSMS: A secure SMS protocol for VAS and other applications","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894414770&doi=10.1016%2fj.jss.2013.12.031&partnerID=40&md5=43ad02f36bdc6b15f3ff076cefaaadfd","Nowadays, the SMS is a very popular communication channel for numerous value added services (VAS), business and commercial applications. Hence, the security of SMS is the most important aspect in such applications. Recently, the researchers have proposed approaches to provide end-to-end security for SMS during its transmission over the network. Thus, in this direction, many SMS-based frameworks and protocols like Marko's SMS framework, Songyang's SMS framework, Alfredo's SMS framework, SSMS protocol, and, Marko and Konstantin's protocol have been proposed but these frameworks/protocols do not justify themselves in terms of security analysis, communication and computation overheads, prevention from various threats and attacks, and the bandwidth utilization of these protocols. The two protocols SMSSec and PK-SIM have also been proposed to provide end-to-end security and seem to be little better in terms of security analysis as compared to the protocols/framework mentioned above. In this paper, we propose a new secure and optimal protocol called SecureSMS, which generates less communication and computation overheads. We also discuss the possible threats and attacks in the paper and provide the justified prevention against them. The proposed protocol is also better than the above two protocols in terms of the bandwidth utilization. On an average the SecureSMS protocol reduces 71% and 59% of the total bandwidth used in the authentication process as compared to the SMSSec and PK-SIM protocols respectively. Apart from this, the paper also proposes a scheme to store and implement the cryptographic algorithms onto the SIM card. The proposed scheme provides end-to-end SMS security with authentication (by the SecureSMS protocol), confidentiality (by encryption AES/Blowfish; preferred AES-CTR), integrity (SHA1/MD5; preferred SHA1) and non-repudiation (ECDSA/DSA; preferred ECDSA). © 2014 Elsevier Inc.","Authentication; SMS; VAS"
"An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900605085&doi=10.1016%2fj.jss.2013.12.038&partnerID=40&md5=7f3d3292c13ffe2d6939fd1ecd408e13","Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures. © 2014 Elsevier Inc.","Dynamic Software Product Lines; Dynamic variability; Feature models; Software architecture"
"Supporting SIP-based end-to-end Data Distribution Service QoS in WANs","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905818466&doi=10.1016%2fj.jss.2014.03.078&partnerID=40&md5=386682594302b052e52cc798b956ab6b","Assuring end-to-end QoS in enterprise distributed real-time and embedded (DRE) systems is hard due to the heterogeneity and transient behavior of communication networks, the lack of integrated mechanisms that schedule communication and computing resources holistically, and the scalability limits of IP multicast in wide-area networks (WANs). This paper makes three contributions to research on overcoming these problems in the context of enterprise DRE systems that use the OMG Data Distribution Service (DDS) quality-of-service (QoS)-enabled publish/subscribe (pub/sub) middleware over WANs. First, it codifies the limitations of conventional DDS implementations deployed over WANs. Second, it describes a middleware component called Proxy DDS that bridges multiple, isolated DDS domains deployed over WANs. Third, it describes the NetQSIP framework that combines multi-layer, standards-based technologies including the OMG-DDS, Session Initiation Protocol (SIP), and IP DiffServ to support end-to-end QoS in a WAN and shield pub/sub applications from tedious and error-prone details of network QoS mechanisms. The results of experiments using Proxy DDS and NetQSIP show how combining DDS with SIP in DiffServ networks significantly improves dynamic resource reservation in WANs and provides effective end-to-end QoS management. © 2014 Elsevier B.V. All rights reserved.","DiffServ; Proxy DDS; SIP"
"End-user development by application-domain configuration","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900578735&doi=10.1016%2fj.jss.2013.11.1121&partnerID=40&md5=c61f96115a0f54c3155339e4db414a0a","An application generator/tailoring tool aimed at end users is described. It employs conceptual models of problem domains to drive configuration of an application generator suitable for a related set of applications, such as reservation and resource allocation. The tool supports a two-phase approach of configuring the general architecture for a domain, such as reservation-booking problems, then customisation and generation of specific applications. The tool also provides customisable natural language-style queries for spatial and temporal terms. Development and use of the tool to generate two applications, service engineer call allocation, and airline seat reservation, are reported with a specification exercise to configure the generic architecture to a new problem domain for monitoring-sensing applications. The application generator/tailoring tool is evaluated with novice end users and experts to demonstrate its effectiveness. © 2013 Elsevier Inc.","Application generation; Domain-oriented design; End-user development"
"A component-based process with separation of concerns for the development of embedded real-time software systems","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906951301&doi=10.1016%2fj.jss.2014.05.076&partnerID=40&md5=dd1aafc179a694ac12e0313c8a0655e3","Numerous component models have been proposed in the literature, a testimony of a subject domain rich with technical and scientific challenges, and considerable potential. Unfortunately however, the reported level of adoption has been comparatively low. Where successes were had, they were largely facilitated by the manifest endorsement, where not the mandate, by relevant stakeholders, either internal to the industrial adopter or with authority over the application domain. The work presented in this paper stems from a comprehensive initiative taken by the European Space Agency (ESA) and its industrial suppliers. This initiative also enjoyed significant synergy with interests shown for similar goals by the telecommunications and railways domain, thanks to the interaction between two parallel project frameworks. The ESA effort aimed at favouring the adoption of a software reference architecture across its software supply chain. The center of that strategy revolves around a component model and the software development process that builds on it. This paper presents the rationale, the design and implementation choices made in their conception, as well as the feedback obtained from a number of industrial case studies that assessed them. © 2014 The Authors.","Component model; Embedded real-time systems; Non-functional properties; Separation of concerns"
"Empirical evaluation of a privacy-focused threat modeling methodology","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906950333&doi=10.1016%2fj.jss.2014.05.075&partnerID=40&md5=1ee86422f9e1fe9162090f50583c32d8","Privacy is a key issue in today's society. Software systems handle more and more sensitive information concerning citizens. It is important that such systems are privacy-friendly by design. In previous work, we proposed a privacy threat analysis methodology, named LINDDUN. The methodology supports requirements engineers and software architects in identifying privacy weaknesses in the system they contribute to developing. As this is a fairly new technique, its results when applied in realistic scenarios are yet unknown. This paper presents a series of three empirical studies that thoroughly evaluate LINDDUN from a multi-faceted perspective. Our assessment characterizes the correctness and completeness of the analysis results produced by LINDDUN, as well as the productivity associated with executing the methodology. We also look into aspects such as the ease of use and reliability of LINDDUN. The results are encouraging, overall. However, some areas for further improvement have been identified as a result of this empirical inquiry. © 2014 Elsevier Inc.","Empirical study; Privacy; Threats"
"Demand-based schedulability analysis for real-time multi-core scheduling","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895065060&doi=10.1016%2fj.jss.2013.09.029&partnerID=40&md5=ac056052449b0d740358de635196b0e2","In real-time systems, schedulability analysis has been widely studied to provide offline guarantees on temporal correctness, producing many analysis methods. The demand-based schedulability analysis method has a great potential for high schedulability performance and broad applicability. However, such a potential is not yet fully realized for real-time multi-core scheduling mainly due to (i) the difficulty of calculating the resource demand under dynamic priority scheduling algorithms that are favorable to multi-cores, and (ii) the lack of understanding how to combine the analysis framework with deadline-miss conditions specialized for those scheduling algorithms. Addressing those two issues, to the best of our knowledge, this paper presents the first demand-based schedulability analysis for dynamic job-priority scheduling algorithms: EDZL (Earliest Deadline first until Zero-Laxity) and LLF (Least Laxity First), which are known to be effective for real-time multi-core scheduling. To this end, we first derive demand bound functions that compute the maximum possible amount of resource demand of jobs of each task while the priority of each job can change dynamically under EDZL and LLF. Then, we develop demand-based schedulability analyses for EDZL and LLF, by incorporating those new demand bound functions into the existing demand-based analysis framework. Finally, we combine the framework with additional deadline-miss conditions specialized for those two laxity-based dynamic job-priority scheduling algorithms, yielding tighter schedulability analyses. Via simulations, we demonstrate that the proposed schedulability analyses outperform the existing schedulability analyses for EDZL and LLF. © 2013 Elsevier Inc.","Demand-based schedulability analysis; Multi-core systems; Real-time systems"
"Power-aware fixed priority scheduling for sporadic tasks in hard real-time systems","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894458342&doi=10.1016%2fj.jss.2013.12.032&partnerID=40&md5=26b0a8d3cc625a9b411dda6a90ee9289","In this paper, we consider the generalized power model in which the focus is the dynamic power and the static power, and we study the problem of the canonical sporadic task scheduling based on the rate-monotonic (RM) scheme. Moreover, we combine with the dynamic voltage scaling (DVS) and dynamic power management (DPM). We present a static low power sporadic tasks scheduling algorithm (SSTLPSA), assuming that each task presents its worst-case work-load to the processor at every instance. In addition, a more energy efficient approach called a dynamic low power sporadic tasks scheduling algorithm (DSTLPSA) is proposed, based on reclaiming the dynamic slack and adjusting the speed of other tasks on-the-fly in order to reduce energy consumption while still meeting the deadlines. The experimental results show that the SSTLPSA algorithm consumes 26.55-38.67% less energy than that of the RM algorithm and the DSTLPSA algorithm reduces the energy consumption up to 18.38-30.51% over the existing DVS algorithm. © 2014 Elsevier Inc.","Energy management; Real-time system; Sporadic task"
"Assessment of institutions, scholars, and contributions on agile software development (2001-2012)","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900827317&doi=10.1016%2fj.jss.2014.03.006&partnerID=40&md5=d4b8596567d6e246384b2f7597d3ffdf","The number of scholarly publications on agile software development has grown significantly in recent years. Several researchers reviewed and attempted to synthesize studies on agile software development. However, no work has ranked the contributions of scholars and institutions to publications using a thorough process. This study presents findings on top publications, institutions, and scholars in the agile software development field from 2001 to 2012 based on the publication of such works in Science Citation Index journals. This paper highlights the key outlets for agile research and summarizes the most influential researchers and institutions as well as the most studied research areas. This study concludes by providing directions for future research. © 2014 Elsevier Inc.","Agile software development; Literature assessment; Research productivity"
"Exploiting the potential of DTN for energy-efficient internetworking","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894489007&doi=10.1016%2fj.jss.2013.12.035&partnerID=40&md5=e071e988a88167f5e63556f04fb2e868","Since its original conception as a space-oriented communications architecture, Delay Tolerant Networking (DTN) has been generalized to address issues in terrestrial networks as well. In this work we employ DTN to form an internetworking overlay that exploits the surplus capacity of last hop wireless channels in order to prolong battery life for mobile networking devices. We propose a novel rendezvous mechanism and show experimentally that the DTN overlay can shape traffic, allowing the wireless interface of the mobile device to switch to the sleep state during idle intervals without degrading performance. The simulation experiments are based on a comprehensive DTN agent that incorporates the rendezvous mechanism and facilitates quantifying energy conservation. The DTN agent, implemented in ns-2, enables the study of additional Bundle Protocol design issues, such as route selection, bundle sizing, retransmission strategies, and highlights the need for cross-layer interaction between DTN and the underlying transport protocol. © 2014 Elsevier Inc.","DTN; Energy efficiency; Internetworking; ns-2; Rendezvous mechanism"
"Transforming an enterprise model into a use case model in business process systems","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906948931&doi=10.1016%2fj.jss.2014.06.007&partnerID=40&md5=a8d48e9c47ae5df28c17437d0a687c94","One of the responsibilities of requirements engineering is to transform stakeholder requirements into system and software requirements. For enterprise systems, this transformation must consider the enterprise context where the system will be deployed. Although there are some approaches for detailing stakeholder requirements, some of them even considering the enterprise context, this task is executed manually. Based on model-driven engineering concepts, this study proposes a semi-automatic transformation from an enterprise model to a use case model. The enterprise model is used as a source of information about the stakeholder requirements and domain knowledge, while the use case model is used as software requirements model. This study presents the source and target metamodels, a set of transformation rules, and a tool to support the transformation. An experiment analyzes the use of the proposed transformation to investigate its benefits and if it can be used in practice, from the point of view of students in the context of a requirements refinement. The results indicate that the approach can be used in practice, as it did not influence the quality of the generated use cases. However, the empirical analysis does not indicate benefits of using the transformation, even if the qualitative results were positive. © 2014 Elsevier Inc.","Stakeholder requirement; Transformation; Use case"
"Sustainability of Open Source software communities beyond a fork: How and why has the LibreOffice project evolved?","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895065584&doi=10.1016%2fj.jss.2013.11.1077&partnerID=40&md5=fbd68c370016475f96b1e9da712db69b","Many organisations are dependent upon long-term sustainable software systems and associated communities. In this paper we consider long-term sustainability of Open Source software communities in Open Source software projects involving a fork. There is currently a lack of studies in the literature that address how specific Open Source software communities are affected by a fork. We report from a study aiming to investigate the developer community around the LibreOffice project, which is a fork from the OpenOffice.org project. In so doing, our analysis also covers the OpenOffice.org project and the related Apache OpenOffice project. The results strongly suggest a long-term sustainable LibreOffice community and that there are no signs of stagnation in the LibreOffice project 33 months after the fork. Our analysis provides details on developer communities for the LibreOffice and Apache OpenOffice projects and specifically concerning how they have evolved from the OpenOffice.org community with respect to project activity, developer commitment, and retention of committers over time. Further, we present results from an analysis of first hand experiences from contributors in the LibreOffice community. Findings from our analysis show that Open Source software communities can outlive Open Source software projects and that LibreOffice is perceived by its community as supportive, diversified, and independent. The study contributes new insights concerning challenges related to long-term sustainability of Open Source software communities. © 2013 The Authors.","Community evolution; Fork; Open Source software"
"Reviewing the quality of awareness support in collaborative applications","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895062234&doi=10.1016%2fj.jss.2013.11.1078&partnerID=40&md5=bd8285c826d81fa1356cd8dbbe82d929","Awareness to users is a valuable feature of a collaborative system. Therefore, the designers of a system of this type may find it useful to receive hints on the awareness support provided by the system when it is under development or evolution. This paper proposes a tool for their use to obtain suggestions on the awareness features provided by the system and those not currently supported by it. The considered kinds of awareness were obtained from a review of a significant number of proposals from the literature. The tool is based on a checklist of design elements related to these awareness types to be applied by the application designer. The construction of this checklist was done as follows. The process started with an analysis of the types of awareness to be provided. This step ended with 54 selected design elements and six awareness types. Experts on the development of collaborative systems used their experience to provide correlations between the design elements and the types of awareness previously identified, thus encapsulating their expertise within the checklist. The proposal was applied to three existing collaborative systems and the results are presented. The obtained results suggest that the checklist is adequate to provide helpful hints that may be used to improve an application's awareness support. © 2013 Elsevier Inc.","Awareness; Collaborative applications; Formative evaluation; Software quality"
"A new chaotic map based image encryption schemes for several image formats","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908307088&doi=10.1016%2fj.jss.2014.08.066&partnerID=40&md5=d90d281d29701001401edd4f8fa08b55","This paper proposes several image encryption schemes for popular image formats as Joint Photographic Experts Group (JPEG), Graphics Interchange Format (GIF), Portable Network Graphics (PNG), and Tagged Image File Format (TIFF). A cross chaotic map proposed based on Devaney's theory and dynamic block dividing of the 3D baker using the cross chaotic map are used for diffusion and permutation in encryption. Moreover, in order to verify user's identity, authentication is carried out using information hiding based on the cross chaotic function. In our methods, image files syntax and structure are not destructed, and the original image can be recovered lossless. For GIF, it keeps the property of animation successfully. The security test results indicate the proposed methods have high security, and the speed of our algorithm is faster than classical solutions. JPEG, GIF, TIFF and PNG image formats are popular contemporarily. Therefore this paper shows that the prospect of chaotic image encryption is promising. © 2014 Elsevier Inc. All rights reserved.","Image encryption Cross chaotic map Sequence generator"
"Estimating confidence interval of software reliability with adaptive testing strategy","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027930935&doi=10.1016%2fj.jss.2014.08.004&partnerID=40&md5=bf15c1cf83be017b9079b3a469e307a7","Software reliability assessment is a critical problem in safety-critical and mission-critical systems. In the reliability assessment of such a system, both an accurate reliability estimate and a tight confidence interval are required. Adaptive testing (AT) is an on-line testing framework, which dynamically selects test cases from different subdomains to achieve some optimization object. Although AT has been proved effective in minimizing reliability estimator variance, its performance on providing the corresponding confidence interval has not been investigated. In order to address this issue, an AT strategy combined with Bayesian inference (AT-BI) is proposed in this study. The novel AT-BI strategy is expected to be effective in providing both a low-variance estimator and a tight confidence interval. Experiments are set up to validate the effectiveness of the AT-BI strategy. © 2014 Elsevier Inc. All rights reserved.","Adaptive testing; Confidence interval; Reliability assessment"
"GUI testing assisted by human knowledge: Random vs. functional","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895057174&doi=10.1016%2fj.jss.2013.09.043&partnerID=40&md5=38ce175b00fef6ccfa2ad80c2cfd3b62","Software testing is a labor-intensive task in software development life-cycle. Human knowledge is useful in the practices of software testing, especially GUI testing. There are many strategies for GUI testing assisted by human knowledge, in which manual random testing and manual functional testing are two of widely used ones. In this paper, an empirical study is conducted to compare random testing and functional testing in order to provide guidelines for GUI testing. 234 participants were recruited to create thousands of random and functional test cases for open source GUI applications. Some of these test cases were selected with certain coverage criteria and then run on GUI applications to evaluate random testing and functional testing. We study three aspects on the two testing strategies: effectiveness, complementarity and impact of test case length. Some useful observations in the empirical study are: (1) Random testing is more effective in the early stage of testing on small applications and functional testing has more extensive applicability for testing large sized applications. (2) Random testing and functional testing exhibit some complementarity in our experiment. (3) Short test cases can reveal some faults more quickly and long test cases can reveal more faults lastingly. © 2013 Elsevier Inc.","Functional testing; Human knowledge; Random testing"
"Empirical research methodologies and studies in Requirements Engineering: How far did we come?","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905815244&doi=10.1016%2fj.jss.2014.06.035&partnerID=40&md5=797c00fa5e056f78d8d18591c181a089","Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE. © 2014 Elsevier B.V. All rights reserved.",""
"Automatic multi-partite graph generation from arbitrary data","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902537939&doi=10.1016%2fj.jss.2014.03.022&partnerID=40&md5=59de37bfebc6c2886249f509a0629118","In this paper we present a generic model for automatic generation of basic multi-partite graphs obtained from collections of arbitrary input data following user indications. The paper also presents GraphGen, a tool that implements this model. The input data is a collection of complex objects composed by a set or list of heterogeneous elements. Our tool provides a simple interface for the user to specify the types of nodes that are relevant for the application domain in each case. The nodes and the relationships between them are derived from the input data through the application of a set of derivation rules specified by the user. The resulting graph can be exported in the standard GraphML format so that it can be further processed with other graph management and mining systems. We end by giving some examples in real scenarios that show the usefulness of this model. © 2014 Elsevier Inc.","Automatic graph generation; Graph processing and analysis"
"A method to optimize the scope of a software product platform based on end-user features","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908291258&doi=10.1016%2fj.jss.2014.08.034&partnerID=40&md5=b78a56f37674d45f046b08e09e784143","Context: Due to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.; Aim: This paper proposes a novel method to find the optimized scope of a software product platform based on end-user features.; Method: The proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives.; Results In a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope. © 2014 Elsevier Inc. All rights reserved.","Product platform scope Software product line engineering Commonality decision"
"A systematic review of software architecture visualization techniques","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902547442&doi=10.1016%2fj.jss.2014.03.071&partnerID=40&md5=973d79648f7072676e4c4c0ddb6ae102","Context Given the increased interest in using visualization techniques (VTs) to help communicate and understand software architecture (SA) of large scale complex systems, several VTs and tools have been reported to represent architectural elements (such as architecture design, architectural patterns, and architectural design decisions). However, there is no attempt to systematically review and classify the VTs and associated tools reported for SA, and how they have been assessed and applied. Objective This work aimed at systematically reviewing the literature on software architecture visualization to develop a classification of VTs in SA, analyze the level of reported evidence and the use of different VTs for representing SA in different application domains, and identify the gaps for future research in the area. Method We used systematic literature review (SLR) method of the evidence-based software engineering (EBSE) for reviewing the literature on VTs for SA. We used both manual and automatic search strategies for searching the relevant papers published between 1 February 1999 and 1 July 2011. Results We selected 53 papers from the initially retrieved 23,056 articles for data extraction, analysis, and synthesis based on pre-defined inclusion and exclusion criteria. The results from the data analysis enabled us to classify the identified VTs into four types based on the usage popularity: graph-based, notation-based, matrix-based, and metaphor-based VTs. The VTs in SA are mostly used for architecture recovery and architectural evolution activities. We have also identified ten purposes of using VTs in SA. Our results also revealed that VTs in SA have been applied to a wide range of application domains, among which ""graphics software"" and ""distributed system"" have received the most attention. Conclusion SA visualization has gained significant importance in understanding and evolving software-intensive systems. However, only a few VTs have been employed in industrial practice. This review has enabled us to identify the following areas for further research and improvement: (i) it is necessary to perform more research on applying visualization techniques in architectural analysis, architectural synthesis, architectural implementation, and architecture reuse activities; (ii) it is essential to pay more attention to use more objective evaluation methods (e.g., controlled experiment) for providing more convincing evidence to support the promised benefits of using VTs in SA; (iii) it is important to conduct industrial surveys for investigating how software architecture practitioners actually employ VTs in architecting process and what are the issues that hinder and prevent them from adopting VTs in SA. © 2014 Elsevier Inc.","Software architecture; Software architecture visualization; Visualization techniques"
"RGB color image encryption based on Choquet fuzzy integral","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908219803&doi=10.1016%2fj.jss.2014.07.025&partnerID=40&md5=42d44a7339003ce6590a0e40484cb4c3","In recent years, one can see an increasing interest in the security of digital images. This research presents a new RGB color image encryption using keystream generator based on Choquet fuzzy integral (CFI). The properties of the dynamical keystream generator with mathematical analysis are presented in this work. In the proposed method, the CFI is first used to generate pseudo-random keystreams. Then, each of the color pixels is decomposed into three gray-level components. The output of the CFI is used to randomly shift the bits of three gray-level components. Finally, three components of RGB color pixels and the generated keystream are coupled to encrypt the permuted components. Performance aspects of the proposed algorithm such as the entropy analysis, differential analysis, statistical analysis, cipher random analysis, and cipher sensitivity analysis are introduced to evaluate the security of the new scheme. The experimental results reveal the fact that the proposed algorithm is suitable for practical use in protecting the security of digital image information distributed via the Internet. © 2014 Elsevier Inc. All rights reserved.","Choquet fuzzy integral; Image encryption; Mathematical analysis"
"Generating combinatorial test suite using combinatorial optimization","2014","Journal of Systems and Software","10.1016/j.jss.2014.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908285096&doi=10.1016%2fj.jss.2014.09.001&partnerID=40&md5=74ec914aead4dd9c0a64e07bf6f46086","Combinatorial testing (CT) is an effective technique to test software with multiple configurable parameters. It is used to detect interaction faults caused by the combination effect of parameters. CT test generation aims at generating covering arrays that cover all t-way parameter combinations, where t is a given covering strength. In practical CT usage scenarios, there are usually constraints between parameters, and the performance of existing constraint-handling methods degrades fast when the number of constraints increases. The contributions of this paper are (1) we propose a new one-test-at-a-time algorithm for CT test generation, which uses pseudo-Boolean optimization to generate each new test case; (2) we have found that pursuing the maximum coverage for each test case is uneconomic, and a possible balance point is to keep the approximation ratio in [0.8,0.9]; (3) we propose a new self-adaptive mechanism to stop the optimization process at a proper time when generating each test case; (4) extensive experimental results show that our algorithm works fine on existing benchmarks, and the constraint-handling ability is better than existing approaches when the number of constraints is large; and (5) we propose a method to translate shielding parameters (a common type of constraints) into normal constraints. © 2014 Elsevier Inc. All rights reserved.","Combinatorial testing Test generation Combinatorial optimization"
"Slice-based statistical fault localization","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895063332&doi=10.1016%2fj.jss.2013.08.031&partnerID=40&md5=05416751704127384f16f54a3cb3953d","Recent techniques for fault localization statistically analyze coverage information of a set of test runs to measure the correlations between program entities and program failures. However, coverage information cannot identify those program entities whose execution affects the output and therefore weakens the aforementioned correlations. This paper proposes a slice-based statistical fault localization approach to address this problem. Our approach utilizes program slices of a set of test runs to capture the influence of a program entity's execution on the output, and uses statistical analysis to measure the suspiciousness of each program entity being faulty. In addition, this paper presents a new slicing approach called approximate dynamic backward slice to balance the size and accuracy of a slice, and applies this slice to our statistical approach. We use two standard benchmarks and three real-life UNIX utility programs as our subjects, and compare our approach with a sufficient number of fault localization techniques. The experimental results show that our approach can significantly improve the effectiveness of fault localization. © 2013 Elsevier Inc.","Debugging; Fault localization; Program slices; Statistical analysis"
"Architectural reliability analysis of framework-intensive applications: A web service case study","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902551698&doi=10.1016%2fj.jss.2014.03.070&partnerID=40&md5=0a5604635fce72a4aee87cf4ec17dfec","A novel methodology for modeling the reliability and performance of web services (WSs) is presented. To present the methodology, an experimental environment is developed in house, where WSs are treated as atomic entities but the underlying middleware is partitioned into layers. WSs are deployed in JBoss AS. Web service requests are generated to a remote middleware on which JBoss runs, and important performance parameters under various configurations are collected. In addition, a modularized simulation model in Petri net is developed from the architecture of the middleware and run-time behavior of the WSs. The results show that (1) the simulation model provides for measuring the performance and reliability of WSs under different loads and conditions that may be of great interest to WS designers and the professionals involved; (2) configuration parameters have substantial impact on the overall performance; (3) the simulation model provides a basis for aggregating the modules (layers), nullifying modules, or to include additional aspects of the WS architecture; and (4) the model is beneficial to predict the performance of WSs for those cases that are difficult to replicate in a field study. © 2014 Elsevier Inc.","Architecture-based software reliability; Petri Net; Service oriented architecture"
"A learning-based module extraction method for object-oriented systems","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908163044&doi=10.1016%2fj.jss.2014.07.038&partnerID=40&md5=6b7dd5db260c973635c2c77223a19382","Developers apply object-oriented (OO) design principles to produce modular, reusable software. Therefore, service-specific groups of related software classes called modules arise in OO systems. Extracting the modules is critical for better software comprehension, efficient architecture recovery, determination of service candidates to migrate legacy software to a service-oriented architecture, and transportation of such services to cloud-based distributed systems. In this study, we propose a novel approach to automatic module extraction to identify services in OO software systems. In our approach, first we create a weighted and directed graph of the software system in which vertices and edges represent the classes and their relations, respectively. Then, we apply a clustering algorithm over the graph to extract the modules. We calculate the weight of an edge by considering its probability of being within a module or between modules. To estimate these positional probabilities, we propose a machine-learning-based classification system that we train with data gathered from a real-world OO reference system. We have implemented an automatic module extraction tool and evaluated the proposed approach on several open-source and industrial projects. The experimental results show that the proposed approach generates highly accurate decompositions that are close to authoritative module structures and outperforms existing methods. © 2014 Elsevier Inc. All rights reserved.","SOA; Software architecture recovery; Software modularization"
"Efficient distributed skyline computation using dependency-based data partitioning","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900830435&doi=10.1016%2fj.jss.2014.03.021&partnerID=40&md5=ac2ff5c0e2d4622b499bfb95737b1f91","Skyline queries, together with other advanced query operators, are essential in order to help identify sets of interesting data points buried within huge amount of data readily available these days. A skyline query retrieves sets of non-dominated data points in a multi-dimensional dataset. As computing infrastructures become increasingly pervasive, connected by readily available network services, data storage and management have become inevitably more distributed. Under these distributed environments, designing efficient skyline querying with desirable quick response time and progressive returning of answers faces new challenges. To address this, in this paper, we propose a novel skyline query scheme termed MpSky. MpSky is based on a novel space partitioning scheme, employing the dependency relationships among data points on different servers. By grouping points of each server using dependencies, we are able to qualify a skyline point by only comparing it with data on dependent servers, and parallelize the skyline computation among non-dependent partitions that are from different servers or individual servers. By controlling the query propagation among partitions, we are able to generate skyline results progressively and prune partitions and points efficiently. Analytical and extensive simulation results show the effectiveness of the proposed scheme. © 2014 Elsevier Inc.","Data partitioning; Distributed systems; Skyline query"
"Change impact analysis and changeability assessment for a change proposal: An empirical study","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906950913&doi=10.1016%2fj.jss.2014.05.036&partnerID=40&md5=0f9167472528f96a610c67de3704a8d3","Software change is a fundamental ingredient of software maintenance and evolution. Effectively supporting software modification is essential to provide a reliable high-quality evolution of software systems, as even a slight change may cause some unpredictable and undesirable effects on other parts of the software. To address this issue, this work used change impact analysis (CIA) to guide software modification. CIA can be used to help make correct decision on the change proposal, that is changeability assessment, and to implement effective changes for a change proposal. In this article, we conducted an empirical study on three Java open-source systems to show how CIA can be used during software modification. The results indicate that: (1) assessing changeability of a change proposal based on the impact results of the CIA is not accurate from the precision perspective; (2) the proposed impactness metric is an effective indicator of changeability assessment for the change proposal; and (3) CIA can make the change implementation process more efficient and easier. © 2014 Elsevier Inc.","Change impact analysis; Changeability assessment; Empirical study"
"Efficient customization of multi-tenant Software-as-a-Service applications with service lines","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900647452&doi=10.1016%2fj.jss.2014.01.021&partnerID=40&md5=9a1e0a2dfd319e4fcb994d7532aa8624","Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort. © 2014 Elsevier Inc.","Multi-tenancy; SaaS; Variability"
"Workload-aware anomaly detection for web applications","2014","Journal of Systems and Software","10.1016/j.jss.2013.03.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895059766&doi=10.1016%2fj.jss.2013.03.060&partnerID=40&md5=73136eb09a0d902f5d3f62f3259d0e34","The failure of Web applications often affects a large population of customers, and leads to severe economic loss. Anomaly detection is essential for improving the reliability of Web applications. Current approaches model correlations among metrics, and detect anomalies when the correlations are broken. However, dynamic workloads cause the metric correlations to change over time. Moreover, modeling various metric correlations are difficult in complex Web applications. This paper addresses these problems and proposes an online anomaly detection approach for Web applications. We present an incremental clustering algorithm for training workload patterns online, and employ the local outlier factor (LOF) in the recognized workload pattern to detect anomalies. In addition, we locate the anomalous metrics with the Student's t-test method. We evaluated our approach on a testbed running the TPC-W industry-standard benchmark. The experimental results show that our approach is able to (1) capture workload fluctuations accurately, (2) detect typical faults effectively and (3) has advantages over two contemporary ones in accuracy. © 2013 Elsevier Inc.","Anomaly detection; Local outlier factor; Web applications"
"Cooperation, collaboration and pair-programming: Field studies on backup behavior","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900575880&doi=10.1016%2fj.jss.2013.12.037&partnerID=40&md5=996b7c2b50104e30bd9a996ede8fd855","Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research. © 2014 Elsevier Inc.","Backup-behavior; Field study; Pair-programming"
"Improving the communication performance of distributed animation rendering using BitTorrent file system","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908154450&doi=10.1016%2fj.jss.2014.07.050&partnerID=40&md5=8581c01ef51ef3af1defb3f839cfc9bd","Rendering is a crucial process in the production of computer generated animation movies. It executes a computer program to transform 3D models into series of still images, which will eventually be sequenced into a movie. Due to the size and complexity of 3D models, rendering process becomes a tedious, time-consuming and unproductive task on a single machine. Accordingly, animation rendering is commonly carried out in a distributed computing environment where numerous computers execute in parallel to speedup the rendering process. In accordance with distribution of computing, data dissemination to all computers also needs certain mechanisms which allow large 3D models to be efficiently moved to those distributed computers to ensure the reduction of time and cost in animation production. This paper presents and evaluates BitTorrent file system (BTFS) for improving the communication performance of distributed animation rendering. The BTFS provides an efficient, secure and transparent distributed file system which decouples the applications from complicated communication mechanism. By having data disseminated in a peer-to-peer manner and using local cache, rendering time can be reduced. Its performance comparison with a production-grade 3D animation favorably shows that the BTFS outperforms traditional distributed file systems by more than 3 times in our test configuration. © 2014 Elsevier Inc. All rights reserved.","Animation rendering; Distributed file system; Peer-to-peer"
"FPGA implementation of reversible watermarking in digital images using reversible contrast mapping","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906946010&doi=10.1016%2fj.jss.2014.05.079&partnerID=40&md5=1afba34def795fd7da923d7e34e9facd","Reversible contrast mapping (RCM) and its various modified versions are used extensively in reversible watermarking (RW) to embed secret information into the digital contents. RCM based RW accomplishes a simple integer transform applied on pair of pixels and their least significant bits (LSB) are used for data embedding. It is perfectly invertible even if the LSBs of the transformed pixels are lost during data embedding. RCM offers high embedding rate at relatively low visual distortion (embedding distortion). Moreover, low computation cost and ease of hardware realization make it attractive for real-time implementation. To this aim, this paper proposes a field programmable gate array (FPGA) based very large scale integration (VLSI) architecture of RCM-RW algorithm for digital images that can serve the purpose of media authentication in real-time environment. Two architectures, one for block size (8 × 8) and the other one for (32 × 32) block are developed. The proposed architecture allows a 6-stage pipelining technique to speed up the circuit operation. For a cover image of block size (32 × 32), the proposed architecture requires 9881 slices, 9347 slice flip-flops, 11291 number 4-input LUTs, 3 BRAMs and a data rate of 1.0395 Mbps at an operating frequency as high as 98.76 MHz. © 2014 Elsevier Inc.","FPGA; Reversible contrast mapping; Reversible watermarking"
"An evolutionary approach to identify logical components","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906948273&doi=10.1016%2fj.jss.2014.05.033&partnerID=40&md5=7638cdf49fc71d8004e6539365895d56","Context Identifying suitable components during the software design phase is an important way to obtain more maintainable software. Many methods including Graph Partitioning, Clustering-based, CRUD-based, and FCA-based methods have been proposed to identify components at an early stage of software design. However, most of these methods use classical clustering techniques, which rely on expert judgment. Objective In this paper, we propose a novel method for component identification, called SBLCI (Search-Based Logical Component Identification), which is based on GA (genetic algorithm), and complies with an iterative scheme to obtain logical components. Method SBLCI identifies logical components of a system from its analysis models using a customized GA, which considers cohesion and coupling metrics as its fitness function, and has four novel guided GA operators based on the cohesive component concept. In addition, SBLCI has an iterative scheme in which it initially identifies high-level components in the first iteration. Then, in the next iterations, it identifies low-level sub-components for each identified component in previous iterations. Results We evaluated the effectiveness of SBLCI with three real-world cases. Results revealed that SBLCI is a better alternative for identifying logical components and sub-components in comparison with existing component identification methods. © 2014 Elsevier Inc.","Genetic algorithm; Logical component identification; Search-based software design"
"Toward a new aspect-mining approach for multi-agent systems","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908307878&doi=10.1016%2fj.jss.2014.08.030&partnerID=40&md5=b224bcff17d0bc1709974771bfb3ac35","Many aspect mining techniques have been proposed for object-oriented systems. Unfortunately, aspect mining for multi-agent systems is an unexplored research area. The inherent specificities of multi-agent systems (such as autonomy, pro-activity, reactivity, and adaptability) make it difficult to understand, reuse and maintain their code. We propose, in this paper, a (semi-automatic) hybrid aspect mining approach for agent-oriented code. The technique is based on both static and dynamic analyzes. The main motivations of this work are (1) identifying cross-cutting concerns in existing agent-oriented code, and (2) making them explicitly available to software engineers involved in the evolution of agent-oriented code in order to facilitate its refactoring and, consequently, to improve its understandability, reusability and maintainability. The proposed approach is supported by a software tool, called MAMIT (MAS Aspect-MIning Tool), that we developed. The approach and the associated tool are illustrated using a concrete case study. © 2014 Elsevier Inc. All rights reserved.","Multi-agent systems Aspect mining Aspect-oriented programming"
"Assessing a requirements evolution approach: Empirical studies in the air traffic management domain","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1098","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905864021&doi=10.1016%2fj.jss.2013.11.1098&partnerID=40&md5=da7410bb8cdedfc1bdf922b2337e9ebb","Requirements evolution is still a challenging problem in engineering practices. In this paper, we report the results of the empirical evaluation of a novel approach for modeling and reasoning on evolving requirements. We evaluated the effectiveness of the approach in modeling requirements evolution by means of a series of empirical studies in the air traffic management (ATM) domain. As we also wanted to assess whether the knowledge of the method and/or the application domain influences the effectiveness of the approach, the studies involved researchers, master students and domain experts with different level of knowledge of the approach and of the ATM domain. The participants have applied the approach to a real evolutionary scenario which focuses on the introduction of a new queue management tool, the Arrival MANager (AMAN) and a new network for information sharing (SWIM) connecting the main ATM actors. The results from the studies show that the modeling approach is effective in capturing requirements evolution. In addition, domain knowledge and method knowledge do not have an observable effect on the effectiveness of the approach. Furthermore, the evaluation provided us useful insights on how to improve the modeling approach. © 2014 Elsevier B.V. All rights reserved.","Air traffic management domain; Change management; Evolution; Requirements engineering; User study"
"A methodology to automatically optimize dynamic memory managers applying grammatical evolution","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900640820&doi=10.1016%2fj.jss.2013.12.044&partnerID=40&md5=3c7439f5d0adc19d1ffcae10ca2a1e20","Modern consumer devices must execute multimedia applications that exhibit high resource utilization. In order to efficiently execute these applications, the dynamic memory subsystem needs to be optimized. This complex task can be tackled in two complementary ways: optimizing the application source code or designing custom dynamic memory management mechanisms. Currently, the first approach has been well established, and several automatic methodologies have been proposed. Regarding the second approach, software engineers often write custom dynamic memory managers from scratch, which is a difficult and error-prone work. This paper presents a novel way to automatically generate custom dynamic memory managers optimizing both performance and memory usage of the target application. The design space is pruned using grammatical evolution converging to the best dynamic memory manager implementation for the target application. Our methodology achieves important improvements (62.55% and 30.62% better on average in performance and memory usage, respectively) when its results are compared to five different general-purpose dynamic memory managers. © 2014 Elsevier Inc.","Dynamic memory manager; Genetic programming; Multi-objective optimization"
"Scalable news recommendation using multi-dimensional similarity and Jaccard-Kmeans clustering","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905860185&doi=10.1016%2fj.jss.2014.04.046&partnerID=40&md5=7d025999387104c976d2fb83c0e3158f","In order to solve the scalability problem in news recommendation, a scalable news recommendation method is proposed. The method includes the multi-dimensional similarity calculation, the Jaccard-Kmeans fast clustering and the Top-N recommendation. The multi-dimensional similarity calculation method is used to compute the integrated similarity between users, which considers abundant content feature of news, behaviors of users, and the time of these behaviors occurring. Based on traditional K-means algorithm, the Jaccard-Kmeans fast clustering method is proposed. This clustering method first computes the above multi-dimensional similarity, then generates multiple cluster centers with user behavior feature and news content feature, and evaluates the clustering results according to cohesiveness. The Top-N recommendation method integrates a time factor into the final recommendation. Experiment results prove that the proposed method can enhance the scalability of news recommendation, significantly improve the recommendation accuracy in condition of data sparsity, and improve the timeliness of news recommendation. © 2014 Elsevier B.V. All rights reserved.","Clustering; News recommendation; Similarity calculation"
"Development and validation of customized process models","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906946777&doi=10.1016%2fj.jss.2014.05.063&partnerID=40&md5=4a2285d29fe4fa40c61e0e5efa6a9756","Configurable reference process models encompass common and variable processes of organizations from different business domains. These reference process models are designed and reused to guide and derive customized business processes according to the requirements of stakeholders. The customization process is generally initiated by a configuration step, selecting a subset of the reference process model. Configuration is followed by a customization step, which assumes adapting or extending the configured business process based on the specific or unforeseen requirements. Hence, it is crucial to validate the correctness and compliance of the final customized business process with respect to the patterns and business constraints that are specified in the reference model. In this paper, we firstly introduce a technique to develop a customized process model and then present a set of identified inconsistency patterns that may happen during the configuration of a reference model and the customization of configured process models. Furthermore, we describe our proposed approach including formal representations and algorithms that provide logical reasoning and enable automatic inconsistency detection by leveraging description logic. In order to explore the scalability of the approach, we designed the experiments with various process models sizes and inconsistency distributions. The results of the experiments revealed the scalability of our approach with large size process models (500 activities). © 2014 Elsevier Inc.","Description logics; Feature models; Reference process models"
"Modeling and analysis of customer premise equipments registration process in IEEE 802.22 WRAN cell","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908307912&doi=10.1016%2fj.jss.2014.08.036&partnerID=40&md5=400aeec07a6aa9bb88e0234b7811d18c","The development of the IEEE 802.22 standard is aimed at providing broadband access in rural areas by effectively utilizing the unused TV band, provided no harmful interference is caused to the incumbent operation. The motivation behind TV band selection is of having lower frequencies compared to other licensed bands, which, therefore, results in lower propagation path loss. Due to this quality, the spectral power density of the radio signal reduces slowly, which results in a high coverage area. Further, it has been observed that many TV channels largely remain unoccupied, as most households and businesses rely on cable and satellite TV services. This is the first international standard for a wireless regional area network (WRAN) based on cognitive radio technologies. This standard provides both PHY and MAC layer functionalities in an infrastructure based network for communication between customer premise equipments (CPEs) through a base station (BS). The Spectrum Manager is the central part of the BS, which plays a significant role in maintaining spectrum availability information, channel selection, channel management, scheduling quiet periods for spectrum sensing, accessing to the database and implementing IEEE 802.22 policies. A WRAN can particularly accommodate up to 512 CPEs in a cell. Contention may occur during initial ranging, periodic ranging, bandwidth request and urgent coexistence situation notification. The medium access control (MAC) incorporates several schemes to control contention between CPEs within a cell and overlapping cells sharing the same channel. A CPE has to make decision to resolve collisions in the upstream direction. In the case of initial ranging and periodic ranging, code division multiple access (CDMA) is employed to resolve collisions. For bandwidth and UCS notification, either a CDMA or exponential time backoff approach can be applied for collision resolution. This paper presents the analytical framework to evaluate the number of active CPEs in a cognitive radio network. It is important to note that when the arrival rate becomes equal to the service rate, the active CPEs curve attains a constant value. Further, the active CPEs length is highly dependent on service rate. The different special cases have been addressed and the effectiveness of the proposed framework has been validated through various evaluation results. © 2014 Elsevier Inc. All rights reserved.","Customer premise equipment (CPE) Cognitive radio (CR) Wireless regional area network (WRAN)"
"DYSCS: A platform to build geographically and semantically enhanced social content sites","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902550240&doi=10.1016%2fj.jss.2014.03.059&partnerID=40&md5=6c7f64eb3268233668f5c7b2f0c235e8","Social content sites allow ordinary internet users to upload, edit, share, and annotate Web content with freely chosen keywords called tags. However, tags are only useful to the extent that they are processable by users and machines, which is often not the case since users frequently provide ambiguous and idiosyncratic tags. Thereby, many social content sites are starting to allow users to enrich their tags with semantic metadata, such as the GeoSocial Content Sites, for example, where users can annotate their tags with geographic metadata. But geographic metadata alone only unveils a very specific facet of a tag, which leads to the need for more general purpose semantic metadata. This paper introduces DYSCS - Do it Yourself Social Content Sites - a platform that combines Web 2.0 and Semantic Web technologies for assisting users in creating their own social content sites enriched with geographic and general purpose semantics. Moreover, DYSCS is highly reusable and interoperable, which are consequences of its ontology driven architecture. © 2014 Elsevier Inc.","GeoSocial Network; Semantic Web; Social content sites"
"Stakeholder logistics of an interactive system","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905859486&doi=10.1016%2fj.jss.2013.11.1095&partnerID=40&md5=39ce8e25f0721ac9208779b1702df672","Although it seems that software metrics have moved beyond mere performance measurement, it is not too clear how machine effectiveness, efficiency, and effort pertain to human requirements on such matters. In industry as well as academia, the ISO 9241-11 norm provides the dominant view on usability, stating that usability is a function of effectiveness, efficiency, and satisfaction. Although intuitively, usability requirements should be part of a software's design in an early stage, conceptually and empirically, it seems more likely that performance requirements (i.e., the absence of errors) should be the center of concern. This paper offers an elaborated view on usability, satisfaction, and performance. Certain theoretical conceptions are tested with data gathered from professional users of banking and hospital systems by means of a 4-year single-item survey and a structured questionnaire, respectively. Results suggested that performance factors (i.e., efficiency) are more important than usability in understanding why stakeholders are satisfied with a system or not. Moreover, it neither is dissatisfaction with a system nor that a system is less usable that predicate requirements change. Instead, avoiding machine inaccuracy best predicted the variability in agreement to ""must have"" requirements, while achieving human accuracy predicted the variability in agreement to the ""won't have"" requirements. The present contribution provides a consistent research framework that can bring more focus to design (i.e., prioritization), clarify discussions about design trade-offs, makes concepts measurable, and eventually may lead to better-informed designs. © 2014 Elsevier B.V. All rights reserved.","Effectiveness; Efficiency; Effort; Human-computer interaction; Performance; Usability"
"Scalable network file systems with load balancing and fault tolerance for web services","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900834589&doi=10.1016%2fj.jss.2014.02.057&partnerID=40&md5=e1faa333086eb4796397cd72b180d45f","Because of the rapid growth of the World Wide Web and the popularization of smart phones, tablets and personal computers, the number of web service users is increasing rapidly. As a result, large web services require additional disk space, and the required disk space increases with the number of web service users. Therefore, it is important to design and implement a powerful network file system for large web service providers. In this paper, we present three design issues for scalable network file systems. We use a variable number of objects within a bucket to decrease internal fragmentation in small files. We also propose a free space and access load-balancing mechanism to balance overall loading on the bucket servers. Finally, we propose a mechanism for caching frequently accessed data to lower the total disk I/O. These proposed mechanisms can effectively improve scalable network file system performance for large web services. © 2014 Elsevier Inc.","Fault tolerance; Load balancing; Network file system"
"Conceptual modeling of natural language functional requirements","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891601603&doi=10.1016%2fj.jss.2013.08.036&partnerID=40&md5=755850dc60bec09f29d103fbf299d17e","Requirements analysts consider a conceptual model to be an important artifact created during the requirements analysis phase of a software development life cycle (SDLC). A conceptual, or domain model is a visual model of the requirements domain in focus. Owing to its visual nature, the model serves as a platform for the deliberation of requirements by stakeholders and enables requirements analysts to further refine the functional requirements. Conceptual models may evolve into class diagrams during the design and execution phases of the software project. Even a partially automated conceptual model can save significant time during the requirements phase, by quickening the process of graphical communication and visualization. This paper presents a system to create a conceptual model from functional specifications, written in natural language in an automated manner. Classes and relationships are automatically identified from the functional specifications. This identification is based on the analysis of the grammatical constructs of sentences, and on Object Oriented principles of design. Extended entity-relationship (EER) notations are incorporated into the class relationships. Optimizations are applied to the identified entities during a post-processing stage, and the final conceptual model is rendered. The use of typed dependencies, combined with rules to derive class relationships offers a granular approach to the extraction of the design elements in the model. The paper illustrates the model creation process using a standard case study, and concludes with an evaluation of the usefulness of this approach for the requirements analysis. The analysis is conducted against both standard published models and conceptual models created by humans, for various evaluation parameters. © 2013 Elsevier Inc. All rights reserved.","Automated requirements analysis; Conceptual modeling; Natural language processing; Syntactic analysis"
"A recommendation framework for remote sensing images by spatial relation analysis","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894443068&doi=10.1016%2fj.jss.2013.12.030&partnerID=40&md5=587f9ef0c7283b3a1a8279fafee3dab9","In recent years, Remote Sensing Images (RS-Images) are widely recognized as an essential geospatial data due to their superior ability to offer abundant and instantaneous ground truth information. One of the active RS-Image approaches is the RS-Image recommendation from the Internet for meeting the user's queried Area-of-Interest (AOI). Although a number of studies on RS-Image ranking and recommendation have been proposed, most of them only consider the spatial distance between RS-Image and AOI. It is inappropriate since both of the RS-Image and AOI not only have the spatial information but also the cover range information. In this paper, we propose a novel framework named Location-based rs-Image Finding Engine (LIFE) to rank and recommend a series of relevant RS-Images to users according to the user-specific AOI. In LIFE, we first propose a cluster-based RS-Image index structure to efficiently maintain the large amount of RS-Images. Then, two quantitative indicators named Available Space (AS) and Image Extension (IE) are proposed to measure the Extensibility and Centrality between RS-Image and AOI, respectively. To our best knowledge, this is the first work on RS-Image recommendation that considers the issues of extensibility and centrality simultaneously. Through comprehensive experimental evaluations, the experiment result shows that both indicators have their own distinguished ranking behaviors and are able to successfully recommend meaningful RS-Image results. Besides, the experimental results show that the proposed LIFE framework outperforms the state-of-the-art approach Hausdorff in terms of Precision, Recall and Normalized Discounted Cumulative Gain (NDCG). © 2014 Elsevier Inc.","Data mining; Remote sensing image; Spatial ranking"
"Avoiding, finding and fixing spreadsheet errors - A survey of automated approaches for spreadsheet QA","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902547222&doi=10.1016%2fj.jss.2014.03.058&partnerID=40&md5=c381ce8abf38e735ea134bc9deeb6d38","Spreadsheet programs can be found everywhere in organizations and they are used for a variety of purposes, including financial calculations, planning, data aggregation and decision making tasks. A number of research surveys have however shown that such programs are particularly prone to errors. Some reasons for the error-proneness of spreadsheets are that spreadsheets are developed by end users and that standard software quality assurance processes are mostly not applied. Correspondingly, during the last two decades, researchers have proposed a number of techniques and automated tools aimed at supporting the end user in the development of error-free spreadsheets. In this paper, we provide a review of the research literature and develop a classification of automated spreadsheet quality assurance (QA) approaches, which range from spreadsheet visualization, static analysis and quality reports, over testing and support to model-based spreadsheet development. Based on this review, we outline possible opportunities for future work in the area of automated spreadsheet QA. © 2014 Elsevier Inc.","Quality assurance; Spreadsheet; Tool support"
"Dynamic adaptation of service compositions with variability models","2014","Journal of Systems and Software","10.1016/j.jss.2013.06.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900626386&doi=10.1016%2fj.jss.2013.06.034&partnerID=40&md5=e39b75edeb0604a46e0439760cff8d70","Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime. © 2013 Elsevier Inc.","Autonomic computing; Constraint programming; Dynamic adaptation; Dynamic software product line; Models at runtime; Variability; Verification; Web service composition"
"Imperceptible visible watermarking based on postcamera histogram operation","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905869680&doi=10.1016%2fj.jss.2014.04.038&partnerID=40&md5=0fdbd2ffe15a69d3dd6f70ef4f5a8676","A real-world scene captured via digital devices, such as a digital still camera, video recorder and mobile device, is a common behavior in recent decades. With the increasing availability, reproduction and sharing of media, the intellectual property of digital media is incapable of guaranty. To claim the ownership of digital camera media, the imperceptible visible watermarking (IVW) mechanism was designed based on the observation that most camera devices contain the postcamera histogram operation. The IVW approach can achieve advantages both the content readability of invisible watermarking methodology and the visual ownership identification of visible watermarking methodology. The computational complexity of IVW is low and can be effectively applied to almost any of the digital electronic devices when capturing the real-world scene without additional instruments. The following results and analysis demonstrate the novel scheme is effective and applicable for versatile images and videos captured. © 2014 Elsevier B.V. All rights reserved.","Digital watermarking; Histogram operation; Imperceptible"
"Memory leak detection in Java: Taxonomy and classification of approaches","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928959066&doi=10.1016%2fj.jss.2014.06.005&partnerID=40&md5=ff9807c9249797e21d771e910a104821","Memory leaks are usually not associated with runtime environments with automatic garbage collection; however, memory leaks do happen in such environments and present a challenge to detect and find a root cause. Currently in the industry manual heap dump analysis is the most popular way of finding memory leaks, regardless of the number of automated methods proposed by scientists over the years. However, heap dump analysis alone cannot answer all questions needed to fix the leak effectively. The current paper reviews memory leak detection approaches proposed over the years and classifies them from the point of view of assessed metrics, performance overhead and intrusiveness. In addition, we classify the methods into online, offline and hybrid groups based on their features. © 2014 Elsevier Inc.","Garbage collection; Java; Memory leak detection"
"Software product management - An industry evaluation","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905828775&doi=10.1016%2fj.jss.2013.12.042&partnerID=40&md5=cc1c02073e2adfe1460061bd5e0e575e","Product management is a key success factor for software products as it spans the entire life-cycle and thus ensures both a technical and business perspective. With its many interfaces to various business processes and stakeholders across the life-cycle, it is a primary driver for requirements engineering in its focus on value-orientation and consistency across releases. This article provides an overview on product management in software and IT. It summarizes experiences with introducing, improving and deploying the role of a product manager. In order to get a profound industry overview we performed a field study with interviews and concrete insight across fifteen different organizations world-wide on the role of the product manager and its success factors. As a technical solution we present four success factors identified from the research and show how they address the challenges we identified in practice. The novel part of this research and technical study is the industry survey and evaluation of resulting solution proposals. We found that with increasing institutionalization of a consistent and empowered product management role, the success rate of projects in terms of schedule predictability, quality and project duration improves. © 2014 Elsevier B.V. All rights reserved.","Industry survey; Product management; Software business"
"Evolutionary instance selection for text classification","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894484786&doi=10.1016%2fj.jss.2013.12.034&partnerID=40&md5=bfe141612a830c6d8cf8ce732a4d0ab7","Text classification is usually based on constructing a model through learning from training examples to automatically classify text documents. However, as the size of text document repositories grows rapidly, the storage requirement and computational cost of model learning become higher. Instance selection is one solution to solve these limitations whose aim is to reduce the data size by filtering out noisy data from a given training dataset. In this paper, we introduce a novel algorithm for these tasks, namely a biological-based genetic algorithm (BGA). BGA fits a ""biological evolution"" into the evolutionary process, where the most streamlined process also complies with the reasonable rules. In other words, after long-term evolution, organisms find the most efficient way to allocate resources and evolve. Consequently, we can closely simulate the natural evolution of an algorithm, such that the algorithm will be both efficient and effective. The experimental results based on the TechTC-100 and Reuters-21578 datasets show the outperformance of BGA over five state-of-the-art algorithms. In particular, using BGA to select text documents not only results in the largest dataset reduction rate, but also requires the least computational time. Moreover, BGA can make the k-NN and SVM classifiers provide similar or slightly better classification accuracy than GA. © 2014 Elsevier Inc.","Genetic algorithms; Instance selection; Text classification"
"A reliability model for Service Component Architectures","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895063075&doi=10.1016%2fj.jss.2013.11.002&partnerID=40&md5=a4f020863805b05437e79e19e0ebaf10","Service-oriented applications are dynamically built by assembling existing, loosely coupled, distributed, and heterogeneous services. Predicting their reliability is very important to appropriately drive the selection and assembly of services, to evaluate design feasibility, to compare design alternatives, to identify potential failure areas and to maintain an acceptable reliability level under environmental extremes. This article presents a model for predicting reliability of a service-oriented application based on its architecture specification in the lightweight formal language SCA-ASM. The SCA-ASM component model is based on the OASIS standard Service Component Architecture for heterogeneous service assembly and on the formal method Abstract State Machines for modeling service behavior, interactions, and orchestration in an abstract but executable way. The proposed method provides an automatic and compositional means for predicting reliability both at system-level and component-level by combining a reliability model for an SCA assembly involving SCA-ASM components, and a reliability model of an SCA-ASM component. It exploits ideas from architecture-based and path-based reliability models. A set of experimental results shows the effectiveness of the proposed approach and its comparison with a state-of-the art BPEL-based approach. © 2013 Elsevier Inc.","Abstract State Machines; Service Component Architecture; Software reliability models"
"Broker-based SLA-aware composite service provisioning","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906949499&doi=10.1016%2fj.jss.2014.06.027&partnerID=40&md5=cc07cad7d2be35dd88673a97acb85d2f","QoS-aware service composition aims to satisfy users' quality of services (QoS) needs during service composition. Traditional methods simply attempt to maximize user satisfaction by provisioning the composite service instance with the best QoS. These ""best-effort"" methods fail to take into account that there also exist other consumers competing for the service resources and their decisions of service selection/composition can impact on QoS. Since user's QoS needs can be met once the demanded level is reached, in this paper, we propose an ""on-demand"" strategy for QoS-aware service composition to replace the traditional ""best-effort"" strategy. The service broker is introduced to facilitate implementation of this strategy: it first purchases a number of service instances for each component from providers and then provisions the composite services with different QoS classes to consumers. This paper focuses on how the broker follows the service level agreement (SLA) to provision composite services in the ""on-demand"" manner. This problem is formally expressed as the minimization of the QoS distance function between SLA and QoS of composite service instances, under a series of constraints. Heuristic approaches are proposed for the problem and experiments are conducted at last to verify their effectiveness and efficiency. © 2014 Elsevier Inc.","On-demand provisioning; Service composition; Service level agreement"
"GPU accelerated pivoting rules for the simplex algorithm","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906949136&doi=10.1016%2fj.jss.2014.04.047&partnerID=40&md5=6ec40b77f3a7c998d3964473df968308","Simplex type algorithms perform successive pivoting operations (or iterations) in order to reach the optimal solution. The choice of the pivot element at each iteration is one of the most critical step in simplex type algorithms. The flexibility of the entering and leaving variable selection allows to develop various pivoting rules. In this paper, we have proposed some of the most well-known pivoting rules for the revised simplex algorithm on a CPU-GPU computing environment. All pivoting rules have been implemented in MATLAB and CUDA. Computational results on randomly generated optimal dense linear programs and on a set of benchmark problems (Netlib-optimal, Kennington, Netlib-infeasible, Mészáros) are also presented. These results showed that the proposed GPU implementations of the pivoting rules outperform the corresponding CPU implementations. © 2014 Elsevier Inc.","Compute Unified Device Architecture; Graphical Processing Unit; Linear programming; MATLAB; Pivoting rules; Simplex algorithm"
"The social smart grid: Dealing with constrained energy resources through social coordination","2014","Journal of Systems and Software","10.1016/j.jss.2013.04.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895064618&doi=10.1016%2fj.jss.2013.04.052&partnerID=40&md5=810ea67f6209b31f7033ffd5ed09a6b3","The smart grid promises to improve the efficiency and reliability of tomorrow's energy supply. One of the biggest achievements of future smart grids will be their distributed mode of operation which effectively eliminates vulnerable nodes causing single points of failures in the grid. However, due to the lack of centralized energy production and control, the coordination of energy consumption becomes first priority. Because there do not exist technologies to store energy at large-scale yet, all energy that is required must be produced at the same time. The biggest challenge of energy producers is therefore to reliably predict and provide the right amount of required energy to avoid shortages and breakdowns. In this paper, we propose a novel way to let smart grid stakeholders, i.e., energy producers and consumers, coordinate their energy demands themselves. For that purpose we combine traditional social network models and service-oriented computing concepts with the smart grid to allow consumers to form communities according to their energy consumption behavior. These communities enable them to interact with other grid stakeholders to coordinate energy consumption plans and set up private energy sharing alliances. This way, the utility provider and industrial energy producers can rely on a better predictable and a smoother energy demand of customers. We introduce a software framework, making use of widely adopted standards, demonstrate its feasibility with an agent-based simulation, and discuss its overall applicability. © 2013 Elsevier Inc.","Community-driven energy sharing; Energy consumption balancing; Social smart grid"
"Uncertainty handling in goal-driven self-optimization - Limiting the negative effect on adaptation","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894477622&doi=10.1016%2fj.jss.2013.12.033&partnerID=40&md5=426e23a683140ab679e3d61a754a28d3","Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling. © 2014 Elsevier Inc.","Goal-driven self-optimization; Requirements goal models; Uncertainty"
"Hybrid address spaces: A methodology for implementing scalable high-level programming models on non-coherent many-core architectures","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908162331&doi=10.1016%2fj.jss.2014.06.058&partnerID=40&md5=cab0f03ff02310cbdd0ce1902b9ae578","This paper introduces hybrid address spaces as a fundamental design methodology for implementing scalable runtime systems on many-core architectures without hardware support for cache coherence. We use hybrid address spaces for an implementation of MapReduce, a programming model for large-scale data processing, and the implementation of a remote memory access (RMA) model. Both implementations are available on the Intel SCC and are portable to similar architectures. We present the design and implementation of HyMR, a MapReduce runtime system whereby different stages and the synchronization operations between them alternate between a distributed memory address space and a shared memory address space, to improve performance and scalability. We compare HyMR to a reference implementation and we find that HyMR improves performance by a factor of 1.71× over a set of representative MapReduce benchmarks. We also compare HyMR with Phoenix++, a state-of-art implementation for systems with hardware-managed cache coherence in terms of scalability and sustained to peak data processing bandwidth, where HyMR demonstrates improvements of a factor of 3.1× and 3.2× respectively. We further evaluate our hybrid remote memory access (HyRMA) programming model and assess its performance to be superior of that of message passing. © 2014 Elsevier Inc. All rights reserved.","Many-core processors; Parallel programming models; Runtime systems"
"A critical examination of recent industrial surveys on agile method usage","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902546443&doi=10.1016%2fj.jss.2014.03.041&partnerID=40&md5=9857f4e8fccf2132fea653ffc48a7cca","Context Practitioners and researchers often claim that agile methods have moved into the mainstream for the last few years. To support this claim they refer to recent industrial surveys which tend to report high rates of agile method usage. However many of these industrial surveys are conducted by agile consultants, tool vendors, professional societies and independent technology and market research organizations. This raises some important concerns about the possible conflict of interest and the overall trustworthiness of these studies. Objective In response to the above concerns, a secondary study was carried out. Its objective was to examine industrial surveys published in 2011 and 2012, determine the extent to which we could trust their reported high rates of agile method usage and provide recommendations on how quality of research could be improved in the future. Method Following a rigorous search procedure, nine industrial surveys on agile method usage published in 2011 and 2012 were extracted from both academia and industry. Their thoroughness in reporting and trustworthiness were evaluated using a newly proposed assessment framework based on Guba's four attributes of trustworthiness (truth value, applicability, consistency and neutrality) and a number of methods for assessing survey research in related fields as information, communication and management studies. Results The careful examination of the reviewed surveys shows that most of the studies have insufficient thoroughness in reporting and (subsequently) low trustworthiness. Only one (out of nine) study is considered as a scientific contribution in determining the current 2011/2012 rate of agile method usage. Conclusions The obtained results support our initial considerations about the trustworthiness of recent industrial surveys on agile method usage and suggest a number of recommendations for increasing the quality and value of future survey research in this regard. © 2014 Elsevier Inc.","Agile software development; Research synthesis; Survey research"
"Investigating the applicability of Agility assessment surveys: A case study","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908296633&doi=10.1016%2fj.jss.2014.08.067&partnerID=40&md5=25b67a603de1079f32d2c405053b1b9b","Context: Agile software development has become popular in the past decade without being sufficiently defined. The Agile principles can be instantiated differently which creates different perceptions of Agility. This has resulted in several frameworks being presented in the research literature to evaluate the level of Agility. However, the evidence of their actual use in practice is limited.; Objective: The objective is to identify online surveys that assess/profile Agility in practice, and to evaluate the surveys in an industrial setting.; Method: The Agility assessment surveys were identified through searching the web. Then, they were explored and two surveys were identified as most promising for our objective. The selected surveys were evaluated in a case study with three Agile teams in a software consultancy company.; Results: Each team and its customer separately judged the team's Agility. This outcome was compared with the two survey results in focus-group meetings, and finally one of the surveys was agreed to provide a more holistic assessment of Agility.; Conclusions: Different surveys may judge Agility differently, which supports the viewpoint that Agile is not well defined. Therefore, practitioners must decide what Agile means to them and select the assessment survey that matches their definition. © 2014 Elsevier Inc. All rights reserved.","Abbreviations CI Continuous integration GSD Global software development PP Pair programming RQ Research question S1 Survey 1 S2 Survey 2 TDD Test driven development TR Trouble report T1 Team 1 T2 Team 2 T3 Team 3 XP Extreme programming"
"A component- and connector-based approach for end-user composite web applications development","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902544540&doi=10.1016%2fj.jss.2014.03.039&partnerID=40&md5=db2a4dff14405dc742aa881a324b4fcb","Enabling real end-user development is the next logical stage in the evolution of Internet-wide service-based applications. Successful composite applications rely on heavyweight service orchestration technologies that raise the bar far above end-user skills. This weakness can be attributed to the fact that the composition model does not satisfy end-user needs rather than to the actual infrastructure technologies. In our opinion, the best way to overcome this weakness is to offer end-to-end composition from the user interface to service invocation, plus an understandable abstraction of building blocks and a visual composition technique empowering end users to develop their own applications. In this paper, we present a visual framework for end users, called FAST, which fulfils this objective. FAST implements a novel composition model designed to empower non-programmer end users to create and share their own self-service composite applications in a fully visual fashion. We projected the development environment implementing this model as part of the European FP7 FAST Project, which was used to validate the rationale behind our approach. © 2014 Elsevier Inc.","Composition visual languages and techniques; User-centred service-oriented architectures; Visual component-based programming"
"Existence of dumb nodes in stationary wireless sensor networks","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900633108&doi=10.1016%2fj.jss.2013.12.039&partnerID=40&md5=93580cb6041c8324f992c8c5b5bb34e3","Wireless sensor networks (WSNs), which are typically autonomous and unattended, require energy-efficient and fault-tolerant protocols to maximize the network lifetime and operations. In this work, we consider a previously unexplored aspect of the sensing nodes - dumb behavior. A sensor node is termed as ""dumb"", when it can sense its surroundings, but cannot communicate with its neighbors due to shrinkage in communication range attributed to adverse environmental effects and can behave normally in the presence of favorable environment. As a result of this temporary behavior, a node may get isolated from the network when adverse environmental effects are present, but re-connects with the network with the resumption of favorable environmental conditions. We consider the effects of dumb nodes on the, otherwise, energy-efficient stationary WSNs having complete network coverage achieved using sufficient number of activated sensor nodes. While the presence of redundancy in the deployment of nodes, or the number of active nodes can guarantee communication opportunities, such deployment is not necessarily energy-efficient and cost-effective. The dumb behavior of nodes results in wastage of power, thereby reducing the lifetime of a network. Such effects can be detrimental to the performance of WSN applications. The simulation results exhibit that the network performance degrades in the presence of dumb nodes in stationary WSNs. © 2014 Elsevier Inc.","Dumb nodes; Environmental effects; Wireless sensor networks"
"A weight-aware channel assignment algorithm for mobile multicast in wireless mesh networks","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902535782&doi=10.1016%2fj.jss.2014.03.040&partnerID=40&md5=e42ce9321aa06424ae5ce6fa1b99f221","Wireless mesh networks (WMNs) are one of key technologies for next generation wireless networks. In this paper, we propose a heuristic channel assignment algorithm with weight awareness to support mobile multicast in WMNs. To enhance network throughput, our algorithm is based on the path forwarding weight to perform channel assignment. In addition to non-overlapping channels, partially-overlapping channels are also used in channel assignment. To fully exploit all available channels in channel assignment, we devise a new channel selection metric to consider the channel separation and the distance between nodes. In mobile multicast, the multicast tree structure cannot be fixed due to receiver (multicast member) mobility. The change of the multicast tree structure will result in channel re-assignment. The proposed algorithm is based on a critical-event driven manner to reduce the times of channel re-assignment as much as possible. Finally, we perform simulation experiments to show the effectiveness of the proposed channel assignment algorithm. © 2014 Elsevier Inc.","Channel assignment; Mobile multicast; Wireless mesh network"
"Guilt-based handling of software performance antipatterns in palladio architectural models","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905826841&doi=10.1016%2fj.jss.2014.03.081&partnerID=40&md5=ec4801a3928b277d7171abf5efce354b","Antipatterns are conceptually similar to patterns in that they document recurring solutions to common design problems. Software performance antipatterns document common performance problems in the design as well as their solutions. The definition of performance antipatterns concerns software properties that can include static, dynamic, and deployment aspects. To make use of such knowledge, we propose an approach that helps software architects to identify and solve performance antipatterns. Our approach provides software performance feedback to architects, since it suggests the design alternatives that allow overcoming the detected performance problems. The feedback process may be quite complex since architects may have to assess several design options before achieving the architectural model that best fits the end-user expectations. In order to optimise such process we introduce a ranking methodology that identifies, among a set of detected antipatterns, the ""guilty"" ones, i.e. the antipatterns that more likely contribute to the violation of specific performance requirements. The introduction of our ranking process leads the system to converge towards the desired performance improvement by discarding a consistent part of design alternatives. Four case studies in different application domains have been used to assess the validity of the approach. © 2014 Elsevier B.V. All rights reserved.","Architectural feedback; Palladio architectural models; Software performance antipatterns"
"Blending design patterns with aspects: A quantitative study","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908287365&doi=10.1016%2fj.jss.2014.08.041&partnerID=40&md5=8a4c05817c529a9687368be8ca395007","Design patterns often need to be blended (or composed) when they are instantiated in a software system. The composition of design patterns consists of assigning multiple pattern elements into overlapping sets of classes in a software system. Whenever the modularity of each design pattern is not preserved in the source code, their implementation becomes tangled with each other and with the classes' core responsibilities. As a consequence, the change or removal of each design pattern will be costly or prohibitive as the software system evolves. In fact, composing design patterns is much harder than instantiating them in an isolated manner. Previous studies have found design pattern implementations are naturally crosscutting in object-oriented systems, thereby making it difficult to modularly compose them. Therefore, aspect-oriented programming (AOP) has been pointed out as a natural alternative for modularizing and blending design patterns. However, there is little empirical knowledge on how AOP models influence the composability of widely used design patterns. This paper investigates the influence of using AOP models for composing the Gang-of-Four design patterns. Our study categorizes different forms of pattern composition and studies the benefits and drawbacks of AOP in these contexts. We performed assessments of several pair-wise compositions taken from 3 medium-sized systems implemented in Java and two AOP models, namely, AspectJ and Compose∗. We also considered complex situations where more than two patterns involved in each composition, and the patterns were interacting with other aspects implementing other crosscutting concerns of the system. In general, we observed two dominant factors impacting the pattern composability with AOP: (i) the category of the pattern composition, and (ii) the AspectJ idioms used to implement the design patterns taking part in the composition. © 2014 Elsevier Inc. All rights reserved.","Design patterns Aspect-oriented programming Composability Empirical studies Metrics"
"Delta-oriented model-based integration testing of large-scale systems","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900631200&doi=10.1016%2fj.jss.2013.11.1096&partnerID=40&md5=4fd79d5bcdc7f5d53b38f2d50b217e7f","Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain. © 2013 Elsevier Inc.","Large-scale systems; Model-based testing; Regression testing; Variable software architectures"
"Processes versus people: How should agile software development maturity be defined?","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908151047&doi=10.1016%2fj.jss.2014.07.030&partnerID=40&md5=1ab07222c390ec5c12e7e4b209bf72c3","Maturity in software development is currently defined by models such as CMMI-DEV and ISO/IEC 15504, which emphasize the need to manage, establish, measure and optimize processes. Teams that develop software using these models are guided by defined, detailed processes. However, an increasing number of teams have been implementing agile software development methods that focus on people rather than processes. What, then, is maturity for these agile teams that focus less on detailed, defined processes? This is the question we sought to answer in this study. To this end, we asked agile practitioners about their perception of the maturity level of a number of practices and how they defined maturity in agile software development. We used cluster analysis to analyze quantitative data and triangulated the results with content analysis of the qualitative data. We then proposed a new definition for agile software development maturity. The findings show that practitioners do not see maturity in agile software development as process definition or quantitative management capabilities. Rather, agile maturity means fostering more subjective capabilities, such as collaboration, communication, commitment, care, sharing and self-organization. © 2014 Elsevier Inc. All rights reserved.","Agile software development; Maturity; Software process improvement"
"Radigost: Interoperable web-based multi-agent platform","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894477759&doi=10.1016%2fj.jss.2013.12.029&partnerID=40&md5=61658d1244d73d959522d1866141120c","Recent improvements of web development technologies, commonly referred to as HTML5, have resulted in an excellent framework for developing a fully-featured, purely web-based multi-agent platform. This paper presents an architecture of such a platform, named Radigost. Radigost agents and parts of the system itself are implemented in JavaScript and executed inside the client's web browser, while an additional set of Java-based components is deployed on an enterprise application server. Radigost is platform-independent, capable of running, without any prior installation or configuration steps, on a wide variety of software and hardware configurations, including personal computers, smartphones, tablets, and modern television sets. The system is standards-compliant and fully interoperable, in the sense that its agents can transparently interact with agents in existing, third-party multi-agent solutions. Finally, performance evaluation results show that the execution speed of Radigost is comparable to that of a non web-based implementation. © 2014 Elsevier Inc.","Distributed architecture; Multi-agent platform; Software agents; Web development"
"Extended U+F social network protocol: Interoperability, reusability, data protection and indirect relationships in web based social networks","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902548055&doi=10.1016%2fj.jss.2014.04.044&partnerID=40&md5=1a1cd1e3bcd1f2ff5714dd1b9dde7a1f","An interconnected world is what current technologies look for, being Web Based Social Networks (WBSNs) a promising development in this regard. Four desirable WBSN features are identified, namely, interoperability, reusability, protection against WBSNs providers and indirect relationships. A protocol, called U+F, addressed interoperability and reusability of identity data, resources and access control policies between different WBSNs. In order to address the remaining couple of features, that is, achieving the protection of data against WBSNs providers and indirect relationships management across different WBSNs, this paper presents eU+F, an extension of U+F. A prototype is developed to verify the feasibility of implementing the proposed protocol in a real environment, as well as to compare its workload regarding three well-known WBSNs, Facebook, MySpace and LinkedIn. © 2014 Elsevier Inc.","Data disclosures; Interoperability; Web Based Social Networks"
"O1FS: Flash file system with O(1) crash recovery time","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908205934&doi=10.1016%2fj.jss.2014.07.008&partnerID=40&md5=91e19cf42d461a9e7f4faa14751ef4ce","The crash recovery time of NAND flash file systems increases with flash memory capacity. Crash recovery usually takes several minutes for a gigabyte of flash memory and becomes a serious problem for mobile devices. To address this problem, we propose a new flash file system, O1FS. A key concept of our system is that a small number of blocks are modified exclusively until we change the blocks explicitly. To recover from crashes, O1FS only accesses the most recently modified blocks rather than the entire flash memory. Therefore, the crash recovery time is bounded by the size of the blocks. We develop mathematical models of crash recovery techniques and prove that the time complexity of O1FS is O(1), whereas that of other methods is proportional to the number of blocks in the flash memory. Our evaluation shows that the crash recovery of O1FS is about 18.5 times faster than that of a state-of-the-art method. © 2014 Elsevier Inc. All rights reserved.","Crash recovery technique; NAND flash file system; O1FS"
"Sources of value in application ecosystems","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906947819&doi=10.1016%2fj.jss.2014.05.064&partnerID=40&md5=3d6694000bd92cd0012d48e6690546c6","Mobile application stores have revolutionised the dynamics of mobile ecosystems. Research on mobile application ecosystems has been significantly driven by data that is focused on the visualisation of an ecosystem's dynamics. This is a valuable step towards understanding the nature of the ecosystems, but it is limited in its explanatory power. Thus, a theory-driven approach is needed to understand the overall dynamics of such systems. This study applies a theoretical framework of value creation in e-business in the context of mobile application ecosystems, with a focus on application developers. A qualitative research strategy is employed in testing operationalisation in a sample of developers. The sample comprises 27 application developers from the three leading mobile application ecosystems. The results show that efficiency is the main source of value, products seldom create value through complementarities, and approaches towards lock-in and novelty seem to vary among application developers. The managerial and theoretical implications of such biased value creation in mobile ecosystems are considered. © 2014 Elsevier Inc.","App economy; Software ecosystem; Value creation"
"Using a grounded theory approach for exploring software product management challenges","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905817067&doi=10.1016%2fj.jss.2014.03.050&partnerID=40&md5=28fa984b768359fb2fe0ae0ae2ff412d","The traditional requirements engineering (RE) research paradigm, along with most engineering research and practice, is commonly seen to belong to the philosophical tradition of positivism, which construes knowledge as accruing through the systematic observation of stable and knowable phenomena. Consequently, RE methods tend to ignore social issues. However, due to the dominant role of the human being in RE, there has been an increasing need to rely on research methods of the social sciences, arts, and humanities for RE related findings. This paper illustrates one example of how social aspects in RE have been explored with a research method adopted from social sciences research tradition. Drawing heavily on the research reported in the doctoral thesis of the principal author, we describe in this paper: (1) how a study using a grounded theory approach was designed and conducted for exploring market-driven requirements engineering (MDRE) challenges in seven companies, (2) how the analysis eventually proceeded toward a proposed theory, and (3) our experiences of using a grounded theory approach within the discipline of RE. © 2014 Elsevier B.V. All rights reserved.","Grounded theory; Qualitative research; Requirements engineering"
"A systematic literature review on the industrial use of software process simulation","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908160600&doi=10.1016%2fj.jss.2014.06.059&partnerID=40&md5=62c4c0af6fd2596cfc83de88d7820919","Context Software process simulation modelling (SPSM) captures the dynamic behaviour and uncertainty in the software process. Existing literature has conflicting claims about its practical usefulness: SPSM is useful and has an industrial impact; SPSM is useful and has no industrial impact yet; SPSM is not useful and has little potential for industry. Objective To assess the conflicting standpoints on the usefulness of SPSM. Method A systematic literature review was performed to identify, assess and aggregate empirical evidence on the usefulness of SPSM. Results In the primary studies, to date, the persistent trend is that of proof-of-concept applications of software process simulation for various purposes (e.g. estimation, training, process improvement, etc.). They score poorly on the stated quality criteria. Also only a few studies report some initial evaluation of the simulation models for the intended purposes. Conclusion There is a lack of conclusive evidence to substantiate the claimed usefulness of SPSM for any of the intended purposes. A few studies that report the cost of applying simulation do not support the claim that it is an inexpensive method. Furthermore, there is a paramount need for improvement in conducting and reporting simulation studies with an emphasis on evaluation against the intended purpose. © 2014 Elsevier Inc. All rights reserved.","Evidence based software engineering; Software process simulation; Systematic literature review"
"Search-based metamodel matching with structural and syntactic measures","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908213981&doi=10.1016%2fj.jss.2014.06.040&partnerID=40&md5=a7856eb7a85362e48021fd891afe6a3d","The use of different domain-specific modeling languages and diverse versions of the same modeling language often entails the need to translate models between the different languages and language versions. The first step in establishing a transformation between two languages is to find their corresponding concepts, i.e.; finding correspondences between their metamodel elements. Although, metamodels use heterogeneous terminologies and structures, they often still describe similar language concepts. In this paper, we propose to combine structural metrics (e.g.; number of properties per concept) and syntactic metrics to generate correspondences between metamodels. Because metamodel matching requires to cope with a huge search space of possible element combinations, we adapted a local and a global metaheuristic search algorithm to find the best set of correspondences between metamodels. The efficiency and effectiveness of our proposal is evaluated on different matching scenarios based on existing benchmarks. In addition, we compared our technique to state-of-the-art ontology matching and model matching approaches. © 2014 Elsevier Inc. All rights reserved.","Model matching; Search-based software engineering; Simulated annealing"
"FlexIQ: A flexible interactive Querying Framework by Exploiting the Skyline Operator","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908157710&doi=10.1016%2fj.jss.2014.07.011&partnerID=40&md5=b32b03fcb3f796f365d93e2d7a96d4f6","Skyline operator has gained much attention in the last decade and is proved to be valuable for multi-criteria decision making. This paper presents a novel Flexible Interactive Querying (FlexIQ) framework for user feedback-based Select-Project-Join (SPJ) query refinement in databases. In FlexIQ, the user feedback is used to discover the query intent. In addition, we have used the skyline operator to confine the search space of the proposed query refinement algorithms. The user feedback consists of both unexpected information currently present in the query output and expected information that is missing from the query output. Once the feedback is given by the user, our framework refines the initial query by exploiting the skyline operator to minimize the unexpected information as well as maximize the expected information in the refined query output. In our framework, the user can also control different quality metric such as quality of results (e.g.; false positive rates, false negative rates and accuracy) and complexity (i.e.; quantified as the number of subqueries) in the refined query. We have validated our framework both theoretically and experimentally. In particular, we have demonstrated the effectiveness of our proposed framework by comparing its performance with the naï ve decision tree based query refinement. © 2014 Elsevier Inc. All rights reserved.","Query refinement; Skyline operator; User feedback"
"Efficient unveiling of multi-members in a social network","2014","Journal of Systems and Software","10.1016/j.jss.2013.06.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902547807&doi=10.1016%2fj.jss.2013.06.061&partnerID=40&md5=5d668b68f45539dfc071553e20ec3d0a","With the rapid growth of the Web 2.0, the discovery of key actors in social networks, called influencers, mediators, ambassadors or experts, has recently received a renewed of attention. In this article, we consider a particular type of actor that we call a multi-member since he belongs to several communities. We introduce a methodological framework to identify these actors in a hypergraph, in which the vertices are the actors and the hyperedges are the communities. We also show that detecting such multi-members is similar to the problem of the determination of a subset of minimal transversals of a hypergraph. An efficient algorithm that relies on the connection between the definition of a multi-member and that of an essential itemset is also introduced. Experiments done on several datasets showed that the introduced algorithm outperforms the pioneering ones of the literature. © 2013 Elsevier Inc.","Essential itemset; Hypergraph; Minimal transversal; Social network"
"Waste identification as the means for improving communication in globally distributed agile software development","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905852633&doi=10.1016%2fj.jss.2014.03.080&partnerID=40&md5=9f6d4f61a14fd134164b4484bfdbcde0","Agile approaches highly values communication between team members to improve software development processes, even though, communication in globally distributed agile teams can be difficult. Literature proposes solutions for mitigating the challenges encountered in these environments. These solutions range from general-level recommendations and practices to the use of communication tools. However, an approach covering the whole development process for identifying challenges, and improving communication in globally distributed agile development projects, is missing. In order to address this, we conducted a case study within a globally distributed agile software development project focused on using the concept of waste as a lens for identifying non-value producing communication elements. In order to achieve this, we constructed a waste identification approach through which we identified five communication wastes, and solutions to mitigate them. These wastes can help companies identify communication issues that are present in their development efforts, while the presented waste identification technique gives them a mechanism for waste identification and mitigation. This work contributes to the scientific community by increasing the knowledge about communication in globally distributed agile development efforts. © 2014 Elsevier B.V. All rights reserved.","Communication; Distributed agile software development; Lean software development"
"Improved anti-forensics of JPEG compression","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900603142&doi=10.1016%2fj.jss.2013.12.043&partnerID=40&md5=8c63464542647b44866b08ddb3b5b9de","The comblike histogram of DCT coefficients on each subband and the blocking artifacts among adjacent blocks are the two main fingerprints of the image that was once compressed by JPEG. Stamm and Liu proposed an anti-forensics method for removing these fingerprints by dithering the DCT coefficients and adding noise into the pixels. However, some defects emerge inside the anti-forensically processed images. First, the noise distributions are abnormal in the resulting images; and second, the quality of the processed image is poor compared with the original image. To fill these gaps, this paper proposes an improved anti-forensics method for JPEG compression. After analyzing the noise distribution, we propose a denoising algorithm to remove the grainy noise caused by image dithering, and a deblocking algorithm to combat Fan and Queiroz's forensics method against blocking artifacts. With the proposed anti-forensics method, fingerprints of the comblike histograms and the blocking artifacts are removed, noise distribution abnormality is avoided, and the quality of the processed image is improved. © 2014 Elsevier Inc.","Anti-forensics; Blocking artifact; Image forensics"
"A formal methodology for integral security design and verification of network protocols","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895064183&doi=10.1016%2fj.jss.2013.09.020&partnerID=40&md5=5ad531b6bc0cd3e66be56fd1570825af","In this work we propose a methodology for incorporating the verification of the security properties of network protocols as a fundamental component of their design. This methodology can be separated in two main parts: context and requirements analysis along with its informal verification; and formal representation of protocols and the corresponding procedural verification. Although the procedural verification phase does not require any specific tool or approach, automated tools for model checking and/or theorem proving offer a good trade-off between effort and results. In general, any security protocol design methodology should be an iterative process addressing in each step critical contexts of increasing complexity as result of the considered protocol goals and the underlying threats. The effort required for detecting flaws is proportional to the complexity of the critical context under evaluation, and thus our methodology avoids wasting valuable system resources by analyzing simple flaws in the first stages of the design process. In this work we provide a methodology in coherence with the step-by-step goals definition and threat analysis using informal and formal procedures, being our main concern to highlight the adequacy of such a methodology for promoting trust in the accordingly implemented communication protocols. Our proposal is illustrated by its application to three communication protocols: MANA III, WEP's Shared Key Authentication and CHAT-SRP. © 2013 Elsevier Inc.","Formal model; Formal verification; Protocols security; Secure development methodology; WEP analysis"
"With a little help from new friends: Boosting information cascades in social networks based on link injection","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908308657&doi=10.1016%2fj.jss.2014.08.023&partnerID=40&md5=6d66e4d17b14247b45e9e18e413f65c9","We investigate information cascades in the context of viral marketing applications. Recent research has identified that communities in social networks may hinder cascades. To overcome this problem, we propose a novel method for injecting social links in a social network, aiming at boosting the spread of information cascades. Unlike the proposed approach, existing link prediction methods do not consider the optimization of information cascades as an explicit objective. In our proposed method, the injected links are being predicted in a collaborative-filtering fashion, based on factorizing the adjacency matrix that represents the structure of the social network. Our method controls the number of injected links to avoid an ""aggressive"" injection scheme that may compromise the experience of users. We evaluate the performance of the proposed method by examining real data sets from social networks and several additional factors. Our results indicate that the proposed scheme can boost information cascades in social networks and can operate as a ""people recommendations"" strategy complementary to currently applied methods that are based on the number of common neighbors (e.g., ""friend of friend"") or on the similarity of user profiles. © 2014 Elsevier Inc. All rights reserved.","Information cascades Viral marketing Social networks Matrix factorization"
"Face recognition based on curvelets and local binary pattern features via using local property preservation","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905855355&doi=10.1016%2fj.jss.2014.04.037&partnerID=40&md5=824570824efcc583d9d3016e97574095","In this paper, we propose a new feature extraction approach for face recognition based on Curvelet transform and local binary pattern operator. The motivation of this approach is based on two observations. One is that Curvelet transform is a new anisotropic multi-resolution analysis tool, which can effectively represent image edge discontinuities; the other is that local binary pattern operator is one of the best current texture descriptors for face images. As the curvelet features in different frequency bands represent different information of the original image, we extract such features using different methods for different frequency bands. Technically, the lowest frequency band component is processed using the local binary pattern method, and only the medium frequency band components are normalized. And then, we combine them to create a feature set, and use the local preservation projection to reduce its dimension. Finally, we classify the test samples using the nearest neighbor classifier in the reduced space. Extensive experiments on the Yale database, the extended Yale B database, the PIE pose 09 database, and the FRGC database illustrate the effectiveness of the proposed method. © 2014 Elsevier B.V. All rights reserved.","Curvelet transform; Face recognition; Local binary pattern; Local property preservation"
"Selecting software reliability growth models and improving their predictive accuracy using historical projects data","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908286057&doi=10.1016%2fj.jss.2014.08.033&partnerID=40&md5=10ddc7f9372a324d5c17f18627583a91","During software development two important decisions organizations have to make are: how to allocate testing resources optimally and when the software is ready for release. SRGMs (software reliability growth models) provide empirical basis for evaluating and predicting reliability of software systems. When using SRGMs for the purpose of optimizing testing resource allocation, the model's ability to accurately predict the expected defect inflow profile is useful. For assessing release readiness, the asymptote accuracy is the most important attribute. Although more than hundred models for software reliability have been proposed and evaluated over time, there exists no clear guide on which models should be used for a given software development process or for a given industrial domain. Using defect inflow profiles from large software projects from Ericsson, Volvo Car Corporation and Saab, we evaluate commonly used SRGMs for their ability to provide empirical basis for making these decisions. We also demonstrate that using defect intensity growth rate from earlier projects increases the accuracy of the predictions. Our results show that Logistic and Gompertz models are the most accurate models; we further observe that classifying a given project based on its expected shape of defect inflow help to select the most appropriate model. © 2014 Elsevier Inc. All rights reserved.","Embedded software Defect inflow Software reliability growth models"
"WAS: A weighted attribute-based strategy for cluster test selection","2014","Journal of Systems and Software","10.1016/j.jss.2014.08.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908301610&doi=10.1016%2fj.jss.2014.08.032&partnerID=40&md5=3e304c9c11f0a70589fe68158957f70a","In past decades, many techniques have been proposed to generate and execute test cases automatically. However, when a test oracle does not exist, execution results have to be examined manually. With increasing functionality and complexity of today's software, this process can be extremely time-consuming and mistake-prone. A CTS-based (cluster test selection) strategy provides a feasible solution to mitigate such deficiency by examining the execution results only with respect to a small number of selected test cases. It groups test cases with similar execution profiles into the same cluster and selects them from each cluster. Some well-known CTS-based strategies are one per cluster, n (a predefined value which is greater than 1) per cluster, adaptive sampling, and execution-spectra-based sampling (ESBS). The ultimate goal is to reduce testing cost by quickly identifying the executions that are likely to fail. However, improperly grouping the test cases will significantly diminish the effectiveness of these strategies (by examining results of more successful executions and fewer failed executions). To overcome this problem, we propose a weighted attribute-based strategy (WAS). Instead of clustering test cases based on the similarity of their execution profiles only once like the aforementioned CTS-based strategies, WAS will conduct more than one iteration of clustering using weighted execution profiles by also considering the suspiciousness of each program element (statement, basic block, decision, etc.), where the suspiciousness in terms of the likelihood of containing bugs can be computed by using various software fault localization techniques. Case studies using seven programs (make, ant, sed, flex, grep, gzip, and space) and four CTS-based strategies (one per cluster sampling, n per cluster sampling, adaptive sampling, and ESBS) were conducted to evaluate the effectiveness of WAS on 184 faulty versions containing either single or multiple bugs. Experimental results suggest that the proposed WAS strategy outperforms other four CTS-based strategies with respect to both recall and precision such that output verification is focused more strongly on failed executions. © 2014 Elsevier Inc. All rights reserved.","Weighted execution profile Cluster test selection Software fault localization"
"Measure-independent characterization of contrast optimal visual cryptography schemes","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905825612&doi=10.1016%2fj.jss.2014.03.079&partnerID=40&md5=adf50822d5cabf42e759ad46f303d609","Visual cryptography has been studied in two models and visual cryptography schemes have been evaluated using different contrast measures. Naor and Shamir introduced the deterministic model while Kafri and Keren introduced the random grid model. In the deterministic model, three different measures of contrast have been proposed, γns, γvv and γes, although only γns, has been thoroughly studied. Tight bounds on γns are known for several classes of schemes. In the random grid model the contrast is γrg. In this paper we focus the attention on the deterministic model and follow a measure-independent approach, which, by using the structural properties of the schemes, enables us to provide a characterization of optimal schemes that is independent of the specific measure used to assess the contrast. In particular we characterize and provide constructions of optimal schemes for the cases of (2, n)-threshold and (n, n)-threshold schemes. Then, we apply the measure-independent results to the three measures γns, γvv and γes, that have been used in the literature obtaining both new characterizations and constructions of optimal schemes as well as alternative proofs of known results. Finally we provide a connection between the deterministic and the random grid models showing that γes is the equivalent of γrg. This opens up a door between the two models which have been so far treated separately. © 2014 Elsevier B.V. All rights reserved.","Contrast optimal schemes; Secret sharing; Visual cryptography"
"A dynamic code coverage approach to maximize fault localization efficiency","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894483083&doi=10.1016%2fj.jss.2013.12.036&partnerID=40&md5=f54c3698c3461e53ad4149d86fa2e405","Spectrum-based fault localization is amongst the most effective techniques for automatic fault localization. However, abstractions of program execution traces, one of the required inputs for this technique, require instrumentation of the software under test at a statement level of granularity in order to compute a list of potential faulty statements. This introduces a considerable overhead in the fault localization process, which can even become prohibitive in, e.g.; resource constrained environments. To counter this problem, we propose a new approach, coined dynamic code coverage (DCC), aimed at reducing this instrumentation overhead. This technique, by means of using coarser instrumentation, starts by analyzing coverage traces for large components of the system under test. It then progressively increases the instrumentation detail for faulty components, until the statement level of detail is reached. To assess the validity of our proposed approach, an empirical evaluation was performed, injecting faults in six real-world software projects. The empirical evaluation demonstrates that the dynamic code coverage approach reduces the execution overhead that exists in spectrum-based fault localization, and even presents a more concise potential fault ranking to the user. We have observed execution time reductions of 27% on average and diagnostic report size reductions of 77% on average. © 2014 Elsevier Inc.","Dynamic coverage; Software diagnosis; Spectrum-based fault localization"
"Distributed collaborative filtering with singular ratings for large scale recommendation","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905867335&doi=10.1016%2fj.jss.2014.04.045&partnerID=40&md5=af4557a40f9bbd4bc45100c43ea1538e","Collaborative filtering (CF) is an effective technique addressing the information overloading problem, where each user is associated with a set of rating scores on a set of items. For a chosen target user, conventional CF algorithms measure similarity between this user and other users by utilizing pairs of rating scores on common rated items, but discarding scores rated by one of them only. We call these comparative scores as dual ratings, while the non-comparative scores as singular ratings. Our experiments show that only about 10% ratings are dual ones that can be used for similarity evaluation, while the other 90% are singular ones. In this paper, we propose SingCF approach, which attempts to incorporate multiple singular ratings, in addition to dual ratings, to implement collaborative filtering, aiming at improving the recommendation accuracy. We first estimate the unrated scores for singular ratings and transform them into dual ones. Then we perform a CF process to discover neighborhood users and make predictions for each target user. Furthermore, we provide a MapReduce-based distributed framework on Hadoop for significant improvement in efficiency. Experiments in comparison with the state-of-the-art methods demonstrate the performance gains of our approaches. © 2014 Elsevier B.V. All rights reserved.","Collaborative filtering; Distributed framework; Recommender systems"
"Twitter data analysis by means of Strong Flipping Generalized Itemsets","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902548199&doi=10.1016%2fj.jss.2014.03.060&partnerID=40&md5=925f10c1b34d78830d73b61a53d666ad","Twitter data has recently been considered to perform a large variety of advanced analysis. Analysis of Twitter data imposes new challenges because the data distribution is intrinsically sparse, due to a large number of messages post every day by using a wide vocabulary. Aimed at addressing this issue, generalized itemsets - sets of items at different abstraction levels - can be effectively mined and used to discover interesting multiple-level correlations among data supplied with taxonomies. Each generalized itemset is characterized by a correlation type (positive, negative, or null) according to the strength of the correlation among its items. This paper presents a novel data mining approach to supporting different and interesting targeted analysis - topic trend analysis, context-aware service profiling - by analyzing Twitter posts. We aim at discovering contrasting situations by means of generalized itemsets. Specifically, we focus on comparing itemsets discovered at different abstraction levels and we select large subsets of specific (descendant) itemsets that show correlation type changes with respect to their common ancestor. To this aim, a novel kind of pattern, namely the Strong Flipping Generalized Itemset (SFGI), is extracted from Twitter messages and contextual information supplied with taxonomy hierarchies. Each SFGI consists of a frequent generalized itemset X and the set of its descendants showing a correlation type change with respect to X. Experiments performed on both real and synthetic datasets demonstrate the effectiveness of the proposed approach in discovering interesting and hidden knowledge from Twitter data. © 2014 Elsevier Inc.","Data mining and knowledge discovery; Generalized itemset mining; Social network analysis and mining"
"Recommending software upgrades with Mojave","2014","Journal of Systems and Software","10.1016/j.jss.2014.05.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906946625&doi=10.1016%2fj.jss.2014.05.019&partnerID=40&md5=c8d3025e12ef8b0d5d62efb0f4ac3704","Software upgrades are frequent. Unfortunately, many of the upgrades either fail or misbehave. We argue that many of these failures can be avoided for users of the new version of the software by exploiting the characteristics of the upgrade and feedback from the users that have already installed it. To demonstrate that this can be achieved, we build Mojave, the first recommendation system for software upgrades. Mojave leverages data from the existing and new users, machine learning, and static and dynamic source analyses. For each new user, Mojave computes the likelihood that the upgrade will fail for him/her. Based on this value, Mojave recommends for or against the upgrade. We evaluate Mojave for three real upgrade problems with the OpenSSH suite, and one synthetic upgrade problem each in the SQLite database and the uServer Web server. Our results show that it provides accurate recommendations to the new users. © 2014 Elsevier Inc.","Dynamic analysis; Manageability; Recommendation systems; Software upgrades; Static analysis"
"Surfing the optimization space of a multiple-GPU parallel implementation of a X-ray tomography reconstruction algorithm","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905860957&doi=10.1016%2fj.jss.2014.03.083&partnerID=40&md5=2254a2f5873eb6bf6dbeff607500e45a","The increasing popularity of massively parallel architectures based on accelerators have opened up the possibility of significantly improving the performance of X-ray computed tomography (CT) applications towards achieving real-time imaging. However, achieving this goal is a challenging process, as most CT applications have not been designed for exploiting the amount of parallelism existing in these architectures. In this paper we present the massively parallel implementation and optimization of Mangoose++, a CT application for reconstructing 3D volumes from 2D images collected by scanners based on cone-beam geometry. The main contribution of this paper are the following. First, we develop a modular application design that allows to exploit the functional parallelism inside the application and to facilitate the parallelization of individual application phases. Second, we identify a set of optimizations that can be applied individually and in combination for optimally deploying the application on a massively parallel multi-GPU system. Third, we present a study of surfing the optimization space of the modularized application and demonstrate that a significant benefit can be obtained from employing the adequate combination of application optimizations. © 2014 Elsevier B.V. All rights reserved.","CT reconstruction; GPGPU; Optimization; Paralellism; Tomography"
"An efficient design and validation technique for secure handover between 3GPP LTE and WLANs systems","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900635206&doi=10.1016%2fj.jss.2014.01.002&partnerID=40&md5=9c95dd7f99707dc3b449c2e7a68100a3","Future generations wireless systems, which integrate different wireless access networks together, will support a secured seamless mobility and a wide variety of applications and services with different quality of service (QoS) requirements. Most of the existing re-authentication protocols during vertical handover still have certain limitations such as man in the middle, eavesdropping and session hijacking attacks, and unacceptable delay for real time applications. In this article, we propose two re-authentication schemes to secure handover between 3GPP LTE and WLANs systems: Initial Handover Re-authentication Protocol, and Local Re-authentication Protocol. The second proposed protocol is executed locally in a WLAN network without contacting the authentication server of the home network for credentials verification. In fact, after a successful execution of the Initial Handover Re-authentication Protocol, the local key (LK) is shared between USIM and the authentication server of the WLAN. It is then used for securing handover and traffic in WLAN networks. Performance evaluation results obtained using simulation analysis show that the proposed re-authentication protocol enhances handover parameters such as handover latency, handover blocking rate and packet loss rate. Additionally, the proposed enhanced fast re-authentication protocol has been modeled and verified using the software AVISPA and is found to be safe. © 2014 Elsevier Inc.","Authentication; Handover; Heterogeneous wireless networks"
"Xen2MX: High-performance communication in virtualized environments","2014","Journal of Systems and Software","10.1016/j.jss.2014.04.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905828738&doi=10.1016%2fj.jss.2014.04.036&partnerID=40&md5=049ecf47aab42e5879f651f1260f87fa","Cloud computing infrastructures provide vast processing power and host a diverse set of computing workloads, ranging from service-oriented deployments to high-performance computing (HPC) applications. As HPC applications scale to a large number of VMs, providing near-native network I/O performance to each peer VM is an important challenge. In this paper we present Xen2MX, a paravirtual interconnection framework over generic Ethernet, binary compatible with Myrinet/MX and wire compatible with MXoE. Xen2MX combines the zero-copy characteristics of Open-MX with Xen's memory sharing techniques. Experimental evaluation of our prototype implementation shows that Xen2MX is able to achieve nearly the same raw performance as Open-MX running in a non-virtualized environment. On the latency front, Xen2MX performs as close as 96% to the case where virtualization layers are not present. Regarding throughput, Xen2MX saturates a 10 Gbps link, achieving 1159 MB/s, compared to 1192 MB/s of the non-virtualized case. Scales efficiently with the number of VMs, saturating the link for even smaller messages when 40 single-core VMs put pressure on the network adapters. © 2014 Elsevier B.V. All rights reserved.","10G Ethernet; Open-MX; Virtualization"
"Empirical research methods for technology validation: Scaling up to practice","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905825625&doi=10.1016%2fj.jss.2013.11.1097&partnerID=40&md5=515a6b05b23a26cf124c182d1f1b1b31","Before technology is transferred to the market, it must be validated empirically by simulating future practical use of the technology. Technology prototypes are first investigated in simplified contexts, and these simulations are scaled up to conditions of practice step by step as more becomes known about the technology. This paper discusses empirical research methods for scaling up new requirements engineering (RE) technology. When scaling up to practice, researchers want to generalize from validation studies to future practice. An analysis of scaling up technology in drug research reveals two ways to generalize, namely inductive generalization using statistical inference from samples, and analogic generalization using similarity between cases. Both are supported by abductive inference using mechanistic explanations of phenomena observed in the simulations. Illustrations of these inferences both in drug research and empirical RE research are given. Next, four kinds of methods for empirical RE technology validation are given, namely expert opinion, single-case mechanism experiments, technical action research and statistical difference-making experiments. A series of examples from empirical RE will illustrate the use of these methods, and the role of inductive generalization, analogic generalization, and abductive inference in them. Finally, the four kinds of empirical validation methods are compared with lists of validation methods known from empirical software engineering. The lists are combined to give an overview of some of the methods, instruments and data analysis techniques that may be used in empirical RE. © 2014 Elsevier B.V. All rights reserved.","Empirical research methodology; Scaling up to practice; Technology validation"
"Peer impressions in open source organizations: A survey","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902543985&doi=10.1016%2fj.jss.2014.03.061&partnerID=40&md5=93a44878a1e1b624483f405a8be4a7a0","In virtual organizations, such as Open Source Software (OSS) communities, we expect that the impressions members have about each other play an important role in fostering effective collaboration. However, there is little empirical evidence about how peer impressions form and change in virtual organizations. This paper reports the results from a survey designed to understand the peer impression formation process among OSS participants in terms of perceived expertise, trustworthiness, productivity, experiences collaborating, and other factors that make collaboration easy or difficult. While the majority of survey respondents reported positive experiences, a non-trivial fraction had negative experiences. In particular, volunteer participants were more likely to report negative experiences than participants who were paid. The results showed that factors related to a person's project contribution (e.g., quality and understandability of committed codes, important design related decisions, and critical fixes made) were more important than factors related to work style or personal traits. Although OSS participants are very task focused, the respondents believed that meeting their peers in person is beneficial for forming peer impressions. Having an appropriate impression of one's OSS peers is crucial, but the impression formation process is complicated and different from the process in traditional organizations. © 2014 Elsevier Inc.","Impression formation; Open source software; Virtual teams"
"Predictable integration and reuse of executable real-time components","2014","Journal of Systems and Software","10.1016/j.jss.2013.12.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900676128&doi=10.1016%2fj.jss.2013.12.040&partnerID=40&md5=35bc52bc4bc15afbbd3b19c6621c2fff","We present the concept of runnable virtual node (RVN) as a means to achieve predictable integration and reuse of executable real-time components in embedded systems. A runnable virtual node is a coarse-grained software component that provides functional and temporal isolation with respect to its environment. Its interaction with the environment is bounded both by a functional and a temporal interface, and the validity of its internal temporal behaviour is preserved when integrated with other components or when reused in a new environment. Our realization of RVN exploits the latest techniques for hierarchical scheduling to achieve temporal isolation, and the principles from component-based software-engineering to achieve functional isolation. It uses a two-level deployment process, i.e. deploying functional entities to RVNs and then deploying RVNs to physical nodes, and thus also gives development benefits with respect to composability, system integration, testing, and validation. In addition, we have implemented a server-based inter-RVN communication strategy to not only support the predictable integration and reuse properties of RVNs by keeping the communication code in a separate server, but also increasing the maintainability and flexibility to change the communication code without affecting the timing properties of RVNs. We have applied our approach to a case study, implemented in the ProCom component technology executing on top of a FreeRTOS-based hierarchical scheduling framework and present the results as a proof-of-concept. © 2014 Elsevier Inc.","Component reuse; Hierarchical scheduling; Real-time components' integration"
"Software trustworthiness 2.0 - A semantic web enabled global source code analysis approach","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895060280&doi=10.1016%2fj.jss.2013.08.030&partnerID=40&md5=95b090369a08fd6ca61464eb8e261345","There has been an ongoing trend toward collaborative software development using open and shared source code published in large software repositories on the Internet. While traditional source code analysis techniques perform well in single project contexts, new types of source code analysis techniques are ermerging, which focus on global source code analysis challenges. In this article, we discuss how the Semantic Web, can become an enabling technology to provide a standardized, formal, and semantic rich representations for modeling and analyzing large global source code corpora. Furthermore, inference services and other services provided by Semantic Web technologies can be used to support a variety of core source code analysis techniques, such as semantic code search, call graph construction, and clone detection. In this paper, we introduce SeCold, the first publicly available online linked data source code dataset for software engineering researchers and practitioners. Along with its dataset, SeCold also provides some Semantic Web enabled core services to support the analysis of Internet-scale source code repositories. We illustrated through several examples how this linked data combined with Semantic Web technologies can be harvested for different source code analysis tasks to support software trustworthiness. For the case studies, we combine both our linked-data set and Semantic Web enabled source code analysis services with knowledge extracted from StackOverflow, a crowdsourcing website. These case studies, we demonstrate that our approach is not only capable of crawling, processing, and scaling to traditional types of structured data (e.g., source code), but also supports emerging non-structured data sources, such as crowdsourced information (e.g., StackOverflow.com) to support a global source code analysis context. © 2013 Elsevier Inc.","Global source code analysis; Linked data; Semantic Web; Source code analysis"
"A trustworthy QoS-based collaborative filtering approach for web service discovery","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900833618&doi=10.1016%2fj.jss.2014.01.036&partnerID=40&md5=869711209f14dea001fa407228c10e98","Many network services which process a large quantity of data and knowledge are available in the distributed network environment, and provide applications to users based on Service-Oriented Architecture (SOA) and Web services technology. Therefore, a useful web service discovery approach for data and knowledge discovery process in the complex network environment is a very significant issue. Using the traditional keyword-based search method, users find it difficult to choose the best web services from those with similar functionalities. In addition, in an untrustworthy real world environment, the QoS-based service discovery approach cannot verify the correctness of the web services' Quality of Service (QoS) values, since such values guaranteed by a service provider are different from the real ones. This work proposes a trustworthy two-phase web service discovery mechanism based on QoS and collaborative filtering, which discovers and recommends the needed web services effectively for users in the distributed environment, and also solves the problem of services with incorrect QoS information. In the experiment, the theoretical analysis and simulation experiment results show that the proposed method can accurately recommend the needed services to users, and improve the recommendation quality. © 2014 Elsevier Inc.","Collaborative filtering; Quality of Service (QoS); Service discovery; Service selection"
"Comparing model-based and dynamic event-extraction based GUI testing techniques: An empirical study","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908212997&doi=10.1016%2fj.jss.2014.06.039&partnerID=40&md5=88821dc9e475e84b2d16302f464093e9","Graphical user interfaces are pervasive in modern software systems, and to ensure their quality it is important to test them. Two primary classes of automated GUI testing approaches, those based on static models and those based on dynamic event-extraction, present tradeoffs in cost and effectiveness. For example, static model-based GUI testing techniques can create test cases that contain nonexecutable events, whereas dynamic event-extraction based GUI testing techniques can create larger numbers of duplicate test cases. To better understand the effects of these tradeoffs, we created a GUI testing framework that facilitates fair comparison of different GUI testing techniques, and we conducted a controlled experiment comparing representative versions of static-model based and dynamic event-extraction based testing techniques on several GUI-based Java applications. Our study reveals several cost and effectiveness tradeoffs between the techniques, with implications for research and practice. © 2014 Elsevier Inc. All rights reserved.","Dynamic event-extraction based testing; GUI testing; Model-based testing; Test case generation"
"Efficient implementation of chaotic image encryption in transform domains","2014","Journal of Systems and Software","10.1016/j.jss.2014.07.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908211820&doi=10.1016%2fj.jss.2014.07.026&partnerID=40&md5=ed4697c2c589cd27199dca3b03f8af23","The primary goal of this paper is security management in data image transmission and storage. Because of the increased use of images in industrial operations, it is necessary to protect the secret data of the image against unauthorized access. In this paper, we introduce a novel approach for image encryption based on employing a cyclic shift and the 2-D chaotic Baker map in different transform domains. The Integer Wavelet Transform (IWT), the Discrete Wavelet Transform (DWT), and the Discrete Cosine Transform (DCT) are exploited in the proposed encryption approach. The characteristics of the transform domains are studied and used to carry out the chaotic encryption. A comparison study between the transform-domain encryption approaches in the presence of attacks shows the superiority of encryption in the DWT domain. © 2014 Elsevier Inc. All rights reserved.","DCT; DWT; Image encryption"
"Synthesizing interpreted domain-specific models to manage smart microgrids","2014","Journal of Systems and Software","10.1016/j.jss.2014.06.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906948125&doi=10.1016%2fj.jss.2014.06.006&partnerID=40&md5=09a38ee1dc0276ff3156e4d986708933","The increase in prominence of model-driven software development (MDSD) has placed emphasis on the use of domain-specific modeling languages (DSMLs) during the development process. DSMLs allow for domain concepts to be conceptualized and represented at a high level of abstraction. Currently, most DSML models are converted into high-level languages (HLLs) through a series of model-to-model and/or model-to-text transformations before they are executed. An alternative approach for model execution is the interpretation of models directly without converting them into an HLL. These models are created using interpreted DSMLs (i-DSMLs) and realized using a semantic-rich execution engine or domain-specific virtual machine (DSVM). In this article we present an approach for model synthesis, the first stage of model interpretation, that separates the domain-specific knowledge (DSK) from the model of execution (MoE). Previous work on model synthesis tightly couples the DSK and MoE reducing the ability for implementations of the DSVM to be easily reused in other domains. To illustrate how our approach to model synthesis works for i-DSMLs, we have created MGridML, an i-DSML for energy management in smart microgrids, and an MGridVM prototype, the DSVM for MGridML. We evaluated our approach by performing experiments on the model synthesis aspect of MGridVM and comparing the results to a DSVM from the user-centric communication domain. © 2014 Elsevier Inc.","Domain-specific modeling languages; Microgrids; Model of execution"
"On the use of software design models in software development practice: An empirical investigation","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.082","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905864992&doi=10.1016%2fj.jss.2014.03.082&partnerID=40&md5=7f8ffb76cf4bb6023686870abf6d4f74","Research into software design models in general, and into the UML in particular, focuses on answering the question how design models are used, completely ignoring the question if they are used. There is an assumption in the literature that the UML is the de facto standard, and that use of design models has had a profound and substantial effect on how software is designed by virtue of models giving the ability to do model-checking, code generation, or automated test generation. However for this assumption to be true, there has to be significant use of design models in practice by developers. This paper presents the results of a survey summarizing the answers of 3785 developers answering the simple question on the extent to which design models are used before coding. We relate their use of models with (i) total years of programming experience, (ii) open or closed development, (iii) educational level, (iv) programming language used, and (v) development type. The answer to our question was that design models are not used very extensively in industry, and where they are used, the use is informal and without tool support, and the notation is often not UML. The use of models decreased with an increase in experience and increased with higher level of qualification. Overall we found that models are used primarily as a communication and collaboration mechanism where there is a need to solve problems and/or get a joint understanding of the overall design in a group. We also conclude that models are seldom updated after initially created and are usually drawn on a whiteboard or on paper. © 2014 Elsevier B.V. All rights reserved.","Empirical industrial survey; Model-driven engineering (MDD, MDE, UML); Software design models"
"Handling slowly changing dimensions in data warehouses","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902543907&doi=10.1016%2fj.jss.2014.03.072&partnerID=40&md5=20aead043c072973c1bbc838738d41cb","Analysis of historical data in data warehouses contributes significantly toward future decision-making. A number of design factors including, slowly changing dimensions (SCDs), affect the quality of such analysis. In SCDs, attribute values may change over time and must be tracked. They should maintain consistency and correctness of data, and show good query performance. We identify that SCDs can have three types of validity periods: disjoint, overlapping, and same validity periods. We then show that the third type cannot be handled through the temporal star schema for temporal data warehouses (TDWs). We further show that a hybrid/Type6 scheme and temporal star schema may be used to handle this shortcoming. We demonstrate that the use of a surrogate key in the hybrid scheme efficiently identifies data, avoids most time comparisons, and improves query performance. Finally, we compare the TDWs and a surrogate key-based temporal data warehouse (SKTDW) using query formulation, query performance, and data warehouse size as parameters. The results of our experiments for 23 queries of five different types show that SKTDW outperforms TDW for all type of queries, with average and maximum performance improvements of 165% and 1071%, respectively. The results of our experiments are statistically significant. © 2014 Elsevier Inc.","Slowly changing dimensions; Temporal data warehouses; Temporal multi-dimensional model"
"Performance improvement of web caching in Web 2.0 via knowledge discovery","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887023295&doi=10.1016%2fj.jss.2013.04.060&partnerID=40&md5=47b200dedecdabdf67798063aba27f09","Web 2.0 systems are more unpredictable and customizable than traditional web applications. This causes that performance techniques, such as web caching, limit their improvements. Our study was based on the hypotheses that the use of web caching in Web 2.0 applications, particularly in content aggregation systems, can be improved by adapting the content fragment designs. We proposed to base this adaptation on the analysis of the characterization parameters of the content elements and on the creation of a classification algorithm. This algorithm was deployed with decision trees, created in an off-line knowledge discovery process. We also defined a framework to create and adapt fragments of the web documents to reduce the user-perceived latency in web caches. The experiment results showed that our solution had a remarkable reduction in the user-perceived latency even losses in the cache hit ratios and in the overhead generated on the system, in comparison with other web cache schemes. © 2013 Elsevier Inc.","Knowledge discovery; Web 2.0; Web caching"
"Privacy-preserving computation of participatory noise maps in the cloud","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898782448&doi=10.1016%2fj.jss.2014.01.035&partnerID=40&md5=9709d0164f8776b2cb26a5330cfed80f","This paper presents a privacy-preserving system for participatory sensing, which relies on cryptographic techniques and distributed computations in the cloud. Each individual user is represented by a personal software agent, deployed in the cloud, where it collaborates on distributed computations without loss of privacy, including with respect to the cloud service providers. We present a generic system architecture involving a cryptographic protocol based on a homomorphic encryption scheme for aggregating sensing data into maps, and demonstrate security in the Honest-But-Curious model both for the users and the cloud service providers. We validate our system in the context of NoiseTube, a participatory sensing framework for noise pollution, presenting experiments with real and artificially generated data sets, and a demo on a heterogeneous set of commercial cloud providers. To the best of our knowledge our system is the first operational privacy-preserving system for participatory sensing. While our validation pertains to the noise domain, the approach used is applicable in any crowd-sourcing application relying on location-based contributions of citizens where maps are produced by aggregating data - also beyond the domain of environmental monitoring. © 2014 Elsevier Inc.","Cloud computing; Participatory sensing; Privacy-preserving computation"
"A domain-specific language for context modeling in context-aware systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884130833&doi=10.1016%2fj.jss.2013.07.008&partnerID=40&md5=b342953491f47ba8fb502d74e8da3eca","Context-awareness refers to systems that can both sense and react based on their environment. One of the main difficulties that developers of context-aware systems must tackle is how to manage the needed context information. In this paper we present MLContext, a textual Domain-Specific Language (DSL) which is specially tailored for modeling context information. It has been implemented by applying Model-Driven Development (MDD) techniques to automatically generate software artifacts from context models. The MLContext abstract syntax has been defined as a metamodel, and model-to text transformations have been written to generate the desired software artifacts. The concrete syntax has been defined with the EMFText tool, which generates an editor and model injector. MLContext has been designed to provide a high-level abstraction, to be easy to learn, and to promote reuse of context models. A domain analysis has been applied to elicit the requirements and design choices to be taken into account in creating the DSL. As a proof of concept of the proposal, the generative approach has been applied to two different middleware platforms for context management. © 2013 Elsevier Inc.","Context aware; Context modeling; Model Driven Development"
"Chaos-based selective encryption for H.264/AVC","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887015704&doi=10.1016%2fj.jss.2013.07.054&partnerID=40&md5=6d80a9f6f23bdec98f235f8776a1fd07","Encryption techniques are usually employed to maintain the secrecy of the video streams transmitted via a public network. However, full encryption using strong cryptographic algorithms is usually not necessary if the purpose is to destroy the commercial value by preventing pleasant viewing. For this purpose, selective encryption is preferred as its operating efficiency is higher. Here, a chaos-based selective encryption scheme implemented on the H.264/AVC standard is proposed. The scheme employs multiple Rényi chaotic maps to generate a pseudorandom bit sequence which is used to mask the selected H.264/AVC syntax elements. It provides sufficient protection against full reconstruction while keeping the format compliance property so as not to cause decoding error without the key. The operating efficiency is high due to the low computational complexity of the Rényi chaotic map, as justified by the simulation results using video clips at various resolutions. Moreover, the security analyses show that the proposed algorithm is highly sensitive to the secret key and possesses good perceptual security. © 2013 Elsevier Inc. All rights reserved.","Chaos; H.264/AVC; Video Encryption"
"A language-independent approach to black-box testing using Erlang as test specification language","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887016915&doi=10.1016%2fj.jss.2013.07.021&partnerID=40&md5=f9dad0f24d4aa27a48231e27b7229cff","Integration of reused, well-designed components and subsystems is a common practice in software development. Hence, testing integration interfaces is a key activity, and a whole range of technical challenges arise from the complexity and versatility of such components. In this paper, we present a methodology to fully test different implementations of a software component integration API. More precisely, we propose a black-box testing approach, based on the use of QuickCheck and inspired by the TTCN-3 test architecture, to specify and test the expected behavior of a component. We have used a real-world multimedia content management system as case study. This system offers the same integration API for different technologies: Java, Erlang and HTTP/XML. Using our method, we have tested all integration API implementations using the same test specification, increasing the confidence in its interoperability and reusability. © 2013 Elsevier Inc.","Black-box testing; Functional testing; Test automation"
"A distributed framework for demand-driven software vulnerability detection","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888643750&doi=10.1016%2fj.jss.2013.08.033&partnerID=40&md5=565fec8737b2377e114b346afa87b6d9","Security testing aims at detecting program security flaws through a set of test cases and has become an active area of research. The challenge is how to efficiently produce test cases that are highly effective in detecting security flaws. This paper presents a novel distributed demand-driven security testing system to address this challenge. It leverages how end users use the software to increase the coverage of essential paths for security testing. The proposed system consists of many client sites and one testing site. The software under test is installed at each client site. Whenever a new path is about to be exercised by a user input, it will be sent to the testing site for security testing. At the testing site, symbolic execution is used to check any potential vulnerability on this new path. If a vulnerability is detected, a signature is automatically generated and updated to all client sites for protection. The benefits are as follows. First, it allows us to focus testing on essential paths, i.e., the paths that are actually being explored by users or attackers. Second, it stops an attacker from exploiting an unreported vulnerability at the client site. A prototype system has been implemented to evaluate the performance of the proposed system. The results show that it is both effective and efficient in practice. © 2013 Elsevier Inc. All rights reserved.","Security testing; Software vulnerability; Test decomposition"
"On the relationships between QoS and software adaptability at the architectural level","2014","Journal of Systems and Software","10.1016/j.jss.2013.07.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888643112&doi=10.1016%2fj.jss.2013.07.053&partnerID=40&md5=18c173c7014fa5abc28ff7e3ea9ffec6","Modern software operates in highly dynamic and often unpredictable environments that can degrade its quality of service. Therefore, it is increasingly important having systems able to adapt their behavior. However, the achievement of software adaptability can influence other software quality attributes, such as availability, performance or cost. This paper proposes an approach for analyzing tradeoffs between the system adaptability and its quality of service. The proposed approach is based on a set of metrics that allow the system adaptability evaluation. The approach can help software architects to guide decisions on system adaptation for fulfilling system quality requirements. The application and effectiveness of the approach are illustrated through examples and a wide set of experiments carried out with a tool we have developed. © 2013 Elsevier Inc.","Adaptability; Quality of service; Software architectures"
"Reversible data hiding based on PDE predictor","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882580940&doi=10.1016%2fj.jss.2013.05.077&partnerID=40&md5=bfcf3e6092c8b96f4e46cf439cec00dd","In this paper, we propose a prediction-error expansion based reversible data hiding by using a new predictor based on partial differential equation (PDE). For a given pixel, PDE predictor uses the mean of its four nearest neighboring pixels as initial prediction, and then iteratively updates the prediction until the value goes stable. Specifically, for each pixel, by calculating the gradients of four directions, the direction with small magnitude of gradient will be weighted larger in the iteration process, and finally a more accurate prediction can be obtained. Since PDE predictor can better exploit image redundancy, the proposed method introduces less distortion for embedding the same payload. Experimental results show that our method outperforms some state-of-the-art methods. © 2013 Elsevier Inc.","Partial differential equation(PDE); Prediction-error expansion (PEE); Reversible data hiding"
"Genetic algorithm and difference expansion based reversible watermarking for relational databases","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884163910&doi=10.1016%2fj.jss.2013.06.023&partnerID=40&md5=8b139743a5281e11f1d207b9340752a0","In this paper, we present a new robust and reversible watermarking approach for the protection of relational databases. Our approach is based on the idea of difference expansion and utilizes genetic algorithm (GA) to improve watermark capacity and reduce distortion. The proposed approach is reversible and therefore, distortion introduced after watermark insertion can be fully restored. Using GA, different attributes are explored to meet the optimal criteria rather than selecting less effective attributes for watermark insertion. Checking only the distortion tolerance of two attributes for a selected tuple may not be useful for watermark capacity and distortion therefore, distortion tolerance of different attributes are explored. Distortion caused by difference expansion can help an attacker to predict watermarked attribute. Thus, we have incorporated tuple and attribute-wise distortion in the fitness function of GA, making it tough for an attacker to predict watermarked attribute. From experimental analysis, it is concluded that the proposed technique provides improved capacity and reduced distortion compared to existing approaches. Problem of false positives and change in attribute order at detection side is also resolved. Additionally, the proposed technique is resilient against a wide range of attacks such as addition, deletion, sorting, bit flipping, tuple-wise-multifaceted, attribute-wise-multifaceted, and additive attacks. © 2013 Elsevier Inc.","Difference expansion; Distortion; Genetic algorithm; Relational database; Reversible watermarking; Robust watermarking; Watermark attack; Watermark capacity"
"Model-based cache-aware dispatching of object-oriented software for multicore systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884128081&doi=10.1016%2fj.jss.2013.06.025&partnerID=40&md5=3338cf32b578b3fadfaa810c60501382","In recent years, processor technology has evolved towards multicore processors, which include multiple processing units (cores) in a single package. Those cores, having their own private caches, often share a higher level cache memory dedicated to each processor die. This multi-level cache hierarchy in multicore processors raises the importance of cache utilization problem. Assigning parallel-running software components with common data to processor cores that do not share a common cache increases the number of cache misses. In this paper we present a novel approach that uses model-based information to guide the OS scheduler in assigning appropriate core affinities to software objects at run-time. We build graph models of software and cache hierarchies of processors and devise a graph matcher algorithm that provides mapping between these two graphs. Using this mapping we obtain candidate core sets that each software object can be affiliated with at run-time. These affiliations are determined based on the idea that software components that have the potential to share common data at run-time should run on cores that share a common cache. We also develop an object dispatcher algorithm that keeps track of object affiliations at run-time and dispatches objects by using the information from the compile-time graph matcher. We apply our approach on design pattern implementations and two different application program running on servers using CFS scheduling. Our results show that cache-aware dispatching based on information obtained from software model, decreases number of cache misses significantly and improves CFS' scheduling performance. © 2013 Elsevier Inc.","Cache-aware object dispatching; Model-based scheduling; Object-oriented design for multicore systems"
"Generation and validation of traces between requirements and architecture based on formal trace semantics","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891623818&doi=10.1016%2fj.jss.2013.10.006&partnerID=40&md5=8e7325ad6e0de47dc383bffad59d1886","The size and complexity of software systems make integration of the new/modified requirements to the software system costly and time consuming. The impact of requirements changes on other requirements, design elements and source code should be traced to determine parts of the software to be changed. Considerable research has been devoted to relating requirements and design artifacts with source code. Less attention has been paid to relating requirements (R) with architecture (A) by using well-defined semantics of traces. Traces between R&A might be manually assigned. This is time-consuming, and error prone. Traces might be incomplete and invalid. In this paper, we present an approach for automatic trace generation and validation of traces between requirements (R) and architecture (A). Requirements relations and architecture verification techniques are used. A trace metamodel is defined with commonly used trace types between R&A. We use the semantics of traces and requirements relations for generating and validating traces with a tool support. The tool provides the following: (1) generation and validation of traces by using requirements relations and/or verification of architecture, (2) generation and validation of requirements relations by using traces. The tool is based on model transformation in ATL and term-rewriting logic in Maude. © 2013 Elsevier Inc. All rights reserved.","Requirements metamodel; Software architecture; Trace generation and validation; Trace metamodel"
"A clustering-based model for class responsibility assignment problem in object-oriented analysis","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900808495&doi=10.1016%2fj.jss.2014.02.053&partnerID=40&md5=847d4f1c7855f60e31ac7e98e77149f5","Assigning responsibilities to classes is a vital task in object-oriented analysis and design, and it directly affects the maintainability and reusability of software systems. There are many methodologies to help recognize the responsibilities of a system and assign them to classes, but all of them depend greatly on human judgment and decision-making. In this paper, we propose a clustering-based model to solve the class responsibility assignment (CRA) problem. The proposed model employs a novel interactive graph-based method to find inheritance hierarchies, and two novel criteria to determine the appropriate number of classes. It reduces the dependency of CRA on human judgment and provides a decision-making support for CRA in class diagrams. To evaluate the proposed model, we apply three different hierarchical agglomerative clustering algorithms and two different types of similarity measures. By comparing the obtained results of clustering techniques with the models designed by multi-objective genetic algorithm (MOGA), it is revealed that clustering techniques yield promising results. © 2014 Elsevier Inc.","Automated software design; Class responsibility assignment; Clustering techniques"
"Architecture for embedded open software ecosystems","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898803901&doi=10.1016%2fj.jss.2014.01.009&partnerID=40&md5=efa1da520008c53e536017f4d52cfb08","Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem the paper proposes open software ecosystem as an alternative approach to develop software for embedded systems, and elaborates on the necessary quality attributes of an embedded platform underlying such an ecosystem the paper then defines a reference architecture consisting of 17 key decisions together with four architectural patterns, and provides the rationale why they are essential for an open software ecosystem platform for embedded systems in general and automotive systems in particular the reference architecture is validated through a prototypical platform implementation in an industrial setting, providing a deeper understanding of how the architecture could be realised in the automotive domain. Four potential existing platforms, all targeted at the embedded domain (Android, OKL4, AUTOSAR and Robocop), are evaluated against the identified quality attributes to see how they could serve as a basis for an open software ecosystem platform with the conclusion that while none of them is a perfect fit they all have fundamental mechanisms necessary for an open software ecosystem approach. © 2014 Elsevier Inc.","Embedded software; Software architecture; Software ecosystem"
"Low bit-rate information hiding method based on search-order-coding technique","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884135565&doi=10.1016%2fj.jss.2013.06.066&partnerID=40&md5=22473393d1b2993cdb00b041d0830eeb","Information hiding method with low bit rate is important in secure communications. To reduce bit rate we propose a new embedding method in this paper based on SOC (search-order coding) compression technique. Compared to Chang et al.'s scheme in 2004, our scheme completely avoids the transform from SOC coding to OIV (original index values) coding to significantly reduce bit rate. In order to further reduce bit rate, Chang et al. proposed a reversible data hiding scheme using hybrid encoding strategies by introducing the side-match vector quantization (SMVQ) in 2013. But it needed additional 1 bit indicator to distinguish the two statuses to determine OIV is belonged to G1 or G2. This overhead gave a large burden to compression rate and could not reduce the bit rate significantly. In contrast, our scheme completely avoids this indicator. The experimental results show that the proposed method can efficiently reduce the bit rate and have the same embedding capacity compared with Chang et al.'s scheme in 2004 and Chang et al.'s scheme in 2013. Moreover, our proposed scheme can also achieve a better performance in both the embedding capacity and bit rate than other related VQ-based information hiding schemes. © 2013 Elsevier Inc.","Embedding capacity; Information hiding; Low bit rate; Search-order coding (SOC); Vector quantization (VQ)"
"Minimizing test-point allocation to improve diagnosability in business process models","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884126868&doi=10.1016%2fj.jss.2013.05.105&partnerID=40&md5=d508b0990103f19c2ec34772f487518b","Diagnosability analysis aims to determine whether observations available during the execution of a system are sufficient to precisely locate the source of a problem. Previous work deals with the diagnosability problem in contexts such as circuits and systems, but no with the adaptation of the diagnosability problem to business processes. In order to improve the diagnosability, a set of test points needs to be allocated. Therefore, the aim of this contribution is to determine a test-point allocation to obtain sufficient observable data in the dataflow to allow the discrimination of faults for a later diagnosis process. The allocation of test points depends on the strategies of the companies, for this reason we defined two possibilities: to improve the diagnosability of a business process for a fixed number of test points and the minimization of the number of test points for a given level of diagnosability. Both strategies have been implemented in the Test-Point Allocator tool in order to facilitate the integration of the test points in the business process model life cycle. Experimental results indicate that diagnosability of business processes can be improved by allocating test points in an acceptable time. © 2013 Elsevier Inc.","Business process improvement; Diagnosability; Test points"
"Resource failures risk assessment modelling in distributed environments","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891624184&doi=10.1016%2fj.jss.2013.09.017&partnerID=40&md5=f05e16007a42b9b106e87bb794fc4b77","Service providers offer access to resources and services in distributed environments such as Grids and Clouds through formal Service level Agreements (SLA), and need well-balanced infrastructures so that they can maximise the Quality of Service (QoS) they offer and minimise the number of SLA violations. We propose a mathematical model to predict the risk of failure of resources in such environments using a discrete-time analytical model driven by reliability functions fitted to observed data. The model relies on the resource historical data so as to predict the risk of failure for a given time interval. The model is evaluated by comparing the predicted risk of failure with the observed risk of failure, and is shown to accurately predict the resources risk of failure, allowing a service provider to selectively choose which SLA request to accept. Crown Copyright © 2013 Published by Elsevier Inc. All rights reserved.","Cloud computing; Grid computing; Markov Chains; Quality of Service; Resource failure; Risk assessment"
"Programming mobile context-aware applications with TOTAM","2014","Journal of Systems and Software","10.1016/j.jss.2013.07.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898825372&doi=10.1016%2fj.jss.2013.07.031&partnerID=40&md5=180ddd0f3aa6d22380fca71ad94dfa6c","In tuple space approaches to context-aware mobile systems, the notion of context is defined by the presence or absence of certain tuples in the tuple space. Existing approaches define such presence either by collocation of devices holding the tuples or by replication of tuples across all devices. We show that both approaches can lead to an erroneous perception of context. Collocation ties the perception of context to network connectivity which does not always yield the expected result. Tuple replication can cause that a certain context is perceived even if the device has left the context a long time ago. We propose a tuple space approach in which tuples themselves carry a predicate that determines whether they are in the right context or not. We present a practical API for our approach and show its use by means of the implementation of various mobile applications. Benchmarks show that our approach can lead to a significant increase in performance compared to other approaches. © 2013 Elsevier Inc.","Context-awareness; Programming abstractions; Tuple spaces"
"Process fragmentation, distribution and execution using an event-based interaction scheme","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895057179&doi=10.1016%2fj.jss.2013.11.1111&partnerID=40&md5=829614ab9ab12cf40eb03fa2a449dea9","The combination of service oriented architectures and business processes creates an enactment environment in which processes can be deployed and executed automatically. From a managerial and technical point of view, the interpretation, control and execution of a process flow happen very often at one point in the organizational and IT structure. This creates an inflexible environment in which control over and visibility of cross-departmental processes cannot be distributed across these organizational entities. Although the process model may need to be designed as a whole (to have an end-to-end definition), the actual execution of the process may need to be distributed across all participating partners. There are several ways to achieve this distribution. In this paper, we look at an event-based process deployment and execution infrastructure in which a process model can be automatically partitioned and distributed over different enactment entities, provided some given distribution definition. We compare the performance and flexibility of the proposed technique with other approaches and discuss the potential advantages and drawbacks of the event-based distribution. © 2013 Elsevier Inc.","Business process enactment/execution; Distributed business processes; Event based architecture"
"Robust reversible watermarking scheme using Slantlet transform matrix","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891630884&doi=10.1016%2fj.jss.2013.09.033&partnerID=40&md5=912978f215f616ddd1adcea6e55abcc3","The need for a robust reversible watermarking method has recently attracted more attention. This paper presents a novel robust reversible watermarking scheme based on using the Slantlet transform matrix to transform small blocks of the original image and hiding the watermark bits by modifying the mean values of the carrier subbands. The problem of overflow/underflow has been avoided by using histogram modification process. Extensive experimental tests based on 100 general images and 100 medical images demonstrate the efficiency of the proposed scheme. The proposed scheme has robustness against different kinds of attacks and the results prove that it is completely reversible with improved capacity, robustness, and invisibility in comparison with the previous methods. © 2013 Elsevier Inc. All rights reserved.","Histogram modification; Robust reversible watermarking (RRW); Slantlet transform (SLT)"
"Programmable context awareness framework","2014","Journal of Systems and Software","10.1016/j.jss.2013.07.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898825764&doi=10.1016%2fj.jss.2013.07.046&partnerID=40&md5=5641db472ea8e27b295fc380ea352b1d","Context-awareness enables applications to provide end-users with a richer experience by enhancing their interactions with contextual information. Several frameworks have already been proposed to simplify the development of context-aware applications these frameworks are focused on provisioning context data and on providing common semantics, definitions and representations of these context data they assume that applications share the same semantic, which limits the range of use cases where a framework can be used, as that assumption induces a strong coupling between context management and application logic. This article proposes a framework that decouples context management from application business logic the aim is to reduce the overhead on applications that run on resource-limited devices while still providing mechanisms to support context-awareness and behavior adaptation the article presents an innovative approach that involves third-parties in context processing definition by structuring it using atomic functions these functions can be designed by third-party developers using an XML-based programming language. Its implementation and evaluation demonstrates the benefits, in terms of flexibility, of using proven design patterns from software engineering for developing context-aware application. © 2013 Elsevier Inc.","Adaptation; Context-awareness; Privacy; Software engineering; XML"
"S-IDE: A tool framework for optimizing deployment architecture of High Level Architecture based simulation systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882626082&doi=10.1016%2fj.jss.2013.03.013&partnerID=40&md5=4a3ac6874fe56884e966d2221ea4ac4f","One of the important problems in High Level Architecture (HLA) based distributed simulation systems is the allocation of the different simulation modules to the available physical resources. Usually, the deployment of the simulation modules to the physical resources can be done in many different ways, and each deployment alternative will have a different impact on the performance. Although different algorithmic solutions have been provided to optimize the allocation with respect to the performance, the problem has not been explicitly tackled from an architecture design perspective. Moreover, for optimizing the deployment of the simulation system, tool support is largely missing. In this paper we propose a method for automatically deriving deployment alternatives for HLA based distributed simulation systems. The method extends the IEEE Recommended Practice for High Level Architecture Federation Development and Execution Process by providing an approach for optimizing the allocation at the design level. The method is realized by the tool framework, S-IDE (Simulation-IDE) that we have developed to provide an integrated development environment for deriving a feasible deployment alternative based on the simulation system and the available physical resources at the design phase. The method and the tool support have been validated using a case study for the development of a traffic simulation system. © 2013 Elsevier Inc.","Deployment model optimization; Distributed simulation; FEDEP; High Level Architecture (HLA); Metamodel based tool development; Metamodeling; Model transformations; Software architecture"
"Adapting system execution traces to support analysis of software system performance properties","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884140754&doi=10.1016%2fj.jss.2013.06.060&partnerID=40&md5=d46d6d5b435ac4d0d3a7559234c78598","UNITE is a method and tool that analyzes software system performance properties, e.g., end-to-end response time, throughput, and service time, via system execution traces. UNITE, however, assumes that a system execution trace contains properties (e.g., identifiable keywords, unique message instances, and enough variation among the same event types) to support performance analysis. With proper planning, it is possible to ensure that properties required to support such analysis are incorporated in the generated system execution trace. It, however, is not safe to assume this to be the case with many existing software systems. This article therefore presents a method and a tool called the System Execution Trace Adaptation Framework (SETAF), which is built atop of UNITE and adapts system execution traces to support performance analysis of software systems. It also presents examples and results of applying SETAF to different open-source projects. The results show that SETAF enables proper performance analysis via system execution traces without requiring developers to make modifications to the originating software system's source code, which can be a expensive and time-consuming task. © 2013 Elsevier Inc.","Adaptation; SETAF; System execution traces"
"Reliability guaranteed energy-aware frame-based task set execution strategy for hard real-time systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887025803&doi=10.1016%2fj.jss.2013.06.067&partnerID=40&md5=2c332d00ed9b384f0b486833ba9978cf","In this paper, we study the problem of how to execute a real-time frame-based task set on DVFS-enabled processors so that the system's reliability can be guaranteed and the energy consumption of executing the task set is minimized. To ensure the reliability requirement, processing resources are reserved to re-execute tasks when transient faults occur. However, different from commonly used approaches that the reserved processing resources are shared by all tasks in the task set, we judiciously select a subset of tasks to share these reserved resources for recovery purposes. In addition, we formally prove that for a give task set, the system achieves the highest reliability and consumes the least amount of energy when the task set is executed with a uniform frequency (or neighboring frequencies if the desired frequency is not available). Based on the theoretic conclusion, rather than heuristically searching for appropriate execution frequency for each individual task as used in many research work, we directly calculate the optimal execution frequency for the task set. Our simulation results have shown that with our approach, we can not only guarantee the same level of system reliability, but also have up to 15% energy saving improvement over other fault recovery-based approaches existed in the literature. Furthermore, as our approach does not require frequent frequency changes, it works particularly effective on processors where frequency switching overhead is large and not negligible. © 2013 Elsevier Inc.","Energy saving; Real-time; Transient fault"
"Efficient optimization of large probabilistic models","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882637923&doi=10.1016%2fj.jss.2013.03.078&partnerID=40&md5=535732e4aa1424e0e9f8be1249331628","The development of safety critical systems often requires design decisions which influence not only dependability, but also other properties which are often even antagonistic to dependability, e.g.; cost. Finding good compromises considering different goals while at the same time guaranteeing sufficiently high safety of a system is a very difficult task. We propose an integrated approach for modeling, analysis and optimization of safety critical systems. It is fully automated with an implementation based on the Eclipse platform. The approach is tool-independent, different analysis tools can be used and there exists an API for the integration of different optimization and estimation algorithms. For safety critical systems, a very important criterion is the hazard occurrence probability, whose computation can be quite costly. Therefore we also provide means to speed up optimization by devising different combinations of stochastic estimators and illustrate how they can be integrated into the approach. We illustrate the approach on relevant case-studies and provide experimental details to validate its effectiveness and applicability. © 2013 Elsevier Inc.","Formal methods; Multi-objective optimization; Safety analysis; Safety optimization"
"Factors that motivate software engineering teams: A four country empirical study","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898821843&doi=10.1016%2fj.jss.2014.01.008&partnerID=40&md5=f6011dc2eef34ad45d9f41d61f2f7986","Motivation, although difficult to quantify, is considered to be the single largest factor in developer productivity; there are also suggestions that low motivation is an important factor in software development project failure. We investigate factors that motivate software engineering teams using survey data collected from software engineering practitioners based in Australia, Chile, USA and Vietnam. We also investigate the relationship between team motivation and project outcome, identifying whether the country in which software engineering practitioners are based affects this relationship. Analysis of 333 questionnaires indicates that failed projects are associated with low team motivation. We found a set of six common team motivational factors that appear to be culturally independent (project manager has good communication with project staff, project risks reassessed, controlled and managed during the project, customer has confidence in the project manager and the development team, the working environment is good, the team works well together, and the software engineer had a pleasant experience). We also found unique groupings of team motivational factors for each of the countries investigated. This indicates that there are cultural differences that project managers need to consider when working in a global environment. © 2014 Elsevier Inc.","Motivational factors; Project success; Software development team motivation"
"From AADL to Timed Abstract State Machines: A verified model transformation","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900826199&doi=10.1016%2fj.jss.2014.02.058&partnerID=40&md5=3f118329a59b4655a5271637d679dec6","Architecture Analysis and Design Language (AADL) is an architecture description language standard for embedded real-time systems widely used in the avionics and aerospace industry to model safety-critical applications. To verify and analyze the AADL models, model transformation technologies are often used to automatically extract a formal specification suitable for analysis and verification. In this process, it remains a challenge to prove that the model transformation preserves the semantics of the initial AADL model or, at least, some of the specific properties or requirements it needs to satisfy. This paper presents a machine checked semantics-preserving transformation of a subset of AADL (including periodic threads, data port communications, mode changes, and the AADL behavior annex) into Timed Abstract State Machines (TASM). The AADL standard itself lacks at present a formal semantics to make this translation validation possible. Our contribution is to bridge this gap by providing two formal semantics for the subset of AADL. The execution semantics provided by the AADL standard is formalized as Timed Transition Systems (TTS). This formalization gives a reference expression of AADL semantics which can be compared with the TASM-based translation (for verification purpose). Finally, the verified transformation is mechanized in the theorem prover Coq. © 2014 Elsevier Inc.","Architecture Analysis and Design Language (AADL); Coq; Model transformation; Model-driven engineering; Semantics preservation; Timed Abstract State Machine (TASM)"
"Accurate sub-swarms particle swarm optimization algorithm for service composition","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894415325&doi=10.1016%2fj.jss.2013.11.1113&partnerID=40&md5=1cfe9b019b5dc880da9282819461713e","Service composition (SC) generates various composite applications quickly by using a novel service interaction model. Before composing services together, the most important thing is to find optimal candidate service instances compliant with non-functional requirements. Particle swarm optimization (PSO) is known as an effective and efficient algorithm, which is widely used in this process. However, the premature convergence and diversity loss of PSO always results in suboptimal solutions. In this paper, we propose an accurate sub-swarms particle swarm optimization (ASPSO) algorithm by adopting parallel and serial niching techniques. The ASPSO algorithm locates optimal solutions by using sub-swarms searching grid cells in which the density of feasible solutions is high. Simulation results demonstrate that the proposed algorithm improves the accuracy of the standard PSO algorithm in searching the optimal solution of service selection problem. © 2013 Elsevier Inc.","Multi-constraint optimal service; Particle swarm optimization; Service composition"
"A study of cyclic dependencies on defect profile of software components","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887017156&doi=10.1016%2fj.jss.2013.07.039&partnerID=40&md5=df51d34784f52c1ddd1d7ff04212badc","Background Empirical evidence shows that dependency cycles among software components are pervasive in real-life software systems, although such cycles are known to be detrimental to software quality attributes such as understandability, testability, reusability, build-ability and maintainability. Research goals Can the use of extended object-oriented metrics make us better understand the relationships among cyclic related components and their defect-proneness? Approach First, we extend such metrics to mine and classify software components into two groups - the cyclic and the non-cyclic ones. Next, we have performed an empirical study of six software applications. Using standard statistical tests on four different hypotheses, we have determined the significance of the defect profiles of both groups. Results Our results show that most defects and defective components are concentrated in cyclic-dependent components, either directly or indirectly. Discussion and conclusion These results have important implications for software maintenance and system testing. By identifying the most defect-prone set in a software system, it is possible to effectively allocate testing resources in a cost efficient manner. Based on these results, we demonstrate how additional structural properties could be collected to understand component's defect proneness and aid decision process in refactoring defect-prone cyclic related components. © 2013 Elsevier Inc. All rights reserved.","Defect-prone components; Defects; Dependency cycle"
"Distributed debugging for mobile networks","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894468298&doi=10.1016%2fj.jss.2013.11.1099&partnerID=40&md5=8eadd8876f9c16b0fe2a7bca9c29ed16","Debuggers are an integral part, albeit often neglected, of the development of distributed applications. Ambient-oriented programming (AmOP) is a distributed paradigm for applications running on mobile ad hoc networks. In AmOP the complexity of programming in a distributed setting is married with the network fragility and open topology of mobile applications. To our knowledge, there is no debugging approach that tackles both these issues. In this paper we argue that a novel kind of distributed debugger that we term an ambient-oriented debugger, is required. We present REME-D (read as remedy), an online ambient-oriented debugger that integrates techniques from distributed debugging (event-based debugging, message breakpoints) and proposes facilities to deal with ad hoc, fragile networks - epidemic debugging, and support for frequent disconnections. © 2013 Elsevier Inc.","Distributed debugging; Distributed object-oriented applications; Mobile networks"
"Performance models and dynamic characteristics analysis for HDFS write and read operations: A systematic view","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900799346&doi=10.1016%2fj.jss.2014.02.038&partnerID=40&md5=2563fc6ed318555c73f5eebdc4a72677","Hadoop has emerged as a successful framework for large-scale data-intensive computing applications. However, there is no research on performance models for the Hadoop Distributed File System (HDFS). Due to the complexity of HDFS and the difficulty of modeling the multiple impact factors for HDFS performance, to establish HDFS performance models based directly on these impact factors is very complicated. In this paper, the relationship between file size and HDFS Write/Read (denoted as W/R for short) throughput, i.e., the average flow rate of a HDFS W/R operation, is studied to build HDFS performance models from a systematic view. Based on the measured data of specially designed experiments (in which HDFS W/R operations can be viewed as single-input single-output systems), a system identification-based approach is applied to construct performance models for HDFS W/R operations under different conditions. Furthermore, dynamic characteristics metrics for HDFS performance are defined, and based on the identified performance models and these metrics, the dynamic characteristics of HDFS W/R operations, such as steady state and overshoot, are studied, and the relationships between impact factors and dynamic characteristics are analyzed. These analysis results can provide effective guidance and implications for the design and configuration of HDFS and Hadoop-based applications. © 2014 Elsevier Inc.","Dynamic characteristics; HDFS; Performance model; System identification; System modeling"
"A cross-layer middleware for context-aware cooperative application on mobile ad hoc peer-to-peer network","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898818502&doi=10.1016%2fj.jss.2013.10.007&partnerID=40&md5=5a6f1b659a88e711e484d7d61fa059c1","Mobile ad hoc peer-to-peer (P2P) applications become popular for providing the file sharing, voice communicating, and video streaming services due to entertainments and disaster recovery. However, both the topology of wireless network and the overlay of P2P network are dynamic, so the middleware is proposed to integrate such architectures of service-oriented applications therefore, we propose context-aware cooperative application (CACA) to overcome the frequent churn and high mobility problems. CACA proposes a cross-layer middleware to integrate DHT-based lookup, anycast query, and P2P delivery via the IPv6 routing header. Through anycast query, the response delay can be shortened and the query duplication can be minimized. Via IPv6 routing header, the delivery efficiency can be improved. Through the cross-layer design, the finger table in overlay layer is combined with the routing table in network layer to heighten proximity the simulation results demonstrate that CACA has the outstanding performances of short download delay, high playback continuity, and low signaling overhead in mobile ad hoc network. © 2013 Elsevier Inc.","Anycast; Cross-layer middleware; IPv6; Mobile ad hoc network; Mobile wireless P2P; Routing header"
"Supporting concept location through identifier parsing and ontology extraction","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884166097&doi=10.1016%2fj.jss.2013.07.009&partnerID=40&md5=da1fc93acc055e74683b7b3aeb8acc72","Identifier names play a key role in program understanding and in particular in concept location. Programmers can easily ""parse"" identifiers and understand the intended meaning. This, however, is not trivial for tools that try to exploit the information in the identifiers to support program understanding. To address this problem, we resort to natural language analyzers, which parse tokenized identifier names and provide the syntactic relationships (dependencies) among the terms composing the identifiers. Such relationships are then mapped to semantic relationships. In this study, we have evaluated the use of off-the-shelf and trained natural language analyzers to parse identifier names, extract an ontology and use it to support concept location. In the evaluation, we assessed whether the concepts taken from the ontology can be used to improve the efficiency of queries used in concept location. We have also investigated if the use of different natural language analyzers has an impact on the ontology extracted and the support it provides to concept location. Results show that using the concepts from the ontology significantly improves the efficiency of concept location queries (e.g., in some cases, an improvement of 127% is observed). The results also indicate that the efficiency of concept location queries is not affected by the differences in the ontologies produced by different analyzers. © 2013 Elsevier Inc.","Concept location; Natural language parsing; Program understanding"
"Virtualized Web server cluster self-configuration to optimize resource and power use","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884147930&doi=10.1016%2fj.jss.2013.06.033&partnerID=40&md5=446e9f57bad29711391f3edc8ee5551c","This work proposes a reusable architecture that enables the self-configuration of a supporting infrastructure for Web server clusters using virtual machines. The goal of the architecture is to ensure service quality, evaluating how broadly it complies with the application's operating restrictions and proportionally acting on the configuration of physical servers (hosts) or virtual machines. In addition, through the rational use of resources, the proposal aims at saving energy. A prototype of the architecture was developed and a performance evaluation carried out with two different resource management approaches. This evaluation shows how fully functional and advantageous the proposal is in terms of using resources, avoiding waste, yet maintaining the application's quality of service within acceptable levels. The architecture also shows to be flexible enough to accept, with a reasonable amount of effort, different resource self-configuration policies. © 2013 Elsevier Inc.","Energy saving; Resource management; Virtualization"
"Flexible resource monitoring of Java programs","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900816089&doi=10.1016%2fj.jss.2014.02.022&partnerID=40&md5=d87ef6fb2c2e02c30a5c21e60d036817","Monitoring resource consumptions is fundamental in software engineering, e.g., in validation of quality requirements, performance engineering, or adaptive software systems. However, resource monitoring does not come for free as it typically leads to overhead in the observed program. Minimizing this overhead and increasing the reliability of the monitored data is a major goal in realizing resource monitoring tools. Typically, this is achieved by limiting capabilities, e.g., supported resources, granularity of the monitoring focus, or runtime access to results. Thus, in practice often several approaches must be combined to obtain relevant information. We describe SPASS-meter, a novel resource monitoring approach for Java and Android Apps, which combines these conflicting capabilities with low overhead. SPASS-meter supports a large set of resources, flexible configuration of the monitoring scope even for user-defined semantic units (components), runtime analysis and online access to monitoring results in a platform-independent way. We discuss the concepts of SPASS-meter, its architecture, realization and validation, the latter in terms of case studies and an overhead analysis based on performance experiments with SPASS-meter, OpenCore and Kieker. SPASS-meter provides a detailed view of the runtime resource consumption at reasonable overhead of less than 3% processing power and 0.5% memory consumption in our experiments. © 2014 Elsevier Inc.","Empirical analysis; Java; Monitoring overhead; Performance engineering; Resource monitoring; Software components"
"Hybrid multi-attribute QoS optimization in component based software systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882598370&doi=10.1016%2fj.jss.2013.03.081&partnerID=40&md5=3816ffabd78ddfff2a0373d27f45ffe7","Design decisions for complex, component-based systems impact multiple quality of service (QoS) properties. Often, means to improve one quality property deteriorate another one. In this scenario, selecting a good solution with respect to a single quality attribute can lead to unacceptable results with respect to the other quality attributes. A promising way to deal with this problem is to exploit multi-objective optimization where the objectives represent different quality attributes. The aim of these techniques is to devise a set of solutions, each of which assures an optimal trade-off between the conflicting qualities. Our previous work proposed a combined use of analytical optimization techniques and evolutionary algorithms to efficiently identify an optimal set of design alternatives with respect to performance and costs. This paper extends this approach to more QoS properties by providing analytical algorithms for availability-cost optimization and three-dimensional availability-performance-cost optimization. We demonstrate the use of this approach on a case study, showing that the analytical step provides a better-than-random starting population for the evolutionary optimization, which lead to a speed-up of 28% in the availability-cost case. © 2013 Elsevier Inc.","Availability; Quality of service; Software architecture optimization"
"Failure factors of small software projects at a global outsourcing marketplace","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898792359&doi=10.1016%2fj.jss.2014.01.034&partnerID=40&md5=eb7455bc6c40645b27b29f83928e749b","The presented study aims at a better understanding of when and why small-scale software projects at a global outsourcing marketplace fail the analysis is based on a data set of 785,325 projects/tasks completed at vWorker.com. A binary logistic regression model relying solely on information known at the time of a project's start-up correctly predicted 74% of the project failures and 67% of the non-failures the model-predicted failure probability corresponded well with the actual frequencies of failures for most levels of failure risk the model suggests that the factors connected to the strongest reduction in the risk of failure are related to previous collaboration between the client and the provider and a low failure rate of previous projects completed by the provider. We found the characteristics of the client to be almost as important as those of the provider in explaining project failures and that the risk of project failure increased with an increased client emphasis on low price and with an increased project size the identified relationships seem to be reasonable stable across the studied project size categories. © 2014 Elsevier Inc.","Outsourcing; Project failures; Risk management"
"Recovering test-to-code traceability using slicing and textual analysis","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891634221&doi=10.1016%2fj.jss.2013.10.019&partnerID=40&md5=12ac69e5b840f20c9b4ae19ca2bd6299","Test suites are a valuable source of up-to-date documentation as developers continuously modify them to reflect changes in the production code and preserve an effective regression suite. While maintaining traceability links between unit test and the classes under test can be useful to selectively retest code after a change, the value of having traceability links goes far beyond this potential savings. One key use is to help developers better comprehend the dependencies between tests and classes and help maintain consistency during refactoring. Despite its importance, test-to-code traceability is not common in software development and, when needed, traceability information has to be recovered during software development and evolution. We propose an advanced approach, named SCOTCH+ (Source code and COncept based Test to Code traceability Hunter), to support the developer during the identification of links between unit tests and tested classes. Given a test class, represented by a JUnit class, the approach first exploits dynamic slicing to identify a set of candidate tested classes. Then, external and internal textual information associated with the classes retrieved by slicing is analyzed to refine this set of classes and identify the final set of candidate tested classes. The external information is derived from the analysis of the class name, while internal information is derived from identifiers and comments. The approach is evaluated on five software systems. The results indicate that the accuracy of the proposed approach far exceeds the leading techniques found in the literature. © 2013 Elsevier Inc. All rights reserved.","Dynamic slicing; Information retrieval; Test-to-code traceability"
"Adaptive reversible watermarking with improved embedding capacity","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884167627&doi=10.1016%2fj.jss.2013.06.055&partnerID=40&md5=e0d84fa69b7324ae305dc93ebc79f7d0","Embedding capacity is one of the most important issues of the reversible watermarking. However, the theoretical maximum embedding capacity of most reversible watermarking algorithms is only 1.0 bits per pixel (bpp). To achieve a higher capacity, we have to modify the least significant bit (LSB) multiple times which definitely lowers the quality of the embedded image. To this end, this paper proposes a novel reversible watermarking algorithm by employing histogram shifting and adaptive embedding. Specifically, the amount of the embedded watermark is adaptively determined in terms of the context of each pixel. For pixels with small prediction error, we modify the second, third and even the fourth LSBs as well to embed more than one watermark bit. Consequently, the proposed method achieves the embedding capacity larger than 1.0 bpp in single-pass embedding as well as bringing relatively low embedding distortion. The superiority of the proposed method is experimental verified by comparing with other existing schemes. © 2013 Elsevier Inc.","Histogram shifting; Reversible data hiding; Reversible watermarking"
"The presence and development of competency in IT programs","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887019405&doi=10.1016%2fj.jss.2013.07.029&partnerID=40&md5=3a3b7c76582612d08482ee9d7de3c087","Information technology (IT) programs are coordinated IT projects with a common business objective or underlying similar theme. Driving success in an IT program requires that the projects all work to achieve more global organizational goals than those of each individual project. These goals are better achieved in the presence of critical program team competences that include personnel development, dissemination of methodologies, and a key customer focus. These competences need to be developed to promote higher program performance where programs are dedicated to achieving business objectives of an organization. We propose a model based on the human resource model that considers the development of the critical competences when essential self and social competences are present in team members. Participation mechanisms of interpersonal cooperation and mutual support assist in the development of the critical competences. The model is supported by data collected from both quantitative survey and qualitative interviews with matched pairs of IT program managers and IT project managers. The results confirm the need to insure the presence of certain competences in team members and the construction of an environment that builds mutual support and cooperation. The human resource model is thus extended to include the inter-team environment of IT programs and further variables important to vendor competence. © 2013 Elsevier Inc. All rights reserved.","Human resource model; Program management; Program team competences"
"A three-phase energy-saving strategy for cloud storage systems","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888642604&doi=10.1016%2fj.jss.2013.08.018&partnerID=40&md5=56e99d62b351c215edce1117f2d814fe","In the running process of cloud data center, the idle data nodes will generate a large amount of unnecessary energy consumption. Furthermore, the resource misallocation will also cause a great waste of energy. This paper proposes a three-phase energy-saving strategy named TPES in order to save energy and operational costs for cloud suppliers. The three phases are replica management based on variable replication factor, cluster reconfiguration according to the optimal total costs and state transition based on observed and predicted workloads. These three phases save energy for the system at different levels which enhance the adaptability of our strategy. We evaluate our strategy using the expanded CloudSim toolkit and the results show that the proposed strategy achieves better energy reduction under different conditions in comparison with the existing schemes. © 2013 Elsevier Inc. All rights reserved.","Cloud storage systems; Cost efficient; Energy saving"
"Evaluating the perceived and estimated quality in use of Web 2.0 applications","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887015603&doi=10.1016%2fj.jss.2013.05.071&partnerID=40&md5=eaefad6eba1f2ab37278babc43f40554","Web 2.0 refers to a new generation of web applications where individuals are able to participate, collaborate, and share created artefacts. Despite the fact that Web 2.0 applications are widely used for both educational and professional purposes, a consolidated methodology for their evaluation is still not available. This paper presents and discusses the results of two empirical studies on the case of mind mapping and diagramming Web 2.0 applications. Both studies employed logging actual use method to measure the estimated quality in use, while the retrospective thinking aloud method and an online questionnaire were applied to assess the perceived quality in use. Achieved analytical results showed that the results of the estimated and the perceived quality in use match partially, which indicates that quality in use should be measured with both subjective and objective instruments. The work presented in this paper is the first step towards a comprehensive methodology for evaluating the quality in use of Web 2.0 applications. Consequently, the usage of the proposed quality in use model for other types of Web 2.0 applications as well as contexts of use needs to be investigated in order to draw generalizable conclusions. © 2013 Elsevier Inc. All rights reserved.","Evaluation methodology; Quality in use; Web 2.0"
"Visualizing protected variations in evolving software designs","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891625536&doi=10.1016%2fj.jss.2013.10.044&partnerID=40&md5=2d87a89b15f0c057648edfaf7c3b9773","Identifying and tracking evolving software structures at a design level is a challenging task. Although there are ways to visualize this information statically, there is a need for methods that help analyzing the evolution of software design elements. In this paper, we present a new visual approach to identify variability zones in software designs and explore how they evolve over time. To verify the usefulness of our approach, we did a user study in which participants had to browse software histories and find visual patterns. Most participants were able to find interesting observations and found our approach intuitive and useful. We present a number of design aspects that were observed by participants and the authors using our IHVis tool on four open-source projects. © 2013 Elsevier Inc. All rights reserved.","Information hiding; Software design; Software evolution; Software visualization"
"Adding semantic modules to improve goal-oriented analysis of data warehouses using I-star","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891626985&doi=10.1016%2fj.jss.2013.10.011&partnerID=40&md5=913db6a005df8cd166a1e68bcc3ff44e","The success rate of data warehouse (DW) development is improved by performing a requirements elicitation stage in which the users' needs are modeled. Currently, among the different proposals for modeling requirements, there is a special focus on goal-oriented models, and in particular on the i* framework. In order to adapt this framework for DW development, we previously developed a UML profile for DWs. However, as the general i* framework, the proposal lacks modularity. This has a specially negative impact for DW development, since DW requirement models tend to include a huge number of elements with crossed relationships between them. In turn, the readability of the models is decreased, harming their utility and increasing the error rate and development time. In this paper, we propose an extension of our i* profile for DWs considering the modularization of goals. We provide a set of guidelines in order to correctly apply our proposal. Furthermore, we have performed an experiment in order to assess the validity our proposal. The benefits of our proposal are an increase in the modularity and scalability of the models which, in turn, increases the error correction capability, and makes complex models easier to understand by DW developers and non expert users. © 2013 Elsevier Inc. All rights reserved.","Data warehouses; I-star; User requirements"
"Assessing the reliability, validity and acceptance of a classification scheme of usability problems (CUP)","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888644199&doi=10.1016%2fj.jss.2013.08.014&partnerID=40&md5=bde25708fd8cf225dbcd4fd855b66539","The aim of this study was to evaluate the Classification of Usability Problems (CUP) scheme. The goal of CUP is to classify usability problems further to give user interface developers better feedback to improve their understanding of usability problems, help them manage usability maintenance, enable them to find effective fixes for UP, and prevent such problems from reoccurring in the future. First, reliability was evaluated with raters of different levels of expertise and experience in using CUP. Second, acceptability was assessed with a questionnaire. Third, validity was assessed by developers in two field studies. An analytical comparison was also made to three other classification schemes. CUP reliability results indicated that the expertise and experience of raters are critical factors for assessing reliability consistently, especially for the more complex attributes. Validity analysis results showed that tools used by developers must be tailored to their working framework, knowledge and maturity. The acceptability study showed that practitioners are concerned with the effort spent in applying any tool. To understand developers' work and the implications of this study two theories are presented for understanding and prioritising UP. For applying classification schemes, the implications of this study are that training and context are needed. © 2013 Elsevier Inc. All rights reserved.","Defect classification; Usability problems; Validity"
"Modeling users on the World Wide Web based on cognitive factors, navigation behavior and clustering techniques","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887022191&doi=10.1016%2fj.jss.2013.04.029&partnerID=40&md5=70681e70cadcd2b9e2a3fd6a0dce786c","This paper focuses on modeling users' cognitive styles based on a set of Web usage mining techniques on user navigation patterns and clickstream data. Main aim is to investigate whether specific clustering techniques can group users of particular cognitive style using measures obtained from psychometric tests and content navigation behavior. Three navigation metrics are proposed and utilized to find identifiable groups of users that have similar navigation patterns in relation to their cognitive style. The proposed work has been evaluated with two user studies which entail a psychometric-based survey for extracting the users' cognitive styles, combined with a real usage scenario of users navigating in a controlled Web 2.0 environment. A total of 106 participants of age between 17 and 25 participated in the study providing interesting insights with respect to cognitive styles and navigation behavior of users. Studies like the reported one can be useful for modeling users and assist adaptive Web 2.0 environments to organize and present information and functionalities in an adaptive format to diverse user groups. © 2013 Elsevier Inc. All rights reserved.","Cognitive styles; User modeling; Web navigation behavior"
"A context awareness framework for cross-platform distributed applications","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891630028&doi=10.1016%2fj.jss.2013.10.018&partnerID=40&md5=5eaa84a7cb25d49831edfa7fd664f067","With the introduction of interconnected cross-platform middleware, a new area of opportunities for ubiquitous/pervasive computing has emerged. Context aware applications can be enhanced to practically and realistically incorporate multiple facets of human-machine interactions in everyday life that are not limited to a device-centered model for deducing context. In this paper, we propose that they can rather extend this model to a human-centered, device and platform independent model, based on a personal distributed application and data cloud ecosystem. For this to be achieved, webinos, a set of web runtime extensions that enable web applications and services to be used and shared consistently and securely over a broad spectrum of converged and connected devices, is used to provide this ecosystem. The webinos Context Awareness Framework described here is accessible to each webinos-enabled application. After strict policy enforcement, it can collect contextual information, either via an automatic mechanism that intercepts native calls made by webinos applications through the various webinos APIs, via an automatic polling mechanism to these APIs, or via custom, application-specific context schema extensions. It can then distribute the contextual information from its own personal cloud storage mechanism, in the form of simple, manageable and intuitive Context Objects, to and from all webinos-enabled devices owned by the same user, or even other, authorized users. © 2013 Elsevier Inc. All rights reserved.","Context awareness; Cross-platform; Webinos"
"SelfMotion: A declarative approach for adaptive service-oriented mobile applications","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898771265&doi=10.1016%2fj.jss.2013.10.057&partnerID=40&md5=ba292de8b6c9b5854ac49fdc3c15f513","Modern society increasingly relies on mobile devices. This explains the growing demand for high quality software for such devices. To improve the efficiency of the development life-cycle, shortening time-to-market while keeping quality under control, mobile applications are typically developed by composing together ad-hoc developed components, services available on-line, and other third-party mobile applications. Applications are thus built as heterogeneous compositions, whose characteristics strongly depend on the components and services they integrate. To cope with unpredictable changes and failures, but also with the various settings offered by the plethora of available devices, mobile applications need to be as adaptive as possible. However, mainstream adaptation strategies are usually defined imperatively and require complex control strategies strongly intertwined with the application logic, yielding to applications that are difficult to build, maintain, and evolve. We address this issue by proposing a declarative approach to compose adaptive heterogeneous mobile applications the advantages of this approach are demonstrated through an example inspired by an existing worldwide distributed mobile application, while the implementation of the proposed solution has been validated through a set of simulations and experiments aimed at illustrating its performance. © 2013 Elsevier Inc.","Declarative language; Mobile applications; Self-adaptive systems"
"Predicting software defects with causality tests","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900800922&doi=10.1016%2fj.jss.2014.01.033&partnerID=40&md5=6aa8b63f97cceb60ccc8627c27b7567b","In this paper, we propose a defect prediction approach centered on more robust evidences towards causality between source code metrics (as predictors) and the occurrence of defects. More specifically, we rely on the Granger causality test to evaluate whether past variations in source code metrics values can be used to forecast changes in time series of defects. Our approach triggers alarms when changes made to the source code of a target system have a high chance of producing defects. We evaluated our approach in several life stages of four Java-based systems. We reached an average precision greater than 50% in three out of the four systems we evaluated. Moreover, by comparing our approach with baselines that are not based on causality tests, it achieved a better precision. © 2014 Elsevier Inc.","Causality; Defect prediction; Granger test"
"Common weaving approach in mainstream languages for software security hardening","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882691817&doi=10.1016%2fj.jss.2013.05.044&partnerID=40&md5=05b39cbec967bb9817a66c1062552cc3","In this paper, we propose a novel aspect-oriented approach based on GIMPLE, a language-independent and a tree-based representation generated by the GNU Compiler Collection (GCC), for the systemization of application security hardening. The security solutions are woven into GIMPLE representations in a systematic way, eliminating the need for manual hardening that might generate a considerable number of errors. To achieve this goal, we present a formal specification for GIMPLE weaving and the implementation strategies of the proposed weaving semantics. Syntax for a common aspect-oriented language that is abstract and multi-language support together with syntax for a core set for GIMPLE constructs are presented to express the weaving semantics. GIMPLE weaving accompanied by a common aspect-oriented language (1) allows security experts providing security solutions using this common language, (2) lets developers focus on the main functionality of programs by relieving them from the burden of security issues, (3) unifies the matching and the weaving processes for mainstream languages, and (4) facilitates introducing new security features in AOP languages. We handle the correctness and the completeness of GIMPLE weaving in two different ways. In the first approach, we prove them according to the rules and algorithms provided in this paper. In the second approach, we accommodate Kniesel's discipline that ensures that security solutions specified by our approach are applied at all and only the required points in source code, taking into consideration weaving interactions and interferences. Finally, we explore the viability and the relevance of our propositions by applying the defined approach for systematic security hardening to develop case studies. © 2013 Elsevier Inc.","Application security hardening; Aspect-oriented programming (AOP); GIMPLE"
"Detecting Web requirements conflicts and inconsistencies under a model-based perspective","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887015555&doi=10.1016%2fj.jss.2013.05.045&partnerID=40&md5=f3fce5f3b7d1e1489a1e423445b3ddba","Web requirements engineering is an essential phase in the software project life cycle for the project results. This phase covers different activities and tasks that in many situations, depending on the analyst's experience or intuition, help getting accurate specifications. One of these tasks is the conciliation of requirements in projects with different groups of users. This article presents an approach for the systematic conciliation of requirements in big projects dealing with a model-based approach. The article presents a possible implementation of the approach in the context of the NDT (Navigational Development Techniques) Methodology and shows the empirical evaluation in a real project by analysing the improvements obtained with our approach. The paper presents interesting results that demonstrate that we can get a reduction in the time required to find conflicts between requirements, which implies a reduction in the global development costs. © 2013 Elsevier Inc. All rights reserved.","Consistency; Contradiction; Web requirements"
"Web application testing: A systematic literature review","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900597825&doi=10.1016%2fj.jss.2014.01.010&partnerID=40&md5=0f1abd2f495de26808fd8b888395c4fe","Context The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field. Methods We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area. Results Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community. © 2014 Elsevier Inc.","Systematic literature review; Testing; Web application"
"Towards an ideal service QoS in fuzzy logic-based adaptation planning middleware","2014","Journal of Systems and Software","10.1016/j.jss.2013.07.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898804862&doi=10.1016%2fj.jss.2013.07.023&partnerID=40&md5=9eeead0ab25f0dcaefe255a21d3c3a00","Mobile applications require an adaptation phase to adapt to the user's and application context. Utility functions or rules are most often used to make the adaptation planning or decision, i.e. select the most adapted variant for each required service. Fuzzy controllers are used when it is difficult or even impossible to construct precise mathematical models. In the case of mobile applications, the large number of Quality of Service (QoS) and context parameters causes an exponential increase in the number of rules (aka. rule explosion problem), that increases the processing time of the adaptation planning. To reduce the processing time and simplify the fuzzy control system, we propose the concept of ideal QoS. Fuzzy values of ideal QoS parameters are calculated using several fuzzy control systems to fit the context state and user preferences. A fuzzy logic similarity metric based on fuzzy sets and fuzzy operators is proposed to select the service variant having the nearest QoS values to the ideal. Experiments show that our approach can significantly improve both the number of rules and the processing time when selecting the variant that well adapts to environment changes. © 2013 Elsevier Inc.","Adaptation planning; Context aware computing; Fuzzy logic; Ideal QoS; Middleware"
"What are the roles of software product managers? An empirical investigation","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887017858&doi=10.1016%2fj.jss.2013.07.045&partnerID=40&md5=aa7452915e87e1359a0ebd5f4c438bf5","Software product management covers both technical and business activities to management of products like roadmaps, strategic, tactical, and release planning. In practice, one product manager is seldom responsible for all these activities but several persons share the responsibilities. Therefore, it is important to understand the boundaries of product managers' work in managing software products, as well as the impact a product manager has on the company business. The purpose of the study is to clarify what roles of software product managers exist and understand how these roles are interrelated with each other and the whole structure and business of an organization. The study is designed as an interpretative qualitative study using grounded theory as the research method. Based on the gathered data we developed a framework that reveals the role of a product manager in the organization and shows how this role can evolve by extending the level of responsibilities. Using the framework, we identified four stereotypical roles of product managers in the studied organizations: experts, strategists, leaders, and problem solvers. The presented framework shows that product managers' roles are not limited to the conception of the ""mini-CEO."" The results allow product managers and top management to collaborate effectively by assigning responsibilities and managing expectations by having a common tool for understanding the role of product managers in the organization. © 2013 Elsevier Inc. All rights reserved.","Grounded theory; Management roles; Software product management"
"MOO: An architectural framework for runtime optimization of multiple system objectives in embedded control software","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882688378&doi=10.1016%2fj.jss.2013.04.002&partnerID=40&md5=43c816dc5e614c0e6cb3793360199d89","Today's complex embedded systems function in varying operational conditions. The control software adapts several control variables to keep the operational state optimal with respect to multiple objectives. There exist well-known techniques for solving such optimization problems. However, current practice shows that the applied techniques, control variables, constraints and related design decisions are not documented as a part of the architecture description. Their implementation is implicit, tailored for specific characteristics of the embedded system, tightly integrated into and coupled with the control software, which hinders its reusability, analyzability and maintainability. This paper presents an architectural framework to design, document and realize multi-objective optimization in embedded control software. The framework comprises an architectural style together with its visual editor and domain-specific analysis tools, and a code generator. The code generator generates an optimizer module specific for the given architecture and it employs aspect-oriented software development techniques to seamlessly integrate this module into the control software. The effectiveness of the framework is validated in the context of an industrial case study from the printing systems domain. © 2013 Elsevier Inc.","Architectural framework; Control software; Embedded systems; Multi-objective optimization; Runtime adaptation"
"Real-time risk monitoring in business processes: A sensor-based approach","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884134229&doi=10.1016%2fj.jss.2013.07.024&partnerID=40&md5=065331a5304efda8a8184c0a2a6faf31","This article proposes an approach for real-time monitoring of risks in executable business process models. The approach considers risks in all phases of the business process management lifecycle, from process design, where risks are defined on top of process models, through to process diagnosis, where risks are detected during process execution. The approach has been realized via a distributed, sensor-based architecture. At design-time, sensors are defined to specify risk conditions which when fulfilled, are a likely indicator of negative process states (faults) to eventuate. Both historical and current process execution data can be used to compose such conditions. At run-time, each sensor independently notifies a sensor manager when a risk is detected. In turn, the sensor manager interacts with the monitoring component of a business process management system to prompt the results to process administrators who may take remedial actions. The proposed architecture has been implemented on top of the YAWL system, and evaluated through performance measurements and usability tests with students. The results show that risk conditions can be computed efficiently and that the approach is perceived as useful by the participants in the tests. © 2013 Elsevier Inc.","Business process management; Risk; YAWL"
"Power-aware scheduling algorithms for sporadic tasks in real-time systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882726153&doi=10.1016%2fj.jss.2013.04.075&partnerID=40&md5=4d131e03fd3f546448b1f78b0ddb2301","In this paper, we consider the canonical sporadic task model with the system-wide energy management problem. Our solution uses a generalized power model, in which the static power and the dynamic power are considered. We present a static solution to schedule the sporadic task set, assuming worst-case execution time for each sporadic tasks release, and propose a dynamic solution to reclaim the slacks left by the earlier completion of tasks than their worst-case estimations. The experimental results show that the proposed static algorithm can reduce the energy consumption by 20.63%-89.70% over the EDF* algorithm and the dynamic algorithm consumes 2.06%-24.89% less energy than that of the existing DVS algorithm. © 2013 Elsevier Inc.","Dynamic voltage scaling; Real-time system; Sporadic task"
"An empirical study on the use of mutant traces for diagnosis of faults in deployed systems","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894496663&doi=10.1016%2fj.jss.2013.11.1094&partnerID=40&md5=a4283f152b27caf716dcc3a99fab1750","Debugging deployed systems is an arduous and time consuming task. It is often difficult to generate traces from deployed systems due to the disturbance and overhead that trace collection may cause on a system in operation. Many organizations also do not keep historical traces of failures. On the other hand earlier techniques focusing on fault diagnosis in deployed systems require a collection of passing-failing traces, in-house reproduction of faults or a historical collection of failed traces. In this paper, we investigate an alternative solution. We investigate how artificial faults, generated using software mutation in test environment, can be used to diagnose actual faults in deployed software systems. The use of traces of artificial faults can provide relief when it is not feasible to collect different kinds of traces from deployed systems. Using artificial and actual faults we also investigate the similarity of function call traces of different faults in functions. To achieve our goal, we use decision trees to build a model of traces generated from mutants and test it on faulty traces generated from actual programs. The application of our approach to various real world programs shows that mutants can indeed be used to diagnose faulty functions in the original code with approximately 60-100% accuracy on reviewing 10% or less of the code; whereas, contemporary techniques using pass-fail traces show poor results in the context of software maintenance. Our results also show that different faults in closely related functions occur with similar function call traces. The use of mutation in fault diagnosis shows promising results but the experiments also show the challenges related to using mutants. © 2013 Elsevier Inc.","Fault diagnosis; Mutation; Software maintenance"
"Combining mutation and fault localization for automated program debugging","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894497912&doi=10.1016%2fj.jss.2013.10.042&partnerID=40&md5=6d0516401526933d0dba591f03da9992","This paper proposes a strategy for automatically fixing faults in a program by combining the ideas of mutation and fault localization. Statements ranked in order of their likelihood of containing faults are mutated in the same order to produce potential fixes for the faulty program. The proposed strategy is evaluated using 8 mutant operators against 19 programs each with multiple faulty versions. Our results indicate that 20.70% of the faults are fixed using selected mutant operators, suggesting that the strategy holds merit for automatically fixing faults. The impact of fault localization on efficiency of the overall fault-fixing process is investigated by experimenting with two different techniques, Tarantula and Ochiai, the latter of which has been reported to be better at fault localization than Tarantula, and also proves to be better in the context of fault-fixing using our proposed strategy. Further experiments are also presented to evaluate stopping criteria with respect to the mutant examination process and reveal that a significant fraction of the (fixable) faults can be fixed by examining a small percentage of the program code. We also report on the relative fault-fixing capabilities of mutant operators used and present discussions on future work. © 2013 Elsevier Inc.","Fault localization; Fault-fixing; Mutation; Program debugging; Software testing"
"Evolving feature model configurations in software product lines","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888645293&doi=10.1016%2fj.jss.2013.10.010&partnerID=40&md5=8ba32b418ee9714829c54449d3e18c41","The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps. © 2013 Elsevier Inc.","Feature model; Multi-step configuration; Software product line"
"CodeCloud: A platform to enable execution of programming models on the Clouds","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900795283&doi=10.1016%2fj.jss.2014.02.005&partnerID=40&md5=840486559c938ce47fb216eb9cf047ad","This paper presents a platform that supports the execution of scientific applications covering different programming models (such as Master/Slave, Parallel/MPI, MapReduce and Workflows) on Cloud infrastructures. The platform includes (i) a high-level declarative language to express the requirements of the applications featuring software customization at runtime, (ii) an approach based on virtual containers to encapsulate the logic of the different programming models, (iii) an infrastructure manager to interact with different IaaS backends, (iv) a configuration software to dynamically configure the provisioned resources and (v) a catalog and repository of virtual machine images. By using this platform, an application developer can adapt, deploy and execute parallel applications agnostic to the Cloud backend. © 2014 Elsevier Inc.","Cloud computing; Elasticity; Virtual infrastructures"
"The Meta-Protocol framework","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.096","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884127500&doi=10.1016%2fj.jss.2013.05.096&partnerID=40&md5=a7532d32448890510635ec53a3edf9cc","A communication protocol is a set of rules shared by two or more communicating parties on the sequence of operations and the format of messages to be exchanged. Standardization organizations define protocols in the form of recommendations (e.g., RFC) written in technical English, which requires a manual translation of the specification into the protocol implementation. This human translation is error-prone due in part to the ambiguities of natural language and in part due to the complexity of some protocols. To mitigate these problems, we divided the expression of a protocol specification into two parts. First, we designed an XML-based protocol specification language (XPSL) that allows for the high-level specification of a protocol - expressed as a Finite State Machine (FSM) - using Component-Based Software Engineering (CBSE) principles. Then, the components required by the protocol are specified in any suitable technical language (formal or informal). In addition, we developed the multi-layer Meta-Protocol framework, which allows for on-the-fly protocol discovery and negotiation, distribution of protocol specifications and components, and automatic protocol implementation in any programming language. © 2013 Elsevier Inc.","Protocol; Security; XML"
"Code smells as system-level indicators of maintainability: An empirical study","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882716230&doi=10.1016%2fj.jss.2013.05.007&partnerID=40&md5=0032e0f1876939cca348e7a7be80fd5c","Context Code smells are manifestations of design flaws that can degrade code maintainability. So far, no research has investigated if these indicators are useful for conducting system-level maintainability evaluations. Aim The research in this paper investigates the potential of code smells to reflect system-level indicators of maintainability. Method We evaluated four medium-sized Java systems using code smells and compared the results against previous evaluations on the same systems based on expert judgment and the Chidamber and Kemerer suite of metrics. The systems were maintained over a period of up to 4 weeks. During maintenance, effort (person-hours) and number of defects were measured to validate the different evaluation approaches. Results Most code smells are strongly influenced by size; consequently code smells are not good indicators for comparing the maintainability of systems differing greatly in size. Also, from the comparison of the different evaluation approaches, expert judgment was found as the most accurate and flexible since it considered effects due to the system's size and complexity and could adapt to different maintenance scenarios. Conclusion Code smell approaches show promise as indicators of the need for maintenance in a way that other purely metric-based approaches lack. © 2013 Elsevier Inc.","Code smells; Empirical study; Maintainability; System evaluation"
"A MIH-based approach for best network selection in heterogeneous wireless networks","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898785076&doi=10.1016%2fj.jss.2014.01.031&partnerID=40&md5=9b9cfa466f77e5fbe3778cd641613a0f","In the next generation wireless networks, different technologies belonging to one or more operators should be integrated to form a heterogeneous environment based on an IP core network infrastructure. This ensures user mobility and service continuity by maintaining connections when switching between various technologies and it introduces new resources and possibilities for applications. In this context, an automatic interface selection based on instantaneous and practical constraints and user preferences (Quality of Service (QoS) parameters, available resources, security, power consumption, etc.) is therefore required the different network selection and handover schemes proposed in the literature can be classified into three approaches according to who is responsible for making the handover decision: the terminal, the network or by a cooperation between both of them. However, these approaches keep presenting some drawbacks; namely the problem of resources management and network load balancing whenever the selection is controlled by the mobile terminal (MT) and the problem of scalability and unknown operator's management policy whenever the selection is rather controlled by the network. In this article, first we propose a MIH based approach for handover initiation and preparation for heterogeneous wireless network the proposed framework is based on the principals of IEEE 802.21 for context information gathering and optimized handover decision making. Second, we propose a new architecture and new network selection scheme that explicitly take into account the current resource usage and the user preferences. Furthermore, our solution ensures the selection of the most suitable network for each flow while taking into consideration its expectations in terms of QoS. A feasibility study of implementing a new architecture on a single MT is evaluated by using typical scenarios and using various algorithms. Thanks to the introduced function entities and modules in the proposed architecture, network utilization balancing and user and application expectations, which are successfully assured without operator intervention. Performance analysis shows that the proposed algorithm best meets the common quality requirements. © 2014 Elsevier Inc.","Best network selection; Heterogeneous wireless networks; Media independent handover"
"A novel approach to evaluate software vulnerability prioritization","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884133469&doi=10.1016%2fj.jss.2013.06.040&partnerID=40&md5=2ffed7f23a7d318395c4020a7c7f3cf7","The aim of this study is to formulate an analysis model which can express the security grades of software vulnerability and serve as a basis for evaluating danger level of information program or filtering hazardous weaknesses of the system and improve it to counter the threat of different danger factors. Through the utilization of fuzzy analytic hierarchy process (FAHP), we will organize the crossover factors of the software blind spots and build an evaluation framework. First of all, via the fuzzy Delphi method the aspects and relative determinants affecting security will be filtered out. Then we will identify the value equation of each factor and settle down the fuzzy synthetic decision making model of software vulnerability. Thanks to this model we will be able to analyze the various degrees to which the vulnerability is affecting the security and this information will serve as a basis for future ameliorations of the system itself. The higher the security score obtained therefore imply securer system. Beside this, this study also propose an improvement from the traditional fuzzy synthetic decision making model for measuring the fuzziness between enhancement and independence of various aspects and criteria. Furthermore taking into consideration the subjectivity of human in reality and constructing the fuzzy integral decision making model. Through case study, we show that the evaluation model in question is practical and can be applied on the new software vulnerabilities and measure their degree of penetration. The fuzzy integral decision making emphasize through formulation the multiply-add effect between different factors influencing information security.© 2013 Elsevier Inc.","Fuzzy AHP; Fuzzy integral; Fuzzy synthetic; Security evaluation; Software vulnerability"
"A review on E-business Interoperability Frameworks","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900796812&doi=10.1016%2fj.jss.2014.02.004&partnerID=40&md5=024f4da5481f8a03919830c9534bc1b4","Interoperability frameworks present a set of assumptions, concepts, values, and practices that constitute a method of dealing with interoperability issues in the electronic business (e-business) context. Achieving interoperability in the e-business generates numerous benefits. Thus, interoperability frameworks are the main component of e-business activities. This paper describes the existing interoperability frameworks for e-business, and performs a comparative analysis among their findings to determine the similarities and differences in their philosophy and implementation. This analysis yields a set of recommendations for any party that is open to the idea of creating or improving an E-business Interoperability Framework. © 2014 Elsevier Inc.","E-business; Framework; Interoperability"
"Affinity-aware modeling of CPU usage with communicating virtual machines","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882680347&doi=10.1016%2fj.jss.2013.04.085&partnerID=40&md5=f38fa72b806a3627bac29745e21991b7","Use of virtualization in Infrastructure as a Service (IaaS) environments provides benefits to both users and providers: users can make use of resources following a pay-per-use model and negotiate performance guarantees, whereas providers can provide quick, scalable and hardware-fault tolerant service and also utilize resources efficiently and economically. With increased acceptance of virtualization-based systems, an important issue is that of virtual machine migration-enabled consolidation and dynamic resource provisioning. Effective resource provisioning can result in higher gains for users and providers alike. Most hosted applications (for example, web services) are multi-tiered and can benefit from their various tiers being hosted on different virtual machines. These mutually communicating virtual machines may get colocated on the same physical machine or placed on different machines, as part of consolidation and flexible provisioning strategies. In this work, we argue the need for network affinity-awareness in resource provisioning for virtual machines. First, we empirically quantify the change in CPU resource usage due to colocation or dispersion of communicating virtual machines for both Xen and KVM virtualization technologies. Next, we build models based on these empirical measurements to predict the change in CPU utilization when transitioning between colocated and dispersed placements. Due to the modeling process being independent of virtualization technology and specific applications, the resultant model is generic and application-agnostic. Via extensive experimentation, we evaluate the applicability of our models for synthetic and benchmark application workloads. We find that the models have high prediction accuracy - maximum prediction error within 2% absolute CPU usage. © 2013 Elsevier Inc.","Network-affinity; Platform virtualization; Resource provisioning"
"Surviving sensor node failures by MMU-less incremental checkpointing","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888638235&doi=10.1016%2fj.jss.2013.09.001&partnerID=40&md5=2ebefd19fb48b4681d8220c1d0769542","For some critical safety applications, sensor nodes embed valuable information, and they should be able to operate unattended and unfailing for several months or years. One promising solution is to adopt a checkpointing that periodically saves the state of a sensor node, thereby maintaining node reliability and network availability. Thus, this study first shows the design and implementation of a full checkpointing for WSNs. However, checkpointing is expensive. Therefore, incremental checkpointing was previously proposed to eliminate the checkpoint overhead by relying on the page protection hardware to identify dirty pages. Because sensor nodes are resource-constrained and do not equip with the page protection hardware, previous incremental checkpointings cannot be directly applied. To address this issue, this paper proposes three incremental checkpointings for WSNs. These three methods differ in the granularity of the checkpoint memory data unit and module execution overhead. In addition, we designed an incremental checkpoint file format that simultaneously supports proposed three different incremental checkpointings and accommodates them with sensor network characteristics. We implemented the full and three incremental checkpointings on SOS in the mica2 sensor motes. A performance evaluation of the three incremental checkpointings is presented. We also discuss and evaluate a method for selecting the appropriate incremental checkpointing. To the best of our knowledge, this study is the first to design and implement incremental checkpointing in MMU-less WSNs. © 2013 Elsevier Inc. All rights reserved.","Checkpointing; Incremental checkpointing; SOS"
"A secure Boolean-based multi-secret image sharing scheme","2014","Journal of Systems and Software","10.1016/j.jss.2014.01.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898780463&doi=10.1016%2fj.jss.2014.01.001&partnerID=40&md5=911a04cf4bae1d521acab51740eb781c","An (n, n) multi-secret image sharing scheme shares n secret images among n shared images. In this type of schemes, n shared images can be used to recover all n secret images, but the loss of any shared image prevents the recovery of any secret image. Among existing image sharing techniques, Boolean-based secret schemes have good performance because they only require XOR calculation. This study presents a secure Boolean-based secret image sharing scheme that uses a random image generating function to generate a random image from secret images or shared images the proposed function efficiently increases the sharing capacity on free of sharing the random image the use of a bit shift subfunction in the random image generating function produces a random image to meet the random requirement. Experimental results show that the proposed scheme requires minimal CPU computation time to share or recover secret images the time required to share n secret images is nearly the time as that required to recover n secret images the bit shift subfunction takes more computation load than the XOR subfunction needs. © 2014 Elsevier Inc.","Boolean; Multiple secret images; XOR"
"Identifying organizational barriers - A case study of usability work when developing software in the automation industry","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891625113&doi=10.1016%2fj.jss.2013.09.019&partnerID=40&md5=e484ea7553c54b297fcb1a5ee14a8eb6","This study investigates connections between usability efforts and organizational factors. This is an important field of research which so far appears to be insufficiently studied and discussed. It illustrates problems when working with software engineering tasks and usability requirements. It deals with a large company that manufactures industrial robots with an advanced user interface, which wanted to introduce usability KPIs, to improve product quality. The situation in the company makes this difficult, due to a combination of organizational and behavioural factors that led to a ""wicked problem"" that caused conflicts, breakdowns and barriers. Addressing these problems requires a holistic view that places context in the foreground and technological solutions in the background. Developing the right product requires communication and collaboration between multiple stakeholders. The inclusion of end users, who fully understand their own work context, is vital. Achieving this is dependent on organizational change, and management commitment. One step to beginning this change process may be through studying ways to introduce user-centred design processes. © 2013 Elsevier Inc. All rights reserved.","Human computer interaction; KPI; Usability; User centred design; Wicked problem"
"An approach to testing commercial embedded systems","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891627498&doi=10.1016%2fj.jss.2013.10.041&partnerID=40&md5=1f67d1fe8939c9bd742bde650400d02c","A wide range of commercial consumer devices such as mobile phones and smart televisions rely on embedded systems software to provide their functionality. Testing is one of the most commonly used methods for validating this software, and improved testing approaches could increase these devices' dependability. In this article we present an approach for performing such testing. Our approach is composed of two techniques. The first technique involves the selection of test data; it utilizes test adequacy criteria that rely on dataflow analysis to distinguish points of interaction between specific layers in embedded systems and between individual software components within those layers, while also tracking interactions between tasks. The second technique involves the observation of failures: it utilizes a family of test oracles that rely on instrumentation to record various aspects of a system's execution behavior, and compare observed behavior to certain intended system properties that can be derived through program analysis. Empirical studies of our approach show that our adequacy criteria can be effective at guiding the creation of test cases that detect faults, and our oracles can help expose faults that cannot easily be found using typical output-based oracles. Moreover, the use of our criteria accentuates the fault-detection effectiveness of our oracles. © 2013 Elsevier Inc. All rihgts reserved.","Embedded systems; Software test adequacy criteria; Test oracles"
"Re-engineering legacy Web applications into RIAs by aligning modernization requirements, patterns and RIA features","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887025568&doi=10.1016%2fj.jss.2013.04.053&partnerID=40&md5=b62aacedf0058899d1a4da6267369ffe","Rich Internet Applications (RIAs) have become a common platform for Web developments. Its adoption has been accelerated thanks to different factors, among others, the appearance of patterns for typical RIA behaviors and the extension of different Model Driven Web Engineering methodologies to introduce RIA concepts. The real fact is that more and more developers are switching to RIA technologies and, thus, the modernization of legacy Web applications into RIAs has become a trend topic. However, this modernization process lacks of a systematic approach. Currently, it is done in an ad hoc manner, being expensive and error-prone. This work presents a systematic process to modernize legacy Web applications into RIAs. The process is based on the use of traceability matrices that relate modernization requirements, RIA features and patterns. Performing some operations on these matrices, they provide the analyst with the necessary information about the suitability of a pattern or set of patterns to address a given requirement. This work also introduces two measures, the degree of requirement realization and the degree of pattern realization, which are used to discuss the pattern selection. Finally, the applicability of the approach is evaluated by using it in several Web systems. © 2013 Elsevier Inc.","Model driven web engineering; Modernization; RIAs"
"Mosco: A privacy-aware middleware for mobile social computing","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898792164&doi=10.1016%2fj.jss.2013.11.1110&partnerID=40&md5=8a37285c80b086638dce538c98474271","The proliferation of mobile devices coupled with Internet access is generating a tremendous amount of highly personal and sensitive data. Applications such as location-based services and quantified self harness such data to bring meaningful context to users' behavior. As social applications are becoming prevalent, there is a trend for users to share their mobile data the nature of online social networking poses new challenges for controlling access to private data, as compared to traditional enterprise systems. First, the user may have a large number of friends, each associated with a unique access policy. Second, the access control policies must be dynamic and fine-grained, i.e they are content-based, as opposed to all-or-nothing. In this paper, we investigate the challenges in sharing of mobile data in social applications. We design and evaluate a middleware running on Google App Engine, named Mosco, that manages and facilitates sharing of mobile data in a privacy-preserving manner. We use Mosco to develop a location sharing and a health monitoring application. Mosco helps shorten the development process. Finally, we perform benchmarking experiments with Mosco, the results of which indicate small overhead and high scalability. © 2013 Elsevier Inc.","Fine-grained access control; Google App Engine; Social computing; XACML"
"On the reliability of mapping studies in software engineering","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882695716&doi=10.1016%2fj.jss.2013.04.076&partnerID=40&md5=097cdd6ea83b702f2c96adbfb2e4edda","Background Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies. Objective This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering. Method The research is based on an in-depth case study of two published mapping studies on software product line testing. Results We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies. Conclusions From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners. © 2013 Elsevier Inc.","Review of reviews; Software product lines; Software testing; Systematic literature review; Systematic mapping study"
"Software generated device exception for more intensive device-related software testing: An industrial field study","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887015859&doi=10.1016%2fj.jss.2013.07.058&partnerID=40&md5=d69b7a3c9a857f4e8eb3d23c1f89e2d0","It is more important to properly handle exceptions, than to prevent exceptions from occurring, because they arise from so many different causes. In embedded systems, a vast number of exceptions are caused by hardware devices. In such cases, numerous software components are involved in these hardware device-originated exceptions, ranging from the device itself to the device driver, the kernel, and applications. Therefore, it takes a lot of time to debug software that fails to handle exceptions. This paper proposes a lightweight device exception testing method, and a related automation tool, AMOS v3.0. The proposed method artificially triggers more realistic device exceptions in runtime, and monitors how software components handle exceptions in detail. AMOS v3.0 has been applied to the exception testing of car-infotainment systems in an automobile company. The results based on this industrial field study have revealed that 39.13% of the failures in exception handling were caused by applications, 36.23% of the failures were caused by device drivers, and 24.64% were derived from the kernel. We conclude that the proposed method is highly effective, in that it can allow developers to identify the root cause of failure for exception handling. © 2013 Elsevier Inc. All rights reserved.","Device exception; Device manager hacking; Embedded software; Exception test; Fault-injection method"
"A high-performance reversible data-hiding scheme for LZW codes","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884165874&doi=10.1016%2fj.jss.2013.06.024&partnerID=40&md5=0bbbeff4391eabdf059f55b38fbf61cd","Hiding a message in compression codes can reduce transmission costs and simultaneously make the transmission more secure. In this paper, we propose a high-performance, data-hiding Lempel-Ziv-Welch (HPDH-LZW) scheme, which reversibly embeds data in LZW compression codes by modifying the value of the compression codes, where the value of the LZW code either remains unchanged or is changed to the original value of the LZW code plus the LZW dictionary size according to the data to be embedded. Compared to other information-hiding schemes based on LZW compression codes, the proposed scheme achieves better hiding capacity by increasing the number of symbols available to hide secrets and also achieves faster hiding and extracting speeds due to the lower computation requirements. Our experimental results with the proposed scheme have confirmed both its high embedding capacity and its high speed when hiding and extracting data. © 2013 Elsevier Inc.","Information-hiding; LZW; Steganography"
"A model view controller based Self-Adjusting Clustering Framework","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1112","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895065032&doi=10.1016%2fj.jss.2013.11.1112&partnerID=40&md5=d8bf7d1f1d7e83bcf21fa27aa9fc505f","A load balanced cluster is an abstraction for set of servers that are configured to share the workload. Each server of the cluster hosts the same set of applications or services. The process of application deployment is laborious and it is essential to efficiently manage a cluster. The existing techniques allow automation of the initial deployment and dynamic scaling. However, after the initial deployment, they do not ensure cluster members' consistency. This is quite important, as change is inevitable in the life of a software application. A new application may need to be deployed to an existing cluster or an existing application may need to be upgraded. In this paper, we propose a Model View Controller based, Self-Adjusting Cluster Framework (SACF) that enables auto deployment, auto upgradation and cluster members' consistency. © 2013 Elsevier Inc.","Auto deployment; Cluster consistency; Cluster management"
"Real-time data dissemination in mobile peer-to-peer networks","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894464138&doi=10.1016%2fj.jss.2013.11.1122&partnerID=40&md5=b0d6a71061b6c32a5bfd69355d52ed24","Mobile peer-to-peer networks have found many uses such as streaming of audio and video data. There are circumstances, such as emergency situations and disaster recovery, when real-time delivery is a fundamental requirement. The problem is challenging due to the limited network capacity, the variable transmission rates and the unpredictability with respect to the network conditions in the mobile peer-to-peer network. In this paper we address the problem of real-time data dissemination of multimedia streams in mobile peer-to-peer networks. Four routing algorithms are proposed based on a packet's deadline, priority or a combination of these metrics. They are simulated under different setups in a mobile peer-to-peer network with Bluetooth connectivity and nodes broadcasting audio and video streams using different priorities. We compare the performance of the algorithms using a number of metrics. Detailed experimental results are presented. Based on these results, propositions on the usage of the algorithms and the design of network requirements are presented. © 2013 Elsevier Inc.","Mobile peer-to-peer networks; Peer-to-peer streaming; Real-time dissemination"
"Mobile cloud middleware","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898800045&doi=10.1016%2fj.jss.2013.09.012&partnerID=40&md5=5ee13e95dcffb2a93c63edc280a2fabf","Mobile Cloud Computing (MCC) is arising as a prominent research area that is seeking to bring the massive advantages of the cloud to the constrained smartphones. Mobile devices are looking towards cloud-aware techniques, driven by their growing interest to provide ubiquitous PC-like functionality to mobile users these functionalities mainly target at increasing storage and computational capabilities. Smartphones may integrate those functionalities from different cloud levels, in a service oriented manner within the mobile applications, so that a mobile task can be delegated by direct invocation of a service. However, developing these kind of mobile cloud applications requires to integrate and consider multiple aspects of the clouds, such as resource-intensive processing, programmatically provisioning of resources (Web APIs) and cloud intercommunication. To overcome these issues, we have developed a Mobile Cloud Middleware (MCM) framework, which addresses the issues of interoperability across multiple clouds, asynchronous delegation of mobile tasks and dynamic allocation of cloud infrastructure. MCM also fosters the integration and orchestration of mobile tasks delegated with minimal data transfer. A prototype of MCM is developed and several applications are demonstrated in different domains. To verify the scalability of MCM, load tests are also performed on the hybrid cloud resources the detailed performance analysis of the middleware framework shows that MCM improves the quality of service for mobiles and helps in maintaining soft-real time responses for mobile cloud applications. © 2013 Elsevier Inc.","Code offloading; Interoperability; Mobile Cloud Computing; Task delegation"
"Automatic optimisation of system architectures using EAST-ADL","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882683983&doi=10.1016%2fj.jss.2013.04.001&partnerID=40&md5=9196e94221e6c3dc13ec6d430569228a","There are many challenges which face designers of complex system architectures, particularly safety-critical or real-time systems. The introduction of Architecture Description Languages (ADLs) has helped to meet these challenges by consolidating information about a system and providing a platform for modelling and analysis capabilities. However, managing this wealth of information can still be problematic, and evaluation of potential design decisions is still often performed manually. Automatic architectural optimisation can be used to assist this decision process, enabling designers to rapidly explore many different options and evaluate them according to specific criteria. In this paper, we present a multi-objective optimisation approach based on EAST-ADL, an ADL in the automotive domain, with the goal of combining the advantages of ADLs and architectural optimisation. The approach is designed to be extensible and leverages the capabilities of EAST-ADL to provide support for evaluation according to different factors, including dependability, timing/performance, and cost. The technique is applied to an illustrative example system featuring both hardware and software perspectives, demonstrating the potential benefits of this concept to the design of embedded system architectures. © 2013 Elsevier Inc.","Architectural description languages; Dependability Analysis; Multi-objective optimization; Timing Analysis"
"Using SPIN for automated debugging of infinite executions of Java programs","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894490580&doi=10.1016%2fj.jss.2013.10.056&partnerID=40&md5=9969ba80fa1ec011dafab24862cd38ae","This paper presents an approach for the automated debugging of reactive and concurrent Java programs, combining model checking and runtime monitoring. Runtime monitoring is used to transform the Java execution traces into the input for the model checker, the purpose of which is twofold. First, it checks these execution traces against properties written in linear temporal logic (LTL), which represent desirable or undesirable behaviors. Second, it produces several execution traces for a single Java program by generating test inputs and exploring different schedulings in multithreaded programs. As state explosion is the main drawback to model checking, we propose two abstraction approaches to reduce the memory requirements when storing Java states. We also present the formal framework to clarify which kinds of LTL safety and liveness formulas can be correctly analysed with each abstraction for both finite and infinite program executions. A major advantage of our approach comes from the model checker, which stores the trace of each failed execution, allowing the programmer to replay these executions to locate the bugs. Our current implementation, the tool TJT, uses Spin as the model checker and the Java Debug Interface (JDI) for runtime monitoring. TJT is presented as an Eclipse plug-in and it has been successfully applied to debug complex public Java programs. © 2013 The Authors.",""
"On the verification of UML/OCL class diagrams using constraint programming","2014","Journal of Systems and Software","10.1016/j.jss.2014.03.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900793681&doi=10.1016%2fj.jss.2014.03.023&partnerID=40&md5=033a54be4c4e09c3213d4f7ebd130396","Assessment of the correctness of software models is a key issue to ensure the quality of the final application. To this end, this paper presents an automatic method for the verification of UML class diagrams extended with OCL constraints. Our method checks compliance of the diagram with respect to several correctness properties including weak and strong satisfiability or absence of constraint redundancies among others. The method works by translating the UML/OCL model into a Constraint Satisfaction Problem (CSP) that is evaluated using state-of-the-art constraint solvers to determine the correctness of the initial model. Our approach is particularly relevant to current MDA and MDD methods where software models are the primary artifacts of the development process and the basis for the (semi-)automatic code-generation of the final application. © 2014 Elsevier Inc.","Constraint programming; Model verification; UML/OCL"
"Reversible watermarking method based on asymmetric-histogram shifting of prediction errors","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.086","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882654635&doi=10.1016%2fj.jss.2013.04.086&partnerID=40&md5=b321d5f8537937a0b41c6bb1ad57a48f","This paper tries to provide a new perspective for the research of reversible watermarking based on histogram shifting of prediction errors. Instead of obtaining one prediction error for the current pixel, we calculate multiple prediction errors by designing a multi-prediction scheme. An asymmetric error histogram is then constructed by selecting the suitable one from these errors. Compared with traditional symmetric histogram, the asymmetric error histogram reduces the amount of shifted pixels, thus improving the watermarked image quality. Moreover, a complementary embedding strategy is proposed by combining the maximum and minimum error histograms. As the two error histograms shift in the opposite directions during the embedding, some watermarked pixels will be restored to their original values, thus the image quality is further improved. Experimental findings also show that the proposed method re-creates watermarked images of higher quality that carry larger embedding capacity compared to conventional symmetric histogram methods, such as Tsai et al.'s and Luo et al.'s works. © 2013 Elsevier Inc.","Asymmetric selection function; Histogram shifting; Multi-prediction scheme"
"Effective scheduling strategies for boosting performance on rule-based spam filtering frameworks","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887023077&doi=10.1016%2fj.jss.2013.07.036&partnerID=40&md5=aad7ecf160e5a8be33f8b41441b3164f","Despite the enormous importance of e-mail to current worldwide communication, the increase of spam deliveries has had a significant adverse effect for all its users. In order to adequately fight spam, both the filtering industry and scientific community have developed and deployed the fastest and most accurate filtering techniques. However, the increasing volume of new incoming messages needing classification together with the lack of adequate support for anti-spam services on the cloud, make filtering efficiency an absolute necessity. In this context, and given the extensive utilization and increasing significance of rule-based filtering frameworks for the anti-spam domain, this work studies and analyses the importance of both existing and novel scheduling strategies to make the most of currently available anti-spam filtering techniques. Results obtained from the experiments demonstrated that some scheduling alternatives resulted in time savings of up to 26% for filtering messages, while maintaining the same classification accuracy. © 2013 Elsevier Inc. All rights reserved.","Performance boosting; Reduction of computational requirements; Rule scheduling; Rule-based anti-spam systems; Smart filter evaluation"
"SAAD, a content based Web Spam Analyzer and Detector","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884160149&doi=10.1016%2fj.jss.2013.07.007&partnerID=40&md5=cea70c8ff48752afeaff887052d0bbab","Web Spam is one of the main difficulties that crawlers have to overcome and therefore one of the main problems of the WWW. There are several studies about characterising and detecting Web Spam pages. However, none of them deals with all the possible kinds of Web Spam. This paper shows an analysis of different kinds of Web Spam pages and identifies new elements that characterise it, to define heuristics which are able to partially detect them. We also discuss and explain several heuristics from the point of view of their effectiveness and computational efficiency. Taking them into account, we study several sets of heuristics and demonstrate how they improve the current results. Finally, we propose a new Web Spam detection system called SAAD (Spam Analyzer And Detector), which is based on the set of proposed heuristics and their use in a C4.5 classifier improved by means of Bagging and Boosting techniques. We have also tested our system in some well known Web Spam datasets and we have found it to be very effective. © 2013 Elsevier Inc.","Data mining; Malware; Statistical properties of Web Spam; Web characterization; Web Spam"
"Modeling continuous integration practice differences in industry software development","2014","Journal of Systems and Software","10.1016/j.jss.2013.08.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888645752&doi=10.1016%2fj.jss.2013.08.032&partnerID=40&md5=3ac0b6d9129e547b271fececce281e99","Continuous integration is a software practice where developers integrate frequently, at least daily. While this is an ostensibly simple concept, it does leave ample room for interpretation: what is it the developers integrate with, what happens when they do, and what happens before they do? These are all open questions with regards to the details of how one implements the practice of continuous integration, and it is conceivable that not all such implementations in the industry are alike. In this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case. Based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences. The application of the model to an industry software development project is then described in an illustrative case study. © 2013 Elsevier Inc.","Agile software development; Continuous integration"
"A secure palm vein recognition system","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884143016&doi=10.1016%2fj.jss.2013.06.065&partnerID=40&md5=796d0eab2796243bdfda8c4d4844ad46","With the increasing needs in security systems, vein recognition is one of the important and reliable solutions of identity security for biometrics-based identification systems. The obvious and stable line-feature-based approach can be used to clearly describe a palm vein patterns for personal identification. In this paper, a directional filter bank involving different orientations is designed to extract the vein pattern and the minimum directional code (MDC) is employed to encode the line-based vein features in binary code. In addition, there are many non vein pixels in the vein image and those pixels are unmeaning for vein recognition. To improve the accuracy, the non-vein pixels are detected by evaluating the directional filtering magnitude (DFM) and considered the non-orientation code. A total of 5120 palm vein images from 256 persons are used to verify the validity of the proposed palm vein recognition approach. High accuracies (>99%) and low equal error rate (0.54%) obtained by the proposed method show that our proposed approach is feasible and effective for palm vein recognition. © 2013 Elsevier Inc.","Biometrics; Directional filter bank; Identity security; Minimum directional code; Palm vein recognition"
"Top-k query processing for replicated data in mobile peer to peer networks","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898794339&doi=10.1016%2fj.jss.2013.10.043&partnerID=40&md5=7976af7d6e4b89e0d9ba07479d83bb16","In mobile ad hoc peer to peer (M-P2P) networks, since nodes are highly resource constrained, it is effective to retrieve data items using a top-k query, in which data items are ordered by the score of a particular attribute and the query-issuing node acquires data items with the k highest scores. However, when network partitioning occurs, the query-issuing node cannot connect to some nodes having data items included in the top-k query result, and thus, the accuracy of the query result decreases. To solve this problem, data replication is a promising approach. However, if each node sends back its own data items (replicas) responding to a query without considering replicas held by others, same data items are sent back to the query-issuing node more than once through long paths, which results in increase of traffic. In this paper, we propose a top-k query processing method considering data replication in M-P2P networks. This method suppresses duplicate transmissions of same data items through long paths. Moreover, an intermediate node stops transmitting a query message on-demand. © 2013 Elsevier Inc.","Data replication; Mobile peer to peer network; Top-k query"
"Domain-Specific Modeling Languages to improve framework instantiation","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887017945&doi=10.1016%2fj.jss.2013.07.030&partnerID=40&md5=2d43ed1c9f426296ac63fc0fd22ac8a4","Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach. © 2013 Elsevier Inc. All rights reserved.","Domain-Specific Modeling Language; Framework; Reuse"
"A decision support framework for metrics selection in goal-based measurement programs: GQM-DSFMS","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887020408&doi=10.1016%2fj.jss.2013.07.022&partnerID=40&md5=ed71bb7d92fbe408eb37d448b7be1e07","Software organizations face challenges in managing and sustaining their measurement programs over time. The complexity of measurement programs increase with exploding number of goals and metrics to collect. At the same time, organizations usually have limited budget and resources for metrics collection. It has been recognized for quite a while that there is the need for prioritizing goals, which then ought to drive the selection of metrics. On the other hand, the dynamic nature of the organizations requires measurement programs to adapt to the changes in the stakeholders, their goals, information needs and priorities. Therefore, it is crucial for organizations to use structured approaches that provide transparency, traceability and guidance in choosing an optimum set of metrics that would address the highest priority information needs considering limited resources. This paper proposes a decision support framework for metrics selection (DSFMS) which is built upon the widely used Goal Question Metric (GQM) approach. The core of the framework includes an iterative goal-based metrics selection process incorporating decision making mechanisms in metrics selection, a pre-defined Attributes/Metrics Repository, and a Traceability Model among GQM elements. We also discuss alternative prioritization and optimization techniques for organizations to tailor the framework according to their needs. The evaluation of the GQM-DSFMS framework was done through a case study in a CMMI Level 3 software company. © 2013 Elsevier Inc.","Decision support; Goal based measurement; Goal Question Metric; GQM; Optimization; Prioritization; Software measurement program"
"A high capacity data hiding scheme for binary images based on block patterns","2014","Journal of Systems and Software","10.1016/j.jss.2014.02.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900826122&doi=10.1016%2fj.jss.2014.02.023&partnerID=40&md5=ae7669fccc9dcdd5f5d0b99bb2ef1035","This paper proposes a high capacity data hiding scheme for binary images based on block patterns, which can facilitate the authentication and annotation of scanned images. The scheme proposes block patterns for a 2 × 2 block to enforce specific block-based relationship in order to embed a significant amount of data without causing noticeable artifacts. In addition, two kinds of matching pair (MP) methods, internal adjustment MP and external adjustment MP, are designed to decrease the embedding changes. Shuffling is applied before embedding to reduce the distortion and improve the security. Experimental results show that the proposed scheme gives a significantly improved embedding capacity than previous approaches in the same level of embedding distortion. We also analyze the perceptual impact and discuss the robustness and security issues. © 2014 Elsevier Inc.","Authentication; Binary image; Data hiding; Watermarking"
"Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891625335&doi=10.1016%2fj.jss.2013.10.040&partnerID=40&md5=dd69d4becd8b94d2bc063e89827148e2","Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods. © 2013 Elsevier Inc. All rights reserved.","Agile methods; Requirements engineering; Software product line scoping"
"Information centric services in Smart Cities","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891623671&doi=10.1016%2fj.jss.2013.10.029&partnerID=40&md5=996556e5bce23fed11b174c7043b9a1c","A ""Smart City"" is intended as an urban environment which, supported by pervasive ICT systems, is able to offer advanced and innovative services to citizens in order to improve the overall quality of their life. In this context, the present contribution formulates a pioneering proposal, by drawing an advanced information centric platform for supporting the typical ICT services of a Smart City. It can easily embrace all available and upcoming wireless technologies, while enforcing, at the same time, ubiquitous and secure applications in many domains, such as, e-government and public administration, intelligent transportation systems, public safety, social, health-care, educational, building and urban planning, environmental, and energy and water management applications. All the details of the proposed approach have been carefully described by means of pragmatical use-cases, such as the management of administrative procedures, the starting of a new business in a given country, the navigation assistance, the signaling of an urban accident aimed at improving the public safety, the reservation of a medical examination, the remote assistance of patients, and the management of waste in a city. This description makes evident the real effectiveness of the present proposal in future urban environments. © 2013 Elsevier Inc. All rights reserved.","Information Centric Networking; NDN; Smart Cities"
"Cell-related location area planning for 4G PCS networks with variable-order Markov model","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882591928&doi=10.1016%2fj.jss.2013.05.031&partnerID=40&md5=c6d87a1ba4d24dbf558565cf5ae7cc9f","Location management is a critical issue in personal communication service (PCS) networks, tracking the location of user equipment (UE) with the goal of minimizing total signaling cost. Previous work can be classified into two categories: static and dynamic. Static schemes partition networks into fixed size LAs. However, these schemes are inefficient because they do not take UEs' mobility and the call arrival rate into account. On the other hand, focusing on individual UEs, dynamic schemes have minimized the location management cost. However, they are difficult to implement because recording the individual information of numerous UEs and planning each of their LAs consume uncontrollable cost. Because of these reasons, we propose a cell-based scheme between static and dynamic schemes. Considering people usually stay in specific zones for long periods and the movement of UEs usually presents a strong moving direction in areas, this study presents a distributed algorithm by employing variable-order Markov models to find the mobility characteristic shared by UEs to plan better LAs with lower location management cost. When the order of Markov model is set to 1, our method is equal to a pure cell-centric LAP scheme; while the order of Markov model is high, it is more like a profile-based dynamic scheme. So, the setting of the order actually is a trade-off problem between the overall location management cost and the computing complexity. We present to retrieve a balance by using the expected location management cost and the number of total states of Markov models. In simulations, the origin-destination matrix (O-D matrix) from the Taipei Rapid Transit Corporation is used for representing the association between two cells. Simulation results demonstrate that the proposed scheme achieves good performance. © 2013 Elsevier Inc.","Location area planning; Location management; Mobile computing; Variable-order Markov model"
"HSFal: Effective fault localization using hybrid spectrum of full slices and execution slices","2014","Journal of Systems and Software","10.1016/j.jss.2013.11.1109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894438130&doi=10.1016%2fj.jss.2013.11.1109&partnerID=40&md5=3d55c69cee3656f59189f06388816635","Most of the existing fault localization approaches use execution coverage of test cases to isolate the suspicious codes that likely contain faults. Program slicing can extract the dependencies of program entities with respect to a specific criterion. Therefore this technique is expected to have a beneficial effect on fault localization. In this paper, we propose a novel approach using a hybrid spectrum of full slices and execution slices to improve the effectiveness of fault localization. In particular, our approach firstly computes full slices of failed test cases and execution slices of passed test cases respectively. Secondly it constructs the hybrid spectrum by intersecting full slices and execution slices. Finally it computes the suspiciousness of each statement in the hybrid slice spectrum and generates a fault location report with descending suspiciousness of each statement. We also implement our proposed approach in our prototype tool HSFal by Java programming language. To verify the effectiveness of our approach, we performed an empirical study by the prototype on several widely used open source programs. Our approach is compared with eight representative coverage-based and slice-based fault localization approaches. Final experimental results show that our proposed approach is more effective in fault localization than other compared approaches, and can reduce almost 2.98-31.79% of the average cost of examined code significantly. © 2013 Elsevier Inc.","Dynamic slicing; Execute slicing; Fault localization"
"Using SMCD to reduce inconsistencies in misuse case models: A subject-based empirical evaluation","2014","Journal of Systems and Software","10.1016/j.jss.2013.10.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888637639&doi=10.1016%2fj.jss.2013.10.017&partnerID=40&md5=407efa63f7a42d3b14e91f6e1808172c","Security is a crucial requirement in software systems which need to be addressed as early as the requirements phase. The technique of misuse case modeling has been introduced slightly over a decade ago to elicit and specify functional security requirements. Development efforts downstream will be driven by the functional security requirements specified in misuse case models. Consequently, the quality of a misuse case model influences the effectiveness of downstream development efforts. Inconsistencies are an undesired attribute that can severely reduce the quality of misuse case models. In this paper, a controlled experiment involving students is presented which evaluates the reduction of inconsistencies in misuse case models resulting from utilizing a structure called SMCD (Structured Misuse Case Descriptions). The experiment also examines the impact of using SMCD upon other quality attributes of misuse case models. The results of the experiment indicate that using SMCD improves the consistency levels of the developed misuse case models. © 2013 Elsevier Inc. All rights reserved.","Misuse cases; SMCD; Subject-based experiment"
"A model-driven approach to develop high performance web applications","2013","Journal of Systems and Software","10.1016/j.jss.2013.07.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887023249&doi=10.1016%2fj.jss.2013.07.028&partnerID=40&md5=4c0d42457d496f32daef68a1e259d164","The evolution of web technologies in the last few years has contributed to the improvement of web applications, and with the appearance of AJAX and Web 2.0 technology, a new breed of applications for the Internet has emerged: widgets, gadgets and mashups are some examples of this trend. However, as web applications become more and more complex, development costs are increasing in an exponential rate, and we believe that considering a software engineering methodology in the development process of this type of applications, contributes to the solution of this problem. In order to solve this question, this paper proposes a model-driven architecture to support web application development from the design to the implementation model. With this aim, the following tasks have been performed: first a new profile extends UML with new concepts extracted from the web domain, then a new framework supports web application development by composing heterogeneous web elements, and finally, a transformation model generates web applications from the UML extension proposed. The main contribution of this work is a cost and complexity reduction due to the incorporation of a model-driven architecture into the web application development process, but other advantages that can be mentioned are a high performance degree achieved by a prefetching cache mechanism, and a high reusability, since web elements can be reused in different web applications. © 2013 Elsevier Inc. All rights reserved.","Model-driven architecture; Rich internet applications; Web applications"
"Coherent clusters in source code","2014","Journal of Systems and Software","10.1016/j.jss.2013.07.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891624119&doi=10.1016%2fj.jss.2013.07.040&partnerID=40&md5=8056208dda6b06419c9ebd4a29823401","This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have 'coherent' shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure. For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality. Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates. A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering. Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution. © 2013 The Authors. All rights reserved.","Dependence analysis; Program comprehension; Software clustering"
"Performance optimization of deployed software-as-a-service applications","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888637751&doi=10.1016%2fj.jss.2013.09.013&partnerID=40&md5=8c70ef216a8a4661541043b1a58b5bb5","The goal of performance maintenance is to improve the performance of a software system after delivery. As the performance of a system is often characterized by unexpected combinations of metric values, manual analysis of performance is hard in complex systems. In this paper, we propose an approach that helps performance experts locate and analyze spots-so called performance improvement opportunities (PIOs)-for possible performance improvements. PIOs give performance experts a starting point for performance improvements, e.g., by pinpointing the bottleneck component. The technique uses a combination of association rules and performance counters to generate the rule coverage matrix, a matrix which assists with the bottleneck detection. In this paper, we evaluate our technique in two case studies. In the first one, we show that our technique is accurate in detecting the time frame during which a PIO occurs. In the second one, we show that the starting point given by our approach is indeed useful and assists a performance expert in diagnosing the bottleneck component in a system with high precision. © 2013 Elsevier Inc.","Performance analysis; Performance maintenance"
"Supporting end-to-end quality of service properties in OMG data distribution service publish/subscribe middleware over wide area networks","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882644950&doi=10.1016%2fj.jss.2013.04.074&partnerID=40&md5=0bdebd91489e52147877447153b74f50","Assuring end-to-end quality-of-service (QoS) in distributed real-time and embedded (DRE) systems is hard due to the heterogeneity and scale of communication networks, transient behavior, and the lack of mechanisms that holistically schedule different resources end-to-end. This paper makes two contributions to research focusing on overcoming these problems in the context of wide area network (WAN)-based DRE applications that use the OMG Data Distribution Service (DDS) QoS-enabled publish/subscribe middleware. First, it provides an analytical approach to bound the delays incurred along the critical path in a typical DDS-based publish/subscribe stream, which helps ensure predictable end-to-end delays. Second, it presents the design and evaluation of a policy-driven framework called Velox. Velox combines multi-layer, standards-based technologies - including the OMG DDS and IP DiffServ - to support end-to-end QoS in heterogeneous networks and shield applications from the details of network QoS mechanisms by specifying per-flow QoS requirements. The results of empirical tests conducted using Velox show how combining DDS with DiffServ enhances the schedulability and predictability of DRE applications, improves data delivery over heterogeneous IP networks, and provides network-level differentiated performance. © 2013 Elsevier Inc.","DDS services; DiffServ; QoS Framework; Schedulability"
"Software architecture review by association","2014","Journal of Systems and Software","10.1016/j.jss.2013.09.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891628677&doi=10.1016%2fj.jss.2013.09.044&partnerID=40&md5=510aa8201a0a6362b4afeb4fdd6d299d","During the process of software design, software architects have their reasons to choose certain software components to address particular software requirements and constraints. However, existing software architecture review techniques often rely on the design reviewers' knowledge and experience, and perhaps using some checklists, to identify design gaps and issues, without questioning the reasoning behind the decisions made by the architects. In this paper, we approach design reviews from a design reasoning perspective. We propose to use an association-based review procedure to identify design issues by first associating all the relevant design concerns, problems and solutions systematically; and then verifying if the causal relationships between these design elements are valid. Using this procedure, we discovered new design issues in all three industrial cases, despite their internal architecture reviews and one of the three systems being operational. With the newly found design issues, we derive eight general design reasoning failure scenarios. © 2013 Elsevier Inc. All rights reserved.","Design reasoning; Software architecture review; Verification of software architecture"
"Adaptive reversible data hiding based on block median preservation and modification of prediction errors","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880167513&doi=10.1016%2fj.jss.2013.04.020&partnerID=40&md5=b038400bfeb52bdda69a59e400d60132","In this paper, two enhanced reversible data hiding methods are proposed; both of them are based on two novel reversible data hiding techniques. A latest predictor is adopted to achieve better data hiding capability for the first predicative reversible data hiding scheme, whereas another scheme utilizes a new approach by considering the nature of different images to classify the smoothness for each piece of image blocking regions such that more secret data can be hidden into the smooth regions rather than the non-smooth ones resulting in a better embedding capability. The experiments verify that these schemes outperform the original reversible data hiding algorithms and some state-of-the-art reversible data hiding schemes. © 2013 Elsevier Inc. All rights reserved.","Histogram shifting; Image subsampling; Reversible data hiding"
"FORTUNA - A framework for the design and development of hardware-based secure systems","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880144996&doi=10.1016%2fj.jss.2013.03.059&partnerID=40&md5=ca5b2e92aae60f68728de395fba02eaa","Security requires a holistic view. In this work we contribute to this goal by taking a new viewpoint, with the proposal of the logic-probabilistic framework FORTUNA to support the design and development of hardware-based- security systems (HwBSS). It extends and further substantiates our ideas presented in a previous conference paper (Gallo et al.; 2011). Our contributions in this article are: (a) to extend and validate FORTUNA, and (b) to illustrate its effectiveness uncovering an unreported SPARC V8 architectural security flaw. © 2013 Elsevier Inc. All rights reserved.","Formal methods for security; Hardware-based security; Information security; Security evaluation tool; Security metrics"
"Performing and analyzing non-formal inspections of entity relationship diagram (ERD)","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880146772&doi=10.1016%2fj.jss.2013.03.106&partnerID=40&md5=966560914e6c3b7e667db4cb472e8f5f","Designing and understanding of diagrammatic representations is a critical issue for the success of software projects because diagrams in this field provide a collection of related information with various perceptual signs and they help software engineers to understand operational systems at different levels of information system development process. Entity relationship diagram (ERD) is one of the main diagrammatic representations of a conceptual data model that reflects users' data requirements in a database system. In today's business environment, the business model is in a constant change which creates highly dynamic data requirements which also requires additional processes like modifications of ERD. However, in the literature there are not many measures to better understand the behaviors of software engineers during designing and understanding these representations. Hence, the main motivation of this study is to develop measures to better understand performance of software engineers during their understanding process of ERD. Accordingly, this study proposes two measures for ERD defect detection process. The defect detection difficulty level (DF) measures how difficult a defect to be detected according to the other defects for a group of software engineers. Defect detection performance (PP) measure is also proposed to understand the performance of a software engineer during the defect detection process. The results of this study are validated through the eye tracker data collected during the defect detection process of participants. Additionally, a relationship between the defect detection performance (PP) of a software engineer and his/her search patterns within an ERD is analyzed. Second experiment with five participants is also conducted to show the correlation between the proposed metric results and eye tracker data. The results of experiment-2 also found to be similar for DF and PP values. The results of this study are expected to provide insights to the researchers, software companies, and to the educators to improve ERD reasoning process. Through these measures several design guidelines can be developed for better graphical representations and modeling of the information which would improve quality of these diagrams. Moreover, some reviewing instructions can be developed for the software engineers to improve their reviewing process in ERD. These guidelines in turn will provide some tools for the educators to improve design and review skills of future software engineers. © 2013 Elsevier Inc. All rights reserved.","Defect detection; ERD; Eye tracking"
"Applying hybrid learning approach to RoboCup's strategy","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879940353&doi=10.1016%2fj.jss.2013.03.031&partnerID=40&md5=d29262f955ebe13260e1e40f329d6280","RoboCup (Robot world cup tournament) soccer game is a competitive game that has become a popular research domain in recent years since it involves a complex system for the behavior of multiple agents. In this paper, a hybrid approach, case-based reasoning genetic algorithm (CBR-GA) is applied to the soccer game for providing better strategies. By using CBR-GA, the soccer robots can obtain the suitable strategies for different conditions and store the related experiences, which may be reused in the future. Rule-based reasoning (RBR) will be employed to create a new strategy for the soccer robots when CBR-GA cannot provide a suitable one. A multi-agent learning system, constructed by combining case-based reasoning genetic algorithm with RBR strategy (CGRS), is implemented on the latest WrightEagle simulation platform that is released in 2011. In the CGRS system, two kinds of agent, namely ""coach agent"" and ""movement agent"", are designed for the soccer game. The coach agent is responsible for deciding on the strategy goal and assigning tasks to the movement agents. Every movement agent will then execute its respective task for achieving the strategy goal. Better basic skills will facilitate the movement agents to execute more effectively the assigned tasks or plans; hence, many basic skills are designed for training the movement agents. To increase learning efficiency, the strategy cycle time is reduced with a suitable case base. To validate the effectiveness of the proposed approach, our soccer team played with the WrightEagle soccer team which has remained in the top two positions in simulation 2d in recent years. Our team gradually gets higher winning frequency in 50 rounds. Furthermore, a comparison experiment shows that the proposed approach has higher winning frequency than other methods including CBR-GA, CBR-RBR and RBR. Finally, the proposed approach is also found to have better learning mechanisms than other learning approaches in soccer game. © 2013 Elsevier Inc. All rights reserved.","Case-based reasoning; Genetic algorithm; RoboCup; Strategy"
"MostoDE: A tool to exchange data amongst semantic-web ontologies","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876407337&doi=10.1016%2fj.jss.2013.01.037&partnerID=40&md5=d1f6ae6bc7bfcd2a1590715775a50cda","A semantic-web ontology, simply known as ontology, comprises a data model and data that should comply with it. Due to their distributed nature, there exist a large amount of heterogeneous ontologies, and a strong need for exchanging data amongst them, i.e.; populating a target ontology using data that come from one or more source ontologies. Data exchange may be implemented using correspondences that are later transformed into executable mappings; however, exchanging data amongst ontologies is not a trivial task, so tools that help software engineers to exchange data amongst ontologies are a must. In the literature, there are a number of tools to automatically generate executable mappings; unfortunately, they have some drawbacks, namely: (1) they were designed to work with nested-relational data models, which prevents them to be applied to ontologies; (2) they require their users to handcraft and maintain their executable mappings, which is not appealing; or (3) they do not attempt to identify groups of correspondences, which may easily lead to incoherent target data. In this article, we present MostoDE, a tool that assists software engineers in generating SPARQL executable mappings and exchanging data amongst ontologies. The salient features of our tool are as follows: it allows to automate the generation of executable mappings using correspondences and constraints; it integrates several systems that implement semantic-web technologies to exchange data; and it provides visual aids for helping software engineers to exchange data amongst ontologies. © 2013 Elsevier Inc. All rights reserved.","Data exchange; Executable mappings; Semantic web; SPARQL"
"An enhanced variable-length arithmetic coding and encryption scheme using chaotic maps","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875238131&doi=10.1016%2fj.jss.2013.01.012&partnerID=40&md5=9ccbfb95befcae8239e84a5647d28aee","We enhance the simultaneous arithmetic coding and encryption scheme previously proposed by us. By encoding a block of variable number of symbols to a codeword within the length of the computation register, the operating efficiency has been substantially improved. Moreover, the compressed sequence is processed by an additional diffusion operation which strengthens the security of the original scheme by having higher key and plaintext sensitivities. Simulation results show that the enhanced scheme runs faster than the original scheme and the traditional compress-then-encrypt approach at a comparable compression performance. © 2013 Elsevier Inc.","Chaotic map; Simultaneous compression and encryption; Variable-length arithmetic coding"
"A robust data hiding algorithm for H.264/AVC video streams","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880180613&doi=10.1016%2fj.jss.2013.03.101&partnerID=40&md5=9e622375a81eac2be2da3fbf65413891","This paper presents a robust readable data hiding algorithm for H.264/AVC video streams without intra-frame distortion drift. We first encode the embedded data using BCH (n, k, t) syndrome code before data hiding to improve robustness, then we embed the encoded data into coefficients of the 4 × 4 luminance discrete cosine transform (DCT) blocks in I frames which meet our conditions to avert the distortion drift, and finally we recover the original video as much as possible when the hidden data is extracted out. The experimental results show that our scheme can get more robustness, effectively avert intra-frame distortion drift and get high visual quality. © 2013 Elsevier Inc. All rights reserved.","BCH syndrome code; Data hiding; H.264/AVC"
"Securing web-clients with instrumented code and dynamic runtime monitoring","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876390130&doi=10.1016%2fj.jss.2013.02.047&partnerID=40&md5=5f935b4fa38681eb2a7a63e26ebaa720","Security and privacy concerns remain a major factor that hinders the whole scale adoption of web-based technology in sensitive situations, such as financial transactions (Gao and Owolabi, 2008; Lichtenstein and Williamson, 2006). These concerns impact both end users and content generators. To tackle this problem requires a complimentary technology to the already developed and deployed infrastructure for web security. Hence, we have developed a multi-layer framework for web client security based on mobile code instrumentation. This architecture seeks to isolate exploitable security vulnerabilities and enforce runtime policies against malicious code constructs. Our instrumentation process uniquely integrates both static and dynamic engines and is driven by flexible (XML based) rewrite rules for a scalable operation and transparent deployment. Based on secure equivalents for vulnerable JavaScript objects and methods, our mechanism offers superior runtime performance compared to other approaches. Extensive investigation using four case studies shows that the instrumentation technique provides a potential solution to curb the rising number of security exploits that exist on the web today. In addition, performance data gathered from evaluations on active websites demonstrate that the mechanism has very little impact in terms of user experience; thus making it plausible for adoption by end-users. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","CSRF; CSS; Secure browsing; Security system; Web browsing security"
"An orchestrated survey of methodologies for automated software test case generation","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880168690&doi=10.1016%2fj.jss.2013.02.061&partnerID=40&md5=0a1140a445639ad71cc86ce2c56de25a","Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment. © 2013 Elsevier Inc. All rights reserved.","Adaptive random testing; Combinatorial testing; Model-based testing; Orchestrated survey; Search-based software testing; Software testing; Symbolic execution; Test automation; Test case generation"
"Robust and secure watermarking scheme for breath sound","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876407059&doi=10.1016%2fj.jss.2013.02.022&partnerID=40&md5=cd76296bf73b4c5ea87f8c91cb1992ac","Due to the development of the Internet, security and intellectual property protection have attracted significant interest in the copyright protection field recently. A novel watermarking scheme for breath sounds, combining lifting wavelet transform (LWT), discrete cosine transform (DCT), singular value decomposition (SVD) and dither modulation (DM) quantization is proposed in this paper as a way to insert encrypted source and identity information in breath sounds while maintaining significant biological signals. In the proposed scheme, LWT is first performed to decompose the signal, and then DCT is applied on the approximate coefficients. SVD is carried out on the LWT-DCT coefficients to derive singular values. DM is adopted to quantize the singular values of each of the LWT-DCT blocks; thus, the watermark extraction is blind by using the DM algorithm. The novelty of our proposed method also includes the introduction of the particle swarm optimization (PSO) technique to optimize the quantization steps for the DM approach. The experimental results demonstrate that the proposed watermarking scheme obtains good robustness against common manipulation attacks and preserves imperceptivity. The performance comparison results verify that our scheme outperforms existing approaches in terms of robustness and imperceptibility. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Breath sound; Digital watermarking; LWT-DCT; PSO; Robust; SVD"
"An object-oriented approach to language compositions for software language engineering","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.087","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881477025&doi=10.1016%2fj.jss.2013.04.087&partnerID=40&md5=443550c7090eac0354245ccc6f4f85d6","In this paper, it is shown that inheritance, a core concept from object-oriented programming, is a possible solution for realizing composition of computer languages. Language composability is a property of language descriptions, which can be further classified into informal (language syntax and semantics are hard-coded in compiler/interpreter) and formal language descriptions (syntax and semantics are formally specified with one of several formal methods for language definition). However, language composition is much easier to achieve with declarative formal language descriptions into which the notion of inheritance is introduced. Multiple attribute grammar inheritance, as implemented in the language implementation system LISA, can assist in realizing all of the different types of language compositions identified in Erdweg et al. (2012). Different examples are given throughout the paper using an easy to understand domain-specific language that describes simple robot movement. © 2013 Elsevier Inc.","Domain-specific languages; Language composition; Software language engineering"
"Clustering navigation sequences to create contexts for guiding code navigation","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880157146&doi=10.1016%2fj.jss.2013.03.103&partnerID=40&md5=8ffeacb4ddc0b38db6ea39b2401ccbc4","To guide programmer code navigation, previous approaches such as TeamTracks recommend pieces of code to visit by mining the associations between pieces of code in programmer interaction histories. However, these result in low recommendation accuracy. To create more accurate recommendations, we propose NavClus an approach that clusters navigation sequences from programmer interaction histories. NavClus automatically forms collections of code that are relevant to the tasks performed by programmers, and then retrieves the collections best matched to a programmer's current navigation path. This makes it possible to recommend the collections of code that are relevant to the programmer's given task. We compare NavClus' recommendation accuracy with TeamTracks' by simulating recommendations using 4397 interaction histories. The comparative experiment shows that the recommendation accuracy of NavClus is twice as high as that of TeamTracks. © 2013 Elsevier Inc. All rights reserved.","Code navigation; Context aware code recommender; Data clustering techniques; Data stream mining; Programmer interaction histories"
"Group and link analysis of multi-relational scientific social networks","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879977636&doi=10.1016%2fj.jss.2013.02.024&partnerID=40&md5=caed45bda40b0f7adcef28aa604e767c","Analyzing social networks enables us to detect several inter and intra connections between people in and outside their organizations. We model a multi-relational scientific social network where researchers may have four different types of relationships with each other. We adopt some criteria to enable the modeling of a scientific social network as close as possible to reality. Using clustering techniques with maximum flow measure, we identify the social structure and research communities in a way that allows us to evaluate the knowledge flow in the Brazilian scientific community. Finally, we evaluate the temporal evolution of scientific social networks to suggest/predict new relationships. © 2013 Elsevier Inc. All rights reserved.","Link prediction/suggestion; Max-flow grouping algorithm; Multi-relational scientific social network analysis"
"Optimization of adaptation plans for a service-oriented architecture with cost, reliability, availability and performance tradeoff","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.929","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872681755&doi=10.1016%2fj.jss.2012.10.929&partnerID=40&md5=1542c6b3d7b867f963f6ac2d58d01087","A service-based system may require adaptation for several reasons, such as service evolution (e.g.; a new version may be available), hardware volatility (e.g.; network quality changes), and varying user demands and new requirements (e.g.; a new functionality or a different level of quality of service). Therefore, it is suitable to dynamically adapt a service-based system in an automated manner. However, service adaptations often do not consider software quality attributes and, if they do, they relay on a single attribute in isolation. In this paper, we present an optimization model, which aims to minimize the adaptation costs of a Service-Oriented Architecture (SOA), in correspondence with a certain change scenario (i.e.; a set of new requirements) under reliability, availability and performance tradeoff. The model predicts the quality of the new SOA obtained by changing both its structure and behavior. Specifically, it suggests how to replace existing services with available instances and/or adding new services, and how to remove or introduce interaction(s) between existing services and/or new services. We show how our model works on a smartphone mobile application example, and through the sensitivity analysis we highlight its potential to drive architectural decisions. © 2012 Elsevier Inc. All rights reserved.","Optimization model; Selecting adaptation plans; Software cost; Software performance; Software reliability"
"A framework to support selection of cloud providers based on security and privacy requirements","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881474672&doi=10.1016%2fj.jss.2013.03.011&partnerID=40&md5=1a88f8a99e2f0fd24c20271c73a825e9","Cloud computing is an evolving paradigm that is radically changing the way humans store, share and access their digital files. Despite the many benefits, such as the introduction of a rapid elastic resource pool, and on-demand service, the paradigm also creates challenges for both users and providers. In particular, there are issues related to security and privacy, such as unauthorised access, loss of privacy, data replication and regulatory violation that require adequate attention. Nevertheless, and despite the recent research interest in developing software engineering techniques to support systems based on the cloud, the literature fails to provide a systematic and structured approach that enables software engineers to identify security and privacy requirements and select a suitable cloud service provider based on such requirements. This paper presents a novel framework that fills this gap. Our framework incorporates a modelling language and it provides a structured process that supports elicitation of security and privacy requirements and the selection of a cloud provider based on the satisfiability of the service provider to the relevant security and privacy requirements. To illustrate our work, we present results from a real case study. © 2013 Elsevier Inc. All rights reserved.","Cloud computing; Privacy; Secure; software engineering"
"An improved DCT-based perturbation scheme for high capacity data hiding in H.264/AVC intra frames","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.922","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872685039&doi=10.1016%2fj.jss.2012.10.922&partnerID=40&md5=82ca21fe7944b3f271e42a035e1c8674","Recently, Ma et al. proposed an efficient error propagation-free discrete cosine transform-based (DCT-based) data hiding algorithm that embeds data in H.264/AVC intra frames. In their algorithm, only 46% of the 4 × 4 luma blocks can be used to embed hidden bits. In this paper, we propose an improved error propagation-free DCT-based perturbation scheme that fully exploits the remaining 54% of luma blocks and thereby doubles the data hiding capacity of Ma et al.'s algorithm. Further, in order to preserve the visual quality and increase the embedding capacity of the embedded video sequences, a new set of sifted 4 × 4 luma blocks is considered in the proposed DCT-based perturbation scheme. The results of experiments on twenty-six test video sequences confirm the embedding capacity superiority of the proposed improved algorithm while keeping the similar human visual effect in terms of SSIM (structural similarity) index. © 2012 Elsevier Inc. All rights reserved.","Data hiding; DCT; Embedding capacity; Error propagation-free; H.264/AVC; Human visual effect; Intra prediction"
"Countermeasure graphs for software security risk assessment: An action research","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881475641&doi=10.1016%2fj.jss.2013.04.023&partnerID=40&md5=b2999cba988075a666d7201764e5bba3","Software security risk analysis is an important part of improving software quality. In previous research we proposed countermeasure graphs (CGs), an approach to conduct risk analysis, combining the ideas of different risk analysis approaches. The approach was designed for reuse and easy evolvability to support agile software development. CGs have not been evaluated in industry practice in agile software development. In this research we evaluate the ability of CGs to support practitioners in identifying the most critical threats and countermeasures. The research method used is participatory action research where CGs were evaluated in a series of risk analyses on four different telecom products. With Peltier (used prior to the use of CGs at the company) the practitioners identified attacks with low to medium risk level. CGs allowed practitioners to identify more serious risks (in the first iteration 1 serious threat, 5 high risk threats, and 11 medium threats). The need for tool support was identified very early, tool support allowed the practitioners to play through scenarios of which countermeasures to implement, and supported reuse. The results indicate that CGs support practitioners in identifying high risk security threats, work well in an agile software development context, and are cost-effective. © 2013 Elsevier Inc.","Countermeasure graphs; Risk analysis; Software security"
"Closed inter-sequence pattern mining","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876420447&doi=10.1016%2fj.jss.2013.02.010&partnerID=40&md5=e3e63894facd7be2afe5691bcfaf0588","Inter-sequence pattern mining can find associations across several sequences in a sequence database, which can discover both a sequential pattern within a transaction and sequential patterns across several different transactions. However, inter-sequence pattern mining algorithms usually generate a large number of recurrent frequent patterns. We have observed mining closed inter-sequence patterns instead of frequent ones can lead to a more compact yet complete result set. Therefore, in this paper, we propose a model of closed inter-sequence pattern mining and an efficient algorithm called CISP-Miner for mining such patterns, which enumerates closed inter-sequence patterns recursively along a search tree in a depth-first search manner. In addition, several effective pruning strategies and closure checking schemes are designed to reduce the search space and thus accelerate the algorithm. Our experiment results demonstrate that the proposed CISP-Miner algorithm is very efficient and outperforms a compared EISP-Miner algorithm in most cases. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Closed patterns; Data mining; Inter-sequence pattern"
"Measuring the impact of changes to the complexity and coupling properties of automotive software systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875256634&doi=10.1016%2fj.jss.2012.12.021&partnerID=40&md5=a7041ad6e6a83982cc38b96306203bf3","Background: In the past few decades exponential increase in the amount of software used in cars has been recorded together with enhanced requirements for functional safety of their embedded software. As the evolution of software systems in cars often entails changes to software architecture, it is important to be able to monitor their impact. Method: We conducted a case study on a distributed software system in cars at Volvo Car Corporation with the goal to develop, apply and evaluate measures of complexity and coupling which could support software architects in monitoring changes. Results: The results showed that two metrics-structural complexity and coupling measures-can guide architectural work and turn attention of architects to most complex subsystems. The results were confirmed by monitoring a complete electrical system of a vehicle under two releases. Conclusion: By applying the metrics after each significant change in the architecture, it is possible to verify that certain quality attributes have not deteriorated and to identify new testing areas. Using these metrics increases the product quality with respect to stability, reliability, and maintainability and also has potential to reduce long-term software development/maintenance costs. © 2012 Elsevier Inc.","Architectural change; Automotive software system; Complexity; Coupling; Maintainability; Quality metric"
"On the integration of model-driven design and dynamic assertion-based verification for embedded software","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880143434&doi=10.1016%2fj.jss.2012.08.061&partnerID=40&md5=6b1feaa843941f7b67f605bac522403b","Model-driven design (MDD) aims at elevating design to a higher level of abstraction than that provided by third-generation programming languages. Concurrently, assertion-based verification (ABV) relies on the definition of temporal assertions to enhance functional verification targeting the correctness of the design execution with respect to the expected behavior. Both MDD and ABV have affirmed as effective methodologies for design and verification of HW components of embedded systems. Nonetheless, MDD and ABV individually suffer some limitations that prevent their integration in the embedded-software (ESW) design and verification flow. In particular, MDD requires the integration of an effective methodology for monitoring specification conformance, and dynamic ABV relies on simulation assumptions, satisfied in the HW domain, but which cannot be straightforward guaranteed during the execution of ESW. In this work, we present a suitable combination of MDD and dynamic ABV as an effective solution for ESW design and verification. A suite composed of two off-the-shelf tools has been developed for supporting this integrated approach. The MDD tool, i.e.; radCASE, is a rapid-application-development environment for ESW that provides the user with a comprehensive approach to cover the complete modeling and synthesis process of ESW. The dynamic ABV environment, i.e.; radCHECK, integrates computer-aided and template-based assertion definition, automatic checker generation, and effective stimuli generation, making dynamic ABV really practical to check the correctness of the radCASE outcome. © 2013 Elsevier Inc. All rights reserved.","Dynamic assertion-based verification; Embedded software; Model-driven design"
"Testing real-time embedded systems using timed automata based approaches","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875258655&doi=10.1016%2fj.jss.2012.12.030&partnerID=40&md5=8188125227e94276f9f3fd47429dc71f","Real-Time Embedded Systems (RTESs) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. We previously introduced the 'priority-based' approach which tests the logical and timing behaviour of an RTES modelled formally as UPPAAL automata. The 'priority-based' approach was based on producing sets of timed test traces by achieving clock region coverage. In this paper, we empirically validate the 'priority-based' approach with comparison to well-known timed testing approaches based on a Timed Automata (TA) formalism using a complete test bed based on an industrial-strength case study (production cell). The validation assessment is based on both fault coverage and structural coverage by a minimal number of generated test traces; the former is achieved using the Mutation Analysis Technique (MAT) by introducing a set of timed and functional mutation operators. The latter is based on clock region coverage as a main timed structural coverage criterion. This study shows that 'priority-based' approach can combine a high fault coverage and clock region coverage with a relatively small number of test traces in comparison with other test approaches. A set of experiences and lessons learned are highlighted as result of the real-time test bed. © 2012 Elsevier Inc.","Real-Time Embedded Systems; Test bed; Timed Automata; Timed model-based testing"
"SETZ logistics models and system framework for manufacturing and exporting large engineering assets","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879964336&doi=10.1016%2fj.jss.2012.09.032&partnerID=40&md5=4c305ae887b1caebcc498a33a8feb116","Given the dynamic and increasingly competitive nature of international commerce, manufacturing companies must plan global logistics operations for sustainable competitive advantage. Many enterprises build collaborative manufacturing networks across multinational regions to reduce production costs and gain access to new and often unfamiliar markets. Recognizing the strategic importance of globalization, government agencies are developing Special Economic Trade Zones (SETZ). These zones are regulated industrial districts which encourage manufacturing by offering incentives such as new logistic designs linked with the latest information technologies. The objective of this paper is to analyze and design SETZ logistics hub models and system framework for linking manufacturers. By defining the characteristics of the different types of specialized trade zones, and briefly discussing the older types of trade models that are no longer competitive, this research uses the case of a Taiwan power transformer manufacturer to analyze the supply chain logistics processes for manufacturing and exporting large engineering assets within a SETZ. The logistics models and information system framework developed provide a general reference for other governments, companies, and industrial sectors that intend to design export-oriented industrial parks incorporating IT-centric and globally oriented SETZ techniques. © 2012 Elsevier Inc.","Large-scale engineering assets; Logistics model; Special economic and trade zones"
"3D architecture viewpoints on service automation","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875243618&doi=10.1016%2fj.jss.2012.12.035&partnerID=40&md5=f44ccb374c8cfb700c615aeb02e7dad4","Service-oriented architecture is an emerging paradigm for the execution of business-oriented as well as technical infrastructure processes by means of services. Automating the execution of services is of paramount importance in order to fulfill the needs of companies. However we have found that automation-although important-is seldom addressed explicitly as a concern when stating requirements or designing the software architecture of the service-based applications (SBAs). In this paper we define three architectural viewpoints framing the concerns about service automation. These three viewpoints, called 3D (Decisions, Degree, Data), respectively: express architectural decisions about automation; help identifying the level (degree) of automation required, and represent the specific data required to support automation in services. They have been applied to three industrial case studies and one academic experiment. Results show that they successfully support both technical and non-technical stakeholders in understanding how, and communicating upon, their concerns related to service automation have been addressed. The application of the 3D service automation viewpoints to different domains exhibits promising reusability.© 2012 Elsevier Inc. All rights reserved.","Architecture viewpoint; Automation; Service-oriented architecture"
"An approach for constructing private storage services as a unified fault-tolerant system","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879905183&doi=10.1016%2fj.jss.2013.02.056&partnerID=40&md5=ad78fa9cab8665dd3833287935e942b3","Organizations are gradually outsourcing storage services such as online hosting files, backup, and archival to public providers. There are however concerns with this process because organizations cannot access files when the service provider is unavailable as well as they have no control and no assurance on the management procedures related to data. As a result, organizations are exploring alternatives to build their own multi-tenant storage capacities. This paper presents the design, implementation and performance evaluation of an approach for constructing private online storage services. A hierarchical multi-tier architecture has been proposed to concentrate these services in a unified storage system, which applies fault-tolerant and availability strategies to the files by passing redundant information among the services or tiers. Our approach automates the construction of such a unified system, the data allocation procedure and the recovery process to overcome site failures. The parameters involved in the performance of the storage services are concentrated into intuitive metrics based on utilization percentage, which simplifies the administration of the storage system. We show our performance assessments and the lessons learned from a case study in which a federated storage network has been built from four trusted organizations spanning two different continents. © 2013 Elsevier Inc. All rights reserved.","Fault-tolerance; Federation; Storage services; Virtualization; Web-based storage"
"An exploration of technical debt","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876410801&doi=10.1016%2fj.jss.2012.12.052&partnerID=40&md5=6903e8d158bbe62fc05364ab2d040cf3","Context: Whilst technical debt is considered to be detrimental to the long term success of software development, it appears to be poorly understood in academic literature. The absence of a clear definition and model for technical debt exacerbates the challenge of its identification and adequate management, thus preventing the realisation of technical debt's utility as a conceptual and technical communication device. Objective: To make a critical examination of technical debt and consolidate understanding of the nature of technical debt and its implications for software development. Method: An exploratory case study technique that involves multivocal literature review, supplemented by interviews with software practitioners and academics to establish the boundaries of the technical debt phenomenon. Result: A key outcome of this research is the creation of a theoretical framework that provides a holistic view of technical debt comprising a set of technical debts dimensions, attributes, precedents and outcomes, as well as the phenomenon itself and a taxonomy that describes and encompasses different forms of the technical debt phenomenon. Conclusion: The proposed framework provides a useful approach to understanding the overall phenomenon of technical debt for practical purposes. Future research should incorporate empirical studies to validate heuristics and techniques that will assist practitioners in their management of technical debt. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Benefits and drawbacks; Code debt; Multivocal literature review; Outcomes; Precedents; Technical debt"
"A novel semantic information retrieval system based on a three-level domain model","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875239115&doi=10.1016%2fj.jss.2013.01.029&partnerID=40&md5=2277dafe73837263777305e6de7efa15","This paper presents a methodology and a prototype for extracting and indexing knowledge from natural language documents. The underlying domain model relies on a conceptual level (described by means of a domain ontology), which represents the domain knowledge, and a lexical level (based on WordNet), which represents the domain vocabulary. A stochastic model (the ME-2L-HMM2, which mixes-in a novel way-HMM and maximum entropy models) stores the mapping between such levels, taking into account the linguistic context of words. Not only does such a context contain the surrounding words; it also contains morphologic and syntactic information extracted using natural language processing tools. The stochastic model is then used, during the document indexing phase, to disambiguate word meanings. The semantic information retrieval engine we developed supports simple keyword-based queries, as well as natural language-based queries. The engine is also able to extend the domain knowledge, discovering new and relevant concepts to add to the domain model. The validation tests indicate that the system is able to disambiguate and extract concepts with good accuracy. A comparison between our prototype and a classic search engine shows that the proposed approach is effective in providing better accuracy.© 2012 Elsevier Inc. All rights reserved.","HMM; MaxEnt; Ontology; Semantic information retrieval; Word sense disambiguation; WordNet"
"Setting the best view of a virtual teacher in a mixed reality physical-task learning support system","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879950146&doi=10.1016%2fj.jss.2012.08.060&partnerID=40&md5=9ee45741ffb5602f9ca87553bd4ca987","In this research, we investigated the virtual teacher's positions and orientations that led to optimal learning outcome in mixed-reality environment. First, this study showed that the virtual teacher's position and orientation have an effect on learning efficiency, when some teacher-settings are more comfortable and easy to watch than others. A sequence of physical-task learning experiments have been conducted using mixed-reality technology. The result suggested that the virtual-teacher's close side-view is the optimal view for learning physical-tasks that include significant one-hand movements. However, when both hands are used, or rotates around, a rotation-angle adjustment becomes necessary. Therefore, we proposed a software automatic-adjustment method governing the virtual teacher's horizontal rotation angle, so that the learner can easily observe important body motions. The proposed software method was revealed to be effective for motions that gradually reposition the most important moving part. Finally, to enhance the proposed method in the future, we conducted an experiment to find out the effect of setting the vertical view-angle. The result recommended that the more motion's rotation involved the more vertical view angles are wanted to see the whole motion clear. © 2012 Elsevier Inc. All rights reserved.","Collaborative computing; Human computer interaction; Mixed reality; Physical tasks learning; Virtual reality"
"Multi-sprint planning and smooth replanning: An optimization model","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881475474&doi=10.1016%2fj.jss.2013.04.028&partnerID=40&md5=8992afe591117c0f1003064774162767","Most agile methods divide a project into sprints (iterations), and include a sprint planning phase that is critical to ensure the project success. Several factors impact on the optimality of a sprint plan, which makes the planning problem difficult. In this paper we formalize the planning problem and propose an optimization model that, given the estimates made by the project team and a set of development constraints, produces a multi-sprint optimal plan that maximizes the business value perceived by users. To cope with the inherent flexibility and uncertainty of agile projects, our approach ensures that a baseline plan can be revised and re-optimized during project execution without disrupting it, which we call smooth replanning. The planning problem is converted into a generalized assignment problem, given a linear programming formulation, and solved using the IBM ILOG CPLEX Optimizer. Our model is validated on both real and synthetic projects. In particular, a case study on two real projects confirms the effectiveness of our approach; as to efficiency, for medium-sized problems an exact solution is found in a few minutes, while for large problems a heuristic solution that is less than 1% far from the exact one is returned in a few seconds. Finally, some smooth replanning tests investigate the trade-off between plan quality and stability. © 2013 Elsevier Inc.","Agile methods; Linear programming; Optimization models; Scrum; Software engineering"
"A reference architecture for organizing the internal structure of metadata-based frameworks","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875235796&doi=10.1016%2fj.jss.2012.12.024&partnerID=40&md5=4db064325c0a1df5ddffcb4270518b09","Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes. © 2012 Elsevier Inc.","Framework; Metadata; Metadata-based framework; Pattern language; Reference architecture; Software architecture"
"Cooperative clustering for software modularization","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880150522&doi=10.1016%2fj.jss.2013.03.080&partnerID=40&md5=b341c32d68355ef2b0df1c8fd82c84b4","Clustering is a useful technique to group data entities. Many different algorithms have been proposed for software clustering. To combine the strengths of various algorithms, researchers have suggested the use of Consensus Based Techniques (CBTs), where more than one actors (e.g. algorithms) work together to achieve a common goal. Although the use of CBTs has been explored in various disciplines, no work has been done for modularizing software. In this paper, the main research question we investigate is whether the Cooperative Clustering Technique (CCT), a type of CBT, can improve software modularization results. The main contributions of this paper are as follows. First, we propose our CCT in which more than one similarity measure cooperates during the hierarchical clustering process. To this end, we present an analysis of well-known measures. Second, we present a cooperative clustering approach for two types of well-known agglomerative hierarchical software clustering algorithms, for binary as well as non-binary features. Third, to evaluate our proposed CCT, we conduct modularization experiments on five software systems. Our analysis identifies certain cases that reveal weaknesses of the individual similarity measures. The experimental results support our hypothesis that these weaknesses may be overcome by using more than one measure, as our CCT produces better modularization results for test systems in which these cases occur. We conclude that CCTs are capable of showing significant improvement over individual clustering algorithms for software modularization. © 2013 Elsevier Inc.","Cooperative clustering; Ellenberg-NM; Feature vector cases; Jaccard-NM; Software clustering; Unbiased"
"SCRUMIA - An educational game for teaching SCRUM in computing courses","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882573854&doi=10.1016%2fj.jss.2013.05.030&partnerID=40&md5=35f398f571a1589e63dc8d2d97e4b93b","Due to the increasing use of agile methods, teaching SCRUM as an agile project management methodology has become more and more important. In order to teach students to be able to apply SCRUM in concrete situations, often educational (simulation) games are used. However, most of these games have been developed more for professional trainings than taking into consideration typical restrictions of university courses (such as, class duration and low financial resources for instructional materials). Therefore, we present a manual paper and pencil game to reinforce and teach the application of SCRUM in undergraduate computing programmes complementing theoretical lectures. The game has been developed following a systematic instructional design process and based on our teaching experience. It has been applied several times in two undergraduate project management courses. We evaluated motivation, user experience and the game's contribution to learning through case studies on Kirkpatrick's level one based on the perception of the students. First results indicate the potential of the game to contribute to the learning of SCRUM in an engaging way, keeping students immersed in the learning task. In this regard, the game offers a low-budget alternative to complement traditional instructional strategies for teaching SCRUM in the classroom. © 2013 Elsevier Inc.","Game; SCRUM; Teaching"
"Industry's role in data and software curation in the cloud","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881478220&doi=10.1016%2fj.jss.2013.01.051&partnerID=40&md5=bc711dfd9fb4ee698e092dd5e77cb98c","The cloud is a de facto place now for storage and for software. For ordinary researchers, it is not as easy to use as one would hope. This column explores how a researcher painlessly leverages the cloud to archive, share and publish scientific data that is richer than text; and to showcase experimental tools in any environment, for review or classes. We coin the phrases middleware and baseware as they apply to the cloud and give two examples of middleware that is freely available for data and for software curation and sharing. We end with some ideas about the future of the cloud from the industry-researcher standpoint. © 2013 Elsevier Inc.","Cloud computing; Data curation; Middleware"
"Research state of the art on GoF design patterns: A mapping study","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879951544&doi=10.1016%2fj.jss.2013.03.063&partnerID=40&md5=e25a1531cb1ade4d8bfb6735b2a700b8","Design patterns are used in software development to provide reusable and documented solutions to common design problems. Although many studies have explored various aspects of design patterns, no research summarizing the state of research related to design patterns existed up to now. This paper presents the results of a mapping study of about 120 primary studies, to provide an overview of the research efforts on Gang of Four (GoF) design patterns. The research questions of this study deal with (a) if design pattern research can be further categorized in research subtopics, (b) which of the above subtopics are the most active ones and (c) what is the reported effect of GoF patterns on software quality attributes. The results suggest that design pattern research can be further categorized to research on GoF patterns formalization, detection and application and on the effect of GoF patterns on software quality attributes. Concerning the intensity of research activity of the abovementioned subtopics, research on pattern detection and on the effect of GoF patterns on software quality attributes appear to be the most active ones. Finally, the reported research to date on the effect of GoF patterns on software quality attributes are controversial; because some studies identify one pattern's effect as beneficial whereas others report the same pattern's effect as harmful. © 2013 Elsevier Inc. All rights reserved.","Design patterns; Mapping study; Software quality attributes"
"Beyond ATAM: Early architecture evaluation method for large-scale distributed systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.923","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872684842&doi=10.1016%2fj.jss.2012.10.923&partnerID=40&md5=1cc63c2166627a61bace9f7e08401d3d","The development of large-scale distributed software systems involves substantial investment and is exposed to a high level of risk. Early architectural decisions define how the system is organised in terms of permanent data management, data communication, data input and output, coarse-grained modularisation and allocation within the organisational structure. Such a system's ""back-bone"" has been referred to as the System Organisation Pattern. Analysing architecture early in the development life cycle can help identify significant technical risks and mitigate them at a minimal cost. However, architecture assessment methods, such as the Architecture Trade-off Analysis Method, cannot easily be applied very early for architecture defined only conceptually. In addition, the influence of the System Organisation Pattern on the detailed properties of the final system cannot be precisely quantified, which makes applying known architecture analysis methods even more difficult. The Early Architecture Evaluation Method has been developed to assess the System Organisation Pattern much earlier than an ATAM-based assessment would be possible, i.e. in the inception phase of the Rational Unified Process. The method defines an architecture evaluation process, at the heart of which is an assessment model based on the Goal-Question-Metric scheme. The method identifies substantial risks posed by the architectural decisions comprising the System Organisation Pattern. The method has been evaluated on seven real-life examples of large-scale systems. © 2012 Elsevier Inc.","Architectural decisions; Architecture analysis; Architecture reviews"
"An approach to software reliability prediction based on time series modeling","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879942021&doi=10.1016%2fj.jss.2013.03.045&partnerID=40&md5=1290cf0fd710abbeb4e31a676e7318b6","Reliability is the key factor for software system quality. Several models have been introduced to estimate and predict reliability based on results of software testing activities. Software Reliability Growth Models (SRGMs) are considered the most commonly used to achieve this goal. Over the past decades, many researchers have discussed SRGMs' assumptions, applicability, and predictability. They have concluded that SRGMs have many shortcomings related to their unrealistic assumptions, environment-dependent applicability, and questionable predictability. Several approaches based on non-parametric statistics, Bayesian networks, and machine learning methods have been proposed in the literature. Based on their theoretical nature, however, they cannot completely address the SRGMs' limitations. Consequently, addressing these shortcomings is still a very crucial task in order to provide reliable software systems. This paper presents a well-established prediction approach based on time series ARIMA (Autoregressive Integrated Moving Average) modeling as an alternative solution to address the SRGMs' limitations and provide more accurate reliability prediction. Using real-life data sets on software failures, the accuracy of the proposed approach is evaluated and compared to popular existing approaches. © 2013 Elsevier Inc. All rights reserved.","Reliability prediction; Software Reliability Growth Models; Time series ARIMA models"
"A survey study of critical success factors in agile software projects in former Yugoslavia IT companies","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876413177&doi=10.1016%2fj.jss.2013.02.027&partnerID=40&md5=4cdc76e424410d90984b949912672a33","Determining the factors that have an influence on the success of the software development projects has been the focus of extensive research for more than 30 years. In recent years agile methodology of software development has become the dominant one for all kinds of software development projects. In this paper we present the results of empirical study for determining critical factors that influence the success of agile software projects which we conducted among senior developers and project managers from IT companies located in the former Yugoslavia countries within South Eastern Europe (SEE) region. This study is inspired by the similar study conducted 5 years ago (Chow and Cao, 2008). With this study we were not able to confirm the model developed in the previous study. Moreover it disconfirmed not only part of the factors, but very much questioned the whole scheme. However, we were able to shed additional light regarding agile software development in former Yugoslavia countries from SEE region as a reference region for investigating outsourced projects done in agile way. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Agile methods; Critical success factors; Software development"
"A sliding window based algorithm for frequent closed itemset mining over data streams","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872683902&doi=10.1016%2fj.jss.2012.10.011&partnerID=40&md5=a41dadc67ceb529b00605fc5e919d8b9","Frequent pattern mining over data streams is an important problem in the context of data mining and knowledge discovery. Mining frequent closed itemsets within sliding window instead of complete set of frequent itemset is very interesting since it needs a limited amount of memory and processing power. Moreover, handling concept change within a compact set of closed patterns is faster. However, it requires flexible and efficient data structures as well as intuitive algorithms. In this paper, we have introduced an effective and efficient algorithm for closed frequent itemset mining over data streams operating in the sliding window model. This algorithm uses a novel data structure for storing transactions of the window and corresponding frequent closed itemsets. Moreover, the support of a new frequent closed itemset is efficiently computed and an old pattern is removed from the monitoring set when it is no longer frequent closed itemset. Extensive experiments on both real and synthetic data streams show that the proposed algorithm is superior to previously devised algorithms in terms of runtime and memory usage.","Closed frequent itemsets; Concept change; Data mining; Data stream; Sliding window"
"Optimal univariate microaggregation with data suppression","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.901","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872686183&doi=10.1016%2fj.jss.2012.10.901&partnerID=40&md5=0b2fe64030649a27b84ac75cef485be1","Microaggregation is a disclosure limitation method that provides security through k-anonymity by modifying data before release but does not allow suppression of data. We define the microaggregation problem with suppression (MPS) to accommodate data suppression, and present a polynomial-time algorithm, based on dynamic programming, for optimal univariate microaggregation with suppression. Experimental results demonstrate the practical benefits of suppressing a few carefully selected data points during microaggregation using our method. © 2012 Elsevier Inc.","Data security; Disclosure control; Dynamic programming; Microaggregation; Privacy"
"Map-matched trajectory compression","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876415910&doi=10.1016%2fj.jss.2013.01.071&partnerID=40&md5=2959febf7ddddc32425ef5de6db0b697","The wide usage of location aware devices, such as GPS-enabled cellphones or PDAs, generates vast volumes of spatiotemporal streams of location data raising management challenges, such as efficient storage and querying. Therefore, compression techniques are inevitable also in the field of moving object databases. Related work is relatively limited and mainly driven by line simplification and data sequence compression techniques. Moreover, due to the (unavoidable) erroneous measurements from GPS devices, the problem of matching the location recordings with the underlying traffic network has recently gained the attention of the research community. So far, the proposed compression techniques have not been designed for network constrained moving objects, while on the other hand, existing map matching algorithms do not take compression aspects into consideration. In this paper, we propose solutions tackling the combined, map matched trajectory compression problem, the efficiency of which is demonstrated through an extensive experimental evaluation on offline and online trajectory data using synthetic and real trajectory datasets. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Compression; Map-matching; Network trajectory"
"Improving logic-based testing","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880176279&doi=10.1016%2fj.jss.2012.08.024&partnerID=40&md5=9eb13633eb221edfb5cd8d4d723ace5b","Logic-based testers design tests from logical expressions that appear in software artifacts such as source code, design models, and requirements specifications. This paper presents three improvements to logic-based test design. First, in the context of mutation testing, we present fault hierarchies for the six relational operators. Applying the ROR mutation operator causes each relational operator to generate seven mutants per clause. The fault hierarchies show that only three of these seven mutants are needed. Second, we show how to bring the power of the ROR operator to logic-based test criteria such as the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. Third, we present theoretical results supported by empirical data that show that the more recent coverage criterion of minimal-MUMCUT can find significantly more faults than MCDC. The paper has three specific recommendations: (1) Change the way the ROR mutation operator is defined in existing and future mutation systems. (2) Augment logic-based test criteria to incorporate relational operator replacement from mutation. (3) Replace the use of MCDC with minimal-MUMCUT, both in practice and in standards documents like FAA-DO178B. © 2013 Elsevier Inc. All rights reserved.","Logic-based testing; MCDC; Mutation analysis; Software testing"
"MDE software process lines in small companies","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875270026&doi=10.1016%2fj.jss.2012.09.033&partnerID=40&md5=b4bd0bb9378eaba09a42ff8306d01824","Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.© 2012 Elsevier Inc. All rights reserved.","Model-driven engineering; Process asset reuse; Software process lines"
"Does decision documentation help junior designers rationalize their decisions? A comparative multiple-case study","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876408156&doi=10.1016%2fj.jss.2013.01.057&partnerID=40&md5=a22f5d54e7ecc3b0c78480f8bfaea806","Software architecture design is challenging, especially for junior software designers. Lacking practice and experience, junior designers need process support in order to make rational architecture decisions. In this paper, we present the results of a comparative multiple-case study conducted to find out if decision viewpoints from van Heesch et al. (2012, in press) can provide such a support. The case study was conducted with four teams of software engineering students working in industrial software projects. Two of the four teams were instructed to document their decisions using decision viewpoints; the other two teams were not instructed to do so. We observed the students for a period of seven weeks by conducting weekly focus groups and by analyzing their work artifacts and minutes. Our findings suggest that junior designers who use decision viewpoints are more systematic in exploring and evaluating solution options. However, the decision viewpoints did not help them in managing requirements and complexity. © 2013 Elsevier Inc. All rights reserved.","Architecture decisions; Case study; Design reasoning; ISO/IEC/IEEE 42010; Software architecture; Viewpoints"
"Continuous range k-nearest neighbor queries in vehicular ad hoc networks","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875266698&doi=10.1016%2fj.jss.2012.12.034&partnerID=40&md5=395f7e7c55aad33eb502baf6da474e8e","A driver should constantly keep an eye on nearby vehicles in order to avoid collisions. Unfortunately, the driver often does not see nearby vehicles because of obstacles (e.g.; other vehicles, trees, buildings, etc.). This paper introduces a novel type of query, called a continuous range k-nearest neighbor (CRNN) query, in vehicular ad hoc networks, and it presents a new approach to process such a query. Most existing solutions to continuous nearest neighbor (CNN) queries focus on static objects, such as gas stations and restaurants, while this work concentrates on CRNN queries over moving vehicles. This is a challenging problem due to the high mobility of the vehicles. The CRNN query has characteristics in common with continuous range (CR) and CNN queries. In terms of CNN queries, the proposed approach achieves the same goal as the existing solutions, which is to decide effectively on valid intervals during which the query result remains unchanged. The proposed scheme aims to minimize the use of wireless network bandwidth, the computational cost, and the local storage while preserving information on the continuous movement of vehicles within the broadcast range of a given vehicle. Extensive experimental results confirm the effectiveness and superiority of the proposed scheme in comparison with an existing method. © 2012 Elsevier Inc.","Collision avoidance; Location awareness; Nearest neighbor queries; Vehicular ad hoc network"
"Introducing automated procedures in 3G network planning and optimization","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876407749&doi=10.1016%2fj.jss.2013.01.072&partnerID=40&md5=cf6c8a03e8d3765f3076897394199f57","Third generation (3G) networks have been launched for quite some time and cellular operators have already started to face the challenges when operating such kind of networks, compared to their 2G predecessors. In this paper, we give an overview of the network planning and optimization processes in 3G networks. Emphasis is given on the interdependence between coverage and capacity, which increases the complexity of operating 3G networks in a cost effective and quality assuring manner. We stretch out the importance of introducing automated procedures in these challenging tasks and describe main inputs and outputs of these procedures, such as initial network configuration and constraints, drive test data, and key performance indicators statistics and goals. We use two case studies from a operational 3G system to indicate the improvements on the network when these automated procedures are employed. © 2013 Elsevier Inc.","3G; Optimization; Planning"
"Bringing knowledge into recommender systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879913293&doi=10.1016%2fj.jss.2012.10.002&partnerID=40&md5=f6cc882546b561fbf73012881754f5ee","Recommender systems are largely used nowadays to support collaborative tasks. However, it is important to consider each user's knowledge of the system for the recommended subject. In this paper we describe the use of user knowledge to improve the recommender system of the Business Process Cooperative Editor (BPCE), a collaborative business process modeling tool. We use the concept of the Knowledge Vector, developed in a previous work on collaborative navigation, to factor user knowledge into recommendations. We present Knowledge Vectors and how they are applied to the editor. A simulation to evaluate the effectiveness of the editor's new recommender system is presented. © 2012 Elsevier Inc. All rights reserved.","Knowledge; Knowledge Vector; Recommender systems"
"Design and testbed evaluation of RDMA-based middleware for high-performance data transfer applications","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879981939&doi=10.1016%2fj.jss.2013.01.070&partnerID=40&md5=06f637e10dba2d254c994a097d82afd4","Providing high-speed data transfer is vital to various data-intensive applications supported by data center networks. We design a middleware layer of high-speed communication based on Remote Direct Memory Access (RDMA) that serves as the common substrate to accelerate various data transfer tools, such as FTP, HTTP, file copy, sync and remote file I/O. This middleware offers better end-to-end bandwidth performance than the traditional TCP-based alternatives, while it hides the heterogeneity of the underlying high-speed architecture. This paper describes this middleware's function modules, including resource abstraction and task synchronization and scheduling, that maximize the parallelism and performance of RDMA operations. For networks without RDMA hardware acceleration, we integrate Linux kernel optimization techniques to reduce data copy and processing in the middleware. We provide a reference implementation of the popular file-transfer protocol over this RDMA-based middleware layer, called RFTP. Our experimental results show that our RFTP outperforms several TCP-based FTP tools, such as GridFTP, while it maintains very low CPU consumption on a variety of data center platforms. Furthermore, those results confirm that our RFTP tool achieves near line-speed performance in both LAN and WAN, and scales consistently from 10 Gbps Ethernet to 40 Gbps Ethernet and InfiniBand environments. © 2013 Elsevier Inc. All rights reserved.","Distributed systems; Middleware; Remote Direct Memory Access"
"Testing techniques selection based on ODC fault types and software metrics","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876413660&doi=10.1016%2fj.jss.2013.02.020&partnerID=40&md5=a3e55df12b31ae87bd1ca3d34645885b","Software testing techniques differ in the type of faults they are more prone to detect, and their performance varies depending on the features of the application being tested. Practitioners often use informally their knowledge about the software under test in order to combine testing techniques for maximizing the number of detected faults. This work presents an approach to enable practitioners to select testing techniques according to the features of the software to test. A method to build a testing-related base of knowledge for tailoring the techniques selection process to the specific application(s) is proposed. The method grounds upon two basic steps: (i) constructing, on an empirical basis, models to characterize the software to test in terms of fault types it is more prone to contain; (ii) characterizing testing techniques with respect to fault types they are more prone to detect in the given context. Using the created base of knowledge, engineers within an organization can define the mix of techniques so as to maximize the effectiveness of the testing process for their specific software. © 2013 Elsevier Inc. All rights reserved.","Software testing"
"An authentication model towards cloud federation in the enterprise","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881480645&doi=10.1016%2fj.jss.2012.12.031&partnerID=40&md5=0445cf1e87ce6f61bb79a568bc589626","Cloud computing has emerged as a new paradigm which brought business opportunities as well as software engineering challenges. In The Cloud computing, business participants such as service providers, enterprise solutions, and marketplace applications are required to adopt a cloud architecture engineered for security and performance. Marketplace applications offer a great opportunity for enterprises to employ new Cloud capabilities to add value and extend business functionality. One of the major hurdles of formal adoption of Marketplace in the enterprise is performance. Enterprise applications (e.g. Lync Server, SAP, SharePoint, and Exchange Server) require a mechanism to predict and manage performance expectations. In previous research, we provided optimization for OAuth 2.0 adoption in the Enterprise. In this research, we extend the optimization to include identity federation in the Marketplace. This optimization is achieved by introducing provisioning steps to pre-establish trust amongst enterprise applications' Resource Servers, its associated Authorization Server and the clients interested in access to protected resources. We then introduce the notion of referral tokens to enable Marketplace applications federation across organizations. In this architecture, trust is provisioned and synchronized as a pre-requisite step to authentication amongst all communicating entities in OAuth protocol, and referral tokens are used to establish trust federation for Marketplace applications across organizations. A real-life case study and a simulation test were used to validate the results. © 2012 Elsevier Inc.","Cloud performance; Cloud security; Engineering for federated clouds"
"Solidifying the foundations of the cloud for the next generation Software Engineering","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881477908&doi=10.1016%2fj.jss.2013.05.063&partnerID=40&md5=87621d26d6255882e69c0b9aa4dc336a","Infrastructure clouds are expected to play an important role in the next generation Software Engineering but currently there are some drawbacks. These clouds are too infrastructure oriented and they lack advanced service oriented capabilities such as service elasticity, quality of service or admission control to perform a holistic management of a whole application. The deployment of complex multi-tier applications on top of IaaS infrastructures requires to provide the IaaS platforms with an extra service layer that provides advanced service management functionality. © 2013 Elsevier Inc.","Cloud computing; IaaS"
"A goal-oriented simulation approach for obtaining good private cloud-based system architectures","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881375569&doi=10.1016%2fj.jss.2012.10.028&partnerID=40&md5=e26983f46b693599a63b7c08b3c58473","The fast-growing Cloud Computing paradigm makes it possible to use unprecedented amounts of computing resources at lower costs, among other benefits such as fast provisioning and reliability. In designing a good architecture - the numbers, types and layouts of devices - for a cloud-based system, which meets the goals of all stakeholders, such goals need to be factored in from the earliest stages. However, there seems to be a lack of methodologies for incorporating stakeholder goals into the design process for such systems, and for assuring with higher confidence that the designs are likely to be good enough for the stated goals. In this paper, we propose a goal-oriented simulation approach for cloud-based system design whereby stakeholder goals are captured, together with such domain characteristics as workflows, and used in creating a simulation model as a proxy for the cloud-based system architecture. Simulations are then run, in an interleaving manner, against various configurations of the model as a way of rationally exploring, evaluating and selecting among incrementally better architectural alternatives. We illustrate important aspects of this approach for the private cloud deployment model and report on our experiments, using a smartcard-based public transportation system. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; CloudSim; Goal-oriented; Little's Law; NFR framework; Requirements engineering; Simulation model; System architecture"
"A framework for query refinement with user feedback","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.069","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876410954&doi=10.1016%2fj.jss.2013.01.069&partnerID=40&md5=7cac0bddb35bfd049af0f1241772f281","SQL queries in the existing relational data model implement the binary satisfaction of tuples. That is, a data tuple is filtered out from the result set if it does not satisfy the constraints expressed in the predicates of the user submitted query. Posing appropriate queries for ordinary users is very difficult in the first place if they lack knowledge of the underlying dataset. Therefore, imprecise queries are commonplace for many users. In connection with this, this paper presents a framework for capturing user intent through feedback for refining the initial imprecise queries that can fulfill the users' information needs. The feedback in our framework consists of both unexpected tuples currently present in the query output and expected tuples that are missing from the query output. We show that our framework does not require users to provide the complete set of feedback tuples because only a subset of this feedback can suffice. We provide the point domination theory to complement the other members of feedback. We also provide algorithms to handle both soft and hard requirements for the refinement of initial imprecise queries. Experimental results suggest that our approach is promising compared to the decision tree based query refinement approach. © 2013 Elsevier Inc. All rights reserved.","Imprecise query; Query refinement; User feedback"
"Supporting adaptation of decentralized software based on application scenarios","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879954395&doi=10.1016%2fj.jss.2013.02.057&partnerID=40&md5=592170b5790065e757468d9838b23b0b","Software systems formed by autonomous software entities are greatly different from traditional software systems and it challenges researchers to find effective methods of supporting the adaptation of software systems. In this paper, an approach based on application scenarios is put forward for facilitating dynamic adaptations of decentralized software systems in unpredicted situations. Scenarios offer behavior norms to regulate the behavior of autonomous software entities under specific situations so that software entities can take fitted and coordinative actions when they are confronted with diverse and even unpredicted situations. At the end of this paper, a simulation traffic system is developed and studied. The experimental results show that the adaptability of the system is improved remarkably after application scenarios are deployed. In the case study, the efficiency and scalability of the scenario-based adaptation mechanism are also experimented and analyzed. © 2013 Elsevier Inc. All rights reserved.","Adaptive software; Application scenario; Autonomous component"
"Quality-driven optimization of system architecture: Industrial case study on an automotive sub-system","2013","Journal of Systems and Software","10.1016/j.jss.2013.05.109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882580109&doi=10.1016%2fj.jss.2013.05.109&partnerID=40&md5=39eac8c16d2696482eae6e012df1c5db","Due to the complexity of today's embedded systems and time-to-market competition between companies developing embedded systems, system architects have to perform a complex task. To design a system which meets all its quality requirements becomes increasingly difficult because of customer demand for new innovative user functions. Methods and tools are needed to assist the architect during system design. The goal of this paper is to show how metaheuristic optimization approaches can improve the process of designing efficient architectures for a set of given quality attributes. A case study is conducted in which an architecture optimization framework is applied to an existing sub-system in the automotive industry. The case study shows that metaheuristic optimization approaches can find efficient solutions for all quality attributes while fulfilling given constraints. By optimizing multiple quality attributes the framework proposes revolutionary architecture solutions in contrast to human architects, who tend to propose solutions based on previous architectures. Although the case study shows savings in manual effort, it also shows that the proposed architecture solutions should be assessed by the human architect. So, the paper demonstrates how an architecture optimization framework complements the domain knowledge and experience of the architect. © 2013 Elsevier Inc.","Component-based software engineering (CBSE); Optimization of software architecture design; System architecture for embedded systems"
"Cloud engineering is Search Based Software Engineering too","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881480406&doi=10.1016%2fj.jss.2012.10.027&partnerID=40&md5=dca42cd82dc597160df6e3eaa69c8c4f","Many of the problems posed by the migration of computation to cloud platforms can be formulated and solved using techniques associated with Search Based Software Engineering (SBSE). Much of cloud software engineering involves problems of optimisation: performance, allocation, assignment and the dynamic balancing of resources to achieve pragmatic trade-offs between many competing technical and business objectives. SBSE is concerned with the application of computational search and optimisation to solve precisely these kinds of software engineering challenges. Interest in both cloud computing and SBSE has grown rapidly in the past five years, yet there has been little work on SBSE as a means of addressing cloud computing challenges. Like many computationally demanding activities, SBSE has the potential to benefit from the cloud; 'SBSE in the cloud'. However, this paper focuses, instead, of the ways in which SBSE can benefit cloud computing. It thus develops the theme of 'SBSE for the cloud', formulating cloud computing challenges in ways that can be addressed using SBSE. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Search Based Software Engineering (SBSE)"
"Teaching cloud computing: A software engineering perspective","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881479802&doi=10.1016%2fj.jss.2013.01.050&partnerID=40&md5=0ad9a60698cde9acb909c8aac088bd42","This article discusses the teaching of cloud computing in a software engineering course. It suggests that all courses should have some material introducing students to cloud computing, that practical teaching should focus on Platform as a Service and that there is scope for a graduate course in cloud software engineering covering map-reduce, schema-free databases, service-oriented computing, security and compliance and design for resilience. © 2013 Elsevier Inc.","Cloud computing; Education; Software engineering"
"Threshold visual secret sharing by random grids with improved contrast","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880171372&doi=10.1016%2fj.jss.2013.03.062&partnerID=40&md5=1fadb83ad40d2cba1589acc9f17d0a74","A (k, n) visual cryptographic scheme (VCS) is a secret sharing method, which encodes a secret image S into n share images in such a way that the stacking of any more than or equal to k share images will reveal S, while any less than k share images provide no information about S. Kafri and Keren (1987) firstly implements (2,2)-VCS by random grids (RG-based VCS). Compared to conventional solutions of VCS, RG-based VCSs need neither extra pixel expansion nor complex codebook design. However, for a long period, RG-based VCSs are confined to (2,2) access structure. Until recently, Chen and Tsao (2011) proposed the first (k, n) RG-based VCS. In this paper, we improve the contrast of Chen and Tsao (2011)'s threshold scheme. The experimental results show that the proposed scheme outperforms Chen and Tsao (2011)'s scheme significantly in visual quality. © 2013 Elsevier Inc.","Contrast; Random grids; Visual cryptography"
"Compositional real-time scheduling framework for periodic reward-based task model","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876407257&doi=10.1016%2fj.jss.2013.02.052&partnerID=40&md5=5eb21c5f5e70f52063593fc9f726a1c4","As the size and complexity of embedded software systems increase, compositional real-time scheduling framework is widely accepted as means to build large and complex systems. A compositional real-time scheduling framework proposes to decompose a system into independent subsystems and provides ways to assemble them into a flexible hierarchical real-time scheduling system while guaranteeing the internal real-time requirements of each subsystem. In this paper, we consider the imprecise reward-based periodic task model in compositional scheduling framework. Thus, we introduce the imprecise periodic resource model to characterize the imprecise resource allocations provided by the system to a single component, and the interface model to abstract the imprecise real-time requirements of the component. The schedulability of mandatory parts is also analyzed to meet the minimum requirement of tasks. Finally, we provide a scheduling algorithm to guarantee a certain amount of reward, which makes it feasible to efficiently compose multiple imprecise components. © 2013 Elsevier Inc. All rights reserved.","Compositional and hierarchical real-time scheduling; Imprecise computation; Interface model; Resource model; Reward function"
"An improvement of diamond encoding using characteristic value positioning and modulus function","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875266401&doi=10.1016%2fj.jss.2012.12.053&partnerID=40&md5=ee7a6ef39ac36c68b11227450c3cf7a6","The diamond encoding technique controls payload and image quality by a k value. Although a small increase in k can increase payload, it also increases image distortion. In this paper, we proposed an improvement scheme to the diamond encoding technique for reducing image distortion during a k change. Another aim is to increase payload size. A square matrix conversion of the diamond matrix is used to lower the MSE values in high payload when k = 3. A lower MSE reduces image distortion. In low payload, that is when k changes from 1 to 2, a one dimension matrix modulus is used to reduce image distortion. Experimental results showed that payload may be increased while image quality is not significantly reduced. © 2013 Elsevier Inc.","Diamond characteristics; Diamond encoding; Exploiting modification direction"
"User Interface Transition Diagrams for customer-developer communication improvement in software development projects","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881474758&doi=10.1016%2fj.jss.2013.04.022&partnerID=40&md5=12d9ad9e497da22f115f3dcf4aae1c7e","We formalize the definition and construction of the User Interface Transition Diagram (UITD) which is a modelling notation for the transitions between UI presentations and the necessary conditions to trigger these transitions. We show how the UITD is able to improve the communication between stakeholders in a software development project: Human-Computer Interaction specialists, Software Engineers and customers who have little or no training in specialized modelling notations. We compare the UITD with other existing similar modelling notations highlighting the features that are better expressed in the UITD. We also include a case study in order to show how the UITD can be helpful in different phases of a software development project. The understandability of the UITD was confirmed by means of a test where different types of potential users were involved. © 2013 Elsevier Inc. All rights reserved.","Flow Functional requirements specification; Modelling notation; User Interface"
"A common API for delivering services over multi-vendor cloud resources","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881477740&doi=10.1016%2fj.jss.2013.04.037&partnerID=40&md5=8f42ad7ce8ecc4538beebb1e80818326","The increasing pace of evolution in business computing services leads enterprises to outsource secondary operations that are not part of their core business. The cloud computing market has been growing over the past few years and, consequently, many cloud companies are now offering a rich set of features to their consumers. Unfortunately, those cloud players have created new services with different APIs, which imply that cloud-oriented applications might be instantiated in one single cloud provider. This scenario is not desirable to the IT industry because their applications will become provider-dependent. In this paper we present a platform that allows applications to interoperate with distinct cloud providers' services using a normalized interface. The proposed approach provides a common API that minimizes the present deficit of cloud API standardization and provides secure and redundant services allocation. Moreover, services from different cloud providers can be combined and decorated with additional functionalities like, for instance, redundancy and ciphering on-the-fly. © 2013 Elsevier Inc.","Cloud databases; Cloud services; Cloud standardization; Cloud storage"
"Image encryption based on the Jacobian elliptic maps","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881477343&doi=10.1016%2fj.jss.2013.04.088&partnerID=40&md5=23bfda9f1d082a85f559a46f01cea84f","In this paper, a novel image encryption algorithm based on the Jacobian elliptic maps is presented. To illustrate the effectiveness of the proposed scheme, some security analyses are presented. It can be concluded that, the proposed image encryption technique can be applied for practical applications. Although the Jacobian elliptic maps presented in this paper aim at image encryption, it is not just limited to this experience and can be directly applied in other information security fields such as video encryption. © 2013 Elsevier Inc.","Cryptography; Image encryption; Jacobian elliptic maps; Lyapunov characteristic exponent"
"Agile requirements prioritization in large-scale outsourced system projects: An empirical study","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875238853&doi=10.1016%2fj.jss.2012.12.046&partnerID=40&md5=8d2a7bd950695ab7a5f710f8bd1c2796","The application of agile practices for requirements prioritization in distributed and outsourced projects is a relatively recent trend. Hence, not all of its facets are well-understood. This exploratory study sets out to uncover the concepts that practitioners in a large software organization use in the prioritization process and the practices that they deem good. We seek to provide a rich analysis and a deep understanding of three cases in an exploratory study that was carried out in a large and mature company, widely recognized for its excellence and its engagement in outsourced software development. We used in-depth interviews for data collection and grounded theory techniques for data analysis. Our exploration efforts yielded the following findings: (i) understanding requirements dependencies is of paramount importance for the successful deployment of agile approaches in large outsourced projects. (ii) Next to business value, the most important prioritization criterion in the setting of outsourced large agile projects is risk. (iii) The software organization has developed a new artefact that seems to be a worthwhile contribution to agile software development in the large: 'delivery stories', which complement user stories with technical implications, effort estimation and associated risk. The delivery stories play a pivotal role in requirements prioritization. (iv) The vendor's domain knowledge is a key asset for setting up successful client-developer collaboration. (v) The use of agile prioritization practices depends on the type of project outsourcing arrangement. Our findings contribute to the empirical software engineering literature by bringing a rich analysis of cases in agile and distributed contexts, from a vendor's perspective. We also discuss the possible implications of the results for research and in practice. © 2013 Elsevier Inc.","Agile requirements engineering; Case study; Distributed project management Qualitative research; Large projects; Outsourced software development; Requirements dependencies; Requirements prioritization"
"Certified Information Access","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881476253&doi=10.1016%2fj.jss.2013.04.089&partnerID=40&md5=15ba2679edc58fbbccf96186d56f6ca9","Certified Information Access (CIA) primitive allows a user to obtain answers to database queries in a way that she can verify the correctness of the received information. The database owner answers a query by providing the information matching the query along with a proof that such information are consistent with the actual content of the database. Current solutions to this problem require a computationally intensive setup phase. We describe two secure distributed implementations of a CIA service. In the first one, the database owner distributes the evaluation of a computation intensive function (e.g.; exponentiations) among a set of untrusted peers and locally reconstructs the result of such an evaluation. In the second one, we propose a protocol for securely outsourcing the whole computation of the data structures used in the implementations of the CIA primitive. In this case, the main issue to be considered is the need of guaranteeing on the one hand the confidentiality of the database contents and, on the other hand, the correctness and soundness of the answers obtained by the users. We argue that classical cryptographic primitives are not sufficient for our purposes and we introduce a new primitive, the Verifiable Deterministic Envelope, that may be of independent interest. © 2013 Elsevier Inc.","Cryptography; Secure computation outsourcing; Secure database"
"A fault induction technique based on voltage underfeeding with application to attacks against AES and RSA","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879950230&doi=10.1016%2fj.jss.2013.02.021&partnerID=40&md5=3e50212d8cf04e0d5690236aea6c9d8c","Fault injection attacks have proven to be a powerful tool to exploit the implementation weaknesses of cryptographic algorithms. Several techniques perturbing the computation of a cipher have been devised and successfully employed to leak secret information from erroneous results. We present a low-cost, non-invasive and effective technique to inject transient faults into a general purpose processor through lowering its feeding voltage, and to characterize the effects on the computing system. This technique is effective enough to lead attacks against a software implementation of a cryptosystem running on a full fledged ARM9 CPU with a complete operating system. We validate the effectiveness of the fault model through attacking OpenSSL implementations of the RSA and AES cryptosystems. A new attack against AES, able to retrieve the full 256-bit key, is described, and the number of faults to be collected is delineated. In addition, we propose a generalization of the attack against the RSA encryption presented in Barenghi et al. (2009), to a multi-bit fault model, and the analysis of its computational complexity. The attacks against AES retrieve all the round keys regardless of their derivation strategy, the number of cipher rounds and the diffusion layer, while the attacks against RSA retrieve either the message or the secret key. © 2013 Elsevier Inc. All rights reserved.","Embedded system security; Fault attacks; Side channel attacks"
"Triple-image encryption scheme based on one-time key stream generated by chaos and plain images","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872685910&doi=10.1016%2fj.jss.2012.11.026&partnerID=40&md5=89c75f3440cf5538261ae9ba7a7e9e0f","A triple color image encryption scheme based on chaos is designed. After decomposing the red, green and blue components of the three color images into three grayscale images, recombine them into one color image, then bitwise circularly shift each color pixel for randomly finite number of times. Finally, encrypt the shifted image by three pseudo-random sequences generated by the chaos with dynamical initial conditions, which are generated by the 256-bit long hash value that dependent on the three plain images. Numerical simulation has been performed to test the validity of the scheme, it's suitable for encrypting images in batches. © 2012 Elsevier Inc.","Chaos; Hash function; One-time keys; Triple-image encryption"
"A groupware system to support collaborative programming: Design and experiences","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879990652&doi=10.1016%2fj.jss.2012.08.039&partnerID=40&md5=f01f2d114eba722b6c990622682b08d9","The advances in network and collaboration technologies enable the creation of powerful environments for collaborative programming. One such environment is COLLECE, a groupware system to support collaborative edition, compilation and execution of programs in a synchronous distributed fashion, which includes advanced tools for communication, coordination and workspace awareness. The article analyses firstly some usability and design issues, discussing strengths and weaknesses of the system as a basis for the development of groupware tools to support collaborative programming. Then, the focus is on a number of experimental activities carried out. COLLECE was used to conduct a set of experimental activities about work productivity and program quality when comparing the activity of pair and solo programmers, and to analyse potential associations between ways of working and collaborating, and specific characteristics of the programs produced. © 2012 Elsevier Inc. All rights reserved.","Collaboration and interaction analysis; Collaborative learning environments; Collaborative programming; Distributed pair programming"
"Quality-adaptive visual secret sharing by random grids","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875252529&doi=10.1016%2fj.jss.2012.12.022&partnerID=40&md5=7725c7715d194e2dc802302e834c027c","Visual secret sharing (VSS), classified into visual-cryptography (VC)-based and random-grid (RG)-based, suffers from the contrast problem that the reconstructed secret with low visual quality is not easy to recognize. Even worse, the more share images stacked, the lower visual quality of reconstructed secrets revealed. Therefore, it is promising to remove this innate drawback. In this paper, with security still kept, the light transmission of share images generated by the proposed scheme is redesigned to be higher than before such that the better visual quality of reconstructed secrets is obtained. To demonstrate the feasibility, the experimental results show the reconstructed secrets are visually recognizable and the goal that the more share images stacked, the better quality of reconstructed secrets we have is achieved. © 2012 Elsevier Inc.","Quality adaptive; Random grid; Visual cryptography; Visual secret sharing"
"An efficient tree-based algorithm for mining sequential patterns with multiple minimum supports","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875269907&doi=10.1016%2fj.jss.2012.12.020&partnerID=40&md5=4a5ec261e28c55e0a36bec58be7d69b8","Sequential pattern mining (SPM) is an important technique for determining time-related behavior in sequence databases. In real-life applications, the frequencies for various items in a sequence database are not exactly equal. If all items are set with the same minimum support, the rare item problem may result, meaning that we are unable to effectively retrieve interesting patterns regardless of whether minsup is set too high or too low. Liu (2006) first included the concept of multiple minimum supports (MMSs) to SPM. It allows users to specify the minimum item support (MIS) for each item according to its natural frequency. A generalized sequential pattern-based algorithm, named Multiple Supports-Generalized Sequential Pattern (MS-GSP), was also developed to mine complete set of sequential patterns. However, the MS-GSP adopts candidate generate-and-test approach, which has been recognized as a costly and time-consuming method in pattern discovery. For the efficient mining of sequential patterns with MMSs, this study first proposes a compact data structure, called a Preorder Linked Multiple Supports tree (PLMS-tree), to store and compress the entire sequence database. Based on a PLMS-tree, we develop an efficient algorithm, Multiple Supports-Conditional Pattern growth (MSCP-growth), to discover the complete set of patterns. The experimental result shows that the proposed approach achieves more preferable findings than the MS-GSP and the conventional SPM. © 2012 Elsevier Inc.","Data mining; Multiple minimum supports; PLWAP-tree; Sequential patterns"
"Communities of Web service registries: Construction and management","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872686282&doi=10.1016%2fj.jss.2012.11.019&partnerID=40&md5=55c297cbf88fd9968c5fb78f321becfd","The last few years have seen a democratization in the use of Internet technologies, mainly Web services, for electronic B2B transactions. This has triggered an increase in the number of companies' Web service registries. In this paper, we propose to use communities as a means to organize Web service registries in such a context. We provide an automatic and implicit approach to create communities of Web service registries using registries' WSRD descriptions. We also define the needed management operations to ensure the communities consistency during a registry/community life-cycle. Experiments we have made show the feasibility and validity of our community creation approach as well as the specified managing operations. © 2012 Elsevier Inc. All rights reserved.","Communities of registries; Managing communities of registries; Organization of Web service registries; Service oriented computing"
"Software effort models should be assessed via leave-one-out validation","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879923542&doi=10.1016%2fj.jss.2013.02.053&partnerID=40&md5=223b7f8add37aaff77b069df5f7a9878","Context: More than half the literature on software effort estimation (SEE) focuses on model comparisons. Each of those requires a sampling method (SM) to generate the train and test sets. Different authors use different SMs such as leave-one-out (LOO), 3Way and 10Way cross-validation. While LOO is a deterministic algorithm, the N-way methods use random selection to build their train and test sets. This introduces the problem of conclusion instability where different authors rank effort estimators in different ways. Objective: To reduce conclusion instability by removing the effects of a sampling method's random test case generation. Method: Calculate bias and variance (B&V) values following the assumption that a learner trained on the whole dataset is taken as the true model; then demonstrate that the B&V and runtime values for LOO are similar to N-way by running 90 different algorithms on 20 different SEE datasets. For each algorithm, collect runtimes, B&V values under LOO, 3Way and 10Way. Results: We observed that: (1) the majority of the algorithms have statistically indistinguishable B&V values under different SMs and (2) different SMs have similar run times. Conclusion: In terms of their generated B&V values and runtimes, there is no reason to prefer N-way over LOO. In terms of reproducibility, LOO removes one cause of conclusion instability (the random selection of train and test sets). Therefore, we depreciate N-way and endorse LOO validation for assessing effort models. © 2013 Elsevier Inc. All rights reserved.","Bias; Prediction system; Software cost estimation; Variance"
"A novel approach to collaborative testing in a crowdsourcing environment","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880167663&doi=10.1016%2fj.jss.2013.03.079&partnerID=40&md5=5e8479544441ab91598b8e3b14fe07f2","Software testing processes are generally labor-intensive and often involve substantial collaboration among testers, developers, and even users. However, considerable human resource capacity exists on the Internet in social networks, expert communities, or internet forums - referred to as crowds. Effectively using crowd resources to support collaborative testing is an interesting and challenging topic. This paper defines the collaborative testing problem in a crowd environment as an NP-Complete job assignment problem and formulates it as an integer linear programming (ILP) problem. Although package tools can be used to obtain the optimal solution to an ILP problem, computational complexity makes these tools unsuitable for solving large-scale problems. This study uses a greedy approach with four heuristic strategies to solve the problem. This is called the crowdsourcing-based collaborative testing approach. This approach includes two phases, training phase and testing phase. The training phase transforms the original problem into an ILP problem. The testing phase solves the ILP using heuristic strategies. A prototype system, called the Collaborative Testing System (COTS), is also implemented. The experiment results show that the proposed heuristic algorithms produce good quality approximate solutions in an acceptable timeframe. © 2013 Elsevier Inc.","Cloud computing; Collaborative testing; Crowdsourcing; Integer linear programming; Software testing"
"Supporting real-time multiple data items query in multi-RSU vehicular ad hoc networks (VANETs)","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880152069&doi=10.1016%2fj.jss.2013.03.073&partnerID=40&md5=f899899d9925ca1e397fab80b75e42ff","There has been increasing interest in the issue of multi-item queries in wireless broadcasting systems recently. Query starvation and bandwidth utilization have been identified as key issues for improved performance. In this paper, we examine this problem in the context of VANETs with multiple cooperating Road Side Units (RSUs). We characterize a query with two deadlines: Query Total Deadline (QTD) which is the actual deadline of a query and Query Local Deadline (QLD) which is the duration a query is valid for serving in an RSU. By considering these two deadlines together with vehicle speed, RSU range and inter-RSU distance, we formulate a Cooperative Query Serving (CQS) scheme which allows multiple RSUs to share residual bandwidth and effectively address the query starvation as well as the bandwidth utilization problems, hence maximizing the chance of serving multiple items queries. Extensive simulation results confirm that CQS outperforms other existing scheduling algorithms. © 2013 Elsevier Inc. All rights reserved.","Multi-item queries; Roadside-to-vehicle communication (RVC); Vehicular ad hoc networks (VANETs)"
"Cloud computing security: The scientific challenge, and a survey of solutions","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881472688&doi=10.1016%2fj.jss.2012.12.025&partnerID=40&md5=2a2455a4d4427ef54f58097da22b6240","We briefly survey issues in cloud computing security. The fact that data are shared with the cloud service provider is identified as the core scientific problem that separates cloud computing security from other topics in computing security. We survey three current research directions, and evaluate them in terms of a running software-as-a-service example. © 2013 Elsevier Inc.","Browser security; Cloud computing; Homomorphic encryption; Trusted computing"
"A design rule language for aspect-oriented programming","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881476524&doi=10.1016%2fj.jss.2013.03.104&partnerID=40&md5=2d551c33e11b92f33a8d28cda3248952","Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches. © 2013 Elsevier Inc.","Aspect-oriented programming; Design rules; Modularity"
"A pattern fusion model for multi-step-ahead CPU load prediction","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875267200&doi=10.1016%2fj.jss.2012.12.023&partnerID=40&md5=4aacd9b2c969f461ba871e85b845583a","In distributed systems, resource prediction is an important but difficult topic. In many cases, multiple prediction is needed rather than only performing prediction at a single future point in time. However, traditional approaches are not sufficient for multi-step-ahead prediction. We introduce a pattern fusion model to predict multi-step-ahead CPU loads. In this model, similar patterns are first extracted from the historical data via calculating Euclidean distance and fluctuation pattern distance between historical patterns and current sequence. For a given pattern length, multiple similar patterns of this length can often be found and each of them can produce a prediction. We also propose a pattern weight strategy to merge these prediction. Finally, a machine learning algorithm is used to combine the prediction results obtained from different length pattern sets dynamically. Empirical results on four real-world production servers show that this approach achieves higher accuracy on average than existing approaches for multi-step-ahead prediction.© 2012 Elsevier Inc. All rights reserved.","CPU load; Fluctuation pattern; Multi-step-ahead prediction; Time series"
"An improved VLC-based lossless data hiding scheme for JPEG images","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.102","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880171362&doi=10.1016%2fj.jss.2013.03.102&partnerID=40&md5=e46ba44a1df4db9ef8b8bb4bf968f9c9","In this paper, a lossless data hiding scheme which directly embeds data into the bitstream of JPEG images is presented. For real cases, the JPEG code space is partly occupied, not all variable length codes (VLC) in the Huffman table are used during the JPEG image compression process. Thus, these unused VLCs can be made used of and data hiding can be performed by mapping one or more unused VLCs to one used VLC. Through analyzing the statistics of both used and unused VLCs, the proposed scheme can take full advantage of the unused VLCs by mapping Huffman codes according to a specific mapping strategy and reach higher capacity. The output stego image can keep exactly the same content as the original one and preserve the same file size, and if the file size is allowed to be enlarged, then our scheme can achieve a significant improvement of embedding capacity. © 2013 Elsevier Inc. All rights reserved.","Information hiding; JPEG bitstream; Lossless data hiding"
"Evidence of software inspection on feature specification for software product lines","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875245673&doi=10.1016%2fj.jss.2012.11.044&partnerID=40&md5=625c750c31f8fc3e8ca34cbc8fdb2dea","In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.© 2012 Elsevier Inc. All rights reserved.","Empirical study; Software inspection; Software product lines; Software quality control"
"A computer virus spreading model based on resource limitations and interaction costs","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872680733&doi=10.1016%2fj.jss.2012.11.027&partnerID=40&md5=f4a47d20999a19835ebfd13ac2be0212","Computer viruses are major threats to Internet security and privacy, therefore many researchers are addressing questions linked to virus propagation properties, spreading models, epidemic dynamics, tipping points, and control strategies. We believe that two important factors-resource limitations and costs-are being overlooked in this area due to an overemphasis on power-law connectivity distributions of scale-free networks affecting computer virus epidemic dynamics and tipping points. The study show (a) a significant epidemic tipping point does exists when resource limitations and costs are considered, with the tipping point exhibiting a lower bound; (b) when interaction costs increase or usable resources decrease, epidemic tipping points in scale-free networks grow linearly while density curves decrease linearly; (c) regardless of whether Internet user resources obey delta, uniform, or normal distributions, they retain the same epidemic dynamics and tipping points as long as the average value of those resources remains unchanged across different scale-free networks; (d) it is possible to control the spread of a computer virus in a scale-free network if resources are restricted and if costs associated with infection events are significantly increased through the use of a throttling strategy. © 2012 Elsevier Inc.","Agent-based simulation model; Epidemic dynamics; Power-law connectivity distributions; Scale-free networks; Small-world networks"
"Automated test data generation for branch testing using genetic algorithm: An improved approach using branch ordering, memory and elitism","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875264293&doi=10.1016%2fj.jss.2012.11.045&partnerID=40&md5=eca7e1dc597c9b495f191576d2ff0bba","One of the problems faced in generating test data for branch coverage using a metaheuristic technique is that the population may not contain any individual that encodes test data for which the execution reaches the predicate node of the target branch. In order to deal with this problem, in this paper, we (a) introduce three approaches for ordering branches for selection as targets for coverage with a genetic algorithm (GA) and (b) experimentally evaluate branch ordering together with elitism and memory to improve test data generation performance. An extensive preliminary study was carried out to help frame the research questions and fine tune GA parameters which were then used in the final experimental study. © 2012 Elsevier Inc.","Automated program test data generation; Genetic algorithm; Software testing"
"Metamodel-driven definition of a visual modeling language for specifying interactive groupware applications: An empirical study","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879921378&doi=10.1016%2fj.jss.2012.07.049&partnerID=40&md5=5903de7abc34ca265bc464d824165eba","This work is framed in the area of software development for Computer Supported Cooperative Work (CSCW). These software systems are called groupware systems. The development of groupware systems is a complex task, a problem that can be addressed applying the Model Driven Engineering (MDE) principles and techniques, where the use of models is essential. However, there are no proposals to address all issues to model in this kind of application (group work, shared context, coordination, etc.) and, in particular, there are no proposals that consider the modeling of both interactive and collaborative issues. To solve this deficiency, a domain-specific language (DSL) called Collaborative Interactive Application Notation (CIAN) has been proposed. To define this DSL a metamodel has been created describing the universe of discourse of the applications supporting interactive group work. We have defined the syntax and semantics of this language. We have also implemented a tool (called CIAT) for supporting the edition and validation of models created with CIAN. This tool has been implemented using the metamodeling facilities provided by the Eclipse platform. Finally, an empirical study was conducted with the aim of verifying the suitability of this approach and the perception of software engineers about its usefulness. The results obtained show that our proposal can facilitate the development process of groupware systems. © 2012 Elsevier Inc. All rights reserved.","DSL; Groupware design; Interaction Design; MDE; Metamodel"
"New steganography algorithm to conceal a large amount of secret message using hybrid adaptive neural networks with modified adaptive genetic algorithm","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876415383&doi=10.1016%2fj.jss.2012.12.006&partnerID=40&md5=080d96e3d8f569ed0749d260dbe35a61","In this paper, we propose a new steganography algorithm using non-uniform adaptive image segmentation (NUAIS) with an intelligent computing technique to conceal efficiently a large amount of confidential messages (Smsg) into color images. Whereas, the number of secret bits to be replaced is non uniform from byte to another byte; it based on byte characteristics, which are extracted by using 16 byte levels (BL) with variance distribution of the Neighboring Eight Bytes (NEB) around the current byte. Four security layers are introduced to increase resistance against statistical and visual attacks. These layers are designed to make an excellent imperceptible concealing Smsg with lower distortion of a color plane and high protection of Smsg. The proposed intelligent technique using the hybrid adaptive neural networks and modified adaptive genetic algorithm employing uniform adaptive relaxation (ANN-AGAUAR) is working as the fourth security layer to improve the quality of the stego image (Is). The results are discussed and compared with the previous steganography algorithms; it demonstrates that the proposed algorithm's effectiveness can be concealed efficiently the number of secret bits reached to four bits per byte with better visual quality. © 2012 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Adaptive image segmentation; Concealing with high capacity; Concealing with high security; Genetic algorithm; Neural networks; Steganography"
"A service-oriented framework for developing cross cloud migratable software","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881479590&doi=10.1016%2fj.jss.2012.12.033&partnerID=40&md5=e3579e45accff1886114e2891fdfa82a","Whilst cloud computing has burst into the current scene as a technology that allows companies to access high computing rates at limited costs, cloud vendors have rushed to provide tools that allow developers to build software for their cloud platforms. The software developed with these tools is often tightly coupled to their services and restrictions. Consequently vendor lock in becomes a common problem which multiple cloud users have to tackle in order to exploit the full potential of cloud computing. A scenario where component-based applications are developed for being deployed across several clouds, and each component can independently be deployed in one cloud or another, remains fictitious due to the complexity and the cost of their development. This paper presents a cloud development framework for developing cloud agnostic applications that may be deployed indifferently across multiple cloud platforms. Information about cloud deployment and cloud integration is separated from the source code and managed by the framework. Interoperability between interdependent components deployed in different clouds is achieved by automatically generating services and service clients. This allows software developers to segment their applications into different modules that can easily be deployed and redistributed across heterogeneous cloud platforms. © 2012 Elsevier Inc.","Adaptation; Cloud framework; Cross-cloud applications"
"Sirius: A heuristic-based framework for measuring web usability adapted to the type of website","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872680847&doi=10.1016%2fj.jss.2012.10.049&partnerID=40&md5=61e5ae6ce7c6227da456d79d8908e27e","The unquestionable relevance of the web in our society has led to an enormous growth of websites offering all kinds of services to users. In this context, while usability is crucial in the development of successful websites, many barely consider the recommendations of experts in order to build usable designs. Including the measurement of usability as part of the development process stands out among these recommendations. One of the most accepted methods for usability evaluation by experts is heuristic evaluation. There is abundant literature on this method. However, there is a lack of clear and specific guidelines to be used in the development and evaluation process. This is probably an important factor contributing to the aforementioned generalized deficiency in web usability. We miss an evaluation method based on heuristics whose measure is adapted to the type of evaluated website. In this paper we define Sirius, an evaluation framework based on heuristics to perform expert evaluations that takes into account different types of websites. We also provide a specific set of evaluation criteria, and a usability metric that quantifies the usability level achieved by a website depending on its type. © 2012 Elsevier Inc.","Heuristic evaluation; Usability measurement; Usability metric; Website classification"
"Toward automated refactoring of crosscutting concerns into aspects","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876418498&doi=10.1016%2fj.jss.2012.12.045&partnerID=40&md5=acc83d530ae9a448f8bd4f7f235a5ad7","Aspect-oriented programing (AOP) improves the separation of concerns by encapsulating crosscutting concerns into aspects. Thus, aspect-oriented programing aims to better support the evolution of systems. Along this line, we have defined a process that assists the developer to refactor an object-oriented system into an aspect-oriented one. In this paper we propose the use of association rules and Markov models to improve the assistance in accomplishing some of the tasks of this process. Specifically, we use these techniques to help the developer in the task of encapsulating a fragment of aspectizable code into an aspect. This includes the choice of a fragment of aspectizable code to be encapsulated, the selection of a suitable aspect refactoring, and the analysis and application of additional restructurings when necessary. Our case study of the refactoring of a J2EE system shows that the use of the process reduces the intervention of the developer during the refactoring. © 2013 Elsevier Inc. All rights reserved.","Aspect refactoring; Separation of concerns; Software evolution"
"Relevance, benefits, and problems of software modelling and model driven techniques - A survey in the Italian industry","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880144133&doi=10.1016%2fj.jss.2013.03.084&partnerID=40&md5=81248673209bcfa0a4b3b0aacce9816c","Context Claimed benefits of software modelling and model driven techniques are improvements in productivity, portability, maintainability and interoperability. However, little effort has been devoted at collecting evidence to evaluate their actual relevance, benefits and usage complications. Goal The main goals of this paper are: (1) assess the diffusion and relevance of software modelling and MD techniques in the Italian industry, (2) understand the expected and achieved benefits, and (3) identify which problems limit/prevent their diffusion. Method We conducted an exploratory personal opinion survey with a sample of 155 Italian software professionals by means of a Web-based questionnaire on-line from February to April 2011. Results Software modelling and MD techniques are very relevant in the Italian industry. The adoption of simple modelling brings common benefits (better design support, documentation improvement, better maintenance, and higher software quality), while MD techniques make it easier to achieve: improved standardization, higher productivity, and platform independence. We identified problems, some hindering adoption (too much effort required and limited usefulness) others preventing it (lack of competencies and supporting tools). Conclusions The relevance represents an important objective motivation for researchers in this area. The relationship between techniques and attainable benefits represents an instrument for practitioners planning the adoption of such techniques. In addition the findings may provide hints for companies and universities. © 2013 Elsevier Inc.","Model driven techniques; Personal opinion survey; Software modelling"
"A data mining approach to discovering reliable sequential patterns","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880161310&doi=10.1016%2fj.jss.2013.03.105&partnerID=40&md5=eb439e731feb87d68ef46df70478af54","Sequential pattern mining is a data mining method for obtaining frequent sequential patterns in a sequential database. Conventional sequence data mining methods could be divided into two categories: Apriori-like methods and pattern growth methods. In a sequential pattern, probability of time between two adjacent events could provide valuable information for decision-makers. As far as we know, there has been no methodology developed to extract this probability in the sequential pattern mining process. We extend the PrefixSpan algorithm and propose a new sequential pattern mining approach: P-PrefixSpan. Besides minimum support-count constraint, this approach imposes minimum time-probability constraint, so that fewer but more reliable patterns will be obtained. P-PrefixSpan is compared with PrefixSpan in terms of number of patterns obtained and execution efficiency. Our experimental results show that P-PrefixSpan is an efficient and scalable method for sequential pattern mining. © 2013 Elsevier Inc. All rights reserved.","Data mining; Inter-arrival time probability; Sequential patterns"
"Towards innovation measurement in the software industry","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875262581&doi=10.1016%2fj.jss.2013.01.013&partnerID=40&md5=89fada765119075431cd1250a9cd8991","In today's highly competitive business environments with shortened product and technology life cycle, it is critical for software industry to continuously innovate. This goal can be achieved by developing a better understanding and control of the activities and determinants of innovation. Innovation measurement initiatives assess innovation capability, output and performance to help develop such an understanding. This study explores various aspects relevant to innovation measurement ranging from definitions, measurement frameworks and metrics that have been proposed in literature and used in practice. A systematic literature review followed by an online questionnaire and interviews with practitioners and academics were employed to identify a comprehensive definition of innovation that can be used in software industry. The metrics for the evaluation of determinants, inputs, outputs and performance were also aggregated and categorised. Based on these findings, a conceptual model of the key measurable elements of innovation was constructed from the findings of the systematic review. The model was further refined after feedback from academia and industry through interviews.© 2012 Elsevier Inc. All rights reserved.","Empirical study; Innovation; Measurement; Metrics; Systematic literature review"
"Cloud computing and its impact on mobile software development: Two roads diverged","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881477603&doi=10.1016%2fj.jss.2013.01.063&partnerID=40&md5=ad5271c0b0b2a0d8f02cf525eb976b2e","Today, both desktop and mobile software systems are built to leverage resources available on the World Wide Web. However, in recent years desktop and mobile software systems have evolved in different directions. On desktop computers, the most popular application for accessing content and applications on the Web is the web browser. In mobile devices, in contrast, the majority of web content is consumed via custom-built native web apps. This divergence will not continue indefinitely. In the 2010's we will witness a major battle between two types of technologies: native web apps and Open Web applications that run in a web browser. This ""Battle of the Decade"" will determine the future of the software industry for years to come. © 2013 Elsevier Inc. All rights reserved.","Cloud computing; Open Web; Web applications; Web programming; Web-based software development"
"AFChecker: Effective model checking for context-aware adaptive applications","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872683955&doi=10.1016%2fj.jss.2012.11.055&partnerID=40&md5=92f61adf4629be30e0793d65dd1c9068","Context-aware adaptive applications continually sense and adapt to their changing environments. A large body of such applications relies on user-configured adaptation rules to customize their behavior. We call them rule-based context-aware applications (or RBAs for short). Due to the complexity required for adequately modeling environmental dynamics, adaptation faults are common in these RBAs. One promising approach to detecting such faults is to build a state transition model for an RBA, and exhaustively explore the model's state space. However, it can suffer from numerous false positives. For example, 78.6% of 784 reported faults for one popular RBA-PhoneAdapter, turn out to be false in a real deployment. In this paper, we address this false positive problem by inferring a domain model and an environment model for an RBA. The two models capture the hidden features inside user-configured adaptation rules as well as the RBA's running environment. We formulate these features as deterministic constraints and probabilistic constraints to prune false positives and effectively prioritize remaining faults. Our experiments on two real RBAs report that this approach successfully removes 46.5% of false positives and ranks 86.2% of true positives to the top of the fault list. © 2012 Elsevier Inc.","Adaptation fault; Deterministic constraint; False positive; Fault ranking; Probabilistic constraint"
"Scheduling of scientific workflow in non-dedicated heterogeneous multicluster platform","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879950410&doi=10.1016%2fj.jss.2012.10.029&partnerID=40&md5=8437e98fd9056e2ad68d55e6dbba20c0","Many scientific workflows can be structured as Parallel Task Graphs (PTGs), that is, graphs of data-parallel tasks. Adding data parallelism to a workflow provides opportunities for higher performance and scalability. Workflow tasks are data-parallel and moldable, and clusters are not only heterogeneous but also non-dedicated for workflow execution. Therefore, scheduling such scientific workflow in a multicluster platform becomes a challenging task. To address this problem, we study the scheduling of scientific workflow in a non-dedicated heterogeneous multicluster platform aimed at minimizing the makespan for workflow execution. In this paper, three scheduling algorithms for effective workflow task mapping and resource allocation are proposed, among them MHEFT-RSV and MHEFT-RSV-BD are heuristic algorithms. An exact branch-and-cut scheduling algorithm is implemented, which exploits the intertask precedence and resource constraints thereby accelerating the process of obtaining a feasible schedule with minimized makespan. Detailed simulation experiments show that on average the exact branch-and-cut algorithm obtains shorter makespan for small and medium size workflows, while MHEFT-RSV and MHEFT-RSV-BD achieves better tradeoff between makespan and computation time for large scientific workflows. © 2012 Elsevier Inc. All rights reserved.","Branch-and-cut; Multicluster; Non-dedicated; Scheduling; Scientific workflow"
"Lifetime and QoS-aware energy-saving buffering schemes","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875275989&doi=10.1016%2fj.jss.2013.01.014&partnerID=40&md5=caf82dda4d7180565d2a057bef4dd337","The heterogeneous drive (HDrive), which combines solid-state disk (SSD) and HDD, brings opportunity for energy-saving and has received extensive attention recently. This paper focuses on the file buffering schemes and adaptive disk power management (DPM) scheme for HDrive. As for the first issue, we propose a frequency-energy based replacement (FEBR) scheme based on an energy-cost model; as for the second issue, we present a sliding-window based adaptive DPM scheme by taking the HDD's lifetime into account. To make the trade-off among performance, HDD's lifetime and energy-saving, we contrive a QoS-aware DPM scheme. With extensive experiments on four real-world traces, we have evaluated the effectiveness of existing replacement schemes on energy-efficiency, performance, and HDD's lifetime and compare with our proposed schemes. The experimental results have demonstrated that energy-saving in HDrive is feasible and can reach as high as 60-80%, and that FBR and its variant FEBR, and GDS are the best ones among all those online schemes evaluated while FEBR has some advantage over FBR and GDS on the whole. The results have also revealed that our proposed adaptive sliding-window-based DPM scheme can effectively control the disk's lifetime and the QoS-aware DPM scheme works well in making tradeoffs among performance, HDD's lifetime and energy-saving.© 2012 Elsevier Inc. All rights reserved.","Buffering scheme; Energy conservation; Heterogeneous drive; QoS; Replacement policy"
"Constrained frequent pattern mining on univariate uncertain data","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872682586&doi=10.1016%2fj.jss.2012.11.020&partnerID=40&md5=956ede7c44b738b309c0a1f8e14951c6","In this paper, we propose a new algorithm called CUP-Miner (Constrained Univariate Uncertain Data Pattern Miner) for mining frequent patterns from univariate uncertain data under user-specified constraints. The discovered frequent patterns are called constrained frequent U2 patterns (where ""U2"" represents ""univariate uncertain""). In univariate uncertain data, each attribute in a transaction is associated with a quantitative interval and a probability density function. The CUP-Miner algorithm is implemented in two phases: In the first phase, a U2P-tree (Univariate Uncertain Pattern tree) is constructed by compressing the target database transactions into a compact tree structure. Then, in the second phase, the constrained frequent U2 pattern is enumerated by traversing the U2P-tree with different strategies that correspond to different types of constraints. The algorithm speeds up the mining process by exploiting five constraint properties: succinctness, anti-monotonicity, monotonicity, convertible anti-monotonicity, and convertible monotonicity. Our experimental results demonstrate that CUP-Miner outperforms the modified CAP algorithm, the modified FIC algorithm, the modified U2P-Miner algorithm, and the modified Apriori algorithm. © 2012 Elsevier Inc.","Constrained mining; CUP-Miner; Frequent pattern mining; Univariate uncertain data"
"A high capacity lossless data hiding scheme for JPEG images","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.083","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879918656&doi=10.1016%2fj.jss.2013.03.083&partnerID=40&md5=2569aa526376d184ebf722a35f575a87","In this paper, we propose a new high-capacity reversible data hiding method for JPEG-compressed images. This method is based on modifying the quantization table and quantized discrete cosine transformation (DCT) coefficients. Some elements of the quantization table are divided by an integer while the corresponding quantized DCT coefficients are multiplied by the same integer and added by an adjustment value to make space for embedding the data. By analyzing the effect of each single quantized DCT coefficient on the image quality, an embedding sequence is chosen in order to help control the increase of file size after hiding the data meanwhile the PSNR value between the original uncompressed image and stego JPEG image is high. Experimental results show that the proposed method achieves both high capacity and high image quality. © 2013 Elsevier Inc. All rights reserved.","Information hiding; Lossless steganography; Reversible data hiding"
"Controlling ERP consultants: Client and provider practices","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875257147&doi=10.1016%2fj.jss.2013.01.030&partnerID=40&md5=f408302eba26627bdfc276cf47001e2d","Hiring consultants to implement the multiple components of an ERP installation is a common practice for securing expertise not found in client organizations. Ensuring the consultants work to the benefit of their client is potentially problematic as the consultants must adopt the goals of the client, coordinate with stakeholders within the client organization, and coordinate with those installing other components of the ERP. Controls are mechanisms that keep consultants on track with the objectives of client organizations. Control forms vary depending on the nature of the activity and the levels of expertise across clients and consultants. Just what forms of control are typically employed over consultants to promote the likelihood of a successful ERP implementation is not identified in the prior literature. Control theory is employed by this study to formulate an expectation of control, which is then examined through a multiple case study. Interviews of consultants, project managers in the client organizations, and project managers in the consulting firms confirm that client organizations employ performance controls on specified outcomes while the provider firms employ both outcome controls and behavioral controls to keep the ERP implementation project in line with client goals. © 2013 Elsevier Inc.","Consultants; ERP implementation; Project control"
"Graph-based reference table construction to facilitate entity matching","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876411593&doi=10.1016%2fj.jss.2013.02.026&partnerID=40&md5=19181c5041aac9cfe14d8c4c51b484e7","Entity matching plays a crucial role in information integration among heterogeneous data sources, and numerous solutions have been developed. Entity resolution based on reference table has the benefits of high efficiency and being easy to update. In such kind of methods, the reference table is important for effective entity matching. In this paper, we focus on the construction of effective reference table by relying on co-occurring relationship between tokens to identify suitable entity names. To achieve high efficiency and accuracy, we first model data set as graph, and then cluster the vertices in the graph in two stages. Based on the connectivity between vertices, we also mine synonyms and get the expansive reference table. We develop an iterative system and conduct an experimental study using real data. Experimental results show that the method in this paper achieves both high accuracy and efficiency. © 2013 Elsevier Inc. © 2013 Elsevier Inc. All rights reserved.","Entity matching; Graph clustering; Reference table"
"Incremental service level agreements violation handling with time impact analysis","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876407550&doi=10.1016%2fj.jss.2013.01.052&partnerID=40&md5=feca54d42bdda16ded307ff8404c7205","This research addresses a critical issue of service level agreement (SLA) violation handling, i.e.; time constraint violation related to service-based systems (SBS). Whenever an SLA violation occurs to a service, it can potentially impact dependent services, leading to unreliable SBS. Therefore, an SLA violation handling support is much required to produce a robust and adaptive SBS. There are several approaches to realizing exceptions and faults handling support for SBS, focusing on the detection stage, the analysis stage, and the resolution stage. However, the current works have not considered the handling strategy that takes the impact information into account to reduce the amount of change. This is essential to effectively handle the violation while consuming a reasonable recovery execution time. Therefore, in this research, we propose an incremental SLA violation handling with time impact analysis. The main role of the time impact analysis in the approach is to automatically generate an impact region based on the negative time impact conditions. Furthermore, the time impact analysis generates the appropriate time requirements. Both the region and the requirement are useful to support the recovery process. Based on a simplified evaluation study, the outcome suggests that the proposed approach can reduce the amount of service change within a reasonable recovery execution time. © 2013 Elsevier Inc. All rights reserved.","Impact analysis; Recovery; Service level agreements; Service-based systems; Time constraints; Violation handling"
"A high performance peer to cloud and peer model augmented with hierarchical secure communications","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879929787&doi=10.1016%2fj.jss.2012.08.062&partnerID=40&md5=4763974dc6fceb1e7fdbc7ca7a3858bb","This paper presents a secure storage model named Peer to Cloud and Peer (P2CP). P2CP uses the cloud storage system as a backbone storage system. However, when data transmission occurs, the data nodes, cloud user, and the non-cloud user are involved to complete the transaction all together. The users, typically the client peers, can communicate with each other directly, thus bypassing servers on the cloud. Similarly, cloud servers can communicate with each other in a P2P mode. We also introduce a ""hierarchy security"" method to guarantee the data security in the proposed P2CP storage model. A key feature of our P2CP is that it has three data transmission tunnels: the cloud-user data transmission tunnel, the clients' data transmission tunnel, and the common data transmission tunnel. Assuming that the P2CP model follows the Poisson process or Little's law, we not only mathematically prove that the speed of P2CP is generally better than that of the pure peer-to-peer (P2P) model, the peer to server and peer (P2SP) model or the pure cloud model, but also testify the results through simulations. Beyond security, we also investigate the performance of another characteristic of usability of the data storage, namely availability, where P2CP is more robust to the failures of peers or servers in the cloud environment. Crown Copyright © 2012 Published by Elsevier Inc. All rights reserved.","Cloud; P2P; Performance; Security"
"SPAPE: A semantic-preserving amorphous procedure extraction method for near-miss clones","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880180325&doi=10.1016%2fj.jss.2013.03.061&partnerID=40&md5=743ced42d897899bdb6a362b635d267b","Cloned code, also known as duplicated code, is among the bad ""code smells"". Procedure extraction can be used to remove clones and to make a software system more maintainable. While the existing procedure extraction techniques can handle automatic extraction of exact clones effectively, they fail to do so for near-miss clones, which are the code fragments that are similar but not the same. To address this gap, we developed SPAPE, a novel semantic-preserving amorphous procedure extraction method to extract near-miss clones. SPAPE relaxes the constraint of having the same syntax and uses the structural semantic information. We evaluated the performance, effectiveness, and benefits of SPAPE. Our results show that SPAPE can extract more near-miss clones than the best applicable method for ten open-source-software products in an efficient and effective fashion. We conclude that SPAPE can be a useful contribution to the toolsets of software managers and developers, and it can help them improve code structure and reduce software maintenance and overall project costs. © 2013 Elsevier Inc. All rights reserved.","Amorphous procedure extraction; Near-miss clones; Refactoring"
"User acceptance of software as a service: Evidence from customers of China's leading e-commerce company, Alibaba","2013","Journal of Systems and Software","10.1016/j.jss.2013.03.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880151593&doi=10.1016%2fj.jss.2013.03.012&partnerID=40&md5=41704d48596587ad8dbf7a0df860b0d4","This paper proposes a model with which to analyze the user acceptance of Software as a Service (SaaS). To develop this model, empirical surveys were conducted through four rounds of questionnaires obtained from customers of China's leading e-commerce company, Alibaba. Firstly, based on the data from the first three rounds (1399 respondents), a SaaSQual of operationalizing perceived e-service quality of SaaS was developed, and its four dimensions (ease of use, security, reliability and responsiveness) were identified. Secondly, based on the data from the fourth round (1532 respondents), it was found that the level of three user perceptions (e-service quality, usefulness, and social influence) were predictive of the users' behavioral intention to use SaaS, and their direct and indirect influences were tested. This study recommends engineering improvements to SaaS based upon a better understanding of the level of user acceptance of this service. © 2013 Elsevier Inc. All rights reserved.","E-service quality; SaaSQual; Software as a Service (SaaS); User acceptance"
"PS-QUASAR: A publish/subscribe QoS aware middleware for Wireless Sensor and Actor Networks","2013","Journal of Systems and Software","10.1016/j.jss.2013.02.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876420107&doi=10.1016%2fj.jss.2013.02.028&partnerID=40&md5=d4f311d1a8bdc8846b102a222681399f","It has been more than 30 years since the first research into Wireless Sensor and Actor Networks appeared. However, WSANs are still not a ubiquitous technology due to several factors which include a lack of Quality of Service (QoS) support or the absence of high level programming models. New applications with heterogeneous QoS requirements where WSANs can be successfully applied, such as Critical Infrastructure Protection (CIP), have been recognized. PS-QUASAR, a middleware for WSANs that deals with these two issues, offers a high level simple programming model based on the publish/subscribe paradigm. In this model all nodes in the network are potential publishers of each of the topics. PS-QUASAR also handles QoS (reliability, deadline, priority) and supports a many-to-many exchange of messages between nodes in a fully distributed way by means of multicasting techniques. Performance evaluation via simulation using the Contiki operating system shows that the protocol can handle multiple publishers and subscribers at the same time whilst dealing with QoS requirements. © 2013 Elsevier Inc. All rights reserved.","Programming abstraction; Publish/subscribe; QoS-aware middleware; Wireless Sensor and Actor Networks"
"Building ubiquitous computing applications using the VERSAG adaptive agent framework","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871923283&doi=10.1016%2fj.jss.2012.09.026&partnerID=40&md5=8da9e1f1e84dac68ab8bd25eb5da8657","In this article, we describe a novel approach to build ubiquitous computing applications using adaptive software agents. Towards this, we propose VERSAG, a novel agent framework and architecture which combines agent mobility with the ability to dynamically change the internal structure and capabilities of agents and leads to highly versatile and lightweight software agents. We describe the framework in depth and provide design and implementation details of our prototype implementation. A case study scenario is used to illustrate the functional benefits achievable through the use of this framework in ubiquitous computing environments. Further experimental evaluation confirms the efficiency and feasibility of the VERSAG framework which outperforms traditional mobile agents, and also demonstrates applicability of the proposed framework to agent based systems where varying capabilities are required by agents over their lifecycle. © 2012 Elsevier Inc.","Agent architecture; Mobile agents; Ubiquitous computing"
"A variable-length model for masquerade detection","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865209137&doi=10.1016%2fj.jss.2012.05.049&partnerID=40&md5=298f9d0c82a8a28c0144855251f2357b","Masquerade detection is now one of the major concerns of system security research and its difficulty is to model user behavior on the nonstationary audit data. Many previous works represent the user behavior based on fixed-length models. In this paper, we propose a variable-length model to overcome their weakness in the precision and adaptability of user profiling. In the model, the user's normal behavior is profiled by Markov chain with states of variable-length sequences. At first multiple shell command streams of different lengths are generated and different shell command sequences are hierarchically merged into several sets to form the library of general sequences. Then the variable-length behavioral patterns of a valid user are mined and the Markov chain is constructed. While performing detection, the probabilities of short state sequences are calculated, smoothed with sliding windows, and finally used to classify the monitored user's activity as normal or abnormal. Our experiments with standard datasets such as Purdue data and SEA data reveal that the proposed model can achieve higher detection accuracy, require less memory and take shorter time than the other traditional methods and is amenable for real-time intrusion detection. © 2012 Elsevier Inc. All rights reserved.","Anomaly detection; Intrusion detection; Markov chain; Masquerade detection; Shell command"
"Improvement of trace-driven I-Cache timing attack on the RSA algorithm","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868662709&doi=10.1016%2fj.jss.2012.07.020&partnerID=40&md5=c75566fa0ef7be55869ce175c311fe1f","The previous I-Cache timing attacks on the RSA algorithm which exploit the instruction path of a cipher are mostly proof-of-concept, and it is harder to put them into practice than D-Cache timing attacks. We propose a trace-driven timing attack model on the RSA algorithm via spying on the whole I-Cache, instead of the partial instruction cache to which the multiplication function mapped, by analyzing the complications in the previous I-Cache timing attack on the RSA algorithm. Then, an improved analysis algorithm of the exponent using the characteristic of the window size in SWE algorithm is provided, which could further reduce the search space of the key bits than the former. We further demonstrate how to recover the private key d from the scattered known bits of d p and d q, through demonstrating some conclusions and validating it by experimentation. In addition, an error detection mechanism to detect some erroneous decisions of the operation sequences is provided to reduce the number of the erroneous recovered bits, and improve the precision of decision. We implement an I-Cache timing attack on RSA of OpenSSL in a practical environment, the experimental results show that the feasibility and effectiveness of I-Cache timing attack can be improved. © 2012 Elsevier Inc. All rights reserved.","Error detection; Instruction cache timing attacks; RSA cryptographic algorithm; Side-channel attacks; Trace-driven"
"Performance analysis of SCOOP programs","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865229549&doi=10.1016%2fj.jss.2012.05.076&partnerID=40&md5=439da6f717a283d2ed2d1ea96642f3a9","To support developers in writing reliable and efficient concurrent programs, novel concurrent programming abstractions have been proposed in recent years. Programming with such abstractions requires new analysis tools because the execution semantics often differs considerably from established models. We present a performance analyzer that is based on new metrics for programs written in SCOOP, an object-oriented programming model for concurrency. We discuss how the metrics can be used to discover performance issues, and we use the tool to optimize a concurrent robotic control software. © 2012 Elsevier Inc. All rights reserved.","Concurrent programming; Performance analysis; Performance metric; Profiling; SCOOP; Tracing"
"SMSCrypto: A lightweight cryptographic framework for secure SMS transmission","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872676720&doi=10.1016%2fj.jss.2012.11.004&partnerID=40&md5=ea538e44fed7b4e784a2e47da8ec4c82","Despite the continuous growth in the number of smartphones around the globe, Short Message Service (SMS) still remains as one of the most popular, cheap and accessible ways of exchanging text messages using mobile phones. Nevertheless, the lack of security in SMS prevents its wide usage in sensitive contexts such as banking and health-related applications. Aiming to tackle this issue, this paper presents SMSCrypto, a framework for securing SMS-based communications in mobile phones. SMSCrypto encloses a tailored selection of lightweight cryptographic algorithms and protocols, providing encryption, authentication and signature services. The proposed framework is implemented both in Java (target at JVM-enabled platforms) and in C (for constrained SIM Card processors) languages, thus being suitable for a wide range of scenarios. In addition, the signature model adopted does not require an on-line infrastructure and the inherent overhead found in the Public Key Infrastructure (PKI) model, facilitating the development of secure SMS-based applications. We evaluate the proposed framework on a real phone and on SIM Card-comparable microcontroller. © 2012 Elsevier Inc. All rights reserved.","Cryptography; Elliptic curve cryptosystems; Security; SMS"
"Side channel analysis attacks using AM demodulation on commercial smart cards with SEED","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867337073&doi=10.1016%2fj.jss.2012.06.063&partnerID=40&md5=5570e9f12fbddba8f2fb752968c06555","We investigate statistical side channel analysis attacks on the SEED block cipher implemented in two commercial smart cards used in a real-world electronic payment system. The first one is a contact-only card and the second one is a combination card. Both cards have no masking scheme at algorithm level and the combination card supports only hiding techniques in hardware level. Our results show that an unprotected implementation of SEED allows one to recover the secret key with low number of power or electromagnetic traces. Moreover, this paper clearly confirms that, although hiding countermeasures such as random current and random noise may increase the number of power traces needed for a successful attack, it is difficult to provide sufficient resistance to side channel attacks for itself. We believe that our results in this research will also be beneficial to the analysis and protection of other algorithms and commercial smart cards.© 2012 Elsevier Inc. All rights reserved.","AM demodulation; CEMA; Commercial smart card; Contactless smart card; CPA; DPA; SEED; Side channel attack"
"Layer assessment of object-oriented software: A metric facilitating white-box reuse","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871920727&doi=10.1016%2fj.jss.2012.08.041&partnerID=40&md5=4db2d6183cad575bca09041ec5267f43","Software reuse has the potential to shorten delivery times, improve quality and reduce development costs. However software reuse has been proven challenging for most organizations. The challenges involve both organizational and technical issues. In this work we concentrate on the technical issues and we propose a new metric facilitating the reuse of object-oriented software based on the popular Chidamber and Kemerer suite for object-oriented design. We derive this new metric using linear regression on a number of OSS java projects. We compare and contrast this new metric with three other metrics proposed in the literature. The purpose of the proposed metric is to assist a software developer during the development of a software system in achieving reusability of classes considered important for future reuse and also in providing assistance during re-architecting and componentization activities of existing systems. © 2012 Elsevier Inc. All rights reserved.","Object-oriented metrics; Software metrics; Software reuse"
"Grouping target paths for evolutionary generation of test data in parallel","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865233927&doi=10.1016%2fj.jss.2012.05.071&partnerID=40&md5=81324d36be394d523a27be7bae561874","Generating test data covering multiple paths using multi-population parallel genetic algorithms is a considerable important method. The premise on which the method above is efficient is appropriately grouping target paths. Effective methods of grouping target paths, however, have been absent up to date. The problem of grouping target paths for generation of test data covering multiple paths is investigated, and a novel method of grouping target paths is presented. In this method, target paths are divided into several groups according to calculation resources available and similarities among target paths, making a small difference in the number of target paths belonging to different groups, and a great similarity among target paths in the same group. After grouping these target paths, a mathematical model is built for parallel generation of test data covering multiple paths, and a multi-population genetic algorithm is adopted to solve the model above. The proposed method is applied to several benchmark or industrial programs, and compared with a previous method. The experimental results show that the proposed method can make full use of calculation resources on the premise of meeting the requirement of path coverage, improving the efficiency of generating test data. © 2012 Elsevier Inc. All rights reserved.","Multiple paths coverage; Software testing; Test data"
"HPobSAM for modeling and analyzing IT Ecosystems - Through a case study","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867331419&doi=10.1016%2fj.jss.2012.03.007&partnerID=40&md5=6bff1033bdf084f65ad28fee9e3fa48f","The next generation of software systems includes systems composed of a large number of distributed, decentralized, autonomous, interacting, cooperating, organically grown, heterogeneous, and continually evolving subsystems, which we call IT Ecosystems. Clearly, we need novel models and approaches to design and develop such systems which can tackle the long-term evolution and complexity problems. In this paper, our framework to model IT Ecosystems is a combination of centralized control (top-down) and self-organizing (bottom-up) approach. We use a flexible formal model, HPobSAM, that supports both behavioral and structural adaptation/evolution. We use a detailed, close to real-life, case study of a smart airport to show how we can use HPobSAM in modeling, analyzing and developing an IT Ecosystem. We provide an executable formal specification of the model in Maude, and use LTL model checking and bounded state space search provided by Maude to analyze the model. We develop a prototype of our case study designed by HPobSAM using Java and Ponder2. Due to the complexity of the model, we cannot check all properties at design time using Maude. We propose a new approach for run-time verification of our case study, and check different types of properties which we could not verify using model checking. As our model uses dynamic policies to control the behavior of systems which can be modified at runtime, it provides us a suitable capability to react to the property violation by modification of policies.© 2012 Elsevier Inc. All rights reserved.","Formal modeling; Large-scale software systems; Self-adaptive systems; Self-organizing systems; Verification"
"A reliability optimization method for RAID-structured storage systems based on active data migration","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871920721&doi=10.1016%2fj.jss.2012.09.023&partnerID=40&md5=a57852dd5c394d2f53a1924d62e9861e","The reliability of RAID system can utilize reconstruction operations to recover data when the disk fails; however, it will result in longer recovery time and negative impact on system performances. Moreover, the possibility of secondary failure will be increased during the recovery period. This paper proposed a new reliability optimization method for RAID system with Active data Migration called RAM, which allocates reserved space in every disk and arranges to redirect the data in the non-repairable sectors to the reserved space called Internal-disk Data Migration. When the number of the bad sectors in the disk exceeds threshold or the disk is in poor reliability, the system will copy data of the unreliable disk to a new one called External-disk Data Migration, this process can avoid lengthy data reconstruction. It can dynamically adjust migration speed to reduce the impact on the front end performance. When the I/O load of system is bursting, the I/O requests from user have high-priority. The overall results indicate that the RAM can improve reliability of the system with little influence. © 2012 Elsevier Inc.","Active Data Migration; External-disk Data Migration; Internal-disk Data Migration"
"An evaluation of multi-model self-managing control schemes for adaptive performance management of software systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867336547&doi=10.1016%2fj.jss.2012.05.077&partnerID=40&md5=316857639b6560c7ef3c3dfe0bce1941","Due to the increasing complexity of software systems and the dynamic unpredictable environments they operate in, methodologies to incorporate self-adaptation into these systems have been investigated in recent years. The feedback control loop has been one of the key concepts used in building self-adaptive software systems to manage their performance among other quality aspects. In order to design an effective feedback control loop for a software system, modeling the behavior of the software system with sufficient accuracy is paramount. In general, there are many environmental conditions and system states that impact on the performance of a software system. As a consequence, it is impractical to characterize the diverse behavior of such a software system using a single system model. To represent such highly nonlinear behavior and to provide effective runtime control, the design, integration and self-management (automatic switching) of multiple system models and controllers are required. In this paper, we investigate a control engineering approach, called Multi-Model Switching and Tuning (MMST) adaptive control, to assess its effectiveness for the adaptive performance management of software systems. We have conducted a range of experiments with two of the most promising MMST adaptive control schemes under different operating conditions of a representative software system. The experiment results have shown that the MMST control schemes are superior in managing the performance of the software system, compared with a number of other control schemes based on a single model. We have also investigated the impact of the configuration parameters for the MMST schemes to provide design guidance. A library of MMST schemes has been implemented to aid the software engineer in developing MMST-based self-managing control schemes for software systems.© 2012 Elsevier Inc. All rights reserved.","Adaptive control; Feedback control; Multi-model; Quality of service; Reconfiguring control; Self-managing systems"
"Constraint-based specification of model transformations","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871927010&doi=10.1016%2fj.jss.2012.09.006&partnerID=40&md5=8c65370c30bbb610c5f44396e6aa138c","Model transformations are a central element of model-driven development (MDD) approaches. The correctness, modularity and flexibility of model transformations is critical to their effective use in practical software development. In this paper we describe an approach for the automated derivation of correct-by-construction transformation implementations from high-level specifications. We illustrate this approach on a range of model transformation case studies of different kinds (re-expression, refinement, quality improvement and abstraction transformations) and describe ways in which transformations can be composed and evolved using this approach. © 2012 Elsevier Inc. All rights reserved.","Model transformations; Model-driven development; Software synthesis"
"A delay-constrained and priority-aware channel assignment algorithm for efficient multicast in wireless mesh networks","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872675465&doi=10.1016%2fj.jss.2012.11.017&partnerID=40&md5=7a0cdc4ab077eb1de479341f19eded11","Many popular applications of wireless mesh networks (WMNs) depend on delay-constraint multicast communication. To support such multicast communication, this paper proposes a distributed and polynomial-time heuristic channel assignment algorithm for WMNs. The proposed algorithm considers that the multicast session requests arrive dynamically and have different priorities. When a delay-constrained multicast session is issued, the multicast tree corresponding to the session is first established. The proposed algorithm divides the path delay constraint of the multicast tree into a number of the node-based delay constraints. This algorithm also devises multiple channel selection criteria to exploit all available channels of the WMN. Using these selection criteria, each node on the multicast tree can select the best channel to meet its node delay constraint and minimize the total interference for all existing multicast sessions. In the interference minimization, the priority factor is taken into account to prevent high-priority multicast sessions from incurring more interference than low-priority multicast sessions. Finally, this paper performs simulations to demonstrate the effectiveness of the proposed heuristic channel assignment algorithm through comparison with the optimal solution. © 2012 Elsevier Inc.","Channel assignment algorithm; Interference; Multicast; Priority; Wireless mesh networks"
"Improved multi-precision squaring for low-end RISC microcontrollers","2013","Journal of Systems and Software","10.1016/j.jss.2012.06.074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868641501&doi=10.1016%2fj.jss.2012.06.074&partnerID=40&md5=f586051b8fe1acb214eed95806f01a23","We present an enhanced multi-precision squaring algorithm for low-end RISC microcontrollers. Generally, they have many general-purpose registers and limited bus size (8-32 bits). The proposed scheme employs a new technique, ""lazy doubling"" with optimizing computing sequences; so, it is significantly faster than the previous algorithms. Mathematical analysis shows that the number of clocks required by the proposed algorithm is about 67 of those required by the carry-catcher squaring algorithm. To the best of our knowledge this is known to be the fastest squaring algorithm. Experimental results on the ATmega128 microprocessor show that our algorithm is about 1.5 times faster than the carry-catcher squaring algorithm in terms of the number of clocks required. As squaring is a key operation in public key cryptography, the proposed algorithm can contribute to lowering power consumption in secure WSNs (wireless sensor networks) or secure embedded systems. © 2012 Elsevier Inc. All rights reserved.","Low-end microprocessor; Multi-precision squaring; Public key cryptography; Security; Sensor networks"
"Empirical validation of a usability inspection method for model-driven Web development","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868660492&doi=10.1016%2fj.jss.2012.07.043&partnerID=40&md5=f12447c93750a10ce2c6a41fc53b4813","Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participants' effectiveness, efficiency, perceived ease of use and perceived satisfaction when using WUEP in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that WUEP is more effective and efficient than HE in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, WUEP has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes. © 2012 Elsevier Inc. All rights reserved.","Family of experiments; Model-driven development; Usability inspection; Web applications"
"Efficient Hamming weight-based side-channel cube attacks on PRESENT","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872680015&doi=10.1016%2fj.jss.2012.11.007&partnerID=40&md5=400368d6e3fa488583345ee63a581793","The side-channel cube attack (SCCA) is a powerful cryptanalysis technique that combines the side-channel and cube attack. This paper proposes several advanced techniques to improve the Hamming weight-based SCCA (HW-SCCA) on the block cipher PRESENT. The new techniques utilize non-linear equations and an iterative scheme to extract more information from leakage. The new attacks need only 28.95 chosen plaintexts to recover 72 key bits of PRESENT-80 and 29.78 chosen plaintexts to recover 121 key bits of PRESENT-128. To the best of our knowledge, these are the most efficient SCCAs on PRESENT-80/128. To show the feasibility of the proposed techniques, real attacks have been conducted on PRESENT on an 8-bit microcontroller, which are the first SCCAs on PRESENT on a real device. The proposed HW-SCCA can successfully break PRESENT implementations even if they have some countermeasures such as random delay and masking.","Hamming weight; Masking; PRESENT; Random delay; Side-channel cube attack"
"Self-control of the time complexity of a constraint satisfaction problem solver program","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867335563&doi=10.1016%2fj.jss.2012.05.060&partnerID=40&md5=35921e25d0973611ba70243d60ac495b","This paper presents the self-controlling software paradigm and reports on its use to control the branch and bound based constraint satisfaction problem solving algorithm. In this paradigm, an algorithm is first conceptualized as a dynamical system and then a feedback control loop is added to control its behavior. The loop includes a Quality of Service component that assesses the performance of the algorithm during its run time and a controller that adjusts the parameters of the algorithm in order to achieve the control goal. Although other approaches - generally termed as ""self-*"" - make use of control loops, this use is limited to the structure of the software system, rather than to its behavior and its dynamics. This paper advocates the analysis of dynamics of any program with control loops. The self-controlling software paradigm is evaluated on two different NP-hard constraint satisfaction and optimization problems. The results of the evaluation show an improvement in the performance due to the added control loop for both of the tested constraint satisfaction problems.© 2012 Elsevier Inc. All rights reserved.","Branch and bound algorithm; Constraint satisfaction problem; Fixture design problem; Job scheduling problem; PID controller; Self-controlling software"
"A development framework and methodology for self-adapting applications in ubiquitous computing environments","2012","Journal of Systems and Software","10.1016/j.jss.2012.07.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867336898&doi=10.1016%2fj.jss.2012.07.052&partnerID=40&md5=fb3db70a83ff970065aad8250919a4e6","Today software is the main enabler of many of the appliances and devices omnipresent in our daily life and important for our well being and work satisfaction. It is expected that the software works as intended, and that the software always and everywhere provides us with the best possible utility. This paper discusses the motivation, technical approach, and innovative results of the MUSIC project. MUSIC provides a comprehensive software development framework for applications that operate in ubiquitous and dynamic computing environments and adapt to context changes. Context is understood as any information about the user needs and operating environment which vary dynamically and have an impact on design choices. MUSIC supports several adaptation mechanisms and offers a model-driven application development approach supported by a sophisticated middleware that facilitates the dynamic and automatic adaptation of applications and services based on a clear separation of business logic, context awareness and adaptation concerns. The main contribution of this paper is a holistic, coherent presentation of the motivation, design, implementation, and evaluation of the MUSIC development framework and methodology. © 2012 Elsevier Inc. All rights reserved.","Adaptive software; Middleware; Mobile computing; Model-driven development; Ubiquitous computing"
"Nearest neighbor selection for iteratively kNN imputation","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865249371&doi=10.1016%2fj.jss.2012.05.073&partnerID=40&md5=d5f61aa8de2d3a74b240ddfd823fc658","Existing kNN imputation methods for dealing with missing data are designed according to Minkowski distance or its variants, and have been shown to be generally efficient for numerical variables (features, or attributes). To deal with heterogeneous (i.e., mixed-attributes) data, we propose a novel kNN (k nearest neighbor) imputation method to iteratively imputing missing data, named GkNN (gray kNN) imputation. GkNN selects k nearest neighbors for each missing datum via calculating the gray distance between the missing datum and all the training data rather than traditional distance metric methods, such as Euclidean distance. Such a distance metric can deal with both numerical and categorical attributes. For achieving the better effectiveness, GkNN regards all the imputed instances (i.e., the missing data been imputed) as observed data, which with complete instances (instances without missing values) together to iteratively impute other missing data. We experimentally evaluate the proposed approach, and demonstrate that the gray distance is much better than the Minkowski distance at both capturing the proximity relationship (or nearness) of two instances and dealing with mixed attributes. Moreover, experimental results also show that the GkNN algorithm is much more efficient than existent kNN imputation methods. © 2012 Elsevier Inc. All rights reserved.","K nearest neighbors; KNN imputation; Missing data"
"3E: Energy-efficient elastic scheduling for independent tasks in heterogeneous computing systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871927529&doi=10.1016%2fj.jss.2012.08.017&partnerID=40&md5=a7b213b101f66e01d9a3468a92198f4b","Reducing energy consumption is a major design constraint for modern heterogeneous computing systems to minimize electricity cost, improve system reliability and protect environment. Conventional energy-efficient scheduling strategies developed on these systems do not sufficiently exploit the system elasticity and adaptability for maximum energy savings, and do not simultaneously take account of user expected finish time. In this paper, we develop a novel scheduling strategy named energy-efficient elastic (3E) scheduling for aperiodic, independent and non-real-time tasks with user expected finish times on DVFS-enabled heterogeneous computing systems. The 3E strategy adjusts processors' supply voltages and frequencies according to the system workload, and makes trade-offs between energy consumption and user expected finish times. Compared with other energy-efficient strategies, 3E significantly improves the scheduling quality and effectively enhances the system elasticity. © 2012 Elsevier Inc. All rights reserved.","DVFS; Elastic; Energy-efficient; Heterogeneous computing system; Scheduling"
"RDOTE - Publishing relational databases into the semantic web","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868639634&doi=10.1016%2fj.jss.2012.07.018&partnerID=40&md5=db45e6b4ba93f24fc479c528a18b0ee9","A necessary step for the evolution of the traditional Web into a Semantic Web is the transformation of the vast quantities of data, currently residing in Relational Databases into semantically aware data. In addition, in cases where new ontology schemata are developed, considerable experimentation with real data for testing the consistency of classes, properties and entailment rules is required. During the last decade, there has been intense research and development in creating methodologies and tools able to map Relational Databases with the Resource Description Framework (RDF). Although some systems have gained wider acceptance in the Semantic Web community, they either require users to learn a declarative language for encoding mappings or, in case they support friendly user interfaces, they provide limited expressivity. Thereupon, we present RDOTE, a framework for easily transporting data residing in Relational Databases into the Semantic Web. RDOTE is available under GNU/GPL license and it provides friendly graphical user interfaces, as well as enough expressivity for creating automatic and custom RDF dumps of relational data. RDOTE is also compatible with D2RQ and R2RML mapping definitions. © 2012 Elsevier Inc. All rights reserved.","D2RQ; R2RML; RDB2RDF; RDF dump; Relational Databases to ontology transformation"
"Common carotid artery condition recognition technology using waveform features extracted from ultrasound spectrum images","2013","Journal of Systems and Software","10.1016/j.jss.2012.06.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868637215&doi=10.1016%2fj.jss.2012.06.046&partnerID=40&md5=f0323035d1e169dce3f90c1db7f44ac3","Medical image recognition algorithms have been widely applied to help with the diagnoses of various diseases, reducing human resource investment while enhancing diagnostic accuracy. This paper proposes a new scheme that specifies in the reading of ultrasound spectrum images of common carotid artery blood flow. The proposed scheme automatically extracts effective waveform features from the images for diagnostic purposes by using five criteria, which are ratio of waveform region, waveform region area target under horizontal baseline, waveform region area under horizontal baseline, highest point of waveform region, and lowest point of waveform region. Traditionally used by physicians to differentiate between normal blood flow patterns and five abnormal blood flow types, these five criteria are now employed by the new scheme to digitally diagnose vascular disorders at an accuracy rate as high as 0.97. © 2012 Elsevier Inc. All rights reserved.","Common carotid artery; Effective waveform feature; Image recognition; Ultrasound spectrum image"
"Efficient reversible data hiding algorithm based on gradient-based edge direction prediction","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871923384&doi=10.1016%2fj.jss.2012.09.041&partnerID=40&md5=8589b23415ba07a3578108ba51217a97","In this paper, we present an efficient RDH algorithm based on a new gradient-based edge direction prediction (GEDP) scheme. Since the proposed GEDP scheme can generate more accurate prediction results, the prediction errors tend to form a sharper Laplacian distribution. Therefore, the proposed algorithm can guarantee larger embedding capacity and produce better quality of marked images. The determination of appropriate thresholds is also a critical issue for a RDH algorithm, so we design a new systematic way to tackle this problem. In addition, a modified embedding order determination strategy is presented to reduce the distortion of a marked image. Based on typical test images, experimental results demonstrate the superior properties of the proposed algorithm in terms of embedding capacity and marked image quality. © 2012 Elsevier Inc. All rights reserved.","Difference expansion; Edge direction; Embedding capacity; Marked image quality; Prediction; Reversible data hiding"
"Promoting cooperation in service-oriented MAS through social plasticity and incentives","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871928035&doi=10.1016%2fj.jss.2012.09.031&partnerID=40&md5=0d07ecb2d965831d8924b49ebb57888e","In distributed environments where entities only have a partial view of the system, cooperation plays a key issue. In the case of decentralized service discovery in open service-oriented multi-agent systems, agents only know about the services they provide and their direct neighbors. Therefore, they need the cooperation of their neighbors in order to locate the required services. However, cooperation is not always present in open and distributed systems. Non-cooperative agents pursuing their own goals could refuse to forward queries from other agents to avoid the cost of this action; therefore, the efficiency of the decentralized service discovery could be seriously damaged. In this paper, we propose the combination of local structural changes and incentives in order to promote cooperation in the service discovery process. The results show that, even in scenarios where the predominant behavior is not collaborative the cooperation emerges. © 2012 Elsevier Inc. All rights reserved.","Complex networks; Cooperation; Incentives; Service discovery"
"From Teleo-Reactive specifications to architectural components: A model-driven approach","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865210429&doi=10.1016%2fj.jss.2012.05.067&partnerID=40&md5=8cdc4ef69790ec9c7b25b543da597df8","The Teleo-Reactive approach designed by N.J. Nilsson offers a high-level programming model that permits the development of reactive systems, such as robotic vehicles. Teleo-Reactive programs are written in a manner that allows engineers to define the behaviour of the system while taking into account goals and changes in the state of the environment. This article presents a systematic approach that makes it possible to derive architectural models, with structural descriptions and behaviour, from Teleo-Reactive Programs. The development of reactive systems can therefore benefit significantly from a combination of two approaches: (1) the Teleo-Reactive approach, which is oriented towards a description of the system from the standpoint of the goals identified and the state of the environment and (2) the architectural approach, which is oriented towards the design of component-based software, in which decisions are conditioned by the need to reuse already tested solutions. The integration of this work into a development environment that allows code to be generated via model transformations opens up new possibilities in the development of this type of systems. The proposal is validated through a case study that is representative of the domain, and a survey carried out with post-graduate students. © 2012 Elsevier Inc. All rights reserved.","Component-based software development; Model-driven software development; Reactive systems; Robotics; Teleo-Reactive programs"
"A content-aware bridging service for publish/subscribe environments","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868652968&doi=10.1016%2fj.jss.2012.07.033&partnerID=40&md5=3d8f65e506e0cc8e6d6fc682338aa85e","The OMG DDS (Data Distribution Service) standard specifies a middleware for distributing real-time data using a publish-subscribe data-centric approach. Until now, DDS systems have been restricted to a single and isolated DDS domain, normally deployed within a single multicast-enabled LAN. As systems grow larger, the need to interconnect different DDS domains arises. In this paper, we consider the problem of communicating disjoint data-spaces that may use different schemas to refer to similar information. In this regard, we propose a DDS interconnection service capable of bridging DDS domains as well as adapting between different data schemas. A key benefit of our approach is that is compliant with the latest OMG specifications, thus the proposed service does not require any modifications to DDS applications. The paper identifies the requirements for DDS data-spaces interconnection, presents an architecture that responds to those requirements, and concludes with experimental results gathered on our prototype implementation. We show that the impact of the service on the communications performance is well within the acceptable limits for most real-world uses of DDS (latency overhead is of the order of hundreds of microseconds). Reported results also indicate that our service interconnects remote data-spaces efficiently and reduces the network traffic almost N times, with N being the number of final data subscribers. © 2012 Elsevier Inc. All rights reserved.","Bridging; Data Distribution Service; Interoperability; Middleware; Network communications"
"Semantic ranking of web pages based on formal concept analysis","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868662404&doi=10.1016%2fj.jss.2012.07.040&partnerID=40&md5=a1ffa89a6cc945d801637822879a7452","A web crawler is an important research component in a search engine. In this paper, a new method for measuring the similarity of formal concept analysis (FCA) concepts and a new notion of a web page's rank are proposed that use an information content approach based on users' web logs. First, an extension similarity and an intension similarity that analyze a user's browsing pattern and their hyperlinks are proposed. Second, the information content similarity between two nouns is computed automatically by examining their ISA and Part-Of hierarchy and using a user's web log. A method for computing the semantic similarity between two concepts in two different concept lattices (the base concept lattice and the current concept lattice) and finding the semantic ranking of web pages is proposed. Last, our experiment demonstrates that our crawler is more suitable for crawling focused web pages. It proves that the semantic ranking of web pages is useful and efficient for making a web crawler's choice of a web page for continuing work. © 2012 Elsevier Inc. All rights reserved.","Crawling direction; Formal concept analysis; Search engine; Web crawler"
"From chaos to the systematic harmonization of multiple reference models: A harmonization framework applied in two case studies","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868648105&doi=10.1016%2fj.jss.2012.07.072&partnerID=40&md5=28d5e71e4746b69e3f911c4775dded18","At the present time, we can observe that in an effort to deal with the issue of quality, a variety of models, standards and methodologies have been developed to give support in different domains of the IT industry. This wide range of heterogeneous models makes it possible to resolve multiple needs. In recent years, as the integration of different models has increased, organizations have started to note that their business and technical processes can be aligned with more than one model. Currently, however, we are not aware of any other attempts to provide an explicit and systematic solution that would allow us to address the issue of harmonization of multiple reference models in such a way as to satisfy the needs of the companies. In the quest to help support the work of harmonization of multiple models, this paper presents (i) a framework that defines elements needed to support the harmonization of multiple reference models, (ii) a process, which is the backbone and way of integrating all the elements defined in the framework thus allowing the implementation of a harmonization project to be guided systematically, harmonizing multiple models through the configuration of a harmonization strategy, and (iii) a set of methods, which allows us to know ""what to do"", as well as ""how to put"" two or more models in consonance with each other. The experience of the application of our proposal is illustrated in two case studies. The findings obtained show that the harmonization process has enabled us to harmonize and put the models involved in consonance with each other. © 2012 Elsevier Inc. All rights reserved.","Case study; Harmonization; Harmonization of multiple models; Harmonization process; Improvement process; Multi-model; SPI"
"SF-PMIPv6: A secure fast handover mechanism for Proxy Mobile IPv6 networks","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871926929&doi=10.1016%2fj.jss.2012.09.015&partnerID=40&md5=b1472b79deb8645ddf13ed2a43baef5c","An efficient mobility management mechanism is one of the major challenges for ubiquitous computing. Recently, the IETF NETLMM working group proposed Proxy Mobile IPv6 (PMIPv6), a network-based localized mobility management protocol to support mobility management without the participation of mobile nodes (MNs) in any mobility-related signaling. Unfortunately, PMIPv6 still suffers from high packet losses and long authentication latency during handover. To address these issues, we propose a secure authentication mechanism and fast handover scheme called SF-PMIPv6 for PMIPv6 networks. The scheme provides low handover latency, supports local authentication procedures, resolves the packet loss problem, and deals with out-of-sequence packets. Moreover, SF-PMIPv6 is a robust authentication scheme that resists various attacks. Our simulation results demonstrate that it provides a better solution than existing schemes. © 2012 Elsevier Inc.","Fast handover; Local authentication; Out-of-sequence packet; Proxy Mobile IPv6 (PMIPv6)"
"A survey of software testing practices in Canada","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875265017&doi=10.1016%2fj.jss.2012.12.051&partnerID=40&md5=f4f6e01e3cf6ff84d6cf9d86aaa265cc","Software testing is an important activity in the software development life-cycle. In an earlier study in 2009, we reported the results of a regional survey of software testing practices among practitioners in the Canadian province of Alberta. To get a larger nationwide view on this topic (across Canada), we conducted a newer survey with a revised list of questions in 2010. Compared to our previous Alberta-wide survey (53 software practitioners), the nation-wide survey had larger number of participants (246 practitioners). We report the survey design, execution and results in this article. The survey results reveal important and interesting findings about software testing practices in Canada. Whenever possible, we also compare the results of this survey to other similar studies, such as the ones conducted in the US, Sweden and Australia, and also two previous Alberta-wide surveys, including our 2009 survey. The results of our survey will be of interest to testing professionals both in Canada and world-wide. It will also benefit researchers in observing the latest trends in software testing industry identifying the areas of strength and weakness, which would then hopefully encourage further industry-academia collaborations in this area. Among the findings are the followings: (1) the importance of testing-related training is increasing, (2) functional and unit testing are two common test types that receive the most attention and efforts spent on them, (3) usage of the mutation testing approach is getting attention among Canadian firms, (4) traditional Test-last Development (TLD) style is still dominating and a few companies are attempting the new development approaches such as Test-Driven Development (TDD), and Behavior-Driven Development (BDD), (5) in terms of the most popular test tools, NUnit and Web application testing tools overtook JUnit and IBM Rational tools, (6) most Canadian companies use a combination of two coverage metrics: decision (branch) and condition coverage, (7) number of passing user acceptance tests and number of defects found per day (week or month) are regarded as the most important quality assurance metrics and decision factors to release, (8) in most Canadian companies, testers are out-numbered by developers, with ratios ranging from 1:2 to 1:5, (9) the majority of Canadian firms spent less than 40% of their efforts (budget and time) on testing during development, and (10) more than 70% of respondents participated in online discussion forums related to testing on a regular basis. © 2012 Elsevier Inc.","Canada; Industry practices; Software testing; Survey"
"A simulation model for strategic management process of software projects","2013","Journal of Systems and Software","10.1016/j.jss.2012.06.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868665233&doi=10.1016%2fj.jss.2012.06.042&partnerID=40&md5=c2220d86b635477fe158b7bdc74002f3","In this study, a simulation model for the strategic management process of software development projects is presented. The proposed model simulates the implications of strategic decisions on factors such as cost, risk, budget and schedule of software projects. The main advantage of the proposed model is that it provides an integrated framework wherein risk management, cost estimation, and project management planning for the strategic management process of software development projects are linked. The results of the simulation of the project management planning determine the budget and schedule required for a project. Different strategic management decisions pose different sets of risks, each of which require different cost commitments. Hence, each strategic decision requires a project management plan with its own unique budget and schedule of software development. Thus, the simulation model estimates the risk and cost under different strategic decisions and maps them according to the project management plans. Therefore, the proposed integrated framework helps identify the best strategic option for the development and management of software projects. The proposed simulation model is nonspecific because it contains generic plug and play components that facilitate the use of any set of risk assessment, cost estimation models and project management tools. Therefore, it provides a flexible solution to software organisations and managers of software development projects. The simulation model is applied to a case study, which showed the effect of different strategic decisions on the risk and cost of the different phases of software development and ultimately on the budget and schedule required to complete the project. It therefore provides critical insights in identifying the best strategy for the development of software projects. © 2012 Elsevier Inc. All rights reserved.","Cost estimation; Decision analysis systems; Risk analysis; Simulation modelling; Strategic management"
"Refactoring legacy AJAX applications to improve the efficiency of the data exchange component","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868644948&doi=10.1016%2fj.jss.2012.07.019&partnerID=40&md5=ca401279b6c4df9727734868ea1af52c","The AJAX paradigm encodes data exchange XML formats. Recently, JSON has also become a popular data exchange format. XML has numerous benefits including human-readable structures and self-describing data. However, JSON provides significant performance gains over XML due to its light weight nature and native support for JavaScript. This is especially important for Rich Internet Applications (RIA). Therefore, it is necessary to change the data format from XML to JSON for efficiency purposes. This paper presents a refactoring system (XtoJ) to safely assist programmers migrate existing AJAX-based applications utilizing XML into functionally equivalent AJAX-based applications utilizing JSON. We empirically demonstrate that our transformation system significantly improves the efficiency of AJAX applications. © 2012 Elsevier Inc. All rights reserved.","AJAX; Efficiency; JavaScript; JSON; Refactoring; XML"
"Effective pattern-driven concurrency bug detection for operating systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871919561&doi=10.1016%2fj.jss.2012.08.063&partnerID=40&md5=de331cf7da66fb13121f52476249c842","As multi-core hardware has become more popular, concurrent programming is being more widely adopted in software. In particular, operating systems such as Linux utilize multi-threaded techniques heavily to enhance performance. However, current analysis techniques and tools for validating concurrent programs often fail to detect concurrency bugs in operating systems (OSes) due to the complex characteristics of OSes. To detect concurrency bugs in OSes in a practical manner, we have developed the COncurrency Bug dETector (COBET) framework based on composite bug patterns augmented with semantic conditions. The effectiveness, efficiency, and applicability of COBET were demonstrated by detecting 10 new bugs in file systems, device drivers, and network modules of Linux 2.6.30.4 as confirmed by the Linux maintainers. © 2012 Elsevier Inc.","Bug pattern; Concurrency bug; Linux; Static analysis"
"Collusion resilient spread spectrum watermarking in M-band wavelets using GA-fuzzy hybridization","2013","Journal of Systems and Software","10.1016/j.jss.2012.06.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868637369&doi=10.1016%2fj.jss.2012.06.057&partnerID=40&md5=fb2fcd11e9843450bee4b6b0d9e8ae62","This paper proposes a collusion resilient optimized spread spectrum (SS) image watermarking scheme using genetic algorithms (GA) and multiband (M-band) wavelets. M-band decomposition of the host image offers advantages of better scale-space tiling and good energy compactness. This bandpass-like decomposition makes watermarking robust against frequency selective fading-like gain (intelligent collusion) attack. On the other hand, GA would determine threshold value of the host coefficients (process gain i.e. the length of spreading code) selection for watermark casting along with the respective embedding strengths compatible to the gain of frequency response. First, a single bit watermark embedding algorithm is developed using independent and identically distributed (i.i.d) Gaussian watermark. This is further modified to design a high payload system for binary watermark image using a set of binary spreading code patterns. Watermark decoding performance is improved by multiple stage detection through cancelation of multiple bit interference (MBI) effect. Fuzzy logic is used to classify decision magnitudes in multiple group combined interference cancelation (MGCIC) used in the intermediate stage(s). Simulation results show convergence of GA and validate relative performance gain achieved in this algorithm compared to the existing works. © 2012 Elsevier Inc. All rights reserved.","Fuzzy logic; GA; High payload; M-band wavelets; Optimization; Spread spectrum watermarking"
"A covert communication method via spreadsheets by secret sharing with a self-authentication capability","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871928964&doi=10.1016%2fj.jss.2012.08.048&partnerID=40&md5=4552f277becdc5446b68ae881908fba0","A new covert communication method with a self-authentication capability for secret data hiding in spreadsheets using the information sharing technique is proposed. At the sender site, a secret message is transformed into shares by Shamir's (k, n)-threshold secret sharing scheme with n = k + 1, and the generated k + 1 shares are embedded into the number items in a spreadsheet as if they are part of the spreadsheet content. And at the receiver site, every k shares among the k + 1 ones then are extracted from the stego-spreadsheet to recover k + 1 copies of the secret, and the consistency of the k + 1 copies in value is checked to determine whether the embedded shares are intact or not, achieving a new type of blind self-authentication of the embedded secret. By dividing the secret message into segments and applying to each segment the secret sharing scheme, the integrity and fidelity of the hidden secret message can be verified, achieving a covert communication process with the double functions of information hiding and self-authentication. Experimental results and discussions on data embedding capacity, authentication precision, and steganalysis issues are also included to show the feasibility of the proposed method. © 2012 Elsevier Inc.","Covert communication; Information hiding; Secret sharing; Self-authentication; Spreadsheet"
"A posteriori operation detection in evolving software models","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871927997&doi=10.1016%2fj.jss.2012.09.037&partnerID=40&md5=990cbeb00ffc170c3d674143bddaafa8","As every software artifact, also software models are subject to continuous evolution. The operations applied between two successive versions of a model are crucial for understanding its evolution. Generic approaches for detecting operations a posteriori identify atomic operations, but neglect composite operations, such as refactorings, which leads to cluttered difference reports. To tackle this limitation, we present an orthogonal extension of existing atomic operation detection approaches for detecting also composite operations. Our approach searches for occurrences of composite operations within a set of detected atomic operations in a post-processing manner. One major benefit is the reuse of specifications available for executing composite operations also for detecting applications of them. We evaluate the accuracy of the approach in a real-world case study and investigate the scalability of our implementation in an experiment. © 2012 Elsevier Inc. All rights reserved.","Model comparison; Model evolution; Model refactoring"
"Histogram-shifting-imitated reversible data hiding","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871924367&doi=10.1016%2fj.jss.2012.08.029&partnerID=40&md5=6fba167dca01367e27fd0483c6363908","This paper proposes a novel reversible data hiding scheme based on the histogram-shifting-imitated approach. Instead of utilizing the peak point of an image histogram, the proposed scheme manipulates the peak points of segments based on image intensity. The secret data can be embedded into the cover image by changing the peak point pixel value into other pixel value in the same segment. The proposed method uses a location map to guarantee the correct extraction of the secret data. Since the modification of the pixel value is limited within each segment, the quality of the stego image is only related to the size of the segmentation, which means after embedding data into the cover image, it can be reused to do the multi-layer data embedding while maintaining the high quality of the final stego image. The experimental results of comparison with other existing schemes demonstrate the performance of the proposed scheme is superior to the others. © 2012 Elsevier Inc.","Image authentication; Lossless watermarking; Reversible data hiding; Steganography"
"Cross-layer end-to-end label switching protocol for WiMAX-MPLS heterogeneous networks","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865229435&doi=10.1016%2fj.jss.2012.05.050&partnerID=40&md5=3e227d5ff3ab716ff9bbdd47b4d58710","The integration of WiMAX networks and multi-protocol label switching (MPLS) networks, called WiMPLS networks, is the trend for nomadic Internet access in the fourth generation (4G) wireless networks. The base station (BS) in such heterogeneous networks will play the role of bridge and router between the IEEE 802.16 subscriber stations (SSs) and MPLS networks. However, there is no such integrated solution so far and the switching efficiency of the BS should be considered as well. This paper, therefore, adopts a cross-layer fashion (from network layer to MAC layer) to design the end-to-end label switching protocol (ELSP) for filling this gap. ELSP provides the mechanism of end-to-end (SS-to-SS) and layer 2 switching transfer for switching performance enhancement by assigning the SS with the MPLS labels (M-labels). The M-label can be carried by the IEEE 802.16e extended subheader within the MAC protocol data unit (MPDU), which is fully compliant with the IEEE 802.16e standard. The security issue caused by M-label usage is also concerned and solved in this paper. This paper also reveals an extra advantage that the switching delay of the BS achieved by ELSP can be as low as hardware-accelerated IP lookup mechanism, e.g., ternary content addressable memory (TCAM). Simulation results show that ELSP efficiently improves the end-to-end transfer delay as well as the throughput for WiMPLS heterogeneous networks. © 2012 Elsevier Inc. All rights reserved.","Cross-layer; Label switching; MPLS; Network; Protocol; WiMAX"
"A novel VQ-based reversible data hiding scheme by using hybrid encoding strategies","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871924002&doi=10.1016%2fj.jss.2012.09.001&partnerID=40&md5=18c9a3d42cc2512bce9503041e223c6e","Reversible data hiding is a special algorithm that not only guarantees the confidential data will be extracted accurately but also allows the original cover image to be reconstructed without distortion after the confidential data are completely extracted. This paper proposes a new index compression and reversible data hiding scheme based on side-match vector quantization (SMVQ) and search order coding (SOC). In this proposed scheme, the confidential data are embedded into the transformed index table of a cover image. During the extracting phase, simple steps are employed to extract the confidential data and reconstruct the original cover image. The experimental results show that with a codebook size of 256, the average compression rate of the proposed scheme is 0.325 bpp, which is superior to that of the methods proposed by Chen and Huang (0.426 bpp) and Chang et al. (0.429 bpp). Additionally, the embedding and extracting times of our scheme are 5.491 s and 0.352 s, respectively, demonstrating that the execution time of the proposed scheme is much faster than that of the methods of Chen and Huang and Chang et al. Moreover, our scheme achieves better performance than other selected reversible data hiding schemes with respect to the data embedding and data compression rates. © 2012 Elsevier Inc.","Image compression; Lossless compression; Reversible data hiding; SMVQ; SOC; VQ"
"Chaos-based detection of LDoS attacks","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868660357&doi=10.1016%2fj.jss.2012.07.065&partnerID=40&md5=d02fd71f8a9b8103dd0a9dd95d511c8b","A low-rate denial of service (LDoS) attack behaves as a small signal in periodic pulses with low average rate, which hides in normal TCP traffic stealthily. LDoS attacks reduce link throughput and degrade QoS of a target. An approach of detecting LDoS attacks is proposed based on Duffing oscillator in chaos systems. The approach detects LDoS attacks by adopting the technology of digital signal processing (DSP), which takes an LDoS attack as a small signal and normal TCP traffic as background noise. Duffing oscillator is used to detect LDoS attacks in normal TCP traffic. Simulations show that the LDoS attacks can be detected through diagram of the chaotic state, and the period and pulse width of LDoS attacks can be estimated. © 2012 Elsevier Inc. All rights reserved.","Chaos; Detection; Duffing oscillator; LDoS"
"Towards an early software estimation using log-linear regression and a multilayer perceptron model","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868662770&doi=10.1016%2fj.jss.2012.07.050&partnerID=40&md5=4d9202728fec01473f7608c4c9814c03","Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original UCP model. Furthermore, a comparison between the MLP and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the MLP model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects. © 2012 Elsevier Inc. All rights reserved.","Log-linear regression model; Multilayer perceptron; Software effort estimation; Use case points"
"Image sharing method for gray-level images","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871918762&doi=10.1016%2fj.jss.2012.09.040&partnerID=40&md5=dc0eb35f92285a4b8641e000b6bddab5","In 1994, Naor and Shamir firstly proposed the concept of visual secret sharing. By using a codebook to encode a binary image into sharing images, nobody can obtain the original information from any one of the shared images unless superimposing all shared images. Although the above method can protect the security of the binary image, pixel expansion and lossy recovery are two unsolved problem. To improve the disadvantages mentioned above, a new image sharing method is proposed in this paper. The proposed method firstly use linear equations of Hill cipher to divide an image into several sub-images. Then the concept of the random grid is applied to the sub-images and to construct the shared images. Experimental result shows that the proposed scheme can effectively improve the above drawbacks. © 2012 Elsevier Inc.","Distortion; Hill cipher; Pixel expansion; Random grid; Visual secret sharing"
"The effects of a shared free form rationale space in collaborative learning activities","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879938617&doi=10.1016%2fj.jss.2012.07.042&partnerID=40&md5=3365067ec9a1328b1b7396107b3178fa","We report the exploration of a technique for promoting reflective thinking in group learning activities in two field studies. In each study, we developed a collaborative tool that provided a dedicated virtual group space with no pre-structure. The students were required to articulate and share their rationales using the space, namely, the rationale space. The rationales in this program are defined as explanations of the reasons underlying one's decisions, conclusions, and interpretations. We discuss our findings on the design of the shared rationale space in a virtual group workspace and the effects of one's rationale awareness in the activities. Rationale awareness is one's awareness of the other group members' decision-making rationale of the shared tasks. Our study suggests that group members' rationale awareness facilitates the group's common ground. In addition, the use of a dedicated rationale space has the potential of benefiting the group process through supporting group members' rationale awareness and encouraging group members to develop practices around sharing thoughts and perspectives. © 2012 Elsevier Inc. All rights reserved.","Activity awareness; Collaborative learning; Rationale awareness"
"On using adversary simulators to evaluate global fixed-priority and FPZL scheduling of multiprocessors","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871925207&doi=10.1016%2fj.jss.2012.09.002&partnerID=40&md5=199494b42197cf9ed6039a58130c320e","There are many real-time systems where it is useful to have an estimate for the worst-case response time of each task. Simulators can be used to establish a lower bound on the worst-case response time. But classic simulators apply arrival patterns originally conceived for uniprocessor and fail to achieve a good estimate for the worst-case response time when multiprocessors are used. An adversary simulator generates arrival patterns to stress the processing capacity of the system and, in this way, to obtain tighter estimates. In this paper we present a new heuristic for adversary simulators specifically designed for fixed-priority zero-laxity (FPZL) scheduling. This new adversary algorithm is simple and fast, and it works with both deadline monotonic (DMPO) and deadline minus computation monotonic (DCMPO) priority assignment policies. The evaluation shows that the adversary simulator proposed in this paper is more effective when FPZL scheduling is used. We also compare four scheduling approaches (FP-DMPO, FP-DCMPO, FPZL-DMPO and FPZL-DCMPO) using an appropriate adversary simulator for each one. © 2012 Elsevier Inc. All rights reserved.","Fixed priority; FPZL; Real-time; Scheduling; Simulation"
"A semantic translation method for data communication protocols","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867337245&doi=10.1016%2fj.jss.2012.06.018&partnerID=40&md5=c2e93649821036dedc982bcefb138ae3","Protocol translation is a method for transforming pieces of information from a source protocol into relevant target protocol formats in order to communicate between heterogeneous legacy systems in interoperability environments. There are some existing protocol translation technologies for network protocols and simple data messages, but these have semantic information problems such as information distortion or incorrect information translations when applied to semantic messages. To deal with these problems, we propose a method that translates messages to semantically equivalent messages. We devised the method to be practical in implementing semantic information-sensitive gateway systems. We conducted a comparative experiment with the translation approach developed in the defense domain. We found that our semantic translation method provides more accurate and efficient message translations in interoperability environments. © 2012 Elsevier Inc. All rights reserved.","Gateway system; Heterogeneous system; Interoperability; Message translation; Semantic translation; Semantic-centric communication"
"A systematic review on the functional testing of semantic web services","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884152279&doi=10.1016%2fj.jss.2013.06.064&partnerID=40&md5=7f79cd002b64ee196cbdc2e0ffb8ff74","Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services. © 2013 Elsevier Inc.","Functional testing; Semantic web services; Systematic literature review; Testing approach"
"Analysing monitoring and switching problems for adaptive systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.07.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867336834&doi=10.1016%2fj.jss.2012.07.062&partnerID=40&md5=70c070d74c261481330098b2a87ed6c1","In the field of pervasive and ubiquitous computing, context-aware adaptive systems need to monitor changes in their environment in order to detect violations of requirements and switch their behaviour in order to continue satisfying requirements. In a complex and rapidly changing environment, identifying what to monitor and deciding when and how to switch behaviours effectively is difficult and error prone. The goal of our research is to provide systematic and, where possible, automated support for the software engineer developing such adaptive systems. In this paper, we investigate the necessary and sufficient conditions for both monitoring and switching in order to adapt the system behaviours as the problem context varies. Necessary and sufficient conditions provide complementary safeguards to ensure that not too much and not too little monitoring and switching are carried out. Our approach encodes monitoring and switching problems into propositional logic constraints in order for these conditions to be analysed automatically using a standard SAT solver. We demonstrate our approach by analysing a mobile phone system problem. We analysed requirements violations caused by changes in the system's operating environment. By providing necessary and sufficient monitoring and switching capabilities to the system, particular requirements violations were avoided.© 2012 Elsevier Inc. All rights reserved.","Monitoring; Problem description; Requirements engineering; Self-adaptive systems; Switching"
"Adam: Identifying defects in context-aware adaptation","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867329814&doi=10.1016%2fj.jss.2012.04.078&partnerID=40&md5=4313cbcdf3a410effed2ad989cb77a62","Context-aware applications, as a typical type of self-adaptive software systems, are receiving increasing attention. These applications continually adapt to environmental changes in an autonomic way. However, their adaptation may contain defects when the complexity of modeling all environmental changes is beyond a developer's ability. Such defects can cause failure to the adaptation and result in application crash or freezing. Relating these failures back to responsible defects is challenging. In this paper we propose a novel approach, called Adam, to assist identifying defects in the context-aware adaptation. Adam monitors runtime errors for an application, logs relevant error information, and relates them to responsible defects in this application. To make our Adam approach feasible, we investigate the error types that are commonly exhibited by various failures reported in context-aware applications. Adam detects these errors in order to identify responsible defects in context-aware applications. To detect these errors, Adam formally models the adaptation semantics for context-aware applications, and integrates into them a set of assertion checkers with respect to these error types. We experimentally evaluated Adam through three context-aware applications. The experiments reported promising results that Adam can effectively detect errors, identify their responsible defects in applications, and give useful hints on how these defects can be fixed. © 2012 Elsevier Inc. All rights reserved.","Context-aware adaptation; Defect; Error detection; Failure"
"On evaluating commercial Cloud services: A systematic review","2013","Journal of Systems and Software","10.1016/j.jss.2013.04.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881480488&doi=10.1016%2fj.jss.2013.04.021&partnerID=40&md5=dbd5dfaeda80f22927d3a3b69ca53c3b","AbstractBackground Cloud Computing is increasingly booming in industry with many competing providers and services. Accordingly, evaluation of commercial Cloud services is necessary. However, the existing evaluation studies are relatively chaotic. There exists tremendous confusion and gap between practices and theory about Cloud services evaluation. Aim To facilitate relieving the aforementioned chaos, this work aims to synthesize the existing evaluation implementations to outline the state-of-the-practice and also identify research opportunities in Cloud services evaluation. Method Based on a conceptual evaluation model comprising six steps, the systematic literature review (SLR) method was employed to collect relevant evidence to investigate the Cloud services evaluation step by step. Results This SLR identified 82 relevant evaluation studies. The overall data collected from these studies essentially depicts the current practical landscape of implementing Cloud services evaluation, and in turn can be reused to facilitate future evaluation work. Conclusions Evaluation of commercial Cloud services has become a world-wide research topic. Some of the findings of this SLR identify several research gaps in the area of Cloud services evaluation (e.g.; Elasticity and Security evaluation of commercial Cloud services could be a long-term challenge), while some other findings suggest the trend of applying commercial Cloud services (e.g.; compared with PaaS, IaaS seems more suitable for customers and is particularly important in industry). This SLR study itself also confirms some previous experiences and records new evidence-based software engineering (EBSE) lessons. © 2013 Elsevier Inc.","Cloud; Cloud service evaluation; Computing; Systematic literature review"
"Architecture-driven reliability optimization with uncertain model parameters","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863623802&doi=10.1016%2fj.jss.2012.04.056&partnerID=40&md5=6abaef5ac40018274fbb9973642c8b45","It is currently considered good software engineering practice to decide between design alternatives based on quantitative architecture evaluations for different quality attributes, such as reliability and performance. However, the results of these quantitative architecture evaluations are dependent on design-time estimates for a series of model-parameters, which may not be accurate and have to be estimated subject heterogeneous uncertain factors. As a result, sub-optimal design decisions may be taken. To overcome this problem, we present a novel robust optimization approach that deals with parameter uncertainties at the design phase of software-intensive systems. This work specifically focuses on architecture-based reliability evaluation models. The proposed approach is able to find good solutions that restrict the impact of parameter uncertainties, and thus provides better decision support. The accuracy and scalability of the presented approach is validated with an industrial case study and a series of experiments with generated examples in different problem sizes. © 2011 Elsevier Inc. All rights reserved.","Architecture optimization; Monte-Carlo simulation; Parameter uncertainty; Probabilistic quality prediction; Reliability"
"Invertible secret image sharing for gray level and dithered cover images","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871926196&doi=10.1016%2fj.jss.2012.09.027&partnerID=40&md5=ca852c0a58e71b33121aedf3900f9044","Secret image sharing approaches have been extended to obtain covers from stego images after the revealing procedure. Lin et al.'s work in 2009 uses modulus operator to decrease the share image distortion while providing recovery of original covers. Their work use gray level or binary image as cover. Stego images have approximately 43 dB and 38 dB PSNR for gray level and binary covers respectively. Lin et al.'s work in 2010 provides enhanced embedding capacity but does not support binary covers. Gray level covers' PSNR is reported approximately 40 dB. The proposed method enhances the visual quality of stego images regardless of intensity range of covers. Exploiting Modification Direction method is used to hide the shared values into covers. The method also utilizes modulus operator to recover original cover pixels. Stego image PSNR is approximately 47 dB for gray level covers. The method provides 4-7 dB increase respectively on the stego image quality compared to others. Stego images have also higher PSNR (43 dB) for dithered covers. The proposed method generates stego images with higher PSNR regardless of the intensity range of the cover image. © 2012 Elsevier Inc.","Exploiting Modification Direction; Invertible; Modulus operator; Secret sharing"
"Identification and application of Extract Class refactorings in object-oriented systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863626082&doi=10.1016%2fj.jss.2012.04.013&partnerID=40&md5=e061e5af41544aeec2c79230a2dc988e","Refactoring is recognized as an essential practice in the context of evolutionary and agile software development. Recognizing the importance of the practice, modern IDEs provide some support for low-level refactorings. A notable exception in the list of supported refactorings is the ""Extract Class"" refactoring, which is conceived to simplify large, complex, unwieldy and less cohesive classes. In this work, we describe a method and a tool, implemented as an Eclipse plugin, designed to fulfill exactly this need. Our method involves three steps: (a) recognition of Extract Class opportunities, (b) ranking of the identified opportunities in terms of the improvement each one is anticipated to bring about to the system design, and (c) fully automated application of the refactoring chosen by the developer. The first step relies on an agglomerative clustering algorithm, which identifies cohesive sets of class members within the system classes. The second step relies on the Entity Placement metric as a measure of design quality. Through a set of experiments we have shown that the tool is able to identify and extract new classes that developers recognize as ""coherent concepts"" and improve the design quality of the underlying system. © 2011 Elsevier Inc. All rights reserved.","Clustering; Object-oriented programming; Refactoring; Software reengineering"
"Securing color information of an image by concealing the color palette","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872680060&doi=10.1016%2fj.jss.2012.11.042&partnerID=40&md5=3ba90d64d0502a00332be1f7b962606a","This paper deals with a method to protect the color information of images by providing free access to the corresponding gray level images. Only with a secret key and the gray level images, it is then possible to view the images in color. The approach is based on a color reordering algorithm after a quantization step. Based on a layer scanning algorithm, the color reordering generates gray level images and makes it possible to embed the color palette into the gray level images using a data hiding algorithm. This work was carried out in the framework of a project aimed at providing limited access to the private digital painting database of the Louvre Museum in Paris, France. © 2012 Elsevier Inc. All rights reserved.","Color palette scanning; Color protection; Cultural heritage; Data hiding; Digital painting; Palette reordering"
"JCSI: A tool for checking secure information flow in Java Card applications","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865212584&doi=10.1016%2fj.jss.2012.05.061&partnerID=40&md5=aa6f6b963febdca9dfc8ee0264a4f41d","This paper describes a tool for checking secure information flow in Java Card applications. The tool performs a static analysis of Java Card CAP files and includes a CAP viewer. The analysis is based on the theory of abstract interpretation and on a multi-level security policy assignment. Actual values of variables are abstracted into security levels, and bytecode instructions are executed over an abstract domain. The tool can be used for discovering security issues due to explicit or implicit information flows and for checking security properties of Java Card applications downloaded from untrusted sources. © 2012 Elsevier Inc. All rights reserved.","Abstract interpretation; CAP file; Java bytecode; Java card; Secure information flow"
"Analytical architecture-based performability evaluation of real-time software systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868634558&doi=10.1016%2fj.jss.2012.08.014&partnerID=40&md5=dd830597ce52a231b9397962ff222d3f","Real-time systems are usually employed in dynamic and harsh environments. Real-time software, as one important part of such systems, potentially suffers from two problems: unpredictability in the timing behavior which affects the software performance, and logical faults which affect the software reliability. The former problem is mitigated by improving the software algorithm, architecture, and code. The latter problem is also relieved via software redundancy methods, even though these methods may adversely affect the software performance and architectural complexity. Despite these problems, it is expected to have a guaranteed service level in real-time systems, which the service is defined as the successful completion of the software mission within its deadline. In this paper, we propose two architecture-based analytical methods for simultaneous performance and reliability (performability) evaluation of real-time component-based software: one is accurate and the other is approximate. The accurate method is sound and precise but more complex in the computations, while the approximate method is easy-to-follow with reasonable amounts of computations. Examples of different configurations have been presented to show how well the latter method approximates the former one. Some performability sensitivity analyses with respect to the software component properties have also been done for better depiction of the importance of employing the proposed analytical methods in finding and eliminating the software performability bottlenecks. © 2012 Elsevier Inc. All rights reserved.","Analytical modeling; Component-based software; Fault-tolerance; Performability approximation; Performability evaluation; Real-time systems"
"COTS integration and estimation for ERP","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871924447&doi=10.1016%2fj.jss.2012.09.030&partnerID=40&md5=fcf260d7be2e1e566cf1bcba916bf3c8","This paper presents a comprehensive set of effort and schedule estimating models for predicting Enterprise Resource Planning (ERP) implementations, available in the open literature. The first set of models uses product size to predict ERP software engineering effort as well as total integration effort. Product size is measured in terms of the number of report, interface, conversion, and extension (RICE) objects configured and customized within the commercial ERP tool. Total integration effort captures software engineering plus systems engineering, program management, change management, development test & evaluation, and training development. The second set of models predicts the duration of ERP implementation stages in terms of RICE objects, staffing, and the number of test cases. The statistical models are based on data collected from 20 programs implemented within the federal government over the course of nine years beginning in 2000. The data was collected during the time period from 2006 to 2010. The models focus on the vendor's implementation team, and therefore should be applicable to commercial ERP implementations. Finally, ERP adopters/customers can use these models to validate Vendor's Implementation Team cost proposals or estimates. © 2012 Elsevier Inc.","Cost model; Effort estimation; Enterprise Resource Planning; Schedule estimation; Software engineering"
"Grindstone4Spam: An optimization toolkit for boosting e-mail classification","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867336742&doi=10.1016%2fj.jss.2012.06.027&partnerID=40&md5=1afa2296d40b98f25147f4cb93b61e6b","Resulting from the huge expansion of Internet usage, the problem of unsolicited commercial e-mail (UCE) has grown astronomically. Although a good number of successful content-based anti-spam filters are available, their current utilization in real scenarios is still a long way off. In this context, the SpamAssassin filter offers a rule-based framework that can be easily used as a powerful integration and deployment tool for the fast development of new anti-spam strategies. This paper presents Grindstone4Spam, a publicly available optimization toolkit for boosting SpamAssassin performance. Its applicability has been verified by comparing its results with those obtained by the default SpamAssassin software as well as four well-known anti-spam filtering techniques such as Naïve Bayes, Flexible Bayes, Adaboost and Support Vector Machines in two different case studies. The performance of the proposed alternative clearly outperforms existing approaches working in a cost-sensitive scenario. © 2012 Elsevier Inc. All rights reserved.","Content-based techniques; Genetic algorithm optimization; Naïve Bayes; Spam detection; SpamAssassin framework"
"Percolation-based routing in the Internet","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865260260&doi=10.1016%2fj.jss.2012.05.094&partnerID=40&md5=310f8fa36113971e392ec17983519553","The uncontrollable growth of the Internet, breaking through meshing and multi-homing practices the existing topology-based prefix aggregation mechanisms, creates the necessity of revisiting some fundamental aspects in the inter-domain routing model due to severe scalability issues in routing table size. In this paper, we at first analyze the root causes of these problems and then exploit a promising solution based on on-demand routing and on a widely known uniform caching and searching algorithm. Such algorithm is based on bond percolation, a mathematical phase transition model well-suited for random walk searches in power law networks, automatically shielding nodes with limited connectivity from large traffic volumes and reducing the total traffic to scale sub-linearly with the network size. The proposed solution introduces limited modifications to the BGP protocol, ensuring backward compatibility and allowing gradual deployment throughout the Internet. It dramatically reduces the routing table size requirements in all the nodes participating to the search network while allowing reliable and efficient on-demand discovery of unknown routing information, as demonstrated through extensive simulation experiments. © 2012 Elsevier Inc. All rights reserved.","Inter-domain routing; On-demand routing; Percolation search; Routing scalability"
"A RAMCloud Storage System based on HDFS: Architecture, implementation and evaluation","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872675700&doi=10.1016%2fj.jss.2012.11.025&partnerID=40&md5=384a0ed921dc764b5e483b404977bae7","Few cloud storage systems can handle random read accesses efficiently. In this paper, we present a RAMCloud Storage System, RCSS, to enable efficient random read accesses in cloud environments. Based on the Hadoop Distributed File System (HDFS), RCSS integrates the available memory resources in an HDFS cluster to form a cloud storage system, which backs up all data on HDFS-managed disks, and fetches data from disks into memory for handy accesses when files are opened for read or specified by users for memory storage. We extend the storage capacity of RCSS to that of the substrate disk-based HDFS by multiplexing all the available memory resources. Furthermore, RCSS supports MapReduce, which is a popular cloud computing paradigm. By serving data from memory instead of disks, RCSS can yield high random I/O performance with low latency and high throughput, and can achieve good availability and scalability as HDFS. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; Cloud Storage; HDFS; RAMCloud"
"Execution of natural language requirements using state machines synthesised from Behavior Trees","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865230441&doi=10.1016%2fj.jss.2012.06.013&partnerID=40&md5=a0245f53c392662bd0b444150f7a145c","This paper defines a transformation from Behavior Tree models to UML state machines. Behavior Trees are a graphical modelling notation for capturing and formalising dynamic system behaviour described in natural language requirements. But state machines are more widely used in software development, and are required for use with many tools, such as test case generators. Combining the two approaches provides a formal path from natural language requirements to an executable model of the system. This in turn facilitates requirements validation and transition to model-driven software development methods. The approach is demonstrated by defining a mapping from Behavior Trees to UML state machines using the ATLAS Transformation Language (ATL) in the Eclipse Modeling Framework. A security-alarm system case study is used to illustrate the use of Behavior Trees and execution to debug requirements. © 2012 Elsevier Inc. All rights reserved.","Behavior Engineering; Behavior Trees; MDE; Model Transformation; Requirements; Requirements Validation; UML State Machine"
"A zero-watermark scheme with geometrical invariants using SVM and PSO against geometrical attacks for image protection","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871922285&doi=10.1016%2fj.jss.2012.08.040&partnerID=40&md5=143bb6c1491d7fb88c5ea67729a71fc0","This paper proposes a zero-watermark scheme with geometrical invariants using support vector machine (SVM) classifier against geometrical attacks for image authentication. Here geometrical attacks merely address rotation, scale, and translation (RST) operations on images. The proposed scheme is called the SVM-based zero-watermark (SZW) scheme hereafter. The SZW method makes no changes to original images while embedding the owner signature of images so as to achieve high transparency. Moreover, in order to promote the robustness to RST operations, it integrates the discrete Fourier transform (DFT) with the log-polar mapping (LPM) for finding out RST invariants of images. The SZW method then generates the secret key for a host image via performing a logical operation exclusive disjunction, an exclusive-or (XOR) operation, on the original watermark and a set of the characteristics of the RST invariants of the host image. Subsequently, a trained SVM (TSVM) is regarded as a mapping so that it can memorize the relationships between the set of characteristics of RST invariants and the secret key. During the watermark-extraction process of the SZW method, the TSVM is first fed with the set of characteristics of RST invariants of the watermarked image to get the estimated secret key. The SZW method then extracts the estimated watermark by performing the XOR operation on the set of characteristics of RST invariants and the estimated secret key. Consequently, the SZW method requires no original image while retrieving watermarks. In the paper, the particle swarm optimization (PSO) algorithm is also employed to search for a set of nearly optimal parameters of the SVM. Finally, the experimental results show that, in average, the SZW method outperforms other existing methods against RST attacks under consideration here. © 2012 Elsevier Inc.","Digital watermarking; Geometrical attack; Particle swarm optimization; Support vector machine; Zero watermark"
"Specification and monitoring of data-centric temporal properties for service-based systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867331166&doi=10.1016%2fj.jss.2012.05.075&partnerID=40&md5=a0cf66070437fadf5234ae5014379054","Service-based systems operate in a very dynamic environment. To guarantee functional and non-functional objective at runtime, an adaptation mechanism is usually expected to monitor software changes, make appropriate decisions, and act accordingly. However, existing runtime monitoring solutions consider only the constraints on the sequence of messages exchanged between partner services and ignore the actual data contents inside the messages. As a result, it is difficult to monitor some dynamic properties such as how message data of interest is processed between different participants. To address this issue, we propose an efficient, non-intrusive online monitoring approach to dynamically analyze data-centric properties for service-oriented applications involving multiple participants. By introducing Par-BCL - a Parametric Behavior Constraint Language for Web services - to define monitoring parameters, various data-centric temporal behavior properties for Web services can be specified and monitored. This approach broadens the monitored patterns to include not only message exchange orders, but also data contents bound to the parameters. To reduce runtime overhead, we statically analyze the monitored properties and combine two different indexing mechanisms to optimize monitoring. The experiments show that our solution is efficient and promising. © 2012 Elsevier Inc. All rights reserved.","Runtime monitoring; Temporal property; Web services composition"
"Stitch: A language for architecture-based self-adaptation","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867334399&doi=10.1016%2fj.jss.2012.02.060&partnerID=40&md5=150d272cb3980e087628ac2537b749f8","Requirements for high availability in computing systems today demand that systems be self-adaptive to maintain expected qualities-of-service in the presence of system faults, variable environmental conditions, and changing user requirements. Autonomic computing tackles the challenge of automating tasks that humans would otherwise have to perform to achieve this goal. However, existing approaches to autonomic computing lack the ability to capture routine human repair tasks in a way that takes into account the business context humans use in selecting an appropriate form of adaptation, while dealing with timing delays and uncertainties in outcome of repair actions. In this article, we present Stitch, a language for representing repair strategies within the context of an architecture-based self-adaptation framework. Stitch supports the explicit representation of repair decision trees together with the ability to express business objectives, allowing a self-adaptive system to select a strategy that has optimal utility in a given context, even in the presence of potential timing delays and outcome uncertainty.© 2012 Elsevier Inc. All rights reserved.","Rainbow; Self-adaptation; Strategy; Tactic; Uncertainty; Utility"
"Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.079","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867333343&doi=10.1016%2fj.jss.2012.04.079&partnerID=40&md5=737a99c20b651c4957afd32bb66bb84d","Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. © 2012 Elsevier Inc. All rights reserved.","Earned business value; Feedback control theory; Goal-oriented reasoning; Preference; Self-tuning"
"The lean gap: A review of lean approaches to large-scale software systems development","2013","Journal of Systems and Software","10.1016/j.jss.2013.06.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884161764&doi=10.1016%2fj.jss.2013.06.035&partnerID=40&md5=76698d7ac3e9e8303a3a8768046e6ecc","Lean approaches to product development (LPD) have had a strong influence on many industries and in recent years there have been many proponents for lean in software development as it can support the increasing industry need of scaling agile software development. With it's roots in industrial manufacturing and, later, industrial product development, it would seem natural that LPD would adapt well to large-scale development projects of increasingly software-intensive products, such as in the automotive industry. However, it is not clear what kind of experience and results have been reported on the actual use of lean principles and practices in software development for such large-scale industrial contexts. This was the motivation for this study as the context was an ongoing industry process improvement project at Volvo Car Corporation and Volvo Truck Corporation. The objectives of this study are to identify and classify state of the art in large-scale software development influenced by LPD approaches and use this established knowledge to support industrial partners in decisions on a software process improvement (SPI) project, and to reveal research gaps and proposed extensions to LPD in relation to its well-known principles and practices. For locating relevant state of the art we conducted a systematic mapping study, and the industrial applicability and relevance of results and said extensions to LPD were further analyzed in the context of an actual, industrial case. A total of 10,230 papers were found in database searches, of which 38 papers were found relevant. Of these, only 42 percent clearly addressed large-scale development. Furthermore, a majority of papers (76 percent) were non-empirical and many lacked information about study design, context and/or limitations. Most of the identified results focused on eliminating waste and creating flow in the software development process, but there was a lack of results for other LPD principles and practices. Overall, it can be concluded that research in the much hyped field of lean software development is in its nascent state when it comes to large scale development. There is very little support available for practitioners who want to apply lean approaches for improving large-scale software development, especially when it comes to inter-departmental interactions during development. This paper explicitly maps the area, qualifies available research, and identifies gaps, as well as suggests extensions to lean principles relevant for large scale development of software intensive systems. © 2013 Elsevier Inc.","Agile software development; Automotive software development; Lean product development; Lean software development; Software engineering; Systematic mapping study"
"Improving Graph Cuts algorithm to transform sequence of stereo image to depth map","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868656590&doi=10.1016%2fj.jss.2012.07.044&partnerID=40&md5=8a2a14e62ba65fbd0b302182011d0a07","Recently, 3D display systems are getting considerable attentions not only from theater but also from home. 3D multimedia content development plays very important role in helping to setup a visual reality entertainment program. Lenticular Autostereoscopic display is one of the 3D-TV having the following advantages such as improving 3D viewing experience, supporting wider viewing angles for multiple viewers, and no requiring any special glasses. However, most of the current 3D movie and camera do not support the Autostereoscopic function. Therefore, we proposed a system that can transform the current 3D stereoscopic image sequence to the depth map sequence. These sequences can be warped into the multiplexed image by DIBR (Depth Image Based Rendering), and show with Autostereoscopic. Some recent techniques that transform the stereoscopic correspondence problem are based on Graph Cuts. They transform the matching problem to a minimization of a global energy function. However, it has been difficult to include high level information in the formulation of the Graph Cut. In this paper, we describe a new technique for generating depth map sequence from stereoscopic image sequence. We improve the Graph Cuts from pixel-based matching to region-based by using the Mean Shift 3D regions clustering to link the features of images before segmentation. And we also use the result of 3D regions clustering to assign depth values to time domain. After the sequence of depth map has been obtained, the DIBR method was used in transformation process. The experimental result shows that our system not only establishes a mechanism of depth transformation but also improves the accuracy and effectiveness on traditional Graph Cuts. © 2012 Elsevier Inc. All rights reserved.","Autostereoscopic; Depth map; DIBR (Depth Image Based Rendering); Graph Cuts; Mean Shift; Stereoscopic Image Sequence"
"Towards automated traceability maintenance","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863631251&doi=10.1016%2fj.jss.2011.10.023&partnerID=40&md5=849d0fb7feb203f1f910ec579206ae03","Traceability relations support stakeholders in understanding the dependencies between artifacts created during the development of a software system and thus enable many development-related tasks. To ensure that the anticipated benefits of these tasks can be realized, it is necessary to have an up-to-date set of traceability relations between the established artifacts. This goal requires the creation of traceability relations during the initial development process. Furthermore, the goal also requires the maintenance of traceability relations over time as the software system evolves in order to prevent their decay. In this paper, an approach is discussed that supports the (semi-) automated update of traceability relations between requirements, analysis and design models of software systems expressed in the UML. This is made possible by analyzing change events that have been captured while working within a third-party UML modeling tool. Within the captured flow of events, development activities comprised of several events are recognized. These are matched with predefined rules that direct the update of impacted traceability relations. The overall approach is supported by a prototype tool and empirical results on the effectiveness of tool-supported traceability maintenance are provided. © 2011 Elsevier Inc. All rights reserved.","Event-based development activity recognition; Model changes; Requirements traceability; Rule-based traceability maintenance; Software system evolution; Traceability decay; Traceability maintenance"
"Matthew effect, ABC analysis and project management of scale-free information systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871920257&doi=10.1016%2fj.jss.2012.08.013&partnerID=40&md5=6541831972a6887f7915dfdd34610288","The Matthew effect widely exists in natural and artificial systems. This paper studies the Matthew effect emerging from large information systems by investigating their topological complexity. The Matthew effect of the topological complexity reveals scale-free behavior that can be characterized by the general scale-free model [Physics Letters A 303 (2002) 337-344]. The poor-rich demarcation of the Matthew effect is analyzed to classify modules of information systems based on ABC analysis that is consistent with the ABC classification of human resources. Thus the Matthew effect is applied to envisage the critical fusion of high performance and low cost in software engineering. Finally, the Matthew effect of two typical open-source software systems written in Java and C/C++ languages is also investigated. © 2012 Elsevier Inc.","ABC analysis; Matthew effect; Poor-rich demarcation; Scale-free network"
"A robust blind color image watermarking in quaternion Fourier transform domain","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871921984&doi=10.1016%2fj.jss.2012.08.015&partnerID=40&md5=66bdf0f4b7abb2193e8520f4c087f845","Most of the existing color image watermarking schemes were designed to mark the image luminance component only, which have some disadvantages: (i) they are sensitive to color attacks because of ignoring the correlation between different color channels, (ii) they are always not robust to geometric distortions for neglecting the watermark desynchronization. It is a challenging work to design a robust color image watermarking scheme. Based on quaternion Fourier transform and least squares support vector machine (LS-SVM), we propose a robust blind color image watermarking in quaternion Fourier transform domain, which has good visual quality. Firstly, the original color image is divided into color image blocks. Then, the fast quaternion Fourier transform is performed on the color image block. Finally, the digital watermark is embedded into original color image by adaptively modulating the real quaternion Fourier transform coefficients of color image block. For watermark decoding, the LS-SVM correction with pseudo-Zernike moments is utilized. Experimental results show that the proposed color image watermarking is not only robust against common image processing operations such as filtering, JPEG compression, histogram equalization, and image blurring, but also robust against the geometrical distortions. © 2012 Elsevier Inc.","Color attack; Color image watermarking; Geometric distortion; Least squares support vector machine; Quaternion Fourier transform"
"Strong fuzzy c-means in medical image data analysis","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865210972&doi=10.1016%2fj.jss.2011.12.020&partnerID=40&md5=04dffc0a78ef86e34c3ecd05543195d6","This paper presents a robust fuzzy c-means (FCM) for an automatic effective segmentation of breast and brain magnetic resonance images (MRI). This paper obtains novel objective functions for proposed robust fuzzy c-means by replacing original Euclidean distance with properties of kernel function on feature space and using Tsallis entropy. By minimizing the proposed effective objective functions, this paper gets membership partition matrices and equations for successive prototypes. In order to reduce the computational complexity and running time, center initialization algorithm is introduced for initializing the initial cluster center. The initial experimental works have done on synthetic image and benchmark dataset to investigate the effectiveness of proposed, and then the proposed method has been implemented to differentiate the different region of real breast and brain magnetic resonance images. In order to identify the validity of proposed fuzzy c-means methods, segmentation accuracy is computed by using silhouette method. The experimental results show that the proposed method is more capable in segmentation of medical images than existed methods. © 2011 Elsevier Inc. All rights reserved.","Center knowledge; Clustering; Fuzzy c-means; Image segmentation; Kernel function; MR imaging"
"An endurance solution for solid state drives with cache","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865214706&doi=10.1016%2fj.jss.2012.05.072&partnerID=40&md5=8c18e95c1fe9c583b5b80ebf967e2f77","NAND flash memory has become one of the most popular storage media for portable devices, such as MP3 players, MMC cards and solid state drives (SSDs). Due to erase-before-write characteristics of NAND flash memory, wear-leveling strategy is very important in determining the performance and lifetime of NAND flash memory in solid state drives. In this paper, to prolong the lifetime and improve the performance of SSDs with cache, we propose an effective wear-leveling algorithm based on a novel ""triple-pool"" design. Comparing with previous wear-leveling algorithms, experimental results show that our algorithm lengthens the lifetime and reduces the write amplification. © 2012 Elsevier Inc. All rights reserved.","Algorithm; Solid-state drives; Triple-pool; Wear leveling"
"A mapping study to investigate component-based software system metrics","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872676227&doi=10.1016%2fj.jss.2012.10.001&partnerID=40&md5=231182a8ef6c90d93b9416e72d49c7b4","A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future. © 2012 Elsevier Inc.","Component-based software system; Software components; Software metrics; Software quality; Systematic mapping study"
"AIOLOS: Middleware for improving mobile application performance through cyber foraging","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865204463&doi=10.1016%2fj.jss.2012.06.011&partnerID=40&md5=dee2b4b554997ad0987b734a2861603d","As the popularity of smartphones and tablets increases, the mobile platform is becoming a very important target for application developers. Despite recent advances in mobile hardware, most mobile devices fail to execute complex multimedia applications (such as image processing) with an acceptable level of user experience. Cyber foraging is a well-known computing technique to enhance the capabilities of mobile devices, where the mobile device offloads parts of the application to a nearby discovered server in the network. Although first introduced in 2001, cyber foraging is still not widely adopted in current smartphone platforms or applications. In this respect, two major challenges are to be tackled. First, a suitable adaptive decision engine is needed to determine the optimal offloading decision, that takes into account the potentially high and variable latency between the device and the server. Second, an integrated cyber foraging platform with sufficient support for application developers is not publicly available on popular mobile platforms such as Android. In this paper, we present AIOLOS, a mobile middleware framework for cyber foraging on the Android platform. AIOLOS uses an estimation model that takes into account server resources and network state to decide at runtime whether or not a method call should be offloaded. We also introduce developer tools to integrate the AIOLOS framework in the Android platform, enabling easy development of cyber foraging enabled applications. A prototype implementation is presented and evaluated in detail by means of both a chess application and a newly developed photo editor application. © 2012 Elsevier Inc. All rights reserved.","Cyber foraging; Distributed systems; Mobile computing"
"Design of component-based real-time applications","2013","Journal of Systems and Software","10.1016/j.jss.2012.09.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871925484&doi=10.1016%2fj.jss.2012.09.036&partnerID=40&md5=33cc56c40828a5365585de843f47123d","This paper presents the key aspects of a model-based methodology that is proposed for the design of component-based applications with hard real-time requirements. The methodology relies on RT-CCM (Real-time Container Component Model), a component technology aimed to make the timing behaviour of the applications predictable and inspired in the Lightweight CCM specification of the OMG. Some new mechanisms have been introduced in the underlying framework that make it possible to schedule the execution of code and the transmission of messages of an application while guaranteeing that the application will meet its timing requirements when executed. The added mechanisms also enable the application designer to configure this scheduling without interfering with the opacity typically required in component management. Moreover, the methodology includes a process for generating the real-time model of a component-based application as a composition of the reusable real-time models of the components that form it. From the analysis of this model the application designer obtains the configuration values that must be applied to the component instances and the elements of the framework in order to make the application fulfil its timing requirements. © 2012 Elsevier Inc.","Component-based applications; Reactive model; Real-time systems; Schedulability; Software component; Software reusability"
"A high performance inter-domain communication approach for virtual machines","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871924431&doi=10.1016%2fj.jss.2012.08.054&partnerID=40&md5=51fe5d924dd4008e37239e188bfcde66","In virtualization technology field, researches mainly focus on strengthening the isolation barrier between virtual machines (VMs) that are co-resident within a single physical machine. At the same time, there are many kinds of distributed communication-intensive applications such as web services, transaction processing, graphics rendering and high performance grid applications, which need to communicate with other virtual machines at the same platform. Unfortunately, current inter-VM communication method cannot adequately satisfy the requirement of such applications. In this paper, we present the design and implementation of a high performance inter-VM communication method called IVCOM based on Xen virtual machine environment. In para-virtualization, IVCOM achieves high performance by bypassing some protocol stacks and privileged domain, shunning page flipping and providing a direct and high-performance communication path between VMs residing in the same physical machine. But in full-virtualization, IVCOM applies a direct communication channel between domain 0 and Hardware Virtualization based VM (HV2M) and can greatly reduce the VM entry/exit operations, which has improved the HV2M performance. In the evaluation of para-virtualization consisting of a few of benchmarks, we observe that IVCOM can reduce the inter-VM round trip latency by 70% and increase throughput by up to 3 times, which prove the efficiency of IVCOM in para-virtualized environment. In the full-virtualized one, IVCOM can reduce 90% VMX transition operations in the communication between domain 0 and HV2M. © 2012 Elsevier Inc. All rights reserved.","Communication path; Inter-VM communication; Overhead; Virtualization"
"SQLIA detection and prevention approach for RFID systems","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872677398&doi=10.1016%2fj.jss.2012.11.022&partnerID=40&md5=a4d4da453c7f6a9388b4b772ee4886a4","While SQL injection attacks have been plaguing web application systems for years, the possibility of them affecting RFID systems was only identified very recently. However, very little work exists to mitigate this serious security threat to RFID-enabled enterprise systems. At the same time, the drop in RFID tag prices coupled with the increase in storage capacity of the tags have motivated users to store more and more data on the tags for ease of access. This in turn has increased the ability that attackers have of leveraging the tags to try and mount SQLIA based malware attacks on RFID systems thereby increasing the potential threat that RFID-enabled systems pose to the enterprise systems. In this paper, we propose a detection and prevention method from RFID tag-born SQLIA attacks. We have tested all possible types of dynamic queries that may be generated in RFID systems with all possible types of attacks that can be mounted on those systems. We present an analysis and evaluation of the proposed approach to demonstrate its effectiveness in mitigating SQLIA attack. © 2012 Elsevier Inc.","Detection and prevention; RFID; Security; SQL injection attack; Tag-born malware"
"Software ecosystems-A systematic literature review","2013","Journal of Systems and Software","10.1016/j.jss.2012.12.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875249659&doi=10.1016%2fj.jss.2012.12.026&partnerID=40&md5=cd4f47b4b69647ad1219340a300c45b0","A software ecosystem is the interaction of a set of actors on top of a common technological platform that results in a number of software solutions or services. Arguably, software ecosystems are gaining importance with the advent of, e.g.; the Google Android, Apache, and Salesforce.com ecosystems. However, there exists no systematic overview of the research done on software ecosystems from a software engineering perspective. We performed a systematic literature review of software ecosystem research, analyzing 90 papers on the subject taken from a gross collection of 420. Our main conclusions are that while research on software ecosystems is increasing (a) there is little consensus on what constitutes a software ecosystem, (b) few analytical models of software ecosystems exist, and (c) little research is done in the context of real-world ecosystems. This work provides an overview of the field, while identifying areas for future research.© 2012 Elsevier Inc. All rights reserved.","Software ecosystem; Software ecosystems; Systematic literature review"
"Authentication of images for 3D cameras: Reversibly embedding information using intelligent approaches","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865264725&doi=10.1016%2fj.jss.2012.06.015&partnerID=40&md5=bbde7674e90c534412abf0e12c131a46","In this work, a reversible watermarking approach for authentication of 3D cameras based on computational intelligence is presented. Two intelligent techniques based on differential evolution (DE) and hybrid DE are employed to optimize the tradeoff between watermark imperceptibility and capacity. The proposed approach is suitable for images of 3D cameras. These cameras generally work on the concept of time-of-flight and not only produce the 2D image but also generate the corresponding depth map. In this approach, the depth map is considered as secret information and is hidden in the integer wavelet transform of the corresponding 2D image. The proposed technique is prospective for authenticating 3D camera images and allows the secure transmission of its depth map. It has the advantage of the lossless recovery of original 2D image as and when needed. The watermarking of the 2D images is based on integer wavelet transform and threshold optimization. The threshold map thus obtained using the intelligent optimization approaches is not only used for watermark embedding, but is also utilized for authentication purpose by correlating it with the corresponding 2D transformed image. Experiments conducted on images and depth maps obtained using 3D camera validate the proposed concept. © 2012 Elsevier Inc. All rights reserved.","3D camera; Depth maps; Differential evolution algorithm; Integer wavelet transform; Particle swarm optimization; Reversible watermarking"
"Masquerade attacks based on user's profile","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865274658&doi=10.1016%2fj.jss.2012.06.014&partnerID=40&md5=acf4f9b9fa1f7d29959fe3c19a1128e2","This paper presents a set of methods for building masquerade attacks. Each method takes into account the profile of the user to be impersonated, thus capturing an intruder strategy. Knowledge about user behavior is extracted from several statistics, including the frequency at which a user types a specific group of commands. It is then expressed by rules, which are applied to synthesize computer sessions that mimic the attack as ordinary user behavior. The masquerade attack datasets have been validated by making a set of Intrusion Detection Systems (IDS) try to detect user impersonation, this way showing the capabilities of each masquerade synthesis method for evading detection. Results demonstrate that a better performance of masquerade attacks can be obtained by using methods based on behavioral rules rather than those based only on a single statistic. Summing up, masquerade attacks exhibit a good strategy for bypassing an IDS. © 2012 Elsevier Inc. All rights reserved.","IDS systems; Masquerade attacks; User profiles"
"A compression-based text steganography method","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863627196&doi=10.1016%2fj.jss.2012.05.027&partnerID=40&md5=0f510853326bd44ce8d7a5d3ebdf27a2","In this study, capacity and security issues of text steganography have been considered to improve by proposing a novel approach. For this purpose, a text steganography method that employs data compression has been proposed. Because of using textual data in steganography, the employed data compression algorithm has to be lossless. Accordingly, LZW data compression algorithm has been chosen due to its frequent use in the literature and significant compression ratio. The proposed method constructs - uses stego keys and employs Combinatorics-based coding in order to increase security. Secret information has been hidden in the chosen text from the previously constructed text base that consists of naturally generated texts. Email has been chosen as communication channel between the two parties, so the stego cover has been arranged as a forward mail platform. By means of the proposed scheme, capacity has been reached to 7.042% for the secret message containing 300 characters (or 300·8 bits). Finally, comparison of the proposed scheme with the other contemporary methods in the literature has been carried out. Experimental results show that the proposed scheme provided a significant increment in terms of capacity. © 2012 Elsevier Inc.","Data compression; LZW algorithm; Steganography; Text steganography"
"Improving feature location using structural similarity and iterative graph mapping","2013","Journal of Systems and Software","10.1016/j.jss.2012.10.270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872679463&doi=10.1016%2fj.jss.2012.10.270&partnerID=40&md5=4a9989e60c39da0882745093613dec2b","Locating program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyse each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input. In this paper, we propose to address the above issues in feature location using an iterative context-aware approach. The underlying intuition is that features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: (1) it takes into account the structural similarity between a feature and a program element to determine feature-element relevance and (2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighbouring features and program elements. We evaluate our approach using two different systems, DirectBank, a small-scale industry financial system, and Linux kernel, a large-scale open-source operating system. Our evaluation suggests that the proposed approach is more robust and can significantly increase the recall of feature location with only a minor decrease of precision. © 2012 Elsevier Inc.","Feature location; Information retrieval; Structural similarity; Traceability recovery"
"Efficient support of dynamic inheritance for class- and prototype-based languages","2013","Journal of Systems and Software","10.1016/j.jss.2012.08.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871921279&doi=10.1016%2fj.jss.2012.08.016&partnerID=40&md5=494aa39a73ae2f2c3a7e7bc7fb26b9ff","Dynamically typed languages are becoming increasingly popular for different software development scenarios where runtime adaptability is important. Therefore, existing class-based platforms such as Java and.Net have been gradually incorporating dynamic features to support the execution of these languages. The implementations of dynamic languages on these platforms commonly generate an extra layer of software over the virtual machine, which reproduces the reflective prototype-based object model provided by most dynamic languages. Simulating this model frequently involves a runtime performance penalty, and makes the interoperation between class- and prototype-based languages difficult. Instead of simulating the reflective model of dynamic languages, our approach has been to extend the object-model of an efficient class-based virtual machine with prototype-based semantics, so that it can directly support both kinds of languages. Consequently, we obtain the runtime performance improvement of using the virtual machine JIT compiler, while a direct interoperation between languages compiled to our platform is also possible. In this paper, we formalize dynamic inheritance for both class- and prototype-based languages, and implement it as an extension of an efficient virtual machine that performs JIT compilation. We also present an extensive evaluation of the runtime performance and memory consumption of the programming language implementations that provide dynamic inheritance, including ours. © 2012 Elsevier Inc. All rights reserved.","Dynamic inheritance; JIT compilation; Prototype-based object-oriented model"
"The influence of SPI on business success in software SMEs: An empirical study","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863630115&doi=10.1016%2fj.jss.2012.05.024&partnerID=40&md5=63bac42c52a8194edb40c067a02b9074","In this paper, we present the findings of a study into the relationship between software process improvement (SPI) and business success in software development small- to medium-sized companies (software SMEs). A number of earlier related studies investigated the benefits of SPI in software SMEs, particularly in terms of improvements in product quality and adherence to budgetary and schedule constraints. However, only limited or indirect research has examined the relationship between SPI and business success. In this study, we adopt the Holistic Scorecard (HSC) (Sureshchandar and Leisten, 2005) as a business success reference framework, thus examining both the financial and the non-financial aspects of business success. In addition, we utilise ISO/IEC 12207 (ISO/IEC, 2008) as a comprehensive reference framework for the investigation of SPI activity in software SMEs. Through the use of new metrics introduced in this paper, the study findings establish that there is a positive association between SPI and business success in software SMEs, highlighting the importance of SPI in successful software SMEs. This is the first time that this relationship has been demonstrated using empirical data, and therefore, the findings represent a valuable new addition to the body of knowledge. © 2012 Elsevier Inc.","Business success; Software process improvement"
"BotMosaic: Collaborative network watermark for the detection of IRC-based botnets","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872680683&doi=10.1016%2fj.jss.2012.11.005&partnerID=40&md5=67590392c57516babf0e003ab89b1de0","Recent research has made great strides in the field of detecting botnets. However, botnets of all kinds continue to plague the Internet, as many ISPs and organizations do not deploy these techniques. We aim to mitigate this state by creating a very low-cost method of detecting infected bot host. Our approach is to leverage the botnet detection work carried out by some organizations to easily locate collaborating bots elsewhere. We created BotMosaic as a countermeasure to IRC-based botnets. BotMosaic relies on captured bot instances controlled by a watermarker, who inserts a particular pattern into their network traffic. This pattern can then be detected at a very low cost by client organizations and the watermark can be tuned to provide acceptable false-positive rates. A novel feature of the watermark is that it is inserted collaboratively into the flows of multiple captured bots at once, in order to ensure the signal is strong enough to be detected. BotMosaic can also be used to detect stepping stones and to help trace back to the botmaster. It is content agnostic and can operate on encrypted traffic. We evaluate BotMosaic using simulations and a testbed deployment. © 2012 Elsevier Inc. All rights reserved.","Botmaster traceback; Botnet detection; Network flow watermarking; Network security; Traffic analysis"
"Adaptive application offloading using distributed abstract class graphs in mobile environments","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867333778&doi=10.1016%2fj.jss.2012.05.091&partnerID=40&md5=d3f080fbab1e6a878d0a27112eea1b9b","Self-adaptation of software has been used as a mechanism to address complexity and constraint in mobile and pervasive computing environments. Adaptive offloading is a software adaptation mechanism in which an application dynamically distributes portions of itself to remote devices to achieve context specific optimizations. The feasibility of using adaptive offloading in pervasive environments is determined by the computational efficiency of adaptation algorithms and the efficacy of their decisions. However, existing state-of-the-art approaches incur overheads from storing, updating and partitioning complete application graphs on each device, which limits their utility and scalability in resource constrained mobile environments. Hence, this paper presents a novel distributed approach to application representation in which each device maintains a graph consisting only of components in its memory space, while maintaining abstraction elements for components in remote devices. This approach removes the need to store and update complete application graphs on each device and reduces the cost of partitioning an application during adaptation. In addition, an extension to an existing application graph partitioning heuristic is proposed to utilize this representation approach. An evaluation involving computationally heavy open-source applications adapting in a heterogeneous collaboration showed that the new approach reduced graph update network cost by 100%, collaboration-wide memory cost by between 37% and 50%, power usage by between 63% and 93%, and adaptation time by between 19.47% and 98%, while improving efficacy of adaptation by 12% and 34% for two of the considered applications. © 2012 Elsevier Inc. All rights reserved.","Adaptive offloading; Application partitioning; Class graph; Object mobility; Self-adaptive software systems"
"Achieving dynamic adaptation via management and interpretation of runtime models","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867331344&doi=10.1016%2fj.jss.2012.05.033&partnerID=40&md5=ebb1edcdfad69fa30401167d7699be51","In this article, we present a generic model-centric approach for realizing fine-grained dynamic adaptation in software systems by managing and interpreting graph-based models of software at runtime. We implemented this approach as the Graph-based Runtime Adaptation Framework (GRAF), which is particularly tailored to facilitate and simplify the process of evolving and adapting current software towards runtime adaptivity. As a proof of concept, we present case study results that show how to achieve runtime adaptivity with GRAF and sketch the framework's capabilities for facilitating the evolution of real-world applications towards self-adaptive software. The case studies also provide some details of the GRAF implementation and examine the usability and performance of the approach.© 2012 Elsevier Inc. All rights reserved.","Adaptation framework; Model transformation; Models at runtime; Runtime adaptivity; Self-adaptive software"
"A mixed-method approach for the empirical evaluation of the issue-based variability modeling","2013","Journal of Systems and Software","10.1016/j.jss.2013.01.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879943433&doi=10.1016%2fj.jss.2013.01.038&partnerID=40&md5=2fc1982c04931e26e6518b14ddf38d75","Background: Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling. Objective: We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability. Approach: We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution. Results: We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds. © 2013 Elsevier Inc. All rights reserved.","Empirical software engineering; Mixed-methods; Rationale management; Requirements engineering; Software product lines; Variability"
"LossEstimate: Distributed failure estimation in wireless networks","2012","Journal of Systems and Software","10.1016/j.jss.2012.07.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867337729&doi=10.1016%2fj.jss.2012.07.051&partnerID=40&md5=d711720de99d0d31d033cc269e984418","The ongoing evolution of software-intensive distributed systems to ultra-large-scale (ULS) systems require innovative methods for building, running, and managing these systems. Component self-adaptation and self-configuration properties are thus becoming mandatory requirements in order to cope with application complexity. An increasing number of systems, such as video content distribution, make use of distributed feed-back mechanisms to build-up intelligent, robust and self-managing services. Technology wise, with the wide-spread usage of wireless communication interfaces on today's mobile devices, communication failures are an ever increasing nuisance in the design of distributed self-adaptive services and applications. Communication protocols designed for wired networks are not suited for this new class of networks (including mobile ad-hoc networks, wireless sensor networks, vehicular ad-hoc networks, etc.) due to the several orders of magnitude higher amount of communication failures. Although virtually every single existing communication protocol tries to deal with the various effects introduced by communication failures, almost all existing state of the art relies on previous knowledge about the amount of errors occurring at run time (information usually collected from previous deployments). A survey of current literature easily shows that, in contrast, applications that make use of distributed feedback mechanisms via online estimation of communication errors has received relatively small attention. In this paper we introduce a new distributed feedback mechanism, named LossEstimate, for runtime quantification of the global amount of communication failures present in a large-scale network. The new algorithm helps building self-adaptive services and has the advantage of being fully distributed - each node computes an estimate of the amount of errors using a gossip-alike approach. The algorithm is adaptive in the sense that it can follow changes in the mean value of the amount of communication failures over time. We focus our analysis on the impact of various network topologies, discussing the case of fully connected networks (relevant for the case of peer-to-peer networks), static multihop topologies (mapping on the case of wireless sensor networks) and mobile multihop networks (mapping on the case of mobile ad-hoc networks and vehicular ad-hoc networks). The results show that the algorithm performs well in all three scenarios, without requiring specific adaptations. Besides the lack of an alternative protocol, the gossip-alike characteristics make LossEstimate an attractive choice for building a distributed feedback mechanism via the online quantification of the amount of communication failures in large-scale networks, due to the fact that it exhibits a small communication overhead and has a small convergence time. It stands as an important building-block for engineering self-adaptive distributed applications and services, such as video streaming, by means of distributed feedback mechanisms. © 2012 Elsevier Inc. All rights reserved.","Distributed algorithm; Distributed mobile networks; Online failure detection; Self-adaptive"
"Failure prediction based on log files using Random Indexing and Support Vector Machines","2013","Journal of Systems and Software","10.1016/j.jss.2012.06.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868647465&doi=10.1016%2fj.jss.2012.06.025&partnerID=40&md5=46e233aaf1dc902e2445302a0140929d","Research problem: The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures. Contribution: We describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs). Method: RI is applied to represent sequences: each operation is characterized in terms of its context. SVMs associate sequences to a class of failures or non-failures. Weighted SVMs are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company. Results: According to our results, weighted SVMs sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications. Conclusions: Overall, our approach is very reliable in predicting both failures and non-failures. © 2012 Elsevier Inc. All rights reserved.","Event sequence data; Failure prediction; Log files; Random Indexing; Support Vector Machine (SVM)"
"Ontology driven bee's foraging approach based self adaptive online recommendation system","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865228668&doi=10.1016%2fj.jss.2011.12.018&partnerID=40&md5=3a6d2f574b3561bcf0c9c21045b56c89","Online recommendation system is the modern software system used in all the e-commerce sites to capture the user intent and recommend the web pages that contain user expected information. The important challenges for such a system must include a need of being self-adaptive because the needs for online users may change dynamically. Classifier plays a very important role to improve the overall system accuracy. Here, we proposed the Ontology driven bee's foraging approach (ODBFA) that accurately classify the current user activity to any of the navigation profiles and predict the navigations that most likely to be visited by online users. Our proposed ODBFA method uses the Honey bee foraging behaviour in selecting the more profitable navigation profile for the current user activity. This approach makes the system self adaptive by capturing the changing needs of online user with the help of ontological framework comprising of ontology based similarity comparison and scoring algorithm. This approach effectively outperforms the other methods in achieving accurate classification and prediction of future navigation for the current online user. © 2011 Elsevier Inc. All rights reserved.","Bee's foraging; Online Recommendation system; Ontology; Self adaptive"
"QoS and energy management with Petri nets: A self-adaptive framework","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867333084&doi=10.1016%2fj.jss.2012.04.077&partnerID=40&md5=86ba76fab2d277adbb262a974fff5411","Energy use is becoming a key design consideration in computing infrastructures and services. In this paper we focus on service-based applications and we propose an adaptation framework that can be used to reduce power consumption according to the observed workload. The adaptation guarantees a trade-off between energy consumption and system performance. The approach is based on the principle of proportional energy consumption obtained by scaling down energy for unused resources, considering both the number of servers switched on and their operating frequencies. Stochastic Petri nets are proposed for the modeling of the framework concerns, their analyses give results about the trade-offs. The application of the approach to a simple case study shows its usefulness and practical applicability. Finally, different types of workloads are analyzed with validation purposes.© 2012 Elsevier Inc. All rights reserved.","Energy; Performance; Software architectures; Stochastic Petri nets"
"Improved results on impossible differential cryptanalysis of reduced-round Camellia-192/256","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865231392&doi=10.1016%2fj.jss.2012.05.051&partnerID=40&md5=4505ec3dc8e33121b6693b7b76c2a5ac","As an international standard adopted by ISO/IEC, the block cipher Camellia has been used in various cryptographic applications. In this paper, we reevaluate the security of Camellia against impossible differential cryptanalysis. Specifically, we propose several 7-round impossible differentials with the FL/FL -1 layer. Based on one of them, we mount impossible differential attacks on 11-round Camellia-192 and 12-round Camellia-256. The data complexities of our attacks on 11-round Camellia-192 and 12-round Camellia-256 are about 2 120 chosen plaintexts and 2 119.8 chosen plaintexts, respectively. The corresponding time complexities are approximately 2 167.1 11-round encryptions and 2 220.87 12-round encryptions. As far as we know, our attacks are 2 16.9 times and 2 19.13 times faster than the previously best known ones but have slightly more data. © 2012 Elsevier Inc. All rights reserved.","Block cipher; Camellia; Impossible differential cryptanalysis"
"ITravel: A recommender system in mobile peer-to-peer environment","2013","Journal of Systems and Software","10.1016/j.jss.2012.06.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868666418&doi=10.1016%2fj.jss.2012.06.041&partnerID=40&md5=fe73effbf8ee53fc7c655b8188baff7e","Recommender systems in mobile tourism have attracted considerable interest during the past decade. However, most existing recommender systems in mobile tourism fail to exploit information, evaluations or ratings provided by other tourists of similar interests. In this research, we propose to facilitate attraction recommendation task by exploring other tourists' ratings on their visited attractions. The proposed approach employs mobile peer-to-peer communications for exchanging ratings via their mobile devices. A cost-effective travel recommender system - iTravel - thus is developed to provide tourists with on-tour attraction recommendation. We propose three data exchange methods that allow users to effectively exchange their ratings toward visited attractions. Simulated experiments are performed to evaluate the proposed data exchange methods and a user study is conducted to validate the usability of the proposed iTravel system. © 2012 Elsevier Inc. All rights reserved.","Mobile commerce; Mobile peer-to-peer network; Recommender system; Travel information system"
"Interpretation problems related to the use of regression models to decide on economy of scale in software development","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865267962&doi=10.1016%2fj.jss.2012.05.068&partnerID=40&md5=400b458b458b5a87f4061726efa5a188","Many research studies report an economy of scale in software development, i.e., an increase in productivity with increasing project size. Several software practitioners seem, on the other hand, to believe in a diseconomy of scale, i.e., a decrease in productivity with increasing project size. In this paper we argue that violations of essential regression model assumptions in the research studies to a large extent may explain this disagreement. Particularly illustrating is the finding that the use of the production function (Size = a·Effort b), instead of the factor input model (Effort = a·Size b), would most likely have led to the opposite result, i.e., a tendency towards reporting diseconomy of scale in the research studies. We conclude that there are good reasons to warn against the use of regression analysis parameters to investigate economies of scale and to look for other analysis methods when studying economy of scale in software development contexts. © 2012 Elsevier Inc. All rights reserved.","Economy of scale; Regression analysis; Software economics"
"Software architecture evolution through evolvability analysis","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.085","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865263003&doi=10.1016%2fj.jss.2012.05.085&partnerID=40&md5=43dcee9a22ee1d2e7897a34d216d52c9","Software evolvability is a multifaceted quality attribute that describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for the efficient implementation of strategic decisions, and the increasing economic value of software. For long life systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. However, designing and evolving software architectures are the challenging task. To improve the ability to understand and systematically analyze the evolution of software system architectures, in this paper, we describe software architecture evolution characterization, and propose an architecture evolvability analysis process that provides replicable techniques for performing activities to aim at understanding and supporting software architecture evolution. The activities are embedded in: (i) the application of a software evolvability model; (ii) a structured qualitative method for analyzing evolvability at the architectural level; and (iii) a quantitative evolvability analysis method with explicit and quantitative treatment of stakeholders' evolvability concerns and the impact of potential architectural solutions on evolvability. The qualitative and quantitative assessments manifested in the evolvability analysis process have been applied in two large-scale industrial software systems at ABB and Ericsson, with experiences and reflections described. © 2012 Elsevier Inc. All rights reserved.","Software architecture evolution; Software evolvability"
"Managing data dependencies in service compositions","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865216574&doi=10.1016%2fj.jss.2012.05.092&partnerID=40&md5=406729a67083cb09e35768a77e0cd944","Composing services into service-based systems requires the design of coordination logic, which describes all service interactions realizing the composition. Coordination can be defined as the management of dependencies; in a services context we can discriminate between 'control flow' that manages sequence dependencies and 'data flow' for managing data dependencies. Current research fails to address the management of data dependencies in a systematic way and mostly treats it as subordinate to sequence dependencies. In this article a 'data flow' pattern language is presented that provides a systematic way of designing the data flow aspects of a coordination scenario, orthogonally to the way in which the control flow is designed. Starting from a set of fundamental and basic building blocks, each data dependency will yield a data flow design that takes a set of design criteria (e.g. loose coupling, data confidentiality, etc.) into account. The pattern language is evaluated in three ways. First, it is shown that every potential coordination scenario for managing a data dependency can be composed by the set of patterns. Second, the pattern language was applied in a real-life insurance case to show how it can guide the design of complex data flows. Third, the patterns were implemented in a tool that provides configurable model-to-code transformations for automatically generating BPEL coordination scenarios. In this tool both the data flow and control flow can be designed separately using different sets of patterns. © 2012 Elsevier Inc. All rights reserved.","Coordination logic; Data dependencies; Data flow; Guided design; Patterns; Service composition; Service coordination"
"A reversible data hiding method by histogram shifting in high quality medical images","2013","Journal of Systems and Software","10.1016/j.jss.2012.11.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872675370&doi=10.1016%2fj.jss.2012.11.024&partnerID=40&md5=02eeeb37fd2192889838f631da9c07af","Enormous demands for recognizing complicated anatomical structures in medical images have been demanded on high quality of medical image such as each pixel expressed by 16-bit depth. Now, most of data hiding algorithms are still applied in 8-bit depth medical images. We proposed a histogram shifting method for image reversible data hiding testing on high bit depth medical images. Among image local block pixels, we exploit the high correlation for smooth surface of anatomical structure in medical images. Thus, we apply a different value for each block of pixels to produce a difference histogram to embed secret bits. During data embedding stage, the image blocks are divided into two categories due to two corresponding embedding strategies. Via an inverse histogram shifting mechanism, the original image will be accurately recovered after the hidden data extraction. Due to requirements of medical images for data hiding, we proposed six criteria: (1) well-suited for high quality medical images, (2) without salt-and-pepper, (3) applicable to medical image with smooth surface, (4) well-suited sparse histogram of intensity levels, (5) free location map, (6) ability of adjusting data embedding capacity, PSNR and Inter-Slice PSNR. We proposed a data hiding methods satisfying above 6 criteria. © 2012 Elsevier Inc.","Criteria; Difference expansion; Histogram; Medical image; Reversible data embedding"
"Optimizing virtual machines using hybrid virtualization","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865253327&doi=10.1016%2fj.jss.2012.05.093&partnerID=40&md5=372796e08efd125a5cc260fe82d8b926","Minimizing virtualization overhead and improving the reliability of virtual machines are challenging when establishing virtual machine cluster. Paravirtualization and hardware-assisted virtualization are two mainstream solutions for modern system virtualization. Hardware-assisted virtualization is superior in CPU and memory virtualization and becoming the leading solution, yet paravirtualization is still valuable in some aspects as it is capable of shortening the disposal path of I/O virtualization. Thus we propose the hybrid virtualization which runs the paravirtualized guest in the hardware-assisted virtual machine container to take advantage of both. Experiment results indicate that our hybrid solution outweighs origin paravirtualization by nearly 30 in memory intensive test and 50 in microbenchmarks. Meanwhile, compared with the origin hardware-assisted virtual machine, hybrid guest owns over 16 improvement in I/O intensive workloads. © 2012 Elsevier Inc. All rights reserved.","Hardware-assisted virtualization; Hybrid virtualization; Paravirtualization"
"3 + 1 Challenges for the future of universities","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863633336&doi=10.1016%2fj.jss.2012.05.062&partnerID=40&md5=a704686a4a488b559a3220f3a8cde7d7","Universities are looking for effective strategies to cope with the global changes that have extended across the world in the past years. Existing approaches to research and education are increasingly perceived as unable or at least insufficient to capture and take into account the complexity and the dynamism of the globalized society. This is particularly true for the ICT sector, which has been radically transformed by technologies such as mobile devices, ubiquitous connectivity, and pervasive ICT. Indeed, as these technologies are inherently disruptive, they are profoundly impacting and transforming the economy and the entire society in general. This paper aims at discussing the issues and problems that universities are facing to deal with the growth and evolution of the ICT sector. In particular, the paper proposes 3 + 1 challenges they need to address and master. The challenges deal with three fundamental functions of modern universities: research, innovation, and education. Moreover, the paper proposes a fourth challenge related primarily to the attitude and behavior of faculty members and academic boards. The ultimate goal of the paper is to contribute to the development of an effective and useful debate about the strategies to support the evolution and growth of universities, as key players to promote the public good and the overall progress of our society. © 2012 Elsevier Inc.","Academic institutions; Education; Innovation; Research"
"A lossless copyright authentication scheme based on Bessel-Fourier moment and extreme learning machine in curvature-feature domain","2013","Journal of Systems and Software","10.1016/j.jss.2012.07.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868633631&doi=10.1016%2fj.jss.2012.07.070&partnerID=40&md5=dbe54f20d1500144e818bfa33cc46505","To overcome some drawbacks existing in current zero-watermarking methods, a lossless copyright authentication scheme is proposed in this paper. This scheme designs a multiple zero-watermarking algorithm based on Bessel-Fourier moment and extreme learning machine (ELM) in curvature-feature domain, develops a method for image feature enhancement and noise suppression in curvature-feature domain, and presents a simple algorithm which uses Bessel-Fourier moment phase to estimate the rotation angle of the rotation-attacked image. The experimental results, involving five types of images, indicate the proposed scheme has better overall performance compared to other five current methods, especially in the aspects of resisting high ratio cropping and large angle rotation attacks. Finally, some related factors including phase and magnitude components, feature vector dimension and ELM optimization are considered in the algorithm performance evaluation. © 2012 Elsevier Inc. All rights reserved.","Bessel-Fourier moment; Copyright authentication; Curvature feature; Extreme learning machine (ELM); Multiple zero-watermarking"
"A framework for model-driven development of information systems: Technical decisions and lessons learned","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863612271&doi=10.1016%2fj.jss.2012.04.080&partnerID=40&md5=98f145482481f54b117c0996acba282a","In recent years, the impact of the model-driven engineering (MDE) paradigm has resulted in the advent of a number of model-based methodological proposals that leverage the use of models at any stage of the development cycle. Apart from promoting the role of models, MDE is notable for leveraging the level of automation along the development process. For this to be achieved there is a need of supporting frameworks, tools or environments. This way, while accompanying any methodological proposal of the corresponding technical support has been traditionally recognized as a good practice, it becomes a mandatory requirement in MDE contexts. To address this task, this work presents in a systematic and reasoned way the set of methodological and technical decisions that drove the specification of M2DAT, a technical solution for model-driven development of Information Systems and its reference implementation: M2DAT-DB, a DSL toolkit for model-driven development of modern DB schemas. The objective of this work is to put forward the conclusions and decisions derived from the experience of the authors when designing and building such framework. As a result, this work will help not only MDE practitioners, but also SE practitioners wishing to bring the advantages of MDE to their fields of interest. © 2012 Elsevier Inc.","Domain-specific modeling; Model-driven engineering; Software development frameworks"
"An encoding scheme based on fractional number for querying and updating XML data","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861340936&doi=10.1016%2fj.jss.2012.02.054&partnerID=40&md5=a52caf2759a20be3034674c948f8f23d","In order to facilitate the XML query processing, several labeling schemes have been proposed to directly determine the structural relationships between two arbitrary XML nodes without accessing the original XML documents. However, the existing XML labeling schemes have to re-label the pre-existing nodes or re-calculate the label values when a new node is inserted into the XML document during an update process. In this paper, we devise a novel encoding scheme based on the fractional number to encode the labels of the XML nodes. Moreover, we propose a mapping method to convert our proposed fractional number based encoding scheme to bit string based encoding scheme with the intention to minimize the label size and save the storage space. By applying our proposed bit string encoding scheme to the range-based labeling scheme and the prefix labeling scheme, the process of re-labeling the pre-existing nodes can be avoided when nodes are inserted as leaf nodes and sibling nodes without affecting the order of XML nodes. In addition, we propose an algorithm to control the increment of label size when new nodes are inserted frequently at a fix place of an XML tree. Experimental results show that our proposed bit string encoding scheme provides efficient support to the process of XML updating without sacrificing the query performance when it is applied to the range-based labeling schemes. © 2012 Elsevier Inc. All rights reserved.","Bit string; Dynamic labeling scheme; Fractional number; Skewed insertion; XML query processing; XML updating"
"Achieving key privacy without losing CCA security in proxy re-encryption","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857361860&doi=10.1016%2fj.jss.2011.09.034&partnerID=40&md5=393c8df81ff1f45e9408096367556ae5","In proxy re-encryption (PRE), a semi-trusted proxy can transform a ciphertext under the delegator's public key into another ciphertext that the delegatee can decrypt by his/her own private key. However, the proxy cannot access the plaintext. Due to its transformation property, proxy re-encryption can be used in many applications, such as encrypted email forwarding. Some of these applications require that the underlying PRE scheme is CCA-secure and key-private. However, to the best of our knowledge, none of the existing PRE schemes satisfy this security requirement in the standard model. In this paper, based on the 5-Extended Decision Bilinear Diffie-Hellman assumption and Decision Diffie-Hellman assumption, we propose the first such PRE scheme, which solves an open problem left by Ateniese et al. (2009). © 2011 Elsevier Inc. All rights reserved.","Chosen ciphertext security; Key privacy; Proxy re-encryption; Standard model"
"Quasi-static fault-tolerant scheduling schemes for energy-efficient hard real-time systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862825545&doi=10.1016%2fj.jss.2012.01.020&partnerID=40&md5=1d469dc7e9b3d3969d0c67353e6663bb","This paper investigates fault tolerance and dynamic voltage scaling (DVS) in hard real-time systems. The authors present quasi-static task scheduling algorithms that consist of offline components and online components. The offline components are designed the way they enable the online components to achieve energy savings by using the dynamic slack due to variations in task execution times and uncertainties in fault occurrences. The proposed schemes utilize a fault model that considers the effects of voltage scaling on transient fault rate. Simulation results based on real-life task sets and processor data sheets show that the proposed scheduling schemes achieve energy savings of up to 50 over the state-of-art low-energy offline scheduling techniques and incur negligible runtime overheads. A hard real-time real-life test bed has been developed allowing the validation of the proposed algorithms. © 2012 Elsevier Inc. All rights reserved.","Dynamic voltage scaling (DVS); Energy-efficient dynamic scheduling; Fault tolerance; Hard real-time embedded systems"
"From proprietary to open source - Growing an open source ecosystem","2012","Journal of Systems and Software","10.1016/j.jss.2011.06.071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861099556&doi=10.1016%2fj.jss.2011.06.071&partnerID=40&md5=f4f886e24efd4a620b8d553246f27ec2","In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem. © 2012 Elsevier Inc. All rights reserved.","Open source; Open source engineering; Opening proprietary software; Software ecosystem"
"Methodological construction of product-form stochastic Petri nets for performance evaluation","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861093695&doi=10.1016%2fj.jss.2011.11.1042&partnerID=40&md5=ca4463f6ed742d91b59d82718478c1f6","Product-forms in Stochastic Petri nets (SPNs) are obtained by a compositional technique for the first time, by combining small SPNs with product-forms in a hierarchical manner. In this way, performance engineering methodology is enhanced by the greatly improved efficiency endowed to the steady-state solution of a much wider range of Markov models. Previous methods have relied on analysis of the whole net and so are not incremental - hence they are intractable in all but small models. We show that the product-form condition for open nets depends, in general, on the transition rates, whereas closed nets have only structural conditions for a product-form, except in rather pathological cases. Both the ""building blocks"" formed by the said small SPNs and their compositions are solved for their product-forms using the Reversed Compound Agent Theorem (RCAT), which, to date, has been used exclusively in the context of process-algebraic models. The resulting methodology provides a powerful, general and rigorous route to product-forms in large stochastic models and is illustrated by several detailed examples. © 2012 Elsevier Inc. All rights reserved.","Modular specification; Performance evaluation of software architectures; Petri nets; Product-form solutions; Stochastic modelling"
"A symbolic analysis framework for static analysis of imperative programming languages","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859532705&doi=10.1016%2fj.jss.2011.11.1039&partnerID=40&md5=ea31878f834509e8166fa898dabf94f6","We present a generic symbolic analysis framework for imperative programming languages. Our framework is capable of computing all valid variable bindings of a program at given program points. This information is invaluable for domain-specific static program analyses such as memory leak detection, program parallelization, and the detection of superfluous bound checks, variable aliases and task deadlocks. We employ path expression algebra to model the control flow information of programs. A homomorphism maps path expressions into the symbolic domain. At the center of the symbolic domain is a compact algebraic structure called supercontext. A supercontext contains the complete control and data flow analysis information valid at a given program point. Our approach to compute supercontexts is based purely on algebra and is fully automated. This novel representation of program semantics closes the gap between program analysis and computer algebra systems, which makes supercontexts an ideal symbolic intermediate representation for all domain-specific static program analyses. Our approach is more general than existing methods because it can derive solutions for arbitrary (even intra-loop and nested loop) nodes of reducible and irreducible control flow graphs. We prove the correctness of our symbolic analysis method. Our experimental results show that the problem sizes arising from real-world applications such as the SPEC95 benchmark suite are tractable for our symbolic analysis framework. © 2012 Elsevier Inc. All rights reserved.","Path expression algebra; Programming language semantics; Static program analysis; Symbolic analysis"
"Coordination in co-located agile software development projects","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859545298&doi=10.1016%2fj.jss.2012.02.017&partnerID=40&md5=bb2b858ab08d1a8acf619cf6057ebfcb","Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development. © 2012 Elsevier Inc. All rights reserved.","Agile methods; Agile software development project; Coordination effectiveness; Coordination strategy; Coordination Theory; Extreme Programming; Scrum"
"Data management for component-based embedded real-time systems: The database proxy approach","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857356711&doi=10.1016%2fj.jss.2011.10.036&partnerID=40&md5=adf6983ad21f946d4b86c7d53cbd5d8e","We introduce the concept of database proxies intended to mitigate the gap between two disjoint productivity-enhancing techniques: component based software engineering (CBSE) and real-time database management systems (RTDBMS). The two techniques promote opposing design goals and their coexistence is neither obvious nor intuitive. CBSE promotes encapsulation and decoupling of component internals from the component environment, whilst an RTDBMS provide mechanisms for efficient and predictable global data sharing. A component with direct access to an RTDBMS is dependent on that specific RTDBMS and may not be useable in an alternative environment. For components to remain encapsulated and reusable, database proxies decouple components from an underlying database residing in the component framework, while providing temporally predictable access to data maintained in a database. Our approach provide access to features such as extensive data modeling tools, predictable access to hard real-time data, dynamic access to soft real-time data using standardized queries and controlled data sharing; thus allowing developers to employ the full potential of both CBSE and an RTDBMS. Our approach primarily targets embedded systems with a subset of functionality with real-time requirements. The implementation results show that the benefits of using proxies do not come at the expense of significant run-time overheads or less accurate timing predictions. © 2011 Elsevier Inc. All rights reserved.","CBSE; Embedded systems; Real-time; RTDBMS"
"Generalized aggregate Quality of Service computation for composite services","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861347049&doi=10.1016%2fj.jss.2012.03.005&partnerID=40&md5=2e03ddbd94a005fafaa43709c55ae1b5","This article addresses the problem of estimating the Quality of Service (QoS) of a composite service given the QoS of the services participating in the composition. Previous solutions to this problem impose restrictions on the topology of the orchestration models, limiting their applicability to well-structured orchestration models for example. This article lifts these restrictions by proposing a method for aggregate QoS computation that deals with more general types of unstructured orchestration models. The applicability and scalability of the proposed method are validated using a collection of models from industrial practice. © 2012 Elsevier Inc. All rights reserved.","Quality of service; Service composition; Service orchestration; Service-oriented computing"
"Complex event processing with T-REX","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861348392&doi=10.1016%2fj.jss.2012.03.056&partnerID=40&md5=7a786a55447df7d2722f3e53ebc7afbe","Several application domains involve detecting complex situations and reacting to them. This asks for a Complex Event Processing (CEP) middleware specifically designed to timely process large amounts of event notifications as they flow from the peripheral to the center of the system, to identify the composite events relevant for the application. To answer this need we designed T-Rex, a new CEP middleware that combines expressiveness and efficiency. On the one hand, it adopts a language (TESLA) explicitly conceived to easily and naturally describe composite events. On the other hand, it provides an efficient event detection algorithm based on automata to interpret TESLA rules. Our evaluation shows that the T-Rex engine can process a large number of complex rules with a reduced overhead, even in the presence of challenging workloads. © 2012 Elsevier Inc. All rights reserved.","Complex Event Processing; Distributed systems; Event middleware; Information processing"
"Debugging applications created by a Domain Specific Language: The IPAC case","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857359534&doi=10.1016%2fj.jss.2011.11.1009&partnerID=40&md5=f8f4d5e32659cc4fe4b871370d0f15fd","Nowadays, software developers have created a large number of applications in various research domains of Computer Science. However, not all of them are familiar with the majority of the research domains. Hence, Domain Specific Languages (DSLs) can provide an abstract, concrete description of a domain in terms that can easily be managed by developers. The most important in such cases is the provision of a debugger for debugging the generated software based on a specific DSL. In this paper, we propose and present a simple but efficient debugger created for the needs of the IPAC system. The debugger is able to provide debugging facilities to developers that define applications for autonomous mobile nodes. The debugger can map code lines between the initial application workflow and the final code defined in a known programming language. Finally, we propose a logging server responsible to provide debugging facilities for the IPAC framework. The IPAC system is consisted of a number of middleware services for mobile nodes acting in a network. In this system a number of mobile nodes exchanged messages that are visualized for more efficient manipulation. © 2011 Elsevier Inc. All rights reserved.","Debugger; Domain Specific Language; Software testing"
"Controversy Corner Improving test efficiency through system test prioritization","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865802794&doi=10.1016%2fj.jss.2012.01.007&partnerID=40&md5=579684172b1ee27c7df8fb73b3e437d0","Software testing is an expensive process consuming at least 50% of the total development cost. Among the types of testing, system testing is the most expensive and complex. Companies are frequently faced with budgetary constraints, which may limit their ability to effectively complete testing efforts before delivering a software product. We build upon prior test case prioritization research and present a system-level approach to test case prioritization called Prioritization of Requirements for Test (PORT). PORT prioritizes system test cases based on four factors for each requirement: customer priority, implementation complexity, fault proneness, and requirements volatility. Test cases for requirements with higher priority based upon a weighted average of these factors are executed earlier in system test. An academic feasibility study and three post hoc industrial studies were conducted. Results indicate that PORT can be used to improve the rate of failure detection when compared with a random and operational profile-driven random approach. Furthermore, we investigated the contribution of the prioritization factors towards the improved rate of failure detection and found customer priority was the most significant contributor. Tool support is provided for the PORT scheme which allows for automatic collection of the four factor values and the resultant test case prioritization. © 2012 Elsevier Inc. All rights reserved.","Regression testing; Software quality; Software testing and reliability; System test; Test case prioritization; Value-based testing"
"A longitudinal case study of an emerging software ecosystem: Implications for practice and theory","2012","Journal of Systems and Software","10.1016/j.jss.2011.04.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958283622&doi=10.1016%2fj.jss.2011.04.020&partnerID=40&md5=8fb17817169ad361e9012b12f779eaba","Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change. © 2012 Elsevier Inc. All rights reserved.","Agile software development; Longitudinal case study; Software ecosystems; Software product line engineering"
"Dependency solving: A separate concern in component evolution management","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863623438&doi=10.1016%2fj.jss.2012.02.018&partnerID=40&md5=a87b9a3a8c169e0ab178a3c78b6d29c2","Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task. We show that the complexity of the underlying upgrade planning problem is NP-complete even for seemingly simple component models, and argue that the principal source of complexity lies in multiple available versions of components. We then discuss the need of expressive languages for user preferences, which makes the problem even more challenging. We propose to establish dependency solving as a separate concern from other upgrade aspects, and present CUDF as a formalism to describe upgrade scenarios. By analyzing the result of an international dependency solving competition, we provide evidence that the proposed approach is viable. © 2011 Elsevier Inc. All rights reserved.","Competition; Component; Dependency solving; Open source; Package management; Software evolution"
"Intelligent reversible watermarking in integer wavelet domain for medical images","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857367715&doi=10.1016%2fj.jss.2011.11.005&partnerID=40&md5=bd9294b3f8842bcea7f626cfe6b600eb","The prime requirement of reversible watermarking scheme is that the system should be able to restore the cover work to its original state after extracting the hidden information. Reversible watermarking approaches, therefore, have wide applications in medical and defense imagery. In this paper, an intelligent reversible watermarking approach GA-RevWM for medical images is proposed. GA-RevWM is based on the concept of block-based embedding using genetic algorithm (GA) and integer wavelet transform (IWT). GA based intelligent threshold selection scheme is applied to improve the imperceptibility for a fixed payload or vice versa. The experimental results show that GA-RevWM provides significant improvement in terms of imperceptibility for a desired level of payload against the existing approaches. © 2011 Elsevier Inc. All rights reserved.","Genetic algorithm (GA); Histogram recovery; Integer wavelet transform (IWT); Medical images; Reversible watermarking"
"Learning extended FSA from software: An empirical assessment","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863103938&doi=10.1016%2fj.jss.2012.04.001&partnerID=40&md5=58e792e57ee8902718434829c60be6ca","A number of techniques that infer finite state automata from execution traces have been used to support test and analysis activities. Some of these techniques can produce automata that integrate information about the data-flow, that is, they also represent how data values affect the operations executed by programs. The integration of information about operation sequences and data values into a unique model is indeed conceptually useful to accurately represent the behavior of a program. However, it is still unclear whether handling heterogeneous types of information, such as operation sequences and data values, necessarily produces higher quality models or not. In this paper, we present an empirical comparative study between techniques that infer simple automata and techniques that infer automata extended with information about data-flow. We investigate the effectiveness of these techniques when applied to traces with different levels of sparseness, produced by different software systems. To the best of our knowledge this is the first work that quantifies both the effect of adding data-flow information within automata and the effectiveness of the techniques when varying sparseness of traces. © 2012 Elsevier Inc. All rights reserved.","Behavioral models; Empirical validation; FSA inference"
"An efficient RSA-based certificateless signature scheme","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857368369&doi=10.1016%2fj.jss.2011.09.036&partnerID=40&md5=7ae963d56b45d97dcf1186535630b181","Until now, the only known construction of certificateless signature scheme is mainly based on the rather new and untested assumptions related to bilinear maps. But the implementations of pairings are more time-consuming than exponentiation operator in a RSA group. As an industry standard cryptographic algorithm, RSA is widely applied in real-life scenarios and provides many interfaces for the applied software. However, to the best of our knowledge, there does not exist RSA-based certificateless signature scheme. To overcome this problem, we present a RSA-based construction of certificateless signature scheme in the paper. And the scheme is shown to be secure in the random oracles model. The security of the scheme is closely related to the RSA problem and the discrete logarithm problem. © 2011 Elsevier Inc. All rights reserved.","Certificateless signature; Discrete logarithm problem; RSA; Security proof"
"EClass: An execution classification approach to improving the energy-efficiency of software via machine learning","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857364286&doi=10.1016%2fj.jss.2011.11.1010&partnerID=40&md5=47e0d661b6ce9423dc6843517a7aae71","Energy efficiency at the software level has gained much attention in the past decade. This paper presents a performance-aware frequency assignment algorithm for reducing processor energy consumption using Dynamic Voltage and Frequency Scaling (DVFS). Existing energy-saving techniques often rely on simplified predictions or domain knowledge to extract energy savings for specialized software (such as multimedia or mobile applications) or hardware (such as NPU or sensor nodes). We present an innovative framework, known as EClass, for general-purpose DVFS processors by recognizing short and repetitive utilization patterns efficiently using machine learning. Our algorithm is lightweight and can save up to 52.9% of the energy consumption compared with the classical PAST algorithm. It achieves an average savings of 9.1% when compared with an existing online learning algorithm that also utilizes the statistics from the current execution only. We have simulated the algorithms on a cycle-accurate power simulator. Experimental results show that EClass can effectively save energy for real life applications that exhibit mixed CPU utilization patterns during executions. Our research challenges an assumption among previous work in the research community that a simple and efficient heuristic should be used to adjust the processor frequency online. Our empirical result shows that the use of an advanced algorithm such as machine learning can not only compensate for the energy needed to run such an algorithm, but also outperforms prior techniques based on the above assumption. © 2011 Elsevier Inc. All rights reserved.","DVFS; Energy optimization; Energy saving; Machine learning; Workload prediction"
"Mining frequent patterns from dynamic data streams with data load management","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862816556&doi=10.1016%2fj.jss.2012.01.024&partnerID=40&md5=c010f59528217e1b1409817f95b84f3c","In this paper, we study the practical problem of frequent-itemset discovery in data-stream environments which may suffer from data overload. The main issues include frequent-pattern mining and data-overload handling. Therefore, a mining algorithm together with two dedicated overload-handling mechanisms is proposed. The algorithm extracts basic information from streaming data and keeps the information in its data structure. The mining task is accomplished when requested by calculating the approximate counts of itemsets and then returning the frequent ones. When there exists data overload, one of the two mechanisms is executed to settle the overload by either improving system throughput or shedding data load. From the experimental data, we find that our mining algorithm is efficient and possesses good accuracy. More importantly, it could effectively manage data overload with the overload-handling mechanisms. Our research results may lead to a feasible solution for frequent-pattern mining in dynamic data streams. © 2012 Elsevier Inc. All rights reserved.","Combinatorial approximation; Data mining; Data streams; Frequent patterns; Load shedding; Overload handling"
"An experimental comparison of different real-time schedulers on multicore systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863611347&doi=10.1016%2fj.jss.2012.05.048&partnerID=40&md5=d0ad34c0c56eba6c1bd7c76149c2109f","In this work, an experimental comparison among the Rate Monotonic (RM) and Earliest Deadline First (EDF) multiprocessor real-time schedulers is performed, with a focus on soft real-time systems. We generated random workloads of synthetic periodic task sets and executed them on a big multi-core machine, using Linux as Operating System, gathering an extensive amount of data related to their exhibited performance under various real-time scheduling strategies. The comparison involves the fixed-priority scheduler for multiprocessors as available in the Linux kernel (with priorities set so as to achieve RM), and on our own implementation of EDF, both configured in global, partitioned and clustered mode. The impact of the various scheduling strategies on the performance of the applications, as well as the generated scheduling overheads, are compared presenting an extensive set of experimental results. These provide a comprehensive view of the performance achievable by the different schedulers under various workload conditions. © 2011 Elsevier Inc. All rights reserved.","Experimental evaluation; Global EDF; Multi-core systems; Real-time scheduling"
"On the relationship between comment update practices and Software Bugs","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863620476&doi=10.1016%2fj.jss.2011.09.019&partnerID=40&md5=487f9af06b91d32108980e9b5007186b","When changing source code, developers sometimes update the associated comments of the code (a consistent update), while at other times they do not (an inconsistent update). Similarly, developers sometimes only update a comment without its associated code (an inconsistent update). The relationship of such comment update practices and software bugs has never been explored empirically. While some (in)consistent updates might be harmless, software engineering folklore warns of the risks of inconsistent updates between code and comments, because these updates are likely to lead to out-of-date comments, which in turn might mislead developers and cause the introduction of bugs in the future. In this paper, we study comment update practices in three large open-source systems written in C (FreeBSD and PostgreSQL) and Java (Eclipse). We find that these practices can better explain and predict future bugs than other indicators like the number of prior bugs or changes. Our findings suggest that inconsistent changes are not necessarily correlated with more bugs. Instead, a change in which a function and its comment are suddenly updated inconsistently, whereas they are usually updated consistently (or vice versa), is risky (high probability of introducing a bug) and should be reviewed carefully by practitioners. © 2011 Elsevier Inc. All rights reserved.","Code quality; Empirical studies; Software bugs; Software evolution; Source code comments"
"Improved preimage attack on one-block MD4","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857359525&doi=10.1016%2fj.jss.2011.11.1020&partnerID=40&md5=b4a5290ce7d1e98b299d64eedef3cabd","MD4 is a hash function designed by Rivest in 1990. The design philosophy of many important hash functions, such as MD5, SHA-1 and SHA-2, originated from that of MD4. We propose an improved preimage attack on one-block MD4 with the time complexity 2 95 MD4 compression function operations, as compared to the 2 107 1 complexity of the previous attack by Aoki et al. (SAC 2008). The attack is based on previous methods, but introduces new techniques. We also use the same techniques to improve the pseudo-preimage and preimage attacks on Extended MD4 with 2 25.2 and 2 12.6 improvement factor, as compared to previous attacks by Sasaki et al. (ACISP 2009). © 2011 Elsevier Inc. All rights reserved.","Extended MD4; MD4; Meet-in-the-middle; Preimage"
"An improved swarm optimized functional link artificial neural network (ISO-FLANN) for classification","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859519948&doi=10.1016%2fj.jss.2012.01.025&partnerID=40&md5=9752e14712aba4b3ebb42ef875b18462","Multilayer perceptron (MLP) (trained with back propagation learning algorithm) takes large computational time. The complexity of the network increases as the number of layers and number of nodes in layers increases. Further, it is also very difficult to decide the number of nodes in a layer and the number of layers in the network required for solving a problem a priori. In this paper an improved particle swarm optimization (IPSO) is used to train the functional link artificial neural network (FLANN) for classification and we name it ISO-FLANN. In contrast to MLP, FLANN has less architectural complexity, easier to train, and more insight may be gained in the classification problem. Further, we rely on global classification capabilities of IPSO to explore the entire weight space, which is plagued by a host of local optima. Using the functionally expanded features; FLANN overcomes the non-linear nature of problems. We believe that the combined efforts of FLANN and IPSO (IPSO + FLANN = ISO - FLANN) by harnessing their best attributes can give rise to a robust classifier. An extensive simulation study is presented to show the effectiveness of proposed classifier. Results are compared with MLP, support vector machine(SVM) with radial basis function (RBF) kernel, FLANN with gradiend descent learning and fuzzy swarm net (FSN). © 2012 Elsevier Inc. All rights reserved.","Classification; Data mining; FSN; Functional link artificial neural networks; Improved particle swarm optimization; Multi-layer perception; Particle swarm optimization; SVM"
"A dynamic layout of sliding window for frequent itemset mining over data streams","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857355203&doi=10.1016%2fj.jss.2011.09.055&partnerID=40&md5=0ea299ac13a202b62267cd9332a1016e","Mining frequent patterns over data streams is an interesting and challenging problem due to the emergence of new applications and limited resources of main memory and processing power. In this study, a novel sliding window based method for efficient mining of frequent patterns over data streams is proposed. This method provides a dynamic layout of sliding window by utilizing a set of simple lists for items existing within the window. For every item within the window, the most memory efficient list type based on its frequency is selected to store its occurrence information. A novel window adjustment technique including list type conversions is used to control the memory usage when the concept change occurs. At any time, if a user issues a request for frequent patterns in the recent window, a suitable approach based on the current content of the window is selected for the mining process. In comparison with recently proposed algorithms, empirical results show the superiority of the proposed method with multiple orders of magnitude in terms of runtime and memory usage. © 2011 Elsevier Inc. All rights reserved.","Data stream; Data stream mining; Frequent patterns; Sliding window; Window adjustment"
"A reusable structural design for mobile collaborative applications","2012","Journal of Systems and Software","10.1016/j.jss.2011.05.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366738&doi=10.1016%2fj.jss.2011.05.046&partnerID=40&md5=3acb1694a18c5bc50463dc48400cbf7c","Architecting mobile collaborative applications has always been a challenge for designers. However, counting on a structural design as a reference can help developers to reduce risks and efforts involved in system design. This article presents a reusable architecture which helps modeling the communication and coordination services required by mobile collaborative applications to support collaboration among users. This architecture has been used as a basis for the design of several mobile systems. Two of them are presented in this article to show the applicability of the proposal to real world collaborative systems. © 2011 Elsevier Inc. All rights reserved.","Communication services design; Coordination services design; Mobile collaborative applications; Reusable architecture"
"Scaling up software architecture analysis","2012","Journal of Systems and Software","10.1016/j.jss.2011.03.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861096984&doi=10.1016%2fj.jss.2011.03.050&partnerID=40&md5=ca4939ba0bf711ec401712e4ab8b50aa","This paper will show how architecture design and analysis techniques rest on a small number of foundational principles. We will show how those principles have been instantiated as a core set of techniques. These techniques, combined together, have resulted in several highly successful architecture analysis and design methods. Finally, we will show how these foundations, and the techniques that instantiate them, can be re-combined for new purposes addressing problems of ever-increasing scale, specifically: addressing the highly complex problems of analyzing software-intensive ecosystems. © 2012 Elsevier Inc. All rights reserved.","Analysis; Software architecture; Software-intensive ecosystems; System of systems"
"Blackboard architecture to integrate components and agents in heterogeneous distributed eLearning systems: An application for learning to program","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861100760&doi=10.1016%2fj.jss.2012.02.009&partnerID=40&md5=42807cb2d6ba7495c285165f5832b19c","To build complete and complex eLearning systems, eLearning engineers are used to applying standards that facilitate sharing information as well as distributed service-oriented architectures that provide reuse and interoperability by means of component integration. These concepts lead us to a Component-based Development Process that will allow us to implement tools that give full support to the teaching/learning process, taking advantage of the synergy effect created by the integration of the different components. Thus, throughout this article we analyse the proposals from the most relevant consortia concerned with eLearning standards, showing their service oriented approaches and the middleware technologies which can be used to implement them. This analysis will demonstrate that the use of middleware technologies that use the definition of services' interface can limit the reuse and interoperability requisites desired by the main standards consortia. Then, we will show a proposal which tries to solve this shortfall, using a blackboard-based architecture for integrating and communicating heterogeneous distributed components, as well as a user environment that also allows us to perform component integration. As an example, we will demonstrate how we have built an application for learning to program by applying our approach and following a Component-based Development Process to implement different components (services, agents, clients, etc.) that integrate it. Hence, we will argue that using blackboard architecture and a Component-based Development Process helps us to solve the identified shortcomings. © 2012 Elsevier Inc. All rights reserved.","Component-based development; Eclipse; eLearning standards; Tuple-spaces"
"Formally based semi-automatic implementation of an open security protocol","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.052","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857355689&doi=10.1016%2fj.jss.2011.10.052&partnerID=40&md5=b042ddb28b84a983549a5bbd80971faf","This paper presents an experiment in which an implementation of the client side of the SSH Transport Layer Protocol (SSH-TLP) was semi-automatically derived according to a model-driven development paradigm that leverages formal methods in order to obtain high correctness assurance. The approach used in the experiment starts with the formalization of the protocol at an abstract level. This model is then formally proved to fulfill the desired secrecy and authentication properties by using the ProVerif prover. Finally, a sound Java implementation is semi-automatically derived from the verified model using an enhanced version of the Spi2Java framework. The resulting implementation correctly interoperates with third party servers, and its execution time is comparable with that of other manually developed Java SSH-TLP client implementations. This case study demonstrates that the adopted model-driven approach is viable even for a real security protocol, despite the complexity of the models needed in order to achieve an interoperable implementation. © 2011 Elsevier Inc. All rights reserved.","Automatic code generation; Model-driven-development; Security protocols; Spi2Java"
"""Leagile"" software development: An experience report analysis of the application of lean approaches in agile software development","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862792235&doi=10.1016%2fj.jss.2012.01.061&partnerID=40&md5=3fd40c1586b1e150fffc4f4ab8aa4ae4","In recent years there has been a noticeable shift in attention from those who use agile software development toward lean software development, often labelled as a shift ""from agile to lean"". However, the reality may not be as simple or linear as this label implies. To provide a better understanding of lean software development approaches and how they are applied in agile software development, we have examined 30 experience reports published in past agile software conferences in which experiences of applying lean approaches in agile software development were reported. The analysis identified six types of lean application. The results of our study show that lean can be applied in agile processes in different manners for different purposes. Lean concepts, principles and practices are most often used for continuous agile process improvement, with the most recent introduction being the kanban approach, introducing a continuous, flow-based substitute to time-boxed agile processes. © 2012 Elsevier Inc. All rights reserved.","Agile software development; Experience report; Kanban; Leagile; Lean software development; Scrum; Software engineering"
"Loop fusion and reordering for register file optimization on stream processors","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862829091&doi=10.1016%2fj.jss.2012.02.016&partnerID=40&md5=e5f99c39845f8acf07323950c34b5f32","Stream processors are gaining popularity and getting deployed in many multimedia and scientific applications. stream register file (SRF) is a non-bypassing software-managed on-chip memory. Unlike conventional register files, the input data must be all stored in the SRF when a program is being executed. It is a critical resource in stream processors. When loading a program from the off-chip memory into SRF for execution, the storage consumption and the data transfer time are two key factors which affect the performance. This work applies loop transformation to programs for SRF optimization. We consider two objectives of minimizing the storage consumption and data transfer time. Previous techniques concentrate on the utilization of SRF only. This is the first paper considering both the two factors. We present a cost evaluation function in this paper and apply loop fusion and reordering to improve the performance of stream processors. The experimental results show significant performance improvement. © 2012 Elsevier Inc. All rights reserved.","Loop fusion; Loop reordering; Maximumloop distribution; Stream processor; Stream register file"
"Handling timing constraints violations in soft real-time applications as exceptions","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857356760&doi=10.1016%2fj.jss.2011.11.1021&partnerID=40&md5=b6e043c29d7bbe85b77c4d2e695f0df0","In this paper, an exception-based programming paradigm is envisioned to deal with timing constraints violations occurring in soft real-time and multimedia applications written in the C language. In order to prove viability of the approach, a mechanism allowing to use such paradigm has been designed and implemented as an open-source library of C macros making use of the standard POSIX API (a few Linux-specific optimizations are also briefly discussed). The proposed approach has been validated by modifying mplayer, one of the most widely used multimedia player for Linux, so as to use the introduced library. An extensive experimental evaluation has been made, both when running the player alone and when mixing it with a workload of other synthetic real-time applications. In the latter case, different scheduling policies have been used, including both standard priority-based ones as available on the mainline Linux, and an experimental deadline-based one available as a separate patch. The shown results demonstrate how the exception-based paradigm is effective in improving the audio/video delay exhibited by the player achieving a superior performance and a dramatically better quality of experience as compared to the original heuristic frame-dropping mechanism of the player. © 2011 Elsevier Inc. All rights reserved.","Exception handling; Multimedia; Soft real-time"
"Coding-error based defects in enterprise resource planning software: Prevention, discovery, elimination and mitigation","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861096233&doi=10.1016%2fj.jss.2012.02.034&partnerID=40&md5=c976529877c7efa713b258662e2855d5","Software defects due to coding errors continue to plague the industry with disastrous impact, especially in the enterprise application software category. Identifying how much of these defects are specifically due to coding errors is a challenging problem. In this paper, we investigate the best methods for preventing new coding defects in enterprise resource planning (ERP) software, and discovering and fixing existing coding defects. A large-scale survey-based ex-post-facto study coupled with experiments involving static code analysis tools on both sample code and real-life million lines of code open-source ERP software were conducted for such purpose. The survey-based methodology consisted of respondents who had experience developing ERP software. This research sought to determine if software defects could be merely mitigated or totally eliminated, and what supporting policies, procedures and infrastructure were needed to remedy the problem. In this paper, we introduce a hypothetical framework developed to address our research questions, the hypotheses we have conjectured, the research methodology we have used, and the data analysis methods used to validate the stated hypotheses. Our study revealed that: (a) the best way for ERP developers to discover coding-error based defects in existing programs is to choose an appropriate programming language; perform a combination of manual and automated code auditing, static code analysis, and formal test case design, execution and analysis, (b) the most effective ways to mitigate defects in an ERP system is to track the defect densities in the ERP software, fix the defects found, perform regression testing, and update the resulting defect density statistics, and (c) the impact of epistemological and legal commitments on the defect densities of ERP systems is inconclusive. We feel that our proposed model has the potential to vastly improve the quality of ERP and other similar software by reducing the coding-error defects, and recommend that future research aimed at testing the model in actual production environments. © 2012 Elsevier Inc. All rights reserved.","Code auditing; Coding defects; Defect density; Defect reduction; ERP; Software development; Software testing; Static code analysis"
"Model-driven support for product line evolution on feature level","2012","Journal of Systems and Software","10.1016/j.jss.2011.08.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863621956&doi=10.1016%2fj.jss.2011.08.008&partnerID=40&md5=f26c17458d83e47a327a43cefb7a389c","Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, SPL engineering promises high productivity benefits. There is however, a lack of support for systematic management of SPL evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing SPL evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps. © 2011 Elsevier Inc. All rights reserved.","Evolving systems; Feature modeling; Model-driven engineering; Software Product Lines"
"Free and Open Source Software versus Internet content filtering and censorship: A case study","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857365879&doi=10.1016%2fj.jss.2011.11.1007&partnerID=40&md5=d07dba8461149b83dd9b33f30493dcbb","This study critically investigates the main characteristics and features of anti-filtering packages provided by Free and Open Source Software (FOSS). For over a decade, the digital communities around the globe have used FOSS packages not only as an inexpensive way to access to information available on Internet, but also to disseminate thoughts, opinions and concerns about various socio-political and economic matters. Proxy servers and FOSS played a vital role in helping citizens in repressed countries to bypass the state imposed Internet content filtering and censorship practices. On the one hand, proxy servers act as redirectors to websites, and on the other hand, many of these servers are the main source for downloading FOSS anti-filtering software packages. These packages can provide secure web surfing via anonymous web access, data encryption, IP address masking, location concealment, browser history and cookie clean-ups but they also provide proxy software updates as well as domain name updates. The main objectives of this study are to investigate the role of FOSS packages in combating Internet content filtering and censorship and empowering citizens to effectively participate in communication discourse. By evaluating some of the well known FOSS anti-filtering packages used by Iran's digital community, this study found that despite the success of FOSS in combating filtering and state censorship, the majority of these software packages were not designed to meet the needs of Internet users. In particular, they are poorly adapted to the slow Internet connections in many developing countries such as Iran. In addition, these software packages do not meet the level of sophistication used by authorities to filter the content of the Net. Therefore, this study offers a new model that takes into account not only the existing level of the Internet infrastructure but also the growing number of Internet users demanding more effective FOSS packages for faster access to uncensored information while maintaining anonymity. © 2011 Elsevier Inc. All rights reserved.","Censorship; Communication discourse; Filtering; ICT; Open Source Software; P2P; Software development"
"Understanding post-adoptive agile usage: An exploratory cross-case analysis","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859533021&doi=10.1016%2fj.jss.2012.02.025&partnerID=40&md5=5da87e518039361132cbccb76e90d3f1","While past research has contributed to the understanding of how organizations adopt agile methodologies (AM), little is known about their post-adoptive usage in organizations. By integrating theories from systems development methodologies, diffusion of innovations, and agile methodology literature, this paper proposes a new model that identifies a set of critical factors pertinent to post-adoptive usage of agile practices. This model is used to inform analysis of post-adoptive usage of agile practices in two major organizations. The results indicate relative advantage, team attitude and technical competence, championing, and top management support (TMS) are the key factors determining the extent to which agile practices can be assimilated into an organization. Specifically, both findings and this model confirm that the deeper the assimilation of agile practices into the organization, the better understanding of how assimilation leads to specific improvements in its systems development outcomes. © 2012 Elsevier Inc. All rights reserved.","Effectiveness; Kanban; Post-adoptive agile usage; Scrum"
"Goal alignment in process improvement","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859532572&doi=10.1016%2fj.jss.2012.01.038&partnerID=40&md5=4806d751efe8bd61b1fa9e26d97923f3","Process improvement should improve an organisation's ability to achieve its business goals. While mapping an organisation's strategic goals through various layers of management is common, such mapping does not seem to continue through to their processes that create value to the organisation. Despite a number of process improvement methods being available, and almost two decades of experience with those methods, many process improvement projects do not end successfully. We explore the impact process assessment has on process improvement. In particular, we study the alignment of an organisation's process goals to its business goals; and the contribution of process assessment to this goal alignment. This paper illustrates the data gathered through industry survey reflecting the lack of focus on and alignment of organisation's business goals throughout process improvement. The results indicate that there is little knowledge and experience in industry in aligning the process goals and organisation's business goals. This, in turn, could explain the unsuccessful process improvement efforts or perhaps even the skepticism towards process improvement in general. © 2012 Elsevier Inc. All rights reserved.","Goal alignment; Impact analysis; Organisational business goals; Process assessment; Process goals; Process improvement"
"Adaptive co-scheduling for periodic application and update transactions in real-time database systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861344566&doi=10.1016%2fj.jss.2012.03.055&partnerID=40&md5=c97827e612f195c6c8fa23c061be9df9","In this paper, we study the co-scheduling problem of periodic application transactions and update transactions in real-time database systems for surveillance of critical events. To perform the surveillance functions effectively, it is important to meet the deadlines of the application transactions and maintain the quality of the real-time data objects for their executions. Unfortunately, these two goals are conflicting and difficult to be achieved at the same time. To address the co-scheduling problem, we propose a real-time co-scheduling algorithm, called Adaptive Earliest Deadline First Co-Scheduling (AEDF-Co). In AEDF-Co, a dynamic scheduling approach is adopted to adaptively schedule the update and application jobs based on their deadlines. The performance goal of AEDF-Co is to determine a schedule for given sets of periodic application and update transactions such that the deadline constraints of all the application transactions are satisfied and at the same time the quality of data (QoD) of the real-time data objects is maximized. Extensive simulation experiments have been performed to evaluate the performance of AEDF-Co. The results show that by adaptively adjusting the release times of update jobs and scheduling the update and application jobs dynamically based on their urgencies, AEDF-Co is effective in achieving the performance goals and maximizing the overall system performance. © 2012 Elsevier Inc. All rights reserved.","Data freshness; Real-time co-scheduling; Real-time database systems; Update generation and processing"
"Automatic execution of business process models: Exploiting the benefits of Model-driven Engineering approaches","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857372437&doi=10.1016%2fj.jss.2011.09.022&partnerID=40&md5=c1c23cd36c542b19e8702bf8090e67e8","The business goals of an enterprise process are traced to business process models with the aim of being carried out during the execution stage. The automatic translation from these models to fully executable code which can be simulated and round-trip engineered is still an open challenge in the Business Process Management field. Model-driven Engineering has proposed a set of methodologies with which to solve the existing gap between business analysts and software developers, but the expected results have not as yet been achieved. In this paper, a new approach to solve this challenge is proposed. This approach is based on the integration of SOD-M, a model-driven method for the development of service-oriented systems, and DENEB, a platform for the development and execution of flexible business processes. SOD-M provides business analysts with a methodology that can be used to transform their business goals into composition service models, a type of model that represents business processes. The use of the Eclipse Modelling Framework and the ATLAS Transformation Language allows this model to be automatically transformed into a DENEB workflow model, resulting in a business process that is coded by a class of high-level Petri-nets and is directly executable in DENEB. The application of the proposal presented herein is illustrated by means of a real system related to the management of medical images. © 2011 Elsevier Inc. All rights reserved.","Business processes; Model execution; Model transformation; Model-driven Engineering; Service-oriented development"
"Gateway-oriented password-authenticated key exchange protocol in the standard model","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.061","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857372796&doi=10.1016%2fj.jss.2011.09.061&partnerID=40&md5=d58a766d79c83c977514a67aa2c2210f","A gateway-oriented password-based authenticated key exchange (GPAKE) is a 3-party protocol, which allows a client and a gateway to establish a common session key with the help of an authentication server. GPAKE protocols are suitable for mobile communication environments such as GSM (Global System for Mobile Communications) and 3GPP (The Third Generation Partnership Project). To date, most of the published protocols for GPAKE have been proven secure in the random oracle model. In this paper, we present the first provably-secure GPAKE protocol in the standard model. It is based on the 2-party password- authenticated key exchange protocol of Jiang and Gong. The protocol is secure under the DDH assumption (without random oracles). Furthermore, it can resist undetectable on-line dictionary attacks. Compared with previous solutions, our protocol achieves stronger security with similar efficiency. © 2011 Elsevier Inc. All rights reserved.","DDH; Gateway; Password-based authentication; Standard model"
"A lightweight framework for describing software practices","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366169&doi=10.1016%2fj.jss.2011.09.024&partnerID=40&md5=9d376978d3152a09867d3e5da8fde6b3","In order to maximise software project outcomes, software organisations adapt development methodologies and implement practices in a way that is appropriate for project contexts. This suggests that 'Best Practice' is context-dependent. To better understand the contextual nature of best practice, we want to explore how organisations actually go about achieving software objectives. We require a research framework that captures this information in a way that makes no assumptions about practices and that is descriptive in nature. We have developed a framework based on the perspective that practices exist to meet specific objectives. We have experimented with the framework by using it to capture the practices of three New Zealand organisations and by application to an idealised XP process. Our capture of organisational practices revealed interesting mechanisms for further study, including a dependence upon informal practices linked with strong communication and the idea of 'push' versus 'pull' for information elicitation. Our capture of XP exposed some context-dependent risks. © 2011 Elsevier Inc. All rights reserved.","Industry case study; Research framework; Software organisations; Software practices; Software process discovery"
"Ordering features by category","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861340883&doi=10.1016%2fj.jss.2012.03.025&partnerID=40&md5=bb83197726df428e21f921943dad4e22","Precedence, whereby features are serialized and execute sequentially in response to an event, is a common method for coordinating features that would otherwise interact. However, the effectiveness of precedence lies in the system designer's ability to order features such that their sequential execution results in desired system behaviour. The task of evaluating feature orderings is expensive: a set of n features means that there are n! feature orderings to consider. This paper shows how the cost of ordering features can be reduced by (1) clustering features into categories and ordering the feature categories - a smaller problem; (2) automating the ordering task by evaluating orders with respect to correctness criteria; and (3) optimizing the ordering task by rejecting outright any ordering that includes a suborder of features that are known to violate correctness criteria. We demonstrate our approach on a case study involving 381 telephony features from both academic and industrial sources. The paper also presents analytical arguments that relate the correctness of an ordering of feature categories to the correctness of a corresponding ordering of features. © 2012 Elsevier Inc. All rights reserved.","Feature interaction; Priority; Telecommunication"
"Obstacles to decision making in Agile software development teams","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859527670&doi=10.1016%2fj.jss.2012.01.058&partnerID=40&md5=22e708d6856ee0c0b088f04f3b29e57a","The obstacles facing decision making in Agile development are critical yet poorly understood. This research examines decisions made across four stages of the iteration cycle: Iteration Planning, Iteration Execution, Iteration Review and Iteration Retrospective. A mixed method approach was employed, whereby a focus group was initially conducted with 43 Agile developers and managers to determine decisions made at different points of the iteration cycle. Subsequently, six illustrative mini cases were purposefully conducted as examples of the six obstacles identified in these focus groups. This included interviews with 18 individuals in Agile projects from five different organizations: a global consulting organization, a multinational communications company, two multinational software development companies, and a large museum organization. This research contributes to Agile software development literature by analyzing decisions made during the iteration cycle and identifying six key obstacles to these decisions. Results indicate the six decision obstacles are unwillingness to commit to decisions; conflicting priorities; unstable resource availability; and lack of: implementation; ownership; empowerment. These six decision obstacles are mapped to descriptive decision making principles to demonstrate where the obstacles affect the decision process. The effects of these obstacles include a lack of longer-term, strategic focus for decisions, an ever-growing backlog of delayed work from previous iterations, and a lack of team engagement. © 2012 Elsevier Inc. All rights reserved.","Agile decision making; Agile project management; Agile software development; Case study; Decision making; Decision obstacles; Focus group; Iteration decisions; Iteration Planning; Iteration Review; Retrospective; Scrum; Software engineering; Team decisions"
"High performance dynamic voltage/frequency scaling algorithm for real-time dynamic load management","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.284","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857373409&doi=10.1016%2fj.jss.2011.11.284&partnerID=40&md5=df6f8daed8b372d3c5c01221f9ddff55","Modern cyber-physical systems assume a complex and dynamic interaction between the real world and the computing system in real-time. In this context, changes in the physical environment trigger changes in the computational load to execute. On the other hand, task migration services offered by networked control systems require also management of dynamic real-time computing load in nodes. In such systems it would be difficult, if not impossible, to analyse off-line all the possible combinations of processor loads. For this reason, it is worthwhile attempting to define new flexible architectures that enable computing systems to adapt to potential changes in the environment. We assume a system composed by three main components: the first one is responsible of the management of the requests arisen when new tasks require to be executed. This management component asks to the second component about the resources available to accept the new tasks. The second component performs a feasibility analysis to determine if the new tasks can be accepted coping with its real-time constraints. A new processor speed is also computed. A third component monitors the execution of tasks applying a fixed priority scheduling policy and additionally controlling the frequency of the processor. This paper focus on the second component providing a ""correct"" (a task never is accepted if it is not schedulable) and ""near-exact"" (a task is rarely rejected if it is schedulable) algorithm that can be applicable in practice because its low/medium and predictable computational cost. The algorithm analyses task admission in terms of processor frequency scaling. The paper presents the details of a novel algorithm to analyse tasks admission and processor frequency assignment. Additionally, we perform several simulations to evaluate the comparative performance of the proposed approach. This evaluation is made in terms of energy consumption, task rejection ratios, and real computing costs. The results of simulations show that from the cost, execution predictability, and task acceptance points of view, the proposed algorithm mostly outperforms other constant voltage scaling algorithms. © 2011 Elsevier Inc. All rights reserved.","Dynamic voltage scaling; Feasibility analysis; Power consumption; Real-time scheduling; Task migration"
"Profiling all paths: A new profiling technique for both cyclic and acyclic paths","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862792002&doi=10.1016%2fj.jss.2012.01.046&partnerID=40&md5=3226f5a38f40c74c0eeed90f8110ead5","As an important technique in dynamic program analysis, path profiling collects the execution frequency of different paths, and has been widely used in a variety of areas. However, existing intra-procedural profiling techniques cannot effectively deal with loops, i.e.; they are limited in either working with acyclic paths, or with a small number of loop iteration. This paper presents a new profiling technique called PAP (Profiling All Paths), which can profile all finite-length paths within a procedure. PAP consists of two basic phases, the probe instrumentation phase which assigns a unique pathid to each path, and the backwalk phase which uses the pathids to determine the corresponding executed paths. Furthermore, breakpoints are introduced to store the probe value which may overflow during long executions, and the number of probes is reduced based on the integration of PAP with an existing profiling technique. From our case study and experiments, PAP is found to be effective and efficient in profiling both cyclic and acyclic paths. © 2012 Elsevier Inc. All rights reserved.","Breakpoint; Cyclic paths; Dynamic analysis; Path backwalk; Path profiling; Probe instrumentation"
"Validated templates for specification of complex LTL formulas","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861347039&doi=10.1016%2fj.jss.2012.02.041&partnerID=40&md5=3e32f3b7a68550a79b3fd06f588d2e9d","Formal verification approaches that check software correctness against formal specifications have been shown to improve program dependability. Tools such as Specification Pattern System (SPS) and Property Specification (Prospec) support the generation of formal specifications. SPS has defined a set of patterns (common recurring properties) and scopes (system states over which a pattern must hold) that allows a user to generate formal specifications by using direct substitution of propositions into parameters of selected patterns and scopes. Prospec extended SPS to support the definition of patterns and scopes that include the ability to specify parameters with multiple propositions (referred to as composite propositions or CPs), allowing the specification of sequential and concurrent behavior. Prospec generates formal specifications in Future Interval Logic (FIL) using direct substitution of CPs into pattern and scope parameters. While substitution works trivially for FIL, it does not work for Linear Temporal Logic (LTL), a highly expressive language that supports specification of software properties such as safety and liveness. LTL is important because of its use in the model checker Spin, the ACM 2001 system Software Award winning tool, and NuSMV. This paper introduces abstract LTL templates to support automated generation of LTL formulas for complex properties in Prospec. In addition, it presents formal proofs and testing to demonstrate that the templates indeed generate the intended LTL formulas. © 2012 Elsevier Inc. All rights reserved.","Composite propositions; Formal specifications; LTL; Model checking; Pattern; Scope"
"Cryptanalyzing a chaos-based image encryption algorithm using alternate structure","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863095177&doi=10.1016%2fj.jss.2012.04.002&partnerID=40&md5=483024f1eb346ee18c4d98a9fffdd36c","Recently, a chaos-based image encryption algorithm with an alternate structure (IEAS) was proposed. This paper applies the differential cryptanalysis on the IEAS and finds that some of its properties favor the differential attack which can recover an equivalent secret key with only a few number of chosen plain-images. Detailed procedures for cryptanalyzing IEAS with a lower round number are presented. Both theoretical analysis and experimental results are provided to show the vulnerability of IEAS against differential attack. In addition, some other security defects of IEAS, including insensitivity with respect to changes of plain-images and insufficient size of the key space, are also pointed out and verified. © 2012 Elsevier Inc. All rights reserved.","Chaos; Cryptanalysis; Differential attack; Image encryption"
"Technology flexibility as enabler of robust application development in community source: The case of Kuali and Sakai","2012","Journal of Systems and Software","10.1016/j.jss.2012.06.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867332389&doi=10.1016%2fj.jss.2012.06.026&partnerID=40&md5=da4ab1c2f1440ac0c89a45e7fcc3ec04","Technology flexibility has been an important topic in software engineering since the start of computerized business applications, which require frequent changes to system specifications due to ever changing business requirements. Achieving a higher degree of technology flexibility has been a long-running challenge to software engineers and project managers. Recently, there has been a new software development approach called ""community source"" consisting of numerous development partners that are also users of the software. In community source, technology flexibility is even more important than usual due to the increase in complexity and uncertainty of software requirements by its many development partners in the community. In this paper, we investigate two community source cases, i.e.; Kuali and Sakai, and examine how technology flexibility is achieved in application software engineering. The principles generated from this study should offer useful insights to the continuous efforts toward making more robust business applications in support of agile enterprises. © 2012 Elsevier Inc. All rights reserved.","Agile enterprise; Community source; Open source; Service oriented architecture; Technology flexibility; Workflow technology"
"Empirical findings on team size and productivity in software development","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857361268&doi=10.1016%2fj.jss.2011.09.009&partnerID=40&md5=247dabc541019a5c2ffd28673cd89a32","The size of software project teams has been considered to be a driver of project productivity. Although there is a large literature on this, new publicly available software repositories allow us to empirically perform further research. In this paper we analyse the relationships between productivity, team size and other project variables using the International Software Benchmarking Standards Group (ISBSG) repository. To do so, we apply statistical approaches to a preprocessed subset of the ISBSG repository to facilitate the study. The results show some expected correlations between productivity, effort and time as well as corroborating some other beliefs concerning team size and productivity. In addition, this study concludes that in order to apply statistical or data mining techniques to these type of repositories extensive preprocessing of the data needs to be performed due to ambiguities, wrongly recorded values, missing values, unbalanced datasets, etc. Such preprocessing is a difficult and error prone activity that would need further guidance and information that is not always provided in the repository. © 2011 Elsevier Inc. All rights reserved.","Effort estimation datasets; ISBSG repository; Productivity; Team size"
"Data embedding using pixel value differencing and diamond encoding with multiple-base notational system","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865766048&doi=10.1016%2fj.jss.2011.12.045&partnerID=40&md5=35ac9553df16a80f33cfb95d837d7e41","We propose a new data hiding method that adaptively embeds data into pixel pairs using the diamond encoding (DE) technique. Because the human eyes tolerate more changes in edge and texture areas than in smooth areas, and pixel pairs in these areas often possess larger differences, the method exploits pixel value differences (PVD) to estimate the base of digits to be embedded into pixel pairs. Pixel pairs with larger differences are embedded with digits in larger base than those pixel pairs with smaller differences to maximize the payload and image quality. Two sophisticated pixel pair adjustment processes are provided to maintain the division consistency and to eliminate the overflow/underflow problem. Experimental results reveal that the proposed method offers better embedding performance compared to prior PVD-based works in terms of payload and image quality. © 2012 Elsevier Inc. All rights reserved.","Data hiding; Diamond encoding; Pixel value differencing"
"Job allocation strategies for energy-aware and efficient Grid infrastructures","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861093495&doi=10.1016%2fj.jss.2012.01.050&partnerID=40&md5=c41ed5ca77468e823790a0c8a8cbd074","Complex distributed architectures, like Grid, supply effective platforms to solve computations on huge datasets, often at the cost of increased power consumption. This energy issue affects the sustainability of the infrastructures and increases their environmental impact. On the other hand, due to Grid heterogeneity and scalability, possible power savings could be achieved if effective energy-aware allocation policies were adopted. These policies are meant to implement a better coupling between application requirements and the Grid resources, also taking energy parameters into account. In this paper, we discuss different allocation strategies which address jobs submitted to Grid resources, subject to efficiency and energy constraints. Our aim is to analyze the potential benefits that can be obtained from the adoption of a metric able to capture both performance and energy-savings. Based on an experimental study, we simulated two alternative scenarios aimed at comparing the behavior of different strategies for allocating jobs to resources. Moreover we introduced the Performance/Energy Trade-off function as a useful means to evaluate the tendency of an allocation strategy toward efficiency or power consumption. Our conclusion seems to suggest that performance and energy-savings are not always enemies, and these objectives may be combined if suitable energy metrics are adopted. © 2012 Elsevier Inc. All rights reserved.","Allocation policies; Energy metrics; Energy/efficiency trade-off; Grid; Performance evaluation"
"A family of case studies on business process mining using MARBLE","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859532223&doi=10.1016%2fj.jss.2012.01.022&partnerID=40&md5=35e0e6d4f19ae2e2b0844b2c85a49a8a","Business processes, most of which are automated by information systems, have become a key asset in organizations. Unfortunately, uncontrolled maintenance implies that information systems age overtime until they need to be modernized. During software modernization, ageing systems cannot be entirely discarded because they gradually embed meaningful business knowledge, which is not present in any other artifact. This paper presents a technique for recovering business processes from legacy systems in order to preserve that knowledge. The technique statically analyzes source code and generates a code model, which is later transformed by pattern matching into a business process model. This technique has been validated over a two-year period in several industrial modernization projects. This paper reports the results of a family of case studies that were performed to empirically validate the technique using analysis and meta-analysis techniques. The family of case studies demonstrates that the technique is feasible in terms of effectiveness and efficiency. © 2012 Elsevier Inc. All rights reserved.","ADM; BPMN; Business process; Case study; KDM; Meta-analysis; Static analysis"
"Changing attitudes towards the generation of architectural models","2012","Journal of Systems and Software","10.1016/j.jss.2011.05.047","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857365900&doi=10.1016%2fj.jss.2011.05.047&partnerID=40&md5=0c122c811b78b5fe241d403dda9140bc","Architectural design is an important activity, but the understanding of how it is related to requirements modeling is rather limited. It is worth noting that goal orientation is an increasingly recognized paradigm for eliciting, modeling, specifying, and analyzing software requirements. However, it is not clear how goal models are related to architectural models. In this paper we present an approach based on model transformations to derive architectural structural specifications from system goals. The source and target languages are respectively the i* (iStar) modeling language and the Acme architectural description language. A real case study is used to show the feasibility of our approach. © 2011 Elsevier Inc. All rights reserved.","Architectural design; Model driven development; Model transformations; Requirements engineering"
"Dynamic refinement of search engines results utilizing the user intervention","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861098544&doi=10.1016%2fj.jss.2012.01.049&partnerID=40&md5=97049348b4351d0221f05bebc4744700","Nowadays, modern search engines quite satisfactorily answer users' queries, but the top results returned are not always relevant to the data the user is actually looking for. Hence, considerable efforts are made by search engines in order to rank the most relevant to the query results at the top. This work addresses the above problem and improves the performance of a search engine, especially when it comes to queries which have for example twofold meanings. The matter which the user is interested in is identified based on the results that he/she chooses, and then the most relevant ones are ranked higher. In addition, the results are recognized not only as text but also as semantic entities, which contain various semantic features. The semantic relation between results and text coverage are used as the main tool to achieve an optimized ranking, as opposed to other research papers so far. As a result, a new meta search application is developed, which, given a set of terms, combines Google results and then reorganizes (re-ranks) them based on the disambiguation offered by user clicks. In particular, after a ranking is achieved, the user makes a choice (click), the ranking is updated and the process is repeated. In order to prove our claims, apart from the description of the algorithm for refining the ranking of results, a web application has been developed, which was used to test the effectiveness of the system proposed. © 2012 Elsevier Inc. All rights reserved.","Dynamic refinement; Personalization; Post-ranking; Relevant feedback; Search engines; Semantic matching"
"A user-friendly secret image sharing scheme with reversible steganography based on cellular automata","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862789173&doi=10.1016%2fj.jss.2012.02.046&partnerID=40&md5=d8c4372e77a8cf765683dc7b3acc02bb","Secret image sharing is a mechanism to protect a secret image among a group of participants by encrypting the secret into shares and decrypting the secret with sufficient shares. Conventional schemes generate meaningless shares, which are hard to identify and lead to suspicion of secret image encryption. To overcome these problems, sharing schemes with steganography were presented. The meaningless shared data were embedded into the cover image to form stego images. However, distorted stego images cannot be reverted to original. In this work, a novel secret image sharing scheme with reversible steganography is proposed. Main contribution of this work is that two-dimensional reversible cellular automata with memory is utilized to encrypt a secret image into shared data, which are then embedded into cover image for forming stego images. By collecting sufficient stego images, not only the secret image is lossless reconstructed, but also distorted stego image is reverted to original. Simulation results shows that low computation cost and pleasing stego image quality are also achieved by the proposed scheme. © 2012 Elsevier Inc. All rights reserved.","Cellular automata; Distortion-free; Reversible steganography; Secret image sharing"
"Enabling correct design and formal analysis of Ambient Assisted Living systems","2012","Journal of Systems and Software","10.1016/j.jss.2011.05.022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857371236&doi=10.1016%2fj.jss.2011.05.022&partnerID=40&md5=4dc73722c698a8b0d129c54391674b10","Ambient Assisted Living (AAL) systems intend to provide services that enable people with specific needs to live an independent and safe life. Emergency treatment services are critical, time-constrained, and require compliance to numerous non-functional (or quality) requirements. In conventional approaches, often, non-functional requirements are kept outside the modeling scope and as such, their verification is also overlooked. For this reason, the specification and verification of Non-functional requirements (NFR) in this kind of services is a key issue. This paper presents a verification approach based on timed traces semantics and a methodology based on UML-RT models (MEDISTAM-RT) to check the fulfillment of non-functional requirements, such as timeliness and safety (deadlock freeness), and to assure the correct functioning of the AAL systems. We validate this approach by its application to an Emergency Assistance System for monitoring people suffering from cardiac alteration with syncope. © 2011 Elsevier Inc. All rights reserved.","AAL; Ambient Assisted Living; Formal methods; Time-constraints; Validation; Verification"
"Convex optimization framework for intermediate deadline assignment in soft and hard real-time distributed systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863618608&doi=10.1016%2fj.jss.2012.04.050&partnerID=40&md5=36b7c9b6d9bebbababc315563f051fab","It is generally challenging to determine end-to-end delays of applications for maximizing the aggregate system utility subject to timing constraints. Many practical approaches suggest the use of intermediate deadline of tasks in order to control and upper-bound their end-to-end delays. This paper proposes a unified framework for different time-sensitive, global optimization problems, and solves them in a distributed manner using Lagrangian duality. The framework uses global viewpoints to assign intermediate deadlines, taking resource contention among tasks into consideration. For soft real-time tasks, the proposed framework effectively addresses the deadline assignment problem while maximizing the aggregate quality of service. For hard real-time tasks, we show that existing heuristic solutions to the deadline assignment problem can be incorporated into the proposed framework, enriching their mathematical interpretation. © 2011 Elsevier Inc. All rights reserved.","Convex optimization; Hard real-time distributed systems; Intermediate deadline assignment; Soft real-time distributed systems"
"SEProf: A high-level software energy profiling tool for an embedded processor enabling power management functions","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861345987&doi=10.1016%2fj.jss.2012.03.027&partnerID=40&md5=d70ba3cbfc9c707e24898fe8c80b150f","Energy efficiency has become one of the most important design issues for embedded systems. To examine the power consumption of an embedded system, an energy profiling tool is highly demanded. Although a number of energy profiling tools have been proposed, they are not directly applicable to the embedded processors with power management functions that are widely utilized in battery-operated embedded systems to reduce power consumption. Hence, this study presents a high-level energy profiling tool, called SEProf, that estimates the energy consumption of an embedded system running multithread software and a multitasking operating system (OS) that supports power management functions. This study implements the proposed SEProf in Linux 2.6.19 and evaluates its performance on an ARM11 MPCore processor. Experimental results demonstrate that the proposed tool can provide accurate energy profiling results with a low profiling overhead. © 2012 Elsevier Inc. All rights reserved.","Embedded processor; Energy profiling; Power consumption; Power management"
"Utilizing Layered Taxation to provide incentives in P2P streaming systems","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861344536&doi=10.1016%2fj.jss.2012.03.029&partnerID=40&md5=f889e31c914b88440b156cee0c22180d","Incentive mechanism plays an essential role in guaranteeing the performance of P2P streaming systems. This paper proposed a Credit-line based Layered Taxation (CLT) incentive mechanism, which is fully distributed and does not rely on any central server for credit management. The CLT mechanism also avoids the problem of reputation accumulation in reputation-based systems and trade limitation in the reciprocity-based systems. In addition, the formed layered topology and taxation strategy can inspire peers to contribute their bandwidth, and maximize both individual utility and system utility. Our simulation results indicate that, with our proposed incentive mechanism, cooperative peers can gain much better performance, while free riders would be cleared out of the system. © 2012 Elsevier Inc. All rights reserved.","Credit-line; Incentive mechanism; Live streaming; P2P"
"Attribute-based strong designated-verifier signature scheme","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857372443&doi=10.1016%2fj.jss.2011.11.1008&partnerID=40&md5=2abc3ba6d0fe56f4efd899825ef7db0e","In a strong designated-verifier signature scheme, only the verifier who is designated by a signer can check the validity of a signature via her/his private keys, which process contrasts with the public verifiability of a typical signature scheme. In applications in multi-user communication environments, a user may want to sign a document such that only some specified parties can confirm the signature. To do so, the signer can generate many designated-verifier signatures on a document for various designated verifiers, or she/he signs the document, encrypts the signature, and then sends the encrypted messages to the specified parties. However, the signer has to generate multiple signatures, as determined by the numbers of the designated verifiers in the former method and the latter method, signs a document and then encrypts it, that could lose their designation (which phenomenon is called source hiding). To solve the above problems, this study proposes an attribute-based strong designated-verifier signature scheme with source hiding, such that a signer needs to generate only one signature for a group of designated verifiers with attributes at specified values. To the best of the authors' knowledge, no attribute-based strong designated-verifier signature scheme has been formally presented before. © 2011 Elsevier Inc. All rights reserved.","Attribute-based signatures; Bilinear pairings; Cryptography; Designated-verifier signatures; Information security"
"Investigating intentional distortions in software cost estimation - An exploratory study","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861341260&doi=10.1016%2fj.jss.2012.03.026&partnerID=40&md5=821ed1badc262068705dc863756c1d93","Cost estimation of software projects is an important activity that continues to be a source of problems for practitioners despite improvement efforts. Most of the research on estimation has focused on methodological issues while the research focused on human factors primarily has targeted cognitive biases or perceived inhibitors. This paper focuses on the complex organizational context of estimation and investigates whether estimates may be distorted, i.e. intentionally changed for reasons beyond legitimate changes due to changing prerequisites such as requirements or scope. An exploratory study was conducted with 15 interviewees at six large companies that develop software-intensive products. The interviewees represent five stakeholder roles in estimation, with a majority being project or line managers. Document analysis was used to complement the interviews and provided additional context. The results show that both estimate increase and estimate decrease exist and that some of these changes can be explained as intentional distortions. The direction of the distortion depends on the context and the stakeholders involved. The paper underlines that it is critical to consider also human and organizational factors when addressing estimation problems and that intentional estimate distortions should be given more and direct attention. © 2012 Elsevier Inc. All rights reserved.","Cost estimation; Distortion; Empirical study; Estimation inaccuracy; Human factors; Organizational factors; Organizational politics; Software engineering"
"A loss recovery approach for reliable application layer multicast","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865776369&doi=10.1016%2fj.jss.2012.01.021&partnerID=40&md5=86fff422c3f864f46bd3554b50c3f615","In the application layer multicast (ALM), providing reliable (i.e., lossless) data delivery service is complex. This paper proposes a hierarchical loss recovery solution, called HR, for reliable application layer multicast. HR can be used as an extension to existing tree-based ALM protocols, to provide lossless ALM service. In the HR solution, the group members at the top of the ALM tree are in a recovery plane (i.e., Plane 1), and the members at the bottom of the ALM tree are in another recovery plane (i.e., Plane 2). HR employs a robust and quick approach to recover the losses at the group members in Plane 1, which can avoid the potential losses at the downstream nodes and provide many relatively reliable recovery sources. HR uses a loss recovery approach, which can effectively reduce the link load, to recover the loss found by the group member in Plane 2. In HR, the recovery packet is retransmitted by an area-constrained multicast means, which can recover the loss at one or multiple group members with little recovery diffusion. Through a cooperative and active way, HR can effectively address the error correlation problem of ALM. © 2012 Elsevier Inc. All rights reserved.","Application layer multicast; Data loss; Recovery; Retransmission"
"Mitigating starvation of Linux CPU-bound processes in the presence of network I/O","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861341421&doi=10.1016%2fj.jss.2012.02.042&partnerID=40&md5=ec993a5f39c03f4ac1a066da8b195cb6","In prior research work, it has been demonstrated that Linux can starve CPU-bound processes in the presence of network I/O. The starvation of Linux CPU-bound processes occurs under the two Linux schedulers, namely the 2.6 O(1) scheduler and the more recent 2.6 Completely Fair Scheduler (CFS). In this paper, we analyze the underlying root causes of this starvation problem and we propose effective solutions that can mitigate such starvation. We present detailed implementations of our proposed solutions for both O(1) and CFS Linux schedulers. We empirically evaluate the effectiveness of our proposed solutions in terms of execution time and incoming traffic load. For our experimental study and analysis, we consider two types of mainboard architectures: Uni-Processing (UP) and Symmetric Multi-Processing (SMP). Our empirical results show that the proposed solutions are highly effective in mitigating the starvation problem for CPU-bound processes with no negative impact on the performance of network I/O-bound processes. © 2012 Elsevier Inc. All rights reserved.","CPU scheduler; CPU-bound processes; Linux; Network I/O; Operating system; Performance; Starvation"
"Decision tree classifiers sensitive to heterogeneous costs","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857357329&doi=10.1016%2fj.jss.2011.10.007&partnerID=40&md5=e771171e6391c60bd5cce1bccfb00969","There are nine major types of cost involved in cost-sensitive learning that are with heterogeneous units in general, referred to heterogeneous costs. Extant cost-sensitive learning (CSL) algorithms are based on the assumption that all involved costs can be transformed into a unified unit, called as homogeneous assumption of costs. While it is a challenge to construct many suitable transformation functions for the costs with diverse units, this paper designs a heterogeneous-cost sensitive learning (HCSL) algorithm to make split attribute selection more effective. This paper first proposes an efficient method of reducing the heterogeneity caused by both cost mechanisms and attribute information. And then, all heterogeneous costs with attribution information together are incorporated into the process of split attribute selection, called as HCAI-based split attribute selection. Third, the over-fitting is tackled by designing a simple and effective smoothing strategy, so as to build cost-sensitive decision tree classifiers with the HCSL algorithm. Experiments are conducted to evaluate the proposed HCSL algorithm on six UCI datasets. Experimental results show that the proposed approach outperforms existing methods for handling the heterogeneity caused by cost mechanisms and attribute information. © 2011 Elsevier Inc. All rights reserved.","Cost-sensitive learning; Decision tree classification; Heterogeneous attributes; Heterogeneous costs; Split attribute selection"
"Strongly secure certificateless short signatures","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859548489&doi=10.1016%2fj.jss.2012.01.016&partnerID=40&md5=1c5025244f7d62abd49127f06f934beb","Short certificateless signatures have come into limelight in recent years. On the one hand, the property of certificateless eliminates the certificate management problem in traditional PKI and the key-escrow problem in some ID-based signature schemes. On the other hand, due to the short signature length, short certificateless signatures can be applied to systems where signatures are typed in by human or systems with low-bandwidth channels and/or low-computation power, such as PDAs or cell phones. However, there has been a trade-off between short certificateless signature schemes and their security levels. All existing short certificateless signature schemes can only be proven secure against a normal type adversary rather than a stronger one, who can obtain valid certificateless signatures under public keys replaced by the adversary. In this paper, we solve this open problem by given an efficient strongly secure short certificateless signature scheme. The proposed scheme has the following features. Firstly, it is strongly unforgeable. Secondly, the security can be reduced to the Computational Diffie-Hellman (CDH) assumption - a classic complexity assumption. Lastly, the proposed scheme is provably secure against adversaries with access to a super signing oracle which generates valid certificateless signatures of messages and public keys chosen by the adversary (without providing the corresponding secret values). © 2012 Elsevier Inc. All rights reserved.","Bilinear pairing; Certificateless signature; Random oracle; Short signature; Strongly secure"
"Organizational adoption of open source software","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857358007&doi=10.1016%2fj.jss.2011.09.037&partnerID=40&md5=f9df6fde2d18e35d02e8d6c877caa0e9","Organizations and individuals can use open source software (OSS) for free, they can study its internal workings, and they can even fix it or modify it to make it suit their particular needs. These attributes make OSS an enticing technological choice for a company. Unfortunately, because most enterprises view technology as a proprietary differentiating element of their operation, little is known about the extent of OSS adoption in industry and the key drivers behind adoption decisions. In this article we examine factors and behaviors associated with the adoption of OSS and provide empirical findings through data gathered from the US Fortune-1000 companies. The data come from each company's web browsing and serving activities, gathered by sifting through more than 278 million web server log records and analyzing the results of thousands of network probes. We show that the adoption of OSS in large US companies is significant and is increasing over time through a low-churn transition, advancing from applications to platforms. Its adoption is a pragmatic decision influenced by network effects. It is likelier in larger organizations and those with many less productive employees, and is associated with IT and knowledge-intensive work and operating efficiencies. © 2011 Elsevier Inc. All rights reserved.","Industrial practice; Open source software; Technology adoption"
"Comparison of scheduling schemes for on-demand IaaS requests","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859521799&doi=10.1016%2fj.jss.2012.01.019&partnerID=40&md5=e8f54e7a6bd451f79dc0d0253b393040","Infrastructure-as-a-service (IaaS) is one of emerging powerful cloud computing services provided by IT industry at present. This paper considers the interaction aspects between on-demand requests and the allocation of virtual machines in a server farm operated by a specific infrastructure owner. We formulate an analytic performance model of the server farm taking into account the quality of service (QoS) guaranteed to users and the operational energy consumption in the server farm. We compare several scheduling algorithms from the aspect of the average energy consumption and heat emission of servers as well as the blocking probabilities of on-demand requests. Based on numerical results of a comparison of different allocation strategies, a saving on the energy consumption is possible in the operational range (where on-demand requests do not face unpleasant blocking probability) with the allocation of virtual machines to physical servers based on the priority. © 2012 Elsevier Inc. All rights reserved.","Cloud computing; On-demand IaaS; Scheduling"
"Distributed goal-oriented computing","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861092822&doi=10.1016%2fj.jss.2012.01.045&partnerID=40&md5=f677e63e079ed838aa64e8eb4aaa38d9","For current computing frameworks, the ability to dynamically use the resources that are allocated in the network has become a key success factor. As long as the size of the network increases, it is more difficult to find how to solve the problems that the users are presenting. Users usually do know what they want to do, but they do not know how to do it. If the user knows its goals it could be easier to help him with a different approach. In this work we present a new computing paradigm based on goals. This paradigm is called Distributed goal-oriented computing paradigm. To implement this paradigm an execution framework for a goal-oriented operating system has been designed. In this paradigm users express their goals and the OS is in charge of helping the achievement of these goals by means of a service-oriented approach. © 2012 Elsevier Inc. All rights reserved.","Adaptive systems; Computing paradigms; Goal-oriented systems; Multi-agent systems; Operating systems; Service-oriented systems"
"Shades of gray: Opening up a software producing organization with the open software enterprise model","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861097506&doi=10.1016%2fj.jss.2011.12.007&partnerID=40&md5=736ecf483b2c4bd0e736af699519e265","Software producing organizations are frequently judged by others for being 'open' or 'closed', where a more 'closed' organization is seen as being detrimental to its software ecosystem. These qualifications can harm the reputation of these companies, for they are deemed to promote vendor lock-in, use closed data formats, and are seen as using intellectual property laws to harm others. These judgements, however, are frequently based on speculation and the need arises for a method to establish openness of an organization, such that decisions are no longer based on prejudices, but on an objective assessment of the practices of a software producing organization. In this article the open software enterprise model is presented that enables one to establish the degree of openness of a software producing organization. The model has been evaluated in five interviews, is illustrated using three case studies, and shows that organizational openness and transparency are complex variables, that should not be determined based on belief or prejudice. Furthermore, the model can be used by software producing organizations as a reference for further opening up their business, to stimulate the surrounding software ecosystem, and further their business goals. © 2012 Elsevier Inc. All rights reserved.","Organizational openness; Platforms; Product software vendors; Software development governance; Software ecosystems"
"Sharetouch: A system to enrich social network experiences for the elderly","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862789341&doi=10.1016%2fj.jss.2012.01.023&partnerID=40&md5=2c4fcba95439e3defd7dd064479debab","The Sharetouch system is designed for raising users' participation in community events. We put three subsystems into Sharetouch: (1) community pond, (2) Waterball interactive game, and (3) multimedia sharing. Sharetouch is based on an optical touch device designed by the Joyplux Company with an infrared LED and camera. This device can support multi-touch functions within a large display area. The software of Sharetouch was developed within XNA and.NET frameworks. We project the users as fish in our community pond. Sharetouch displays all the friends as fish when the users log into the system. Therefore, the number of fish equals the number of friends of the users. This design encourages users to make more friends to increase the number of fish. Waterball is a game that combines virtual images and real objects. The concept is based on the Nintendo Wii games, as players hold controllers (real objects) to play the games (virtual images). We also apply the concept of the cloud flash drive to multimedia sharing to avoid the trouble of carrying a real flash disk. This study employed the TAM measure to measure the validity of Sharetouch in this social platform. Our findings indicated that all proposed hypotheses had a positive and significant impact on the intention of older people to interact with Sharetouch. Unlike the computer-based system, Sharetouch is created as a user-friendly interface system. Sharetouch can enrich the users' social network experiences through its hardware and software architectures. © 2012 Elsevier Inc. All rights reserved.","Community pond; Multimedia sharing; Sharetouch; Technology Acceptance Model; Touch panel; Waterball"
"Towards supporting the software architecture life cycle","2012","Journal of Systems and Software","10.1016/j.jss.2011.05.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857369375&doi=10.1016%2fj.jss.2011.05.036&partnerID=40&md5=5d8d3245480aa42551771de8abc37e71","Software architecture is a central element during the whole software life cycle. Among other things, software architecture is used for communication and documentation, for design, for reasoning about important system properties, and as a blueprint for system implementation. This is expressed by the software architecture life cycle, which emphasizes architecture-related activities like architecture design, implementation, and analysis in the context of a software life cycle. While individual activities of the software architecture life cycle are supported very well, a seamless approach for supporting the whole life cycle is still missing. Such an approach requires the integration of disparate information, artifacts, and tools into one consistent information model and environment. In this article we present such an approach. It is based on a semi-formal architecture model, which is used in all activities of the architecture life cycle, and on a set of extensible and integrated tools supporting these activities. Such an integrated approach provides several benefits. Potentially redundant activities like the creation of multiple architecture descriptions are avoided, the captured information is always consistent and up-to-date, extensive tracing between different information is possible, and interleaving activities in incremental development and design are supported. © 2011 Elsevier Inc. All rights reserved.","Software architecture; Software architecture analysis; Software architecture design; Software architecture evaluation; Software architecture knowledge management; Software architecture life cycle; Software architecture model; Software architecture tools"
"Malware characteristics and threats on the internet ecosystem","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861096936&doi=10.1016%2fj.jss.2012.02.015&partnerID=40&md5=6045d97b11dcacf9c1c4524be3122737","Malware encyclopedias now play a vital role in disseminating information about security threats. Coupled with categorization and generalization capabilities, such encyclopedias might help better defend against both isolated and clustered specimens.In this paper, we present Malware Evaluator, a classification framework that treats malware categorization as a supervised learning task, builds learning models with both support vector machines and decision trees and finally, visualizes classifications with self-organizing maps. Malware Evaluator refrains from using readily available taxonomic features to produce species classifications. Instead, we generate attributes of malware strains via a tokenization process and select the attributes used according to their projected information gain. We also deploy word stemming and stopword removal techniques to reduce dimensions of the feature space. In contrast to existing approaches, Malware Evaluator defines its taxonomic features based on the behavior of species throughout their life-cycle, allowing it to discover properties that previously might have gone unobserved. The learning and generalization capabilities of the framework also help detect and categorize zero-day attacks. Our prototype helps establish that malicious strains improve their penetration rate through multiple propagation channels as well as compact code footprints; moreover, they attempt to evade detection by resorting to code polymorphism and information encryption. Malware Evaluator also reveals that breeds in the categories of Trojan, Infector, Backdoor, and Worm significantly contribute to the malware population and impose critical risks on the Internet ecosystem. © 2012 Elsevier Inc. All rights reserved.","Malware characteristics and categorization; Malware propagation mechanisms and payloads; Self-organizing maps; Support vector machines"
"Factors affecting the success of Open Source Software","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857365211&doi=10.1016%2fj.jss.2011.11.010&partnerID=40&md5=ef2152279f14c5db384892fe9cc048e1","With the rapid rise in the use of Open Source Software (OSS) in all types of applications, it is important to know which factors can lead to OSS success. OSS projects evolve and transform over time; therefore success must be examined longitudinally over a period of time. In this research, we examine two measures of project success: project popularity and developer activity, of 283 OSS projects over a span of 3 years, in order to observe changes over time. A comprehensive research model of OSS success is developed which includes both extrinsic and intrinsic attributes. Results show that while many of the hypothesized relationships are supported, there were marked differences in some of the relationships at different points in time lending support to the notion that different factors need to be emphasized as the OSS project unfolds over time. © 2011 Elsevier Inc. All rights reserved.","Extrinsic cues; Intrinsic cues; Longitudinal effects; Open Source Software; System success"
"Deriving detailed design models from an aspect-oriented ADL using MDD","2012","Journal of Systems and Software","10.1016/j.jss.2011.05.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857356574&doi=10.1016%2fj.jss.2011.05.026&partnerID=40&md5=9c96febc134724caedb32313ceb081c8","Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the ""aspects"" identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented UML 2.0 models, closing the gap between ADLs and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to UML 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level. © 2011 Elsevier Inc. All rights reserved.","AO-ADL; Aspect-oriented software development; ATL; Model-driven development; Software architectures; Theme/UML; UML 2.0"
"Further observation on proxy re-encryption with keyword search","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857359203&doi=10.1016%2fj.jss.2011.09.035&partnerID=40&md5=8e71e05556a7b8ef4a0b6484d181dbf4","Recently Shao et al. proposed an interesting cryptographic primitive called proxy re-encryption with keyword search (PRES). The main novelty is simultaneously realizing the functionality of proxy re-encryption and keyword search in one primitive. In this paper, we further extend their research by introducing a new primitive: constrained single-hop unidirectional proxy re-encryption supporting conjunctive keywords search (CPRE-CKS). Our results are as following: (1) In Shao's PRES scheme, the proxy can re-encrypt all the second level ciphertext. While in our CPRE-CKS proposal, the proxy can only re-encrypt those second level ciphertexts which contain the corresponding keywords. (2) We give the definition and security model for CPRE-CKS, and propose a concrete scheme and prove its security. (3) On the way to construct a secure CPRE-CKS scheme, we found a flaw in the security proof of Hwang et al.'s public key encryption with conjunctive keyword search (PECK) scheme proposed in Pairing'07. © 2011 Elsevier Inc. All rights reserved.","Constrained single- hopunidirectional proxy re-encryption supporting conjunctive keywordssearch; Proxy re-encryption with keyword search; Public key encryption with conjunctive keyword search"
"The novel bilateral - Diffusion image encryption algorithm with dynamical compound chaos","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.051","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857359202&doi=10.1016%2fj.jss.2011.10.051&partnerID=40&md5=4c934554e76e692b39df649056902736","Chaos may be degenerated because of the finite precision effect, hence the new compound two-dimensional chaotic function is presented by exploiting two one-dimensional chaotic functions which are switched randomly. A new chaotic sequence generator is designed by the compound chaos which is proved by Devaney's definition of chaos. The properties of dynamical compound chaotic functions and LFSR are also proved rigorously. A novel bilateral-diffusion image encryption algorithm is proposed based on dynamical compound chaotic function and LFSR, which can produce more avalanche effect and more large key space. The entropy analysis, differential analysis, statistical analysis, cipher random analysis, and cipher sensitivity analysis are introduced to test the security of new scheme. Many experiment results show that the novel image encryption method passes SP 800-22 and DIEHARD standard tests and solves the problem of short cycle and low precision of one-dimensional chaotic function. © 2011 Elsevier Inc. All rights reserved.","Bilateral-diffusion; Dynamical compound chaos; Image encryption; LFSR"
"Perpetual development: A model of the Linux kernel life cycle","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857365600&doi=10.1016%2fj.jss.2011.10.050&partnerID=40&md5=dbce46402622372142bf50336c908af4","Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory with intermittent releases. Nevertheless there have been only few lifecycle models that attempt to portray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating between development of new functionality and maintenance of production versions. A unique element of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before the forking of a new development version, which was prominent in early releases of production versions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x versions. We also show that a piecewise linear model with increasing slopes provides the best description of the growth of Linux. The perpetual development model is used as a framework in which commonly recognized benefits of incremental and evolutionary development may be demonstrated, and to comment on issues such as architecture, conservation of familiarity, and failed projects. We suggest that this model and variants thereof may apply to many other projects in addition to Linux. © 2011 Elsevier Inc. All rights reserved.","Linux kernel; Maintenance; Software evolution; Software release"
"Bridging the gap between requirements and design: An approach based on Problem Frames and SysML","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366776&doi=10.1016%2fj.jss.2011.09.046&partnerID=40&md5=de26000006faa9f01739b17eb9de518d","The relation between the requirements specification and the design has been widely investigated with the aim to bridge the gap between the two artifacts. The goal is to find effective mechanisms to generate the system design starting from the analysis and specification of the requirements. This paper contributes to this research stream with an approach to create early design models from requirement artifacts. The approach weaves together the analysis and design phases favoring a tight collaboration between analysts and designers. It is based on Problem Frames, decomposition and re-composition patterns and supported by the System Modeling Language. The proposed solution has the potentiality of easing the development, shortening the development cycle and reducing the associated cost. The proposed design generation guidelines have been implemented as ATLAS Transformation Language rules in a model-based transformation process. The entire approach is model driven, allowing for the generation of the design model through transformations applied to the requirements model. The design model is automatically generated through the application of the transformation rules described in the paper. The proposed rules are fairly general and can be applied to any analysis model built according to the proposed analysis guidelines. The transformation process can be easily re-implemented using any suitable modeling tool that includes the ATLAS Transformation Language interpretation engine. © 2011 Elsevier Inc. All rights reserved.","Architectural patterns; ATLAS Transformation Language; Blackboard; Decomposition criteria; Model based transformations; Problem Frames; System Modeling Language"
"Octopus: An Upperware based system for building personal pervasive environments","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861098326&doi=10.1016%2fj.jss.2012.02.011&partnerID=40&md5=1802a69e910f6952452c5f90678ecebc","As of today, there is no operating system suitable for pervasive computing. Such system must integrate and coordinate heterogeneous devices and systems but, at the same time, it should provide a single system image to let the user feel that there is only a single ""pervasive"" computing environment. Such illusion must consider the Internet as the system backbone, because users move. The challenge is providing a novel system while permitting the seamless integration of traditional legacy systems, which may be required to run on many computers and devices, if only to run their applications. We argue that to build such a system, we should abandon Middleware and use a different technology, that we call Upperware. To back up our claim, we have built an actual system using Upperware: the Octopus. The Octopus has been in use for several years both to build pervasive applications like smart spaces and to provide a general-purpose computing environment. We have been using it through wide area networks, on a daily basis. In this paper we discuss the Upperware approach and present the Octopus as an actual system built out of Upperware, including some evaluation results. © 2012 Elsevier Inc. All rights reserved.","Operating systems; Pervasive computing; Smart spaces"
"Rolling-horizon scheduling for energy constrained distributed real-time embedded systems","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857359161&doi=10.1016%2fj.jss.2011.10.008&partnerID=40&md5=cc619ecd2d97f1e98724a0008321a732","Energy-efficient scheduling approaches are critical to battery driven real-time embedded systems. Traditional energy-aware scheduling schemes are mainly based on the individual task scheduling. Consequently, the scheduling space for each task is small, and the schedulability and energy saving are very limited, especially when the system is heavily loaded. To remedy this problem, we propose a novel rolling-horizon (RH) strategy that can be applied to any scheduling algorithm to improve schedulability. In addition, we develop a new energy-efficient adaptive scheduling algorithm (EASA) that can adaptively adjust supply voltages according to the system workload for energy efficiency. Both the RH strategy and EASA algorithm are combined to form our scheduling approach, RH-EASA. Experimental results show that in comparison with some typical traditional scheduling schemes, RH-EASA can achieve significant energy savings while meeting most task deadlines (namely, high schedulability) for distributed real-time embedded systems with dynamic workloads. © 2011 Elsevier Inc. All rights reserved.","Dynamic scheduling; Embedded systems; Energy-efficient; Rolling-horizon"
"Automated, highly-accurate, bug assignment using machine learning and tossing graphs","2012","Journal of Systems and Software","10.1016/j.jss.2012.04.053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863612007&doi=10.1016%2fj.jss.2012.04.053&partnerID=40&md5=c83165d108c3ad414a98abd972fecb6c","Empirical studies indicate that automating the bug assignment process has the potential to significantly reduce software evolution effort and costs. Prior work has used machine learning techniques to automate bug assignment but has employed a narrow band of tools which can be ineffective in large, long-lived software projects. To redress this situation, in this paper we employ a comprehensive set of machine learning tools and a probabilistic graph-based model (bug tossing graphs) that lead to highly-accurate predictions, and lay the foundation for the next generation of machine learning-based bug assignment. Our work is the first to examine the impact of multiple machine learning dimensions (classifiers, attributes, and training history) along with bug tossing graphs on prediction accuracy in bug assignment. We validate our approach on Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 86.09 prediction accuracy in bug assignment and significantly reduce tossing path lengths. We show that for our data sets the Naïve Bayes classifier coupled with product-component features, tossing graphs and incremental learning performs best. Next, we perform an ablative analysis by unilaterally varying classifiers, features, and learning model to show their relative importance of on bug assignment accuracy. Finally, we propose optimization techniques that achieve high prediction accuracy while reducing training and prediction time. © 2011 Elsevier Inc. All rights reserved.","Bug assignment; Bug tossing; Empirical studies; Machine learning"
"An efficient and secure multi-server authentication scheme with key agreement","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366012&doi=10.1016%2fj.jss.2011.10.049&partnerID=40&md5=02d93eaf762ca430db2d47cf6ad9bb14","Remote user authentication is used to validate the legitimacy of a remote log-in user. Due to the rapid growth of computer networks, many network environments have been becoming multi-server based. Recently, much research has been focused on proposing remote password authentication schemes based on smart cards for securing multi-server environments. Each of these schemes used either a nonce or a timestamp technique to prevent the replay attack. However, using the nonce technique to withstand the replay attack is potentially susceptible to the man-in-the-middle attack. Alternatively, when employing the timestamp method to secure remote password authentication, it will require the cost of implementing clock synchronization. In order to solve the above two issues, this paper proposes a self-verified timestamp technique to help the smart-card-based authentication scheme not only effectively achieve password-authenticated key agreement but also avoid the difficulty of implementing clock synchronization in multi-server environments. A secure authenticated key agreement should accomplish both mutual authentication and session key establishment. Therefore, in this paper we further give the formal proof on the execution of the proposed authenticated key agreement scheme. © 2011 Elsevier Inc. All rights reserved.","Authentication; Computer networks; Information security; Key exchange; Smart card"
"Propagating changes between aligned process models","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861345967&doi=10.1016%2fj.jss.2012.02.044&partnerID=40&md5=24a53a833abca4764cdc972701a108d6","There is a wide variety of drivers for business process modelling initiatives, reaching from organisational redesign to the development of information systems. Consequently, a common business process is often captured in multiple models that overlap in content due to serving different purposes. Business process management aims at flexible adaptation to changing business needs. Hence, changes of business processes occur frequently and have to be incorporated in the respective process models. Once a process model is changed, related process models have to be updated accordingly, despite the fact that those process models may only be loosely coupled. In this article, we introduce an approach that supports change propagation between related process models. Given a change in one process model, we leverage the behavioural abstraction of behavioural profiles for corresponding activities in order to determine a change region in another model. Our approach is able to cope with changes in pairs of models that are not related by hierarchical refinement and show behavioural inconsistencies. We evaluate the applicability of our approach with two real-world process model collections. To this end, we either deduce change operations from different model revisions or rely on synthetic change operations. © 2012 Elsevier Inc. All rights reserved.","Behavioural analysis; Change propagation; Model synchronisation; Process model alignment"
"Wireless sensor network-based fire detection, alarming, monitoring and prevention system for Bord-and-Pillar coal mines","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.015","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857355887&doi=10.1016%2fj.jss.2011.09.015&partnerID=40&md5=7be31d032efb7f850940663014555f26","Fire is a major concern for those who work in underground coal mines. Coal mine fire can occur at any time and often results in partial or total evacuation of mine personnel and could result in the loss of lives. Therefore, having a warning system that is capable of detecting fire and generating an alarm is important. In this paper, we present the response time of a proposed system for detecting fire hazard in a Bord-and-Pillar coal mine panel (Hustrulid and Bullock, 2001). It uses wireless sensor networks (WSNs), and can be used to detect the exact fire location and spreading direction, and also provide the fire prevention system to stop the spread of fire to save the natural resources and the mining personnel from fire. The proposed system is capable of early detection of fire and generating alarm in case of emergencies. The performance of the proposed system has been evaluated through rigorous simulations. Simulation results show that the average network delay varies almost linearly with the increasing the number of hops. © 2011 Elsevier Inc. All rights reserved.","Coal mine; Fire detection; Simulation; Wireless sensor networks"
"Fully CCA2 secure identity-based broadcast encryption with black-box accountable authority","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857361360&doi=10.1016%2fj.jss.2011.09.045&partnerID=40&md5=961eefdf68278e4c4c704365647ca0ca","The concept of accountable authority identity-based encryption was introduced as a convenient tool to reduce the amount of trust in authorities in identity-based encryption. In this model, if the Private Key Generator (PKG) maliciously re-distributes users' decryption keys, it runs the risk of being caught and prosecuted. Libert and Vergnaud proposed an accountable authority identity-based broadcast encryption, which allows white-box tracing or weak black-box tracing. Their scheme was proved only secure in selective-ID model. We present a weak black-box accountable authority identity-based broadcast encryption scheme, which is proven as fully CCA2 secure against adaptive adversary with tight reduction. Our scheme achieves O(m) public keys size, O(m) private keys size, and O(1) ciphertext length, where m is the maximum number of receivers allowed in each broadcast. © 2011 Elsevier Inc. All rights reserved.","Accountability; Black-box traceability; Identity-based broadcast encryption"
"A Self-adaptive hierarchical monitoring mechanism for Clouds","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865788285&doi=10.1016%2fj.jss.2011.11.1043&partnerID=40&md5=1f99cfeafde1100421c3729b8a65f361","While Cloud computing offers the potential to dramatically reduce the cost of software services through the commoditization of IT assets and on-demand usage patterns, one has to consider that Future Internet applications raise the need for environments that can facilitate real-time and interactivity and thus pose specific requirements to the underlying infrastructure. The latter, should be able to efficiently adapt resource provisioning to the dynamic Quality of Service (QoS) demands of such applications. To this direction, in this paper we present a monitoring system that facilitates on-the-fly self-configuration in terms of both the monitoring time intervals and the monitoringparameters.The proposed approach forms a multi-layered monitoring framework for measuring QoS at both application and infrastructure levels targeting trigger events for runtime adaptability of resource provisioning estimation and decision making. Besides, we demonstrate the operation of the implemented mechanism and evaluate its effectiveness using a real-world application scenario, namely Film Post Production. © 2011 Elsevier Inc. All rights reserved.","Cloud computing; Monitoring; Quality of Service; Service oriented architecture"
"Using Pig as a data preparation language for large-scale mining software repositories studies: An experience report","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863618901&doi=10.1016%2fj.jss.2011.07.034&partnerID=40&md5=93bec8254185202c207931f798c053ab","The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses. © 2011 Elsevier Inc. All rights reserved.","MapReduce; Mining Software Repositories; Pig; Software engineering"
"Preserving knowledge in software projects","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863615756&doi=10.1016%2fj.jss.2012.03.028&partnerID=40&md5=b2a4e96e86ccfa1e1fbaa03751a9e7ce","Up-to-date preservation of project knowledge like developer communication and design documents is essential for the successful evolution of software systems. Ideally, all knowledge should be preserved, but since projects only have limited resources, and software systems continuously grow in scope and complexity, one needs to prioritize the subsystems and development periods for which knowledge preservation is more urgent. For example, core subsystems on which the majority of other subsystems build are obviously prime candidates for preservation, yet if these subsystems change continuously, picking a development period to start knowledge preservation and to maintain knowledge for over time become very hard. This paper exploits the time dependence between code changes to automatically determine for which subsystems and development periods of a software project knowledge preservation would be most valuable. A case study on two large open source projects (PostgreSQL and FreeBSD) shows that the most valuable subsystems to preserve knowledge for are large core subsystems. However, the majority of these subsystems (1) are continuously foundational, i.e., ideally for each development period knowledge should be preserved, and (2) experience substantial changes, i.e., preserving knowledge requires substantial effort. © 2011 Elsevier Inc. All rights reserved.","Documentation; Empirical analysis; Knowledge preservation; Mining software repositories; Software maintenance"
"Improving VRSS-based vulnerability prioritization using analytic hierarchy process","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.057","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861349128&doi=10.1016%2fj.jss.2012.03.057&partnerID=40&md5=2bff02e993748e74b65043d93a880a92","The number of vulnerabilities discovered in computer systems has increased explosively. Thus, a key question for system administrators is which vulnerabilities to prioritize. The need for vulnerability prioritization in organizations is widely recognized. The significant role of the vulnerability evaluation system is to separate vulnerabilities from each other as far as possible. There are two major methods to assess the severity of vulnerabilities: qualitative and quantitative methods. In this paper, we first describe the design space of vulnerability evaluation methodology and discuss the measures of well-defined evaluation framework. We analyze 11,395 CVE vulnerabilities to expose the differences among three current vulnerability evaluation systems (X-Force, CVSS and VRSS). We find that vulnerabilities are not separated from each other as much as possible. In order to increase the diversity of the results, we firstly enable vulnerability type to prioritize vulnerabilities using analytic hierarchy process on the basis of VRSS. We quantitatively characterize the vulnerability type and apply the method on the set of 11,395 CVE vulnerabilities. The results show that the quality of the quantitative scores can be improved with the help of vulnerability type. © 2012 Elsevier Inc. All rights reserved.","Analytic hierarchy process; Vulnerability evaluation; Vulnerability prioritization; Vulnerability type"
"A proposal to detect errors in Enterprise Application Integration solutions","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857355232&doi=10.1016%2fj.jss.2011.10.048&partnerID=40&md5=4216a3d6ab5db744d20085b088fa3a18","Enterprise Application Integration (EAI) solutions comprise a set of specific-purpose processes that implement exogenous message workflows. The goal is to keep a number of applications' data in synchrony or to develop new functionality on top of them. Such solutions are prone to errors because they are highly distributed and usually involve applications that were not designed with integration concerns in mind. This has motivated many authors to work on provisioning EAI solutions with fault-tolerance capabilities. In this article we analyse EAI solutions from two orthogonal perspectives: viewpoint (orchestration versus choreography) and execution model (process- versus task-based model). A review of the literature shows that current proposals are bound to a specific viewpoint or execution model or have important limitations. To address the problem, we have devised an error monitor that can be used to provision EAI solutions with fault-tolerance capabilities. Our theoretical analysis proves that the algorithms we use are computationally tractable, and our experimental results prove that they are efficient enough to be used in situations in which the workload is very high. © 2011 Elsevier Inc. All rights reserved.","Enterprise Application Integration; Error detection; Error monitoring; Fault-tolerance"
"Making sense of business process descriptions: An experimental comparison of graphical and textual notations","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366249&doi=10.1016%2fj.jss.2011.09.023&partnerID=40&md5=2f4e5957c5f0dc3405f6b46ba1730560","How effective is a notation in conveying the writer's intent correctly? This paper identifies understandability of design notations as an important aspect which calls for an experimental comparison. We compare the success of university students in interpreting business process descriptions, for an established graphical notation (BPMN) and for an alternative textual notation (based on written use-cases). Because a design must be read by diverse communities, including technically trained professionals such as developers and business analysts, as well as end-users and stakeholders from a wider business setting, we used different types of participants in our experiment. Specifically, we included those who had formal training in process description, and others who had not. Our experiments showed significant increases by both groups in their understanding of the process from reading the textual model. This was not so for the graphical model, where only the trained readers showed significant increases. This finding points at the value of educating readers of graphical descriptions in that particular notation when they become exposed to such models in their daily work. © 2011 Elsevier Inc. All rights reserved.","BPMN; Process models; Understandability; Written use case"
"A fast algorithm for Huffman decoding based on a recursion Huffman tree","2012","Journal of Systems and Software","10.1016/j.jss.2011.11.1019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366922&doi=10.1016%2fj.jss.2011.11.1019&partnerID=40&md5=e3e52ca0800d18c9365cd4570b197cac","This paper focuses on the time efficiency of Huffman decoding. In this paper, we utilize numerical interpretation to speed up the decoding process. The proposed algorithm firstly transforms the given Huffman tree into a recursion Huffman tree. Then, with the help of the recursion Huffman tree, the algorithm has the possibility to decode more than one symbol at a time if the minimum code length is less than or equal to half of the width of the processing unit. When the minimum code length is larger than the half of the width of the processing unit, the proposed method can still increase the average symbols decoded in one table access (thus speeding up the decoding time). In fact, the experimental results of the test files show that the average number of decoded symbols at one time for the proposed method ranges from 1.91 to 2.13 when the processing unit is 10. The experimental comparisons show that, compared to the conventional binary tree search method and the level-compressed Huffman decoding method, the decoding time of the proposed method is a great improvement. © 2011 Elsevier Inc. All rights reserved.","Data compression; Decoding; Huffman code"
"Automatic test case selection for regression testing of composite service based on extensible BPEL flow graph","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862794508&doi=10.1016%2fj.jss.2012.01.036&partnerID=40&md5=51649aae1e06862bcd2db34583c7aa2e","Services are highly reusable, flexible and loosely coupled components whose changes make the evolution and maintenance of composite services more complex. The changes of composite service mainly cover three types, i.e.; the processes, bindings, and interfaces. In this article, an approach is proposed to select test cases for regression testing of different versions of BPEL (business process execution language) composite service where these changes are involved. The approach identifies the changes by performing control flow analysis and comparing the paths in a new version of composite service with those in the old one using a kind of eXtensible BPEL flow graph (XBFG). Message sequence is appended to XBFG path so that XBFG can fully describe the behavior of composite service. The binding and predicate constraint information added in different XBFG elements can be used for path selection and even for test case generation. Both theoretic analysis and case study show that the proposed approach is effective. © 2012 Elsevier Inc. All rights reserved.","Extensible BPEL flow graph; Regression testing; Test case selection; Web service"
"Understanding the role of licenses and evolution in open architecture software ecosystems","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861097522&doi=10.1016%2fj.jss.2012.03.033&partnerID=40&md5=25854498ca2611d7720c8f68d111bb28","The role of software ecosystems in the development and evolution of open architecture systems whose components are subject to different licenses has received insufficient consideration. Such systems are composed of components potentially under two or more licenses, open source or proprietary or both, in an architecture in which evolution can occur by evolving existing components, replacing them, or refactoring. The software licenses of the components both facilitate and constrain the system's ecosystem and its evolution, and the licenses' rights and obligations are crucial in producing an acceptable system. Consequently, software component licenses and the architectural composition of a system help to better define the software ecosystem niche in which a given system lies. Understanding and describing software ecosystem niches for open architecture systems is a key contribution of this work. An example open architecture software system that articulates different niches is employed to this end. We examine how the architecture and software component licenses of a composed system at design time, build time, and run time help determine the system's software ecosystem niche and provide insight and guidance for identifying and selecting potential evolutionary paths of system, architecture, and niches. © 2012 Elsevier Inc. All rights reserved.","Open source software; Software architecture; Software ecosystems; Software evolution; Software licenses"
"Using enterprise architecture and technology adoption models to predict application usage","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861346426&doi=10.1016%2fj.jss.2012.02.035&partnerID=40&md5=ff65963e0e47142310e3e4bcbfc68ca4","Application usage is an important parameter to consider in application portfolio management. This paper presents an enterprise architecture analysis framework which can be used to assess application usage. The framework, in the form of an architecture metamodel, incorporates variables from the previously published Technology Acceptance Model (TAM) and the Task-Technology Fit (TTF) model. The paper describes how the metamodel has been tailored for a specific domain, viz. industry maintenance management. The metamodel was tested in the maintenance management domain through a survey with 55 respondents at five companies. Data collected in the survey showed that the domain-specific metamodel is able to explain variations in maintenance management application usage. Integrating the TAM and TTF variables with an architecture metamodel allows architects to reuse research results smoothly, thereby aiding them in producing good application portfolio decision-support. © 2012 Elsevier Inc. All rights reserved.","Application portfolio management; Architecture analysis; Computerized Maintenance Management Systems; Enterprise architecture; Maintenance management; Metamodel; Task-Technology Fit; Technology Acceptance Model"
"Evolution and change management of XML-based systems","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.038","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857366760&doi=10.1016%2fj.jss.2011.09.038&partnerID=40&md5=8c25e05cc05d36a2fdf0d905453c839f","XML is de-facto a standard language for data exchange. Structure of XML documents exchanged among different components of a system (e.g. services in a Service-Oriented Architecture) is usually described with XML schemas. It is a common practice that there is not only one but a whole family of XML schemas each applied in a particular logical execution part of the system. In such systems, the design and later maintenance of the XML schemas is not a simple task. In this paper we aim at a part of this problem - evolution of the family of the XML schemas. A single change in user requirements or surrounding environment of the system may influence more XML schemas in the family. A designer needs to identify the XML schemas affected by a change and ensure that they are evolved coherently with each other to meet the new requirement. Doing this manually is very time consuming and error prone. In this paper we show that much of the manual work can be automated. For this, we introduce a technique based on the principles of Model-Driven Development. A designer is required to make a change only once in a conceptual schema of the problem domain and our technique ensures semi-automatic coherent propagation to all affected XML schemas (and vice versa). We provide a formal model of possible evolution changes and their propagation mechanism. We also evaluate the approach on a real-world evolution scenario. © 2011 Elsevier Inc. All rights reserved.","Model driven architecture; Propagation of changes; XML data modeling; XML schema evolution"
"Balancing software engineering education and industrial needs","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861092480&doi=10.1016%2fj.jss.2012.01.060&partnerID=40&md5=5d30e040edf683a3a889677d6a48f6cf","In the world of information and communications technologies the demand for professionals with software engineering skills grows at an exponential rate. On this ground, we have conducted a study to help both academia and the software industry form a picture of the relationship between the competences of recent graduates of undergraduate and graduate software engineering programmes and the tasks that these professionals are to perform as part of their jobs in industry. Thanks to this study, academia will be able to observe which skills demanded by industry the software engineering curricula do or do not cater for, and industry will be able to ascertain which tasks a recent software engineering programme graduate is well qualified to perform. The study focuses on the software engineering knowledge guidelines provided in SE2004 and GSwE2009, and the job profiles identified by Career Space. © 2012 Elsevier Inc. All rights reserved.","Software engineer competences; Software engineer skills; Software engineering curricula; Software engineering education; Software industry profiles"
"On ""exploring alternatives for transition verification""","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861343030&doi=10.1016%2fj.jss.2012.03.034&partnerID=40&md5=a5b3fbfac20a5bbae5eaeba1d85c4fd8","Duan and Chen (doi:10.1016/j.jss.2009.05.019) proposed two methods for constructing an input/output sequence (IOS) whose execution on an implementation N of a given deterministic finite state machine (DFSM) M tests that N can be interpreted as a DFSM properly implementing every individual state and transition of M. This paper shows that the methods and three earlier similar methods potentially introduce cyclic dependencies between the essential segments of the produced IOS, meaning that the IOS might fail to be a complete test under the default interpretation. It then proposes modifications provably preventing such cycles. All the methods assume that M is completely specified and strongly connected and possesses a distinguishing set and that N has at most as many states as M. © 2012 Elsevier Inc. All rights reserved.","Checking sequence; Conformance testing; Deterministic finite state machine; Distinguishing set"
"Efficient (n, t, n) secret sharing schemes","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862832565&doi=10.1016%2fj.jss.2012.01.027&partnerID=40&md5=6d8c83e658fe956daa72760ab59bb0e3","Recently, Harn and Lin introduced a notion of strong t-consistency of a (t, n) secret sharing scheme and proposed a strong (n, t, n) verifiable secret sharing (VSS). In this paper, we propose a strong (n, t, n) VSS which is more efficient than Harn and Lin's VSS. Using the same approach, we propose a (n, t, n) multi-secret sharing scheme (MSS) to allow shareholders to share n - t + 1 secrets. Also, the proposed (n, t, n) MSS can be modified to include the verifiable feature. All proposed schemes are unconditionally secure and are based on Shamir's (t, n) secret sharing scheme. © 2012 Elsevier Inc. All rights reserved.","Homomorphism; Multiple secrets; Secret sharing; t-Consistency; Verifiable secret sharing"
"High capacity reversible data hiding scheme based upon discrete cosine transformation","2012","Journal of Systems and Software","10.1016/j.jss.2012.05.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863614434&doi=10.1016%2fj.jss.2012.05.032&partnerID=40&md5=72f126c8e10dfef36e9e8573f2b99aa9","In this paper, we propose a reversible data hiding scheme based on the varieties of coefficients of discrete cosine transformation of an image. Cover images are decomposed into several different frequencies, and the high-frequency parts are embedded with secret data. We use integer mapping to implement our 2-dimensional discrete cosine transformation. Thus, the image recovered from the modified coefficients can be transformed back to the correct data-hidden coefficients. Since the distribution of 2-dimensional DCT coefficients looks close to Gaussian distribution centralized at zero, it is a natural candidate for embedding secret data using the histogram shifting approach. Thus, our approach shifts the positive coefficients around zero to the right and the negative coefficients around zero to the left in order to leave a space to hide the secret data. The experimental comparisons show that, compared to Chang et al. and Lin et al.'s method, the embedding capacity and quality of the stego-image of the proposed method is a great improvement. © 2011 Elsevier Inc. All rights reserved.","Data hiding; DCT; Histogram shifting"
"UniSpaCh: A text-based data hiding method using Unicode space characters","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865743341&doi=10.1016%2fj.jss.2011.12.023&partnerID=40&md5=49dc44b4f6fa55bb7e2cef61054fba45","This paper proposes a text-based data hiding method to insert external information into Microsoft Word document. First, the drawback of low embedding efficiency in the existing text-based data hiding methods is addressed, and a simple attack, DASH, is proposed to reveal the information inserted by the existing text-based data hiding methods. Then, a new data hiding method, UniSpaCh, is proposed to counter DASH. The characteristics of Unicode space characters with respect to embedding efficiency and DASH are analyzed, and the selected Unicode space characters are inserted into inter-sentence, inter-word, end-of-line and inter-paragraph spacings to encode external information while improving embedding efficiency and imperceptivity of the embedded information. UniSpaCh is also reversible where the embedded information can be removed to completely reconstruct the original Microsoft Word document. Experiments were carried out to verify the performance of UniSpaCh as well as comparing it to the existing space-manipulating data hiding methods. Results suggest that UniSpaCh offers higher embedding efficiency while exhibiting higher imperceptivity of white space manipulation when compared to the existing methods considered. In the best case scenario, UniSpaCh produces output document of size almost 9 times smaller than that of the existing method. © 2011 Elsevier Inc. All rights reserved.","DASH; Data hiding; Space manipulation; Unicode character; UniSpaCh"
"Lightweight embedded software performance analysis method by kernel hack and its industrial field study","2012","Journal of Systems and Software","10.1016/j.jss.2011.03.049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755188912&doi=10.1016%2fj.jss.2011.03.049&partnerID=40&md5=7066338a0f52fd7b50dddde204d6b15a","Despite advances in software testing technologies, there are still limitations in directly applying them to embedded software. Since the operational environment of embedded software has severe resource constraints, it is necessary to develop a lightweight testing method that has little impact on the operational environment of embedded software. We propose an agent-based performance analysis method to hack kernel performance counters that manage the system's execution information. The proposed method enables us to collect data required for analyzing performance bottlenecks and identify the causes and locations of bottlenecks with little impact on the test target system's operational environment. We introduce a test automation tool called Analytic Master of System v2.0 that we developed by employing our proposed method. Presently, Analytic Master of System v2.0 is being utilized as a standard tool for performance testing of embedded systems in the automotive industry. In addition, we suggest a guideline for performance analysis and improvement by introducing an industrial field study among our best practices, which analyze the relationship between the memory fault processing of the operating system and the application processing speed. © 2011 Elsevier Inc.","Embedded software; Embedded system; Kernel hack; Operational test; Performance analysis"
"Test coverage optimization for large code problems","2012","Journal of Systems and Software","10.1016/j.jss.2011.05.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755189944&doi=10.1016%2fj.jss.2011.05.021&partnerID=40&md5=bcc3e8a3d1aa892a44932165e2e26fd5","Software developers frequently conduct regression testing on a series of major, minor, or bug-fix software or firmware releases. However, retesting all test cases for each release is time-consuming. For example, it takes about 36 test-bed-days to thoroughly exercise a test suite made up of 2320 test cases for the MPLS testing area that contains 57,758 functions in Cisco IOS. The cost is infeasible for a series of regression testing on the MPLS area. Thus, the test suite needs to be reduced intelligently, not just randomly, and its fault detection capability must be kept as much as possible. The mode of safe regression test selection approach is adopted for seeking a subset of modification-traversing test cases to substitute for fault-revealing test cases. The algorithms, CW-NumMin, CW-CostMin, and CW-CostCov-B, apply the safe-mode approach in selecting test cases for achieving full-modified function coverage. It is assumed that modified functions are fault-prone, and the fault distribution of the testing area is Pareto-like. Moreover, we also assume that once a subject program is getting more mature, its fault concentration will become stronger. Only function coverage criterion is adopted because of the scalability of a software system with large code. The metrics of test's function reachability and function's test intensity are defined in this study for algorithms. Both CW-CovMax and CW-CostMin algorithms are not safe-mode, but the approaches they use still attempt to obtain a test suite with a maximal amount of function coverage under certain constraints, i.e. the effective-confidence level and time restriction. We conclude that the most effective algorithm in this study can significantly reduce the cost (time) of regression testing on the MPLS testing area to 1.10%, on the average. Approaches proposed here can be effectively and efficiently applied to the regression testing on bug-fix releases of a software system with large code, especially to the releases having very few modified functions with low test intensities. © 2011 Elsevier Inc.","Regression testing; Software maintenance; Test case selection; Test coverage; Test intensity"
"Controlling software architecture erosion: A survey","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755136651&doi=10.1016%2fj.jss.2011.07.036&partnerID=40&md5=a2e50361d326ce03280b6b01f0c59c83","Software architectures capture the most significant properties and design constraints of software systems. Thus, modifications to a system that violate its architectural principles can degrade system performance and shorten its useful lifetime. As the potential frequency and scale of software adaptations increase to meet rapidly changing requirements and business conditions, controlling such architecture erosion becomes an important concern for software architects and developers. This paper presents a survey of techniques and technologies that have been proposed over the years either to prevent architecture erosion or to detect and restore architectures that have been eroded. These approaches, which include tools, techniques and processes, are primarily classified into three generic categories that attempt to minimise, prevent and repair architecture erosion. Within these broad categories, each approach is further broken down reflecting the high-level strategies adopted to tackle erosion. These are: process-oriented architecture conformance, architecture evolution management, architecture design enforcement, architecture to implementation linkage, self-adaptation and architecture restoration techniques consisting of recovery, discovery and reconciliation. Some of these strategies contain sub-categories under which survey results are presented. We discuss the merits and weaknesses of each strategy and argue that no single strategy can address the problem of erosion. Further, we explore the possibility of combining strategies and present a case for further work in developing a holistic framework for controlling architecture erosion. © 2011 Elsevier Inc.","Architecture erosion; Controlling architecture erosion; Design erosion; Software architecture; Software decay; Survey"
"Self-tuning of disk input-output in operating systems","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755129061&doi=10.1016%2fj.jss.2011.07.030&partnerID=40&md5=26e907f747cb76d4c2b5e1c63cf51be8","One of the most difficult and hard to learn tasks in computer system management is tuning the kernel parameters in order to get the maximum performance. Traditionally, this tuning has been set using either fixed configurations or the subjective administrator's criteria. The main bottleneck among the subsystems managed by the operating systems is disk input/output (I/O). An evolutionary module has been developed to perform the tuning of this subsystem automatically, using an adaptive and dynamic approach. Any computer change, both at the hardware level, and due to the nature of the workload itself, will make our module adapt automatically and in a transparent way. Thus, system administrators are released from this kind of task and able to achieve some optimal performances adapted to the framework of each of their systems. The experiment made shows a productivity increase in 88.2 of cases and an average improvement of 29.63 with regard to the default configuration of the Linux operating system. A decrease of the average latency was achieved in 77.5 of cases and the mean decrease in the request processing time of I/O was 12.79. © 2011 Elsevier Inc.","Evolutionary computation; Genetic algorithms; IO optimization; Kernel optimization; Operating system"
"Sonata: Flexible connections between interaction and business spaces","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865704713&doi=10.1016%2fj.jss.2011.12.030&partnerID=40&md5=0b145b3eed6d8def6a54956b827f8cae","Every interactive system features a functional core and a user interface. Over the years, several types of software architectures for connecting these conceptual elements have been proposed, all of which fail to conciliate two essential qualities: enabling both business and interaction objects reuse, and limiting the amount of communication-specific code in reusable objects. We have described in previous work the Symphony Architecture, which bridges the gap between the interaction and business spaces, while requiring no code overhead in either business or interaction objects. Resulting development features minimal coupling between technology-agnostic business and interaction constructs, called Symphony Objects, and improves their reusability by clearly isolating them from the applicative logic and from technical objects. In this paper, we present an original software framework, called Sonata, which capitalizes on the conventions used for building and organizing Symphony Architecture instances, for minimizing the amount of configuration required for setting up connections between the business and interaction spaces. © 2011 Elsevier Inc. All rights reserved.","Aspects; Framework; HCI; Information systems; Software architecture"
"A documentation framework for architecture decisions","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857356116&doi=10.1016%2fj.jss.2011.10.017&partnerID=40&md5=59c35a7c212165aabae4741c877e8121","In this paper, we introduce a documentation framework for architecture decisions. This framework consists of four viewpoint definitions using the conventions of ISO/IEC/IEEE 42010, the new international standard for the description of system and software architectures. The four viewpoints, a Decision Detail viewpoint, a Decision Relationship viewpoint, a Decision Chronology viewpoint, and a Decision Stakeholder Involvement viewpoint satisfy several stakeholder concerns related to architecture decision management. With the exception of the Decision Stakeholder Involvement viewpoint, the framework was evaluated in an industrial case study. The results are promising, as they show that decision views can be created with reasonable effort while satisfying many of the stakeholder concerns in decision documentation. © 2011 Elsevier Inc. All rights reserved.","Architectural viewpoints; Architecture decisions; Architecture framework; Architecture knowledge management; Case study; Software architecture"
"Efficient audit service outsourcing for data integrity in clouds","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863498079&doi=10.1016%2fj.jss.2011.12.024&partnerID=40&md5=6f56368d04d044b189b33bddd7266a0d","Cloud-based outsourced storage relieves the client's burden for storage management and maintenance by providing a comparably low-cost, scalable, location-independent platform. However, the fact that clients no longer have physical possession of data indicates that they are facing a potentially formidable risk for missing or corrupted data. To avoid the security risks, audit services are critical to ensure the integrity and availability of outsourced data and to achieve digital forensics and credibility on cloud computing. Provable data possession (PDP), which is a cryptographic technique for verifying the integrity of data without retrieving it at an untrusted server, can be used to realize audit services. In this paper, profiting from the interactive zero-knowledge proof system, we address the construction of an interactive PDP protocol to prevent the fraudulence of prover (soundness property) and the leakage of verified data (zero-knowledge property). We prove that our construction holds these properties based on the computation Diffie-Hellman assumption and the rewindable black-box knowledge extractor. We also propose an efficient mechanism with respect to probabilistic queries and periodic verification to reduce the audit costs per verification and implement abnormal detection timely. In addition, we present an efficient method for selecting an optimal parameter value to minimize computational overheads of cloud audit services. Our experimental results demonstrate the effectiveness of our approach. © 2011 Elsevier Inc. All rights reserved.","Audit service; Cloud storage; Interactive proof system; Provable data possession; Security"
"Efficient and robust probabilistic guarantees for real-time tasks","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865765377&doi=10.1016%2fj.jss.2011.12.042&partnerID=40&md5=de1fafbb87266d922bcaa605e32037fc","This paper presents a new method for providing probabilistic real-time guarantees to tasks scheduled through resource reservations. Previous work on probabilistic analysis of reservation-based schedulers is extended by improving the efficiency and robustness of the probability computation. Robustness is improved by accounting for a possibly incomplete knowledge of the distribution of the computation times (which is typical in realistic applications). The proposed approach computes a conservative bound for the probability of missing deadlines, based on the knowledge of the probability distributions of the execution times and of the inter-arrival times of the tasks. In this paper, such a bound is computed in realistic situations, comparing it with simulative results and with the exact computation of deadline miss probabilities (without pessimistic bounds). Finally, the impact of the incomplete knowledge of the execution times distribution is evaluated. © 2011 Elsevier Inc. All rights reserved.","Real-time computing; Soft real-time; Stochastic analysis"
"Reconciling perspectives: A grounded theory of how people manage the process of software development","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859550248&doi=10.1016%2fj.jss.2012.01.059&partnerID=40&md5=c330206ccfc2c02c86659de6b40a9351","Social factors are significant cost drivers for the process of software development. In this field study we generate a grounded theory of how people manage the process of software development. The main concern of engineers involved in the process of software development is getting the job done. To get the job done, people engage in a four-stage process of Reconciling Perspectives. Reconciling Perspectives represents an attempt to converge individuals' points of view or perspectives about a software project. The process emphasizes the importance of individuals' abilities to both reach out and engage in negotiations and create shelter from environmental noise to bring a software project to fruition. © 2012 Elsevier Inc. All rights reserved.","Agile manifesto agile software development; Grounded theory; Scrum; Shared mental model; Software engineering; Software team"
"Using compressed index structures for processing moving objects in large spatio-temporal databases","2012","Journal of Systems and Software","10.1016/j.jss.2011.08.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755140450&doi=10.1016%2fj.jss.2011.08.005&partnerID=40&md5=f7d2858f18f5e6525448650e98a93b0b","This paper develops a novel, compressed B+-tree based indexing scheme that supports the processing of moving objects in one-, two-, and multi- dimensional spaces. The past, current, and anticipated future trajectories of movements are fully indexed and well organized. No parameterized functions and geometric representations are introduced in our data model so that update operations are not required and the maintenance of index structures can be accomplished by basic insertion and deletion operations. The proposed method has two contributions. First, the spatial and temporal attributes of trajectories are accurately preserved and well organized into compact index structures with very efficient memory space utilization and storage requirement. Second, index maintenance overheads are more economical and query performance is more responsive than those of conventional methods. Both analytical and empirical studies show that our proposed indexing scheme outperforms the TPR-tree. © 2011 Elsevier Inc.","Compressed B<sup>+</sup>-tree; Indexing structures; Moving objects; Spatio-temporal database"
"The changing industry structure of software development for consumer electronics and its consequences for software architectures","2012","Journal of Systems and Software","10.1016/j.jss.2011.08.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755188918&doi=10.1016%2fj.jss.2011.08.007&partnerID=40&md5=8598503b5b47f5e7d3ae98f08d8af3ac","During the last decade the structure of the consumer electronics industry has been changing profoundly. Current consumer electronics products are built using components from a large variety of specialized firms, whereas previously each product was developed by a single, vertically integrated company. Taking a software development perspective, we analyze the transition in the consumer electronics industry using case studies from digital televisions and mobile phones. We introduce a model consisting of five industry structure types and describe the forces that govern the transition between types and we describe the consequences for software architectures. We conclude that, at this point in time, software supply chains are the dominant industry structure for developing consumer electronics products. This is because the modularization of the architecture is limited, due to the lack of industry-wide standards and because resource constrained devices require variants of supplied software that are optimized for different hardware configurations. Due to these characteristics open ecosystems have not been widely adopted. The model and forces can serve the decision making process for individual companies that consider the transition to a different type of industry structure as well as provide a framework for researchers studying the software-intensive industries. © 2011 Elsevier Inc.","Case study; Consumer electronics; Ecosystems; Embedded systems; Industry structures; Mobile phones; Software architecture; Software evolution; Software management; Software supply chains"
"Appraisal and reporting of security assurance at operational systems level","2012","Journal of Systems and Software","10.1016/j.jss.2011.08.013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755140556&doi=10.1016%2fj.jss.2011.08.013&partnerID=40&md5=7a544397cd1d677c8ff3797d70ff32df","In this paper we discuss the issues relating the evaluation and reporting of security assurance of runtime systems. We first highlight the shortcomings of current initiatives in analyzing, evaluating and reporting security assurance information. Then, the paper proposes a set of metrics to help capture and foster a better understanding of the security posture of a system. Our security assurance metric and its reporting depend on whether or not the user of the system has a security background. The evaluation of such metrics is described through the use of theoretical criteria, a tool implementation and an application to a case study based on an insurance company network. © 2011 Elsevier Inc.","Measurement; Metrics specification; Probes; Risk; Security assurance; Security mechanisms; Verification of security; Verification process quality"
"A signature-based Grid index design for main-memory RFID database applications","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865720224&doi=10.1016%2fj.jss.2012.01.026&partnerID=40&md5=a689387b0f9dc8067b333c42717f2291","A large-scale RFID application often requires a highly efficient database system in data processing. This research is motivated by the strong demand for an efficient index structure design for main-memory database systems of RFID applications. In this paper, a signature-based Grid index structure is proposed for efficient data queries and storage. An efficient methodology is proposed to locate duplicates and to execute batch deletions and range queries based on application domain knowhow. The capability of the design is implemented in an open source main-memory database system H2 and evaluated by realistic workloads of RFID applications. © 2012 Elsevier Inc. All rights reserved.","Database; Embedded system; Index; Main memory; RFID; Signature"
"A power efficiency routing and maintenance protocol in wireless multi-hop networks","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755146079&doi=10.1016%2fj.jss.2011.07.012&partnerID=40&md5=35556fbdf2e5fe831fcb4018d112b537","In wireless multi-hop networks, selecting a path that has a high transmission bandwidth or a high delivery rate of packets can reduce power consumption and shorten transmission delay during data transmission. There are two factors that influence the transmission bandwidth: the signal strength of the received packets and contentions in the contention-based MAC layer. These two factors may cause more power to be consumed during data transmission. We analyze these two factors and propose a power-aware routing protocol called MTPCR. MTPCR discovers the desired routing path that has reduced power consumption during data transmission. In addition to finding a desired path to reduce power consumption, MTPCR also takes into account the situations in which the transmission bandwidth of the routing path may decrease, resulting in much power consumption during data transmission because of the mobility of nodes in a network. MTPCR is thus useful in a network: it analyzes power consumption during data transmission with the help of neighboring nodes, and it uses a path maintenance mechanism to maintain good path bandwidth. The density of nodes in a network is used to determine when to activate the path maintenance mechanism in order to reduce the overhead of this mechanism. With the proposed path maintenance mechanism, power consumption during data transmission can be efficiently reduced, as well as the number of path breakages. In our simulation, we compared our proposed routing protocol, MTPCR, with the following protocols: two classical routing protocols, AODV and DSR; two power-aware routing protocols, MMBCR and xMBCR; and one multiple path routing protocol, PAMP. The comparisons are made in terms of throughput of the routing path, power consumption in path discovery, power consumption in data transmission, and network lifetime. © 2011 Elsevier Inc.","Path maintenance mechanism; Power consumption; Power-aware routing; Signal strength; Transmission bandwidth; Wireless multi-hop network"
"SimFuzz: Test case similarity directed deep fuzzing","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755172214&doi=10.1016%2fj.jss.2011.07.028&partnerID=40&md5=1bec55ab75e065f4ffa04137cd5792a2","Fuzzing is widely used to detect software vulnerabilities. Blackbox fuzzing does not require program source code. It mutates well-formed inputs to produce new ones. However, these new inputs usually do not exercise deep program semantics since the possibility that they can satisfy the conditions of a deep program state is low. As a result, blackbox fuzzing is often limited to identify vulnerabilities in input validation components of a program. Domain knowledge such as input specifications can be used to mitigate these limitations. However, it is often expensive to obtain such knowledge in practice. Whitebox fuzzing employs heavy analysis techniques, i.e.; dynamic symbolic execution, to systematically generate test inputs and explore as many paths as possible. It is powerful to explore new program branches so as to identify more vulnerabilities. However, it has fundamental challenges such as unsolvable constraints and is difficult to scale to large programs due to path explosion. This paper proposes a novel fuzzing approach that aims to produce test inputs to explore deep program semantics effectively and efficiently. The fuzzing process comprises two stages. At the first stage, a traditional blackbox fuzzing approach is applied for test data generation. This process is guided by a novel test case similarity metric. At the second stage, a subset of the test inputs generated at the first stage is selected based on the test case similarity metric. Then, combination testing is applied on these selected test inputs to further generate new inputs. As a result, less redundant test inputs, i.e.; inputs that just explore shallow program paths, are created at the first stage, and more distinct test inputs, i.e.; inputs that explore deep program paths, are produced at the second stage. A prototype tool SimFuzz is developed and evaluated on real programs, and the experimental results are promising. © 2011 Elsevier Inc.","Fuzzing; Software testing; Software vulnerability"
"Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755172137&doi=10.1016%2fj.jss.2011.07.037&partnerID=40&md5=9ec2aef05dc691219779db0cccb7074e","Ambient Assisted Living (AAL) investigates the development of systems involving the use of different types of sensors, which monitor activities and vital signs of lonely elderly people in order to detect emergency situations or deviations from desirable medical patterns. Instead of requiring the elderly person to manually push a button to request assistance, state-of-the-art AAL solutions automate the process by 'perceiving' lonely elderly people in their home environment through various sensors and performing appropriate actions under the control of the underlying software. Dependability in the AAL domain is a critical requirement, since poor system availability, reliability, safety, or integrity may cause inappropriate emergency assistance to potentially have fatal consequences. Nevertheless, contemporary research has not focused on assessing dependability in this domain. This work attempts to fill this gap presenting an approach which relies on modern quantitative and qualitative dependability analysis techniques based on software architecture. The analysis method presented in this paper consists of conversion patterns from Unified Modeling Language (UML) behavior models of the AAL software architecture into a formal executable specification, based on a probabilistic process algebra description language, which enables a sound quantitative and qualitative analysis. The UML models specify system component interactions and are annotated with component failure probabilities and system usage profile information. The resulting formal specification is executed on PRISM, a model checking tool adequate for the purpose of our analysis in order to identify a set of domain-specific dependability properties expressed declaratively in Probabilistic Computational Tree Logic (PCTL). The benefits of using these techniques are twofold. Firstly, they allow us to seamlessly integrate the analysis during subsequent software lifecycle stages in critical scenarios. Secondly, we identify the components which have the highest impact on software system dependability, and therefore, be able to address software architecture and individual software component problems prior to implementation and the occurrence of critical errors. © 2011 Elsevier Inc.","Ambient Assisted Living; Component-based system; Dependability analysis"
"ID-based proxy signature scheme with message recovery","2012","Journal of Systems and Software","10.1016/j.jss.2011.08.018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755153735&doi=10.1016%2fj.jss.2011.08.018&partnerID=40&md5=1bb7a2975933906a4dd6703f4c2d7ca2","A proxy signature scheme, introduced by Mambo, Usuda and Okamoto, allows an entity to delegate its signing rights to another entity. Identity based public key cryptosystems are a good alternative for a certificate based public key setting, especially when efficient key management and moderate security are required. From inception several ID-based proxy signature schemes have been discussed, but no more attention has been given to proxy signature with message recovery. In this paper, we are proposing provably secure ID-based proxy signature scheme with message recovery and we have proved that our scheme is secure as existential forgery-adaptively chosen message and ID attack. As proposed scheme is efficient in terms of communication overhead and security, it can be a good alternative for certificate based proxy signatures, used in various applications such as wireless e-commerce, mobile agents, mobile communication and distributed shared object systems, etc. © 2011 Elsevier Inc.","Bilinear pairing; ID-based signature; Mobile agent; Proxy signature; Signature with message recovery"
"DyDAP: A dynamic data aggregation scheme for privacy aware wireless sensor networks","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755190141&doi=10.1016%2fj.jss.2011.07.043&partnerID=40&md5=6fd0cb0292d17872476b9fe5de55523a","End-to-end data aggregation, without degrading sensing accuracy, is a very relevant issue in wireless sensor networks (WSN) that can prevent network congestion to occur. Moreover, privacy management requires that anonymity and data integrity are preserved in such networks. Unfortunately, no integrated solutions have been proposed so far, able to tackle both issues in a unified and general environment. To bridge this gap, in this paper we present an approach for dynamic secure end-to-end data aggregation with privacy function, named DyDAP. It has been designed starting from a UML model that encompasses the most important building blocks of a privacy-aware WSN, including aggregation policies. Furthermore, it introduces an original aggregation algorithm that, using a discrete-time control loop, is able to dynamically handle in-network data fusion to reduce the communication load. The performance of the proposed scheme has been verified using computer simulations, showing that DyDAP avoids network congestion and therefore improves WSN estimation accuracy while, at the same time, guaranteeing anonymity and data integrity. © 2011 Elsevier Inc.","Anonymity; Congestion control; End-to-end data aggregation; Privacy; Wireless sensor networks"
"An adaptive model-free resource and power management approach for multi-tier cloud environments","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865755126&doi=10.1016%2fj.jss.2011.12.043&partnerID=40&md5=7d01874f5567f103505d0427fba27dbe","With the development of cloud environments serving as a unified infrastructure, the resource management and energy consumption issues become more important in the operations of such systems. In this paper, we investigate adaptive model-free approaches for resource allocation and energy management under time-varying workloads and heterogeneous multi-tier applications. Specifically, we make use of measurable metrics, including throughput, rejection amount, queuing state, and so on, to design resource adjustment schemes and to make control decisions adaptively. The ultimate objective is to guarantee the summarized revenue of the resource provider while saving energy and operational costs. To validate the effectiveness, performance evaluation experiments are performed in a simulated environment, with realistic workloads considered. Results show that with the combination of long-term adaptation and short-term adaptation, the fluctuation of unpredictable workloads can be captured, and thus the total revenue can be preserved while balancing the power consumption as needed. Furthermore, the proposed approach can achieve better effect and efficiency than the model-based approaches in dealing with real-world workloads. © 2011 Elsevier Inc. All rights reserved.","Adaptive model-free approach; Cloud environment; Power management; Resource management"
"Context-oriented programming: A software engineering perspective","2012","Journal of Systems and Software","10.1016/j.jss.2012.03.024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861345286&doi=10.1016%2fj.jss.2012.03.024&partnerID=40&md5=6fca2bdb9f2ad12033fdcd88e82b74a8","The implementation of context-aware systems can be supported through the adoption of techniques at the architectural level such as middlewares or component-oriented architectures. It can also be supported by suitable constructs at the programming language level. Context-oriented programming (COP) is emerging as a novel paradigm for the implementation of this kind of software, in particular in the field of mobile and ubiquitous computing. The COP paradigm tackles the issue of developing context-aware systems at the language-level, introducing ad hoc language abstractions to manage adaptations modularization and their dynamic activation. In this paper we review the state of the art in the field of COP in the perspective of the benefits that this technique can provide to software engineers in the design and implementation of context-aware applications. © 2012 Elsevier Inc. All rights reserved.","Context; Context-awareness; Context-oriented programming"
"Analysis and application of an outsourcing risk framework","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861343560&doi=10.1016%2fj.jss.2012.02.040&partnerID=40&md5=3e70ae5cfce021a6dff66fe92774ecf1","There is much reported research on risk, and risk management but research on strategic IT system development outsourcing risk, from the client perspective, remains unclear. A literature-based conceptual risk framework for strategic IT system development outsourcing from the client perspective has been developed. We then investigate, (1) critical client risks for strategic IT system development outsourcing projects, and (2) the most common critical client risk factors for such projects. In order to identify any serious omissions in our framework an initial validation of the risk framework is provided through a review of nine published cases of unsuccessful strategic IT system development outsourcing projects. The risks critical to a client are associated with complexity, contract, execution, financial, legal, the organizational environment, planning and control, scope and requirements, the team, and the user. Risks manifest in all nine published cases, include (1) complexity and (2) the team. Three risk factors not previously identified in the initial framework are included in a revised framework. The risk framework assisted us in identifying a number of critical risk factors affecting the outcome of strategic IT system development outsourcing projects. © 2012 Elsevier Inc. All rights reserved.","Client perspective; Outsourcing; Risks; Strategic IT development project"
"A graphical-based password keystroke dynamic authentication system for touch screen handheld mobile devices","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862870615&doi=10.1016%2fj.jss.2011.12.044&partnerID=40&md5=5d25385c4d033cbcb0a3f82f8129a016","Since touch screen handheld mobile devices have become widely used, people are able to access various data and information anywhere and anytime. Most user authentication methods for these mobile devices use PIN-based (Personal Identification Number) authentication, since they do not employ a standard QWERTY keyboard for conveniently entering text-based passwords. However, PINs provide a small password space size, which is vulnerable to attacks. Many studies have employed the KDA (Keystroke Dynamic-based Authentication) system, which is based on keystroke time features to enhance the security of PIN-based authentication. Unfortunately, unlike the text-based password KDA systems in QWERTY keyboards, different keypad sizes or layouts of mobile devices affect the PIN-based KDA system utility. This paper proposes a new graphical-based password KDA system for touch screen handheld mobile devices. The graphical password enlarges the password space size and promotes the KDA utility in touch screen handheld mobile devices. In addition, this paper explores a pressure feature, which is easy to use in touch screen handheld mobile devices, and applies it in the proposed system. The experiment results show: (1) EER is 12.2% in the graphical-based password KDA proposed system. Compared with related schemes in mobile devices, this effectively promotes KDA system utility; (2) EER is reduced to 6.9% when the pressure feature is used in the proposed system. The accuracy of authenticating keystroke time and pressure features is not affected by inconsistent keypads since the graphical passwords are entered via an identical size (50 mm × 60 mm) human-computer interface for satisfying the lowest touch screen size and a GUI of this size is displayed on all mobile devices. © 2011 Elsevier Inc. All rights reserved.","Graphical-based password authentication; Keystroke dynamics authentication; Keystroke feature; User authentication"
"Automatic testing environment for multi-core embedded software - ATEMES","2012","Journal of Systems and Software","10.1016/j.jss.2011.08.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755139516&doi=10.1016%2fj.jss.2011.08.030&partnerID=40&md5=8d04f14224a5467cf9c735876ee298fd","Software testing during the development process of embedded software is not only complex, but also the heart of quality control. Multi-core embedded software testing faces even more challenges. Major issues include: (1) how demanding efforts and repetitive tedious actions can be reduced; (2) how resource restraints of embedded system platform such as temporal and memory capacity can be tackled; (3) how embedded software parallelism degree can be controlled to empower multi-core CPU computing capacity; (4) how analysis is exercised to ensure sufficient coverage test of embedded software; (5) how to do data synchronization to address issues such as race conditions in the interrupt driven multi-core embedded system; (6) high level reliability testing to ensure customer satisfaction. To address these issues, this study develops an automatic testing environment for multi-core embedded software (ATEMES). Based on the automatic mechanism, the system can parse source code, instrument source code, generate testing programs for test case and test driver, support generating primitive, structure and object types of test input data, multi-round cross-testing, and visualize testing results. To both reduce test engineer's burden and enhance his efficiency when embedded software testing is in process, this system developed automatic testing functions including unit testing, coverage testing, multi-core performance monitoring. Moreover, ATEMES can perform automatic multi-round cross-testing benchmark testing on multi-core embedded platform for parallel programs adopting Intel TBB library to recommend optimized parallel parameters such as pipeline tokens. Using ATEMES on the ARM11 multi-core platform to conduct testing experiments, the results show that our constructed testing environment is effective, and can reduce burdens of test engineer, and can enhance efficiency of testing task. © 2011 Elsevier Inc.","Automatic testing; Coverage testing; Cross-testing; Embedded software testing; Multi-core embedded software testing; Object testing; Parallelism degree testing; TBB testing; Test case generation; Testing tool; Unit testing"
"Noisy data elimination using mutual k-nearest neighbor for classification mining","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865233213&doi=10.1016%2fj.jss.2011.12.019&partnerID=40&md5=42ecfa259f1781c2c3011c88c315fa6c","k nearest neighbor (kNN) is an effective and powerful lazy learning algorithm, notwithstanding its easy-to-implement. However, its performance heavily relies on the quality of training data. Due to many complex real-applications, noises coming from various possible sources are often prevalent in large scale databases. How to eliminate anomalies and improve the quality of data is still a challenge. To alleviate this problem, in this paper we propose a new anomaly removal and learning algorithm under the framework of kNN. The primary characteristic of our method is that the evidence of removing anomalies and predicting class labels of unseen instances is mutual nearest neighbors, rather than k nearest neighbors. The advantage is that pseudo nearest neighbors can be identified and will not be taken into account during the prediction process. Consequently, the final learning result is more creditable. An extensive comparative experimental analysis carried out on UCI datasets provided empirical evidence of the effectiveness of the proposed method for enhancing the performance of the k-NN rule. © 2011 Elsevier Inc. All rights reserved.","Data mining; Data reduction; kNN; Mutual nearest neighbor; Pattern classification"
"Thresholds for error probability measures of business process models","2012","Journal of Systems and Software","10.1016/j.jss.2012.01.017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865505788&doi=10.1016%2fj.jss.2012.01.017&partnerID=40&md5=6bf93ef866b331688bc112719e1255ec","The quality of conceptual business process models is highly relevant for the design of corresponding information systems. In particular, a precise measurement of model characteristics can be beneficial from a business perspective, helping to save costs thanks to early error detection. This is just as true from a software engineering point of view. In this latter case, models facilitate stakeholder communication and software system design. Research has investigated several proposals as regards measures for business process models, from a rather correlational perspective. This is helpful for understanding, for example size and complexity as general driving forces of error probability. Yet, design decisions usually have to build on thresholds, which can reliably indicate that a certain counter-action has to be taken. This cannot be achieved only by providing measures; it requires a systematic identification of effective and meaningful thresholds. In this paper, we derive thresholds for a set of structural measures for predicting errors in conceptual process models. To this end, we use a collection of 2000 business process models from practice as a means of determining thresholds, applying an adaptation of the ROC curve method. Furthermore, an extensive validation of the derived thresholds was conducted by using 429 EPC models from an Australian financial institution. Finally, significant thresholds were adapted to refine existing modeling guidelines in a quantitative way. © 2012 Elsevier Inc. All rights reserved.","Business process models; Measures; Thresholds"
"DRMFS: A file system layer for transparent access semantics of DRM-protected contents1","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865744922&doi=10.1016%2fj.jss.2011.12.008&partnerID=40&md5=e93427d7e6a022d0270f118978e4f5ee","In many digital rights management (DRM) schemes, only a specialized application can decode DRM-protected contents. This restriction is harmful to users because they want to use their purchased digital contents with their preferred applications. To relax this restriction, DRM technology should provide transparent access semantics of DRM-protected contents to authorized applications. Some previous schemes achieve limited transparent access semantics but have efficiency and applicability problems. In this paper, we propose a DRM control scheme at the file system layer (DRMFS) that achieves transparent access semantics of DRM-protected contents with efficiency, applicability, and portability. Since DRMFS is working at the file system layer, any authorized application can access DRM-protected content in the same way as using general files. To implement a prototype of DRMFS, we use the Filesystem in Userspace (FUSE) library that is a well known library used to develop user level file systems. We explain details of the implementation and evaluate its performance. The evaluation results show that DRMFS has acceptable overheads. © 2011 Elsevier Inc. All rights reserved.","Cryptographic file system; Digital rights management; FUSE; Transparent access semantics"
"Towards automated debugging in software evolution: Evaluating delta debugging on real regression bugs from the developers' perspectives","2012","Journal of Systems and Software","10.1016/j.jss.2011.10.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863634790&doi=10.1016%2fj.jss.2011.10.016&partnerID=40&md5=42432d104b7fa99a11ce1241f2bc1933","Delta debugging has been proposed to isolate failure-inducing changes when regressions occur. In this work, we focus on evaluating delta debugging in practical settings from developers' perspectives. A collection of real regressions taken from medium-sized open source programs is used in our evaluation. Towards automated debugging in software evolution, a tool based on delta debugging is created and both the limitations and costs are discussed. We have evaluated two variants of delta debugging. Different from successful isolation in Zeller's initial studies, the results in our experiments vary wildly. Two thirds of isolated changes in studied programs provide direct or indirect clues in locating regression bugs. The remaining results are superfluous changes or even wrong isolations. In the case of wrong isolations, the isolated changes cause the same behaviour of the regression but are failure-irrelevant. Moreover, the hierarchical variant does not yield definite improvements in terms of the efficiency and accuracy. © 2011 Elsevier Inc. All rights reserved.","Automated debugging; Delta debugging; Fault localization; Regression bugs"
"Keyword clustering for user interest profiling refinement within paper recommender systems","2012","Journal of Systems and Software","10.1016/j.jss.2011.07.029","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755181137&doi=10.1016%2fj.jss.2011.07.029&partnerID=40&md5=8210d31a78100f8ae1cad2cff24167f9","To refine user interest profiling, this paper focuses on extending scientific subject ontology via keyword clustering and on improving the accuracy and effectiveness of recommendation of the electronic academic publications in online services. A clustering approach is proposed for domain keywords for the purpose of the subject ontology extension. Based on the keyword clusters, the construction of user interest profiles is presented on a rather fine granularity level. In the construction of user interest profiles, we apply two types of interest profiles: explicit profiles and implicit profiles. The explicit profiles are obtained by relating users' interest-topic relevance factors to users' interest measurements of these topics computed by a conventional ontology-based method, and the implicit profiles are acquired on the basis of the correlative relationships among the topic nodes in topic network graphs. Three experiments are conducted which reveal that the uses of the subject ontology extension approach as well as the two types of interest profiles satisfyingly contribute to an improvement in the accuracy of recommendation. © 2011 Elsevier Inc.","Keyword clustering; Ontology extension; Recommender systems; User interest profiles; Weighted keyword graph"
"Random grid-based visual secret sharing for general access structures with cheat-preventing ability","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865761655&doi=10.1016%2fj.jss.2011.12.041&partnerID=40&md5=f4916f6d80819fe2b42007b8c2d74ff2","Conventional visual secret sharing (VSS) encodes a secret image into shares which are m times as big as the secrets. m is called pixel expansion. Random grid (RG) is an approach to solve pixel expansion problem. However, the existing VSS methods using RGs are confined to (2,n),(n,n) and ( k,n). In this paper, RG-based VSS schemes for general access structures are proposed. The proposed algorithms can encode one secret image into n random grids while qualified sets can recover the secret visually. Furthermore, a cheating immune method is also presented to provided extra ability of cheat-preventing for RG-based VSS. Experimental results demonstrate that both the RG-based VSS for general access structures and cheating immune method are effective. More complicated sharing strategies can be implemented. © 2011 Elsevier Inc. All rights reserved.","Access structure; Cheating immune; Random grid; Visual secret sharing"
"Performance evaluation of moment-based watermarking methods: A review","2012","Journal of Systems and Software","10.1016/j.jss.2012.02.045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861341251&doi=10.1016%2fj.jss.2012.02.045&partnerID=40&md5=a99a97a94bfd5603cc1a7566ed713531","This paper includes a theoretical analysis and performance investigation of representative moment-based watermarking systems. The main contribution of this work, along with the review of the moment-based watermarking algorithms' main properties (advantages and disadvantages), is the sensitivity analysis of those methods regarding some crucial parameters. Through a designed set of specific experiments, the influence of moment order and moment family (Zernike, Pseudo-Zernike, Wavelet, Krawtchouk, Tchebichef, Legendre, Fourier-Mellin) on each methods' performance is investigated and evaluated by applying geometric and signal processing attacks through the well-known benchmark Stirmark. Moreover, a comparative study regarding to methods' robustness, imperceptibility and algorithms' efficiency (time performance) is achieved. Experimental results show that moment's order calibration along with specific moment families enhances methods' performances and brings forth better tradeoffs between robustness and imperceptibility. Additionally, important issues which have been revealed through methods' implementation are discussed and proper separate solutions are proposed. © 2012 Elsevier Inc. All rights reserved.","Data hiding; Image authentication; Image watermarking; Orthogonal moments; Sensitivity analysis"
"A history-based cost-cognizant test case prioritization technique in regression testing","2012","Journal of Systems and Software","10.1016/j.jss.2011.09.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857373389&doi=10.1016%2fj.jss.2011.09.063&partnerID=40&md5=48bd112aa2c84827a13460dbf216f8de","Software testing is typically used to verify whether the developed software product meets its requirements. From the result of software testing, developers can make an assessment about the quality or the acceptability of developed software. It is noted that during testing, the test case is a pair of input and expected output, and a number of test cases will be executed either sequentially or randomly. The techniques of test case prioritization usually schedule test cases for regression testing in an order that attempts to increase the effectiveness. However, the cost of test cases and the severity of faults are usually varied. In this paper, we propose a method of cost-cognizant test case prioritization based on the use of historical records. We gather the historical records from the latest regression testing and then propose a genetic algorithm to determine the most effective order. Some controlled experiments are performed to evaluate the effectiveness of our proposed method. Evaluation results indicate that our proposed method has improved the fault detection effectiveness. It can also been found that prioritizing test cases based on their historical information can provide high test effectiveness during testing. © 2011 Elsevier Inc. All rights reserved.","Fault severity; Regression testing; Software development life cycle; Software testing; Test case prioritization"
"Coopetitive relationships in cross-functional software development teams: How to model and measure?","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865761153&doi=10.1016%2fj.jss.2011.12.027&partnerID=40&md5=c437f53325be3a91463049456db316df","Understanding simultaneous cooperative and competitive (coopetitive) dynamics in cross-functional software development teams is fundamental to the success of software development process. The recent coopetition research is, however, hampered by a lack of conceptual focus, and the corresponding inconsistent treatment of the constructs associated with cross-functional coopetitive relationships. This study conceptualizes and operationalizes the multi-dimensional construct of cross-functional coopetition, and then presents an instrument for measuring this construct. Cross-functional coopetition is conceptualized with 5 distinct and independent constructs; 3 of them are related to cross-functional cooperation, and 2 are associated with cross-functional competition. The data collected from 115 software development project managers in Australia confirms the applicability of the constructs and their measures. This study contributes to the extant literature by providing a consensus on the conceptualization of cross-functional coopetitive behaviors, particularly in multi-party software development teams. The conceptual basis for cross-functional coopetition and its instrument will aid researchers and project managers interested in understanding coopetition in cross-functional collaborative contexts. Research and practical implications are discussed. © 2011 Elsevier Inc. All rights reserved.","Competition; Cooperation; Coopetition; Cross-functional projects; Cross-functional relationships; Cross-functional software development; Cross-functional teams; Multi-party software development teams; Teams"
"The impact of accounting for special methods in the measurement of object-oriented class cohesion on refactoring and fault prediction activities","2012","Journal of Systems and Software","10.1016/j.jss.2011.12.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863423037&doi=10.1016%2fj.jss.2011.12.006&partnerID=40&md5=2bf7ef183aaa20eeea9c48540111c93a","Class cohesion is a key attribute that is used to assess the design quality of a class, and it refers to the extent to which the attributes and methods of the class are related. Typically, classes contain special types of methods, such as constructors, destructors, and access methods. Each of these special methods has its own characteristics, which can artificially affect the class cohesion measurement. Several metrics have been proposed in the literature to indicate class cohesion during high- or low-level design phases. The impact of accounting for special methods in cohesion measurement has not been addressed for most of these metrics. This paper empirically explores the impact of including or excluding special methods on cohesion measurements that were performed using 20 existing class cohesion metrics. The empirical study applies the metrics that were considered to five open-source systems under four different scenarios, including (1) considering all special methods, (2) ignoring only constructors, (3) ignoring only access methods, and (4) ignoring all special methods. This study empirically explores the impact of including special methods in cohesion measurement for two applications of interest to software practitioners, including refactoring and predicting faulty classes. The results of the empirical studies show that the cohesion values for most of the metrics considered differ significantly across the four scenarios and that this difference significantly affects the refactoring decisions, but does not significantly affect the abilities of the metrics to predict faulty classes. © 2011 Elsevier Inc. All rights reserved.","Class cohesion; Class quality; Cohesion metric; Fault prediction; Object-oriented design; Refactoring; Special methods"
"InRob: An approach for testing interoperability and robustness of real-time embedded software","2012","Journal of Systems and Software","10.1016/j.jss.2011.02.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755189031&doi=10.1016%2fj.jss.2011.02.034&partnerID=40&md5=61f506e8c768aa701cf76aef0ab3827d","Advances in digital technologies have contributed for significant reduction in accidents caused by hardware failures. However, the growing complexity of functions performed by embedded software has increased the number of accidents caused by software faults in critical systems. Moreover, due to the highly competitive market, software intensive subsystems are usually developed by different suppliers. Often these subsystems are required to interact with each other in order to provide a collaborative service. Testing approaches for subsystems integration support verification of the quality of service, focusing on the subsystems interfaces. The increasing complexity and tight coupling of real-time subsystems make integration testing unmanageable. The ad-hoc approach for testing is becoming less effective and more expensive. This article presents an integration testing approach denominated InRob, designed to verify the interoperability and robustness related to timing constraints of real-time embedded software. InRob guides the construction of services, based on formal models, aiming at the specifications of interoperability and robustness of test cases related to delays and time-outs of the messages exchanged in the interfaces of interconnected subsystems. The proposed formalism supports automatic test cases generation by verifying the relevant properties in the service behavioral model. As timing constraints are critical properties of aerospace systems, the feasibility of InRob is showed in the integration testing process of a telescope onboard in a satellite. The process is instantiated with existing testing tools and the case study is the software embedded in the telescope. © 2011 Elsevier Inc.","Automatic test case generation; Integration testing; Interoperability test; Robustness test; Service model; Timing deviations"
