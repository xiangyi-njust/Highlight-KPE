{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://pypi.org/project/pytextrank/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\software\\Python\\Python311\\Lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# example text\n",
    "text = \"Compatibility of systems of linear constraints over the set of natural numbers. Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types.\"\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"positionrank\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "# for phrase in doc._.phrases:\n",
    "#     print(phrase.text)\n",
    "#     print(phrase.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词干提取和词干还原，方便结果的比对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "def stemmer(raw_sequences):\n",
    "    stemmed_sequences = []\n",
    "\n",
    "    for i, words in enumerate(raw_sequences):\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if type(word) != str:\n",
    "                # for h_candidates and a_candidates\n",
    "                word = word[0]\n",
    "            items = word.split()\n",
    "            new_word = ' '.join(porter.stem(item) for item in items)\n",
    "            new_words.append(new_word)\n",
    "        stemmed_sequences.append(new_words)\n",
    "\n",
    "    return stemmed_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指标计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calPRF(num_c, num_e, num_s):\n",
    "    F1 = 0.0\n",
    "    P = float(num_c) / float(num_e) if num_e!=0 else 0.0\n",
    "    R = float(num_c) / float(num_s) if num_s!=0 else 0.0\n",
    "    if (P + R == 0.0):\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2 * P * R / (P + R)\n",
    "    return P, R, F1\n",
    "\n",
    "def getPRF(references, predictions, log):\n",
    "    num_c_5, num_c_10, num_c_15 = 0, 0, 0\n",
    "    num_e_5, num_e_10, num_e_15 = 0, 0, 0\n",
    "    num_s = 0\n",
    "    for i  in range(len(references)):\n",
    "        reference = references[i]\n",
    "        prediction = predictions[i]\n",
    "        j = 0\n",
    "        for candidate in prediction[:15]:\n",
    "            if candidate in reference:\n",
    "                if j<5:\n",
    "                    num_c_5 += 1\n",
    "                    num_c_10 += 1\n",
    "                    num_c_15 += 1\n",
    "                elif (j<10 and j>=5):\n",
    "                    num_c_10 += 1\n",
    "                    num_c_15 += 1\n",
    "                elif (j<15 and j>=10):\n",
    "                    num_c_15 += 1\n",
    "            j += 1\n",
    "        \n",
    "        if len(prediction[0:5]) == 5:\n",
    "            num_e_5 += 5\n",
    "        else:\n",
    "            num_e_5 += len(prediction[0:5])\n",
    "        \n",
    "        if len(prediction[0:10]) == 10:\n",
    "            num_e_10 += 10\n",
    "        else:\n",
    "            num_e_10 += len(prediction[0:10])\n",
    "        \n",
    "        if len(prediction[0:15]) == 15:\n",
    "            num_e_15 += 15\n",
    "        else:\n",
    "            num_e_15 += len(prediction[0:15])\n",
    "\n",
    "        num_s += len(reference)\n",
    "    \n",
    "    P, R, F1 = calPRF(num_c_5, num_e_5, num_s)\n",
    "    log.logger.info(\"P@5:{} R@5:{} F1@5:{}\".format(P,R,F1))\n",
    "    P, R, F1 = calPRF(num_c_10, num_e_10, num_s)\n",
    "    log.logger.info(\"P@10:{} R@10:{} F1@10:{}\".format(P,R,F1))\n",
    "    P, R, F1 = calPRF(num_c_15, num_e_15, num_s)\n",
    "    log.logger.info(\"P@15:{} R@15:{} F1@15:{}\".format(P,R,F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将结果存在日志文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class Logger(object):\n",
    "\n",
    "    def __init__(self, filename, level='info'):\n",
    "        level = logging.INFO if level == 'info' else logging.DEBUG\n",
    "        self.logger = logging.getLogger(filename)\n",
    "        self.logger.propagate = False\n",
    "        self.logger.setLevel(level)  #\n",
    "\n",
    "        th = logging.FileHandler(filename, 'a')\n",
    "\n",
    "        self.logger.addHandler(th)\n",
    "\n",
    "log = Logger('PositionRank_Pred/Elsevier-LIS.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键词抽取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeyword(texts):\n",
    "    keywords = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text.lower())\n",
    "        keyword = []\n",
    "        for phrase in doc._.phrases:\n",
    "            keyword.append(phrase.text)\n",
    "        keywords.append(keyword)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_excel('../data/Elsevier-LIS/Texts-lite-abstract.xlsx')\n",
    "links = df['Pii'].tolist()\n",
    "abs = df['Abstract'].tolist()\n",
    "hts = df['Highlights'].tolist()\n",
    "\n",
    "link_to_keywords = {}\n",
    "with open('../data/Elsevier-LIS/Keywords.json', 'r') as f:\n",
    "    link_to_keywords = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ahts, hats = [], []\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    ahts.append(abs[i] + ' ' + hts[i])\n",
    "    hats.append(hts[i] + ' ' + abs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "keywords = []\n",
    "for link in links:\n",
    "    try:\n",
    "        keywords.append(link_to_keywords[link])\n",
    "    except:\n",
    "        keywords.append([])\n",
    "\n",
    "if len(keywords) == len(abs):\n",
    "    print(\"True\")\n",
    "    labels_stem = stemmer(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 抽取未经过滤文本上的关键词集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_from_ab = extractKeyword(abs)\n",
    "keywords_from_ht = extractKeyword(hts)\n",
    "keywords_from_ah = extractKeyword(ahts)\n",
    "keywords_from_ha = extractKeyword(hats)\n",
    "\n",
    "keywords_from_ab_stem = stemmer(keywords_from_ab)\n",
    "keywords_from_ht_stem = stemmer(keywords_from_ht)\n",
    "keywords_from_ah_stem = stemmer(keywords_from_ah)\n",
    "keywords_from_ha_stem = stemmer(keywords_from_ha)\n",
    "\n",
    "results = {\n",
    "    'ab': keywords_from_ab_stem,\n",
    "    'ht': keywords_from_ht_stem,\n",
    "    'ah': keywords_from_ah_stem,\n",
    "    'ha': keywords_from_ha_stem\n",
    "}\n",
    "\n",
    "for key in results.keys():\n",
    "    log.logger.info(key)\n",
    "    getPRF(labels_stem, results[key],log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [19:46<00:00, 148.28s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "filter = [i for i in range(1,9)]\n",
    "\n",
    "for k in tqdm(filter):\n",
    "  # get data\n",
    "  key = 'reserve_' + str(k)\n",
    "  log.logger.info(key)\n",
    "\n",
    "  abs = df[key]\n",
    "  ahts, hats = [] , []\n",
    "  for i, text in enumerate(abs):\n",
    "    ahts.append(text + ' ' + hts[i])\n",
    "    hats.append(hts[i] + ' ' + text)\n",
    "\n",
    "  # extract keywords\n",
    "  keywords_from_ab = extractKeyword(abs)\n",
    "  keywords_from_aht = extractKeyword(ahts)\n",
    "  keywords_from_hat = extractKeyword(hats)\n",
    "\n",
    "  # stemmer\n",
    "  keywords_from_ab_stem = stemmer(keywords_from_ab)\n",
    "  keywords_from_aht_stem = stemmer(keywords_from_aht)\n",
    "  keywords_from_hat_stem = stemmer(keywords_from_hat)\n",
    "\n",
    "  # rejust the data structure\n",
    "  results = {\n",
    "      'ab': keywords_from_ab_stem,\n",
    "      'aht': keywords_from_aht_stem,\n",
    "      'hat': keywords_from_hat_stem\n",
    "  }\n",
    "\n",
    "  # evaluation\n",
    "  for key in results.keys():\n",
    "    log.logger.info(key)\n",
    "    getPRF(labels_stem, results[key],log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
